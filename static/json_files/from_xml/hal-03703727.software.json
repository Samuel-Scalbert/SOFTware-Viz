{"application": "software-mentions", "version": "0.8.0", "date": "2024-03-21T10:04+0000", "md5": "FE76C09D51BC1512E360DD0D8D019954", "mentions": [{"type": "software", "software-type": "software", "software-name": {"rawForm": "SimR", "normalizedForm": "SimR", "offsetStart": 3, "offsetEnd": 7}, "context": "MB SimR significantly contributed in the case of non-stationarity, but a fruitful coordination with MF MemR became crucial in terms of computational cost reduction.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9502077698707581}, "created": {"value": false, "score": 0.00011038780212402344}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999381303787231}, "created": {"value": true, "score": 0.9765201807022095}, "shared": {"value": false, "score": 4.76837158203125e-07}}}, {"type": "software", "software-type": "implicit", "software-name": {"rawForm": "code", "normalizedForm": "code", "offsetStart": 4, "offsetEnd": 8}, "url": {"rawForm": "https://github.com/elimas9/", "normalizedForm": "https://github.com/elimas9"}, "context": "The code for these simulations is available at https://github.com/estherponiatowski/Massi2022.", "mentionContextAttributes": {"used": {"value": false, "score": 0.006973624229431152}, "created": {"value": false, "score": 0.00018101930618286133}, "shared": {"value": true, "score": 0.9931398034095764}}, "documentContextAttributes": {"used": {"value": false, "score": 0.006973624229431152}, "created": {"value": false, "score": 0.0008077025413513184}, "shared": {"value": true, "score": 0.9943554401397705}}}, {"type": "software", "software-type": "implicit", "software-name": {"rawForm": "code", "normalizedForm": "code", "offsetStart": 4, "offsetEnd": 8}, "url": {"rawForm": "https://github.com/elimas9/", "normalizedForm": "https://github.com/elimas9", "offsetStart": 47, "offsetEnd": 74}, "context": "The code for these simulations is available at https://github.com/elimas9/ ", "mentionContextAttributes": {"used": {"value": false, "score": 0.0025986433029174805}, "created": {"value": false, "score": 0.0008077025413513184}, "shared": {"value": true, "score": 0.9943554401397705}}, "documentContextAttributes": {"used": {"value": false, "score": 0.006973624229431152}, "created": {"value": false, "score": 0.0008077025413513184}, "shared": {"value": true, "score": 0.9943554401397705}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "MemR", "normalizedForm": "MemR", "offsetStart": 8, "offsetEnd": 12}, "context": "MF with MemR and MB + MF without MemR have very similar distances to the optimal points, meaning that the contribution of the MB expert is key to adapting to a dynamical environment, but the cost of this computation can largely decrease just when it cooperates with an MF agent with replay, that can learn faster also from the Q-values update of the MB expert.", "mentionContextAttributes": {"used": {"value": false, "score": 0.011326730251312256}, "created": {"value": false, "score": 7.62939453125e-06}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999381303787231}, "created": {"value": true, "score": 0.9765201807022095}, "shared": {"value": false, "score": 4.76837158203125e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "MemR", "normalizedForm": "MemR", "offsetStart": 27, "offsetEnd": 31}, "context": "This suggests that both MF MemR and MB SimR are required to account for the diversity of hippocampal replays.", "mentionContextAttributes": {"used": {"value": false, "score": 0.07881289720535278}, "created": {"value": false, "score": 7.3909759521484375e-06}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999381303787231}, "created": {"value": true, "score": 0.9765201807022095}, "shared": {"value": false, "score": 4.76837158203125e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "MemR", "normalizedForm": "MemR", "offsetStart": 33, "offsetEnd": 37}, "context": "MF with MemR and MB + MF without MemR have very similar distances to the optimal points, meaning that the contribution of the MB expert is key to adapting to a dynamical environment, but the cost of this computation can largely decrease just when it cooperates with an MF agent with replay, that can learn faster also from the Q-values update of the MB expert.", "mentionContextAttributes": {"used": {"value": false, "score": 0.011326730251312256}, "created": {"value": false, "score": 7.62939453125e-06}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999381303787231}, "created": {"value": true, "score": 0.9765201807022095}, "shared": {"value": false, "score": 4.76837158203125e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "MemR", "normalizedForm": "MemR", "offsetStart": 36, "offsetEnd": 40}, "context": "In Section 4, the combination of MF MemR and MB SimR is presented as an interesting proposal to merge the benefits of both techniques: prioritizing the MB expert when the task requires more inference and generalization effectiveness to be solved (for example facing non-stationarity), while on the contrary giving priority to the MF expert when an effective solution can be found relying only on recent experience.", "mentionContextAttributes": {"used": {"value": false, "score": 0.001324474811553955}, "created": {"value": false, "score": 0.000756382942199707}, "shared": {"value": false, "score": 4.76837158203125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999381303787231}, "created": {"value": true, "score": 0.9765201807022095}, "shared": {"value": false, "score": 4.76837158203125e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "MemR", "normalizedForm": "MemR", "offsetStart": 37, "offsetEnd": 41}, "context": "Here, shuffled memory reactivations (MemR) are integrated with the Q-learning algorithm of the MF expert, while simulation reactivations (SimR) constitute the offline MB inference iterations in the value iteration algorithm of the MB expert.", "mentionContextAttributes": {"used": {"value": true, "score": 0.7545603513717651}, "created": {"value": false, "score": 1.2516975402832031e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999381303787231}, "created": {"value": true, "score": 0.9765201807022095}, "shared": {"value": false, "score": 4.76837158203125e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "SimR", "normalizedForm": "SimR", "offsetStart": 39, "offsetEnd": 43}, "context": "This suggests that both MF MemR and MB SimR are required to account for the diversity of hippocampal replays.", "mentionContextAttributes": {"used": {"value": false, "score": 0.07881289720535278}, "created": {"value": false, "score": 7.3909759521484375e-06}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999381303787231}, "created": {"value": true, "score": 0.9765201807022095}, "shared": {"value": false, "score": 4.76837158203125e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "SimR", "normalizedForm": "SimR", "offsetStart": 48, "offsetEnd": 52}, "context": "In Section 4, the combination of MF MemR and MB SimR is presented as an interesting proposal to merge the benefits of both techniques: prioritizing the MB expert when the task requires more inference and generalization effectiveness to be solved (for example facing non-stationarity), while on the contrary giving priority to the MF expert when an effective solution can be found relying only on recent experience.", "mentionContextAttributes": {"used": {"value": false, "score": 0.001324474811553955}, "created": {"value": false, "score": 0.000756382942199707}, "shared": {"value": false, "score": 4.76837158203125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999381303787231}, "created": {"value": true, "score": 0.9765201807022095}, "shared": {"value": false, "score": 4.76837158203125e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "SimR", "normalizedForm": "SimR", "offsetStart": 61, "offsetEnd": 65}, "context": "We will refer to model sampling as Simulation Reactivations (SimR) and sampling from a memory buffer as Memory Reactivations (MemR).", "mentionContextAttributes": {"used": {"value": false, "score": 0.10384416580200195}, "created": {"value": false, "score": 0.17129111289978027}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999381303787231}, "created": {"value": true, "score": 0.9765201807022095}, "shared": {"value": false, "score": 4.76837158203125e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "MemR", "normalizedForm": "MemR", "offsetStart": 61, "offsetEnd": 65}, "context": "In this section, the same MF-RL and MB-RL replay strategies (MemR and SimR, Sections 1, 2) are tested in a more realistic robotic set-up, where the discretization of the environment in multiple Markovian states is autonomously performed by the robot.2", "mentionContextAttributes": {"used": {"value": true, "score": 0.892555296421051}, "created": {"value": false, "score": 3.4928321838378906e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999381303787231}, "created": {"value": true, "score": 0.9765201807022095}, "shared": {"value": false, "score": 4.76837158203125e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "SimR", "normalizedForm": "SimR", "offsetStart": 70, "offsetEnd": 74}, "context": "In this section, the same MF-RL and MB-RL replay strategies (MemR and SimR, Sections 1, 2) are tested in a more realistic robotic set-up, where the discretization of the environment in multiple Markovian states is autonomously performed by the robot.2", "mentionContextAttributes": {"used": {"value": true, "score": 0.892555296421051}, "created": {"value": false, "score": 3.4928321838378906e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999381303787231}, "created": {"value": true, "score": 0.9765201807022095}, "shared": {"value": false, "score": 4.76837158203125e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "ROS Gazebo", "normalizedForm": "ROS Gazebo", "offsetStart": 91, "offsetEnd": 101}, "context": "The simulated implementation of this Simultaneous Location and Mapping Algorithm (SLAM) on ROS Gazebo is called GMapping.", "mentionContextAttributes": {"used": {"value": false, "score": 0.0003510713577270508}, "created": {"value": false, "score": 0.008321642875671387}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.8340436220169067}, "created": {"value": false, "score": 0.008321642875671387}, "shared": {"value": false, "score": 1.1920928955078125e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "MemR", "normalizedForm": "MemR", "offsetStart": 103, "offsetEnd": 107}, "context": "MB SimR significantly contributed in the case of non-stationarity, but a fruitful coordination with MF MemR became crucial in terms of computational cost reduction.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9502077698707581}, "created": {"value": false, "score": 0.00011038780212402344}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999381303787231}, "created": {"value": true, "score": 0.9765201807022095}, "shared": {"value": false, "score": 4.76837158203125e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "SimR", "normalizedForm": "SimR", "offsetStart": 107, "offsetEnd": 111}, "context": "This is because MB-RL prioritized sweeping does not replay memorized past experience, but rather generates SimR through model sampling combined with the value iteration algorithm described above. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.12000608444213867}, "created": {"value": false, "score": 8.702278137207031e-06}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999381303787231}, "created": {"value": true, "score": 0.9765201807022095}, "shared": {"value": false, "score": 4.76837158203125e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "SimR", "normalizedForm": "SimR", "offsetStart": 112, "offsetEnd": 116}, "context": "The results that we are going to illustrate and discuss in the following subsections present the combination of SimR and MemR as a critical resource to optimize the trade-off between the increase in performance and the reduction of computational cost in a hybrid MB-MF RL architecture when solving a more complex non-stationary navigation task than the two previous sections.", "mentionContextAttributes": {"used": {"value": false, "score": 0.007111489772796631}, "created": {"value": true, "score": 0.7250903248786926}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999381303787231}, "created": {"value": true, "score": 0.9765201807022095}, "shared": {"value": false, "score": 4.76837158203125e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "GMapping", "normalizedForm": "GMapping", "offsetStart": 112, "offsetEnd": 120}, "context": "The simulated implementation of this Simultaneous Location and Mapping Algorithm (SLAM) on ROS Gazebo is called GMapping.", "mentionContextAttributes": {"used": {"value": false, "score": 0.0003510713577270508}, "created": {"value": false, "score": 0.008321642875671387}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": false, "score": 0.0003510713577270508}, "created": {"value": false, "score": 0.008321642875671387}, "shared": {"value": false, "score": 1.1920928955078125e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "MemR", "normalizedForm": "MemR", "offsetStart": 121, "offsetEnd": 125}, "context": "The results that we are going to illustrate and discuss in the following subsections present the combination of SimR and MemR as a critical resource to optimize the trade-off between the increase in performance and the reduction of computational cost in a hybrid MB-MF RL architecture when solving a more complex non-stationary navigation task than the two previous sections.", "mentionContextAttributes": {"used": {"value": false, "score": 0.007111489772796631}, "created": {"value": true, "score": 0.7250903248786926}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999381303787231}, "created": {"value": true, "score": 0.9765201807022095}, "shared": {"value": false, "score": 4.76837158203125e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "MemR", "normalizedForm": "MemR", "offsetStart": 126, "offsetEnd": 130}, "context": "We will refer to model sampling as Simulation Reactivations (SimR) and sampling from a memory buffer as Memory Reactivations (MemR).", "mentionContextAttributes": {"used": {"value": false, "score": 0.10384416580200195}, "created": {"value": false, "score": 0.17129111289978027}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999381303787231}, "created": {"value": true, "score": 0.9765201807022095}, "shared": {"value": false, "score": 4.76837158203125e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "ROS Gazebo", "normalizedForm": "ROS Gazebo", "offsetStart": 133, "offsetEnd": 143}, "context": "(A) In the case of the circular maze (Section 3), the navigation and the transition model are acquired after simulated navigation on ROS Gazebo. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.8340436220169067}, "created": {"value": false, "score": 5.3763389587402344e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.8340436220169067}, "created": {"value": false, "score": 0.008321642875671387}, "shared": {"value": false, "score": 1.1920928955078125e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "MemR", "normalizedForm": "MemR", "offsetStart": 136, "offsetEnd": 140}, "context": "First, we are interested in simulating the two baseline cases, pure MF and pure MB algorithms, and how they perform with the respective MemR and SimR and limited budgets.", "mentionContextAttributes": {"used": {"value": false, "score": 0.006107687950134277}, "created": {"value": true, "score": 0.9765201807022095}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999381303787231}, "created": {"value": true, "score": 0.9765201807022095}, "shared": {"value": false, "score": 4.76837158203125e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "SimR", "normalizedForm": "SimR", "offsetStart": 138, "offsetEnd": 142}, "context": "Here, shuffled memory reactivations (MemR) are integrated with the Q-learning algorithm of the MF expert, while simulation reactivations (SimR) constitute the offline MB inference iterations in the value iteration algorithm of the MB expert.", "mentionContextAttributes": {"used": {"value": true, "score": 0.7545603513717651}, "created": {"value": false, "score": 1.2516975402832031e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999381303787231}, "created": {"value": true, "score": 0.9765201807022095}, "shared": {"value": false, "score": 4.76837158203125e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "SimR", "normalizedForm": "SimR", "offsetStart": 145, "offsetEnd": 149}, "context": "First, we are interested in simulating the two baseline cases, pure MF and pure MB algorithms, and how they perform with the respective MemR and SimR and limited budgets.", "mentionContextAttributes": {"used": {"value": false, "score": 0.006107687950134277}, "created": {"value": true, "score": 0.9765201807022095}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999381303787231}, "created": {"value": true, "score": 0.9765201807022095}, "shared": {"value": false, "score": 4.76837158203125e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "SimR", "normalizedForm": "SimR", "offsetStart": 148, "offsetEnd": 152}, "context": "In this case, to have the same total reactivations budget as the other tested algorithm, we have shared the initial 200 reactivations budget to 100 SimR for the MB expert and 100 MemR for the MF one.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9999381303787231}, "created": {"value": false, "score": 3.135204315185547e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999381303787231}, "created": {"value": true, "score": 0.9765201807022095}, "shared": {"value": false, "score": 4.76837158203125e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "DeepRL", "normalizedForm": "DeepRL", "offsetStart": 152, "offsetEnd": 158}, "context": "As mentioned above, this focus on offline reactivations is both inspired by the machine learning techniques created in the 90s and now commonly used in DeepRL and by the neuroscience results on hippocampal reactivations and the probable cohabitation of MB and MF RL systems in the brain. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.0002682805061340332}, "created": {"value": false, "score": 0.0067476630210876465}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "documentContextAttributes": {"used": {"value": false, "score": 0.0002682805061340332}, "created": {"value": false, "score": 0.0067476630210876465}, "shared": {"value": false, "score": 3.5762786865234375e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "SimR", "normalizedForm": "SimR", "offsetStart": 176, "offsetEnd": 180}, "context": "In this section, in addition to pushing robot simulations toward more complex environments with stochasticity and nonstationarity, we want to examine the benefits of combining SimR and MemR in a robot control architecture which includes both MB and MF RL3 .", "mentionContextAttributes": {"used": {"value": false, "score": 9.453296661376953e-05}, "created": {"value": true, "score": 0.8190985918045044}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999381303787231}, "created": {"value": true, "score": 0.9765201807022095}, "shared": {"value": false, "score": 4.76837158203125e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "MemR", "normalizedForm": "MemR", "offsetStart": 179, "offsetEnd": 183}, "context": "In this case, to have the same total reactivations budget as the other tested algorithm, we have shared the initial 200 reactivations budget to 100 SimR for the MB expert and 100 MemR for the MF one.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9999381303787231}, "created": {"value": false, "score": 3.135204315185547e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999381303787231}, "created": {"value": true, "score": 0.9765201807022095}, "shared": {"value": false, "score": 4.76837158203125e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "MemR", "normalizedForm": "MemR", "offsetStart": 185, "offsetEnd": 189}, "context": "In this section, in addition to pushing robot simulations toward more complex environments with stochasticity and nonstationarity, we want to examine the benefits of combining SimR and MemR in a robot control architecture which includes both MB and MF RL3 .", "mentionContextAttributes": {"used": {"value": false, "score": 9.453296661376953e-05}, "created": {"value": true, "score": 0.8190985918045044}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999381303787231}, "created": {"value": true, "score": 0.9765201807022095}, "shared": {"value": false, "score": 4.76837158203125e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "SimR", "normalizedForm": "SimR", "offsetStart": 200, "offsetEnd": 204}, "context": "Starting with simpler and deterministic environments, as the double T-maze experiment presented in Section 2, this research illustrates that as the complexity of the state-action spaces increases, MB SimR become more strategic for the learning capabilities of the agent (Section 3).", "mentionContextAttributes": {"used": {"value": false, "score": 0.00030028820037841797}, "created": {"value": false, "score": 0.0003813505172729492}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999381303787231}, "created": {"value": true, "score": 0.9765201807022095}, "shared": {"value": false, "score": 4.76837158203125e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "SimR", "normalizedForm": "SimR", "offsetStart": 295, "offsetEnd": 299}, "context": "The computed Chebyshev distances are shown in Figure 9C, on the side of each algorithm point, and show a clear picture concerning the proposed solutions; the agent sharing the reactivations budget between the MB and MF is the closest to the optimal point, followed by the MB expert with limited SimR budget.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9996353387832642}, "created": {"value": false, "score": 9.298324584960938e-06}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999381303787231}, "created": {"value": true, "score": 0.9765201807022095}, "shared": {"value": false, "score": 4.76837158203125e-07}}}], "references": [], "runtime": 16320, "id": "4ae7e1bcb516ca9c12211b76156ff729f43c538b", "metadata": {"id": "4ae7e1bcb516ca9c12211b76156ff729f43c538b"}, "original_file_path": "../../datalake/Samuel/SV22/SV22_xml/hal-03703727.grobid.tei.xml", "file_name": "hal-03703727.grobid.tei.xml"}