{"application": "software-mentions", "version": "0.8.0", "date": "2024-03-21T09:54+0000", "md5": "E119FA39694C45E5DC8BFDD58EB8D273", "mentions": [{"type": "software", "software-type": "software", "software-name": {"rawForm": "ELMo", "normalizedForm": "ELMo", "offsetStart": 0, "offsetEnd": 4}, "context": "ELMo: ELMo (Embeddings from Language Models) is a successful NLP framework developed by the Al-lenNLP (Peters et al., 2018) group.", "mentionContextAttributes": {"used": {"value": false, "score": 0.0001035928726196289}, "created": {"value": false, "score": 0.4067387580871582}, "shared": {"value": false, "score": 5.960464477539062e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999966621398926}, "created": {"value": true, "score": 0.9998438358306885}, "shared": {"value": false, "score": 2.6226043701171875e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Longformer", "normalizedForm": "Longformer", "offsetStart": 0, "offsetEnd": 10}, "context": "Longformer (Beltagy et al., 2020) builds on BERT's language masking strategy and supports long document generative sequence-to-sequence tasks.", "mentionContextAttributes": {"used": {"value": false, "score": 3.4689903259277344e-05}, "created": {"value": false, "score": 0.027254164218902588}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999966621398926}, "created": {"value": true, "score": 0.9998438358306885}, "shared": {"value": false, "score": 2.6226043701171875e-06}}, "references": [{"label": "(Beltagy et al., 2020)", "normalizedForm": "Beltagy et al., 2020", "refKey": 5, "offsetStart": 11246, "offsetEnd": 11268}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "ELMo", "normalizedForm": "ELMo", "offsetStart": 6, "offsetEnd": 10}, "context": "ELMo: ELMo (Embeddings from Language Models) is a successful NLP framework developed by the Al-lenNLP (Peters et al., 2018) group.", "mentionContextAttributes": {"used": {"value": false, "score": 0.0001035928726196289}, "created": {"value": false, "score": 0.4067387580871582}, "shared": {"value": false, "score": 5.960464477539062e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999966621398926}, "created": {"value": true, "score": 0.9998438358306885}, "shared": {"value": false, "score": 2.6226043701171875e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "ELMo", "normalizedForm": "ELMo", "offsetStart": 16, "offsetEnd": 20}, "context": "Language models ELMo, and Longformer yield better performance in predicting the brain responses than the LSTM model across all the ROIs.", "mentionContextAttributes": {"used": {"value": false, "score": 0.007526397705078125}, "created": {"value": false, "score": 1.3589859008789062e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999966621398926}, "created": {"value": true, "score": 0.9998438358306885}, "shared": {"value": false, "score": 2.6226043701171875e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Longformer", "normalizedForm": "Longformer", "offsetStart": 22, "offsetEnd": 32}, "context": "We use the pretrained Longformer model with a local attention mechanism, where the default window size is set to 5. To obtain the stimuli representation, we use the last layer token representations where each token dimension is 768.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9995063543319702}, "created": {"value": false, "score": 8.463859558105469e-06}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999966621398926}, "created": {"value": true, "score": 0.9998438358306885}, "shared": {"value": false, "score": 2.6226043701171875e-06}}, "references": [{"label": "(Beltagy et al., 2020)", "normalizedForm": "Beltagy et al., 2020", "refKey": 5}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Longformer", "normalizedForm": "Longformer", "offsetStart": 26, "offsetEnd": 36}, "context": "Language models ELMo, and Longformer yield better performance in predicting the brain responses than the LSTM model across all the ROIs.", "mentionContextAttributes": {"used": {"value": false, "score": 0.007526397705078125}, "created": {"value": false, "score": 1.3589859008789062e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999966621398926}, "created": {"value": true, "score": 0.9998438358306885}, "shared": {"value": false, "score": 2.6226043701171875e-06}}, "references": [{"label": "(Beltagy et al., 2020)", "normalizedForm": "Beltagy et al., 2020", "refKey": 5}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "ELMo", "normalizedForm": "ELMo", "offsetStart": 27, "offsetEnd": 31}, "context": "Unlike earlier embeddings, ELMo embeddings represent words in a contextual fashion using a bidirectional LSTM model.", "mentionContextAttributes": {"used": {"value": false, "score": 9.322166442871094e-05}, "created": {"value": false, "score": 2.5153160095214844e-05}, "shared": {"value": false, "score": 4.76837158203125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999966621398926}, "created": {"value": true, "score": 0.9998438358306885}, "shared": {"value": false, "score": 2.6226043701171875e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Longformer", "normalizedForm": "Longformer", "offsetStart": 28, "offsetEnd": 38}, "context": "The main reason is that the Longformer is designed to process documents of thousands of tokens or longer sequences while GPT-2 models are unable to handle the long-term dependencies (sequence length is fixed to 512 words).", "mentionContextAttributes": {"used": {"value": false, "score": 0.0004032254219055176}, "created": {"value": false, "score": 0.00010073184967041016}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999966621398926}, "created": {"value": true, "score": 0.9998438358306885}, "shared": {"value": false, "score": 2.6226043701171875e-06}}, "references": [{"label": "(Beltagy et al., 2020)", "normalizedForm": "Beltagy et al., 2020", "refKey": 5}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Longformer", "normalizedForm": "Longformer", "offsetStart": 35, "offsetEnd": 45}, "context": "We compare the LSTM model with the Longformer and several other pretrained language models: Random LSTM, ELMo, and GloVe.", "mentionContextAttributes": {"used": {"value": false, "score": 0.23880189657211304}, "created": {"value": false, "score": 1.5735626220703125e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999966621398926}, "created": {"value": true, "score": 0.9998438358306885}, "shared": {"value": false, "score": 2.6226043701171875e-06}}, "references": [{"label": "(Beltagy et al., 2020)", "normalizedForm": "Beltagy et al., 2020", "refKey": 5}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Longformer", "normalizedForm": "Longformer", "offsetStart": 37, "offsetEnd": 47}, "context": "We used Word-Piece tokenizer for the Longformer model and Spacy-tokenizer for the GloVe and ELMo models.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9998948574066162}, "created": {"value": false, "score": 1.4901161193847656e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999966621398926}, "created": {"value": true, "score": 0.9998438358306885}, "shared": {"value": false, "score": 2.6226043701171875e-06}}, "references": [{"label": "(Beltagy et al., 2020)", "normalizedForm": "Beltagy et al., 2020", "refKey": 5}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "ELMo", "normalizedForm": "ELMo", "offsetStart": 38, "offsetEnd": 42}, "context": "We investigate how the performance of ELMo changes at different layers (Embedding layer, LSTM layer-1, and LSTM layer-2), as they are provided in different contexts.", "mentionContextAttributes": {"used": {"value": false, "score": 0.0025311708450317383}, "created": {"value": false, "score": 4.088878631591797e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999966621398926}, "created": {"value": true, "score": 0.9998438358306885}, "shared": {"value": false, "score": 2.6226043701171875e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "ELMo", "normalizedForm": "ELMo", "offsetStart": 41, "offsetEnd": 45}, "context": "Here, we do not train Random LSTM, LSTM, ELMo, and Longformer networks to directly predict brain activities, for two reasons.", "mentionContextAttributes": {"used": {"value": false, "score": 0.003558933734893799}, "created": {"value": false, "score": 0.02697920799255371}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999966621398926}, "created": {"value": true, "score": 0.9998438358306885}, "shared": {"value": false, "score": 2.6226043701171875e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "ELMo", "normalizedForm": "ELMo", "offsetStart": 45, "offsetEnd": 49}, "context": "We encoded the number of words same for both ELMo and Longformer to match the performance.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9999936819076538}, "created": {"value": false, "score": 0.0005941390991210938}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999966621398926}, "created": {"value": true, "score": 0.9998438358306885}, "shared": {"value": false, "score": 2.6226043701171875e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Longformer", "normalizedForm": "Longformer", "offsetStart": 51, "offsetEnd": 61}, "context": "Here, we do not train Random LSTM, LSTM, ELMo, and Longformer networks to directly predict brain activities, for two reasons.", "mentionContextAttributes": {"used": {"value": false, "score": 0.003558933734893799}, "created": {"value": false, "score": 0.02697920799255371}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999966621398926}, "created": {"value": true, "score": 0.9998438358306885}, "shared": {"value": false, "score": 2.6226043701171875e-06}}, "references": [{"label": "(Beltagy et al., 2020)", "normalizedForm": "Beltagy et al., 2020", "refKey": 5}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Longformer", "normalizedForm": "Longformer", "offsetStart": 51, "offsetEnd": 61}, "context": "In the listening task, we observe from Fig. 5 that Longformer displays higher correlation values for many voxels than ELMo.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9569091200828552}, "created": {"value": false, "score": 2.0384788513183594e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999966621398926}, "created": {"value": true, "score": 0.9998438358306885}, "shared": {"value": false, "score": 2.6226043701171875e-06}}, "references": [{"label": "(Beltagy et al., 2020)", "normalizedForm": "Beltagy et al., 2020", "refKey": 5}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Longformer", "normalizedForm": "Longformer", "offsetStart": 54, "offsetEnd": 64}, "context": "We encoded the number of words same for both ELMo and Longformer to match the performance.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9999936819076538}, "created": {"value": false, "score": 0.0005941390991210938}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999966621398926}, "created": {"value": true, "score": 0.9998438358306885}, "shared": {"value": false, "score": 2.6226043701171875e-06}}, "references": [{"label": "(Beltagy et al., 2020)", "normalizedForm": "Beltagy et al., 2020", "refKey": 5}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "ELMo", "normalizedForm": "ELMo", "offsetStart": 72, "offsetEnd": 76}, "context": "The whole-brain prediction Pearson correlation for all the voxels using ELMo and Longformer is shown in Fig. 5.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9998117089271545}, "created": {"value": false, "score": 2.1457672119140625e-06}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999966621398926}, "created": {"value": true, "score": 0.9998438358306885}, "shared": {"value": false, "score": 2.6226043701171875e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "ELMo", "normalizedForm": "ELMo", "offsetStart": 81, "offsetEnd": 85}, "context": "(2) The investigation of longterm context of language model results reveals that ELMo and Longformer representations display better correlation during narrative story listening.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9439234733581543}, "created": {"value": false, "score": 1.8358230590820312e-05}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999966621398926}, "created": {"value": true, "score": 0.9998438358306885}, "shared": {"value": false, "score": 2.6226043701171875e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Longformer", "normalizedForm": "Longformer", "offsetStart": 81, "offsetEnd": 91}, "context": "The whole-brain prediction Pearson correlation for all the voxels using ELMo and Longformer is shown in Fig. 5.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9998117089271545}, "created": {"value": false, "score": 2.1457672119140625e-06}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999966621398926}, "created": {"value": true, "score": 0.9998438358306885}, "shared": {"value": false, "score": 2.6226043701171875e-06}}, "references": [{"label": "(Beltagy et al., 2020)", "normalizedForm": "Beltagy et al., 2020", "refKey": 5}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Longformer", "normalizedForm": "Longformer", "offsetStart": 83, "offsetEnd": 93}, "context": "To overcome this limitation, recently, (Beltagy, Peters, & Cohan, 2020) introduced Longformer making it easy to process documents of thousands of tokens or longer and combining local windowed attention with global attention.", "mentionContextAttributes": {"used": {"value": false, "score": 6.711483001708984e-05}, "created": {"value": true, "score": 0.9391965866088867}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999966621398926}, "created": {"value": true, "score": 0.9998438358306885}, "shared": {"value": false, "score": 2.6226043701171875e-06}}, "references": [{"label": "(Beltagy et al., 2020)", "normalizedForm": "Beltagy et al., 2020", "refKey": 5}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "ELMo", "normalizedForm": "ELMo", "offsetStart": 85, "offsetEnd": 89}, "context": "ROIs, including bilateral TPOJ and bilateral DFL, yield higher correlations with the ELMo and Longformer, which is in line with the language processing hierarchy in the human brain.", "mentionContextAttributes": {"used": {"value": false, "score": 0.005942583084106445}, "created": {"value": false, "score": 2.777576446533203e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999966621398926}, "created": {"value": true, "score": 0.9998438358306885}, "shared": {"value": false, "score": 2.6226043701171875e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "ELMo", "normalizedForm": "ELMo", "offsetStart": 88, "offsetEnd": 114}, "context": "We also investigate how the pretrained bi-directional sequence embedding language model ELMo (Peters et al., 2018) handles the longer context and interprets the LSTM layers representations that better predict brain activity.", "mentionContextAttributes": {"used": {"value": false, "score": 0.0018746256828308105}, "created": {"value": false, "score": 0.00631338357925415}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999966621398926}, "created": {"value": true, "score": 0.9998438358306885}, "shared": {"value": false, "score": 2.6226043701171875e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Longformer", "normalizedForm": "Longformer", "offsetStart": 90, "offsetEnd": 100}, "context": "(2) The investigation of longterm context of language model results reveals that ELMo and Longformer representations display better correlation during narrative story listening.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9439234733581543}, "created": {"value": false, "score": 1.8358230590820312e-05}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999966621398926}, "created": {"value": true, "score": 0.9998438358306885}, "shared": {"value": false, "score": 2.6226043701171875e-06}}, "references": [{"label": "(Beltagy et al., 2020)", "normalizedForm": "Beltagy et al., 2020", "refKey": 5}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "ELMo", "normalizedForm": "ELMo", "offsetStart": 92, "offsetEnd": 96}, "context": "We used Word-Piece tokenizer for the Longformer model and Spacy-tokenizer for the GloVe and ELMo models.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9998948574066162}, "created": {"value": false, "score": 1.4901161193847656e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999966621398926}, "created": {"value": true, "score": 0.9998438358306885}, "shared": {"value": false, "score": 2.6226043701171875e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Longformer", "normalizedForm": "Longformer", "offsetStart": 94, "offsetEnd": 104}, "context": "ROIs, including bilateral TPOJ and bilateral DFL, yield higher correlations with the ELMo and Longformer, which is in line with the language processing hierarchy in the human brain. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.005942583084106445}, "created": {"value": false, "score": 2.777576446533203e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999966621398926}, "created": {"value": true, "score": 0.9998438358306885}, "shared": {"value": false, "score": 2.6226043701171875e-06}}, "references": [{"label": "(Beltagy et al., 2020)", "normalizedForm": "Beltagy et al., 2020", "refKey": 5}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "ELMo", "normalizedForm": "ELMo", "offsetStart": 105, "offsetEnd": 109}, "context": "We compare the LSTM model with the Longformer and several other pretrained language models: Random LSTM, ELMo, and GloVe.", "mentionContextAttributes": {"used": {"value": false, "score": 0.23880189657211304}, "created": {"value": false, "score": 1.5735626220703125e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999966621398926}, "created": {"value": true, "score": 0.9998438358306885}, "shared": {"value": false, "score": 2.6226043701171875e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "ELMo", "normalizedForm": "ELMo", "offsetStart": 106, "offsetEnd": 110}, "context": "(2) We experimented with several language models where the pretrained context representations (such as in ELMo and Longformer) are better predictors of voxel activations.", "mentionContextAttributes": {"used": {"value": false, "score": 0.06219065189361572}, "created": {"value": false, "score": 0.10923361778259277}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999966621398926}, "created": {"value": true, "score": 0.9998438358306885}, "shared": {"value": false, "score": 2.6226043701171875e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "ELMo", "normalizedForm": "ELMo", "offsetStart": 110, "offsetEnd": 114}, "context": "We further analyse in more detail the prediction performance of the encoder model trained on sub ROIs for the ELMo and Longformer in Fig. 4. In the EAC, the sub ROI pbelt (parabelt) display higher Pearson correlation among other sub ROIs, and it is adjacent to the lateral belt on the exposed surface of the superior temporal gyrus (STG).", "mentionContextAttributes": {"used": {"value": true, "score": 0.9997531771659851}, "created": {"value": false, "score": 4.5299530029296875e-06}, "shared": {"value": false, "score": 5.960464477539062e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999966621398926}, "created": {"value": true, "score": 0.9998438358306885}, "shared": {"value": false, "score": 2.6226043701171875e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Longformer", "normalizedForm": "Longformer", "offsetStart": 115, "offsetEnd": 125}, "context": "(2) We experimented with several language models where the pretrained context representations (such as in ELMo and Longformer) are better predictors of voxel activations.", "mentionContextAttributes": {"used": {"value": false, "score": 0.06219065189361572}, "created": {"value": false, "score": 0.10923361778259277}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999966621398926}, "created": {"value": true, "score": 0.9998438358306885}, "shared": {"value": false, "score": 2.6226043701171875e-06}}, "references": [{"label": "(Beltagy et al., 2020)", "normalizedForm": "Beltagy et al., 2020", "refKey": 5}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "ELMo", "normalizedForm": "ELMo", "offsetStart": 118, "offsetEnd": 122}, "context": "In the listening task, we observe from Fig. 5 that Longformer displays higher correlation values for many voxels than ELMo.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9569091200828552}, "created": {"value": false, "score": 2.0384788513183594e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999966621398926}, "created": {"value": true, "score": 0.9998438358306885}, "shared": {"value": false, "score": 2.6226043701171875e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Longformer", "normalizedForm": "Longformer", "offsetStart": 119, "offsetEnd": 129}, "context": "We further analyse in more detail the prediction performance of the encoder model trained on sub ROIs for the ELMo and Longformer in Fig. 4. In the EAC, the sub ROI pbelt (parabelt) display higher Pearson correlation among other sub ROIs, and it is adjacent to the lateral belt on the exposed surface of the superior temporal gyrus (STG).", "mentionContextAttributes": {"used": {"value": true, "score": 0.9997531771659851}, "created": {"value": false, "score": 4.5299530029296875e-06}, "shared": {"value": false, "score": 5.960464477539062e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999966621398926}, "created": {"value": true, "score": 0.9998438358306885}, "shared": {"value": false, "score": 2.6226043701171875e-06}}, "references": [{"label": "(Beltagy et al., 2020)", "normalizedForm": "Beltagy et al., 2020", "refKey": 5}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "PieMan", "normalizedForm": "PieMan", "offsetStart": 128, "offsetEnd": 134}, "context": "Similar to earlier works (Caucheteux, Gramfort, & King, 2021b), we analyze data from 82 subjects listening to the story titled 'PieMan' with 259 TRs (repetition time) 1 . ", "mentionContextAttributes": {"used": {"value": true, "score": 0.6847222447395325}, "created": {"value": false, "score": 0.0021927952766418457}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.6847222447395325}, "created": {"value": false, "score": 0.0021927952766418457}, "shared": {"value": false, "score": 1.1920928955078125e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "ELMo", "normalizedForm": "ELMo", "offsetStart": 142, "offsetEnd": 146}, "context": "In this paper, we uncover insights about the association between fMRI voxel activations and representations of diverse language models: LSTM, ELMo, and Longformer.", "mentionContextAttributes": {"used": {"value": false, "score": 0.0009352564811706543}, "created": {"value": true, "score": 0.9998438358306885}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999966621398926}, "created": {"value": true, "score": 0.9998438358306885}, "shared": {"value": false, "score": 2.6226043701171875e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "ELMo", "normalizedForm": "ELMo", "offsetStart": 142, "offsetEnd": 146}, "context": "In this paper, we train fMRI encoding models using Ridge regression on stimuli representations obtained using four models: Random LSTM, LSTM, ELMo, and Longformer.", "mentionContextAttributes": {"used": {"value": false, "score": 0.46428757905960083}, "created": {"value": true, "score": 0.9716159701347351}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999966621398926}, "created": {"value": true, "score": 0.9998438358306885}, "shared": {"value": false, "score": 2.6226043701171875e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Longformer", "normalizedForm": "Longformer", "offsetStart": 152, "offsetEnd": 162}, "context": "In this paper, we uncover insights about the association between fMRI voxel activations and representations of diverse language models: LSTM, ELMo, and Longformer.", "mentionContextAttributes": {"used": {"value": false, "score": 0.0009352564811706543}, "created": {"value": true, "score": 0.9998438358306885}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999966621398926}, "created": {"value": true, "score": 0.9998438358306885}, "shared": {"value": false, "score": 2.6226043701171875e-06}}, "references": [{"label": "(Beltagy et al., 2020)", "normalizedForm": "Beltagy et al., 2020", "refKey": 5}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Longformer", "normalizedForm": "Longformer", "offsetStart": 152, "offsetEnd": 162}, "context": "In this paper, we train fMRI encoding models using Ridge regression on stimuli representations obtained using four models: Random LSTM, LSTM, ELMo, and Longformer.", "mentionContextAttributes": {"used": {"value": false, "score": 0.46428757905960083}, "created": {"value": true, "score": 0.9716159701347351}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999966621398926}, "created": {"value": true, "score": 0.9998438358306885}, "shared": {"value": false, "score": 2.6226043701171875e-06}}, "references": [{"label": "(Beltagy et al., 2020)", "normalizedForm": "Beltagy et al., 2020", "refKey": 5}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "ELMo", "normalizedForm": "ELMo", "offsetStart": 153, "offsetEnd": 157}, "context": "Here, our main is objective to investigate the influence of context representations of different language models such as sequence-based models: LSTM and ELMo, and popular pretrained Transformer language model (Longformer).", "mentionContextAttributes": {"used": {"value": false, "score": 0.0006406903266906738}, "created": {"value": true, "score": 0.9963659048080444}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999966621398926}, "created": {"value": true, "score": 0.9998438358306885}, "shared": {"value": false, "score": 2.6226043701171875e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "ELMo", "normalizedForm": "ELMo", "offsetStart": 164, "offsetEnd": 168}, "context": "This paper studies the influence of context representations of different language models such as sequence-based models: Long Short-Term Memory networks (LSTMs) and ELMo, and a pretrained Transformer language model (Longformer).", "mentionContextAttributes": {"used": {"value": false, "score": 0.0004082322120666504}, "created": {"value": true, "score": 0.635618269443512}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999966621398926}, "created": {"value": true, "score": 0.9998438358306885}, "shared": {"value": false, "score": 2.6226043701171875e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Longformer", "normalizedForm": "Longformer", "offsetStart": 172, "offsetEnd": 182}, "context": "Hyper-parameter Setting: We used sklearn's ridgeregression with default parameters, 5-fold cross-validation, Stochastic-Average-Gradient Descent Optimizer, Huggingface for Longformer, MSE loss function, and L2-decay (\u03bb):1.0.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9999438524246216}, "created": {"value": false, "score": 2.0265579223632812e-06}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999966621398926}, "created": {"value": true, "score": 0.9998438358306885}, "shared": {"value": false, "score": 2.6226043701171875e-06}}, "references": [{"label": "(Beltagy et al., 2020)", "normalizedForm": "Beltagy et al., 2020", "refKey": 5}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "ELMo", "normalizedForm": "ELMo", "offsetStart": 174, "offsetEnd": 178}, "context": "Further, post hoc pairwise comparisons (Ruxton & Beauchamp, 2008) confirmed the visual observations that on both 2V2 accuracy and Pearson correlation measures, tasks such as ELMo and Longformer performed significantly better compared to other models, as shown in Table 2.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9999274015426636}, "created": {"value": false, "score": 3.6954879760742188e-06}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999966621398926}, "created": {"value": true, "score": 0.9998438358306885}, "shared": {"value": false, "score": 2.6226043701171875e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Longformer", "normalizedForm": "Longformer", "offsetStart": 175, "offsetEnd": 185}, "context": "Also, the narrative dataset consists of longer documents (more than 2000 words in one story); the traditional transformer models consider the context up to 512 words, whereas Longformer handles even longer documents. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.0006625056266784668}, "created": {"value": false, "score": 8.702278137207031e-06}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999966621398926}, "created": {"value": true, "score": 0.9998438358306885}, "shared": {"value": false, "score": 2.6226043701171875e-06}}, "references": [{"label": "(Beltagy et al., 2020)", "normalizedForm": "Beltagy et al., 2020", "refKey": 5}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Longformer", "normalizedForm": "Longformer", "offsetStart": 183, "offsetEnd": 193}, "context": "Further, post hoc pairwise comparisons (Ruxton & Beauchamp, 2008) confirmed the visual observations that on both 2V2 accuracy and Pearson correlation measures, tasks such as ELMo and Longformer performed significantly better compared to other models, as shown in Table 2.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9999274015426636}, "created": {"value": false, "score": 3.6954879760742188e-06}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999966621398926}, "created": {"value": true, "score": 0.9998438358306885}, "shared": {"value": false, "score": 2.6226043701171875e-06}}, "references": [{"label": "(Beltagy et al., 2020)", "normalizedForm": "Beltagy et al., 2020", "refKey": 5}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Longformer", "normalizedForm": "Longformer", "offsetStart": 188, "offsetEnd": 198}, "context": "Given the hierarchical processing of language information across the Transformer layers, we further examine how these Transformer layers encode fMRI brain activity using encoder layers of Longformer.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9248026013374329}, "created": {"value": false, "score": 0.0007878541946411133}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999966621398926}, "created": {"value": true, "score": 0.9998438358306885}, "shared": {"value": false, "score": 2.6226043701171875e-06}}, "references": [{"label": "(Beltagy et al., 2020)", "normalizedForm": "Beltagy et al., 2020", "refKey": 5}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "ELMo", "normalizedForm": "ELMo", "offsetStart": 206, "offsetEnd": 210}, "context": "To explore how and where contextual language features are represented in the brain when listening to stories, we extract internal hidden representations from two sequencebased models: Random LSTM and LSTM, ELMo (obtaining context-dependent word embeddings), and popular pretrained Transformer language model (Longformer) used for describing each stimulus sentence and use them in an encoding model to predict brain responses.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9202614426612854}, "created": {"value": false, "score": 0.008831501007080078}, "shared": {"value": false, "score": 1.430511474609375e-06}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999966621398926}, "created": {"value": true, "score": 0.9998438358306885}, "shared": {"value": false, "score": 2.6226043701171875e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Longformer", "normalizedForm": "Longformer", "offsetStart": 210, "offsetEnd": 220}, "context": "Here, our main is objective to investigate the influence of context representations of different language models such as sequence-based models: LSTM and ELMo, and popular pretrained Transformer language model (Longformer).", "mentionContextAttributes": {"used": {"value": false, "score": 0.0006406903266906738}, "created": {"value": true, "score": 0.9963659048080444}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999966621398926}, "created": {"value": true, "score": 0.9998438358306885}, "shared": {"value": false, "score": 2.6226043701171875e-06}}, "references": [{"label": "(Beltagy et al., 2020)", "normalizedForm": "Beltagy et al., 2020", "refKey": 5}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Longformer", "normalizedForm": "Longformer", "offsetStart": 215, "offsetEnd": 225}, "context": "This paper studies the influence of context representations of different language models such as sequence-based models: Long Short-Term Memory networks (LSTMs) and ELMo, and a pretrained Transformer language model (Longformer).", "mentionContextAttributes": {"used": {"value": false, "score": 0.0004082322120666504}, "created": {"value": true, "score": 0.635618269443512}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999966621398926}, "created": {"value": true, "score": 0.9998438358306885}, "shared": {"value": false, "score": 2.6226043701171875e-06}}, "references": [{"label": "(Beltagy et al., 2020)", "normalizedForm": "Beltagy et al., 2020", "refKey": 5}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "ELMo", "normalizedForm": "ELMo", "offsetStart": 232, "offsetEnd": 236}, "context": "In order to estimate the statistical significance of the performance differences, we performed one-way ANOVA on the mean correlation values for the subjects across the language models (GloVe, LSTM (cell state), LSTM (hidden state), ELMo, and Longformer) for the five brain ROIs.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9999954700469971}, "created": {"value": false, "score": 2.384185791015625e-06}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999966621398926}, "created": {"value": true, "score": 0.9998438358306885}, "shared": {"value": false, "score": 2.6226043701171875e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "ELMo", "normalizedForm": "ELMo", "offsetStart": 238, "offsetEnd": 242}, "context": "Namely, we look at (i) the characteristics of hidden state vectors (h t ) and artificial memory vectors (c t ) in both LSTMs and Random LSTMs, and (ii) local context vectors obtained from performance-optimized deep neural network models (ELMo and Longformer).", "mentionContextAttributes": {"used": {"value": true, "score": 0.8261820673942566}, "created": {"value": false, "score": 0.03525280952453613}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999966621398926}, "created": {"value": true, "score": 0.9998438358306885}, "shared": {"value": false, "score": 2.6226043701171875e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Longformer", "normalizedForm": "Longformer", "offsetStart": 242, "offsetEnd": 252}, "context": "In order to estimate the statistical significance of the performance differences, we performed one-way ANOVA on the mean correlation values for the subjects across the language models (GloVe, LSTM (cell state), LSTM (hidden state), ELMo, and Longformer) for the five brain ROIs.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9999954700469971}, "created": {"value": false, "score": 2.384185791015625e-06}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999966621398926}, "created": {"value": true, "score": 0.9998438358306885}, "shared": {"value": false, "score": 2.6226043701171875e-06}}, "references": [{"label": "(Beltagy et al., 2020)", "normalizedForm": "Beltagy et al., 2020", "refKey": 5}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Longformer", "normalizedForm": "Longformer", "offsetStart": 247, "offsetEnd": 257}, "context": "Namely, we look at (i) the characteristics of hidden state vectors (h t ) and artificial memory vectors (c t ) in both LSTMs and Random LSTMs, and (ii) local context vectors obtained from performance-optimized deep neural network models (ELMo and Longformer).", "mentionContextAttributes": {"used": {"value": true, "score": 0.8261820673942566}, "created": {"value": false, "score": 0.03525280952453613}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999966621398926}, "created": {"value": true, "score": 0.9998438358306885}, "shared": {"value": false, "score": 2.6226043701171875e-06}}, "references": [{"label": "(Beltagy et al., 2020)", "normalizedForm": "Beltagy et al., 2020", "refKey": 5}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "ELMo", "normalizedForm": "ELMo", "offsetStart": 250, "offsetEnd": 254}, "context": "We trained a ridge regression based encoding model to predict the fMRI brain activity associated with the semantic vector representation obtained from each language model: ran-dLSTM (hidden state, cell state), LSTM (hidden state, cell state), GloVe, ELMo, and Longformer.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9999895095825195}, "created": {"value": false, "score": 0.00011849403381347656}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999966621398926}, "created": {"value": true, "score": 0.9998438358306885}, "shared": {"value": false, "score": 2.6226043701171875e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Longformer", "normalizedForm": "Longformer", "offsetStart": 260, "offsetEnd": 270}, "context": "We trained a ridge regression based encoding model to predict the fMRI brain activity associated with the semantic vector representation obtained from each language model: ran-dLSTM (hidden state, cell state), LSTM (hidden state, cell state), GloVe, ELMo, and Longformer.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9999895095825195}, "created": {"value": false, "score": 0.00011849403381347656}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999966621398926}, "created": {"value": true, "score": 0.9998438358306885}, "shared": {"value": false, "score": 2.6226043701171875e-06}}, "references": [{"label": "(Beltagy et al., 2020)", "normalizedForm": "Beltagy et al., 2020", "refKey": 5}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Longformer", "normalizedForm": "Longformer", "offsetStart": 277, "offsetEnd": 287}, "context": "To determine the significant voxel predictions across the whole-brain, we ran the permutation tests where we shuffled the true responses 5000 times, computed the Pearson correlation scores, and finally obtained the FDR corrected p-values for the whole brain results using both Longformer and ELMo.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9999966621398926}, "created": {"value": false, "score": 2.86102294921875e-06}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999966621398926}, "created": {"value": true, "score": 0.9998438358306885}, "shared": {"value": false, "score": 2.6226043701171875e-06}}, "references": [{"label": "(Beltagy et al., 2020)", "normalizedForm": "Beltagy et al., 2020", "refKey": 5}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "ELMo", "normalizedForm": "ELMo", "offsetStart": 281, "offsetEnd": 285}, "context": "(3) In LSTM, the cell state representations (long term memory vector) yield better encoding performance than hidden state representations; thus, internal dynamics of LSTMs seem to have more cognitively plausible activations than classically studied LSTM (4) We different layers of ELMo and Longformer, where higher layers display better correlation for ELMo while intermediate layers show superior performance for Longformer.", "mentionContextAttributes": {"used": {"value": false, "score": 0.07223725318908691}, "created": {"value": false, "score": 1.1563301086425781e-05}, "shared": {"value": false, "score": 1.0728836059570312e-06}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999966621398926}, "created": {"value": true, "score": 0.9998438358306885}, "shared": {"value": false, "score": 2.6226043701171875e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Longformer", "normalizedForm": "Longformer", "offsetStart": 290, "offsetEnd": 300}, "context": "(3) In LSTM, the cell state representations (long term memory vector) yield better encoding performance than hidden state representations; thus, internal dynamics of LSTMs seem to have more cognitively plausible activations than classically studied LSTM (4) We different layers of ELMo and Longformer, where higher layers display better correlation for ELMo while intermediate layers show superior performance for Longformer.", "mentionContextAttributes": {"used": {"value": false, "score": 0.07223725318908691}, "created": {"value": false, "score": 1.1563301086425781e-05}, "shared": {"value": false, "score": 1.0728836059570312e-06}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999966621398926}, "created": {"value": true, "score": 0.9998438358306885}, "shared": {"value": false, "score": 2.6226043701171875e-06}}, "references": [{"label": "(Beltagy et al., 2020)", "normalizedForm": "Beltagy et al., 2020", "refKey": 5}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "ELMo", "normalizedForm": "ELMo", "offsetStart": 292, "offsetEnd": 296}, "context": "To determine the significant voxel predictions across the whole-brain, we ran the permutation tests where we shuffled the true responses 5000 times, computed the Pearson correlation scores, and finally obtained the FDR corrected p-values for the whole brain results using both Longformer and ELMo.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9999966621398926}, "created": {"value": false, "score": 2.86102294921875e-06}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999966621398926}, "created": {"value": true, "score": 0.9998438358306885}, "shared": {"value": false, "score": 2.6226043701171875e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "ELMo", "normalizedForm": "ELMo", "offsetStart": 300, "offsetEnd": 304}, "context": "Experiments across all language model representations provide the following cognitive insights: (i) the representations of LSTM cell states are better aligned with brain recordings than LSTM hidden states, the cell state activity can represent more long-term information, (ii) the representations of ELMo and Longformer display a good predictive performance across brain regions for listening stimuli; (iii) Posterior Medial Cortex (PMC), Temporo-Parieto-Occipital junction (TPOJ), and Dorsal Frontal Lobe (DFL) have higher correlation versus Early Auditory (EAC) and Auditory Association Cortex (AAC).", "mentionContextAttributes": {"used": {"value": true, "score": 0.992056667804718}, "created": {"value": false, "score": 5.7220458984375e-06}, "shared": {"value": false, "score": 1.1920928955078125e-06}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999966621398926}, "created": {"value": true, "score": 0.9998438358306885}, "shared": {"value": false, "score": 2.6226043701171875e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Longformer", "normalizedForm": "Longformer", "offsetStart": 309, "offsetEnd": 319}, "context": "Experiments across all language model representations provide the following cognitive insights: (i) the representations of LSTM cell states are better aligned with brain recordings than LSTM hidden states, the cell state activity can represent more long-term information, (ii) the representations of ELMo and Longformer display a good predictive performance across brain regions for listening stimuli; (iii) Posterior Medial Cortex (PMC), Temporo-Parieto-Occipital junction (TPOJ), and Dorsal Frontal Lobe (DFL) have higher correlation versus Early Auditory (EAC) and Auditory Association Cortex (AAC).", "mentionContextAttributes": {"used": {"value": true, "score": 0.992056667804718}, "created": {"value": false, "score": 5.7220458984375e-06}, "shared": {"value": false, "score": 1.1920928955078125e-06}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999966621398926}, "created": {"value": true, "score": 0.9998438358306885}, "shared": {"value": false, "score": 2.6226043701171875e-06}}, "references": [{"label": "(Beltagy et al., 2020)", "normalizedForm": "Beltagy et al., 2020", "refKey": 5}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Longformer", "normalizedForm": "Longformer", "offsetStart": 309, "offsetEnd": 319}, "context": "To explore how and where contextual language features are represented in the brain when listening to stories, we extract internal hidden representations from two sequencebased models: Random LSTM and LSTM, ELMo (obtaining context-dependent word embeddings), and popular pretrained Transformer language model (Longformer) used for describing each stimulus sentence and use them in an encoding model to predict brain responses.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9202614426612854}, "created": {"value": false, "score": 0.008831501007080078}, "shared": {"value": false, "score": 1.430511474609375e-06}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999966621398926}, "created": {"value": true, "score": 0.9998438358306885}, "shared": {"value": false, "score": 2.6226043701171875e-06}}, "references": [{"label": "(Beltagy et al., 2020)", "normalizedForm": "Beltagy et al., 2020", "refKey": 5}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "ELMo", "normalizedForm": "ELMo", "offsetStart": 353, "offsetEnd": 357}, "context": "(3) In LSTM, the cell state representations (long term memory vector) yield better encoding performance than hidden state representations; thus, internal dynamics of LSTMs seem to have more cognitively plausible activations than classically studied LSTM (4) We different layers of ELMo and Longformer, where higher layers display better correlation for ELMo while intermediate layers show superior performance for Longformer.", "mentionContextAttributes": {"used": {"value": false, "score": 0.07223725318908691}, "created": {"value": false, "score": 1.1563301086425781e-05}, "shared": {"value": false, "score": 1.0728836059570312e-06}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999966621398926}, "created": {"value": true, "score": 0.9998438358306885}, "shared": {"value": false, "score": 2.6226043701171875e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "ELMo", "normalizedForm": "ELMo", "offsetStart": 379, "offsetEnd": 383}, "context": "However, all the sub ROIs in the TPOJ yield higher correlation, as shown in Fig. 4. The control and attention ROIs in the posterior cingulate cortex (for ex., POS1 in PMC), together with the superior frontal language region (sfl in DFL) and TPOJ, are part of the language network associated with narrative comprehension (Nastase et al., 2020): it is encouraging to see that both ELMo and Longformer also relate to semantic analysis of the ongoing narrative because they obtain best performance, showing that capturing longerterm context is important.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9921993613243103}, "created": {"value": false, "score": 7.510185241699219e-06}, "shared": {"value": false, "score": 2.6226043701171875e-06}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999966621398926}, "created": {"value": true, "score": 0.9998438358306885}, "shared": {"value": false, "score": 2.6226043701171875e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Longformer", "normalizedForm": "Longformer", "offsetStart": 388, "offsetEnd": 398}, "context": "However, all the sub ROIs in the TPOJ yield higher correlation, as shown in Fig. 4. The control and attention ROIs in the posterior cingulate cortex (for ex., POS1 in PMC), together with the superior frontal language region (sfl in DFL) and TPOJ, are part of the language network associated with narrative comprehension (Nastase et al., 2020): it is encouraging to see that both ELMo and Longformer also relate to semantic analysis of the ongoing narrative because they obtain best performance, showing that capturing longerterm context is important.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9921993613243103}, "created": {"value": false, "score": 7.510185241699219e-06}, "shared": {"value": false, "score": 2.6226043701171875e-06}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999966621398926}, "created": {"value": true, "score": 0.9998438358306885}, "shared": {"value": false, "score": 2.6226043701171875e-06}}, "references": [{"label": "(Beltagy et al., 2020)", "normalizedForm": "Beltagy et al., 2020", "refKey": 5}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Longformer", "normalizedForm": "Longformer", "offsetStart": 414, "offsetEnd": 424}, "context": "(3) In LSTM, the cell state representations (long term memory vector) yield better encoding performance than hidden state representations; thus, internal dynamics of LSTMs seem to have more cognitively plausible activations than classically studied LSTM (4) We different layers of ELMo and Longformer, where higher layers display better correlation for ELMo while intermediate layers show superior performance for Longformer.", "mentionContextAttributes": {"used": {"value": false, "score": 0.07223725318908691}, "created": {"value": false, "score": 1.1563301086425781e-05}, "shared": {"value": false, "score": 1.0728836059570312e-06}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999966621398926}, "created": {"value": true, "score": 0.9998438358306885}, "shared": {"value": false, "score": 2.6226043701171875e-06}}, "references": [{"label": "(Beltagy et al., 2020)", "normalizedForm": "Beltagy et al., 2020", "refKey": 5}]}], "references": [{"refKey": 5, "tei": "<biblStruct xml:id=\"b5\">\n\t<monogr>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">I</forename><surname>Beltagy</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">M</forename><forename type=\"middle\">E</forename><surname>Peters</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">A</forename><surname>Cohan</surname></persName>\n\t\t</author>\n\t\t<idno>arXiv:2004.05150</idno>\n\t\t<title level=\"m\">Longformer: The long-document transformer</title>\n\t\t<imprint>\n\t\t\t<date>2020</date>\n\t\t</imprint>\n\t</monogr>\n</biblStruct>\n"}], "runtime": 12738, "id": "c249c7f87c9128ba1189947f124abc93864ed6bf", "metadata": {"id": "c249c7f87c9128ba1189947f124abc93864ed6bf"}, "original_file_path": "../../datalake/Samuel/SV22/SV22_xml/hal-03697443.grobid.tei.xml", "file_name": "hal-03697443.grobid.tei.xml"}