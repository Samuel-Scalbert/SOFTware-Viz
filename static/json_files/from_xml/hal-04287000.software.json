{"application": "software-mentions", "version": "0.8.0", "date": "2024-03-21T09:55+0000", "md5": "F1053D86816498A3EA70847E7DBD910F", "mentions": [{"type": "software", "software-type": "software", "software-name": {"rawForm": "Natural Language Toolkit (NL TK)", "normalizedForm": "Natural Language Toolkit (NL TK)", "offsetStart": 21, "offsetEnd": 53}, "context": "The authors used the Natural Language Toolkit (NL TK) for Python to perform tokenization and lemmatization, before extracting textual features and NumPy for generating the final numeric feature vectors. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9999353885650635}, "created": {"value": false, "score": 1.8715858459472656e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999353885650635}, "created": {"value": false, "score": 1.8715858459472656e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "quanteda", "normalizedForm": "quanteda", "offsetStart": 120, "offsetEnd": 128}, "context": "Bae, Korea, 2021 [33] The linguistic features were extracted using the LIWC package and the liwcalike function from the quanteda package. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9999798536300659}, "created": {"value": false, "score": 3.5762786865234375e-06}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999798536300659}, "created": {"value": false, "score": 3.5762786865234375e-06}, "shared": {"value": false, "score": 1.1920928955078125e-07}}}, {"type": "software", "software-type": "software", "wikidataId": "Q197520", "wikipediaExternalRef": 381782, "lang": "en", "confidence": 0.7796, "software-name": {"rawForm": "NumPy", "normalizedForm": "NumPy", "wikidataId": "Q197520", "wikipediaExternalRef": 381782, "lang": "en", "confidence": 0.7796, "offsetStart": 147, "offsetEnd": 152}, "language": {"rawForm": "Python", "normalizedForm": "Python", "wikidataId": "Q28865", "offsetStart": 152, "offsetEnd": 158}, "context": "The authors used the Natural Language Toolkit (NL TK) for Python to perform tokenization and lemmatization, before extracting textual features and NumPy for generating the final numeric feature vectors. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9999353885650635}, "created": {"value": false, "score": 1.8715858459472656e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999353885650635}, "created": {"value": false, "score": 1.8715858459472656e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}}], "references": [], "runtime": 2719, "id": "44a8f37a588227188be6a2709d873011ebd58a97", "metadata": {"id": "44a8f37a588227188be6a2709d873011ebd58a97"}, "original_file_path": "../../datalake/Samuel/SV22/SV22_xml/hal-04287000.grobid.tei.xml", "file_name": "hal-04287000.grobid.tei.xml"}