{"application": "software-mentions", "version": "0.8.0", "date": "2024-03-21T09:59+0000", "md5": "3D2DB407C6F7EFEAF9AB9C80580FEEF7", "mentions": [{"type": "software", "software-type": "software", "software-name": {"rawForm": "BASWebServices", "normalizedForm": "BASWebServices", "offsetStart": 3, "offsetEnd": 17}, "context": "de/BASWebServices/interface et al., 2017) was used with default parameters except for the ASR which was done using Google Speech Cloud ASR2 .", "mentionContextAttributes": {"used": {"value": true, "score": 0.9995790123939514}, "created": {"value": false, "score": 2.7418136596679688e-06}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9995790123939514}, "created": {"value": false, "score": 7.867813110351562e-06}, "shared": {"value": false, "score": 2.384185791015625e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "FeatureExtraction", "normalizedForm": "FeatureExtraction", "offsetStart": 33, "offsetEnd": 50}, "context": "Video analysis pipelines such as FeatureExtraction from OpenFace (Baltrusaitis et al., 2018) is also used to make the most of our multimodal design and generate features from head movements and gaze (see Figure 3). ", "mentionContextAttributes": {"used": {"value": false, "score": 0.0014660358428955078}, "created": {"value": false, "score": 0.0012896060943603516}, "shared": {"value": false, "score": 4.76837158203125e-07}}, "documentContextAttributes": {"used": {"value": false, "score": 0.0014660358428955078}, "created": {"value": false, "score": 0.0012896060943603516}, "shared": {"value": false, "score": 4.76837158203125e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "OpenFace", "normalizedForm": "OpenFace", "offsetStart": 51, "offsetEnd": 87}, "mentionContextAttributes": {"used": {"value": false, "score": 0.32873594760894775}, "created": {"value": false, "score": 0.003972411155700684}, "shared": {"value": false, "score": 2.1457672119140625e-06}}, "documentContextAttributes": {"used": {"value": false, "score": 0.32873594760894775}, "created": {"value": false, "score": 0.003972411155700684}, "shared": {"value": false, "score": 2.1457672119140625e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "BASWebServices", "normalizedForm": "BASWebServices", "offsetStart": 55, "offsetEnd": 91}, "context": "Figure 2: Automatic transcription and annotations from BASWebServices (Kisler et al., 2017) Transcription are to be checked and corrected manually, then realigned using SPPAS (Bigi, 2012) to obtain the corrected word and phoneme levels of alignment. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.947677493095398}, "created": {"value": false, "score": 7.867813110351562e-06}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9995790123939514}, "created": {"value": false, "score": 7.867813110351562e-06}, "shared": {"value": false, "score": 2.384185791015625e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "OpenFace", "normalizedForm": "OpenFace", "offsetStart": 56, "offsetEnd": 92}, "context": "Video analysis pipelines such as FeatureExtraction from OpenFace (Baltrusaitis et al., 2018) is also used to make the most of our multimodal design and generate features from head movements and gaze (see Figure 3). ", "mentionContextAttributes": {"used": {"value": false, "score": 0.0014660358428955078}, "created": {"value": false, "score": 0.0012896060943603516}, "shared": {"value": false, "score": 4.76837158203125e-07}}, "documentContextAttributes": {"used": {"value": false, "score": 0.32873594760894775}, "created": {"value": false, "score": 0.003972411155700684}, "shared": {"value": false, "score": 2.1457672119140625e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Lab Streaming Layer (LSL)", "normalizedForm": "Lab Streaming Layer (LSL)", "offsetStart": 76, "offsetEnd": 114}, "context": "We solved this problem by generating triggers associated with EEG using the Lab Streaming Layer (LSL) (Kothe, 2014). ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9997743964195251}, "created": {"value": false, "score": 0.0008000731468200684}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9997743964195251}, "created": {"value": false, "score": 0.0008000731468200684}, "shared": {"value": false, "score": 2.384185791015625e-07}}}, {"type": "software", "software-type": "implicit", "software-name": {"rawForm": "script", "normalizedForm": "script", "offsetStart": 80, "offsetEnd": 86}, "context": "Stimuli were visually displayed on the laptops' screen using an in-house Python script. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9999878406524658}, "created": {"value": false, "score": 1.0728836059570312e-05}, "shared": {"value": false, "score": 4.76837158203125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999878406524658}, "created": {"value": false, "score": 0.001044154167175293}, "shared": {"value": false, "score": 4.76837158203125e-07}}}, {"type": "software", "software-type": "implicit", "software-name": {"rawForm": "script", "normalizedForm": "script", "offsetStart": 82, "offsetEnd": 88}, "context": "We automatically generated the triggers at regular intervals by means of a Python script, such triggers being integrated with the EEG signal with LSL.", "mentionContextAttributes": {"used": {"value": true, "score": 0.999980092048645}, "created": {"value": false, "score": 0.001044154167175293}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999878406524658}, "created": {"value": false, "score": 0.001044154167175293}, "shared": {"value": false, "score": 4.76837158203125e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "HMAD                          R", "normalizedForm": "HMAD R", "offsetStart": 87, "offsetEnd": 118}, "context": "The generated coordinates for facial landmarks and actions units are then fed into the HMAD (Rauzy and Goujon, 2018) R library for extraction of nods and smile annotations.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9995827078819275}, "created": {"value": false, "score": 9.894371032714844e-06}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9995827078819275}, "created": {"value": false, "score": 9.894371032714844e-06}, "shared": {"value": false, "score": 1.1920928955078125e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Google Speech Cloud", "normalizedForm": "Google Speech Cloud", "offsetStart": 115, "offsetEnd": 134}, "context": "de/BASWebServices/interface et al., 2017) was used with default parameters except for the ASR which was done using Google Speech Cloud ASR2 . ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9995790123939514}, "created": {"value": false, "score": 2.7418136596679688e-06}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9995790123939514}, "created": {"value": false, "score": 2.7418136596679688e-06}, "shared": {"value": false, "score": 2.384185791015625e-07}}}], "references": [], "runtime": 6029, "id": "c6a5a03bee0a7d52632f20fc1b641ae536e74717", "metadata": {"id": "c6a5a03bee0a7d52632f20fc1b641ae536e74717"}, "original_file_path": "../../datalake/Samuel/SV22/SV22_xml/hal-03657477.grobid.tei.xml", "file_name": "hal-03657477.grobid.tei.xml"}