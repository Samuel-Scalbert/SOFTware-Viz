{"application": "software-mentions", "version": "0.8.0", "date": "2024-03-21T10:08+0000", "md5": "482508D05AE6C2723D33C49233405B52", "mentions": [{"type": "software", "software-type": "environment", "wikidataId": "Q169478", "wikipediaExternalRef": 20412, "lang": "en", "confidence": 0.9128, "software-name": {"rawForm": "MATLAB", "normalizedForm": "MATLAB", "wikidataId": "Q169478", "wikipediaExternalRef": 20412, "lang": "en", "confidence": 0.9128, "offsetStart": 14, "offsetEnd": 20}, "context": "All the R and MATLAB codes are available under request from the corresponding author.", "mentionContextAttributes": {"used": {"value": false, "score": 0.0017156004905700684}, "created": {"value": false, "score": 0.0002906918525695801}, "shared": {"value": false, "score": 0.004314899444580078}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9068163633346558}, "created": {"value": false, "score": 0.0002906918525695801}, "shared": {"value": false, "score": 0.004314899444580078}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "spaMM", "normalizedForm": "spaMM", "offsetStart": 21, "offsetEnd": 26}, "context": ", namely RF, RF with spaMM, GBM, XGBOOST, ANNs, KNN, SVM, and GLM. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9947415590286255}, "created": {"value": false, "score": 4.291534423828125e-06}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9992152452468872}, "created": {"value": false, "score": 4.291534423828125e-06}, "shared": {"value": false, "score": 8.344650268554688e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "codes", "normalizedForm": "codes", "offsetStart": 21, "offsetEnd": 26}, "context": "All the R and MATLAB codes are available under request from the corresponding author.", "mentionContextAttributes": {"used": {"value": false, "score": 0.0017156004905700684}, "created": {"value": false, "score": 0.0002906918525695801}, "shared": {"value": false, "score": 0.004314899444580078}}, "documentContextAttributes": {"used": {"value": false, "score": 0.0017156004905700684}, "created": {"value": false, "score": 0.0002906918525695801}, "shared": {"value": false, "score": 0.004314899444580078}}}, {"type": "software", "software-type": "environment", "wikidataId": "Q169478", "wikipediaExternalRef": 20412, "lang": "en", "confidence": 0.9128, "software-name": {"rawForm": "MATLAB", "normalizedForm": "MATLAB", "wikidataId": "Q169478", "wikipediaExternalRef": 20412, "lang": "en", "confidence": 0.9128, "offsetStart": 28, "offsetEnd": 34}, "context": "All maps are generated with MATLAB R2020a (Version 9.8.0.1451342). ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9068163633346558}, "created": {"value": false, "score": 1.5139579772949219e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9068163633346558}, "created": {"value": false, "score": 0.0002906918525695801}, "shared": {"value": false, "score": 0.004314899444580078}}}, {"type": "software", "software-type": "software", "wikidataId": "Q23793561", "wikipediaExternalRef": 49541331, "lang": "en", "confidence": 0.4955, "software-name": {"rawForm": "XGBOOST", "normalizedForm": "XGBOOST", "wikidataId": "Q23793561", "wikipediaExternalRef": 49541331, "lang": "en", "confidence": 0.4955, "offsetStart": 29, "offsetEnd": 36}, "context": "Random forest (RF), GBM, and XGBOOST show better classification performance, with AUC values equal to 0.790, 0.786, 0.783, respectively, while the more traditional algorithms GLM and NB have lower performance, with AUC values of 0.644 and 0.647, respectively (Figure 2, Supplementary Table S3).", "mentionContextAttributes": {"used": {"value": false, "score": 0.006616652011871338}, "created": {"value": false, "score": 2.9802322387695312e-06}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9992152452468872}, "created": {"value": false, "score": 1.5616416931152344e-05}, "shared": {"value": false, "score": 8.344650268554688e-07}}}, {"type": "software", "software-type": "software", "wikidataId": "Q23793561", "wikipediaExternalRef": 49541331, "lang": "en", "confidence": 0.4955, "software-name": {"rawForm": "XGBOOST", "normalizedForm": "XGBOOST", "wikidataId": "Q23793561", "wikipediaExternalRef": 49541331, "lang": "en", "confidence": 0.4955, "offsetStart": 33, "offsetEnd": 40}, "context": ", namely RF, RF with spaMM, GBM, XGBOOST, ANNs, KNN, SVM, and GLM. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9947415590286255}, "created": {"value": false, "score": 4.291534423828125e-06}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9992152452468872}, "created": {"value": false, "score": 1.5616416931152344e-05}, "shared": {"value": false, "score": 8.344650268554688e-07}}}, {"type": "software", "software-type": "software", "wikidataId": "Q23793561", "wikipediaExternalRef": 49541331, "lang": "en", "confidence": 0.4955, "software-name": {"rawForm": "XGBOOST", "normalizedForm": "XGBOOST", "wikidataId": "Q23793561", "wikipediaExternalRef": 49541331, "lang": "en", "confidence": 0.4955, "offsetStart": 78, "offsetEnd": 85}, "context": "RF, GBM, SVM are able to handle such heterogeneous features, while ANNs, KNN, XGBOOST work usually better with numerical and homogeneous inputs (Ali et al., 2019;Zhou, 2021). ", "mentionContextAttributes": {"used": {"value": false, "score": 0.00012767314910888672}, "created": {"value": false, "score": 1.5616416931152344e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9992152452468872}, "created": {"value": false, "score": 1.5616416931152344e-05}, "shared": {"value": false, "score": 8.344650268554688e-07}}}, {"type": "software", "software-type": "software", "wikidataId": "Q23793561", "wikipediaExternalRef": 49541331, "lang": "en", "confidence": 0.4955, "software-name": {"rawForm": "XGBOOST", "normalizedForm": "XGBOOST", "wikidataId": "Q23793561", "wikipediaExternalRef": 49541331, "lang": "en", "confidence": 0.4955, "offsetStart": 135, "offsetEnd": 142}, "context": "Our comparative analysis shows that, RF has the best performance for both classification and quantitative prediction, followed by GBM, XGBOOST, ANNs, SVM, and KNN, while GLM and NB have the worst performance compared to other algorithms. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.3122633099555969}, "created": {"value": false, "score": 8.821487426757812e-06}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9992152452468872}, "created": {"value": false, "score": 1.5616416931152344e-05}, "shared": {"value": false, "score": 8.344650268554688e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "spaMM", "normalizedForm": "spaMM", "offsetStart": 250, "offsetEnd": 255}, "context": "A first series of models are trained to classify yield gain ( YieldCA or NT YieldCT > 1) versus yield loss ( YieldCA or NT YieldCT < 1), namely random forest (Ho, 1995) (RF), random forest with spatial correlations (Rousset and Ferdy, 2014) (RF with spaMM), gradient boosting (Friedman, 1999;Friedman, 2001) (GBM), extreme gradient boosting (Friedman, 1999;Friedman, 2001;Schmidhuber et al., 2018) (XGBOOST), artificial neural networks with different number of hidden layers (Haykin, 1998;Bergmeir and Ben\u00edtez, 2012) (ANNs), k-nearest neighbours (Hastie et al., 2009) (KNN), support vector machines (Cortes and Vapnik, 1995) (SVM), na\u00efve bayes (Hand and Yu, 2001;Hastie et al., 2009) (NB), and generalized linear model (Nelder and Wedderburn, 1972) (GLM).", "mentionContextAttributes": {"used": {"value": true, "score": 0.9992152452468872}, "created": {"value": false, "score": 3.337860107421875e-06}, "shared": {"value": false, "score": 8.344650268554688e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9992152452468872}, "created": {"value": false, "score": 4.291534423828125e-06}, "shared": {"value": false, "score": 8.344650268554688e-07}}}, {"type": "software", "software-type": "software", "wikidataId": "Q23793561", "wikipediaExternalRef": 49541331, "lang": "en", "confidence": 0.4955, "software-name": {"rawForm": "XGBOOST", "normalizedForm": "XGBOOST", "wikidataId": "Q23793561", "wikipediaExternalRef": 49541331, "lang": "en", "confidence": 0.4955, "offsetStart": 399, "offsetEnd": 406}, "context": "A first series of models are trained to classify yield gain ( YieldCA or NT YieldCT > 1) versus yield loss ( YieldCA or NT YieldCT < 1), namely random forest (Ho, 1995) (RF), random forest with spatial correlations (Rousset and Ferdy, 2014) (RF with spaMM), gradient boosting (Friedman, 1999;Friedman, 2001) (GBM), extreme gradient boosting (Friedman, 1999;Friedman, 2001;Schmidhuber et al., 2018) (XGBOOST), artificial neural networks with different number of hidden layers (Haykin, 1998;Bergmeir and Ben\u00edtez, 2012) (ANNs), k-nearest neighbours (Hastie et al., 2009) (KNN), support vector machines (Cortes and Vapnik, 1995) (SVM), na\u00efve bayes (Hand and Yu, 2001;Hastie et al., 2009) (NB), and generalized linear model (Nelder and Wedderburn, 1972) (GLM).", "mentionContextAttributes": {"used": {"value": true, "score": 0.9992152452468872}, "created": {"value": false, "score": 3.337860107421875e-06}, "shared": {"value": false, "score": 8.344650268554688e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9992152452468872}, "created": {"value": false, "score": 1.5616416931152344e-05}, "shared": {"value": false, "score": 8.344650268554688e-07}}}], "references": [], "runtime": 11057, "id": "9804cca1062acb10e5fdff01eb01ef26df0aab9f", "metadata": {"id": "9804cca1062acb10e5fdff01eb01ef26df0aab9f"}, "original_file_path": "../../datalake/Samuel/SV22/SV22_xml/hal-03561174.grobid.tei.xml", "file_name": "hal-03561174.grobid.tei.xml"}