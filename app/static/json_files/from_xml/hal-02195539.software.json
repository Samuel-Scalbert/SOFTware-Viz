{"application": "software-mentions", "version": "0.8.0", "date": "2024-04-12T16:21+0000", "md5": "2AC0A9BCD99B39759CAC810697D227C3", "mentions": [{"type": "software", "software-type": "software", "software-name": {"rawForm": "BelMan", "normalizedForm": "BelMan", "offsetStart": 0, "offsetEnd": 6}, "context": "BelMan leverages its geometric insight to manage the pure exploration bandits only by turning the exposure to infinity. ", "mentionContextAttributes": {"used": {"value": false, "score": 5.322694778442383e-05}, "created": {"value": false, "score": 0.0003159046173095703}, "shared": {"value": false, "score": 1.7881393432617188e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999626278877258}, "created": {"value": true, "score": 0.9990540146827698}, "shared": {"value": false, "score": 4.231929779052734e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "BelMan", "normalizedForm": "BelMan", "offsetStart": 0, "offsetEnd": 6}, "context": "BelMan projects the pseudobelief-focal-reward onto beliefrewards to select an arm.", "mentionContextAttributes": {"used": {"value": false, "score": 0.0003472566604614258}, "created": {"value": true, "score": 0.6142310500144958}, "shared": {"value": false, "score": 2.980232238769531e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999626278877258}, "created": {"value": true, "score": 0.9990540146827698}, "shared": {"value": false, "score": 4.231929779052734e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "BelMan", "normalizedForm": "BelMan", "offsetStart": 0, "offsetEnd": 6}, "context": "BelMan alternates I-and rIprojections between belief-reward distributions of the arms and the pseudobelieffocal-reward distribution for arm selection and information accumulation.", "mentionContextAttributes": {"used": {"value": false, "score": 0.00019502639770507812}, "created": {"value": false, "score": 2.682209014892578e-06}, "shared": {"value": false, "score": 1.7881393432617188e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999626278877258}, "created": {"value": true, "score": 0.9990540146827698}, "shared": {"value": false, "score": 4.231929779052734e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "BelMan", "normalizedForm": "BelMan", "offsetStart": 0, "offsetEnd": 6}, "context": "BelMan can be tuned, using the exposure, to support a continuum from pure exploration to exploration-exploitation, as well as two-phase reinforcement learning.", "mentionContextAttributes": {"used": {"value": false, "score": 2.676248550415039e-05}, "created": {"value": false, "score": 1.8894672393798828e-05}, "shared": {"value": false, "score": 1.7881393432617188e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999626278877258}, "created": {"value": true, "score": 0.9990540146827698}, "shared": {"value": false, "score": 4.231929779052734e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "BelMan", "normalizedForm": "BelMan", "offsetStart": 0, "offsetEnd": 6}, "context": "BelMan: An Alternating Projection Scheme.", "mentionContextAttributes": {"used": {"value": false, "score": 0.0001785755157470703}, "created": {"value": false, "score": 0.0009095668792724609}, "shared": {"value": false, "score": 2.980232238769531e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999626278877258}, "created": {"value": true, "score": 0.9990540146827698}, "shared": {"value": false, "score": 4.231929779052734e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "BelMan", "normalizedForm": "BelMan", "offsetStart": 0, "offsetEnd": 6}, "context": "BelMan (Algorithm 1) performs the first and the last operations by alternately minimising the KL-divergence D KL (. .)", "mentionContextAttributes": {"used": {"value": true, "score": 0.9426904916763306}, "created": {"value": false, "score": 7.927417755126953e-06}, "shared": {"value": false, "score": 1.7881393432617188e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999626278877258}, "created": {"value": true, "score": 0.9990540146827698}, "shared": {"value": false, "score": 4.231929779052734e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "BelMan", "normalizedForm": "BelMan", "offsetStart": 0, "offsetEnd": 6}, "context": "BelMan chooses to play the arm whose belief-reward incurs minimum KL-divergence with respect to the pseudobelief-focal-reward distribution.", "mentionContextAttributes": {"used": {"value": false, "score": 0.0007483959197998047}, "created": {"value": false, "score": 3.165006637573242e-05}, "shared": {"value": false, "score": 2.980232238769531e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999626278877258}, "created": {"value": true, "score": 0.9990540146827698}, "shared": {"value": false, "score": 4.231929779052734e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "BelMan", "normalizedForm": "BelMan", "offsetStart": 0, "offsetEnd": 6}, "context": "BelMan decides which arm to pull by an I-projection of the pseudobelief-focalreward distribution onto the beliefs-rewards of each of the arms (Lines 3-4).", "mentionContextAttributes": {"used": {"value": false, "score": 0.009775340557098389}, "created": {"value": false, "score": 5.543231964111328e-06}, "shared": {"value": false, "score": 2.980232238769531e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999626278877258}, "created": {"value": true, "score": 0.9990540146827698}, "shared": {"value": false, "score": 4.231929779052734e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "BelMan", "normalizedForm": "BelMan", "offsetStart": 0, "offsetEnd": 6}, "context": "BelMan is applicable to any belief-reward distribution for which KL-divergence is computable and finite.", "mentionContextAttributes": {"used": {"value": false, "score": 0.00011235475540161133}, "created": {"value": false, "score": 6.258487701416016e-06}, "shared": {"value": false, "score": 4.172325134277344e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999626278877258}, "created": {"value": true, "score": 0.9990540146827698}, "shared": {"value": false, "score": 4.231929779052734e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "BelMan", "normalizedForm": "BelMan", "offsetStart": 0, "offsetEnd": 6}, "context": "BelMan implements a generic Bayesian information-geometric approach for stochastic multi-armed bandit problems.", "mentionContextAttributes": {"used": {"value": false, "score": 1.901388168334961e-05}, "created": {"value": true, "score": 0.9470946192741394}, "shared": {"value": false, "score": 2.980232238769531e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999626278877258}, "created": {"value": true, "score": 0.9990540146827698}, "shared": {"value": false, "score": 4.231929779052734e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "BelMan", "normalizedForm": "BelMan", "offsetStart": 0, "offsetEnd": 6}, "context": "BelMan, when instantiated to rewards modelled by any distribution of the exponential family, conveniently leads to analytical forms that allow derivation of a well-defined and unique projection as well as to devise an effective and fast computation.", "mentionContextAttributes": {"used": {"value": false, "score": 4.607439041137695e-05}, "created": {"value": false, "score": 5.561113357543945e-05}, "shared": {"value": false, "score": 6.556510925292969e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999626278877258}, "created": {"value": true, "score": 0.9990540146827698}, "shared": {"value": false, "score": 4.231929779052734e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "BelMan", "normalizedForm": "BelMan", "offsetStart": 6, "offsetEnd": 12}, "context": "Thus, BelMan gives us a single algorithmic framework for three setups of bandit problems-pure exploration, exploration-exploitation, and two-phase learning.", "mentionContextAttributes": {"used": {"value": false, "score": 2.0325183868408203e-05}, "created": {"value": false, "score": 0.4496998190879822}, "shared": {"value": false, "score": 1.7881393432617188e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999626278877258}, "created": {"value": true, "score": 0.9990540146827698}, "shared": {"value": false, "score": 4.231929779052734e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "BelMan", "normalizedForm": "BelMan", "offsetStart": 6, "offsetEnd": 12}, "context": "Here, BelMan is inducing the exploitative bias.", "mentionContextAttributes": {"used": {"value": false, "score": 0.0007579326629638672}, "created": {"value": false, "score": 0.16049224138259888}, "shared": {"value": false, "score": 6.556510925292969e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999626278877258}, "created": {"value": true, "score": 0.9990540146827698}, "shared": {"value": false, "score": 4.231929779052734e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "BelMan", "normalizedForm": "BelMan", "offsetStart": 7, "offsetEnd": 13}, "context": "Hence, BelMan is asymptotically consistent.", "mentionContextAttributes": {"used": {"value": false, "score": 8.732080459594727e-05}, "created": {"value": false, "score": 3.3915042877197266e-05}, "shared": {"value": false, "score": 4.172325134277344e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999626278877258}, "created": {"value": true, "score": 0.9990540146827698}, "shared": {"value": false, "score": 4.231929779052734e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "BelMan", "normalizedForm": "BelMan", "offsetStart": 10, "offsetEnd": 16}, "context": "In short, BelMan addresses the issue of the adaptive balance of exploration-exploitation from the perspective of information representation, accumulation, and balanced induction of exploitative bias. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.00010412931442260742}, "created": {"value": false, "score": 0.12292224168777466}, "shared": {"value": false, "score": 6.556510925292969e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999626278877258}, "created": {"value": true, "score": 0.9990540146827698}, "shared": {"value": false, "score": 4.231929779052734e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "BelMan", "normalizedForm": "BelMan", "offsetStart": 10, "offsetEnd": 16}, "context": "2C , then BelMan would satisfy asymptotic consistency", "mentionContextAttributes": {"used": {"value": false, "score": 0.09948551654815674}, "created": {"value": false, "score": 2.4437904357910156e-06}, "shared": {"value": false, "score": 7.748603820800781e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999626278877258}, "created": {"value": true, "score": 0.9990540146827698}, "shared": {"value": false, "score": 4.231929779052734e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "BelMan", "normalizedForm": "BelMan", "offsetStart": 11, "offsetEnd": 17}, "context": "We compare BelMan with state-of-the-art algorithms: UCB [3], KL-UCB, KL-UCB-Exp [14], Bayes-UCB [21], Thompson sampling [34], and Gittins index [17], in these different settings.", "mentionContextAttributes": {"used": {"value": true, "score": 0.5070839524269104}, "created": {"value": false, "score": 6.496906280517578e-06}, "shared": {"value": false, "score": 1.7881393432617188e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999626278877258}, "created": {"value": true, "score": 0.9990540146827698}, "shared": {"value": false, "score": 4.231929779052734e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "pyma-Bandits", "normalizedForm": "pyma-Bandits", "offsetStart": 11, "offsetEnd": 23}, "context": "We use the pyma-Bandits library [9] for implementation of all the algorithms except ours, and run it on MATLAB 2014a. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9971954226493835}, "created": {"value": false, "score": 0.0004163384437561035}, "shared": {"value": false, "score": 1.7881393432617188e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9971954226493835}, "created": {"value": false, "score": 0.0004163384437561035}, "shared": {"value": false, "score": 1.7881393432617188e-07}}, "references": [{"label": "[9]", "normalizedForm": "[9]", "refKey": 0, "offsetStart": 23890, "offsetEnd": 23893}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "BelMan", "normalizedForm": "BelMan", "offsetStart": 14, "offsetEnd": 20}, "context": "This supports BelMan's claim as a generalised, unified framework for stochastic bandit problems.", "mentionContextAttributes": {"used": {"value": false, "score": 7.37309455871582e-05}, "created": {"value": false, "score": 0.06675887107849121}, "shared": {"value": false, "score": 2.980232238769531e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999626278877258}, "created": {"value": true, "score": 0.9990540146827698}, "shared": {"value": false, "score": 4.231929779052734e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "BelMan", "normalizedForm": "BelMan", "offsetStart": 14, "offsetEnd": 20}, "context": "Consequently, BelMan can be uniformly tuned to support pure exploration, exploration-exploitation, and two-phase reinforcement learning problems.", "mentionContextAttributes": {"used": {"value": false, "score": 2.378225326538086e-05}, "created": {"value": false, "score": 0.00014162063598632812}, "shared": {"value": false, "score": 1.7881393432617188e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999626278877258}, "created": {"value": true, "score": 0.9990540146827698}, "shared": {"value": false, "score": 4.231929779052734e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "BelMan", "normalizedForm": "BelMan", "offsetStart": 15, "offsetEnd": 21}, "context": "We instantiate BelMan to stochastic bandits with Bernoulli and exponential rewards, and to a real-life application of scheduling queueing bandits.", "mentionContextAttributes": {"used": {"value": false, "score": 0.016550660133361816}, "created": {"value": true, "score": 0.6810444593429565}, "shared": {"value": false, "score": 2.980232238769531e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999626278877258}, "created": {"value": true, "score": 0.9990540146827698}, "shared": {"value": false, "score": 4.231929779052734e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "BelMan", "normalizedForm": "BelMan", "offsetStart": 15, "offsetEnd": 21}, "context": "We instantiate BelMan for distributions of the exponential family [6].", "mentionContextAttributes": {"used": {"value": true, "score": 0.9999405741691589}, "created": {"value": false, "score": 6.258487701416016e-06}, "shared": {"value": false, "score": 2.980232238769531e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999626278877258}, "created": {"value": true, "score": 0.9990540146827698}, "shared": {"value": false, "score": 4.231929779052734e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "BelMan", "normalizedForm": "BelMan", "offsetStart": 15, "offsetEnd": 21}, "context": "We instantiate BelMan for the problem of scheduling jobs in a multiple-server multiple-queue system with known arrival rates and unknown service rates.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9992185235023499}, "created": {"value": false, "score": 0.45948225259780884}, "shared": {"value": false, "score": 1.7881393432617188e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999626278877258}, "created": {"value": true, "score": 0.9990540146827698}, "shared": {"value": false, "score": 4.231929779052734e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "BelMan", "normalizedForm": "BelMan", "offsetStart": 16, "offsetEnd": 22}, "context": "Following that, BelMan uses the reward collected from the played arm to do Bayesian update of the belief-reward and to update the pseudobelieffocal-reward distribution-reward distribution to the point minimising the sum of KL-divergences from the belief-rewards of all the arms.", "mentionContextAttributes": {"used": {"value": false, "score": 0.21259444952011108}, "created": {"value": false, "score": 9.953975677490234e-06}, "shared": {"value": false, "score": 2.980232238769531e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999626278877258}, "created": {"value": true, "score": 0.9990540146827698}, "shared": {"value": false, "score": 4.231929779052734e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "BelMan", "normalizedForm": "BelMan", "offsetStart": 16, "offsetEnd": 22}, "context": "For this setup, BelMan outperforms other algorithms.", "mentionContextAttributes": {"used": {"value": false, "score": 0.00015163421630859375}, "created": {"value": false, "score": 2.390146255493164e-05}, "shared": {"value": false, "score": 2.980232238769531e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999626278877258}, "created": {"value": true, "score": 0.9990540146827698}, "shared": {"value": false, "score": 4.231929779052734e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "BelMan", "normalizedForm": "BelMan", "offsetStart": 17, "offsetEnd": 23}, "context": "This proves that BelMan is asymptotically consistent for finite-arm stochastic bandit problems.", "mentionContextAttributes": {"used": {"value": false, "score": 0.027799904346466064}, "created": {"value": false, "score": 7.575750350952148e-05}, "shared": {"value": false, "score": 1.7881393432617188e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999626278877258}, "created": {"value": true, "score": 0.9990540146827698}, "shared": {"value": false, "score": 4.231929779052734e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "BelMan", "normalizedForm": "BelMan", "offsetStart": 18, "offsetEnd": 24}, "context": "We also simulated BelMan on exponential bandits: 5 arms with expected rewards {0.2, 0.25, 0.33, 0.5, 1.0}.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9999094605445862}, "created": {"value": false, "score": 3.081560134887695e-05}, "shared": {"value": false, "score": 2.980232238769531e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999626278877258}, "created": {"value": true, "score": 0.9990540146827698}, "shared": {"value": false, "score": 4.231929779052734e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "BelMan", "normalizedForm": "BelMan", "offsetStart": 18, "offsetEnd": 24}, "context": "This demonstrates BelMan's broad applicability and efficient performance in complex scenarios.", "mentionContextAttributes": {"used": {"value": true, "score": 0.5170254111289978}, "created": {"value": false, "score": 0.0003967881202697754}, "shared": {"value": false, "score": 4.172325134277344e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999626278877258}, "created": {"value": true, "score": 0.9990540146827698}, "shared": {"value": false, "score": 4.231929779052734e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "BelMan", "normalizedForm": "BelMan", "offsetStart": 18, "offsetEnd": 24}, "context": "We finally tested BelMan on an exponential bandit consisting of 5-arms with expected rewards {0.2, 0.25, 0.33, 0.5, 1.0}.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9999110102653503}, "created": {"value": false, "score": 0.0001518726348876953}, "shared": {"value": false, "score": 2.980232238769531e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999626278877258}, "created": {"value": true, "score": 0.9990540146827698}, "shared": {"value": false, "score": 4.231929779052734e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "BelMan", "normalizedForm": "BelMan", "offsetStart": 19, "offsetEnd": 25}, "context": "Let us assume that BelMan has played total T times and any arm a for t a T times.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9996181130409241}, "created": {"value": false, "score": 1.8537044525146484e-05}, "shared": {"value": false, "score": 6.556510925292969e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999626278877258}, "created": {"value": true, "score": 0.9990540146827698}, "shared": {"value": false, "score": 4.231929779052734e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "BelMan", "normalizedForm": "BelMan", "offsetStart": 20, "offsetEnd": 26}, "context": "We also instantiate BelMan to the application of queueing bandits [24].", "mentionContextAttributes": {"used": {"value": true, "score": 0.7385631799697876}, "created": {"value": false, "score": 1.436471939086914e-05}, "shared": {"value": false, "score": 2.980232238769531e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999626278877258}, "created": {"value": true, "score": 0.9990540146827698}, "shared": {"value": false, "score": 4.231929779052734e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "BelMan", "normalizedForm": "BelMan", "offsetStart": 20, "offsetEnd": 26}, "context": "Figure 3 shows that BelMan performs more efficiently than state-of-the-art methods for exponential reward  distributions-Thompson sampling, UCBtuned [3], KL-UCB, and KL-UCB-exp, a method tailored for exponential distribution of rewards [14].", "mentionContextAttributes": {"used": {"value": false, "score": 0.0005045533180236816}, "created": {"value": false, "score": 3.874301910400391e-06}, "shared": {"value": false, "score": 1.7881393432617188e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999626278877258}, "created": {"value": true, "score": 0.9990540146827698}, "shared": {"value": false, "score": 4.231929779052734e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "BelMan", "normalizedForm": "BelMan", "offsetStart": 20, "offsetEnd": 26}, "context": "Figure 4 shows that BelMan's regret gradually becomes linear with respect to the logarithmic axis.", "mentionContextAttributes": {"used": {"value": false, "score": 0.05087482929229736}, "created": {"value": false, "score": 7.092952728271484e-06}, "shared": {"value": false, "score": 2.980232238769531e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999626278877258}, "created": {"value": true, "score": 0.9990540146827698}, "shared": {"value": false, "score": 4.231929779052734e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "BelMan", "normalizedForm": "BelMan", "offsetStart": 22, "offsetEnd": 28}, "context": "This mechanism allows BelMan to adaptively balance between the exploration and exploitation components.", "mentionContextAttributes": {"used": {"value": false, "score": 6.335973739624023e-05}, "created": {"value": false, "score": 0.00240480899810791}, "shared": {"value": false, "score": 2.980232238769531e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999626278877258}, "created": {"value": true, "score": 0.9990540146827698}, "shared": {"value": false, "score": 4.231929779052734e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "BelMan", "normalizedForm": "BelMan", "offsetStart": 23, "offsetEnd": 29}, "context": "This is possible since BelMan supports both modes and can transparently switch.", "mentionContextAttributes": {"used": {"value": false, "score": 9.292364120483398e-05}, "created": {"value": false, "score": 0.0001418590545654297}, "shared": {"value": false, "score": 8.940696716308594e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999626278877258}, "created": {"value": true, "score": 0.9990540146827698}, "shared": {"value": false, "score": 4.231929779052734e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "BelMan", "normalizedForm": "BelMan", "offsetStart": 23, "offsetEnd": 29}, "context": "These results validate BelMan's claim as a generic solution to a wide range of bandit problems.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9988642930984497}, "created": {"value": false, "score": 0.0001080632209777832}, "shared": {"value": false, "score": 2.980232238769531e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999626278877258}, "created": {"value": true, "score": 0.9990540146827698}, "shared": {"value": false, "score": 4.231929779052734e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "BelMan", "normalizedForm": "BelMan", "offsetStart": 25, "offsetEnd": 31}, "context": "Results demonstrate that BelMan is not only competitive but also outperforms existing algorithms for challenging setups such as those involving many arms and continuous rewards.", "mentionContextAttributes": {"used": {"value": false, "score": 0.0016131997108459473}, "created": {"value": false, "score": 0.0002765655517578125}, "shared": {"value": false, "score": 2.980232238769531e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999626278877258}, "created": {"value": true, "score": 0.9990540146827698}, "shared": {"value": false, "score": 4.231929779052734e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "BelMan", "normalizedForm": "BelMan", "offsetStart": 26, "offsetEnd": 32}, "context": "We compare performance of BelMan with state-of-the-art frequentist method tailored for exponential distribution of rewards, called KL-UCBExp [15].", "mentionContextAttributes": {"used": {"value": false, "score": 0.011041343212127686}, "created": {"value": false, "score": 1.7344951629638672e-05}, "shared": {"value": false, "score": 1.7881393432617188e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999626278877258}, "created": {"value": true, "score": 0.9990540146827698}, "shared": {"value": false, "score": 4.231929779052734e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "BelMan", "normalizedForm": "BelMan", "offsetStart": 30, "offsetEnd": 36}, "context": "We compare the performance of BelMan with frequentist methods like UCB [3] and KL-UCB [14], and Bayesian methods like Thompson sampling [34] and Bayes-UCB [21].", "mentionContextAttributes": {"used": {"value": true, "score": 0.9344680309295654}, "created": {"value": false, "score": 2.3245811462402344e-06}, "shared": {"value": false, "score": 1.7881393432617188e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999626278877258}, "created": {"value": true, "score": 0.9990540146827698}, "shared": {"value": false, "score": 4.231929779052734e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "BelMan", "normalizedForm": "BelMan", "offsetStart": 30, "offsetEnd": 36}, "context": "On all experiments performed, BelMan outperforms the competing approaches.", "mentionContextAttributes": {"used": {"value": false, "score": 0.15170931816101074}, "created": {"value": false, "score": 8.761882781982422e-06}, "shared": {"value": false, "score": 4.172325134277344e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999626278877258}, "created": {"value": true, "score": 0.9990540146827698}, "shared": {"value": false, "score": 4.231929779052734e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "BelMan", "normalizedForm": "BelMan", "offsetStart": 30, "offsetEnd": 36}, "context": "We are also investigating how BelMan can be extended to other settings such as dependent arms, non-parametric distributions and continuous arms.", "mentionContextAttributes": {"used": {"value": false, "score": 0.00039261579513549805}, "created": {"value": true, "score": 0.9881240725517273}, "shared": {"value": false, "score": 2.980232238769531e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999626278877258}, "created": {"value": true, "score": 0.9990540146827698}, "shared": {"value": false, "score": 4.231929779052734e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "BelMan", "normalizedForm": "BelMan", "offsetStart": 31, "offsetEnd": 37}, "context": "We evaluate the performance of BelMan for two exponential family distributions -Bernoulli and exponential.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9949347376823425}, "created": {"value": false, "score": 5.4895877838134766e-05}, "shared": {"value": false, "score": 1.7881393432617188e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999626278877258}, "created": {"value": true, "score": 0.9990540146827698}, "shared": {"value": false, "score": 4.231929779052734e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "BelMan", "normalizedForm": "BelMan", "offsetStart": 31, "offsetEnd": 37}, "context": "Figure 4 empirically validates BelMan to achieve logarithmic regret like the competitors which are theoretically proven to reach logarithmic regret.", "mentionContextAttributes": {"used": {"value": false, "score": 0.008184552192687988}, "created": {"value": false, "score": 3.2007694244384766e-05}, "shared": {"value": false, "score": 1.7881393432617188e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999626278877258}, "created": {"value": true, "score": 0.9990540146827698}, "shared": {"value": false, "score": 4.231929779052734e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "BelMan", "normalizedForm": "BelMan", "offsetStart": 35, "offsetEnd": 41}, "context": "Experimental results validate that BelMan asymptotically achieves logarithmic regret.", "mentionContextAttributes": {"used": {"value": false, "score": 0.4766700863838196}, "created": {"value": false, "score": 3.635883331298828e-06}, "shared": {"value": false, "score": 2.980232238769531e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999626278877258}, "created": {"value": true, "score": 0.9990540146827698}, "shared": {"value": false, "score": 4.231929779052734e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "BelMan", "normalizedForm": "BelMan", "offsetStart": 35, "offsetEnd": 41}, "context": "Thus, both I-and rI-projections in BelMan are well-defined and unique for exponential family reward distributions.", "mentionContextAttributes": {"used": {"value": false, "score": 0.0004755854606628418}, "created": {"value": false, "score": 2.7954578399658203e-05}, "shared": {"value": false, "score": 1.7881393432617188e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999626278877258}, "created": {"value": true, "score": 0.9990540146827698}, "shared": {"value": false, "score": 4.231929779052734e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "BelMan", "normalizedForm": "BelMan", "offsetStart": 36, "offsetEnd": 42}, "context": "The presence of pseudobelief offers BelMan a chance to explore the less successful arms to minimize the entropy, while the Focal distribution creates the scope of exploiting the present information of the best arm.", "mentionContextAttributes": {"used": {"value": false, "score": 0.0002605915069580078}, "created": {"value": false, "score": 0.0001354217529296875}, "shared": {"value": false, "score": 2.980232238769531e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999626278877258}, "created": {"value": true, "score": 0.9990540146827698}, "shared": {"value": false, "score": 4.231929779052734e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "BelMan", "normalizedForm": "BelMan", "offsetStart": 37, "offsetEnd": 43}, "context": "Figure 2 depicts similar features of BelMan for 20-arm Bernoulli bandits (with means 0.25, 0.22, 0.2, 0.17, 0.17, 0.2, 0.13, 0.13, 0.1, 0.07, 0.07, 0.05, 0.05, 0.05, 0.02, 0.02, 0.02, 0.01, 0.01, and 0.01).", "mentionContextAttributes": {"used": {"value": true, "score": 0.5917906165122986}, "created": {"value": false, "score": 4.231929779052734e-06}, "shared": {"value": false, "score": 4.231929779052734e-06}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999626278877258}, "created": {"value": true, "score": 0.9990540146827698}, "shared": {"value": false, "score": 4.231929779052734e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "BelMan", "normalizedForm": "BelMan", "offsetStart": 37, "offsetEnd": 43}, "context": "Except showing this ideal behaviour, BelMan performs competitively with the contending algorithms.", "mentionContextAttributes": {"used": {"value": false, "score": 0.0003809332847595215}, "created": {"value": false, "score": 1.138448715209961e-05}, "shared": {"value": false, "score": 2.980232238769531e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999626278877258}, "created": {"value": true, "score": 0.9990540146827698}, "shared": {"value": false, "score": 4.231929779052734e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "BelMan", "normalizedForm": "BelMan", "offsetStart": 41, "offsetEnd": 47}, "context": "Comparative performance evaluation shows BelMan to be more stable and efficient than existing algorithms in the queueing bandit literature.", "mentionContextAttributes": {"used": {"value": false, "score": 0.0227167010307312}, "created": {"value": false, "score": 1.9729137420654297e-05}, "shared": {"value": false, "score": 1.7881393432617188e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999626278877258}, "created": {"value": true, "score": 0.9990540146827698}, "shared": {"value": false, "score": 4.231929779052734e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "BelMan", "normalizedForm": "BelMan", "offsetStart": 43, "offsetEnd": 49}, "context": "As it is played and a reward is collected, BelMan updates the belief-reward distribution of the corresponding arm by projecting of the updated belief-reward distributions onto the pseudobelief-focal-reward. Information geometrically these two projections are studied as information (I-) and reverse information (rI-) projections [10], respectively.", "mentionContextAttributes": {"used": {"value": false, "score": 0.11677885055541992}, "created": {"value": false, "score": 8.881092071533203e-06}, "shared": {"value": false, "score": 2.980232238769531e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999626278877258}, "created": {"value": true, "score": 0.9990540146827698}, "shared": {"value": false, "score": 4.231929779052734e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "BelMan", "normalizedForm": "BelMan", "offsetStart": 47, "offsetEnd": 53}, "context": "Given \u03c4 (t) = 1 log t+c\u00d7log log t for any c 0, BelMan will asymptotically converge to choosing the optimal arm in case of a bandit with bounded reward and finite arms.", "mentionContextAttributes": {"used": {"value": false, "score": 0.005006909370422363}, "created": {"value": false, "score": 1.9669532775878906e-06}, "shared": {"value": false, "score": 2.980232238769531e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999626278877258}, "created": {"value": true, "score": 0.9990540146827698}, "shared": {"value": false, "score": 4.231929779052734e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "BelMan", "normalizedForm": "BelMan", "offsetStart": 47, "offsetEnd": 53}, "context": "Given \u03c4 (t) = 1 log t+c\u00d7log log t for any c 0, BelMan will asymptotically converge to choosing the optimal arm in case of a bandit with bounded reward and finite arms.", "mentionContextAttributes": {"used": {"value": false, "score": 0.005006909370422363}, "created": {"value": false, "score": 1.9669532775878906e-06}, "shared": {"value": false, "score": 2.980232238769531e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999626278877258}, "created": {"value": true, "score": 0.9990540146827698}, "shared": {"value": false, "score": 4.231929779052734e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "BelMan", "normalizedForm": "BelMan", "offsetStart": 49, "offsetEnd": 55}, "context": "The experimental results in Figure 6 depict that BelMan is more stable and efficient than the competing algorithms: Q-UCB, Q-Thompson sampling, and Thompson sampling. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.06449198722839355}, "created": {"value": false, "score": 1.2695789337158203e-05}, "shared": {"value": false, "score": 1.7881393432617188e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999626278877258}, "created": {"value": true, "score": 0.9990540146827698}, "shared": {"value": false, "score": 4.231929779052734e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "BelMan", "normalizedForm": "BelMan", "offsetStart": 49, "offsetEnd": 55}, "context": "Theorem 1 and 2 validate these claims in case of BelMan.", "mentionContextAttributes": {"used": {"value": true, "score": 0.92314612865448}, "created": {"value": false, "score": 1.4841556549072266e-05}, "shared": {"value": false, "score": 5.364418029785156e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999626278877258}, "created": {"value": true, "score": 0.9990540146827698}, "shared": {"value": false, "score": 4.231929779052734e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "BelMan", "normalizedForm": "BelMan", "offsetStart": 55, "offsetEnd": 61}, "context": "For the 2-arm Bernoulli bandit (\u03b8 1 = 0.8, \u03b8 2 = 0.9), BelMan performs comparatively well with respect to the contending algorithms, achieving the phase of exploitation faster than others, with significantly less variance.", "mentionContextAttributes": {"used": {"value": false, "score": 0.0011623501777648926}, "created": {"value": false, "score": 1.2814998626708984e-05}, "shared": {"value": false, "score": 1.7881393432617188e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999626278877258}, "created": {"value": true, "score": 0.9990540146827698}, "shared": {"value": false, "score": 4.231929779052734e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "BelMan", "normalizedForm": "BelMan", "offsetStart": 57, "offsetEnd": 63}, "context": "In Section 3, we empirically evaluate the performance of BelMan on different sets of arms and parameters for Bernoulli and exponential distributions, thus showing its applicability to both discrete and continuous rewards.", "mentionContextAttributes": {"used": {"value": true, "score": 0.5558769702911377}, "created": {"value": false, "score": 2.7239322662353516e-05}, "shared": {"value": false, "score": 1.7881393432617188e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999626278877258}, "created": {"value": true, "score": 0.9990540146827698}, "shared": {"value": false, "score": 4.231929779052734e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "BelMan", "normalizedForm": "BelMan", "offsetStart": 58, "offsetEnd": 64}, "context": "Similar to Figure 11, we observe the cumulative regret of BelMan grows at first linearly and then it transits to a state of slow growth.", "mentionContextAttributes": {"used": {"value": true, "score": 0.7535292506217957}, "created": {"value": false, "score": 0.0002579689025878906}, "shared": {"value": false, "score": 6.556510925292969e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999626278877258}, "created": {"value": true, "score": 0.9990540146827698}, "shared": {"value": false, "score": 4.231929779052734e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "BelMan", "normalizedForm": "BelMan", "offsetStart": 59, "offsetEnd": 65}, "context": "Thus, when 1 \u03c4 (t) exceeds the entropy term for a large t, BelMan greedily chooses the arm with highest expected reward.", "mentionContextAttributes": {"used": {"value": false, "score": 0.018559813499450684}, "created": {"value": false, "score": 9.000301361083984e-06}, "shared": {"value": false, "score": 2.980232238769531e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999626278877258}, "created": {"value": true, "score": 0.9990540146827698}, "shared": {"value": false, "score": 4.231929779052734e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "BelMan", "normalizedForm": "BelMan", "offsetStart": 60, "offsetEnd": 66}, "context": "Comparative evaluation with the state of the art shows that BelMan is not only competitive for Bernoulli bandits but in many cases also outperforms other approaches for exponential and queueing bandits.", "mentionContextAttributes": {"used": {"value": false, "score": 0.0001380443572998047}, "created": {"value": false, "score": 3.629922866821289e-05}, "shared": {"value": false, "score": 2.980232238769531e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999626278877258}, "created": {"value": true, "score": 0.9990540146827698}, "shared": {"value": false, "score": 4.231929779052734e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "BelMan", "normalizedForm": "BelMan", "offsetStart": 60, "offsetEnd": 66}, "context": "For the two-phase reinforcement learning, results show that BelMan spontaneously adapts to the explored information, improving the efficiency.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9997954368591309}, "created": {"value": false, "score": 2.3663043975830078e-05}, "shared": {"value": false, "score": 2.980232238769531e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999626278877258}, "created": {"value": true, "score": 0.9990540146827698}, "shared": {"value": false, "score": 4.231929779052734e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "BelMan", "normalizedForm": "BelMan", "offsetStart": 65, "offsetEnd": 71}, "context": "In Figure 14, though KL-UCBexp performs the best, performance of BelMan is still competitive with it.", "mentionContextAttributes": {"used": {"value": false, "score": 0.009546816349029541}, "created": {"value": false, "score": 1.519918441772461e-05}, "shared": {"value": false, "score": 2.980232238769531e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999626278877258}, "created": {"value": true, "score": 0.9990540146827698}, "shared": {"value": false, "score": 4.231929779052734e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "BelMan", "normalizedForm": "BelMan", "offsetStart": 75, "offsetEnd": 81}, "context": "We are investigating the analytical asymptotic efficiency and stability of BelMan.", "mentionContextAttributes": {"used": {"value": false, "score": 0.003882288932800293}, "created": {"value": true, "score": 0.9858672022819519}, "shared": {"value": false, "score": 2.980232238769531e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999626278877258}, "created": {"value": true, "score": 0.9990540146827698}, "shared": {"value": false, "score": 4.231929779052734e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "BelMan", "normalizedForm": "BelMan", "offsetStart": 79, "offsetEnd": 85}, "context": "Thompson sampling [34], informationdirected sampling [32], Bayes-UCB [20], and BelMan are Bayesian algorithms.", "mentionContextAttributes": {"used": {"value": true, "score": 0.7525810599327087}, "created": {"value": false, "score": 2.3245811462402344e-06}, "shared": {"value": false, "score": 1.7881393432617188e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999626278877258}, "created": {"value": true, "score": 0.9990540146827698}, "shared": {"value": false, "score": 4.231929779052734e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "BelMan", "normalizedForm": "BelMan", "offsetStart": 79, "offsetEnd": 85}, "context": "Figures 11 depicts the evolution of cumulative regret and suboptimal draws for BelMan and the other competing algorithms.", "mentionContextAttributes": {"used": {"value": false, "score": 0.24101483821868896}, "created": {"value": false, "score": 2.6404857635498047e-05}, "shared": {"value": false, "score": 1.7881393432617188e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999626278877258}, "created": {"value": true, "score": 0.9990540146827698}, "shared": {"value": false, "score": 4.231929779052734e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "BelMan", "normalizedForm": "BelMan", "offsetStart": 80, "offsetEnd": 86}, "context": "In Section 2, we apply these information-geometric constructions to develop the BelMan algorithm.", "mentionContextAttributes": {"used": {"value": false, "score": 0.0007135868072509766}, "created": {"value": true, "score": 0.9990540146827698}, "shared": {"value": false, "score": 1.7881393432617188e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999626278877258}, "created": {"value": true, "score": 0.9990540146827698}, "shared": {"value": false, "score": 4.231929779052734e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "BelMan", "normalizedForm": "BelMan", "offsetStart": 82, "offsetEnd": 88}, "context": "We prove the law of convergence of the pseudobelief-focal-reward distribution for BelMan, and that BelMan asymptotically converges to the choice of the optimal arm.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9999626278877258}, "created": {"value": false, "score": 0.0002193450927734375}, "shared": {"value": false, "score": 2.980232238769531e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999626278877258}, "created": {"value": true, "score": 0.9990540146827698}, "shared": {"value": false, "score": 4.231929779052734e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "BelMan", "normalizedForm": "BelMan", "offsetStart": 82, "offsetEnd": 88}, "context": "We update this belief eventually as we further draw the arms and compute it using BelMan.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9989802837371826}, "created": {"value": false, "score": 0.11614632606506348}, "shared": {"value": false, "score": 1.7881393432617188e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999626278877258}, "created": {"value": true, "score": 0.9990540146827698}, "shared": {"value": false, "score": 4.231929779052734e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "BelMan", "normalizedForm": "BelMan", "offsetStart": 85, "offsetEnd": 91}, "context": "Thus, both information and reverse information projections [10] that we would use in BelMan are well-defined and unique.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9319839477539062}, "created": {"value": false, "score": 5.418062210083008e-05}, "shared": {"value": false, "score": 1.7881393432617188e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999626278877258}, "created": {"value": true, "score": 0.9990540146827698}, "shared": {"value": false, "score": 4.231929779052734e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "BelMan", "normalizedForm": "BelMan", "offsetStart": 91, "offsetEnd": 97}, "context": "If we also compare it with the initial growth in cumulative regret and suboptimal draws of BelMan in Figure 2, we observe that the regret for the exploration-exploitation phase is less than that of regular BelMan exploration-exploitation.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9990501999855042}, "created": {"value": false, "score": 8.165836334228516e-06}, "shared": {"value": false, "score": 2.980232238769531e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999626278877258}, "created": {"value": true, "score": 0.9990540146827698}, "shared": {"value": false, "score": 4.231929779052734e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "BelMan", "normalizedForm": "BelMan", "offsetStart": 97, "offsetEnd": 103}, "context": "A comparative performance evaluation for queueing systems with Bernoulli service rates show that BelMan performs significantly better than the existing algorithms, such as Q-UCB, Q-ThS, and Thompson sampling.", "mentionContextAttributes": {"used": {"value": false, "score": 0.015266776084899902}, "created": {"value": false, "score": 3.9637088775634766e-05}, "shared": {"value": false, "score": 2.980232238769531e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999626278877258}, "created": {"value": true, "score": 0.9990540146827698}, "shared": {"value": false, "score": 4.231929779052734e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "BelMan", "normalizedForm": "BelMan", "offsetStart": 97, "offsetEnd": 103}, "context": "From Figures 1,2, and 3, we observe that at the very beginning the number of suboptimal draws of BelMan grows linearly and then transitions to a state of slow growth.", "mentionContextAttributes": {"used": {"value": false, "score": 0.009089171886444092}, "created": {"value": false, "score": 4.5239925384521484e-05}, "shared": {"value": false, "score": 5.364418029785156e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999626278877258}, "created": {"value": true, "score": 0.9990540146827698}, "shared": {"value": false, "score": 4.231929779052734e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "BelMan", "normalizedForm": "BelMan", "offsetStart": 99, "offsetEnd": 105}, "context": "We prove the law of convergence of the pseudobelief-focal-reward distribution for BelMan, and that BelMan asymptotically converges to the choice of the optimal arm.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9999626278877258}, "created": {"value": false, "score": 0.0002193450927734375}, "shared": {"value": false, "score": 2.980232238769531e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999626278877258}, "created": {"value": true, "score": 0.9990540146827698}, "shared": {"value": false, "score": 4.231929779052734e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "BelMan", "normalizedForm": "BelMan", "offsetStart": 99, "offsetEnd": 105}, "context": "The setting is that of the 20-arm Bernoulli bandit in Figure 2. The two-phase algorithm is exactly BelMan (Algorithm 1) with \u03c4 (t) = \u221e for an initial phase of length T EXP followed by the decreasing function of t as indicated previously.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9654422998428345}, "created": {"value": false, "score": 2.9206275939941406e-06}, "shared": {"value": false, "score": 2.980232238769531e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999626278877258}, "created": {"value": true, "score": 0.9990540146827698}, "shared": {"value": false, "score": 4.231929779052734e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "BelMan", "normalizedForm": "BelMan", "offsetStart": 103, "offsetEnd": 109}, "context": "Thus, at each iteration, we obtain an optimal and unambiguous computation of the decision variables of BelMan.", "mentionContextAttributes": {"used": {"value": false, "score": 0.43676674365997314}, "created": {"value": false, "score": 9.328126907348633e-05}, "shared": {"value": false, "score": 1.7881393432617188e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999626278877258}, "created": {"value": true, "score": 0.9990540146827698}, "shared": {"value": false, "score": 4.231929779052734e-06}}}, {"type": "software", "software-type": "environment", "wikidataId": "Q169478", "wikipediaExternalRef": 20412, "lang": "en", "confidence": 0.9128, "software-name": {"rawForm": "MATLAB", "normalizedForm": "MATLAB", "wikidataId": "Q169478", "wikipediaExternalRef": 20412, "lang": "en", "confidence": 0.9128, "offsetStart": 104, "offsetEnd": 110}, "version": {"rawForm": "2014a", "normalizedForm": "2014a", "offsetStart": 111, "offsetEnd": 116}, "context": "We use the pyma-Bandits library [9] for implementation of all the algorithms except ours, and run it on MATLAB 2014a. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9971954226493835}, "created": {"value": false, "score": 0.0004163384437561035}, "shared": {"value": false, "score": 1.7881393432617188e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9971954226493835}, "created": {"value": false, "score": 0.0004163384437561035}, "shared": {"value": false, "score": 1.7881393432617188e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "BelMan", "normalizedForm": "BelMan", "offsetStart": 105, "offsetEnd": 111}, "context": "Under this specific setting of beta prior and Bernoulli reward, we compute the targeted KL-divergence of BelMan as Here, N a t = \u03b1 a t + \u03b2 a t is the total number of times the jth arm is played till the nth iteration, N = \u1fb1 + \u03b2 and \u03a8 is the digamma function [5] defined as the derivative of the logarithm of gamma function, i.e. d da (log \u0393 (a)).", "mentionContextAttributes": {"used": {"value": true, "score": 0.9954455494880676}, "created": {"value": false, "score": 3.516674041748047e-06}, "shared": {"value": false, "score": 1.2516975402832031e-06}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999626278877258}, "created": {"value": true, "score": 0.9990540146827698}, "shared": {"value": false, "score": 4.231929779052734e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "BelMan", "normalizedForm": "BelMan", "offsetStart": 108, "offsetEnd": 114}, "context": "Under this specific setting of gamma prior and exponential reward, we compute the targeted KL-divergence of BelMan as + \u1fb1t-1 log \u03b2 a t ] + K log Zt + K log (\u0393 (\u1fb1 t-1 )) -K \u1fb1t-1 log \u03b2t-1 .", "mentionContextAttributes": {"used": {"value": true, "score": 0.9963976144790649}, "created": {"value": false, "score": 2.205371856689453e-06}, "shared": {"value": false, "score": 2.980232238769531e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999626278877258}, "created": {"value": true, "score": 0.9990540146827698}, "shared": {"value": false, "score": 4.231929779052734e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "BelMan", "normalizedForm": "BelMan", "offsetStart": 110, "offsetEnd": 116}, "context": ". Though Assumption 1 is followed throughout this paper, we note it is not essential to develop the framework BelMan relies on, though it makes calculations easier.", "mentionContextAttributes": {"used": {"value": false, "score": 0.00013899803161621094}, "created": {"value": true, "score": 0.9300388097763062}, "shared": {"value": false, "score": 4.172325134277344e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999626278877258}, "created": {"value": true, "score": 0.9990540146827698}, "shared": {"value": false, "score": 4.231929779052734e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "BelMan", "normalizedForm": "BelMan", "offsetStart": 112, "offsetEnd": 118}, "context": "Since exponential family distributions are flat with respect to KL-divergence [1], both I-and rI-projections in BelMan are well-defined and unique.", "mentionContextAttributes": {"used": {"value": false, "score": 0.0006387829780578613}, "created": {"value": false, "score": 4.351139068603516e-06}, "shared": {"value": false, "score": 1.7881393432617188e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999626278877258}, "created": {"value": true, "score": 0.9990540146827698}, "shared": {"value": false, "score": 4.231929779052734e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "BelMan", "normalizedForm": "BelMan", "offsetStart": 152, "offsetEnd": 158}, "context": "We have also run the experiments 50 times with horizon 50 000 for the 20 arm Bernoulli bandit setting of Figure 2 to verify the asymptotic behaviour of BelMan.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9999439120292664}, "created": {"value": false, "score": 0.16746193170547485}, "shared": {"value": false, "score": 1.7881393432617188e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999626278877258}, "created": {"value": true, "score": 0.9990540146827698}, "shared": {"value": false, "score": 4.231929779052734e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "BelMan", "normalizedForm": "BelMan", "offsetStart": 206, "offsetEnd": 212}, "context": "If we also compare it with the initial growth in cumulative regret and suboptimal draws of BelMan in Figure 2, we observe that the regret for the exploration-exploitation phase is less than that of regular BelMan exploration-exploitation.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9990501999855042}, "created": {"value": false, "score": 8.106231689453125e-06}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999626278877258}, "created": {"value": true, "score": 0.9990540146827698}, "shared": {"value": false, "score": 4.231929779052734e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "BelMan", "normalizedForm": "BelMan", "offsetStart": 238, "offsetEnd": 244}, "context": "While the pseudobelief-reward facilitates information accumulation through exploration, another mechanism is needed to increase exploitation by gradually focusing on higher rewards, the pseudobelief-focal-reward. Our resulting algorithm, BelMan, alternates between projection of the pseudobelief-focal-reward onto belief-reward distributions to choose the arm to play, and projection of the updated belief-reward distributions onto the pseudobelief-focal-reward. We theoretically prove BelMan to be asymptotically optimal and to incur a sublinear regret growth.", "mentionContextAttributes": {"used": {"value": false, "score": 0.35843056440353394}, "created": {"value": false, "score": 0.004235982894897461}, "shared": {"value": false, "score": 3.337860107421875e-06}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999626278877258}, "created": {"value": true, "score": 0.9990540146827698}, "shared": {"value": false, "score": 4.231929779052734e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "BelMan", "normalizedForm": "BelMan", "offsetStart": 486, "offsetEnd": 492}, "context": "While the pseudobelief-reward facilitates information accumulation through exploration, another mechanism is needed to increase exploitation by gradually focusing on higher rewards, the pseudobelief-focal-reward. Our resulting algorithm, BelMan, alternates between projection of the pseudobelief-focal-reward onto belief-reward distributions to choose the arm to play, and projection of the updated belief-reward distributions onto the pseudobelief-focal-reward. We theoretically prove BelMan to be asymptotically optimal and to incur a sublinear regret growth.", "mentionContextAttributes": {"used": {"value": false, "score": 0.3584301471710205}, "created": {"value": false, "score": 0.004235982894897461}, "shared": {"value": false, "score": 3.337860107421875e-06}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999626278877258}, "created": {"value": true, "score": 0.9990540146827698}, "shared": {"value": false, "score": 4.231929779052734e-06}}}], "references": [{"refKey": 0, "tei": "<biblStruct xml:id=\"b0\">\n\t<analytic>\n\t\t<title level=\"a\" type=\"main\">BelMan: An Information-Geometric Approach to Stochastic Bandits</title>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Debabrota</forename><surname>Basu</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Pierre</forename><surname>Senellart</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">St\u00e9phane</forename><surname>Bressan</surname></persName>\n\t\t</author>\n\t\t<idno type=\"DOI\">10.1007/978-3-030-46133-1_11</idno>\n\t\t<idno>4D736E40B93D9D4233638174723E7597</idno>\n\t</analytic>\n\t<monogr>\n\t\t<title level=\"m\">Machine Learning and Knowledge Discovery in Databases</title>\n\t\t<imprint>\n\t\t\t<publisher>Springer International Publishing</publisher>\n\t\t\t<date type=\"published\" when=\"2020\" />\n\t\t\t<biblScope unit=\"page\" from=\"167\" to=\"183\" />\n\t\t</imprint>\n\t</monogr>\n</biblStruct>\n"}], "runtime": 107527, "id": "aad5f4b2a8859c360726e467594234d0bde54cc8", "metadata": {"id": "aad5f4b2a8859c360726e467594234d0bde54cc8"}, "original_file_path": "../../datalake/Samuel/SOFTware-Sync/data/xml_files/hal-02195539.grobid.tei.xml", "file_name": "hal-02195539.grobid.tei.xml"}