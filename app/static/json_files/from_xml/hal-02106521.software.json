{"application": "software-mentions", "version": "0.8.0", "date": "2024-04-12T15:37+0000", "md5": "65A007CFFD84EA9AA27504A0FB692C07", "mentions": [{"type": "software", "software-type": "software", "software-name": {"rawForm": "YARN", "normalizedForm": "YARN", "offsetStart": 0, "offsetEnd": 4}, "context": "YARN scheduler). ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9021984934806824}, "created": {"value": false, "score": 8.404254913330078e-06}, "shared": {"value": false, "score": 1.4901161193847656e-06}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999886155128479}, "created": {"value": false, "score": 0.0004999041557312012}, "shared": {"value": false, "score": 1.4901161193847656e-06}}}, {"type": "software", "software-type": "software", "wikidataId": "Q7573619", "wikipediaExternalRef": 42164234, "lang": "en", "confidence": 0.5539, "software-name": {"rawForm": "Spark", "normalizedForm": "Spark", "wikidataId": "Q7573619", "wikipediaExternalRef": 42164234, "lang": "en", "confidence": 0.5539, "offsetStart": 0, "offsetEnd": 5}, "version": {"rawForm": "2.3.0", "normalizedForm": "2.3.0"}, "publisher": {"rawForm": "Hadoop", "normalizedForm": "Hadoop"}, "context": "Spark relies on Resilient Distributed Datasets (RDDs), which are fault-tolerant and immutable collections of data distributed among the cluster nodes. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.0005365610122680664}, "created": {"value": false, "score": 0.00020456314086914062}, "shared": {"value": false, "score": 2.980232238769531e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999958872795105}, "created": {"value": true, "score": 0.9998844265937805}, "shared": {"value": false, "score": 5.364418029785156e-07}}}, {"type": "software", "software-type": "software", "wikidataId": "Q7573619", "wikipediaExternalRef": 42164234, "lang": "en", "confidence": 0.5539, "software-name": {"rawForm": "Spark", "normalizedForm": "Spark", "wikidataId": "Q7573619", "wikipediaExternalRef": 42164234, "lang": "en", "confidence": 0.5539, "offsetStart": 0, "offsetEnd": 5}, "version": {"rawForm": "2.3.0", "normalizedForm": "2.3.0"}, "publisher": {"rawForm": "Hadoop", "normalizedForm": "Hadoop"}, "context": "Spark also supports broadcast variables, i.e., collections of the data that are first gathered at the driver and then are broadcast (copies shipped over network) to all the cluster nodes. ", "mentionContextAttributes": {"used": {"value": false, "score": 4.309415817260742e-05}, "created": {"value": false, "score": 0.000164031982421875}, "shared": {"value": false, "score": 2.980232238769531e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999958872795105}, "created": {"value": true, "score": 0.9998844265937805}, "shared": {"value": false, "score": 5.364418029785156e-07}}}, {"type": "software", "software-type": "software", "wikidataId": "Q7573619", "wikipediaExternalRef": 42164234, "lang": "en", "confidence": 0.5539, "software-name": {"rawForm": "Spark", "normalizedForm": "Spark", "wikidataId": "Q7573619", "wikipediaExternalRef": 42164234, "lang": "en", "confidence": 0.5539, "offsetStart": 0, "offsetEnd": 5}, "version": {"rawForm": "2.3.0", "normalizedForm": "2.3.0"}, "publisher": {"rawForm": "Hadoop", "normalizedForm": "Hadoop"}, "context": "Spark parameters relevant for our performance analysis were set as follows:", "mentionContextAttributes": {"used": {"value": true, "score": 0.9999958872795105}, "created": {"value": false, "score": 4.667043685913086e-05}, "shared": {"value": false, "score": 2.980232238769531e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999958872795105}, "created": {"value": true, "score": 0.9998844265937805}, "shared": {"value": false, "score": 5.364418029785156e-07}}}, {"type": "software", "software-type": "software", "wikidataId": "Q7573619", "wikipediaExternalRef": 42164234, "lang": "en", "confidence": 0.5539, "software-name": {"rawForm": "Spark", "normalizedForm": "Spark", "wikidataId": "Q7573619", "wikipediaExternalRef": 42164234, "lang": "en", "confidence": 0.5539, "offsetStart": 2, "offsetEnd": 7}, "version": {"rawForm": "2.3.0", "normalizedForm": "2.3.0"}, "publisher": {"rawForm": "Hadoop", "normalizedForm": "Hadoop"}, "context": "A Spark cluster has a set of worker nodes and a driver, which coordinates the run of an application and communicates with cluster manager (e.g.", "mentionContextAttributes": {"used": {"value": false, "score": 0.0008353590965270996}, "created": {"value": false, "score": 0.0001697540283203125}, "shared": {"value": false, "score": 2.980232238769531e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999958872795105}, "created": {"value": true, "score": 0.9998844265937805}, "shared": {"value": false, "score": 5.364418029785156e-07}}}, {"type": "software", "software-type": "software", "wikidataId": "Q7573619", "wikipediaExternalRef": 42164234, "lang": "en", "confidence": 0.5539, "software-name": {"rawForm": "Spark", "normalizedForm": "Spark", "wikidataId": "Q7573619", "wikipediaExternalRef": 42164234, "lang": "en", "confidence": 0.5539, "offsetStart": 2, "offsetEnd": 7}, "version": {"rawForm": "2.3.0", "normalizedForm": "2.3.0"}, "publisher": {"rawForm": "Hadoop", "normalizedForm": "Hadoop"}, "context": "A Spark executor is a process working on a piece of data in a worker node.", "mentionContextAttributes": {"used": {"value": false, "score": 0.00019121170043945312}, "created": {"value": false, "score": 0.03752332925796509}, "shared": {"value": false, "score": 1.7881393432617188e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999958872795105}, "created": {"value": true, "score": 0.9998844265937805}, "shared": {"value": false, "score": 5.364418029785156e-07}}}, {"type": "software", "software-type": "software", "wikidataId": "Q7573619", "wikipediaExternalRef": 42164234, "lang": "en", "confidence": 0.5539, "software-name": {"rawForm": "Spark", "normalizedForm": "Spark", "wikidataId": "Q7573619", "wikipediaExternalRef": 42164234, "lang": "en", "confidence": 0.5539, "offsetStart": 5, "offsetEnd": 10}, "version": {"rawForm": "2.3.0", "normalizedForm": "2.3.0"}, "publisher": {"rawForm": "Hadoop", "normalizedForm": "Hadoop"}, "context": "Each Spark application consists of jobs that are divided into stages, each divided into tasks. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.0025135278701782227}, "created": {"value": false, "score": 0.0003896355628967285}, "shared": {"value": false, "score": 4.172325134277344e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999958872795105}, "created": {"value": true, "score": 0.9998844265937805}, "shared": {"value": false, "score": 5.364418029785156e-07}}}, {"type": "software", "software-type": "software", "wikidataId": "Q7573619", "wikipediaExternalRef": 42164234, "lang": "en", "confidence": 0.5539, "software-name": {"rawForm": "Spark", "normalizedForm": "Spark", "wikidataId": "Q7573619", "wikipediaExternalRef": 42164234, "lang": "en", "confidence": 0.5539, "offsetStart": 8, "offsetEnd": 13}, "version": {"rawForm": "2.3.0", "normalizedForm": "2.3.0", "offsetStart": 14, "offsetEnd": 19}, "publisher": {"rawForm": "Hadoop", "normalizedForm": "Hadoop", "offsetStart": 25, "offsetEnd": 31}, "context": "We used Spark 2.3.0 with Hadoop's YARN scheduler 2.9.0. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9999886155128479}, "created": {"value": false, "score": 1.6093254089355469e-06}, "shared": {"value": false, "score": 2.980232238769531e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999958872795105}, "created": {"value": true, "score": 0.9998844265937805}, "shared": {"value": false, "score": 5.364418029785156e-07}}}, {"type": "software", "software-type": "software", "wikidataId": "Q7573619", "wikipediaExternalRef": 42164234, "lang": "en", "confidence": 0.5539, "software-name": {"rawForm": "Spark", "normalizedForm": "Spark", "wikidataId": "Q7573619", "wikipediaExternalRef": 42164234, "lang": "en", "confidence": 0.5539, "offsetStart": 10, "offsetEnd": 15}, "version": {"rawForm": "2.3.0", "normalizedForm": "2.3.0"}, "publisher": {"rawForm": "Hadoop", "normalizedForm": "Hadoop"}, "context": "Following Spark guidelines, unless otherwise specified, we pick C E = 4.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9285440444946289}, "created": {"value": false, "score": 1.8894672393798828e-05}, "shared": {"value": false, "score": 4.172325134277344e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999958872795105}, "created": {"value": true, "score": 0.9998844265937805}, "shared": {"value": false, "score": 5.364418029785156e-07}}}, {"type": "software", "software-type": "software", "wikidataId": "Q7573619", "wikipediaExternalRef": 42164234, "lang": "en", "confidence": 0.5539, "software-name": {"rawForm": "Spark", "normalizedForm": "Spark", "wikidataId": "Q7573619", "wikipediaExternalRef": 42164234, "lang": "en", "confidence": 0.5539, "offsetStart": 11, "offsetEnd": 16}, "version": {"rawForm": "2.3.0", "normalizedForm": "2.3.0"}, "publisher": {"rawForm": "Hadoop", "normalizedForm": "Hadoop"}, "context": "We give to Spark and YARN 100GB of RAM and 36 cores at each machine.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9992489814758301}, "created": {"value": false, "score": 0.0004999041557312012}, "shared": {"value": false, "score": 1.7881393432617188e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999958872795105}, "created": {"value": true, "score": 0.9998844265937805}, "shared": {"value": false, "score": 5.364418029785156e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "YARN", "normalizedForm": "YARN", "offsetStart": 21, "offsetEnd": 25}, "context": "We give to Spark and YARN 100GB of RAM and 36 cores at each machine. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9992489814758301}, "created": {"value": false, "score": 0.0004999041557312012}, "shared": {"value": false, "score": 1.7881393432617188e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999886155128479}, "created": {"value": false, "score": 0.0004999041557312012}, "shared": {"value": false, "score": 1.4901161193847656e-06}}}, {"type": "software", "software-type": "software", "wikidataId": "Q7573619", "wikipediaExternalRef": 42164234, "lang": "en", "confidence": 0.5539, "software-name": {"rawForm": "Spark", "normalizedForm": "Spark", "wikidataId": "Q7573619", "wikipediaExternalRef": 42164234, "lang": "en", "confidence": 0.5539, "offsetStart": 27, "offsetEnd": 32}, "version": {"rawForm": "2.3.0", "normalizedForm": "2.3.0"}, "publisher": {"rawForm": "Hadoop", "normalizedForm": "Hadoop"}, "context": "Adapting our algorithms to Spark. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.0001590251922607422}, "created": {"value": true, "score": 0.5963078737258911}, "shared": {"value": false, "score": 2.980232238769531e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999958872795105}, "created": {"value": true, "score": 0.9998844265937805}, "shared": {"value": false, "score": 5.364418029785156e-07}}}, {"type": "software", "software-type": "software", "wikidataId": "Q460584", "wikipediaExternalRef": 3254510, "lang": "en", "confidence": 0.591, "software-name": {"rawForm": "Scala", "normalizedForm": "Scala", "wikidataId": "Q460584", "wikipediaExternalRef": 3254510, "lang": "en", "confidence": 0.591, "offsetStart": 33, "offsetEnd": 38}, "version": {"rawForm": "2.11", "normalizedForm": "2.11", "offsetStart": 39, "offsetEnd": 43}, "context": "We implemented our algorithms in Scala 2.11.", "mentionContextAttributes": {"used": {"value": false, "score": 0.00016689300537109375}, "created": {"value": true, "score": 0.979759156703949}, "shared": {"value": false, "score": 2.980232238769531e-07}}, "documentContextAttributes": {"used": {"value": false, "score": 0.00016689300537109375}, "created": {"value": true, "score": 0.979759156703949}, "shared": {"value": false, "score": 2.980232238769531e-07}}}, {"type": "software", "software-type": "software", "wikidataId": "Q11354", "wikipediaExternalRef": 2581, "lang": "en", "confidence": 0.8046, "software-name": {"rawForm": "Apache", "normalizedForm": "Apache", "wikidataId": "Q11354", "wikipediaExternalRef": 2581, "lang": "en", "confidence": 0.8046, "offsetStart": 33, "offsetEnd": 39}, "context": "We instantiate our algorithms to Apache Spark platform; our experiments demonstrate the merit of our approach.", "mentionContextAttributes": {"used": {"value": true, "score": 0.892501175403595}, "created": {"value": true, "score": 0.9713102579116821}, "shared": {"value": false, "score": 2.980232238769531e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.892501175403595}, "created": {"value": true, "score": 0.9713102579116821}, "shared": {"value": false, "score": 2.980232238769531e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "YARN", "normalizedForm": "YARN", "offsetStart": 34, "offsetEnd": 38}, "context": "We used Spark 2.3.0 with Hadoop's YARN scheduler 2.9.0.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9999886155128479}, "created": {"value": false, "score": 1.6093254089355469e-06}, "shared": {"value": false, "score": 2.980232238769531e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999886155128479}, "created": {"value": false, "score": 0.0004999041557312012}, "shared": {"value": false, "score": 1.4901161193847656e-06}}}, {"type": "software", "software-type": "software", "wikidataId": "Q7573619", "wikipediaExternalRef": 42164234, "lang": "en", "confidence": 0.5539, "software-name": {"rawForm": "Spark", "normalizedForm": "Spark", "wikidataId": "Q7573619", "wikipediaExternalRef": 42164234, "lang": "en", "confidence": 0.5539, "offsetStart": 39, "offsetEnd": 44}, "version": {"rawForm": "2.3.0", "normalizedForm": "2.3.0"}, "publisher": {"rawForm": "Hadoop", "normalizedForm": "Hadoop"}, "context": "We adapt our distributed algorithms to Spark's RDD-based computation model as follows.", "mentionContextAttributes": {"used": {"value": false, "score": 0.0032196044921875}, "created": {"value": true, "score": 0.998895525932312}, "shared": {"value": false, "score": 2.980232238769531e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999958872795105}, "created": {"value": true, "score": 0.9998844265937805}, "shared": {"value": false, "score": 5.364418029785156e-07}}}, {"type": "software", "software-type": "software", "wikidataId": "Q7573619", "wikipediaExternalRef": 42164234, "lang": "en", "confidence": 0.5539, "software-name": {"rawForm": "Spark", "normalizedForm": "Spark", "wikidataId": "Q7573619", "wikipediaExternalRef": 42164234, "lang": "en", "confidence": 0.5539, "offsetStart": 40, "offsetEnd": 45}, "version": {"rawForm": "2.3.0", "normalizedForm": "2.3.0"}, "publisher": {"rawForm": "Hadoop", "normalizedForm": "Hadoop"}, "context": "We instantiate our algorithms to Apache Spark platform; our experiments demonstrate the merit of our approach.", "mentionContextAttributes": {"used": {"value": true, "score": 0.892501175403595}, "created": {"value": true, "score": 0.9713102579116821}, "shared": {"value": false, "score": 2.980232238769531e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999958872795105}, "created": {"value": true, "score": 0.9998844265937805}, "shared": {"value": false, "score": 5.364418029785156e-07}}}, {"type": "software", "software-type": "software", "wikidataId": "Q7573619", "wikipediaExternalRef": 42164234, "lang": "en", "confidence": 0.5539, "software-name": {"rawForm": "Spark", "normalizedForm": "Spark", "wikidataId": "Q7573619", "wikipediaExternalRef": 42164234, "lang": "en", "confidence": 0.5539, "offsetStart": 41, "offsetEnd": 46}, "version": {"rawForm": "2.3.0", "normalizedForm": "2.3.0"}, "publisher": {"rawForm": "Hadoop", "normalizedForm": "Hadoop"}, "context": "We have implemented our algorithms using Spark; our experiments confirm that parallelism allows to distribute work and speed up execution.", "mentionContextAttributes": {"used": {"value": false, "score": 0.00048273801803588867}, "created": {"value": true, "score": 0.9998385906219482}, "shared": {"value": false, "score": 2.980232238769531e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999958872795105}, "created": {"value": true, "score": 0.9998844265937805}, "shared": {"value": false, "score": 5.364418029785156e-07}}}, {"type": "software", "software-type": "software", "wikidataId": "Q7573619", "wikipediaExternalRef": 42164234, "lang": "en", "confidence": 0.5539, "software-name": {"rawForm": "Spark", "normalizedForm": "Spark", "wikidataId": "Q7573619", "wikipediaExternalRef": 42164234, "lang": "en", "confidence": 0.5539, "offsetStart": 54, "offsetEnd": 59}, "version": {"rawForm": "2.3.0", "normalizedForm": "2.3.0"}, "publisher": {"rawForm": "Hadoop", "normalizedForm": "Hadoop"}, "context": "Section 3.4 shows how to tailor our algorithms to the Spark framework used in our implementation.", "mentionContextAttributes": {"used": {"value": false, "score": 0.06568938493728638}, "created": {"value": false, "score": 0.16687816381454468}, "shared": {"value": false, "score": 2.980232238769531e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999958872795105}, "created": {"value": true, "score": 0.9998844265937805}, "shared": {"value": false, "score": 5.364418029785156e-07}}}, {"type": "software", "software-type": "software", "wikidataId": "Q7573619", "wikipediaExternalRef": 42164234, "lang": "en", "confidence": 0.5539, "software-name": {"rawForm": "Spark", "normalizedForm": "Spark", "wikidataId": "Q7573619", "wikipediaExternalRef": 42164234, "lang": "en", "confidence": 0.5539, "offsetStart": 54, "offsetEnd": 59}, "version": {"rawForm": "2.3.0", "normalizedForm": "2.3.0"}, "publisher": {"rawForm": "Hadoop", "normalizedForm": "Hadoop"}, "context": "We recall here that M denotes the number of machines (Spark workers).", "mentionContextAttributes": {"used": {"value": true, "score": 0.9955654144287109}, "created": {"value": false, "score": 4.285573959350586e-05}, "shared": {"value": false, "score": 2.980232238769531e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999958872795105}, "created": {"value": true, "score": 0.9998844265937805}, "shared": {"value": false, "score": 5.364418029785156e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "YARN", "normalizedForm": "YARN", "offsetStart": 58, "offsetEnd": 62}, "context": "We set the lower bound for R E as a minimal memory of the YARN container, within the memory limit for executor we need to hold out around 1GB for a memory overhead (memory for JVM in YARN).", "mentionContextAttributes": {"used": {"value": true, "score": 0.9997990131378174}, "created": {"value": false, "score": 4.7266483306884766e-05}, "shared": {"value": false, "score": 2.980232238769531e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999886155128479}, "created": {"value": false, "score": 0.0004999041557312012}, "shared": {"value": false, "score": 1.4901161193847656e-06}}}, {"type": "software", "software-type": "software", "wikidataId": "Q7573619", "wikipediaExternalRef": 42164234, "lang": "en", "confidence": 0.5539, "software-name": {"rawForm": "Spark", "normalizedForm": "Spark", "wikidataId": "Q7573619", "wikipediaExternalRef": 42164234, "lang": "en", "confidence": 0.5539, "offsetStart": 65, "offsetEnd": 70}, "version": {"rawForm": "2.3.0", "normalizedForm": "2.3.0"}, "publisher": {"rawForm": "Hadoop", "normalizedForm": "Hadoop"}, "context": "Recalling also Figure 6, we believe there is some variability in Spark in-memory execution performance, that we were not able to control precisely.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9999591708183289}, "created": {"value": false, "score": 4.774332046508789e-05}, "shared": {"value": false, "score": 1.7881393432617188e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999958872795105}, "created": {"value": true, "score": 0.9998844265937805}, "shared": {"value": false, "score": 5.364418029785156e-07}}}, {"type": "software", "software-type": "software", "wikidataId": "Q7573619", "wikipediaExternalRef": 42164234, "lang": "en", "confidence": 0.5539, "software-name": {"rawForm": "Spark", "normalizedForm": "Spark", "wikidataId": "Q7573619", "wikipediaExternalRef": 42164234, "lang": "en", "confidence": 0.5539, "offsetStart": 107, "offsetEnd": 112}, "version": {"rawForm": "2.3.0", "normalizedForm": "2.3.0"}, "publisher": {"rawForm": "Hadoop", "normalizedForm": "Hadoop"}, "context": "We studied the design of the algorithms dedicated to a distributed setting, and we implemented them in the Spark framework.", "mentionContextAttributes": {"used": {"value": false, "score": 0.17364752292633057}, "created": {"value": true, "score": 0.9998844265937805}, "shared": {"value": false, "score": 2.980232238769531e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999958872795105}, "created": {"value": true, "score": 0.9998844265937805}, "shared": {"value": false, "score": 5.364418029785156e-07}}}, {"type": "software", "software-type": "software", "wikidataId": "Q7573619", "wikipediaExternalRef": 42164234, "lang": "en", "confidence": 0.5539, "software-name": {"rawForm": "Spark", "normalizedForm": "Spark", "wikidataId": "Q7573619", "wikipediaExternalRef": 42164234, "lang": "en", "confidence": 0.5539, "offsetStart": 117, "offsetEnd": 122}, "version": {"rawForm": "2.3.0", "normalizedForm": "2.3.0"}, "publisher": {"rawForm": "Hadoop", "normalizedForm": "Hadoop"}, "context": "To do this, we need to identify the (duplicate-free) set of node labels from G, operation which we implemented using Spark's distinct() function, which eliminates 3 Our online summary visualizations 1 also apply two post-processing steps to make summaries even smaller: group nodes by their most general type (\u00e0 la [GM18]) and draw leaf nodes as attributes of their parents.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9444169402122498}, "created": {"value": false, "score": 0.00029528141021728516}, "shared": {"value": false, "score": 5.364418029785156e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999958872795105}, "created": {"value": true, "score": 0.9998844265937805}, "shared": {"value": false, "score": 5.364418029785156e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "YARN", "normalizedForm": "YARN", "offsetStart": 183, "offsetEnd": 187}, "context": "We set the lower bound for R E as a minimal memory of the YARN container, within the memory limit for executor we need to hold out around 1GB for a memory overhead (memory for JVM in YARN).", "mentionContextAttributes": {"used": {"value": true, "score": 0.9997989535331726}, "created": {"value": false, "score": 4.7206878662109375e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999886155128479}, "created": {"value": false, "score": 0.0004999041557312012}, "shared": {"value": false, "score": 1.4901161193847656e-06}}}], "references": [], "runtime": 133819, "id": "18892d2babcb96ac4eab781f33e318345a7bf899", "metadata": {"id": "18892d2babcb96ac4eab781f33e318345a7bf899"}, "original_file_path": "../../datalake/Samuel/SOFTware-Sync/data/xml_files/hal-02106521.grobid.tei.xml", "file_name": "hal-02106521.grobid.tei.xml"}