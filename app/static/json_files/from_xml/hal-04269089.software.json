{"application": "software-mentions", "version": "0.8.0", "date": "2024-04-12T17:30+0000", "md5": "E9D358DCC34B955D97C1C2A4FD04D785", "mentions": [{"type": "software", "software-type": "software", "software-name": {"rawForm": "Code", "normalizedForm": "Code", "offsetStart": 0, "offsetEnd": 4}, "version": {"rawForm": "3.10", "normalizedForm": "3.10", "offsetStart": 27, "offsetEnd": 31}, "context": "Code was written in Python 3.10. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9996540546417236}, "created": {"value": false, "score": 0.39176666736602783}, "shared": {"value": false, "score": 4.172325134277344e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9996540546417236}, "created": {"value": false, "score": 0.39176666736602783}, "shared": {"value": false, "score": 4.172325134277344e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "SQUALL", "normalizedForm": "SQUALL", "offsetStart": 0, "offsetEnd": 6}, "context": "SQUALL has a very high coverage of SPARQL queries and updates.", "mentionContextAttributes": {"used": {"value": false, "score": 0.00017523765563964844}, "created": {"value": false, "score": 8.64267349243164e-06}, "shared": {"value": false, "score": 5.364418029785156e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999871253967285}, "created": {"value": false, "score": 0.007201194763183594}, "shared": {"value": false, "score": 5.364418029785156e-07}}, "references": [{"label": "[6]", "normalizedForm": "[6]", "refKey": 6}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "SQUALL", "normalizedForm": "SQUALL", "offsetStart": 0, "offsetEnd": 6}, "context": "SQUALL is based on Montague grammars [13] that combine context-free grammars, first order logic, and \u03bb-calculus.", "mentionContextAttributes": {"used": {"value": false, "score": 0.0023356080055236816}, "created": {"value": false, "score": 0.00018167495727539062}, "shared": {"value": false, "score": 5.364418029785156e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999871253967285}, "created": {"value": false, "score": 0.007201194763183594}, "shared": {"value": false, "score": 5.364418029785156e-07}}, "references": [{"label": "[6]", "normalizedForm": "[6]", "refKey": 6}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "SQUALL", "normalizedForm": "SQUALL", "offsetStart": 0, "offsetEnd": 6}, "context": "SQUALL also supports a direct conversion into SPARQL such that we could verify that both queries return the same output modulo differences in language expressivity.", "mentionContextAttributes": {"used": {"value": false, "score": 0.00493931770324707}, "created": {"value": false, "score": 0.001438140869140625}, "shared": {"value": false, "score": 1.7881393432617188e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999871253967285}, "created": {"value": false, "score": 0.007201194763183594}, "shared": {"value": false, "score": 5.364418029785156e-07}}, "references": [{"label": "[6]", "normalizedForm": "[6]", "refKey": 6}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "SQUALL", "normalizedForm": "SQUALL", "offsetStart": 0, "offsetEnd": 6}, "context": "SQUALL is both more natural and more compact than Sparklis, i.e. it can express a query in less tokens, which could explain the performance gap.", "mentionContextAttributes": {"used": {"value": false, "score": 7.766485214233398e-05}, "created": {"value": false, "score": 2.664327621459961e-05}, "shared": {"value": false, "score": 1.7881393432617188e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999871253967285}, "created": {"value": false, "score": 0.007201194763183594}, "shared": {"value": false, "score": 5.364418029785156e-07}}, "references": [{"label": "[6]", "normalizedForm": "[6]", "refKey": 6}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Mintaka", "normalizedForm": "Mintaka", "offsetStart": 0, "offsetEnd": 7}, "context": "Mintaka has seven categories of questions including (1) count, (2) intersection, (3) superlative, (4) difference, (5) comparative, (6) generic and ( 7) ordinal. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.00013828277587890625}, "created": {"value": false, "score": 3.2007694244384766e-05}, "shared": {"value": false, "score": 2.980232238769531e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999870657920837}, "created": {"value": true, "score": 0.9942536950111389}, "shared": {"value": false, "score": 8.344650268554688e-07}}, "references": [{"label": "[25]", "normalizedForm": "[25]", "refKey": 25}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Mintaka", "normalizedForm": "Mintaka", "offsetStart": 0, "offsetEnd": 7}, "context": "Mintaka [25] is a natural and multilingual question answering dataset containing 20k manually written NL queries in English.", "mentionContextAttributes": {"used": {"value": false, "score": 0.0003529787063598633}, "created": {"value": false, "score": 2.1278858184814453e-05}, "shared": {"value": false, "score": 2.980232238769531e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999870657920837}, "created": {"value": true, "score": 0.9942536950111389}, "shared": {"value": false, "score": 8.344650268554688e-07}}, "references": [{"label": "[25]", "normalizedForm": "[25]", "refKey": 25, "offsetStart": 20228, "offsetEnd": 20232}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Sparklis", "normalizedForm": "Sparklis", "offsetStart": 0, "offsetEnd": 8}, "context": "Sparklis acts as an interactive SPARQL endpoint explorer that completely hides SPARQL behind a CNL. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.00018286705017089844}, "created": {"value": false, "score": 0.0024172067642211914}, "shared": {"value": false, "score": 1.7881393432617188e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999931454658508}, "created": {"value": false, "score": 0.007201194763183594}, "shared": {"value": false, "score": 4.172325134277344e-07}}, "references": [{"label": "[7]", "normalizedForm": "[7]", "refKey": 7}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Sparklis", "normalizedForm": "Sparklis", "offsetStart": 0, "offsetEnd": 8}, "context": "Sparklis [7] and SQUALL [6] are examples of SPARQL-oriented CNLs for querying and updating knowledge graphs.", "mentionContextAttributes": {"used": {"value": false, "score": 5.8591365814208984e-05}, "created": {"value": false, "score": 8.761882781982422e-06}, "shared": {"value": false, "score": 1.7881393432617188e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999931454658508}, "created": {"value": false, "score": 0.007201194763183594}, "shared": {"value": false, "score": 4.172325134277344e-07}}, "references": [{"label": "[7]", "normalizedForm": "[7]", "refKey": 7, "offsetStart": 13712, "offsetEnd": 13715}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Sparklis", "normalizedForm": "Sparklis", "offsetStart": 0, "offsetEnd": 8}, "context": "Sparklis' CNL is slightly less expressive and less natural than SQUALL, and it only covers SELECT queries.", "mentionContextAttributes": {"used": {"value": false, "score": 9.22083854675293e-05}, "created": {"value": false, "score": 4.470348358154297e-06}, "shared": {"value": false, "score": 2.980232238769531e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999931454658508}, "created": {"value": false, "score": 0.007201194763183594}, "shared": {"value": false, "score": 4.172325134277344e-07}}, "references": [{"label": "[7]", "normalizedForm": "[7]", "refKey": 7}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "SQUALL", "normalizedForm": "SQUALL", "offsetStart": 4, "offsetEnd": 10}, "context": "For SQUALL, the queries are manually written based on the Sparklis query.", "mentionContextAttributes": {"used": {"value": false, "score": 0.030053913593292236}, "created": {"value": false, "score": 0.00020456314086914062}, "shared": {"value": false, "score": 1.7881393432617188e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999871253967285}, "created": {"value": false, "score": 0.007201194763183594}, "shared": {"value": false, "score": 5.364418029785156e-07}}, "references": [{"label": "[6]", "normalizedForm": "[6]", "refKey": 6}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Mintaka", "normalizedForm": "Mintaka", "offsetStart": 4, "offsetEnd": 11}, "context": "The Mintaka dataset reports exactly one correct result for each query, which is why we did not include further execution metrics such as micro precision and recall.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9981927275657654}, "created": {"value": false, "score": 1.9490718841552734e-05}, "shared": {"value": false, "score": 1.7881393432617188e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999870657920837}, "created": {"value": true, "score": 0.9942536950111389}, "shared": {"value": false, "score": 8.344650268554688e-07}}, "references": [{"label": "[25]", "normalizedForm": "[25]", "refKey": 25}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Sparklis", "normalizedForm": "Sparklis", "offsetStart": 4, "offsetEnd": 12}, "context": "For Sparklis, we used the exact match score as an estimate for hits@1 execution accuracy.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9999931454658508}, "created": {"value": false, "score": 2.8014183044433594e-06}, "shared": {"value": false, "score": 2.980232238769531e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999931454658508}, "created": {"value": false, "score": 0.007201194763183594}, "shared": {"value": false, "score": 4.172325134277344e-07}}, "references": [{"label": "[7]", "normalizedForm": "[7]", "refKey": 7}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "SQUALL", "normalizedForm": "SQUALL", "offsetStart": 6, "offsetEnd": 12}, "context": "Using SQUALL leads to higher accuracies for all models except T5, which could be due to it being even closer to natural language than Sparklis and generally being more compact.", "mentionContextAttributes": {"used": {"value": false, "score": 0.0003268718719482422}, "created": {"value": false, "score": 2.3663043975830078e-05}, "shared": {"value": false, "score": 1.7881393432617188e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999871253967285}, "created": {"value": false, "score": 0.007201194763183594}, "shared": {"value": false, "score": 5.364418029785156e-07}}, "references": [{"label": "[6]", "normalizedForm": "[6]", "refKey": 6}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Mintaka", "normalizedForm": "Mintaka", "offsetStart": 8, "offsetEnd": 15}, "context": "We used Mintaka as a base for our studies since it is the only KGQA dataset we are aware of that contains a high number of queries and does not contain questions that are, at least in part, automatically generated (e.g.", "mentionContextAttributes": {"used": {"value": true, "score": 0.8914615511894226}, "created": {"value": false, "score": 9.769201278686523e-05}, "shared": {"value": false, "score": 2.980232238769531e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999870657920837}, "created": {"value": true, "score": 0.9942536950111389}, "shared": {"value": false, "score": 8.344650268554688e-07}}, "references": [{"label": "[25]", "normalizedForm": "[25]", "refKey": 25}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "SQUALL", "normalizedForm": "SQUALL", "offsetStart": 11, "offsetEnd": 17}, "context": "Generally, SQUALL is slightly more expressive than Sparklis, where some features are not directly supported: e.g. ", "mentionContextAttributes": {"used": {"value": false, "score": 4.416704177856445e-05}, "created": {"value": false, "score": 1.4483928680419922e-05}, "shared": {"value": false, "score": 4.172325134277344e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999871253967285}, "created": {"value": false, "score": 0.007201194763183594}, "shared": {"value": false, "score": 5.364418029785156e-07}}, "references": [{"label": "[6]", "normalizedForm": "[6]", "refKey": 6}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Mintaka", "normalizedForm": "Mintaka", "offsetStart": 13, "offsetEnd": 20}, "context": "The original Mintaka dataset also has another category named of yes/no questions which we did not consider in this work, since Sparklis does not support SPARQL ASK queries.", "mentionContextAttributes": {"used": {"value": false, "score": 0.012661099433898926}, "created": {"value": false, "score": 4.9054622650146484e-05}, "shared": {"value": false, "score": 2.980232238769531e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999870657920837}, "created": {"value": true, "score": 0.9942536950111389}, "shared": {"value": false, "score": 8.344650268554688e-07}}, "references": [{"label": "[25]", "normalizedForm": "[25]", "refKey": 25}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Mintaka", "normalizedForm": "Mintaka", "offsetStart": 15, "offsetEnd": 22}, "context": "(Note that the Mintaka test set is a superset of our Mintaka-CNL test set and the reported approaches use weak supervision rather than strong supervision.", "mentionContextAttributes": {"used": {"value": true, "score": 0.7985810041427612}, "created": {"value": false, "score": 1.055002212524414e-05}, "shared": {"value": false, "score": 1.7881393432617188e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999870657920837}, "created": {"value": true, "score": 0.9942536950111389}, "shared": {"value": false, "score": 8.344650268554688e-07}}, "references": [{"label": "[25]", "normalizedForm": "[25]", "refKey": 25}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "SQUALL", "normalizedForm": "SQUALL", "offsetStart": 16, "offsetEnd": 22}, "context": "We observe that SQUALL is generally more accurate across all data sizes.", "mentionContextAttributes": {"used": {"value": false, "score": 0.0005546808242797852}, "created": {"value": false, "score": 1.710653305053711e-05}, "shared": {"value": false, "score": 1.7881393432617188e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999871253967285}, "created": {"value": false, "score": 0.007201194763183594}, "shared": {"value": false, "score": 5.364418029785156e-07}}, "references": [{"label": "[6]", "normalizedForm": "[6]", "refKey": 6}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "SQUALL", "normalizedForm": "SQUALL", "offsetStart": 17, "offsetEnd": 23}, "context": "Sparklis [7] and SQUALL [6] are examples of SPARQL-oriented CNLs for querying and updating knowledge graphs.", "mentionContextAttributes": {"used": {"value": false, "score": 5.8591365814208984e-05}, "created": {"value": false, "score": 8.761882781982422e-06}, "shared": {"value": false, "score": 1.7881393432617188e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999871253967285}, "created": {"value": false, "score": 0.007201194763183594}, "shared": {"value": false, "score": 5.364418029785156e-07}}, "references": [{"label": "[6]", "normalizedForm": "[6]", "refKey": 6, "offsetStart": 13727, "offsetEnd": 13730}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "SQUALL", "normalizedForm": "SQUALL", "offsetStart": 20, "offsetEnd": 26}, "context": "For both SPARQL and SQUALL, we observed that accuracy is usually much higher than the exact match score.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9997870922088623}, "created": {"value": false, "score": 1.913309097290039e-05}, "shared": {"value": false, "score": 1.7881393432617188e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999871253967285}, "created": {"value": false, "score": 0.007201194763183594}, "shared": {"value": false, "score": 5.364418029785156e-07}}, "references": [{"label": "[6]", "normalizedForm": "[6]", "refKey": 6}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Mintaka", "normalizedForm": "Mintaka", "offsetStart": 27, "offsetEnd": 34}, "context": "Unlike most KGQA datasets, Mintaka contains very natural questions, as they were collected manually in a high-quality data collection process.", "mentionContextAttributes": {"used": {"value": false, "score": 0.33943015336990356}, "created": {"value": false, "score": 5.549192428588867e-05}, "shared": {"value": false, "score": 1.7881393432617188e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999870657920837}, "created": {"value": true, "score": 0.9942536950111389}, "shared": {"value": false, "score": 8.344650268554688e-07}}, "references": [{"label": "[25]", "normalizedForm": "[25]", "refKey": 25}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Sparklis", "normalizedForm": "Sparklis", "offsetStart": 27, "offsetEnd": 35}, "context": "where $language \u2208 {SPARQL, Sparklis, SQUALL} is the target language.", "mentionContextAttributes": {"used": {"value": false, "score": 0.04054301977157593}, "created": {"value": false, "score": 1.0907649993896484e-05}, "shared": {"value": false, "score": 4.172325134277344e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999931454658508}, "created": {"value": false, "score": 0.007201194763183594}, "shared": {"value": false, "score": 4.172325134277344e-07}}, "references": [{"label": "[7]", "normalizedForm": "[7]", "refKey": 7}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "ReFinED", "normalizedForm": "ReFinED", "offsetStart": 31, "offsetEnd": 42}, "context": "We employ the state-of-the-art ReFinED [1] entity linker to enrich the LLM prompt with entity information. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9986280798912048}, "created": {"value": false, "score": 0.004945039749145508}, "shared": {"value": false, "score": 1.7881393432617188e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9986280798912048}, "created": {"value": false, "score": 0.021582722663879395}, "shared": {"value": false, "score": 2.980232238769531e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Mintaka", "normalizedForm": "Mintaka", "offsetStart": 34, "offsetEnd": 41}, "context": "We report our results in Table 2. Mintaka is generally a challenging dataset with hits@1 scores reported [25] to be between 0.12 and 0.2 on its test set for 3 different approaches.", "mentionContextAttributes": {"used": {"value": true, "score": 0.919899582862854}, "created": {"value": false, "score": 1.7821788787841797e-05}, "shared": {"value": false, "score": 1.7881393432617188e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999870657920837}, "created": {"value": true, "score": 0.9942536950111389}, "shared": {"value": false, "score": 8.344650268554688e-07}}, "references": [{"label": "[25]", "normalizedForm": "[25]", "refKey": 25}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Mintaka", "normalizedForm": "Mintaka", "offsetStart": 35, "offsetEnd": 42}, "context": "Table 2. Evaluation Results on the Mintaka-CNL dataset for different combinations of language models and target languages.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9999658465385437}, "created": {"value": false, "score": 2.8908252716064453e-05}, "shared": {"value": false, "score": 2.980232238769531e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999870657920837}, "created": {"value": true, "score": 0.9942536950111389}, "shared": {"value": false, "score": 8.344650268554688e-07}}, "references": [{"label": "[25]", "normalizedForm": "[25]", "refKey": 25}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Sparklis", "normalizedForm": "Sparklis", "offsetStart": 36, "offsetEnd": 44}, "context": "For this, we first turned them into Sparklis queries using its query construction interface 6 .", "mentionContextAttributes": {"used": {"value": true, "score": 0.9999886155128479}, "created": {"value": false, "score": 0.00033271312713623047}, "shared": {"value": false, "score": 1.7881393432617188e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999931454658508}, "created": {"value": false, "score": 0.007201194763183594}, "shared": {"value": false, "score": 4.172325134277344e-07}}, "references": [{"label": "[7]", "normalizedForm": "[7]", "refKey": 7}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Sparklis", "normalizedForm": "Sparklis", "offsetStart": 38, "offsetEnd": 46}, "context": "We showcase the corresponding SPARQL, Sparklis and SQUALL versions of this question.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9870645999908447}, "created": {"value": false, "score": 0.0008565187454223633}, "shared": {"value": false, "score": 1.7881393432617188e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999931454658508}, "created": {"value": false, "score": 0.007201194763183594}, "shared": {"value": false, "score": 4.172325134277344e-07}}, "references": [{"label": "[7]", "normalizedForm": "[7]", "refKey": 7}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Mintaka", "normalizedForm": "Mintaka", "offsetStart": 39, "offsetEnd": 46}, "context": "After sampling relevant questions from Mintaka, we constructed their equivalent CNL and SPARQL queries.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9999760985374451}, "created": {"value": false, "score": 0.3395950198173523}, "shared": {"value": false, "score": 1.7881393432617188e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999870657920837}, "created": {"value": true, "score": 0.9942536950111389}, "shared": {"value": false, "score": 8.344650268554688e-07}}, "references": [{"label": "[25]", "normalizedForm": "[25]", "refKey": 25}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "ReFinED", "normalizedForm": "ReFinED", "offsetStart": 42, "offsetEnd": 53}, "context": "In our experiments, we decided to use the ReFinED [1] entity linker, since it achieves state-of-the-art results on Wikidata and is runtime efficient. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.7443948984146118}, "created": {"value": false, "score": 0.021582722663879395}, "shared": {"value": false, "score": 2.980232238769531e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9986280798912048}, "created": {"value": false, "score": 0.021582722663879395}, "shared": {"value": false, "score": 2.980232238769531e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "SQUALL", "normalizedForm": "SQUALL", "offsetStart": 47, "offsetEnd": 53}, "context": "Given the question and the entities generate a SQUALL query!", "mentionContextAttributes": {"used": {"value": true, "score": 0.9010825753211975}, "created": {"value": false, "score": 3.4868717193603516e-05}, "shared": {"value": false, "score": 2.980232238769531e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999871253967285}, "created": {"value": false, "score": 0.007201194763183594}, "shared": {"value": false, "score": 5.364418029785156e-07}}, "references": [{"label": "[6]", "normalizedForm": "[6]", "refKey": 6}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "SQUALL", "normalizedForm": "SQUALL", "offsetStart": 49, "offsetEnd": 55}, "context": "For the top-performing model, the accuracy using SQUALL is roughly twice as high compared to using SPARQL, so the choice of target language has a strong influence on the performance of the KGQA system.", "mentionContextAttributes": {"used": {"value": false, "score": 0.011475324630737305}, "created": {"value": false, "score": 1.329183578491211e-05}, "shared": {"value": false, "score": 1.7881393432617188e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999871253967285}, "created": {"value": false, "score": 0.007201194763183594}, "shared": {"value": false, "score": 5.364418029785156e-07}}, "references": [{"label": "[6]", "normalizedForm": "[6]", "refKey": 6}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Mintaka", "normalizedForm": "Mintaka", "offsetStart": 49, "offsetEnd": 56}, "context": "To create our dataset, we sampled questions from Mintaka using the following criteria: a) diversity in question themes, b) diversity in question complexity, c) preference of questions with more than one entity, and d) having at least three examples of the same type of question in each category.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9999528527259827}, "created": {"value": false, "score": 0.0017082691192626953}, "shared": {"value": false, "score": 4.172325134277344e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999870657920837}, "created": {"value": true, "score": 0.9942536950111389}, "shared": {"value": false, "score": 8.344650268554688e-07}}, "references": [{"label": "[25]", "normalizedForm": "[25]", "refKey": 25}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Sparklis", "normalizedForm": "Sparklis", "offsetStart": 49, "offsetEnd": 57}, "context": "rather than selecting the \"5th US president\" the Sparklis query returns \"US presidents\" in increasing \"point in time\").", "mentionContextAttributes": {"used": {"value": false, "score": 0.40862900018692017}, "created": {"value": false, "score": 1.4841556549072266e-05}, "shared": {"value": false, "score": 2.980232238769531e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999931454658508}, "created": {"value": false, "score": 0.007201194763183594}, "shared": {"value": false, "score": 4.172325134277344e-07}}, "references": [{"label": "[7]", "normalizedForm": "[7]", "refKey": 7}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Linux", "normalizedForm": "Linux", "offsetStart": 50, "offsetEnd": 55}, "context": "The fine-tuning was done on 2 Nvidia A100 GPUs on Linux and on Ope-nAI servers for GPT-3. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9998660087585449}, "created": {"value": false, "score": 3.2961368560791016e-05}, "shared": {"value": false, "score": 9.47713851928711e-06}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9998660087585449}, "created": {"value": false, "score": 3.2961368560791016e-05}, "shared": {"value": false, "score": 9.47713851928711e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "SQUALL", "normalizedForm": "SQUALL", "offsetStart": 50, "offsetEnd": 56}, "context": "We also observed that the LLM sometimes generates SQUALL queries that are substantially different from the ground truth, but are semantically equally good and often lead to the same execution results.", "mentionContextAttributes": {"used": {"value": true, "score": 0.5098643898963928}, "created": {"value": false, "score": 1.1980533599853516e-05}, "shared": {"value": false, "score": 2.980232238769531e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999871253967285}, "created": {"value": false, "score": 0.007201194763183594}, "shared": {"value": false, "score": 5.364418029785156e-07}}, "references": [{"label": "[6]", "normalizedForm": "[6]", "refKey": 6}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Sparklis", "normalizedForm": "Sparklis", "offsetStart": 50, "offsetEnd": 58}, "context": "SQUALL is both more natural and more compact than Sparklis, i.e. it can express a query in less tokens, which could explain the performance gap.", "mentionContextAttributes": {"used": {"value": false, "score": 7.766485214233398e-05}, "created": {"value": false, "score": 2.664327621459961e-05}, "shared": {"value": false, "score": 1.7881393432617188e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999931454658508}, "created": {"value": false, "score": 0.007201194763183594}, "shared": {"value": false, "score": 4.172325134277344e-07}}, "references": [{"label": "[7]", "normalizedForm": "[7]", "refKey": 7}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "SQUALL", "normalizedForm": "SQUALL", "offsetStart": 51, "offsetEnd": 57}, "context": "We showcase the corresponding SPARQL, Sparklis and SQUALL versions of this question.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9870645999908447}, "created": {"value": false, "score": 0.0008565187454223633}, "shared": {"value": false, "score": 1.7881393432617188e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999871253967285}, "created": {"value": false, "score": 0.007201194763183594}, "shared": {"value": false, "score": 5.364418029785156e-07}}, "references": [{"label": "[6]", "normalizedForm": "[6]", "refKey": 6}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Sparklis", "normalizedForm": "Sparklis", "offsetStart": 51, "offsetEnd": 59}, "context": "Generally, SQUALL is slightly more expressive than Sparklis, where some features are not directly supported: e.g. ", "mentionContextAttributes": {"used": {"value": false, "score": 4.416704177856445e-05}, "created": {"value": false, "score": 1.4483928680419922e-05}, "shared": {"value": false, "score": 4.172325134277344e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999931454658508}, "created": {"value": false, "score": 0.007201194763183594}, "shared": {"value": false, "score": 4.172325134277344e-07}}, "references": [{"label": "[7]", "normalizedForm": "[7]", "refKey": 7}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Mintaka", "normalizedForm": "Mintaka", "offsetStart": 53, "offsetEnd": 60}, "context": "(Note that the Mintaka test set is a superset of our Mintaka-CNL test set and the reported approaches use weak supervision rather than strong supervision.", "mentionContextAttributes": {"used": {"value": true, "score": 0.7985810041427612}, "created": {"value": false, "score": 1.055002212524414e-05}, "shared": {"value": false, "score": 1.7881393432617188e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999870657920837}, "created": {"value": true, "score": 0.9942536950111389}, "shared": {"value": false, "score": 8.344650268554688e-07}}, "references": [{"label": "[25]", "normalizedForm": "[25]", "refKey": 25}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Sparklis", "normalizedForm": "Sparklis", "offsetStart": 55, "offsetEnd": 63}, "context": "Increasing the number of fine-tuning examples leads to Sparklis performing slightly better than SPARQL for GPT-3 Davinci with a constant margin.", "mentionContextAttributes": {"used": {"value": false, "score": 0.008262574672698975}, "created": {"value": false, "score": 7.808208465576172e-06}, "shared": {"value": false, "score": 1.7881393432617188e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999931454658508}, "created": {"value": false, "score": 0.007201194763183594}, "shared": {"value": false, "score": 4.172325134277344e-07}}, "references": [{"label": "[7]", "normalizedForm": "[7]", "refKey": 7}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Sparklis", "normalizedForm": "Sparklis", "offsetStart": 58, "offsetEnd": 66}, "context": "For SQUALL, the queries are manually written based on the Sparklis query.", "mentionContextAttributes": {"used": {"value": false, "score": 0.030053913593292236}, "created": {"value": false, "score": 0.00020456314086914062}, "shared": {"value": false, "score": 1.7881393432617188e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999931454658508}, "created": {"value": false, "score": 0.007201194763183594}, "shared": {"value": false, "score": 4.172325134277344e-07}}, "references": [{"label": "[7]", "normalizedForm": "[7]", "refKey": 7}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Sparklis", "normalizedForm": "Sparklis", "offsetStart": 61, "offsetEnd": 69}, "context": "The hits@1 results for the two controlled natural languages, Sparklis and SQUALL, are both higher than the SPARQL baseline.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9995822310447693}, "created": {"value": false, "score": 2.568960189819336e-05}, "shared": {"value": false, "score": 1.7881393432617188e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999931454658508}, "created": {"value": false, "score": 0.007201194763183594}, "shared": {"value": false, "score": 4.172325134277344e-07}}, "references": [{"label": "[7]", "normalizedForm": "[7]", "refKey": 7}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "SQUALL", "normalizedForm": "SQUALL", "offsetStart": 62, "offsetEnd": 68}, "context": "For the string similarity proxy metrics (BLEU, METEOR, ROUGE) SQUALL and Sparklis are similar, and consistently higher than SPARQL.", "mentionContextAttributes": {"used": {"value": false, "score": 0.18348026275634766}, "created": {"value": false, "score": 2.4139881134033203e-05}, "shared": {"value": false, "score": 1.7881393432617188e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999871253967285}, "created": {"value": false, "score": 0.007201194763183594}, "shared": {"value": false, "score": 5.364418029785156e-07}}, "references": [{"label": "[6]", "normalizedForm": "[6]", "refKey": 6}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Sparklis", "normalizedForm": "Sparklis", "offsetStart": 63, "offsetEnd": 71}, "context": "We provide three variants of those logical forms: SPARQL [10], Sparklis [7] and SQUALL [6].", "mentionContextAttributes": {"used": {"value": false, "score": 0.0003896355628967285}, "created": {"value": false, "score": 6.020069122314453e-06}, "shared": {"value": false, "score": 1.7881393432617188e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999931454658508}, "created": {"value": false, "score": 0.007201194763183594}, "shared": {"value": false, "score": 4.172325134277344e-07}}, "references": [{"label": "[7]", "normalizedForm": "[7]", "refKey": 7, "offsetStart": 5379, "offsetEnd": 5382}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "SQUALL", "normalizedForm": "SQUALL", "offsetStart": 64, "offsetEnd": 70}, "context": "Sparklis' CNL is slightly less expressive and less natural than SQUALL, and it only covers SELECT queries.", "mentionContextAttributes": {"used": {"value": false, "score": 9.22083854675293e-05}, "created": {"value": false, "score": 4.470348358154297e-06}, "shared": {"value": false, "score": 2.980232238769531e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999871253967285}, "created": {"value": false, "score": 0.007201194763183594}, "shared": {"value": false, "score": 5.364418029785156e-07}}, "references": [{"label": "[6]", "normalizedForm": "[6]", "refKey": 6}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Mintaka", "normalizedForm": "Mintaka", "offsetStart": 66, "offsetEnd": 73}, "context": "To verify our research hypothesis, we conducted a study using the Mintaka dataset [25] based on Wikidata.", "mentionContextAttributes": {"used": {"value": true, "score": 0.999979555606842}, "created": {"value": false, "score": 0.00018906593322753906}, "shared": {"value": false, "score": 1.7881393432617188e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999870657920837}, "created": {"value": true, "score": 0.9942536950111389}, "shared": {"value": false, "score": 8.344650268554688e-07}}, "references": [{"label": "[25]", "normalizedForm": "[25]", "refKey": 25, "offsetStart": 4623, "offsetEnd": 4627}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Sparklis", "normalizedForm": "Sparklis", "offsetStart": 67, "offsetEnd": 75}, "context": "For lower amounts of fine-tuning data SPARQL is more accurate than Sparklis for GPT-3 Davinci -probably because it was seen more in pre-training.", "mentionContextAttributes": {"used": {"value": false, "score": 0.004497349262237549}, "created": {"value": false, "score": 2.205371856689453e-06}, "shared": {"value": false, "score": 1.7881393432617188e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999931454658508}, "created": {"value": false, "score": 0.007201194763183594}, "shared": {"value": false, "score": 4.172325134277344e-07}}, "references": [{"label": "[7]", "normalizedForm": "[7]", "refKey": 7}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Sparklis", "normalizedForm": "Sparklis", "offsetStart": 69, "offsetEnd": 77}, "context": "Setting Language BLEU METEOR ROUGE Exact Match Hits@1 \u2206 Hits@1 GPT-  Sparklis is less expressive than SPARQL and SQUALL.", "mentionContextAttributes": {"used": {"value": false, "score": 0.00010007619857788086}, "created": {"value": false, "score": 8.761882781982422e-06}, "shared": {"value": false, "score": 2.980232238769531e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999931454658508}, "created": {"value": false, "score": 0.007201194763183594}, "shared": {"value": false, "score": 4.172325134277344e-07}}, "references": [{"label": "[7]", "normalizedForm": "[7]", "refKey": 7}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Mintaka", "normalizedForm": "Mintaka", "offsetStart": 72, "offsetEnd": 79}, "context": "Figure 1 shows an example natural language query that is taken from the Mintaka dataset.", "mentionContextAttributes": {"used": {"value": true, "score": 0.7555277943611145}, "created": {"value": false, "score": 3.7610530853271484e-05}, "shared": {"value": false, "score": 2.980232238769531e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999870657920837}, "created": {"value": true, "score": 0.9942536950111389}, "shared": {"value": false, "score": 8.344650268554688e-07}}, "references": [{"label": "[25]", "normalizedForm": "[25]", "refKey": 25}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "SQUALL", "normalizedForm": "SQUALL", "offsetStart": 73, "offsetEnd": 79}, "context": "Our study with two different CNL shows that the more compact and natural SQUALL language leads to much better results compared to a more regular, verbose Sparklis language.", "mentionContextAttributes": {"used": {"value": false, "score": 0.2011358141899109}, "created": {"value": false, "score": 0.00034117698669433594}, "shared": {"value": false, "score": 2.980232238769531e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999871253967285}, "created": {"value": false, "score": 0.007201194763183594}, "shared": {"value": false, "score": 5.364418029785156e-07}}, "references": [{"label": "[6]", "normalizedForm": "[6]", "refKey": 6}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "SPARQL 1", "normalizedForm": "SPARQL 1", "offsetStart": 73, "offsetEnd": 81}, "context": "To access the information in such knowledge graphs, query languages like SPARQL 1 are used. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.23604637384414673}, "created": {"value": false, "score": 6.085634231567383e-05}, "shared": {"value": false, "score": 2.980232238769531e-07}}, "documentContextAttributes": {"used": {"value": false, "score": 0.23604637384414673}, "created": {"value": false, "score": 6.085634231567383e-05}, "shared": {"value": false, "score": 2.980232238769531e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Sparklis", "normalizedForm": "Sparklis", "offsetStart": 73, "offsetEnd": 81}, "context": "For the string similarity proxy metrics (BLEU, METEOR, ROUGE) SQUALL and Sparklis are similar, and consistently higher than SPARQL.", "mentionContextAttributes": {"used": {"value": false, "score": 0.18348026275634766}, "created": {"value": false, "score": 2.4139881134033203e-05}, "shared": {"value": false, "score": 1.7881393432617188e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999931454658508}, "created": {"value": false, "score": 0.007201194763183594}, "shared": {"value": false, "score": 4.172325134277344e-07}}, "references": [{"label": "[7]", "normalizedForm": "[7]", "refKey": 7}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "SQUALL", "normalizedForm": "SQUALL", "offsetStart": 74, "offsetEnd": 80}, "context": "The hits@1 results for the two controlled natural languages, Sparklis and SQUALL, are both higher than the SPARQL baseline.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9995822310447693}, "created": {"value": false, "score": 2.568960189819336e-05}, "shared": {"value": false, "score": 1.7881393432617188e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999871253967285}, "created": {"value": false, "score": 0.007201194763183594}, "shared": {"value": false, "score": 5.364418029785156e-07}}, "references": [{"label": "[6]", "normalizedForm": "[6]", "refKey": 6}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Sparklis", "normalizedForm": "Sparklis", "offsetStart": 75, "offsetEnd": 83}, "context": "Three researchers and two student assistants were involved in creating the Sparklis queries.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9995481371879578}, "created": {"value": false, "score": 0.005177497863769531}, "shared": {"value": false, "score": 1.7881393432617188e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999931454658508}, "created": {"value": false, "score": 0.007201194763183594}, "shared": {"value": false, "score": 4.172325134277344e-07}}, "references": [{"label": "[7]", "normalizedForm": "[7]", "refKey": 7}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "SQUALL", "normalizedForm": "SQUALL", "offsetStart": 76, "offsetEnd": 82}, "context": "We could also observe that the LLM likely obtains a better understanding of SQUALL, e.g. it can generate correct syntactic variations of the ground truth and very rarely generates meaningless or duplicated patterns.", "mentionContextAttributes": {"used": {"value": false, "score": 0.0076555609703063965}, "created": {"value": false, "score": 0.00020742416381835938}, "shared": {"value": false, "score": 4.172325134277344e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999871253967285}, "created": {"value": false, "score": 0.007201194763183594}, "shared": {"value": false, "score": 5.364418029785156e-07}}, "references": [{"label": "[6]", "normalizedForm": "[6]", "refKey": 6}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "SQUALL", "normalizedForm": "SQUALL", "offsetStart": 80, "offsetEnd": 86}, "context": "We provide three variants of those logical forms: SPARQL [10], Sparklis [7] and SQUALL [6].", "mentionContextAttributes": {"used": {"value": false, "score": 0.0003896355628967285}, "created": {"value": false, "score": 6.020069122314453e-06}, "shared": {"value": false, "score": 1.7881393432617188e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999871253967285}, "created": {"value": false, "score": 0.007201194763183594}, "shared": {"value": false, "score": 5.364418029785156e-07}}, "references": [{"label": "[6]", "normalizedForm": "[6]", "refKey": 6, "offsetStart": 5394, "offsetEnd": 5397}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Mintaka", "normalizedForm": "Mintaka", "offsetStart": 82, "offsetEnd": 89}, "context": "For the fourth research question, we analyzed the different types of questions in Mintaka in terms of absolute performance and performance improvement when using SQUALL instead of SPARQL.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9999870657920837}, "created": {"value": false, "score": 5.543231964111328e-06}, "shared": {"value": false, "score": 1.7881393432617188e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999870657920837}, "created": {"value": true, "score": 0.9942536950111389}, "shared": {"value": false, "score": 8.344650268554688e-07}}, "references": [{"label": "[25]", "normalizedForm": "[25]", "refKey": 25}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Sparklis", "normalizedForm": "Sparklis", "offsetStart": 91, "offsetEnd": 99}, "context": "In counterpart, queries can be built incrementally through user interaction: at each step, Sparklis suggests some query refinements, among which the user selects the refinement that fits her information needs. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.00010842084884643555}, "created": {"value": false, "score": 0.00014209747314453125}, "shared": {"value": false, "score": 2.980232238769531e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999931454658508}, "created": {"value": false, "score": 0.007201194763183594}, "shared": {"value": false, "score": 4.172325134277344e-07}}, "references": [{"label": "[7]", "normalizedForm": "[7]", "refKey": 7}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Mintaka", "normalizedForm": "Mintaka", "offsetStart": 92, "offsetEnd": 99}, "context": "To be able to investigate our research hypothesis, we labelled a subset of 550 questions of Mintaka with their corresponding logical forms.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9999783635139465}, "created": {"value": true, "score": 0.8772538900375366}, "shared": {"value": false, "score": 2.980232238769531e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999870657920837}, "created": {"value": true, "score": 0.9942536950111389}, "shared": {"value": false, "score": 8.344650268554688e-07}}, "references": [{"label": "[25]", "normalizedForm": "[25]", "refKey": 25}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "SQUALL", "normalizedForm": "SQUALL", "offsetStart": 102, "offsetEnd": 108}, "context": "Syntactic errors should rarely happen with LLMs, but we observed several of them -most likely because SQUALL permits a variable structure allowing multiple expressions that map to the same query.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9862257242202759}, "created": {"value": false, "score": 1.9252300262451172e-05}, "shared": {"value": false, "score": 2.980232238769531e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999871253967285}, "created": {"value": false, "score": 0.007201194763183594}, "shared": {"value": false, "score": 5.364418029785156e-07}}, "references": [{"label": "[6]", "normalizedForm": "[6]", "refKey": 6}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "ReFinED", "normalizedForm": "ReFinED", "offsetStart": 102, "offsetEnd": 109}, "context": "For a given input utterance, we retrieve all entities above a threshold probability value returned by ReFinED. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9984978437423706}, "created": {"value": false, "score": 0.00025081634521484375}, "shared": {"value": false, "score": 1.7881393432617188e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9986280798912048}, "created": {"value": false, "score": 0.021582722663879395}, "shared": {"value": false, "score": 2.980232238769531e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Mintaka", "normalizedForm": "Mintaka", "offsetStart": 106, "offsetEnd": 113}, "context": "Using hyperrelations via so called qualifiers in Wikidata is indeed required for several questions in the Mintaka dataset which we evaluate on.", "mentionContextAttributes": {"used": {"value": true, "score": 0.7213630080223083}, "created": {"value": false, "score": 4.45246696472168e-05}, "shared": {"value": false, "score": 1.7881393432617188e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999870657920837}, "created": {"value": true, "score": 0.9942536950111389}, "shared": {"value": false, "score": 8.344650268554688e-07}}, "references": [{"label": "[25]", "normalizedForm": "[25]", "refKey": 25}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "SQUALL", "normalizedForm": "SQUALL", "offsetStart": 113, "offsetEnd": 119}, "context": "Setting Language BLEU METEOR ROUGE Exact Match Hits@1 \u2206 Hits@1 GPT-  Sparklis is less expressive than SPARQL and SQUALL.", "mentionContextAttributes": {"used": {"value": false, "score": 0.00010007619857788086}, "created": {"value": false, "score": 8.761882781982422e-06}, "shared": {"value": false, "score": 2.980232238769531e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999871253967285}, "created": {"value": false, "score": 0.007201194763183594}, "shared": {"value": false, "score": 5.364418029785156e-07}}, "references": [{"label": "[6]", "normalizedForm": "[6]", "refKey": 6}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Mintaka", "normalizedForm": "Mintaka", "offsetStart": 113, "offsetEnd": 120}, "context": "However, due to the significant effort that would be required to collect (natural language, logical form) pairs, Mintaka only contains (natural language, response) pairs, i.e. a so called weak supervision setting.", "mentionContextAttributes": {"used": {"value": false, "score": 0.0009188652038574219}, "created": {"value": false, "score": 4.583597183227539e-05}, "shared": {"value": false, "score": 2.980232238769531e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999870657920837}, "created": {"value": true, "score": 0.9942536950111389}, "shared": {"value": false, "score": 8.344650268554688e-07}}, "references": [{"label": "[25]", "normalizedForm": "[25]", "refKey": 25}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "SQUALL", "normalizedForm": "SQUALL", "offsetStart": 116, "offsetEnd": 122}, "context": "Using the most powerful LLM in our test, the semantic parsing accuracy could be doubled by switching from SPARQL to SQUALL.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9999825358390808}, "created": {"value": false, "score": 3.88026237487793e-05}, "shared": {"value": false, "score": 1.7881393432617188e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999871253967285}, "created": {"value": false, "score": 0.007201194763183594}, "shared": {"value": false, "score": 5.364418029785156e-07}}, "references": [{"label": "[6]", "normalizedForm": "[6]", "refKey": 6}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "SQUALL", "normalizedForm": "SQUALL", "offsetStart": 123, "offsetEnd": 129}, "context": "For the third research question, we computed the relative improvement for each language model in the execution accuracy of SQUALL compared to SPARQL in the last column of Table 2.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9999832510948181}, "created": {"value": false, "score": 4.708766937255859e-06}, "shared": {"value": false, "score": 1.7881393432617188e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999871253967285}, "created": {"value": false, "score": 0.007201194763183594}, "shared": {"value": false, "score": 5.364418029785156e-07}}, "references": [{"label": "[6]", "normalizedForm": "[6]", "refKey": 6}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Sparklis", "normalizedForm": "Sparklis", "offsetStart": 127, "offsetEnd": 135}, "context": "The original Mintaka dataset also has another category named of yes/no questions which we did not consider in this work, since Sparklis does not support SPARQL ASK queries.", "mentionContextAttributes": {"used": {"value": false, "score": 0.012661099433898926}, "created": {"value": false, "score": 4.9054622650146484e-05}, "shared": {"value": false, "score": 2.980232238769531e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999931454658508}, "created": {"value": false, "score": 0.007201194763183594}, "shared": {"value": false, "score": 4.172325134277344e-07}}, "references": [{"label": "[7]", "normalizedForm": "[7]", "refKey": 7}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "SQUALL", "normalizedForm": "SQUALL", "offsetStart": 134, "offsetEnd": 140}, "context": "The principles of their intermediate representation language are similar to CNL but with less emphasis on naturalness compared to the SQUALL language which we are investigating.", "mentionContextAttributes": {"used": {"value": false, "score": 0.00031638145446777344}, "created": {"value": false, "score": 0.004480719566345215}, "shared": {"value": false, "score": 2.980232238769531e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999871253967285}, "created": {"value": false, "score": 0.007201194763183594}, "shared": {"value": false, "score": 5.364418029785156e-07}}, "references": [{"label": "[6]", "normalizedForm": "[6]", "refKey": 6}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Sparklis", "normalizedForm": "Sparklis", "offsetStart": 134, "offsetEnd": 142}, "context": "Using SQUALL leads to higher accuracies for all models except T5, which could be due to it being even closer to natural language than Sparklis and generally being more compact.", "mentionContextAttributes": {"used": {"value": false, "score": 0.0003268718719482422}, "created": {"value": false, "score": 2.3663043975830078e-05}, "shared": {"value": false, "score": 1.7881393432617188e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999931454658508}, "created": {"value": false, "score": 0.007201194763183594}, "shared": {"value": false, "score": 4.172325134277344e-07}}, "references": [{"label": "[7]", "normalizedForm": "[7]", "refKey": 7}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "SQUALL", "normalizedForm": "SQUALL", "offsetStart": 147, "offsetEnd": 153}, "context": "From our data, we cannot generally conclude that using a controlled natural language always reduces training data requirements -it is the case for SQUALL but to a lesser degree for Sparklis.", "mentionContextAttributes": {"used": {"value": false, "score": 0.0004875063896179199}, "created": {"value": false, "score": 0.007201194763183594}, "shared": {"value": false, "score": 2.980232238769531e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999871253967285}, "created": {"value": false, "score": 0.007201194763183594}, "shared": {"value": false, "score": 5.364418029785156e-07}}, "references": [{"label": "[6]", "normalizedForm": "[6]", "refKey": 6}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Sparklis", "normalizedForm": "Sparklis", "offsetStart": 154, "offsetEnd": 162}, "context": "Our study with two different CNL shows that the more compact and natural SQUALL language leads to much better results compared to a more regular, verbose Sparklis language.", "mentionContextAttributes": {"used": {"value": false, "score": 0.2011358141899109}, "created": {"value": false, "score": 0.00034117698669433594}, "shared": {"value": false, "score": 2.980232238769531e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999931454658508}, "created": {"value": false, "score": 0.007201194763183594}, "shared": {"value": false, "score": 4.172325134277344e-07}}, "references": [{"label": "[7]", "normalizedForm": "[7]", "refKey": 7}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "SQUALL", "normalizedForm": "SQUALL", "offsetStart": 162, "offsetEnd": 168}, "context": "For the fourth research question, we analyzed the different types of questions in Mintaka in terms of absolute performance and performance improvement when using SQUALL instead of SPARQL.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9999871253967285}, "created": {"value": false, "score": 5.4836273193359375e-06}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999871253967285}, "created": {"value": false, "score": 0.007201194763183594}, "shared": {"value": false, "score": 5.364418029785156e-07}}, "references": [{"label": "[6]", "normalizedForm": "[6]", "refKey": 6}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Mintaka", "normalizedForm": "Mintaka", "offsetStart": 162, "offsetEnd": 169}, "context": "The last 3 categories are queries that are not judged to be correct, because of a) Wikidata opting for a different KG structure than generated by the LLM, b) the Mintaka benchmark result not being correct or c) Wikidata not returning a result due to (never or no longer) having the data required for answering the query.", "mentionContextAttributes": {"used": {"value": false, "score": 0.11097043752670288}, "created": {"value": false, "score": 2.86102294921875e-06}, "shared": {"value": false, "score": 8.344650268554688e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999870657920837}, "created": {"value": true, "score": 0.9942536950111389}, "shared": {"value": false, "score": 8.344650268554688e-07}}, "references": [{"label": "[25]", "normalizedForm": "[25]", "refKey": 25}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "SQUALL", "normalizedForm": "SQUALL", "offsetStart": 171, "offsetEnd": 177}, "context": "An interesting observation that we made is that the LLMs more frequently tend to generate repeated unnecessary patterns in SPARQL queries whereas this does not happen for SQUALL.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9983797073364258}, "created": {"value": false, "score": 0.0002663731575012207}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999871253967285}, "created": {"value": false, "score": 0.007201194763183594}, "shared": {"value": false, "score": 5.364418029785156e-07}}, "references": [{"label": "[6]", "normalizedForm": "[6]", "refKey": 6}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Mintaka", "normalizedForm": "Mintaka", "offsetStart": 173, "offsetEnd": 180}, "context": "Additionally, we incorporate hits@1 as execution metric, which measures whether the first retrieved answer when actually querying the underlying knowledge graph matches the Mintaka gold standard.", "mentionContextAttributes": {"used": {"value": false, "score": 0.0018021464347839355}, "created": {"value": true, "score": 0.9942536950111389}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999870657920837}, "created": {"value": true, "score": 0.9942536950111389}, "shared": {"value": false, "score": 8.344650268554688e-07}}, "references": [{"label": "[25]", "normalizedForm": "[25]", "refKey": 25}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Sparklis", "normalizedForm": "Sparklis", "offsetStart": 181, "offsetEnd": 189}, "context": "From our data, we cannot generally conclude that using a controlled natural language always reduces training data requirements -it is the case for SQUALL but to a lesser degree for Sparklis.", "mentionContextAttributes": {"used": {"value": false, "score": 0.00048744678497314453}, "created": {"value": false, "score": 0.007201194763183594}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999931454658508}, "created": {"value": false, "score": 0.007201194763183594}, "shared": {"value": false, "score": 4.172325134277344e-07}}, "references": [{"label": "[7]", "normalizedForm": "[7]", "refKey": 7}]}], "references": [{"refKey": 6, "tei": "<biblStruct xml:id=\"b6\">\n\t<analytic>\n\t\t<title level=\"a\" type=\"main\">SQUALL: The expressiveness of SPARQL 1.1 made available as a controlled natural language</title>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">S\u00e9bastien</forename><surname>Ferr\u00e9</surname></persName>\n\t\t</author>\n\t\t<idno type=\"DOI\">10.1016/j.datak.2014.07.010</idno>\n\t</analytic>\n\t<monogr>\n\t\t<title level=\"j\">Data &amp; Knowledge Engineering</title>\n\t\t<title level=\"j\" type=\"abbrev\">Data &amp; Knowledge Engineering</title>\n\t\t<idno type=\"ISSN\">0169-023X</idno>\n\t\t<imprint>\n\t\t\t<biblScope unit=\"volume\">94</biblScope>\n\t\t\t<biblScope unit=\"page\" from=\"163\" to=\"188\" />\n\t\t\t<date type=\"published\" when=\"2014-11\">2014</date>\n\t\t\t<publisher>Elsevier BV</publisher>\n\t\t</imprint>\n\t</monogr>\n</biblStruct>\n"}, {"refKey": 25, "tei": "<biblStruct xml:id=\"b25\">\n\t<analytic>\n\t\t<title level=\"a\" type=\"main\">Expanding End-to-End Question Answering on Differentiable Knowledge Graphs with Intersection</title>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Priyanka</forename><surname>Sen</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Armin</forename><surname>Oliya</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Amir</forename><surname>Saffari</surname></persName>\n\t\t</author>\n\t\t<idno type=\"DOI\">10.18653/v1/2021.emnlp-main.694</idno>\n\t</analytic>\n\t<monogr>\n\t\t<title level=\"m\">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</title>\n\t\t<meeting>the 2021 Conference on Empirical Methods in Natural Language Processing</meeting>\n\t\t<imprint>\n\t\t\t<publisher>Association for Computational Linguistics</publisher>\n\t\t\t<date type=\"published\" when=\"2021\">2022</date>\n\t\t\t<biblScope unit=\"page\">1619</biblScope>\n\t\t</imprint>\n\t</monogr>\n</biblStruct>\n"}, {"refKey": 7, "tei": "<biblStruct xml:id=\"b7\">\n\t<analytic>\n\t\t<title level=\"a\" type=\"main\">Sparklis: An expressive query builder for SPARQL endpoints with guidance in natural language</title>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">S\u00e9bastien</forename><surname>Ferr\u00e9</surname></persName>\n\t\t</author>\n\t\t<idno type=\"DOI\">10.3233/sw-150208</idno>\n\t</analytic>\n\t<monogr>\n\t\t<title level=\"j\">Semantic Web</title>\n\t\t<title level=\"j\" type=\"abbrev\">SW</title>\n\t\t<idno type=\"ISSN\">1570-0844</idno>\n\t\t<idno type=\"ISSNe\">2210-4968</idno>\n\t\t<imprint>\n\t\t\t<biblScope unit=\"volume\">8</biblScope>\n\t\t\t<biblScope unit=\"issue\">3</biblScope>\n\t\t\t<biblScope unit=\"page\" from=\"405\" to=\"418\" />\n\t\t\t<date type=\"published\" when=\"2016-12-06\">2017</date>\n\t\t\t<publisher>IOS Press</publisher>\n\t\t</imprint>\n\t</monogr>\n</biblStruct>\n"}], "runtime": 121289, "id": "4e02db2e40b11dae360cb345818a0b86c22869f6", "metadata": {"id": "4e02db2e40b11dae360cb345818a0b86c22869f6"}, "original_file_path": "../../datalake/Samuel/SOFTware-Sync/data/xml_not_sofctied/hal-04269089.grobid.tei.xml", "file_name": "hal-04269089.grobid.tei.xml"}