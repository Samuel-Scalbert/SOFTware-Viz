{"application": "software-mentions", "version": "0.8.0", "date": "2024-04-12T16:14+0000", "md5": "06220FFCE9F522B6421AC06405887F4E", "mentions": [{"type": "software", "software-type": "software", "wikidataId": "Q29124045", "wikipediaExternalRef": 56283554, "lang": "en", "confidence": 0.4576, "software-name": {"rawForm": "FastText", "normalizedForm": "FastText", "wikidataId": "Q29124045", "wikipediaExternalRef": 56283554, "lang": "en", "confidence": 0.4576, "offsetStart": 0, "offsetEnd": 8}, "context": "FastText is based on the CBOW architecture but using \ud835\udc5b-grams as input instead of full words. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.000470578670501709}, "created": {"value": false, "score": 0.00032782554626464844}, "shared": {"value": false, "score": 2.980232238769531e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9986816048622131}, "created": {"value": false, "score": 0.006463408470153809}, "shared": {"value": false, "score": 1.7881393432617188e-06}}}, {"type": "software", "software-type": "software", "wikidataId": "Q29124045", "wikipediaExternalRef": 56283554, "lang": "en", "confidence": 0.4576, "software-name": {"rawForm": "FastText", "normalizedForm": "FastText", "wikidataId": "Q29124045", "wikipediaExternalRef": 56283554, "lang": "en", "confidence": 0.4576, "offsetStart": 0, "offsetEnd": 8}, "context": "FastText and FlauBERT are more based on a character or sub-word level. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.0062171220779418945}, "created": {"value": false, "score": 5.918741226196289e-05}, "shared": {"value": false, "score": 2.980232238769531e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9986816048622131}, "created": {"value": false, "score": 0.006463408470153809}, "shared": {"value": false, "score": 1.7881393432617188e-06}}}, {"type": "software", "software-type": "software", "wikidataId": "Q29124045", "wikipediaExternalRef": 56283554, "lang": "en", "confidence": 0.4576, "software-name": {"rawForm": "FastText", "normalizedForm": "FastText", "wikidataId": "Q29124045", "wikipediaExternalRef": 56283554, "lang": "en", "confidence": 0.4576, "offsetStart": 42, "offsetEnd": 50}, "context": "We chose a smaller number of features for FastText because of its need of memory storage. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.6972504258155823}, "created": {"value": false, "score": 0.006463408470153809}, "shared": {"value": false, "score": 2.980232238769531e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9986816048622131}, "created": {"value": false, "score": 0.006463408470153809}, "shared": {"value": false, "score": 1.7881393432617188e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "ELMo", "normalizedForm": "ELMo", "offsetStart": 46, "offsetEnd": 50}, "context": "For example, Embeddings from Language Models (ELMo) [5] is a twolayer bidirectional language model using Bi-LSTM.", "mentionContextAttributes": {"used": {"value": false, "score": 5.942583084106445e-05}, "created": {"value": false, "score": 1.1265277862548828e-05}, "shared": {"value": false, "score": 1.7881393432617188e-07}}, "documentContextAttributes": {"used": {"value": false, "score": 0.0008714795112609863}, "created": {"value": false, "score": 0.0008404254913330078}, "shared": {"value": false, "score": 4.172325134277344e-07}}, "references": [{"label": "[5]", "normalizedForm": "[5]", "refKey": 5, "offsetStart": 6126, "offsetEnd": 6129}]}, {"type": "software", "software-type": "software", "wikidataId": "Q29124045", "wikipediaExternalRef": 56283554, "lang": "en", "confidence": 0.4576, "software-name": {"rawForm": "FastText", "normalizedForm": "FastText", "wikidataId": "Q29124045", "wikipediaExternalRef": 56283554, "lang": "en", "confidence": 0.4576, "offsetStart": 52, "offsetEnd": 60}, "context": "Non-contextual methods such as Word2Vec, Doc2Vec or FastText lagged behind. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.24023199081420898}, "created": {"value": false, "score": 3.904104232788086e-05}, "shared": {"value": false, "score": 2.980232238769531e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9986816048622131}, "created": {"value": false, "score": 0.006463408470153809}, "shared": {"value": false, "score": 1.7881393432617188e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "ELMo", "normalizedForm": "ELMo", "offsetStart": 55, "offsetEnd": 59}, "context": "However, although pre-trained (Bi-)LSTM models such as ELMo have recently become available for French, 2 we were not aware of them at the time we planned our experiments.", "mentionContextAttributes": {"used": {"value": false, "score": 0.0006011724472045898}, "created": {"value": false, "score": 0.0003025531768798828}, "shared": {"value": false, "score": 4.172325134277344e-07}}, "documentContextAttributes": {"used": {"value": false, "score": 0.0008714795112609863}, "created": {"value": false, "score": 0.0008404254913330078}, "shared": {"value": false, "score": 4.172325134277344e-07}}, "references": [{"label": "[5]", "normalizedForm": "[5]", "refKey": 5}]}, {"type": "software", "software-type": "software", "wikidataId": "Q29124045", "wikipediaExternalRef": 56283554, "lang": "en", "confidence": 0.4576, "software-name": {"rawForm": "FastText", "normalizedForm": "FastText", "wikidataId": "Q29124045", "wikipediaExternalRef": 56283554, "lang": "en", "confidence": 0.4576, "offsetStart": 60, "offsetEnd": 68}, "context": "The authors find Word2Vec to perform better than Glove, and FastText to be the worst model in that setting.", "mentionContextAttributes": {"used": {"value": true, "score": 0.5158155560493469}, "created": {"value": false, "score": 2.4020671844482422e-05}, "shared": {"value": false, "score": 2.980232238769531e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9986816048622131}, "created": {"value": false, "score": 0.006463408470153809}, "shared": {"value": false, "score": 1.7881393432617188e-06}}}, {"type": "software", "software-type": "software", "wikidataId": "Q29124045", "wikipediaExternalRef": 56283554, "lang": "en", "confidence": 0.4576, "software-name": {"rawForm": "FastText", "normalizedForm": "FastText", "wikidataId": "Q29124045", "wikipediaExternalRef": 56283554, "lang": "en", "confidence": 0.4576, "offsetStart": 62, "offsetEnd": 70}, "context": "Dynomant et al. [9] compare mainly Word2Vec (Skipgram, CBOW), FastText and Glove on a specific dataset (health-related documents) written in French.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9969048500061035}, "created": {"value": false, "score": 2.682209014892578e-06}, "shared": {"value": false, "score": 1.7881393432617188e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9986816048622131}, "created": {"value": false, "score": 0.006463408470153809}, "shared": {"value": false, "score": 1.7881393432617188e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "ELMo", "normalizedForm": "ELMo", "offsetStart": 83, "offsetEnd": 87}, "context": "It has been proven that Transformers are faster and more efficient than (Bi)-LSTM (ELMo) or CNN (Word2Vec).", "mentionContextAttributes": {"used": {"value": false, "score": 0.0008714795112609863}, "created": {"value": false, "score": 0.0008404254913330078}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": false, "score": 0.0008714795112609863}, "created": {"value": false, "score": 0.0008404254913330078}, "shared": {"value": false, "score": 4.172325134277344e-07}}, "references": [{"label": "[5]", "normalizedForm": "[5]", "refKey": 5}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "SciBERT", "normalizedForm": "SciBERT", "offsetStart": 85, "offsetEnd": 92}, "context": "Thus, some specific domain models have been trained such as BERTTweet for Twitter or SciBERT for the biological domain.", "mentionContextAttributes": {"used": {"value": false, "score": 0.0009098052978515625}, "created": {"value": false, "score": 0.03327357769012451}, "shared": {"value": false, "score": 7.152557373046875e-07}}, "documentContextAttributes": {"used": {"value": false, "score": 0.0009098052978515625}, "created": {"value": false, "score": 0.03327357769012451}, "shared": {"value": false, "score": 7.152557373046875e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "ELMo", "normalizedForm": "ELMo", "offsetStart": 87, "offsetEnd": 91}, "context": "BERT uses parallel attention layers instead of sequential recurrent neural networks as ELMo does. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.00013911724090576172}, "created": {"value": false, "score": 7.510185241699219e-06}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": false, "score": 0.0008714795112609863}, "created": {"value": false, "score": 0.0008404254913330078}, "shared": {"value": false, "score": 4.172325134277344e-07}}, "references": [{"label": "[5]", "normalizedForm": "[5]", "refKey": 5}]}, {"type": "software", "software-type": "software", "wikidataId": "Q29124045", "wikipediaExternalRef": 56283554, "lang": "en", "confidence": 0.4576, "software-name": {"rawForm": "FastText", "normalizedForm": "FastText", "wikidataId": "Q29124045", "wikipediaExternalRef": 56283554, "lang": "en", "confidence": 0.4576, "offsetStart": 88, "offsetEnd": 96}, "context": "Then, we tried non-contextual embedding with Word2Vec (Skip-Gram), Doc2Vec (PV-DM), and FastText. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9986816048622131}, "created": {"value": false, "score": 0.0023130178451538086}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9986816048622131}, "created": {"value": false, "score": 0.006463408470153809}, "shared": {"value": false, "score": 1.7881393432617188e-06}}}, {"type": "software", "software-type": "software", "wikidataId": "Q29124045", "wikipediaExternalRef": 56283554, "lang": "en", "confidence": 0.4576, "software-name": {"rawForm": "FastText", "normalizedForm": "FastText", "wikidataId": "Q29124045", "wikipediaExternalRef": 56283554, "lang": "en", "confidence": 0.4576, "offsetStart": 140, "offsetEnd": 151}, "context": "As Word2Vec, two architectures have been built Finally, there exist other models similar to Word2Vec, such as Global Vectors (GloVe) [3] or FastText [4]. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.00032520294189453125}, "created": {"value": false, "score": 0.003997623920440674}, "shared": {"value": false, "score": 1.7881393432617188e-06}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9986816048622131}, "created": {"value": false, "score": 0.006463408470153809}, "shared": {"value": false, "score": 1.7881393432617188e-06}}}, {"type": "software", "software-type": "software", "wikidataId": "Q29124045", "wikipediaExternalRef": 56283554, "lang": "en", "confidence": 0.4576, "software-name": {"rawForm": "FastText", "normalizedForm": "FastText", "wikidataId": "Q29124045", "wikipediaExternalRef": 56283554, "lang": "en", "confidence": 0.4576, "offsetStart": 197, "offsetEnd": 205}, "context": "The comparison of the text representations for the classifications tasks highlights that classical representations such as TD-IDF have better results than non-contextual word-embbedings (Word2vec, FastText, etc.) or quite similar with state-of-the-art models (CamemBERT).", "mentionContextAttributes": {"used": {"value": false, "score": 0.04542189836502075}, "created": {"value": false, "score": 1.8835067749023438e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9986816048622131}, "created": {"value": false, "score": 0.006463408470153809}, "shared": {"value": false, "score": 1.7881393432617188e-06}}}], "references": [{"refKey": 5, "tei": "<biblStruct xml:id=\"b5\">\n\t<analytic>\n\t\t<title level=\"a\" type=\"main\">Deep Contextualized Word Representations</title>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Matthew</forename><surname>Peters</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Mark</forename><surname>Neumann</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Mohit</forename><surname>Iyyer</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Matt</forename><surname>Gardner</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Christopher</forename><surname>Clark</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Kenton</forename><surname>Lee</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Luke</forename><surname>Zettlemoyer</surname></persName>\n\t\t</author>\n\t\t<idno type=\"DOI\">10.18653/v1/n18-1202</idno>\n\t</analytic>\n\t<monogr>\n\t\t<title level=\"m\">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)</title>\n\t\t<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)</meeting>\n\t\t<imprint>\n\t\t\t<publisher>Association for Computational Linguistics</publisher>\n\t\t\t<date type=\"published\" when=\"2018\">2018</date>\n\t\t</imprint>\n\t</monogr>\n</biblStruct>\n"}], "runtime": 23912, "id": "ef200db811657e1bb28c3d5943384c6e454f5519", "metadata": {"id": "ef200db811657e1bb28c3d5943384c6e454f5519"}, "original_file_path": "../../datalake/Samuel/SOFTware-Sync/data/xml_files/hal-04346759.grobid.tei.xml", "file_name": "hal-04346759.grobid.tei.xml"}