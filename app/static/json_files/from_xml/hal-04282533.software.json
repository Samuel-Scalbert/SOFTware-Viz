{"application": "software-mentions", "version": "0.8.0", "date": "2024-04-12T17:07+0000", "md5": "4046EA06F443344B5337B36D7F68CCBD", "mentions": [{"type": "software", "software-type": "software", "software-name": {"rawForm": "-base", "normalizedForm": "-base", "offsetStart": 10, "offsetEnd": 15}, "context": "As Roberta-base and cc_math_roberta have their own tokenizers, the models' output loss and accuracy are based on different numbers of word pieces and are not comparable. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.15596520900726318}, "created": {"value": false, "score": 4.112720489501953e-06}, "shared": {"value": false, "score": 2.980232238769531e-07}}, "documentContextAttributes": {"used": {"value": false, "score": 0.15596520900726318}, "created": {"value": false, "score": 4.112720489501953e-06}, "shared": {"value": false, "score": 2.980232238769531e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "cc_math_roberta", "normalizedForm": "cc_math_roberta", "offsetStart": 10, "offsetEnd": 25}, "context": "Moreover, cc_math_roberta models' performance varies more than Roberta's (see in for our task, implying the benefit of pre-training.", "mentionContextAttributes": {"used": {"value": false, "score": 0.0016068816184997559}, "created": {"value": false, "score": 9.47713851928711e-06}, "shared": {"value": false, "score": 2.980232238769531e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9995611310005188}, "created": {"value": true, "score": 0.9671800136566162}, "shared": {"value": false, "score": 2.7418136596679688e-06}}, "references": [{"label": "(Mishra et al., 2023", "normalizedForm": "(Mishra et al., 2023", "refKey": 12}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Scholarphi", "normalizedForm": "Scholarphi", "offsetStart": 13, "offsetEnd": 43}, "context": "For example, Scholarphi (Head et al., 2021) is an augmented reading interface for papers with publicly available L A T E X sources. ", "mentionContextAttributes": {"used": {"value": false, "score": 3.93986701965332e-05}, "created": {"value": false, "score": 0.0004169344902038574}, "shared": {"value": false, "score": 0.0013023614883422852}}, "documentContextAttributes": {"used": {"value": false, "score": 3.93986701965332e-05}, "created": {"value": false, "score": 0.0004169344902038574}, "shared": {"value": false, "score": 0.0013023614883422852}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "ChatGPT", "normalizedForm": "ChatGPT", "offsetStart": 18, "offsetEnd": 25}, "version": {"rawForm": "7", "normalizedForm": "7"}, "context": "We initially gave ChatGPT only one example in our question and attempted to obtain a IOB2-compliant output. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9990245699882507}, "created": {"value": false, "score": 0.0060231685638427734}, "shared": {"value": false, "score": 2.980232238769531e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9990245699882507}, "created": {"value": false, "score": 0.0060231685638427734}, "shared": {"value": false, "score": 8.344650268554688e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "ChatGPT", "normalizedForm": "ChatGPT", "offsetStart": 19, "offsetEnd": 26}, "version": {"rawForm": "7", "normalizedForm": "7"}, "context": "However, if we ask ChatGPT to return the definienda directly, we get more pertinent results. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9959695339202881}, "created": {"value": false, "score": 0.00016117095947265625}, "shared": {"value": false, "score": 4.172325134277344e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9990245699882507}, "created": {"value": false, "score": 0.0060231685638427734}, "shared": {"value": false, "score": 8.344650268554688e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "cc_math_roberta", "normalizedForm": "cc_math_roberta", "offsetStart": 20, "offsetEnd": 35}, "context": "As Roberta-base and cc_math_roberta have their own tokenizers, the models' output loss and accuracy are based on different numbers of word pieces and are not comparable. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.15596520900726318}, "created": {"value": false, "score": 4.112720489501953e-06}, "shared": {"value": false, "score": 2.980232238769531e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9995611310005188}, "created": {"value": true, "score": 0.9671800136566162}, "shared": {"value": false, "score": 2.7418136596679688e-06}}, "references": [{"label": "(Mishra et al., 2023", "normalizedForm": "(Mishra et al., 2023", "refKey": 12}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "cc_math_roberta", "normalizedForm": "cc_math_roberta", "offsetStart": 20, "offsetEnd": 35}, "context": "Our experience with cc_math_roberta models also open up research about improving the robustness over different NLP tasks of from-scratched domainspecific language models.", "mentionContextAttributes": {"used": {"value": false, "score": 0.00014257431030273438}, "created": {"value": true, "score": 0.9671800136566162}, "shared": {"value": false, "score": 4.172325134277344e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9995611310005188}, "created": {"value": true, "score": 0.9671800136566162}, "shared": {"value": false, "score": 2.7418136596679688e-06}}, "references": [{"label": "(Mishra et al., 2023", "normalizedForm": "(Mishra et al., 2023", "refKey": 12}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "cc_math_roberta", "normalizedForm": "cc_math_roberta", "offsetStart": 21, "offsetEnd": 36}, "context": "Following this idea, cc_math_roberta (Mishra et al., 2023) is a RoBERTa-based model pertained from scratch on mathematical articles from arXiv (Mishra et al., 2023).", "mentionContextAttributes": {"used": {"value": false, "score": 0.0006323456764221191}, "created": {"value": false, "score": 0.025880277156829834}, "shared": {"value": false, "score": 4.172325134277344e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9995611310005188}, "created": {"value": true, "score": 0.9671800136566162}, "shared": {"value": false, "score": 2.7418136596679688e-06}}, "references": [{"label": "(Mishra et al., 2023", "normalizedForm": "(Mishra et al., 2023", "refKey": 12, "offsetStart": 4565, "offsetEnd": 4585}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "pylatexenc", "normalizedForm": "pylatexenc", "offsetStart": 27, "offsetEnd": 37}, "context": "https://github.com/phfaist/pylatexenc", "mentionContextAttributes": {"used": {"value": false, "score": 0.0001609325408935547}, "created": {"value": false, "score": 1.9371509552001953e-05}, "shared": {"value": true, "score": 0.973906934261322}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999838471412659}, "created": {"value": false, "score": 1.9371509552001953e-05}, "shared": {"value": true, "score": 0.973906934261322}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "SciBERT", "normalizedForm": "SciBERT", "offsetStart": 39, "offsetEnd": 69}, "context": "The results show that models involving SciBERT (Beltagy et al., 2019) achieved higher accuracy on most measurements due to the domain similarity between the scholarly documents for pre-training SciBERT and those used in the evaluation. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9999207854270935}, "created": {"value": false, "score": 5.304813385009766e-06}, "shared": {"value": false, "score": 2.980232238769531e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999208450317383}, "created": {"value": false, "score": 5.304813385009766e-06}, "shared": {"value": false, "score": 2.980232238769531e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "cc_math_roberta", "normalizedForm": "cc_math_roberta", "offsetStart": 43, "offsetEnd": 58}, "context": "To our surprise, although the tokenizer of cc_math_roberta models produced fewer word pieces than Roberta's tokenizer, Roberta-base yielded the best performance among the three models in our task, regardless the size of the training set.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9995611310005188}, "created": {"value": false, "score": 5.78761100769043e-05}, "shared": {"value": false, "score": 1.7881393432617188e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9995611310005188}, "created": {"value": true, "score": 0.9671800136566162}, "shared": {"value": false, "score": 2.7418136596679688e-06}}, "references": [{"label": "(Mishra et al., 2023", "normalizedForm": "(Mishra et al., 2023", "refKey": 12}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "RobertaForTokenClassification", "normalizedForm": "RobertaForTokenClassification", "offsetStart": 52, "offsetEnd": 81}, "context": "We used the implementation for token classification RobertaForTokenClassification in the transformers package (Wolf et al., 2020). ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9999361634254456}, "created": {"value": false, "score": 0.0025374293327331543}, "shared": {"value": false, "score": 1.7881393432617188e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999361634254456}, "created": {"value": false, "score": 0.0025374293327331543}, "shared": {"value": false, "score": 1.7881393432617188e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "transformers", "normalizedForm": "transformers", "offsetStart": 89, "offsetEnd": 101}, "context": "We used the implementation for token classification RobertaForTokenClassification in the transformers package (Wolf et al., 2020). ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9999361634254456}, "created": {"value": false, "score": 0.0025374293327331543}, "shared": {"value": false, "score": 1.7881393432617188e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999361634254456}, "created": {"value": false, "score": 0.0025374293327331543}, "shared": {"value": false, "score": 1.7881393432617188e-07}}, "references": [{"label": "(Wolf et al., 2020)", "normalizedForm": "Wolf et al., 2020", "refKey": 18, "offsetStart": 9369, "offsetEnd": 9388}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "OpenAI", "normalizedForm": "OpenAI", "offsetStart": 94, "offsetEnd": 100}, "context": "We thus asked GPT-3.5-Turbo and GPT-4 to identify the definienda in our ground truth data via OpenAI's API. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9997409582138062}, "created": {"value": false, "score": 0.0022513866424560547}, "shared": {"value": false, "score": 2.980232238769531e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9997409582138062}, "created": {"value": false, "score": 0.0022513866424560547}, "shared": {"value": false, "score": 6.556510925292969e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "pylatexenc", "normalizedForm": "pylatexenc", "offsetStart": 101, "offsetEnd": 112}, "context": "We then converted the extracted partial L A T E X code into plain text with Unicode characters using pylatexenc4 . ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9999838471412659}, "created": {"value": false, "score": 1.7464160919189453e-05}, "shared": {"value": false, "score": 1.0132789611816406e-06}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999838471412659}, "created": {"value": false, "score": 1.9371509552001953e-05}, "shared": {"value": true, "score": 0.973906934261322}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "OpenAI", "normalizedForm": "OpenAI", "offsetStart": 106, "offsetEnd": 112}, "context": "Finally, note that these finetuned language models are obviously much less computationally expensive than OpenAI's GPT models.", "mentionContextAttributes": {"used": {"value": false, "score": 0.00016021728515625}, "created": {"value": false, "score": 0.00027561187744140625}, "shared": {"value": false, "score": 6.556510925292969e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9997409582138062}, "created": {"value": false, "score": 0.0022513866424560547}, "shared": {"value": false, "score": 6.556510925292969e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "cc_math_roberta", "normalizedForm": "cc_math_roberta", "offsetStart": 111, "offsetEnd": 126}, "context": "This model outperforms Roberta in a sentence-level classification task while the corpora size for pre-training cc_math_roberta is much smaller than Roberta's.", "mentionContextAttributes": {"used": {"value": false, "score": 0.0005161762237548828}, "created": {"value": false, "score": 2.2172927856445312e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9995611310005188}, "created": {"value": true, "score": 0.9671800136566162}, "shared": {"value": false, "score": 2.7418136596679688e-06}}, "references": [{"label": "(Mishra et al., 2023", "normalizedForm": "(Mishra et al., 2023", "refKey": 12}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "cc_math_roberta", "normalizedForm": "cc_math_roberta", "offsetStart": 126, "offsetEnd": 162}, "context": "We experimented with an out-of-the-box and general language model Roberta-base (Liu et al., 2019) and a domain-specific model cc_math_roberta (Mishra et al., 2023). ", "mentionContextAttributes": {"used": {"value": false, "score": 0.0004975795745849609}, "created": {"value": true, "score": 0.6110381484031677}, "shared": {"value": false, "score": 2.7418136596679688e-06}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9995611310005188}, "created": {"value": true, "score": 0.9671800136566162}, "shared": {"value": false, "score": 2.7418136596679688e-06}}, "references": [{"label": "(Mishra et al., 2023", "normalizedForm": "(Mishra et al., 2023", "refKey": 12}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "SciBERT", "normalizedForm": "SciBERT", "offsetStart": 194, "offsetEnd": 201}, "context": "The results show that models involving SciBERT (Beltagy et al., 2019) achieved higher accuracy on most measurements due to the domain similarity between the scholarly documents for pre-training SciBERT and those used in the evaluation. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9999208450317383}, "created": {"value": false, "score": 5.245208740234375e-06}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999208450317383}, "created": {"value": false, "score": 5.304813385009766e-06}, "shared": {"value": false, "score": 2.980232238769531e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "ChatGPT", "normalizedForm": "ChatGPT", "offsetStart": 210, "offsetEnd": 217}, "version": {"rawForm": "7", "normalizedForm": "7", "offsetStart": 218, "offsetEnd": 219}, "context": "Driven by the growing popularity of few-shot learning with pre-trained language models (Brown et al., 2020), we also query the GPT language model, using different available versions: we first experimented with ChatGPT 7 (based on GPT 3.5) and then used the API versions of GPT-3.5-Turbo and GPT-4. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.1596410870552063}, "created": {"value": false, "score": 0.0001277923583984375}, "shared": {"value": false, "score": 8.344650268554688e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9990245699882507}, "created": {"value": false, "score": 0.0060231685638427734}, "shared": {"value": false, "score": 8.344650268554688e-07}}}], "references": [{"refKey": 12, "tei": "<biblStruct xml:id=\"b12\">\n\t<monogr>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Shrey</forename><surname>Mishra</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Antoine</forename><surname>Gauquier</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Pierre</forename><surname>Senellart</surname></persName>\n\t\t</author>\n\t\t<idno>arXiv:2307.09047</idno>\n\t\t<title level=\"m\">Multimodal machine learning for extraction of theorems and proofs in the scientific literature</title>\n\t\t<imprint>\n\t\t\t<date>2023</date>\n\t\t</imprint>\n\t</monogr>\n</biblStruct>\n"}, {"refKey": 18, "tei": "<biblStruct xml:id=\"b18\">\n\t<analytic>\n\t\t<title level=\"a\" type=\"main\">Transformers: State-of-the-Art Natural Language Processing</title>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Thomas</forename><surname>Wolf</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Lysandre</forename><surname>Debut</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Victor</forename><surname>Sanh</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Julien</forename><surname>Chaumond</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Clement</forename><surname>Delangue</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Anthony</forename><surname>Moi</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Pierric</forename><surname>Cistac</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Tim</forename><surname>Rault</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Remi</forename><surname>Louf</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Morgan</forename><surname>Funtowicz</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Joe</forename><surname>Davison</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Sam</forename><surname>Shleifer</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Patrick</forename><surname>Von Platen</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Clara</forename><surname>Ma</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Yacine</forename><surname>Jernite</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Julien</forename><surname>Plu</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Canwen</forename><surname>Xu</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Teven</forename><surname>Le Scao</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Sylvain</forename><surname>Gugger</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Mariama</forename><surname>Drame</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Quentin</forename><surname>Lhoest</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Alexander</forename><surname>Rush</surname></persName>\n\t\t</author>\n\t\t<idno type=\"DOI\">10.18653/v1/2020.emnlp-demos.6</idno>\n\t</analytic>\n\t<monogr>\n\t\t<title level=\"m\">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</title>\n\t\t<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</meeting>\n\t\t<imprint>\n\t\t\t<publisher>Association for Computational Linguistics</publisher>\n\t\t\t<date type=\"published\" when=\"2020\">2020</date>\n\t\t\t<biblScope unit=\"page\">45</biblScope>\n\t\t</imprint>\n\t</monogr>\n</biblStruct>\n"}], "runtime": 10688, "id": "158dfc5cac2756d62a2b914b44a8fcb06d6065bc", "metadata": {"id": "158dfc5cac2756d62a2b914b44a8fcb06d6065bc"}, "original_file_path": "../../datalake/Samuel/SOFTware-Sync/data/xml_not_sofctied/hal-04282533.grobid.tei.xml", "file_name": "hal-04282533.grobid.tei.xml"}