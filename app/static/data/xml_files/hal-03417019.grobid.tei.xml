<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">PROTECT: A Pipeline for Propaganda Detection and Classification</title>
				<funder ref="#_haCK9Hs">
					<orgName type="full">French government</orgName>
				</funder>
				<funder>
					<orgName type="full">3IA Côte d&apos;Azur Investments</orgName>
				</funder>
				<funder ref="#_ME4DekK">
					<orgName type="full">Agence Nationale de la Recherche</orgName>
					<orgName type="abbreviated">ANR</orgName>
				</funder>
				<funder ref="#_XyUa3pv #_kXYqwcy">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Vorakit</forename><surname>Vorakitphan</surname></persName>
							<email>vorakit.vorakitphan@inria.fr</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Université Côte d&apos;Azur</orgName>
								<orgName type="institution" key="instit2">Inria</orgName>
								<orgName type="institution" key="instit3">CNRS</orgName>
								<address>
									<region>I3S</region>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Université Côte d&apos;Azur</orgName>
								<orgName type="institution" key="instit2">Inria</orgName>
								<orgName type="institution" key="instit3">CNRS</orgName>
								<address>
									<region>I3S</region>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Elena</forename><surname>Cabrio</surname></persName>
							<email>elena.cabrio@univ-cotedazur.fr</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Université Côte d&apos;Azur</orgName>
								<orgName type="institution" key="instit2">Inria</orgName>
								<orgName type="institution" key="instit3">CNRS</orgName>
								<address>
									<region>I3S</region>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Université Côte d&apos;Azur</orgName>
								<orgName type="institution" key="instit2">Inria</orgName>
								<orgName type="institution" key="instit3">CNRS</orgName>
								<address>
									<region>I3S</region>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Serena</forename><surname>Villata</surname></persName>
							<email>villata@i3s.unice.fr</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Université Côte d&apos;Azur</orgName>
								<orgName type="institution" key="instit2">Inria</orgName>
								<orgName type="institution" key="instit3">CNRS</orgName>
								<address>
									<region>I3S</region>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Université Côte d&apos;Azur</orgName>
								<orgName type="institution" key="instit2">Inria</orgName>
								<orgName type="institution" key="instit3">CNRS</orgName>
								<address>
									<region>I3S</region>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">PROTECT: A Pipeline for Propaganda Detection and Classification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">E89170193AA0CB3C0D6D17BCA28BF710</idno>
					<idno type="DOI">10.4000/books.aaccademia.10884</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-04-12T14:45+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>English. Propaganda is a rhetorical technique to present opinions with the deliberate goal of influencing the opinions and the actions of other (groups of) individuals for predetermined misleading ends. The employment of such manipulation techniques in politics and news articles, as well as its subsequent spread on social networks, may lead to threatening consequences for the society and its more vulnerable members. In this paper, we present PRO-TECT (PROpaganda Text dEteCTion), a new system to automatically detect propagandist messages and classify them along with the propaganda techniques employed. PROTECT is designed as a full pipeline to firstly detect propaganda text snippets from the input text, and then classify the technique of propaganda, taking advantage of semantic and argumentation features. A video demo of the PROTECT system is also provided to show its main functionalities.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>dEteCTion), un nuovo sistema per identificare automaticamente i messaggi propagandistici e classificarli rispetto alle tecniche di propaganda utilizzate. PROTECT è un sistema progettato come una pipeline completa per rilevare in primo luogo i frammenti di testo propagandistici dato il testo proposto, e successivamente classificare tali frammenti secondo la tecnica di propaganda usata, sfruttando le caratteristiche semantiche e argomentative del testo. Questo articolo presenta anche un video dimostrativo del sistema PROTECT per mostrare le principali funzionalità fornite all'utente.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Propaganda represents an effective but often misleading communication strategy which is employed to promote a certain viewpoint, for instance in the political context <ref type="bibr" target="#b9">(Lasswell, 1938;</ref><ref type="bibr" target="#b8">Koppang, 2009;</ref><ref type="bibr" target="#b6">Dillard and Pfau, 2009;</ref><ref type="bibr" target="#b12">Longpre et al., 2019)</ref>. The goal of this communication strategy is to persuade the audience about the goodness of such a viewpoint by means of misleading and/or partial arguments, which is particularly harmful for the more vulnerable public in the society (e.g., young or elder people). Therefore the ability to detect the occurrences of propaganda in political discourse and newspaper articles is of main importance, and Natural Language Processing methods and technologies play a main role in this context addressing the propaganda detection and classification task <ref type="bibr" target="#b2">(Da San Martino et al., 2019;</ref><ref type="bibr">Da San Martino et al., 2020a)</ref>. It is, in particular, important to make this vulnerable public aware of the problem and provide them tools able to raise their awareness and develop their critical thinking.</p><p>To achieve this ambitious goal, we present in this paper a new tool called PROTECT (PROpaganda Text dEteCTion) to automatically identify and classify propaganda in texts. In the current version, only English text is processed. This tool has been designed with an easy-to-access user interface and a web-service API to ensure a wide public use of PROTECT online. To the best of our knowledge, PROTECT is the first online tool for propagandist text identification and classification with an interface allowing the user to submit his/her own text to be analysed.<ref type="foot" target="#foot_0">1</ref> PROTECT presents two main functionalities: i) the automatic propaganda detection and classification service, which allows the user to paste or upload a text and returns the text where the propagandist text snippets are highlighted in different colors depending on the propaganda technique which is employed, and ii) the propaganda word clouds, to show in a easy to catch visualisation the identified propagandist text snippets. PROTECT is deployed as a web-service API, allowing users to download the output (the text annotated with the identified propaganda technique) as a json file. The PRO-TECT tool relies on a pipeline architecture to first detect the propaganda text snippets, and second to classify the propaganda text snippets with respect to a specific propaganda technique. We cast this task as a sentence-span classification problem and we address it relying on a transformer architecture. Results reach SoTA systems performances on the tasks of propaganda detection and classification (for a comparison with SoTA algorithms, we refer to <ref type="bibr" target="#b16">(Vorakitphan et al., 2021)</ref>).</p><p>The paper is structured as follows: first, Section 2 discusses the state of the art in propaganda detection and classification and compares our contribution to the literature. Then Section 3 describes the pipeline for the detection and classification of propaganda text snippets as well as the data sets used for the evaluation and the obtained results. Section 4 describes the functionalities of the web interface, followed by the Conclusions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>In the last years, there has been an increasing interest in investigating methods for textual propaganda detection and classification. Among them, <ref type="bibr" target="#b1">(Barrón-Cedeño et al., 2019)</ref> present a sys-tem to organize news events according to the level of propagandist content in the articles, and introduces a new corpus (QProp) annotated with the propaganda vs. trustworthy classes, providing information about the source of the news articles. Recently, a web demo named Prta <ref type="bibr">(Da San Martino et al., 2020b)</ref> has been proposed, trained on disinformation articles. This demo allows a user to enter a plain text or a URL, but it does not allow users to download such results. Similarly to PROTECT, Prta shows the propagandist messages at the snippet level with an option to filter the propaganda techniques to be shown based on the confidence rate, and also analyzes the usage of propaganda technique on determined topics. The implementation of this system relies on the approach proposed in <ref type="bibr" target="#b2">(Da San Martino et al., 2019)</ref>.</p><p>The most recent approaches for propaganda detection are based on language models that mostly involve transformer-based architectures. The approach that performed best on the NLP4IF'19 sentence-level classification task relies on the BERT architecture with hyperparameters tuning without activation function <ref type="bibr" target="#b13">(Mapes et al., 2019)</ref>. <ref type="bibr" target="#b17">(Yoosuf and Yang, 2019)</ref> focused first on the pre-processing steps to provide more information regarding the language model along with existing propaganda techniques, then they employ the BERT architecture casting the task as a sequence labeling problem. The systems that took part in the SemEval 2020 Challenge -Task 11 represent the most recent approaches to identify propaganda techniques based on given propagandist spans. The most interesting and successful approach <ref type="bibr" target="#b7">(Jurkiewicz et al., 2020)</ref> proposes first to extend the training data from a free text corpus as a silver dataset, and second, an ensemble model that exploits both the gold and silver datasets during the training steps to achieve the highest scores.</p><p>As most of the above mentioned systems, also PROTECT relies on language model architectures for the detection and classification of propaganda messages, empowering them with a rich set of features we identified as pivotal in propagandist text from computational social science literature <ref type="bibr" target="#b16">(Vorakitphan et al., 2021)</ref>. In particular, <ref type="bibr" target="#b14">(Morris, 2012)</ref> discusses how emotional markers and affect at word-or phrase-level are employed in propaganda text, whilst <ref type="bibr" target="#b0">(Ahmad et al., 2019)</ref> show that the most effective technique to extract senti-ment for the propaganda detection task is to rely on lexicon-based tailored dictionaries. <ref type="bibr" target="#b10">(Li et al., 2017)</ref> show how to detect degrees of strength from calmness to exaggeration in press releases. Finally, <ref type="bibr" target="#b15">(Troiano et al., 2018)</ref> focus on feature extraction of text exaggeration and show that main factors include imageability, unexpectedness, and the polarity of a sentence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Propaganda Detection and Classification</head><p>PROTECT addresses the task of propaganda technique detection and classification at fragmentlevel, meaning that both the spans and the type of propaganda technique are identified and highlighted in the input sentences. In the following, we describe the datasets used to train and test PRO-TECT, and the approach implemented in the system to address the task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Datasets</head><p>To evaluate the approach on which PROTECT relies, we use two standard benchmarks for Propaganda Detection and Classification, namely the NLP4IF'19 <ref type="bibr" target="#b2">(Da San Martino et al., 2019)</ref> and SemEval'20 datasets <ref type="bibr">(Da San Martino et al., 2020a</ref> Those classes are not uniformly distributed in the data sets.</p><p>Loaded-Language and Name-Calling Labeling are the classes with the 2 https://propaganda.qcri.org/nlp4if-s hared-task/ 3 https://propaganda.qcri.org/semeval2 020-task11/ higher number of instances (representing respectively 32% and 15% of the propagandist messages on all above-mentioned datasets). The classes with the lower number of instances are Whataboutism, Red-Herring, Bandwagon, Straw-Men, respectively occurring in 1%, 0.87%, 0.29%, 0.23% in NLP4IF'19 datasets. In SemEval'20T11 such labels where merged, and the classes Whataboutism Straw-Men Red-Herring, Bandwagon respectively represent 1.33% and 1.29% of the propagandist messages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">PROTECT Architecture</head><p>Given a textual document or a paragraph as input, the system performs two steps. First, it performs a binary classification at token level, to label a token as propagandist or not. Then, it classifies propagandist tokens according to the 14 propaganda categories from SemEval task (T11).</p><p>For instance, given the following example "Manchin says Democrats acted like babies at the SOTU (video) Personal Liberty Poll Exercise your right to vote." the snippets "babies" is first classified as propaganda (step 1), and then more specifically as an instance of the Name-Calling Labeling propaganda technique (step 2).</p><p>Step 1: Propaganda Snippet Detection. To train PROTECT, we merge the training, development and test sets from NLP4IF, and the training set from Semeval'20 T11. The development set from Semeval'20 T11 is instead used to evaluate the system performances. <ref type="foot" target="#foot_1">4</ref> In the preprocessing phase, each sentence is tokenized and tagged with a label per token according to the IOB format.</p><p>For the binary classification, we adopt Pretrained Language Model (PLM) based on BERT (bert-base-uncased model) <ref type="bibr" target="#b5">(Devlin et al., 2019)</ref> architecture. The hyperparameters are a learning rate of 5e-5, a batch of 8, max len of 128. For the evaluation, we compute standard classification metrics<ref type="foot" target="#foot_2">5</ref> at the token-level. We then perform a post-processing step to automatically join tokens labelled with the same propaganda technique into the same textual span.</p><p>Given that PLM is applied at token-level, each token is processed into sub-words (e.g., "running" is tokenized and cut into two tokens: "run" and "##ing"). Such sub-words can mislead the classifier. For instance, in the following sentence: "The next day, Biden said, he was informed by Indian press that there were at least a few Bidens in India.", our system detects least a few Bidens in as a propagandist snippet, but it misclassifies one sub-word ("at" was not considered as part of "at least", and therefore excluded from the propagandist snippet).</p><p>Step 2: Propaganda Technique Classification. We cast this task as a sentence-span multi-class classification problem. More specifically, both the tokenized sentence and the span are used to feed the transformer-based model RoBERTa (robertabase pre-trained model) 7 <ref type="bibr" target="#b11">(Liu et al., 2019)</ref> to per- 6 We are aware that sigmoid function is usually used as default activation function in binary classification. However, in our setting we tested both functions and we obtained better performances with Softmax as activation function (+0.04 F1 with respect to sigmoid).</p><p>7 https://huggingface.co/transformers/ model doc/roberta.html form both a sentence classification and a span classification. More precisely: i) we input a sentence to the tokenizer where max length is set to 128 with padding; ii) we input the span provided by the propaganda span-template from SemEval T11 dataset, and we set max length value of 20 with padding. RoBERTa tokenizer is applied in both cases. If a sentence does not contain propaganda spans, it is labeled as "none-propaganda".</p><p>To take into account context features at sentence-level, a BiLSTM is introduced. For each sentence, semantic and argumentation features are extracted following the methodology proposed in <ref type="bibr" target="#b16">(Vorakitphan et al., 2021)</ref> and given in input to the BiLSTM model (hyper-parameters: 256 hidden size, 1 hidden layer, drop out of 0.1 with ReLU function at the last layer before the joint loss function). Such features proved to be useful to improve the performances of our approach on propagandist messages classification, obtaining SoTA results on some categories (in <ref type="bibr" target="#b16">(Vorakitphan et al., 2021)</ref> we provide a comparison of our model with SoTA systems on both NLP4IF and SemEval datasets).</p><p>To combine the results from sentence-span based RoBERTa with the feature-based BiLSTM we apply the joint loss strategy proposed in <ref type="bibr" target="#b16">(Vorakitphan et al., 2021)</ref>. Each model produces a loss per batch using CrossEntropy loss function L. Following the function: loss joint loss = α × (loss sentence +lossspan+loss semantic argumentation features )</p><p>N loss</p><p>where each loss value is produced from CrossEntropy function of its classifier (e.g., loss sentence and loss span from RoBERTa models of sentence and span, loss semantic argumentation features from the BiLSTM model.) To train the above mentioned methods for the propaganda technique classification task, we merged the data sets of NLP4IF'19 and Se-mEval'20 T11 (same setting as in Step 1). Then we tested the full pipeline of PROTECT on the development set from Semeval'20 T11. The output of the snippet detection task (Step 1) are provided as a span-pattern to the models performing Step 2. Table <ref type="table">1</ref> reports on the obtained results of the full pipeline (Step 1+Step 2) averaged over 5 runs (we cannot provide a fair comparison of those results with SoTA systems, given that in SemEval the two tasks are separately evaluated and no pipeline results are provided). We can notice however, that our results in a pipeline are comparable with the Given the high complexity of the propaganda technique classification task and the classes' unbalance, some examples are miss-classified by the system. For instance, in the following sentence "The Mueller probe saw several within Trump's orbit indicted, but not Trump ' as family or Trump himself", the system annotated the snippet in italics as "Name Calling,Labeling", while the correct labels would have been "Repetition".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">PROTECT Functionalities</head><p>As previously introduced, PROTECT allows a user to input plain text and retrieve the propagandist spans in the message as output by the system. In the current version of the system, two services are provided through the web interface (and the API), described in the following.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Service 1: Propaganda Techniques Classification</head><p>The system accepts an input plain text in English, and then the architecture described in Section 3.2 is run over such text. The output consists of an annotated version of the input text, where the different propagandist techniques detected by the system are highlighted in different colours. The colour of the highlighted snippet is distinctive of a certain propaganda technique: the darker the color, the higher the confidence score of the system in assigning the label to a textual snippet. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Service 2: Propaganda Word Clouds</head><p>The propagandist snippets output by the system can also be displayed as word clouds, where the size of the words represents the system confidence score in assigning the labels (see Figure <ref type="figure" target="#fig_2">2</ref>). The different sizes represent the confidence score of the prediction, and the colors the propaganda technique (as in Service 1). If multiple techniques are found in the same snippet, it is duplicated in the word cloud. As for the first service, a checkbox on the right side of the word clouds allows the user to select the propagandist techniques to be visualized. Also in this case, a json file can be downloaded with the system prediction.</p><p>The word cloud service has been added to PRO- </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>In this paper, we presented PROTECT, a propaganda detection and classification tool. PROTECT relies on a pipeline to detect propaganda snippets from plain text. We evaluated the proposed pipeline on standard benchmarks achieving stateof-the-art results. PROTECT is deployed as a web-service API that accepts a plain text input, returning downloadable annotated text for further usage. In addition, a propaganda word clouds service allows to gain further insights from such text.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: PROTECT Interface: Propaganda Techniques Classification</figDesc><graphic coords="6,72.00,62.81,450.83,204.92" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Figure  1shows an example of PROTECT web inter-face. Checkboxes on the right side of the page provide the key to interpret the colors, and allow the user to check or un-check (i.e. highlight or not) the different propagandist snippets in the text, filtering the results. Faded to dark colours represent the confidence level of the prediction (the darker the colour, the higher the system confidence). The snippets in bold contain multiple propaganda techniques in the same text spans, that can be unveiled hovering with the mouse over the snippets.As said before, PROTECT can be used through the provided API, and annotated text can be downloaded as a JSON file with the detected propagandist snippet(s) at character indices (start to end indices of a snippet) based on individual sentence, propaganda technique(s) used, and the confidence score(s)).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: PROTECT Interface: Word Cloud</figDesc><graphic coords="7,72.00,62.81,450.84,174.26" type="bitmap" /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>The video demonstrating the PROTECT tool is available here https://1drv.ms/u/s!Ao-qMrhQAfYtkzD69 JPAYY3nSFub?e=oUQbxQ</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_1"><p>The gold annotations of Semeval'20 test set are not available, this is why we selected the development set for evalua-</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_2"><p>tion. 5 https://scikit-learn.org/stable/modu les/generated/sklearn.metrics.precisio n recall fscore support.html</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>This work is partially supported by the <rs type="projectName">AN-SWER</rs> project <rs type="projectName">PIA FSN2</rs> n.</p><p><rs type="grantNumber">P159564-2661789/DOS0060094</rs> between Inria and Qwant. This work has also been supported by the <rs type="funder">French government</rs>, through the <rs type="funder">3IA Côte d'Azur Investments</rs> in the Future project managed by the <rs type="funder">National Research Agency (ANR)</rs> with the reference number <rs type="grantNumber">ANR-19-P3IA-0002</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_XyUa3pv">
					<orgName type="project" subtype="full">AN-SWER</orgName>
				</org>
				<org type="funded-project" xml:id="_kXYqwcy">
					<orgName type="project" subtype="full">PIA FSN2</orgName>
				</org>
				<org type="funding" xml:id="_haCK9Hs">
					<idno type="grant-number">P159564-2661789/DOS0060094</idno>
				</org>
				<org type="funding" xml:id="_ME4DekK">
					<idno type="grant-number">ANR-19-P3IA-0002</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A review of feature selection and sentiment analysis technique in issues of propaganda</title>
		<author>
			<persName><forename type="first">Ahmad</forename><surname>Siti Rohaidah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Muhammad</forename><surname>Zakwan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Muhammad</forename><surname>Rodzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nurlaila</forename><surname>Syafira Shapiei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nurhafizah</forename><surname>Moziyana Mohd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suhaila</forename><surname>Yusop</surname></persName>
		</author>
		<author>
			<persName><surname>Ismail</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Advanced Computer Science and Applications</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">11</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Proppy: Organizing the news based on their propagandistic content</title>
		<author>
			<persName><forename type="first">Alberto</forename><surname>Barrón-Cedeño</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Israa</forename><surname>Jaradat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Giovanni</forename><surname>Martino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Preslav</forename><surname>Nakov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">formation Processing Management</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Fine-grained analysis of propaganda in news article</title>
		<author>
			<persName><forename type="first">Giovanni</forename><surname>Da</surname></persName>
		</author>
		<author>
			<persName><forename type="first">San</forename><surname>Martino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seunghak</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alberto</forename><surname>Barrón-Cedeño</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rostislav</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Preslav</forename><surname>Nakov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-11">2019. November</date>
			<biblScope unit="page" from="5636" to="5646" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">SemEval-2020 task 11: Detection of propaganda techniques in news articles</title>
		<author>
			<persName><forename type="first">Giovanni</forename><surname>Da</surname></persName>
		</author>
		<author>
			<persName><forename type="first">San</forename><surname>Martino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alberto</forename><surname>Barrón-Cedeño</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Henning</forename><surname>Wachsmuth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rostislav</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Preslav</forename><surname>Nakov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourteenth Workshop on Semantic Evaluation</title>
		<meeting>the Fourteenth Workshop on Semantic Evaluation<address><addrLine>Barcelona (online</addrLine></address></meeting>
		<imprint>
			<publisher>International Committee for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1377" to="1414" />
		</imprint>
	</monogr>
	<note>December</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Prta: A system to support the analysis of propaganda techniques in the news</title>
		<author>
			<persName><forename type="first">Giovanni</forename><surname>Da</surname></persName>
		</author>
		<author>
			<persName><forename type="first">San</forename><surname>Martino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaden</forename><surname>Shaar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yifan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seunghak</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alberto</forename><surname>Barrón-Cedeño</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Preslav</forename><surname>Nakov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: System Demonstrations</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics: System Demonstrations</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020-07">2020. July</date>
			<biblScope unit="page" from="287" to="293" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">BERT: pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019</title>
		<editor>
			<persName><forename type="first">Jill</forename><surname>Burstein</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Christy</forename><surname>Doran</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Thamar</forename><surname>Solorio</surname></persName>
		</editor>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019<address><addrLine>Minneapolis, MN, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-06-02">2019. June 2-7, 2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
	<note>Long and Short Papers</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">The Persuasion Handbook: Developments in Theory and Practice</title>
		<author>
			<persName><forename type="first">James</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dillard</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Pfau</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>Sage Publications, Inc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">ApplicaAI at SemEval-2020 task 11: On RoBERTa-CRF, span CLS and whether self-training helps them</title>
		<author>
			<persName><forename type="first">Dawid</forename><surname>Jurkiewicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Łukasz</forename><surname>Borchmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Izabela</forename><surname>Kosmala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Filip</forename><surname>Graliński</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourteenth Workshop on Semantic Evaluation</title>
		<meeting>the Fourteenth Workshop on Semantic Evaluation<address><addrLine>Barcelona (online</addrLine></address></meeting>
		<imprint>
			<publisher>International Committee for Computational Linguistics</publisher>
			<date type="published" when="2020-12">2020. December</date>
			<biblScope unit="page" from="1415" to="1424" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Social influence by manipulation: A definition and case of propaganda</title>
		<author>
			<persName><forename type="first">Haavard</forename><surname>Koppang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Middle East Critique</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="117" to="143" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Propaganda technique in the world war</title>
		<author>
			<persName><forename type="first">Harold</forename><surname>Dwight</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lasswell</forename></persName>
		</author>
		<imprint>
			<date type="published" when="1938">1938</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">An NLP analysis of exaggerated claims in science news</title>
		<author>
			<persName><forename type="first">Yingya</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jieke</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bei</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 EMNLP Workshop: Natural Language Processing meets Journalism</title>
		<meeting>the 2017 EMNLP Workshop: Natural Language Processing meets Journalism<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017-09">2017. September</date>
			<biblScope unit="page" from="106" to="111" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">RoBERTa: A Robustly Optimized BERT Pretraining Approach</title>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno>eprint: 1907.11692</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Persuasion of the undecided: Language vs. the listener</title>
		<author>
			<persName><forename type="first">Liane</forename><surname>Longpre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Esin</forename><surname>Durmus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th Workshop on Argument Mining</title>
		<meeting>the 6th Workshop on Argument Mining<address><addrLine>Florence, Italy, August</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="167" to="176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Divisive language and propaganda detection using multi-head attention transformers with deep learning BERT-based language models for binary classification</title>
		<author>
			<persName><forename type="first">Norman</forename><surname>Mapes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anna</forename><surname>White</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Radhika</forename><surname>Medury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sumeet</forename><surname>Dua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second Workshop on Natural Language Processing for Internet Freedom: Censorship, Disinformation, and Propaganda</title>
		<meeting>the Second Workshop on Natural Language Processing for Internet Freedom: Censorship, Disinformation, and Propaganda<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-11">2019. November</date>
			<biblScope unit="page" from="103" to="106" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Extracting and networking emotions in extremist propaganda</title>
		<author>
			<persName><forename type="first">Travis</forename><surname>Morris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Intelligence and Security Informatics Conference</title>
		<imprint>
			<date type="published" when="2012">2012. 2012</date>
			<biblScope unit="page" from="53" to="59" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A computational exploration of exaggeration</title>
		<author>
			<persName><forename type="first">Enrica</forename><surname>Troiano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlo</forename><surname>Strapparava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gözde</forename><surname>Özbal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Serra</forename><surname>Sinem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tekiroglu</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018-10">2018. October-November</date>
			<biblScope unit="page" from="3296" to="3304" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Don&apos;t discuss&quot;: Investigating Semantic and Argumentative Features for Supervised Propagandist Message Detection and Classification</title>
		<author>
			<persName><forename type="first">Vorakit</forename><surname>Vorakitphan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elena</forename><surname>Cabrio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Serena</forename><surname>Villata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Recent Advances in Natural Language Processing (RANLP 2021)</title>
		<meeting><address><addrLine>Varna (Online), Bulgaria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note>September</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Fine-grained propaganda detection with fine-tuned BERT</title>
		<author>
			<persName><forename type="first">Shehel</forename><surname>Yoosuf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yin</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second Workshop on Natural Language Processing for Internet Freedom: Censorship, Disinformation, and Propaganda</title>
		<meeting>the Second Workshop on Natural Language Processing for Internet Freedom: Censorship, Disinformation, and Propaganda<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-11">2019. November</date>
			<biblScope unit="page" from="87" to="91" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
