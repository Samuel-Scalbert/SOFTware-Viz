<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Fair NLP Models with Differentially Private Text Encoders</title>
				<funder ref="#_83dgPWy">
					<orgName type="full">Agence Nationale de la Recherche</orgName>
					<orgName type="abbreviated">ANR</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-05-12">12 May 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Gaurav</forename><surname>Maheshwari</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">UMR 9189</orgName>
								<orgName type="institution" key="instit1">Univ. Lille</orgName>
								<orgName type="institution" key="instit2">Inria</orgName>
								<orgName type="institution" key="instit3">CNRS</orgName>
								<orgName type="institution" key="instit4">Centrale Lille</orgName>
								<address>
									<addrLine>-CRIStAL</addrLine>
									<postCode>F-59000</postCode>
									<settlement>Lille</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Pascal</forename><surname>Denis</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">UMR 9189</orgName>
								<orgName type="institution" key="instit1">Univ. Lille</orgName>
								<orgName type="institution" key="instit2">Inria</orgName>
								<orgName type="institution" key="instit3">CNRS</orgName>
								<orgName type="institution" key="instit4">Centrale Lille</orgName>
								<address>
									<addrLine>-CRIStAL</addrLine>
									<postCode>F-59000</postCode>
									<settlement>Lille</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Mikaela</forename><surname>Keller</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">UMR 9189</orgName>
								<orgName type="institution" key="instit1">Univ. Lille</orgName>
								<orgName type="institution" key="instit2">Inria</orgName>
								<orgName type="institution" key="instit3">CNRS</orgName>
								<orgName type="institution" key="instit4">Centrale Lille</orgName>
								<address>
									<addrLine>-CRIStAL</addrLine>
									<postCode>F-59000</postCode>
									<settlement>Lille</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Aur√©lien</forename><surname>Bellet</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">UMR 9189</orgName>
								<orgName type="institution" key="instit1">Univ. Lille</orgName>
								<orgName type="institution" key="instit2">Inria</orgName>
								<orgName type="institution" key="instit3">CNRS</orgName>
								<orgName type="institution" key="instit4">Centrale Lille</orgName>
								<address>
									<addrLine>-CRIStAL</addrLine>
									<postCode>F-59000</postCode>
									<settlement>Lille</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Fair NLP Models with Differentially Private Text Encoders</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-05-12">12 May 2022</date>
						</imprint>
					</monogr>
					<idno type="MD5">C2C45E8F62FA06BB7A594D1760EBA335</idno>
					<idno type="arXiv">arXiv:2205.06135v1[cs.CL]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-04-12T14:46+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Encoded text representations often capture sensitive attributes about individuals (e.g., race or gender), which raise privacy concerns and can make downstream models unfair to certain groups. In this work, we propose FEDERATE, an approach that combines ideas from differential privacy and adversarial training to learn private text representations which also induces fairer models. We empirically evaluate the trade-off between the privacy of the representations and the fairness and accuracy of the downstream model on four NLP datasets. Our results show that FEDERATE consistently improves upon previous methods, and thus suggest that privacy and fairness can positively reinforce each other.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Algorithmically-driven decision-making systems raise fairness concerns <ref type="bibr" target="#b41">(Raghavan et al., 2020;</ref><ref type="bibr" target="#b47">van den Broek et al., 2019)</ref> as they can be discriminatory against specific groups of people. These systems have also been shown to leak sensitive information about the data of individuals used for training or inference, and thus pose privacy risks <ref type="bibr" target="#b45">(Shokri et al., 2017)</ref>. Societal pressure as well as recent regulations push for enforcing both privacy and fairness in real-world deployments, which is challenging as these notions are multi-faceted concepts that need to be tailored to the context. Moreover, privacy and fairness can be at odds with one another: recent studies have shown that preventing a model from leaking information about its training data negatively impacts the fairness of the model and vice versa <ref type="bibr" target="#b2">(Bagdasaryan et al., 2019;</ref><ref type="bibr" target="#b40">Pujol et al., 2020;</ref><ref type="bibr" target="#b8">Cummings et al., 2019;</ref><ref type="bibr" target="#b5">Chang and Shokri, 2020)</ref>.</p><p>This paper studies fairness and privacy and their interplay in the NLP context, where these two notions have often been considered independently from one another. Modern NLP heavily relies on learning or fine-tuning encoded representations of text. Unfortunately, such representations often leak sensitive attributes (e.g., gender, race, or age) present explicitly or implicitly in the input text, even when such attributes are known to be irrelevant to the task <ref type="bibr" target="#b46">(Song and Raghunathan, 2020)</ref>. Moreover, the presence of such information in the representations may lead to unfair downstream models, as has been shown on various NLP tasks such as occupation prediction from text bios <ref type="bibr" target="#b9">(De-Arteaga et al., 2019)</ref>, coreference resolution <ref type="bibr" target="#b52">(Zhao et al., 2018)</ref>, or sentiment analysis <ref type="bibr" target="#b27">(Kiritchenko and Mohammad, 2018)</ref>.</p><p>Privatizing encoded representations is thus an important, yet challenging problem for which existing approaches based on subspace projection <ref type="bibr" target="#b4">(Bolukbasi et al., 2016;</ref><ref type="bibr" target="#b49">Wang et al., 2020;</ref><ref type="bibr" target="#b25">Karve et al., 2019;</ref><ref type="bibr" target="#b42">Ravfogel et al., 2020)</ref> or adversarial learning <ref type="bibr" target="#b32">(Li et al., 2018;</ref><ref type="bibr" target="#b7">Coavoux et al., 2018;</ref><ref type="bibr" target="#b22">Han et al., 2021)</ref> do not provide a satisfactory solution. In particular, these methods lack any formal privacy guarantee, and it has been shown that an adversary can still recover sensitive attributes from the resulting representations with high accuracy <ref type="bibr" target="#b15">(Elazar and Goldberg, 2018;</ref><ref type="bibr" target="#b20">Gonen and Goldberg, 2019)</ref>.</p><p>Instead of relying on adversarial learning to prevent attribute leakage, <ref type="bibr" target="#b37">Lyu et al. (2020)</ref>; <ref type="bibr" target="#b39">Plant et al. (2021)</ref> recently propose to add random noise to text representations so as to satisfy differential privacy (DP), a mathematical definition which comes with rigorous guarantees <ref type="bibr" target="#b13">(Dwork et al., 2006)</ref>. However, we uncover a critical error in their privacy analysis which drastically weakens their privacy claims. Moreover, their approach harms accuracy and fairness compared to adversarial learning.</p><p>In this work, we propose a novel approach (called FEDERATE) to learn private text representations and fair models by combining ideas from DP with an adversarial training mechanism. More specifically, we propose a flexible end-to-end architecture in which (i) the output of an arbitrary text encoder is normalized and perturbed using random noise to make the resulting encoder differentially private, and (ii) on top of the encoder, we combine a classifier branch with an adversarial branch to actively induce fairness, improve accuracy and further hide specific sensitive attributes.</p><p>We empirically evaluate the privacy-fairnessaccuracy trade-offs achieved by FEDERATE over four datasets and find that it simultaneously leads to more private representations and fairer models than state-of-the-art methods while maintaining comparable accuracy. Beyond the superiority of our approach, our results bring valuable insights on the complementarity of DP and adversarial learning and the compatibility of privacy and fairness. On the one hand, DP drastically reduces undesired leakage from adversarially trained representations, and has a stabilizing effect on the training dynamics of adversarial learning. On the other hand, adversarial learning improves the accuracy and fairness of models trained over DP text representations.</p><p>Our main contributions are as follows:</p><p>‚Ä¢ We propose a new approach, FEDERATE, which combines a DP encoder with adversarial learning to learn fair and accurate models from private representations.</p><p>‚Ä¢ We identify and fix (with a formal proof) a critical mistake in the privacy analysis of previous work on learning DP text representations.</p><p>‚Ä¢ We empirically show that FEDERATE leads to more private representations and fairer models than state-of-the-art methods while maintaining comparable accuracy.</p><p>‚Ä¢ Unlike previous studies, our empirical results suggest that privacy and fairness are compatible in our setting, and even mutually reinforce each other.</p><p>The paper is organized as follows. Section 2 provides background on differential privacy. Section 3 presents our approach. Section 4 reviews related work. Experimental results and conclusions are given in Sections 5 and 6.</p><p>2 Background: Differential Privacy Differential Privacy (DP) <ref type="bibr" target="#b13">(Dwork et al., 2006)</ref> provides a rigorous mathematical definition of the privacy leakage associated with an algorithm. It does not depend on assumptions about the attacker's capabilities and comes with a powerful algorithmic framework. For these reasons, it has become a de-facto standard in privacy currently used by the US Census Bureau (Abowd, 2018) and several big tech companies <ref type="bibr" target="#b16">(Erlingsson et al., 2014;</ref><ref type="bibr" target="#b17">Fanti et al., 2016;</ref><ref type="bibr" target="#b11">Ding et al., 2017)</ref>. This section gives a brief overview of DP, focusing on the aspects needed to understand our approach (see <ref type="bibr" target="#b14">Dwork and Roth (2014)</ref> for an in-depth review of DP).</p><p>Over the last few years, two main models for DP have emerged: (i) Central DP (CDP) <ref type="bibr" target="#b13">(Dwork et al., 2006)</ref>, where raw user data is collected and processed by a trusted curator, which then releases the result of the computation to a third party or the public, and (ii) Local DP (LDP) <ref type="bibr" target="#b26">(Kasiviswanathan et al., 2011)</ref> which removes the need for a trusted curator by having each user locally perturb their data before sharing it. Our work aims to create an encoder that leads to a private embedding of an input text, which can then be shared with an untrusted curator for learning or inference. We thus consider LDP, defined as follows. Definition 1 (Local Differential Privacy). A randomized algorithm M : X ‚Üí O is -differentially private if for all pairs of inputs x, x ‚àà X and all possible outputs o ‚àà O:</p><formula xml:id="formula_0">Pr[M (x) = o] ‚â§ e Pr[M (x ) = o].</formula><p>(1)</p><p>LDP ensures that the probability of observing a particular output o of M should not depend too much on whether the input is x or x . The strength of privacy is controlled by , which bounds the log-ratio of these probabilities for any x, x . Setting = 0 corresponds to perfect privacy, while ‚Üí ‚àû does not provide any privacy guarantees (as one may be able to uniquely associate an observed output to a particular input). In our approach described in Section 3, x will be an input text and M will be an encoding function which transforms x into a private vector representation that can be safely shared with untrusted parties.</p><p>Laplace mechanism. As clearly seen from Definition 1, an algorithm needs to be randomized to satisfy DP. A classical approach to achieve -DP for vector data is the Laplace mechanism <ref type="bibr" target="#b13">(Dwork et al., 2006)</ref>. Given the desired privacy guarantee and an input vector x ‚àà R D , this mechanism adds centered Laplace noise Lap( ‚àÜ ) independently to each dimension of x. The noise scale ‚àÜ is calibrated to and the L1-sensitivity ‚àÜ of inputs:</p><formula xml:id="formula_1">‚àÜ = max x,x ‚ààX x -x 1 .</formula><p>(2)</p><p>In our work, we propose an architecture in which the Laplace mechanism is applied on top of a trainable encoder to get private representations of input texts, and is further combined with adversarial training to learn fair models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Approach</head><p>We consider a scenario similar to <ref type="bibr" target="#b7">Coavoux et al. (2018)</ref>, where a user locally encodes its input data (text) x into an intermediate representation E priv (x) which is then shared with an untrusted curator to predict the label y associated with x using a classifier C. Additionally, an attacker (which may be the untrusted curator or an eavesdropper) may observe the intermediate representation E priv (x) and try to infer some sensitive (discrete) attribute z about x (e.g., gender, race etc.). Our goal is to learn an encoder E priv and classifier C such that (i) the attacker performs poorly at inferring z from E priv (x), (ii) the classifier C(E priv (x)) is fair with respect to z according to some fairness metric, and (iii) C accurately predicts the label y.</p><p>To achieve the above goals we introduce FEDERATE (for Fair modEls with DiffERentiAlly private Text Encoders), which combines two components: a differentially private encoder and an adversarial branch. Figure <ref type="figure" target="#fig_0">1</ref> shows an overview of our proposed architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Differentially Private Encoder</head><p>We propose a generic private encoder construction E priv = priv ‚Ä¢ E composed of two main components. The first component E can be any encoder which maps the text input to some vector space of dimension D. It can be a pre-trained language model along with a few trainable layers, or it can be trained from scratch. The second component priv is a randomized mapping which transforms the encoded input to a differentially private representation. Given the desired privacy guarantee &gt; 0, this mapping is obtained by applying the Laplace mechanism (see Section 2) to a normalized version of the encoded representation E(x):</p><formula xml:id="formula_2">priv(E(x)) = E(x)/ E(x) 1 + ,<label>(3)</label></formula><p>where each entry of ‚àà R D is sampled independently from Lap( 2 ). We will prove that E priv = priv ‚Ä¢ E satisfies -DP in Section 3.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Adversarial Component</head><p>To improve the fairness of the downstream classifier C, we model the adversary by another classi- fier A which aims to predict z from the privately encoded input E priv (x). The encoder E priv is optimized to fool A while maximizing the accuracy of the downstream classifier C. Specifically, given Œª &gt; 0, we train E priv , C and A (parameterized by Œ∏ E , Œ∏ C , and Œ∏ A respectively) to optimize the following objective:</p><formula xml:id="formula_3">min Œ∏ E ,Œ∏ C max Œ∏ A L class (Œ∏ E , Œ∏ C ) -ŒªL adv (Œ∏ E , Œ∏ A ),<label>(4)</label></formula><p>where L class (Œ∏ E , Œ∏ C ) is the cross-entropy loss for the C ‚Ä¢ E priv branch and L adv (Œ∏ E , Œ∏ A ) is the crossentropy loss for the A ‚Ä¢ E priv branch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Training</head><p>We train the private encoder E priv and the classifier C from a set of public tuples (x, y, z) by optimizing (4) with backpropagation using a gradient reversal layer g Œª <ref type="bibr" target="#b19">(Ganin and Lempitsky, 2015)</ref>. The latter acts like an identity function in the forward pass but scales the gradients passed through it by -Œª in the backward pass. This results in E priv receiving opposite gradients to A. We give pseudo-code in Appendix A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Privacy Analysis</head><p>We show the following privacy guarantee.</p><p>Theorem 1. Our encoder E priv and the downstream predictions C ‚Ä¢ E priv satisfy -DP.</p><p>The proof is given in Appendix B. Theorem 1 shows that the encoded representations produced by E priv have provable privacy guarantees: in particular, it bounds the risk that the sensitive attribute z of a text x is leaked by E priv (x).<ref type="foot" target="#foot_0">1</ref> These privacy guarantees naturally extend to the downstream prediction C(E priv (x)) due to the post-processing properties of DP (see Appendix B for details).</p><p>Error in previous work. We found a critical error in the privacy analysis of previous work on differential private text encoders <ref type="bibr" target="#b37">(Lyu et al., 2020;</ref><ref type="bibr" target="#b39">Plant et al., 2021)</ref>. In a nutshell, they incorrectly state that normalizing each entry of the encoded representation in [0, 1] allows to bound the sensitivity of their representation by 1, while it can in fact be as large as D (the dimension of the representation). As a result, the privacy guarantees are dramatically weaker than what the authors claim: the values they report should be multiplied by D. In contrast, the L1 normalization we use in (3) ensures that the sensitivity of E is bounded by 2. We provide more details in Appendix C.</p><p>Interestingly, <ref type="bibr" target="#b21">Habernal (2021)</ref> recently identified an error in ADePT <ref type="bibr" target="#b29">(Krishna et al., 2021)</ref>, a differentially private auto-encoder for text rewriting. However, the error in ADePT is different from the one in <ref type="bibr" target="#b37">Lyu et al. (2020)</ref>; <ref type="bibr" target="#b39">Plant et al. (2021)</ref>: the problem with ADePT is that it calibrates the noise to L2 sensitivity, while the Laplace mechanism requires L1 sensitivity. These errors call for greater scrutiny of differential privacy-based approaches in NLP-our work contributes to this goal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Related Work</head><p>Adversarial learning. In order to improve model fairness or to prevent leaking sensitive attributes, several approaches employ adversarialbased training. For instance, <ref type="bibr" target="#b32">Li et al. (2018)</ref> propose to use a different adversary for each protected attribute, while <ref type="bibr" target="#b7">Coavoux et al. (2018)</ref> consider additional loss components to improve the privacyaccuracy trade-off of the learned representation. <ref type="bibr" target="#b22">Han et al. (2021)</ref> introduce multiple adversaries focusing on different aspects of the representation by encouraging orthogonality between pairs of adversaries. <ref type="bibr">Recently, Chowdhury et al. (2021)</ref> propose an adversarial scrubbing mechanism. However, they purely focus on information leakage, and not on fairness. Moreover, unlike our approach, these methods do not offer formal privacy guarantees. In fact, it has been observed that one can recover the sensitive attributes from the representations by training a post-hoc non linear classifier <ref type="bibr" target="#b15">(Elazar and Goldberg, 2018)</ref>. This is confirmed by our empirical results in Section 5.</p><p>Sub-space projection. A related line of work focuses on debiasing text representations using projection methods <ref type="bibr" target="#b4">(Bolukbasi et al., 2016;</ref><ref type="bibr" target="#b49">Wang et al., 2020;</ref><ref type="bibr" target="#b25">Karve et al., 2019)</ref>. The general approach involves identifying and removing a sub-space associated with sensitive attributes. However, they rely on a manual selection of words in the vocabulary which is difficult to generalize to new attributes. Furthermore, <ref type="bibr" target="#b20">Gonen and Goldberg (2019)</ref> showed that sensitive attributes still remain present even after applying these approaches.</p><p>Recently, <ref type="bibr" target="#b42">Ravfogel et al. (2020)</ref> propose Iterative Null space Projection (INLP). It involves iteratively training a linear classifier to predict sensitive attributes followed by projecting the representation on the classifier's null space. However, the method can only remove linear information from the representation. By leveraging DP, our approach provides robust guarantees that do not depend on the expressiveness of the adversary, thereby providing protection against a wider range of attacks.</p><p>DP and fairness. Recent work has studied the interplay between DP and (group) fairness in the setting where one seeks to prevent a model from leaking information about individual training points. Empirically, this is evaluated through membership inference attacks, where an attacker uses the model to determine whether a given data point was in the training set <ref type="bibr" target="#b45">(Shokri et al., 2017)</ref>. While <ref type="bibr" target="#b30">Kulynych et al. (2022)</ref> observed that DP reduces disparate vulnerability to such attacks, it has also been shown that DP can exacerbate unfairness <ref type="bibr" target="#b2">(Bagdasaryan et al., 2019;</ref><ref type="bibr" target="#b40">Pujol et al., 2020)</ref>. Conversely, <ref type="bibr" target="#b5">Chang and Shokri (2020)</ref> showed that enforcing a fair model leads to more privacy leakage for the unprivileged group. This tension between DP and fairness is further confirmed by a formal incompatibility result between -DP and fairness proved by <ref type="bibr" target="#b8">Cummings et al. (2019)</ref>, albeit in a restrictive setting. Some recent work attempts to train models under both DP and fairness constraints <ref type="bibr" target="#b8">(Cummings et al., 2019;</ref><ref type="bibr" target="#b51">Xu et al., 2020;</ref><ref type="bibr" target="#b33">Liu et al., 2020)</ref>, but this typically comes at the cost of enforcing weaker privacy guarantees for some groups. Finally, Jagielski et al. ( <ref type="formula">2019</ref>) train a fair model under DP constraints only for the sensitive attribute.</p><p>A fundamental difference between this line of work and our approach lies in the kind of privacy we provide. While the above approaches study (central) DP as a way to design algorithms which protect training points from membership inference attacks on the model, we construct a private encoder such that the encoded representation does not leak sensitive attributes of the input. Thus, unlike previous work, we provide privacy guarantees with respect to the model's intermediate representation for data unseen at training time, and empirically observe that in this case privacy and fairness are compatible and even mutually reinforce each other.</p><p>DP representations for NLP. In a setting similar to ours, <ref type="bibr" target="#b37">Lyu et al. (2020)</ref> propose to use DP to privatize model's intermediate representation. Unlike their method, we actively promote fairness by using an adversarial training mechanism, which leads to more private representations and fairer models in practice. Importantly, we also uncover a critical error in their privacy analysis (see Sec. 3.1).</p><p>Concurrent to and independently from our work, <ref type="bibr" target="#b39">Plant et al. (2021)</ref> propose an adversarial-driven DP training mechanism. However, they do not consider fairness, whereas we focus on enforcing both fairness and privacy. Moreover, their method has the same incorrect analysis as <ref type="bibr" target="#b37">Lyu et al. (2020)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>Recall that we are interested in approaches that are not only accurate but also fair and private at the same time. However, these three dimensions are not independent and are not straightforwardly amenable to a single evaluation metric. Thus, we present experiments aiming at (i) showcasing the privacy-fairness-accuracy tradeoffs of different approaches and then (ii) analyzing privacy-accuracy and fairness-accuracy tradeoffs separately. We begin by describing the datasets and the metrics.</p><p>Datasets. We consider 4 different datasets: (i) Twitter Sentiment (Blodgett et al., 2016) consists of 200k tweets annotated with a binary sentiment label and a binary "race" attribute corresponding to African American English (AAE) vs. Standard American English (SAE) speakers; (ii) Bias in Bios (De-Arteaga et al., 2019) consists of 393,423 textual biographies annotated with an occupation label (28 classes) and a binary gender attribute; (iii) CelebA <ref type="bibr" target="#b34">(Liu et al., 2015)</ref> is a binary classification dataset with a binary sensitive attribute (gender); (iv) Adult Income <ref type="bibr" target="#b28">(Kohavi, 1996)</ref> consists of 48,842 instances with binary sensitive at-tribute (gender). Our setup for the first two dataset is similar to <ref type="bibr" target="#b42">Ravfogel et al. (2020)</ref> and <ref type="bibr" target="#b22">Han et al. (2021)</ref>. Appendix D.2 provides detailed description of these datasets, including sizes, pre-processing, and the challenges they pose to privacy and fairness tasks. Due to lack of space, results and analyses for Adult Income and CelebA dataset are given in Appendix D.5, but note that they exhibit similar trends. The preprocessed versions of the datasets can be downloaded from this anonymized URL. <ref type="foot" target="#foot_1">2</ref>Fairness metrics. For Twitter Sentiment we report the True Positive Rate Gap (TPR-gap), which measures the true positive rate difference between the two sensitive groups (gender/race) and is closely related to the notion of equal opportunity. Formally, denoting by y ‚àà {0, 1} the ground truth binary label, ≈∑ the predicted label and z ‚àà {g, ¬¨g} the sensitive attribute, TPR-gap is defined as: TPR-gap = P g (≈∑ = 1|y = 1)-P ¬¨g (≈∑ = 1|y = 1).</p><p>For Bias in Bios, which has 28 classes, we follow <ref type="bibr" target="#b43">Romanov et al. (2019)</ref> and report the root mean square of TPR-gaps (GRMS) over all occupations y ‚àà O to obtain a single number:</p><formula xml:id="formula_4">GRMS = (1/|O|) y‚ààO (TPR-gap y ) 2 . (5)</formula><p>Privacy metrics. We report two metrics for privacy: (i) Leakage: the accuracy of a two-layer classifier which predicts the sensitive attribute from the encoded representation, and (ii) Minimum Description Length (MDL) <ref type="bibr" target="#b48">(Voita and Titov, 2020)</ref>, which quantifies the amount of "effort" required by such a classifier to achieve a certain accuracy. A higher MDL means that it is more difficult to retrieve the sensitive attribute from the representation. The metric depends on the dataset and the representation dimension, and thus cannot be compared across different datasets. We provide more details about these metrics in Sec. D.1.</p><p>Methods and model architectures. We compare FEDERATE to the following methods: (i) Adversarial implements standard adversarial learning <ref type="bibr" target="#b32">(Li et al., 2018)</ref>, which is equivalent to our approach without the priv layer, (ii) Adversarial + Multiple <ref type="bibr" target="#b22">(Han et al., 2021)</ref> implements multiple adversaries, (iii) INLP <ref type="bibr" target="#b42">(Ravfogel et al., 2020</ref>) is a subspace projection approach, and (iv) Noise learns DP text When RT is increased, we select models with potentially lower accuracy on the validation set but are more fair (lower TPR-gap). Our approach FEDERATE consistently achieves better accuracy-fairness-privacy trade-offs than its competitors across all RTs.</p><p>representations as proposed by <ref type="bibr" target="#b37">Lyu et al. (2020)</ref> but with corrected privacy analysis: this corresponds to our approach without the adversarial component. These methods have been described in details in Section 4 and their hyperparametrs in Appendix D.4. We also report the performance of two simple baselines: Random simply predicts a random label, and Unconstrained optimizes the classification performance without special consideration for privacy or fairness.</p><p>To provide a fair comparison, all methods use the same architecture for the encoder, the classifier and (when applicable) the adversarial branches. In order to evaluate across varying model complexities, we employ different architectures for the different datasets. For Twitter Sentiment, we follow the architecture employed by <ref type="bibr" target="#b22">Han et al. (2021)</ref>, while for Bias in Bios we use a deeper architecture. The exact architecture, hyperparameters, and their tuning details are provided in Appendix D.3-D.4. We implement FEDERATE in PyTorch <ref type="bibr" target="#b38">(Paszke et al., 2019)</ref>. Our implementation, training, and evaluation scripts are available here. 3</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Accuracy-Fairness-Privacy Trade-off</head><p>In this first set of experiments, we explore the tridimensional trade-off between accuracy, fairness, and privacy and the inherent tension between them. These metrics are potentially all equally important and represent different information on different scales. Thus, they cannot be trivially combined into a single metric. Moreover, this trade-off is influenced by the choice of method but also some of its hyperparameters (e.g., the value of and Œª in our approach). Previous studies <ref type="bibr" target="#b22">(Han et al., 2021;</ref><ref type="bibr" target="#b37">Lyu et al., 2020)</ref> essentially selected hyperparameter values that maximize validation accu-3 The work-in-progress version of the codebase is currently available at https://github.com/saist1993/ DPNLP.</p><p>racy, which may lead to undesirable or suboptimal trade-offs. For instance, we found that this strategy does not always induce a fairer model than the Unconstrained baseline, and that it is often possible to obtain significantly more fair models at a negligible cost in accuracy. Based on these observations, we propose to use a Relaxation Threshold (RT): instead of selecting the hyperparameters with highest validation accuracy Œ± * , we consider all models with accuracy in the range [Œ± * -RT, Œ± * ].</p><p>We then select the hyperparameters with best fairness score within that range. <ref type="foot" target="#foot_2">4</ref>Figure <ref type="figure" target="#fig_1">2</ref> presents the (validation) accuracy, fairness and privacy scores related to different RT for each method on Twitter Sentiment. The first thing to note is that FEDERATE achieves the best fairness and privacy results with accuracy higher or comparable to competing approaches. We also observe that setting RT= 0.0 (i.e., choosing the model with highest validation accuracy) leads to a significantly more unfair model in all approaches, while fairness generally improves with increasing RT. This improvement comes at a negligible or small cost in accuracy. In terms of privacy, we find no significant differences across RTs.</p><p>We now showcase detailed results with RT fixed to 1.0 which is found to provide good trade-offs for all approaches in Figure <ref type="figure" target="#fig_1">2</ref>, see Table <ref type="table">1a</ref> for Twitter Sentiment and Table <ref type="table">1b</ref> for Bias in Bios (and Appendix D.6 for additional results). For both datasets, we observe that all adversarial approaches induce a fairer model than Unconstrained or Noise, with FEDERATE performing best. In terms of accuracy, all adversarial approaches perform similarly on Twitter Sentiment. Interestingly, they achieve higher accu- Table <ref type="table">1</ref>: Test results on (a) Twitter Sentiment, and (b) Bias in Bios with fixed Relaxation Threshold of 1.0. Fairness is measured with TPR-Gap or GRMS (lower is better), while privacy is measured by Leakage (lower is better) and MDL (higher is better). The MDL achieved by Random gives an upper bound for that particular dataset. Results have been averaged over 5 different seeds. Our proposed FEDERATE approach is the only method which achieves high levels of both fairness and privacy while maintaining competitive accuracy.</p><p>racy than Unconstrained. We attribute this to a significant mismatch in the train and test distribution due to class imbalance. On Bias in Bios, we observe a small drop in accuracy of our proposed approach in comparison to Adversarial, albeit with a corresponding gain in fairness. We hypothesize that this is due to the choice of possible hyperparameters for FEDERATE (we did not consider very large values of which would recover Adversarial), meaning that FEDERATE pushes for more fairness (and privacy) at a potential cost of some accuracy. We explore the pairwise trade-offs (fairness-accuracy and privacy-accuracy) in more details in Section 5.2.</p><p>In terms of both privacy metrics, FEDERATE significantly outperforms all adversarial methods on both datasets. In fact, in line with previous studies <ref type="bibr" target="#b22">(Han et al., 2021)</ref>, the leakage and MDL of purely adversarial methods are similar to that of Unconstrained. On both datasets, Noise achieves slightly weaker privacy than FEDERATE with much worse accuracy and fairness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>FEDERATE also consistently outperforms INLP in all dimensions.</head><p>In summary, the results show that FEDERATE stands out as the only approach that can simultaneously induce a fairer model and make its representation private while maintaining high accuracy. Furthermore, these results empirically demonstrate that our measures of privacy and fairness are indeed compatible with one another and can even reinforce each other.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Pairwise Trade-offs</head><p>In the previous experiments, we explored the tridimensional trade-off and found FEDERATE to attain better trade-offs than all other methods. Here, we take a closer look at the pairwise fairnessaccuracy and privacy-accuracy trade-offs separately. We find that FEDERATE outperforms the Adversarial and Noise approach in their corresponding dimension, suggesting that FEDERATE is a better choice even for bidimensional trade-offs. This experiment also validates the superiority of combining adversarial learning and DP over using either approach alone.</p><p>Fairness-accuracy trade-off. We plot best validation fairness scores over different accuracy intervals for the two datasets in Figure <ref type="figure" target="#fig_2">3</ref>. The interval is denoted by its mean accuracy (i.e., <ref type="bibr">[71.5, 72.5</ref>] is represented by 72). We then find the corresponding best fairness score for the interval. We observe:</p><p>‚Ä¢ Better fairness-accuracy trade-off: FEDERATE provides better fairness than the Adversarial approach for almost all accuracy intervals. In the case of Bias in Bios, Adversarial is able to achieve higher accuracy (albeit with a loss in fairness). We note that this high accuracy regime can be matched by FEDERATE with a larger .</p><p>‚Ä¢ Smoother fairness-accuracy trade-off: Interestingly, FEDERATE enables a smoother exploration of the accuracy-fairness trade-off space than Adversarial. As adversarial models are notoriously difficult to train, this suggests that the introduction of DP noise has a stabilizing effect on the training dynamics of the adversarial component.</p><p>Privacy-accuracy trade-off. We plot privacy and accuracy with respect to , the parameter controlling the theoretical privacy level in Figure <ref type="figure" target="#fig_3">4</ref>.</p><p>In general, the value of correlates well with the empirical leakage. On Bias in Bios, FEDERATE and Noise are comparable in both accuracy and privacy. However, for Twitter Sentiment, our approach outperforms Noise in both accuracy and privacy for every . We hypothesize this difference in the accuracy to be a case of mismatch between train-test split, suggesting FEDERATE to be more robust to these distributional shifts. These observations suggest that FEDERATE either improves upon Noise in privacy-accuracy tradeoff or remains comparable. For completeness, we also present the same results as a table in Appendix D.6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion and Perspectives</head><p>We proposed a DP-driven adversarial learning approach for NLP. Through our experiments, we showed that our method simultaneously induces private representations and fair models, with a mutually reinforcing effect between privacy and fairness. We also find that our approach improves upon competitors on each dimension separately. While we focused on privatizing sensitive attributes like race or gender, our approach can be used to remove other types of unwanted information from text representations, such as tenses or POS tag information, which might not be relevant for certain NLP tasks.</p><p>A possible limitation of this work is that it not tailored to a specific definition of fairness like equal odds. Instead, it enforces fairness by removing certain protected information, which can correlate with specific fairness notions. Similarly, we do not provide any formal fairness guarantees for our method, as we do for privacy. In the future, we aim to investigate fairness methods that explicitly optimize for a specific fairness definition and explore other privacy threats (e.g., reconstruction attacks).</p><p>We provide the pseudo-code of the training procedure of FEDERATE in Algorithm 1. Note that the combination of Steps 2-3-4 corresponds to E priv in Sec. 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Proof of Theorem 1</head><p>Proof. We start by proving that our noisy encoder</p><formula xml:id="formula_5">E priv : X ‚Üí R D satisfies -DP. Recall that for any input text x ‚àà X E priv (x) = priv ‚Ä¢ E(x) = E(x)/ E(x) 1 + ,</formula><p>where each entry of ‚àà R D is sampled independently from Lap( <ref type="formula">2</ref>), the centered Laplace distribution with scale 2/ . Let ·∫º(x) = E(x)/ E(x) 1 . The L1 sensitivity of ·∫º is</p><formula xml:id="formula_6">‚àÜ ·∫º = max x,x ‚ààX ·∫º(x) -·∫º(x) 1 .</formula><p>Since for any x ‚àà X we have ·∫º(x) 1 = 1, the triangle inequality gives ‚àÜ ·∫º ‚â§ 2. The -DP guarantee then follows from the application of the Laplace mechanism <ref type="bibr" target="#b13">(Dwork et al., 2006)</ref>. </p><formula xml:id="formula_7">Pr[E priv (x) = e] Pr[E priv (x ) = e] = D d=1 p(e d -xd ) p(e d -xd ) (6) = D d=1 e -2 |e d -x d | e -2 |e d -x d | = e 2 D d=1 |e d -x d |-|e d -x d | ‚â§ e 2 D d=1 |x d -x d | (7) = e 2 x-x 1 ‚â§ e 2 ‚àÜ ·∫º = e ,<label>(8)</label></formula><p>where (6) follows from the independence of the noise across dimensions, (7) uses the triangle inequality, and (8) from the definition of ‚àÜ ·∫º and the fact that ‚àÜ ·∫º ‚â§ 2 as shown above.</p><p>The above inequality shows that E priv satisfies -DP as per Definition 1. The fact that C ‚Ä¢ E priv also satisfies -DP follows from the post-processing property of DP, which ensures that the composition of any function with an -DP algorithm also satisfies -DP <ref type="bibr" target="#b14">(Dwork and Roth, 2014)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Error in Privacy Analysis of Previous Work</head><p>As briefly mentioned in Section 4, we found a critical error in the differential privacy analysis made in previous work by <ref type="bibr" target="#b37">Lyu et al. (2020)</ref>. This error is then reproduced in subsequent work by <ref type="bibr" target="#b39">Plant et al. (2021)</ref>. In this section, we explain this error and its consequences for the formal privacy guarantees of these methods, and provide a correction.</p><p>Recall from Section 2 that to achieve -DP with the Laplace mechanism, one must calibrate the scale of the Laplace noise needed to the L1 sensitivity of the encoded representation (see Eq. 2). This sensitivity bounds the worst-case change in L1 norm for any two arbitrary encoded user inputs x and x of dimension D.</p><p>In order to bound the L1 sensitivity, Lyu et al. ( <ref type="formula">2020</ref>) and <ref type="bibr" target="#b39">Plant et al. (2021)</ref> propose to bound each entry of the encoded input x ‚àà R D in the [0, 1] range. Specifically, they normalize as follows:</p><p>x ‚Üê x -min(x)/(max(x) -min(x)),</p><p>where min(x) and max(x) are respectively the minimum and maximum values in the vector x. <ref type="bibr" target="#b37">Lyu et al. (2020)</ref> and <ref type="bibr" target="#b39">Plant et al. (2021)</ref> incorrectly claim that this allows to bound the L1 sensitivity by 1 and thus add Laplace noise of scale 1 . In fact, the sensitivity can be as large as D, as can be seen by considering the two inputs x = [0, 1, . . . , 1] D and x = [1, 0, . . . , 0] for which xx 1 = D. Therefore, to achieve -DP, the scale of the Laplace noise should be D (i.e., D times larger than what the authors use). As a consequence, the differential privacy provided by their method are D times worse than claimed by <ref type="bibr" target="#b37">Lyu et al. (2020)</ref> and <ref type="bibr" target="#b39">Plant et al. (2021)</ref>: the values they report should be multiplied by D, which leads to essentially void privacy guarantees. While <ref type="bibr" target="#b37">Lyu et al. (2020)</ref> claim to follow the approach of <ref type="bibr" target="#b44">Shokri and Shmatikov (2015)</ref>, they missed the fact that <ref type="bibr" target="#b44">Shokri and Shmatikov (2015)</ref> do account for multiple dimensions by scaling the noise to the number of entries (denoted by c in their paper) that are submitted to the server, see Algorithm 1: Training procedure of FEDERATE (one epoch).</p><p>Input: Model architecture composed of encoder E (parameterized by Œ∏ E ), classifier C (parameterized by Œ∏ C ), adversary A (parameterized by Œ∏ A ), loss function L Output: Trained model Data: Samples S={x i , y i , z i } m i=1 where x i is the input text, y i is the task label, and z i is the sensitive attribute. 1 for i ‚Üê 0 to m do // For each sample in the dataset. This can be batch too.</p><formula xml:id="formula_9">2 Encode: x i ‚Üê E(x i ) 3 Normalize: x i ‚Üê x i x i 1 4 Privatize: x i priv ‚Üê x i +</formula><p>, where each entry of the vector ‚àà R D is sampled independently from a centered Laplace distribution with scale 2 5 Adversarial prediction: ·∫ëi ‚Üê A(x i priv )</p><p>6</p><p>Update Œ∏ A by backpropagating the loss L(z i , ·∫ëi )</p><formula xml:id="formula_10">7</formula><p>Task classification: ≈∑i ‚Üê C(x i priv )</p><p>8</p><p>Update Œ∏ E and Œ∏ C by backpropagating the loss</p><formula xml:id="formula_11">L(y i , ≈∑i ) -Œª ‚Ä¢ L(z i , ·∫ëi )</formula><p>pseudo-code in Figure <ref type="figure" target="#fig_1">12</ref> of <ref type="bibr" target="#b44">Shokri and Shmatikov (2015)</ref>.</p><p>In contrast to <ref type="bibr" target="#b37">Lyu et al. (2020)</ref> and <ref type="bibr" target="#b39">Plant et al. (2021)</ref>, our normalization in Eq. 3 guarantees by design that the L1 sensitivity is bounded by 2. We provide a complete and self-contained proof of our privacy guarantees in Section B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Experiments</head><p>This section gives more information on the experimental setup and also provides additional results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.1 Privacy metric</head><p>Leakage: We compute the leakage using a sklearn's MLPClassifier. We use the validation set of the original dataset as the train and the test set of the original dataset as the test.</p><p>Minimum Description Length (MDL) is a information-theoretic probing measure which captures the strength of regularity in the data. In this work, we employ the online coding approach <ref type="bibr" target="#b48">(Voita and Titov, 2020)</ref> to calculate MDL. Online coding captures the regularity by characterizing the effort required to achieve a certain level of accuracy. Here, a portion of data is transmitted to the receiver at each step, which then uses all the data in the previous steps to understand the regularity in the current step. The regularity is obtained by training the model on the previously received data and then evaluating it on the current portion of the data.</p><p>Borrowing, the terminology from <ref type="bibr" target="#b48">Voita and Titov (2020)</ref>, consider a dataset D consisting of {(x 1 , y 1 ), ‚Ä¢ ‚Ä¢ ‚Ä¢ , (x n , y n )} pairs, where the x i 's are the data representation, and the y i 's are the task label. In our case, x i is the output of the encoder, and y i is the sensitive attribute associated with the underlying text. Following the standard information theory setting, consider a sender Alice who wants to transmit labels y 1:n = {y 1 ‚Ä¢ ‚Ä¢ ‚Ä¢ , y n } to a receiver Bob, and both of them have access to the data representation x 1:n = {x 1 ‚Ä¢ ‚Ä¢ ‚Ä¢ , x n }. In order to transmit labels y 1:n efficiently (as few bits possible), Alice encodes y 1:n using a model p(y|x). According to Shannon-Huffman code, the minimum bits required to transmit these labels losslessly is:</p><formula xml:id="formula_12">L p (y 1:n |x 1:n ) = - n i=1 log 2 p(y i |x i ).</formula><p>In the online coding setting of MDL, the labels are transmitted in blocks of n timesteps t 0 &lt; t 1 &lt; ‚Ä¢ ‚Ä¢ ‚Ä¢ t n . Alice starts by encoding y 1:t 1 with a uniform code, then both Alice and Bob learn a model p Œ∏ 1 (y|x) that predicts y from x using data {(x i , y i )} t1 i=1 . Alice then uses this model to communicate the next data block y t 1 :t 2 , and both learns a new model using larger chunk of data {(x i , y i )} t2 i=1 . This continues till the whole set of labels y 1:n is transmitted. The total code length required for transmission using this setting is given as:</p><formula xml:id="formula_13">L online (y 1:n |x 1:n ) = t 1 log 2 C- n-1 i=1 log 2 p Œ∏ i (y t i +1:t i |x t i +1:t i ). (<label>10</label></formula><formula xml:id="formula_14">)</formula><p>where y i ‚àà {1, 2, ‚Ä¢ ‚Ä¢ ‚Ä¢ , C}. In our case, the online code length L online (y 1:n |x 1:n ) is shorter, if it is easier for probing model to perform well with fewer training instances. This implies that the sensitive information is more easily available in the encoder's representation.</p><p>We compute MDL using sklearn's MLPClassifier at timesteps corresponding to 0.1%, 0.2%, 0.4%, 0.8%, 1.6%, 3.2%, 6.25%, 12.5%, 25%, 50% and 100% of each dataset as suggested by <ref type="bibr" target="#b48">Voita and Titov (2020)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2 Datasets</head><p>Twitter Sentiment <ref type="bibr" target="#b3">(Blodgett et al., 2016)</ref> consists of 200k tweets annotated with a binary sentiment label and a binary "race" attribute corresponding to African American English (AAE) vs. Standard American English (SAE) speakers. The initial representation of tweets are obtained from a Deepmoji encoder <ref type="bibr" target="#b18">(Felbo et al., 2017)</ref>. The dataset is evenly balanced with respect to the four sentimentrace subgroup combinations. To create bias in the training data, we follow <ref type="bibr" target="#b15">Elazar and Goldberg (2018)</ref> and change the race proportion in each sentiment class to have 40% AAE-happy, 10% AAEsad, 10% SAE-happy, and 40% SAE-sad. Test data remains balanced. This setup is particularly challenging regarding privacy and fairness, as the model may exploit the correlation between the protected attribute and the main class label, which is reinforced due to skewing. The mismatch between the train-test distribution is also relevant for our setup, where the system may be trained on publicly available datasets or collected via an opt-in policy and may therefore not closely resemble the test distribution. This dataset is made available for research purposes only. <ref type="foot" target="#foot_3">5</ref>Bias in Bios <ref type="bibr" target="#b9">(De-Arteaga et al., 2019)</ref> consists of 393,423 textual biographies annotated with an occupation label (28 classes) and a binary gender attribute. Similar to <ref type="bibr" target="#b42">Ravfogel et al. (2020)</ref>, we encode each biography with BERT <ref type="bibr" target="#b10">(Devlin et al., 2019)</ref>, using the last hidden state over the CLS token. We use the same train-valid-test split as <ref type="bibr" target="#b9">De-Arteaga et al. (2019)</ref>. As the dataset was collected by scrapping the web, it tends to reflect common gender stereotypes and contains explicit gender indicators (e.g., pronouns), making it more challenging to prevent models from relying on these gendered words. It is also more complex than Twitter Sentiment in terms of the number of classes. Dataset is released under MIT License. <ref type="foot" target="#foot_4">6</ref>CelebA <ref type="bibr" target="#b34">(Liu et al., 2015)</ref> consists of over 200,000 images of the human face, alongside with 40 binary attributes labels describing the content of the images. Following the standard setting as described in <ref type="bibr" target="#b35">(Lohaus et al., 2020)</ref>, we use 38 of these attributes as features, "Smiling" as the class label, and "Sex" as the sensitive attribute. We use 60% of the data as train, 20% as validation, and the remaining as the test split. The CelebA dataset is available for non-commercial research purposes. <ref type="foot" target="#foot_5">7</ref>Adult Income <ref type="bibr" target="#b28">(Kohavi, 1996)</ref> consists of a U.S. 1994 Census database segment and has 48842 instances with 14 features each. We apply the preprocessing as proposed by <ref type="bibr" target="#b50">(Wu et al., 2019)</ref> resulting in a total of 9 features for each instance. The objective is to predict whether a given data point earns more than fifty thousand U.S. dollars or less. We consider sex (binary) as the sensitive attribute. Like CelebA, We use 60% of the data as train, 20% as validation, and the remaining as the test split. The license of the dataset is unknown, however it is commonly used in several fairness papers and is avaialbe at <ref type="bibr" target="#b12">(Dua and Graff, 2017)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.3 Model Architecture</head><p>Twitter Sentiment. The encoder consists of two layers with ReLU activation and a fixed dropout of 0.1. The classifier is linear, and the adversarial branch consists of three layers. We use a fixed dropout of 0.1 in all the layers with ReLU activation, apart from the last layer.</p><p>Bias in Bios. The encoder consists of three layers and a fixed dropout of 0.1. The classifier also consists of three layers, and the adversarial branch consists of two layers. We use a fixed dropout of 0.1 in all the layers with ReLU activation, apart from the last layer.</p><p>In case of Adult Income and CelebA dataset we use the same model as for Twitter Sentiment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.4 Hyperparameters</head><p>For all our experiments, we use Adam optimizer with a learning rate of 0.001 and batch size of 2000. We give additional tuning details of the different methods below. A single experiment takes about 30 minutes to run on Intel Xenon CPU. We will also provide the PyTorch model description in the README of the source code for easier reproduction.</p><p>‚Ä¢ Adversarial: We perform a grid search over Œª varying it between 0.1 to 3.0 with an interval of 0.2. Moreover, following previous work <ref type="bibr" target="#b31">(Lample et al., 2017;</ref><ref type="bibr" target="#b1">Adi et al., 2019)</ref>, instead of a constant Œª, we increase it over the epochs using the update scheme Œª i = 2/(1 + e -p i ) -1, where p i is the scaled version of the epoch number. We also experimented with increasing the Œª linearly, as well as keeping it constant, but found the above update scheme to perform the best in various settings. We also use this scheme in all other adversarial approaches.</p><p>‚Ä¢ Adversarial + Multiple: Similar to Adversarial, we vary Œª between 0.1 to 3.0 with an interval of 0.2. Apart from Œª, Adversarial + Multiple has an additional hyperparameter Œª ort which corrresponds to the weight given to the orthogonality loss component. We vary Œª ort between 0.1 and 1.0. Here, we do a simultaneous grid search over Œª and Œª ort resulting in 150 runs for each seed. We fix the number of the adversary to three which is the same as the original implementation by <ref type="bibr" target="#b22">(Han et al., 2021)</ref>.</p><p>‚Ä¢ FEDERATE: In order to have comparable number of runs to Adversarial + Multiple, we experiments with following values: 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 20.0. Similar to above approach, we do a simultaneous grid search over Œª and resulting in 150 runs for each seed.</p><p>‚Ä¢ INLP: In the case of INLP, we always debias the representation after the penultimate classifier layer and before the final layer, which is consistent with the setting considered by the authors <ref type="bibr" target="#b42">(Ravfogel et al., 2020)</ref>. We also observe that this choice empirically led to the best results. We vary the number of iterations as a part of hyperparameter tuning. For Bias in Bios we vary the iterations between 15 and 45, while for Twitter Sentiment we vary between 2 to 7. We found that in case of Bias in Bios, performing less than 15 iterations resulted in the same behaviour as Unconstrained model over validation set while more than 45 iterations resulted in a random classifier. We observed the same in the Twitter Sentiment before 2 and after 7 iterations, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.5 Extended Evaluation</head><p>Tables 2-3 present detailed results on CelebA and Adult Income dataset respectively. In terms of fairness over both the datasets, we observe that adversarial-based approaches induce a more fair model than Unconstrained or Noise, with FEDERATE outperforming all other methods. Interestingly, unlike Twitter Sentiment and Bias in Bios, all approaches have comparable accuracy, including Noise and INLP. We believe this to be the case due to these datasets being relatively more challenging than CelebA and Adult Income. As observed previously, purely adversarial-based approaches leak significantly more information than the DP-based approaches in terms of privacy. We observe that Noise and INLP performs marginally better in privacy than FEDERATE; however, they suffer significantly in the fairness metric.</p><p>In fact, they induce fairness levels which are similar to Unconstrained.</p><p>Overall, the results show FEDERATE as the only viable choice to induce a fairer model and make its representation private while maintaining comparable accuracy. These observations are in line with previous experiments described in Sec. 5.1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.6 Additional Results</head><p>Tables 4-6 present detailed results on Twitter Sentiment with different relaxation thresholds, which were summarized in Figure <ref type="figure" target="#fig_1">2</ref>.</p><p>Table <ref type="table">7</ref> provides the detailed privacy-fairness results which were summarized in Figure <ref type="figure" target="#fig_3">4</ref>.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Overview of our FEDERATE approach. The text input x is transformed to E(x) ‚àà R D by the text encoder E. The encoded input is then made private by the privacy layer priv, which involves normalization and addition of Laplace noise. The resulting private representation E priv (x) ‚àà R D is then used by the main task classifier C. It also serves as input to the adversarial layer A which is connected to the main branch via a radient reversal layer g Œª . The light red boxes represent the Differentially Private Encoder (Sec. 3.1), and the light blue boxes represent the Adversarial component (Sec. 3.2).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure2: Validation accuracy, fairness and privacy of various approaches for different relaxation threshold (RT) (see Section 5.1) on Twitter Sentiment. When RT is increased, we select models with potentially lower accuracy on the validation set but are more fair (lower TPR-gap). Our approach FEDERATE consistently achieves better accuracy-fairness-privacy trade-offs than its competitors across all RTs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Fairness-accuracy trade-off on Twitter Sentiment (top) and Bias in Bios (bottom). A missing point means that the accuracy interval was not found within our hyperparameter search. FEDERATE provides better fairness across most accuracy intervals in comparison to Adversarial over both datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Privacy-accuracy trade-off on Twitter Sentiment (top) and Bias in Bios (bottom), with associated values of . FEDERATE gives lower leakage and better or comparable accuracy to Noise over both datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>.d.f. of Lap(2/ ). Consider two arbitrary input texts x, x ‚àà X and let x = ·∫º(x) ‚àà R D and x = ·∫º(x ) ‚àà R D be their normalized encoded representations. Then, for any possible encoded output e = (e 1 , . . . , e D ) ‚àà R D , we have:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Unconstrained 85.70 ¬± 0.21 12.25 ¬± 2.07 81.3 ¬± 0.89 67.82 ¬± 1.46 INLP 84.81 ¬± 0.47 12.69 ¬± 4.66 66.00 ¬± 1.32 100.17 ¬± 1.65 Noise 85.12 ¬± 0.47 12.49 ¬± 0.58 59.01 ¬± 0.65 103.93 ¬± 0.24 Test results on CelebA dataset with fixed Relaxation Threshold of 1.0. Fairness is measured by TPR-Gap (lower is better), while privacy is measured by Leakage (lower is better) and MDL (higher is better). The MDL achieved by Random gives an upper bound for that particular dataset. The results have been averaged over 5 different seeds.</figDesc><table><row><cell></cell><cell>Accuracy ‚Üë</cell><cell>TPR-gap ‚Üì</cell><cell>Leakage ‚Üì</cell><cell>MDL ‚Üë</cell></row><row><cell>Random</cell><cell>50.00 ¬± 0.00</cell><cell>0.00 ¬± 0.00</cell><cell cols="2">-104.64 ¬± 0.11</cell></row><row><cell>Adversarial</cell><cell>85.34 ¬± 0.22</cell><cell cols="2">7.83 ¬± 0.97 87.00 ¬± 2.22</cell><cell>46.61 ¬± 5.52</cell></row><row><cell cols="2">Adversarial + Multiple 84.92 ¬± 0.12</cell><cell cols="2">5.79 ¬± 1.44 84.38 ¬± 2.07</cell><cell>51.11 ¬± 4.06</cell></row><row><cell>FEDERATE</cell><cell>84.81 ¬± 0.34</cell><cell cols="2">2.68 ¬± 0.60 65.49 ¬± 3.48</cell><cell>98.53 ¬± 4.51</cell></row><row><cell>Method</cell><cell>Accuracy ‚Üë</cell><cell>TPR-gap ‚Üì</cell><cell>Leakage ‚Üì</cell><cell>MDL ‚Üë</cell></row><row><cell>Random</cell><cell>50.00 ¬± 0.00</cell><cell>0.00 ¬± 0.00</cell><cell cols="2">-20.15 ¬± 0.083</cell></row><row><cell>Unconstrained</cell><cell cols="2">83.41 ¬± 0.32 12.73 ¬± 7.17</cell><cell>78.19 ¬± 1.0</cell><cell>16.38 ¬± 0.46</cell></row><row><cell>INLP</cell><cell>83.11 ¬± 0.51</cell><cell cols="2">3.91 ¬± 2.43 74.54 ¬± 0.67</cell><cell>19.93 ¬± 0.35</cell></row><row><cell>Noise</cell><cell>82.87 ¬± 0.37</cell><cell cols="2">8.01 ¬± 1.18 68.12 ¬± 0.94</cell><cell>19.38 ¬± 0.33</cell></row><row><cell>Adversarial</cell><cell>83.14 ¬± 0.53</cell><cell>7.02 ¬± 3.31</cell><cell>78.2 ¬± 0.18</cell><cell>16.1 ¬± 0.36</cell></row><row><cell cols="2">Adversarial + Multiple 83.14 ¬± 0.25</cell><cell cols="2">3.55 ¬± 2.16 81.37 ¬± 0.98</cell><cell>13.5 ¬± 1.09</cell></row><row><cell>FEDERATE</cell><cell>82.29 ¬± 0.9</cell><cell cols="2">2.73 ¬± 2.18 70.25 ¬± 4.81</cell><cell>18.1 ¬± 2.79</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Test results on Adult Income dataset with fixed Relaxation Threshold of 1.0. Fairness is measured by TPR-Gap (lower is better), while privacy is measured by Leakage (lower is better) and MDL (higher is better). The MDL achieved by Random gives an upper bound for that particular dataset. The results have been averaged over 5 different seeds.</figDesc><table><row><cell>Method</cell><cell>Accuracy ‚Üë</cell><cell>TPR-gap ‚Üì</cell><cell>Leakage ‚Üì</cell></row><row><cell>Unconstrained</cell><cell cols="3">72.54 ¬± 0.57 27.17 ¬± 1.76 87.18 ¬± 0.32</cell></row><row><cell>Noise</cell><cell cols="3">71.87 ¬± 0.56 25.14 ¬± 3.47 71.75 ¬± 2.99</cell></row><row><cell>Adversarial</cell><cell>75.49 ¬± 0.71</cell><cell cols="2">8.47 ¬± 3.5 88.03 ¬± 0.24</cell></row><row><cell>Adversarial + Multiple</cell><cell>75.6 ¬± 0.53</cell><cell cols="2">7.74 ¬± 4.17 88.01 ¬± 0.28</cell></row><row><cell>FEDERATE</cell><cell>75.34 ¬± 0.56</cell><cell cols="2">5.46 ¬± 3.59 62.31 ¬± 5.69</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Test set results on Twitter Sentiment dataset (scores averaged over 5 different seeds, RT=0.0).</figDesc><table><row><cell>Method</cell><cell>Accuracy ‚Üë</cell><cell>TPR-gap ‚Üì</cell><cell>Leakage ‚Üì</cell></row><row><cell>Unconstrained</cell><cell cols="3">70.57 ¬± 0.98 20.68 ¬± 0.99 82.91 ¬± 1.65</cell></row><row><cell>Noise</cell><cell cols="3">70.47 ¬± 0.43 19.84 ¬± 0.91 66.83 ¬± 3.32</cell></row><row><cell>Adversarial</cell><cell>74.09 ¬± 1.56</cell><cell cols="2">3.03 ¬± 2.65 88.14 ¬± 0.18</cell></row><row><cell cols="2">Adversarial + Multiple 74.44 ¬± 0.62</cell><cell cols="2">1.07 ¬± 0.74 87.98 ¬± 0.36</cell></row><row><cell>FEDERATE</cell><cell>74.24 ¬± 1.25</cell><cell cols="2">0.89 ¬± 0.46 61.92 ¬± 5.04</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Test set results on Twitter Sentiment dataset (scores averaged over 5 different seeds, RT=3.0).</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>More generally, the DP guarantee bounds the risk that any attribute of x is leaked through Epriv(x).</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>https://drive.google.com/uc?id= 1ZmUE-g6FmzPPbZyw3EOki7z4bpzbKGWk</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2"><p>We can also incorporate privacy into our hyperparameter selection strategy but, for the datasets and methods in our study, we found no significant change in Leakage across different hyperparameters.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_3"><p>http://slanglab.cs.umass.edu/ TwitterAAE/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_4"><p>https://github.com/Microsoft/biosbias</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_5"><p>https://mmlab.ie.cuhk.edu.hk/ projects/CelebA.html</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>The authors would like to thank the <rs type="funder">Agence Nationale de la Recherche</rs> for funding this work under grant number <rs type="grantNumber">ANR-19-CE23-0022</rs>, as well as the ARR reviewers for their feedback and suggestions.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_83dgPWy">
					<idno type="grant-number">ANR-19-CE23-0022</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">The us census bureau adopts differential privacy</title>
		<author>
			<persName><surname>John M Abowd</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2867" to="2867" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">To reverse the gradient or not: an empirical comparison of adversarial and multi-task learning in speech recognition</title>
		<author>
			<persName><forename type="first">Yossi</forename><surname>Adi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neil</forename><surname>Zeghidour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vitaliy</forename><surname>Liptchinsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICASSP.2019.8682468</idno>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP 2019</title>
		<meeting><address><addrLine>Brighton, United Kingdom</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019-05-12">2019. May 12-17, 2019</date>
			<biblScope unit="page" from="3742" to="3746" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Differential privacy has disparate impact on model accuracy</title>
		<author>
			<persName><forename type="first">Eugene</forename><surname>Bagdasaryan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omid</forename><surname>Poursaeed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vitaly</forename><surname>Shmatikov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019</title>
		<meeting><address><addrLine>NeurIPS; Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-12-08">2019. 2019. December 8-14, 2019</date>
			<biblScope unit="page" from="15453" to="15462" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Demographic dialectal variation in social media: A case study of African-American English</title>
		<author>
			<persName><forename type="first">Lin</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lisa</forename><surname>Blodgett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brendan O'</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName><surname>Connor</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D16-1120</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Austin, Texas</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1119" to="1130" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Man is to computer programmer as woman is to homemaker? debiasing word embeddings</title>
		<author>
			<persName><forename type="first">Tolga</forename><surname>Bolukbasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">Y</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Venkatesh</forename><surname>Saligrama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Tauman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kalai</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems 2016</title>
		<meeting><address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-12-05">2016. December 5-10, 2016</date>
			<biblScope unit="page" from="4349" to="4357" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">On the privacy risks of algorithmic fairness</title>
		<author>
			<persName><forename type="first">Hongyan</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Reza</forename><surname>Shokri</surname></persName>
		</author>
		<idno>CoRR, abs/2011.03731</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Adversarial scrubbing of demographic information for text classification</title>
		<author>
			<persName><forename type="first">Somnath</forename><surname>Basu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roy</forename><surname>Chowdhury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sayan</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junier</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shashank</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Snigdha</forename><surname>Chaturvedi</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.emnlp-main.43</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, Virtual Event</title>
		<meeting>the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, Virtual Event<address><addrLine>Punta Cana, Dominican Republic</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021-07-11">2021. 7-11 November, 2021</date>
			<biblScope unit="page" from="550" to="562" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Privacy-preserving neural representations of text</title>
		<author>
			<persName><forename type="first">Maximin</forename><surname>Coavoux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shashi</forename><surname>Narayan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shay</forename><forename type="middle">B</forename><surname>Cohen</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/d18-1001</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018-10-31">2018. October 31 -November 4, 2018</date>
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">On the compatibility of privacy and fairness</title>
		<author>
			<persName><forename type="first">Rachel</forename><surname>Cummings</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Varun</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dhamma</forename><surname>Kimpara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jamie</forename><surname>Morgenstern</surname></persName>
		</author>
		<idno type="DOI">10.1145/3314183.3323847</idno>
	</analytic>
	<monogr>
		<title level="m">Adjunct Publication of the 27th Conference on User Modeling, Adaptation and Personalization, UMAP 2019</title>
		<meeting><address><addrLine>Larnaca, Cyprus</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019-06-09">2019. June 09-12, 2019</date>
			<biblScope unit="page" from="309" to="315" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Bias in bios: A case study of semantic representation bias in a high-stakes setting</title>
		<author>
			<persName><forename type="first">Maria</forename><surname>De-Arteaga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Romanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanna</forename><forename type="middle">M</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jennifer</forename><forename type="middle">T</forename><surname>Chayes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Borgs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandra</forename><surname>Chouldechova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sahin</forename><surname>Cem Geyik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Krishnaram</forename><surname>Kenthapadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Tauman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kalai</forename></persName>
		</author>
		<idno type="DOI">10.1145/3287560.3287572</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Fairness, Accountability, and Transparency, FAT* 2019</title>
		<meeting>the Conference on Fairness, Accountability, and Transparency, FAT* 2019<address><addrLine>Atlanta, GA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019-01-29">2019. January 29-31, 2019</date>
			<biblScope unit="page" from="120" to="128" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">BERT: pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/n19-1423</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019<address><addrLine>Minneapolis, MN, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-06-02">2019. June 2-7, 2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
	<note>Long and Short Papers</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Collecting telemetry data privately</title>
		<author>
			<persName><forename type="first">Janardhan</forename><surname>Bolin Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName><surname>Yekhanin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">UCI machine learning repository</title>
		<author>
			<persName><forename type="first">Dheeru</forename><surname>Dua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Casey</forename><surname>Graff</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Calibrating noise to sensitivity in private data analysis</title>
		<author>
			<persName><forename type="first">Cynthia</forename><surname>Dwork</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Mcsherry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kobbi</forename><surname>Nissim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><forename type="middle">D</forename><surname>Smith</surname></persName>
		</author>
		<idno type="DOI">10.1007/11681878_14</idno>
	</analytic>
	<monogr>
		<title level="m">Theory of Cryptography, Third Theory of Cryptography Conference, TCC 2006</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006-03-04">2006. March 4-7, 2006</date>
			<biblScope unit="volume">3876</biblScope>
			<biblScope unit="page" from="265" to="284" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">The algorithmic foundations of differential privacy</title>
		<author>
			<persName><forename type="first">Cynthia</forename><surname>Dwork</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Roth</surname></persName>
		</author>
		<idno type="DOI">10.1561/0400000042</idno>
	</analytic>
	<monogr>
		<title level="j">Found. Trends Theor. Comput. Sci</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">3-4</biblScope>
			<biblScope unit="page" from="211" to="407" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Adversarial removal of demographic attributes from text data</title>
		<author>
			<persName><forename type="first">Yanai</forename><surname>Elazar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/d18-1002</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018-10-31">2018. October 31 -November 4, 2018</date>
			<biblScope unit="page" from="11" to="21" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Rappor: Randomized aggregatable privacy-preserving ordinal response</title>
		<author>
			<persName><forename type="first">√ölfar</forename><surname>Erlingsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vasyl</forename><surname>Pihur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksandra</forename><surname>Korolova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CCS</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Building a RAPPOR with the unknown: Privacypreserving learning of associations and data dictionaries</title>
		<author>
			<persName><forename type="first">Giulia</forename><surname>Fanti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vasyl</forename><surname>Pihur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">√ölfar</forename><surname>Erlingsson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">PoPETs</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Using millions of emoji occurrences to learn any-domain representations for detecting sentiment, emotion and sarcasm</title>
		<author>
			<persName><forename type="first">Bjarke</forename><surname>Felbo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><surname>Mislove</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anders</forename><surname>S√∏gaard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iyad</forename><surname>Rahwan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sune</forename><surname>Lehmann</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/d17-1169</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, EMNLP 2017</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing, EMNLP 2017<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017-09-09">2017. September 9-11, 2017</date>
			<biblScope unit="page" from="1615" to="1625" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation by backpropagation</title>
		<author>
			<persName><forename type="first">Yaroslav</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Machine Learning</title>
		<meeting>the 32nd International Conference on Machine Learning<address><addrLine>Lille, France</addrLine></address></meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="1180" to="1189" />
		</imprint>
	</monogr>
	<note>Proceedings of Machine Learning Research</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Lipstick on a pig: Debiasing methods cover up systematic gender biases in word embeddings but do not remove them</title>
		<author>
			<persName><forename type="first">Hila</forename><surname>Gonen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Workshop on Widening NLP</title>
		<meeting>the 2019 Workshop on Widening NLP<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="60" to="63" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">When differential privacy meets NLP: the devil is in the detail</title>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Habernal</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.emnlp-main.114</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, Virtual Event</title>
		<meeting>the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, Virtual Event<address><addrLine>Punta Cana, Dominican Republic</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021-07-11">2021. 7-11 November, 2021</date>
			<biblScope unit="page" from="1522" to="1528" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Diverse adversaries for mitigating bias in training</title>
		<author>
			<persName><forename type="first">Xudong</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><surname>Baldwin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Cohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, EACL 2021</title>
		<meeting>the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, EACL 2021<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021-04-19">2021. April 19 -23, 2021</date>
			<biblScope unit="page" from="2760" to="2765" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Differentially private fair learning</title>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Jagielski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Kearns</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jieming</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alina</forename><surname>Oprea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saeed</forename><surname>Sharifi-Malvajerdi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><forename type="middle">R</forename><surname>Ullman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International Conference on Machine Learning, ICML 2019</title>
		<meeting>the 36th International Conference on Machine Learning, ICML 2019<address><addrLine>Long Beach, California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-06">2019. 9-15 June 2019</date>
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="3000" to="3008" />
		</imprint>
	</monogr>
	<note>Proceedings of Machine Learning Research</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title/>
		<author>
			<persName><surname>Pmlr</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Conceptor debiasing of word representations evaluated on WEAT</title>
		<author>
			<persName><forename type="first">Saket</forename><surname>Karve</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lyle</forename><surname>Ungar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jo√£o</forename><surname>Sedoc</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W19-3806</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First Workshop on Gender Bias in Natural Language Processing</title>
		<meeting>the First Workshop on Gender Bias in Natural Language Processing<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="40" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">What can we learn privately?</title>
		<author>
			<persName><forename type="first">Shiva</forename><surname>Prasad Kasiviswanathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Homin</forename><forename type="middle">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kobbi</forename><surname>Nissim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sofya</forename><surname>Raskhodnikova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><forename type="middle">D</forename><surname>Smith</surname></persName>
		</author>
		<idno type="DOI">10.1137/090756090</idno>
	</analytic>
	<monogr>
		<title level="j">SIAM J. Comput</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="793" to="826" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Examining gender and race bias in two hundred sentiment analysis systems</title>
		<author>
			<persName><forename type="first">Svetlana</forename><surname>Kiritchenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saif</forename><surname>Mohammad</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/S18-2005</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventh Joint Conference on Lexical and Computational Semantics</title>
		<meeting>the Seventh Joint Conference on Lexical and Computational Semantics<address><addrLine>New Orleans, Louisiana</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="43" to="53" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Scaling up the accuracy of naivebayes classifiers: A decision-tree hybrid</title>
		<author>
			<persName><forename type="first">Ron</forename><surname>Kohavi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second International Conference on Knowledge Discovery and Data Mining (KDD-96)</title>
		<meeting>the Second International Conference on Knowledge Discovery and Data Mining (KDD-96)<address><addrLine>Portland, Oregon, USA</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="202" to="207" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">ADePT: Auto-encoder based differentially private text transformation</title>
		<author>
			<persName><forename type="first">Satyapriya</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rahul</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christophe</forename><surname>Dupuy</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.eacl-main.207</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume</title>
		<meeting>the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2435" to="2439" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Disparate vulnerability to membership inference attacks</title>
		<author>
			<persName><forename type="first">Bogdan</forename><surname>Kulynych</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Yaghini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Giovanni</forename><surname>Cherubin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Veale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carmela</forename><surname>Troncoso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PETS</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Fader networks: Manipulating images by sliding attributes</title>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neil</forename><surname>Zeghidour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ludovic</forename><surname>Denoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc'aurelio</forename><surname>Ranzato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017</title>
		<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-09">2017. December 4-9, 2017</date>
			<biblScope unit="page" from="5967" to="5976" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Towards robust and privacy-preserving text representations</title>
		<author>
			<persName><forename type="first">Yitong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><surname>Baldwin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Cohn</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P18-2005</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="25" to="30" />
		</imprint>
	</monogr>
	<note>Short Papers</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Fair differential privacy can mitigate the disparate impact on model accuracy</title>
		<author>
			<persName><forename type="first">Wenyan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangfeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xingjian</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junhong</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoling</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyuan</forename><surname>Zha</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Deep learning face attributes in the wild</title>
		<author>
			<persName><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICCV.2015.425</idno>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE International Conference on Computer Vision, ICCV 2015</title>
		<meeting><address><addrLine>Santiago, Chile</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2015-12-07">2015. December 7-13, 2015</date>
			<biblScope unit="page" from="3730" to="3738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Too relaxed to be fair</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Lohaus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Perrot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ulrike</forename><surname>Von</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luxburg</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th International Conference on Machine Learning</title>
		<meeting>the 37th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">119</biblScope>
			<biblScope unit="page" from="6360" to="6369" />
		</imprint>
	</monogr>
	<note>Proceedings of Machine Learning Research</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title/>
		<author>
			<persName><surname>Pmlr</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Differentially private representation for NLP: formal guar-antee and an empirical study on privacy and fairness</title>
		<author>
			<persName><forename type="first">Lingjuan</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuanli</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yitong</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.findings-emnlp.213</idno>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: EMNLP 2020, Online Event</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020-11">2020. November 2020</date>
			<biblScope unit="page" from="2355" to="2365" />
		</imprint>
	</monogr>
	<note>EMNLP 2020 of Findings of ACL</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Raison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alykhan</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sasank</forename><surname>Chilamkurthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benoit</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junjie</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">H</forename><surname>Wallach</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Beygelzimer</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>Alch√©-Buc</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><surname>Fox</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Garnett</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="8024" to="8035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<author>
			<persName><forename type="first">Richard</forename><surname>Plant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dimitra</forename><surname>Gkatzia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Valerio</forename><surname>Giuffrida</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.12318</idno>
		<title level="m">Cape: Context-aware private embeddings for private language learning</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Fair decision making using privacy-protected data</title>
		<author>
			<persName><forename type="first">David</forename><surname>Pujol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Mckenna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Satya</forename><surname>Kuppam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Hay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashwin</forename><surname>Machanavajjhala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gerome</forename><surname>Miklau</surname></persName>
		</author>
		<idno type="DOI">10.1145/3351095.3372872</idno>
	</analytic>
	<monogr>
		<title level="m">FAT* &apos;20: Conference on Fairness, Accountability, and Transparency</title>
		<meeting><address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2020-01-27">2020. January 27-30, 2020</date>
			<biblScope unit="page" from="189" to="199" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Mitigating bias in algorithmic hiring: evaluating claims and practices</title>
		<author>
			<persName><forename type="first">Manish</forename><surname>Raghavan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Solon</forename><surname>Barocas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jon</forename><forename type="middle">M</forename><surname>Kleinberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karen</forename><surname>Levy</surname></persName>
		</author>
		<idno type="DOI">10.1145/3351095.3372828</idno>
	</analytic>
	<monogr>
		<title level="m">FAT* &apos;20: Conference on Fairness, Accountability, and Transparency</title>
		<meeting><address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2020-01-27">2020. January 27-30, 2020</date>
			<biblScope unit="page" from="469" to="481" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Null it out: Guarding protected attributes by iterative nullspace projection</title>
		<author>
			<persName><forename type="first">Shauli</forename><surname>Ravfogel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanai</forename><surname>Elazar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hila</forename><surname>Gonen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Twiton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.647</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020-07-05">2020. July 5-10, 2020</date>
			<biblScope unit="page" from="7237" to="7256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">What&apos;s in a name? Reducing bias in bios without access to protected attributes</title>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Romanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maria</forename><surname>De-Arteaga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanna</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jennifer</forename><surname>Chayes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Borgs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandra</forename><surname>Chouldechova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sahin</forename><surname>Geyik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Krishnaram</forename><surname>Kenthapadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anna</forename><surname>Rumshisky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Kalai</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1424</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4187" to="4195" />
		</imprint>
	</monogr>
	<note>Long and Short Papers</note>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Privacypreserving deep learning</title>
		<author>
			<persName><forename type="first">Reza</forename><surname>Shokri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vitaly</forename><surname>Shmatikov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">CCS</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Membership inference attacks against machine learning models</title>
		<author>
			<persName><forename type="first">Reza</forename><surname>Shokri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Stronati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Congzheng</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vitaly</forename><surname>Shmatikov</surname></persName>
		</author>
		<idno type="DOI">10.1109/SP.2017.41</idno>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Symposium on Security and Privacy, SP 2017</title>
		<meeting><address><addrLine>San Jose, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2017-05-22">2017. May 22-26, 2017</date>
			<biblScope unit="page" from="3" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Information leakage in embedding models</title>
		<author>
			<persName><forename type="first">Congzheng</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ananth</forename><surname>Raghunathan</surname></persName>
		</author>
		<idno type="DOI">10.1145/3372297.3417270</idno>
	</analytic>
	<monogr>
		<title level="m">CCS &apos;20: 2020 ACM SIGSAC Conference on Computer and Communications Security</title>
		<meeting><address><addrLine>Virtual Event, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2020-11-09">2020. November 9-13, 2020</date>
			<biblScope unit="page" from="377" to="390" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Hiring algorithms: An ethnography of fairness in practice</title>
		<author>
			<persName><forename type="first">Elmira</forename><surname>Van Den Broek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anastasia</forename><surname>Sergeeva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marleen</forename><surname>Huysman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th International Conference on Information Systems</title>
		<meeting>the 40th International Conference on Information Systems<address><addrLine>ICIS; Munich, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Information Systems</publisher>
			<date type="published" when="2019-12-15">2019. 2019. December 15-18, 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Informationtheoretic probing with minimum description length</title>
		<author>
			<persName><forename type="first">Elena</forename><surname>Voita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Titov</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.14</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP 2020</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP 2020<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020-11-16">2020. November 16-20, 2020</date>
			<biblScope unit="page" from="183" to="196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Double-hard debias: Tailoring word embeddings for gender bias mitigation</title>
		<author>
			<persName><forename type="first">Tianlu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xi</forename><surname>Victoria Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nazneen</forename><surname>Fatema Rajani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Mccann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vicente</forename><surname>Ordonez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.484</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020-07-05">2020. July 5-10, 2020</date>
			<biblScope unit="page" from="5443" to="5453" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">On convexity and bounds of fairness-aware classification</title>
		<author>
			<persName><forename type="first">Yongkai</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xintao</forename><surname>Wu</surname></persName>
		</author>
		<idno type="DOI">10.1145/3308558.3313723</idno>
	</analytic>
	<monogr>
		<title level="m">The World Wide Web Conference, WWW 2019</title>
		<meeting><address><addrLine>San Francisco, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019-05-13">2019. May 13-17, 2019</date>
			<biblScope unit="page" from="3356" to="3362" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Removing disparate impact of differentially private stochastic gradient descent on model accuracy</title>
		<author>
			<persName><forename type="first">Depeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xintao</forename><surname>Wu</surname></persName>
		</author>
		<idno>CoRR, abs/2003.03699</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Gender bias in coreference resolution: Evaluation and debiasing methods</title>
		<author>
			<persName><forename type="first">Jieyu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianlu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Yatskar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vicente</forename><surname>Ordonez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai-Wei</forename><surname>Chang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/n18-2003</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT<address><addrLine>New Orleans, Louisiana, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018-06-01">2018. June 1-6, 2018</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="15" to="20" />
		</imprint>
	</monogr>
	<note>Short Papers</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
