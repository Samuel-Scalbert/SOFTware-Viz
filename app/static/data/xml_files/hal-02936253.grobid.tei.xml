<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Grammatical Evolution to Mine OWL Disjointness Axioms Involving Complex Concept Expressions</title>
				<funder ref="#_rTQBPEt">
					<orgName type="full">Agence Nationale de la Recherche</orgName>
					<orgName type="abbreviated">ANR</orgName>
				</funder>
				<funder ref="#_wne8yRm">
					<orgName type="full">French government</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Thu</forename><forename type="middle">Huong</forename><surname>Nguyen</surname></persName>
							<email>thu-huong.nguyen@univ-cotedazur.fr</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Université Côte d&apos;Azur</orgName>
								<orgName type="institution" key="instit2">Inria</orgName>
								<orgName type="institution" key="instit3">CNRS</orgName>
								<address>
									<settlement>I3S Nice</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Andrea</forename><forename type="middle">G B</forename><surname>Tettamanzi</surname></persName>
							<email>andrea.tettamanzi@univ-cotedazur.fr</email>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">Université Côte d&apos;Azur</orgName>
								<orgName type="institution" key="instit2">Inria</orgName>
								<orgName type="institution" key="instit3">CNRS</orgName>
								<address>
									<settlement>I3S Nice</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Grammatical Evolution to Mine OWL Disjointness Axioms Involving Complex Concept Expressions</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">D8ED71DFDF1DD0C9CCB0A7B365407187</idno>
					<idno type="DOI">10.1109/CEC48606.2020.9185681</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-04-12T14:50+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Ontology Learning</term>
					<term>OWL Axiom</term>
					<term>Disjointness Axiom</term>
					<term>Grammatical Evolution</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Discovering disjointness axioms is a very important task in ontology learning and knowledge base enrichment. To help overcome the knowledge-acquisition bottleneck, we propose a grammar-based genetic programming method for mining OWL class disjointness axioms from the Web of data. The effectiveness of the method is evaluated by sampling a large RDF dataset for training and testing the discovered axioms on the full dataset. First, we applied Grammatical Evolution to discover axioms based on a random sample of DBpedia, a large open knowledge graph consisting of billions of elementary assertions (RDF triples). Then, the discovered axioms are tested for accuracy on the whole DBpedia. We carried out experiments with different parameter settings and analyze output results as well as suggest extensions.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>There is a significant increase in research interest about detecting disjointness between concepts in knowledge bases. In terms of an ontology <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, viewed as a formal representation of a shared domain of knowledge, the incompatibility between pairs of concepts may be defined in the form of particular types of axioms, namely class disjointness axioms. An ontology can be defined as a quadruple O = C, R, I, A , where C is the set of concepts represented in the form of classes; R is the set of relations, i.e., properties or predicates between classes; I is the set of all assertions, i.e. instances, in which two or more concepts are related to each other; A is the set of axioms. Like other types of axioms, the class disjointness axioms are formalized in the form of logical assertions and play an essential role in enhancing and constraining the information contained in the ontology, thus allowing to check its correctness or derive new information. For example, if there is a constraint of disjointness between the two concepts Animal and FloweringPlant, a reasoner will be able to reveal an error in the modeling of a knowledge base whenever the class Animal is associated to a resource related to the class FloweringPlant. As a consequence, logical inconsistencies of facts can be detected and excluded-thus improving the quality of ontologies, called ontology enrichment.</p><p>On the other hand, the manual acquisition of axioms is exceedingly time-consuming and expensive because of the requirement of involving domain specialists and knowledge engineers. Therefore, instead of applying top-down approaches where axioms will be generated based on schema-level information built by domain experts, bottom-up approaches, should be adopted, whereby learning methods rely on instances from several existing knowledge and information resources to suggest axioms. These methods can go under the name axiom learning and can be considered as one task of ontology learning, which can help alleviate the overall cost of extracting axioms in general. Ontology learning comprises the sets of methods and machine learning techniques referring to the automatic discovery and creation of ontological knowledge from scratch or the enrichment or adaptation of an existing ontology in a semi-automatic fashion using several sources <ref type="bibr" target="#b2">[3]</ref>- <ref type="bibr" target="#b4">[5]</ref>.</p><p>One important point of the learning process is defining the type of input data sources. The use of dynamic data sources where the facts will be updated or changed in time is preferable, if one wants to achieve scalability and evolution, instead of only focusing on mostly small and uniform data collections. Such dynamic information can be extracted from various data resources on the Web, which constitute an open world of information. Indeed, the Web of data, also called the Semantic Web (SW, detailed in Section II), whose data model is the Resoure Data Framework (RDF) and whose the Linked Open Data (LOD) is a prominent implementation, has become a giant real-world data resource for learning axioms. The advantages of LOD with respect to learning described in <ref type="bibr" target="#b5">[6]</ref> is that it is publicly available, highly structured, relational, and large compared with other resources.</p><p>As a consequence of the general lack of class disjointness axioms in existing ontologies, learning implicit knowledge in terms of axioms from a LOD repository in the context of the Semantic Web has been the object of research using several different methods. Prominent research towards the automatic creation of class disjointness axioms from RDF facts include supervised classifiers in the LeDA system <ref type="bibr" target="#b6">[7]</ref>, statistical schema induction via associative rule mining in the GoldMiner system <ref type="bibr" target="#b7">[8]</ref>, learning general class descriptions (including disjointness) from training data based in the DL-Learner framework, as pointed out in <ref type="bibr" target="#b8">[9]</ref>. To these, we can add recent contributions relevant to class disjointness discovery. For instance, Reynaud et al. <ref type="bibr" target="#b9">[10]</ref> use Redescription Mining (RM) to learn class equivalence and disjointness axioms with the ReReMi algorithm. RM is about extracting a category definition in terms of a description shared by all the instances of a given class, i.e. equivalence axioms, and finding incompatible categories which do not share any instance, i.e. class disjointness axioms. Their method, based on Formal Concept Analysis (FCA), a mathematical framework mainly used for classification and knowledge discovery, aims at searching for data subsets with multiple descriptions, like different views of the same objects. While category redescriptions, i.e., equivalence axioms, refer to complex types, defined with the help of relational operators like A ≡ ∃r.C or A ≡ B ⊓ ∃r.C, in the case of incompatible categories, the redescriptions are only based on the set of attributes with the predicates of dct:subject, i.e. axioms involving atomic classes only.</p><p>Another procedure for extracting disjointness axioms <ref type="bibr" target="#b10">[11]</ref> requires a Terminological Cluster Tree (TCT) to search for a set of pairwise disjoint clusters. A decision tree is built and each node in it corresponds to a concept with a logical formula. The tree is traversed to create concept descriptions collecting the concepts installed in the leaf-nodes. Then, by exploring the paths from the root to the leaves, intensional definitions of disjoint concepts are derived. Two concept descriptions are disjoint if they lie on different leaf nodes. An important limitation of the method is the time-consuming and computationally expensive process of growing a TCT. A small change in the data can lead to a large change in the structure of the tree. Also, like other intensional methods, that work relies on the services of a reasoning component, but suffers from scalability problems for the application to large datasets, like the ones found on the LOD, caused by the excessive growth of the decision tree. In <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b11">[12]</ref>, we applied a heuristic method by using Grammatical Evolution (GE) to generate class disjointness axioms from an RDF repository. Extracted axioms include both atomic and complex axioms, i.e., defined with the help of relational operators of intersection and union; in other words, axioms like Dis(C 1 , C 2 ), where C 1 and C 2 are complex class expressions including ⊓ and ⊔ operators. The use of a grammar allows great flexibility: only the grammar needs to be changed to mine different data repositories for different types of axioms. However, the dependence on SPARQL endpoints (i.e., query engines) for testing mined axioms against facts, i.e. instances, in large RDF repositories limits the performance of the method. In addition, evaluating the effectiveness of the method requires the participation of experts in specific domains, i.e. the use of a Gold Standard, which is proportional to the number of concepts. Hence, the extracted axioms are limited to the classes relevant to a small scope of topics, namely the Work topic of DBpedia. <ref type="foot" target="#foot_0">1</ref> Also, complex axioms are defined with the help of relational operators of intersection and union, which can also be mechanically derived from the known atomic axioms.</p><p>Along the lines of extensional (i.e. instance-based) methods and expanding on the GE method proposed in <ref type="bibr" target="#b8">[9]</ref>, we propose a new approach to overcome its limitations as well as to enhance the diversity of discovered types of axioms. Specifically, a set of axioms with more diverse topics is generated from a small sample of an RDF dataset which is randomly extracted from the full RDF repository, more specifically, DBpedia. Also, the type of class disjointness axioms is extended to include the existential quantification ∃r.C and value restriction operators ∀r.C, where r is a property and C a class, which cannot be mechanically derived from a given set of atomic axioms. The grammar is updated to suit these changes. A set of candidate axioms is improved in the evolutionary process through the use of evolutionary operators of crossover and mutation. Finally, the final population of generated axioms is evaluated on the full RDF dataset, specifically the whole DBpedia, which can be considered as the objective benchmark eliminating the need of domain experts to evaluate the ability of generating axioms on a wider variety of topics. The evaluation of generated axioms in each generation of the evolutionary process is thus performed on a reasonably sized data sample, which alleviates the computational cost of query execution and enhances the performance of the method. Following <ref type="bibr" target="#b8">[9]</ref>, we apply a method based on possibility theory to score candidate axioms. It is important to mention that, to the best of our knowledge, no other method has been proposed so-far in the literature to mine the Web of data for class disjointness axioms involving complex class expressions with existential quantifications and value restrictions in addition to conjunctions.</p><p>The rest of the paper is organized as follows: some background notions are provided in Section II. The method to discover class disjointness axioms with a GE approach is presented in Section III. Section IV-C describes the experimental settings. Results are presented and analyzed in Section V. Conclusions and directions for future research are given in Section VI.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. PRELIMINARIES</head><p>The Semantic Web 2 (SW) is an extension of the World Wide Web (WWW) and it can be considered as the movement from the Web of documents to the Web of data. In fact, a huge amount of data on the Web is maintained in human-readable form only. The aim of the SW is to provide information in a structured form that machines too can understand. Machine-readable information combined with automated reasoning mechanisms can improve the capability of finding, retrieving, and exploiting much information not explicitly stated. For instance, only a few results of the thousands of matches typically returned by search engines carry truly relevant content. Some contents are hidden within the identified pages as well as classification and generalization of identifiers are irrelevant to the searching context. To solve this problem, semantic information containing machineprocessable information called metadata-a fundamental component of the SW-is embedded within Web content. Among the metadata, URIs (Uniform Resource Identifiers), defined in the RFC3986 standard 3 are used to identify abstract or physical resources. A URI consisting of a string of characters can be identified as a locator (URL-Uniform Resource Locator), a name (URN-Uniform Resource Name) or both. A URI that provides a means of locating the resource by describing its primary access mechanism is referred to as a URL. Meanwhile, a URI used as a URN refers to providing a globally unique name for a resource. Also, according to RFC3987, <ref type="foot" target="#foot_3">4</ref> an upgraded version of URIs are IRIs (International Resource Identifiers), which extend the ASCII characters of the URI version to a wide range of characters from the Universal Character Set (Unicode), including many special characters in different languages. Compared with the traditional WWW, we can summarize some differences in the structure of the SW. Specifically, the SW uses a common syntax for understandable machine statements, i.e., RDF statements, and common vocabularies for easy distribution and reuse. Also, the SW relies on a logical representation of metadata with decidable logical languages to make it possible for reasoners to deduce implicit information. The Resource Description Framework (RDF)<ref type="foot" target="#foot_4">5</ref>  <ref type="bibr" target="#b12">[13]</ref> is mainly a data model of the SW for describing machine-processable semantics of data. RDF uses as statements triples of the form subject predicate object . E.g., the content of the sentence "The 1997 film Titanic was directed by James Cameron" can be expressed in machine-accessible form as an RDF statement as follow: the subject is Film Titanic 1997; the predicate is hasDirector; the object is James Cameron. The statement can be described in the triple of IRIs and in the shorter representation associated with the prefix aliases, <ref type="foot" target="#foot_5">6</ref>PREFIX dbr: http://dbpedia.org/resource/ PREFIX dbo: http://dbpedia.org/ontology/ dbr:Titanic (1997 film) dbo:director dbr:James Cameron . The query language for RDF is SPARQL, <ref type="foot" target="#foot_6">7</ref> which can be used to express queries across diverse data sources, whether the data is stored natively as RDF or viewed as RDF via some middleware.</p><p>Linked Data (LD)<ref type="foot" target="#foot_7">8</ref> is a method to create a Web of Data, i.e., SW, by linking datasets to one another on the Web; in this case, we talk about RDF datasets. Linked Data comprises a set of principles for sharing machine-readable interlinked data on the Web as follows: HTTP URIs (or IRIs) are used to name things, so that these things can be looked up and linked to other things; useful information is provided in standard format such as RDF on look up.</p><p>Linked Open Data<ref type="foot" target="#foot_8">9</ref> (LOD) is an association of LD and Open Data where data can be linked while being freely available for sharing and reuse. One of the prominent representatives of the LOD is DBpedia, <ref type="foot" target="#foot_9">10</ref> which comprises a rather rich collection of facts extracted from Wikipedia.</p><p>DBpedia covers a broad variety of topics, which makes it a fascinating object of study for a knowledge extraction method. DBpedia owes to the collaborative nature of Wikipedia the characteristic of being incomplete and ridden with inconsistencies and errors. Also, the facts in DBpedia are dynamic, because they can change in time. DBpedia has become a giant repository of RDF triples and, therefore, it looks like a perfect testing ground for the automatic extraction of new knowledge. OWL<ref type="foot" target="#foot_10">11</ref> (Web Ontology Language) and RDFS <ref type="foot" target="#foot_11">12</ref>(RDF schema) are data modeling languages for describing RDF data. Nevertheless, OWL is much more expressive when it comes to the description of classes and properties. OWL not only comprises all the vocabulary from RDFS such as rdfs:subPropertyOf, rdfs:domain, rdfs:range but also provides further sophisticated terms to use in data modeling and reasoning. For example, OWL contains the constructors of complex class descriptions such as owl:UnionOf (⊔), owl:IntersectionOf (⊓), owl:ComplementOf (¬) and can express relations between class descriptions by means of class axioms such as rdfs:SubClassOf (⊑), owl:equivalentClass (≡), and owl:disjointWith. We are interested not only in extracting new knowledge from an existing knowledge base expressed in RDF, but also in being able to inject such extracted knowledge into an ontology in order to be able to exploit it to infer new logical consequences. While the former objective calls for a target language, used to express the extracted knowledge, which is as expressive as possible, lest we throttle our method, the latter objective requires using at most a decidable fragment of first-order logic and, possibly, a language which makes inference problems tractable. OWL strikes a good compromise between these two objectives. In addition, OWL is standardized and promotes interoperability with different applications. Furthermore, depending on the applications, it is possible to select an appropriate profile (corresponding to a different language fragment) exhibiting the desired trade-off between expressiveness and computational complexity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. GRAMMATICAL EVOLUTION FOR AXIOM LEARNING</head><p>We apply GE <ref type="bibr" target="#b13">[14]</ref> to detect disjointness between class expressions, i.e. class disjointness axioms, from a training RDF dataset. Class disjointness axioms here are phenotypes which are mapped from integer strings, i.e. genotypes, based on a given BNF grammar. An evolutionary process is performed on the population of candidate axioms to extract credible and general axioms. Then, the discovered axioms are evaluated on a test dataset. In this section, we first briefly present the grammar structure used for generating OWL class disjointness axioms, then we describe the main ingredients of the evolutionary process. After that, the possibilistic evaluation of the generated candidates is introduced.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. BNF Grammar Construction</head><p>The grammar for generating well-formed OWL class disjointness axioms 13 is designed based on the functional-style grammar in the extended BNF notation used by the W3C. 14  In the functional-style syntax of OWL, 15 class disjointness axioms have the form DisjointClasses(C 1 , C 2 , ..., C n ) where C 1 , C 2 ,...,C n are class expressions which can be atomic classes, i.e. single class identifiers or complex classes involving relational operators and possibly including more than one single class identifier. Like in <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b11">[12]</ref> and without loss of generality, we only consider the case of binary axioms such as DisjointClasses(C 1 , C 2 ) where C 1 and C 2 can be atomic or complex classes like DisjointClasses(Building, ObjectSomeValuesFrom(hasWings, Animals)). In addition, the structure of the BNF grammar here refers to mining well-formed axioms expressing the facts contained in a given RDF triple store. Hence, only resources that actually occur in the RDF dataset should be generated. We follow the approach proposed by <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b11">[12]</ref> to organize the structure of a BNF grammar which ensures that changes in the contents of RDF repositories will not require the grammar to be rewritten. Accordingly, the BNF grammar is split into a static and a dynamic part.</p><p>In the static part, the production rules define the types of axioms that need to be extracted and their syntax. The content of this part is loaded from a hand-crafted text file. Unlike <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b11">[12]</ref>, we specify it to mine only disjointness axioms involving at least one complex axiom, containing a relational operator of existential quantification ∃ or value restriction ∀, i.e., of the form ∃r.C or ∀r.C, where r is a property and C is an atomic class. The remaining class expression can be an atomic class or a complex class expression involving an operator out of ⊓, ∃ or ∀. The static part of the grammar is thus structured as follows:</p><p>The dynamic part contains production rules for the low-level non-terminals, called primitives in <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b11">[12]</ref>. These production rules are automatically filled at run-time by querying the SPARQL endpoint of the RDF data source at hand. The data source here is a training RDF dataset and the primitives are Class and ObjectPropertyOf.</p><p>The production rules for these two primitives are filled by SPARQL queries </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Evolutionary Process</head><p>Like in <ref type="bibr" target="#b8">[9]</ref>, our approach to axiom learning relies on a quite standard implementation of GE, whose pseudo-code is shown in Algorithm 1. In particular, we have adopted the reference implementation in the GEVA framework <ref type="bibr" target="#b14">[15]</ref>. In this section, we focus on the specific adaptations of the standard algorithm to the problem at hand. 2) Genotype-to-Phenotype Mapping: The standard genotype-to-phenotype mapping is used, with at most maxWrap wrapping events. In case of an unsuccessful mapping (because after the maximum allowed number of wrapping events the individual is not yet completely mapped), the individual is assigned a fitness of zero, i.e., the lowest possible fitness.</p><p>3) Parent selection: To prevent the loss of the best axioms due to the application of the variation operators, a small proportion pElite of elite individuals is first selected and directly copied into the next generation (line 7-8 of Algorithm 1). A candidate list for parent selection is established by removing the duplicates from the remaining part of the population to promote diversity. The parent selection mechanism is then carried out using truncation selection. In particular, the top proportion pselectSize of the distinct individuals in the candidate list is replicated until the size popSize of population is reached. The list of selected parents is shuffled and the individuals are paired in order from the beginning to the end of the list to undergo recombination.</p><p>4) Variant operators: Unlike in <ref type="bibr" target="#b8">[9]</ref>, where single-point crossover is applied to genotypes, we use the sub-tree crossover operators at the phenotypic level, with probability pCross. The standard mutation operators are also applied with probability pMut. 5) Survival selection: Like in <ref type="bibr" target="#b8">[9]</ref>, we use the Deterministic Crowding method of Mahfoud <ref type="bibr" target="#b15">[16]</ref> to improve the diversity of the population. However, there is an innovation in measuring the difference between two individuals, the DISTANCE function in Algorithm 2. Specifically, each offspring competes with its most similar peers, based on a phenotypic comparison instead of a genotypic one as in <ref type="bibr" target="#b8">[9]</ref>, to be selected for inclusion in the population of the next generation. The phenotypic distance between individuals is computed as their Levenshtein distance (Edit distance), with the expectation of obtaining more accurate results. </p><formula xml:id="formula_0">2: if(d1 &gt;d2) ListWinners[0]← COMPARE(parent1,child1) ListWinners[1]← COMPARE(parent2,child2) else ListWinners[0]← COMPARE(parent1,child2)</formula><p>ListWinners <ref type="bibr" target="#b0">[1]</ref>← COMPARE(parent2,child1) in which COMPARE(parent, child) -defines which individual in (parent,child) has higher fitness value. 3: return ListWinners 6) Determining the Fitness Value: We follow the evaluation framework, based on possibility theory, presented in <ref type="bibr" target="#b11">[12]</ref>, which was enhanced from <ref type="bibr" target="#b8">[9]</ref> to determine the fitness value of generated axioms in each generation, i.e. the credibility and generality of axioms. To make the paper self-contained, we recall the most important aspects of the approach.</p><p>The incompleteness and noise of some missing and erroneous facts (instances) in the RDF datasets as a result of the heterogeneous and collaborative characters of the Web of data justify adopting an axiom scoring heuristic based on possibility theory <ref type="bibr" target="#b16">[17]</ref>, which is well-suited to incomplete knowledge.</p><p>A candidate axiom φ is viewed as a hypothesis that has to be tested against the evidence contained in an RDF dataset. Its content φ is defined as a finite set of logical consequences</p><formula xml:id="formula_1">content(φ) = {ψ : φ |= ψ},<label>(1)</label></formula><p>obtained through the instantiation of φ to the vocabulary of the RDF repository; every ψ ∈ content(φ) may be readily tested by means of a SPARQL ASK query. The support of axiom φ, u φ is defined as the cardinality of content(φ). The support, together with the number of confirmations u + φ (i.e., the number of ψ for which the test is successful) and the number of counterexamples u - φ (i.e., the number of ψ for which the test is unsuccessful), are used to compute a degree of possibility Π(φ) for axiom φ, defined, for u(φ) &gt; 0, as</p><formula xml:id="formula_2">Π(φ) = 1 -1 - u φ -u - φ u φ 2 .</formula><p>Alongside Π(φ), the dual degree of necessity N (φ) could normally be defined. However, for reasons explained in <ref type="bibr" target="#b11">[12]</ref>, the necessity degree of a formula would not give any useful information for scoring class disjointness axioms against realworld RDF datasets. Possibility alone is a reliable measure of the credibility of a class disjointness axiom.</p><p>In terms of the generality scoring, an axiom is the more general the more facts are in the extension of its components. In <ref type="bibr" target="#b8">[9]</ref>, the generality of an axiom is defined as the cardinality of the sets of the facts in the RDF repository reflecting the support of each axioms, i.e. u φ . However, in case one of the components of an axiom is not supported by any fact, its generality will be zero. Hence, the generality of an axiom should be measured by the minimum of the cardinalities of the extensions of the two class expressions involved, i.e. For the above reasons, instead of the fitness function in <ref type="bibr" target="#b8">[9]</ref>,</p><formula xml:id="formula_3">f (φ) = u φ • Π(φ) + N (φ) 2 ,<label>(2)</label></formula><p>we resorted to the following improved definition, proposed in <ref type="bibr" target="#b11">[12]</ref>:</p><formula xml:id="formula_4">f (φ) = g φ • Π(φ).<label>(3)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTAL SETUP</head><p>In our experimental protocol, two phases are distinguished: (1) mining class disjointness axioms with the GE framework introduced in Section III from a training RDF dataset, i.e., a random sample of DBpedia 2015-04, and (2) testing the resulting axioms against the test dataset, i.e., the entire DBpedia 2015-04, which can be considered as an objective benchmark to evaluate the effectiveness of the method. Before diving into those details, we first describe how we further prepare the training dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Training Dataset Preparation</head><p>We randomly collect 1% of the RDF triples from DBpedia 2015-04 in English version (which contains 665,532,306 RDF triples) to create the Training Dataset (TD). <ref type="foot" target="#foot_12">16</ref> Specifically, a small linked dataset is generated where RDF triples are interlinked with each other and the number of RDF triples accounts for 1% of the triples of DBpedia corresponding to each type of resource, i.e. subjects and objects. A demonstration of this mechanism to extract the sample training dataset is illustrated in Fig. <ref type="figure" target="#fig_2">1</ref>. Let r be an initial resource for the extraction process, e.g., http://dbpedia.org/ontology/Plant; 1% of the RDF triples having r as subject, of the form r p r ′ , and 1% of the triples having r as object, of the form r ′′ p ′ r , will be randomly extracted from DBpedia. Then, the same will be done for every resource r ′ and r ′′ mentioned in the extracted triples, until the size of the training dataset reaches 1% of the size of DBpedia. If the number of triples to be extracted for a resource is less than 1 (following the 1% proportion), we round it to 1 triple.</p><p>We applied the proposed mechanism to extract a training dataset containing 6,739,240 connected RDF triples with a variety of topics from DBpedia. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Parameters Settings for GE runs</head><p>We use the BNF grammar introduced in Section III-A. Given how the grammar was constructed, the mapping of any chromosome of length ≥ 6 will always be successful. Hence, we can set maxWrap = 0.</p><p>In order to investigate the ability of the method to discover class disjointness axioms for different parameter settings, we ran our algorithm in 20 different runs for each of 4 distinct population sizes, namely 1,000; 2,000; 5,000 and 10,000 individuals, respectively. In addition, to make fair comparisons possible, a set of milestones of total effort k (defined as the total number of fitness evaluations) corresponding to each population size are also recorded for each run, namely 100,000; 200,000; 300,000 and 400,000, respectively. The maximum numbers of generations maxGenerations (used as the stopping criterion of the algorithm) are automatically  <ref type="table" target="#tab_2">I</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Performance Evaluation</head><p>We measure the performance of the method using the entire DBpedia 2015-04 as a test set, measuring possibility and generality for every distinct axiom discovered by our algorithm. To avoid overloading DBpedia's SPARQL endpoint, we set up a local mirror using the Virtuoso Universal Server. <ref type="foot" target="#foot_13">17</ref>V. RESULT ANALYSIS Running our method 20 times with the parameters shown in Table <ref type="table" target="#tab_2">I</ref> yielded the number of distinct axioms involving complex expressions listed in Fig. <ref type="figure" target="#fig_0">2</ref>. Together with the mandatory class expression containing the ∀ or ∃ operators, most extracted disjointness axioms contain an atomic class expression. This may be due to the fact that the support of atomic classes is usually larger than the support of a complex class expression. Table <ref type="table" target="#tab_2">II</ref> contains some examples of discovered axioms. Full results are available online. <ref type="foot" target="#foot_14">18</ref> The axioms are presented in short form using prefixes dbo: http://dbpedia.org/ontology and dbprop: http://dbpedia.org/property.</p><p>We also have statistically compared the number of distinct valid axioms, i.e., axioms φ such that Π(φ) &gt; 0 and g φ &gt; 0, discovered using different settings of popSize and total effort k. Overall, we can see a trend whereby the number of discovered axioms increases steadily during the early stage of evolution, i.e. for low values of k, before gradually decaying at the end of the process. This trend is most clearly visible when popSize = 2, 000 and popSize = 5, 000. This phenomenon may be due to the prevalence of exploration in the early phases of the evolutionary process, as opposed to exploitation, when the population, despite our efforts to preserve diversity, begins to converge towards few axioms with particularly high fitness. Depending on the population size, this may happen before reaching the first milestone of total effort k = 100, 000 (as it is the case for popSize = 1000) or in the generations following the last milestone, as one could expect to observe for popsize = 10, 000, if the evolutionary process were allowed to continue. For measuring the accuracy of our results, given that the discovered axioms come with an estimated degree of possibility, which is essentially a fuzzy degree of membership, we propose to use a fuzzy extension of the usual definition of   precision, based on the most widely used definition of fuzzy set cardinality, introduced in <ref type="bibr" target="#b17">[18]</ref> as follows: given a fuzzy set F defined on a countable universe set ∆,</p><formula xml:id="formula_5">F = x∈∆ F (x),<label>(4)</label></formula><p>In our case, we may view Π(φ) as the degree of membership of axiom φ in the (fuzzy) set of the "positive" axioms. The value of precision can thus be computed against the test dataset, i.e. The results in Table III confirm the high accuracy of our axiom discovery method with a precision ranging from 0.969 to 0.998 for all the different considered sizes of population and different numbers of generations (reflected through the values of total effort). We also see that the accuracy remains stable across different values of total effort k in the case of large populations like popSize = 10, 000, whilst there is an opposite trend in the case of smaller populations, where the values decrease slightly as the total effort k increases. This surprising behavior suggests that the method tends to overfit individuals in the population after a high number of generations (reflected by the values of total effort). This overfitting may be the only way to achieve higher fitness values (as computed against the training set), whereas the evaluated axioms actually turn out to be incorrect when evaluated against the test dataset, i.e the full DBpedia. We can witness this phenomenon more clearly from the plots illustrating the distribution of axioms in terms of possibility and generality shown in Table <ref type="table" target="#tab_5">IV</ref>. Even though most discovered axioms are highly possible (Π(φ) close to 1), the number of highly general axioms possessing a lower possibility increases slightly as total effort k increases. This suggests that the evolutionary process should be stopped early before axioms begin overfitting the training dataset. Indeed, with the same value of total effort, the larger populations, which correspond to a lower number of generations, as it is the case for popSize = 10, 000, allow the method to discover axioms that correctly generalize to the full DBpedia and the evidence of the precision values in Table III seems to confirm this hypothesis. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSION AND FUTURE WORK</head><p>In this work, we presented an extension of the use of GE to discover disjointness axioms involving complex class expressions. The study aims at mining axioms containing the relational operators of existential quantification ∃ and value restriction ∀ on a wide variety of topics from DBpedia. A training-testing approach is also implemented to solve current limitations of performance and obtain a fair and objective assessment of its accuracy. The experimental results show that the approach is highly accurate and competitive with related approaches.</p><p>As future work, we will focus on three directions: (1) approach axiom discovery as a two-objective optimization problem to treat axiom credibility and generality as two independent criteria; (2) mine complex axioms involving additional relational operators like owl:hasValue, owl:oneOf ; (3) take some complexity measurement of class expressions into account for evaluating the fitness of axioms.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Algorithm 2 -</head><label>2</label><figDesc>Crowding(parent1, parent2, offspring1, offspring2) Input: parent1, parent2, child 1, child 2: a crowd of individual axioms; Output: A: ListWinners-a list containing two winners of individual axioms 1:d1 ← DISTANCE(parent1,child1) +DISTANCE (parent2,child2) d2 ← DISTANCE(parent1, child2) + DISTANCE(parent2, child1)in which DISTANCE(parent, child) -the number of distinct codons between parent and child.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>g φ = min||[C]||, ||[D]|| where C, D are class expressions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. An illustration of the Training Dataset sampling procedure</figDesc><graphic coords="7,48.96,358.70,257.00,113.30" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>DBpedia 2015-04 according to the formula precision = true positives discovered axioms = φ Π DBpedia (φ) φ Π T D (φ) , (5) where Π T D and Π DBpedia are the possibility measures computed on the training dataset and DBpedia, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Number of axioms discovered over 20 runs.</figDesc><graphic coords="9,48.96,350.54,257.00,155.90" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>SELECT ?class WHERE { ?instance rdf:type ?class.} 13 https://www.w3.org/TR/owl2-syntax/#Disjoint Classes 14 https://www.w3.org/TR/owl2-syntax/ 15 https://www.w3.org/TR/owl2-syntax/#Functional-Style Syntax to extract atomic classes (represented by their IRI) and</figDesc><table><row><cell cols="4">SELECT ?property WHERE { ?subject ?property ?object.</cell></row><row><cell></cell><cell></cell><cell cols="2">FILTER (isIRI(?object))}</cell></row><row><cell cols="4">to extract properties (represented by their IRI) from the RDF</cell></row><row><cell cols="4">dataset. The following is an example representing a small</cell></row><row><cell>excerpt of an RDF dataset:</cell><cell></cell><cell></cell></row><row><cell cols="3">PREFIX dbr: http://dbpedia.org/resource/</cell></row><row><cell cols="3">PREFIX dbo: http://dbpedia.org/ontology/</cell></row><row><cell cols="3">PREFIX dbprop: http://dbpedia.org/property/</cell></row><row><cell cols="4">PREFIX rdf: http://www.w3.org/1999/02/22\-rdf-syntax-ns\#</cell></row><row><cell>dbr:Cavacoa</cell><cell cols="2">rdf:type</cell><cell>dbo:Plant.</cell></row><row><cell cols="3">dbr:Mussaenda_erythrophylla rdf:type</cell><cell>dbo:FloweringPlant.</cell></row><row><cell>dbr:The_Times</cell><cell cols="2">rdf:type</cell><cell>dbo:WrittenWork.</cell></row><row><cell>dbr:Chai_Ling</cell><cell cols="2">dbprop:spouse</cell><cell>dbr:Feng_Congde.</cell></row><row><cell>dbr:Revolution_Radio_Tour</cell><cell cols="2">dbprop:artist</cell><cell>dbr:Green_Day</cell></row><row><cell cols="4">and options for the Class and ObjectPropertyOf non-</cell></row><row><cell cols="2">terminals are represented as follows:</cell><cell></cell></row><row><cell>(r.9) Class := dbo:Plant</cell><cell></cell><cell>(0)</cell></row><row><cell cols="2">| dbo:FloweringPlant</cell><cell>(1)</cell></row><row><cell cols="2">| dbo:WrittenWork</cell><cell>(2)</cell></row><row><cell cols="3">(r.10) ObjectPropertyOf := dbprop:spouse</cell><cell>(0)</cell></row><row><cell cols="3">| dbprop:artist</cell><cell>(1)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Pop: a set of axioms discovered based on Gr 1: Initialize a list of chromosomes L of length initlenChrom. Initialization: The initial population is seeded with pop-Size random chromosomes of initlenChrom codons uniformly distributed over {0, . . . , maxValCodon -1}.</figDesc><table><row><cell>1)</cell><cell></cell></row><row><cell></cell><cell>Each codon value in chromosome are integer.</cell></row><row><cell cols="2">2: Create a population P of size popSize mapped from list of chromosomes L</cell></row><row><cell cols="2">on grammar Gr</cell></row><row><cell cols="2">3: Compute the fitness values for all axioms in Pop.</cell></row><row><cell cols="2">4: Initialize current generation number ( currentGeneration = 0 )</cell></row><row><cell cols="2">5: while( currentGeneration ¡ maxGenerations) do</cell></row><row><cell>6:</cell><cell>Sort Pop by descending fitness values</cell></row><row><cell>7:</cell><cell>Create a list of elite axioms listElites with the propotion pElite of the number</cell></row><row><cell></cell><cell>of the fittest axioms in Pop</cell></row><row><cell>8:</cell><cell>Add all axioms of listElites to a new population newPop</cell></row><row><cell>9:</cell><cell>Select the remaining part of population after elitism selection</cell></row><row><cell></cell><cell>Lr ← Pop\listElites</cell></row><row><cell>10:</cell><cell>Eliminate the duplicates in Lr</cell></row><row><cell></cell><cell>Lr ← Distinct (Lr)</cell></row><row><cell cols="2">11: Create a a list of axioms listCrossover used for crossover operation</cell></row><row><cell></cell><cell>with the propotion pselectSize of the number of</cell></row><row><cell></cell><cell>the fittest individuals in Lr</cell></row><row><cell cols="2">11: Shuffle(listCrossover)</cell></row><row><cell cols="2">12: for (i=0,1....listCrossover.length-2) do</cell></row><row><cell>10:</cell><cell>parent1 ← listCrossover[i]</cell></row><row><cell>13:</cell><cell>parent2 ← listCrossover[i+1]</cell></row><row><cell>14:</cell><cell>child1, child2 ← CROSSOVER(parent1,parent2) with the probability pCross</cell></row><row><cell>15:</cell><cell>for each offspring {child1,child2} do MUTATION(offspring)</cell></row><row><cell>16:</cell><cell>Compute fitness values for child1, child2</cell></row><row><cell>17:</cell><cell>Select w1, w2 -winners of competition between parents and offsprings</cell></row><row><cell></cell><cell>w1,w2 ← CROWDING((parent1, parent2, child1, child2)</cell></row><row><cell>18:</cell><cell>Add w1, w2 to new population newPop</cell></row><row><cell cols="2">19: Pop= newPop</cell></row><row><cell cols="2">20: Increase the number of generation curGeneration by 1</cell></row><row><cell cols="2">21: return Pop</cell></row></table><note><p>Algorithm 1 -GE for discovering axioms from an RDF datasets Input: T: RDF Triples data; Gr: BNF grammar; popSize: the size of the population; initlenChrom: the initialized length of chromosome ; maxWrap: the maximum number of wrapping; pElite: elitism propotion pselectSize: parent selection propotion; pCross: the probability of crossover; pMut: the probability of mutation; Output:</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE I PARAMETER</head><label>I</label><figDesc>VALUES FOR GE.</figDesc><table><row><cell>Parameter</cell><cell>Value</cell></row><row><cell>Total effort k</cell><cell>100,000; 200,000; 300,000; 400,000</cell></row><row><cell>initLenChrom</cell><cell>6</cell></row><row><cell>pCross</cell><cell>80%</cell></row><row><cell>pMut</cell><cell>1%</cell></row><row><cell>popSize</cell><cell>1000; 2000; 5000; 10000</cell></row><row><cell cols="2">determined based on the values of the total effort k so</cell></row><row><cell cols="2">that popSize • maxGenerations = k. The parameters are</cell></row><row><cell>summarized in Table</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE IV POSSIBILITY</head><label>IV</label><figDesc>AND GENERALITY DISTRIBUTION OF THE DISCOVERED AXIOMS FOR DIFFERENT POPULATION SIZES (COLUMNS) AND TOTAL EFFORTS</figDesc><table><row><cell>1,000</cell><cell>2,000</cell><cell>5,000</cell><cell>10,000</cell></row></table><note><p>k = 100, 000, . . . , 400, 000 (ROWS).</p></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>https://wiki.dbpedia.org/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>https://www.w3.org/standards/semanticweb/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>http://www.ietf.org/rfc/rfc3986.txt</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"><p>http://www.ietf.org/rfc/rfc3987.txt</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4"><p>https://www.w3.org/RDF/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5"><p>https://docs.microsoft.com/en-us/windows/desktop/winrm/uri-prefixes</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_6"><p>https://www.w3.org/TR/rdf-sparql-query/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_7"><p>http://linkeddata.org/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_8"><p>https://lod-cloud.net/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10" xml:id="foot_9"><p>https://wiki.dbpedia.org/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="11" xml:id="foot_10"><p>https://www.w3.org/TR/owl-ref/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="12" xml:id="foot_11"><p>https://www.w3.org/TR/rdf-schema/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="16" xml:id="foot_12"><p>Available for download at http://bit.ly/2Kl36wB.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="17" xml:id="foot_13"><p>https://virtuoso.openlinksw.com/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="18" xml:id="foot_14"><p>http://bit.ly/2pisZWB</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>ACKNOWLEDGMENTS</head><p>This research was performed in Wimmics team which is a joint research team of <rs type="institution">Université Côte d'Azur, Inria</rs>, and I3S. Our research motto: AI in bridging social semantics and formal semantics on the Web.</p><p>work has been partially supported by the <rs type="funder">French government</rs>, through the <rs type="programName">3IA Côte d'Azur "Investments in the Future</rs>" project managed by the <rs type="funder">National Research Agency (ANR)</rs> with the reference number <rs type="grantNumber">ANR-19-P3IA-0002</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_wne8yRm">
					<orgName type="program" subtype="full">3IA Côte d&apos;Azur &quot;Investments in the Future</orgName>
				</org>
				<org type="funding" xml:id="_rTQBPEt">
					<idno type="grant-number">ANR-19-P3IA-0002</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">What Is an Ontology? Handbook on Ontologies</title>
		<author>
			<persName><forename type="first">N</forename><surname>Guarino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Oberle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Staab</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Toward principles for the design of ontologies used for knowledge sharing?</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">R</forename><surname>Gruber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Hum.-Comput. Stud</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">5-6</biblScope>
			<biblScope unit="page" from="907" to="928" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Ontology learning for the semantic web</title>
		<author>
			<persName><forename type="first">A</forename><surname>Maedche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Staab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Intelligent Systems</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="72" to="79" />
			<date type="published" when="2001-03">March 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Lehmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Völker</surname></persName>
		</author>
		<title level="m">Perspectives on Ontology Learning, ser. Studies on the Semantic Web</title>
		<imprint>
			<publisher>IOS Press</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Knowledge systematization for ontology learning methods</title>
		<author>
			<persName><forename type="first">A</forename><surname>Konys</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">KES, ser. Procedia Computer Science</title>
		<imprint>
			<biblScope unit="volume">126</biblScope>
			<biblScope unit="page" from="2194" to="2207" />
			<date type="published" when="2018">2018</date>
			<publisher>Elsevier</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">DC proposal: Ontology learning from noisy linked data</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Semantic Web Conference</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="volume">7032</biblScope>
			<biblScope unit="page" from="373" to="380" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning disjointness</title>
		<author>
			<persName><forename type="first">J</forename><surname>Völker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Vrandecic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sure</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hotho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">ESWC, ser. Lecture Notes in Computer Science</title>
		<imprint>
			<biblScope unit="volume">4519</biblScope>
			<biblScope unit="page" from="175" to="189" />
			<date type="published" when="2007">2007</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Automatic acquisition of class disjointness</title>
		<author>
			<persName><forename type="first">J</forename><surname>Völker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Fleischhacker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Stuckenschmidt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Web Sem</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="124" to="139" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning class disjointness axioms using grammatical evolution</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">H</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G B</forename><surname>Tettamanzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">EuroGP, ser. Lecture Notes in Computer Science</title>
		<imprint>
			<biblScope unit="volume">11451</biblScope>
			<biblScope unit="page" from="278" to="294" />
			<date type="published" when="2019">2019</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Redescription mining for learning definitions and disjointness axioms in linked open data</title>
		<author>
			<persName><forename type="first">J</forename><surname>Reynaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Toussaint</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Napoli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">ICCS, ser. Lecture Notes in Computer Science</title>
		<imprint>
			<biblScope unit="volume">11530</biblScope>
			<biblScope unit="page" from="175" to="189" />
			<date type="published" when="2019">2019</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Terminological cluster trees for disjointness axiom discovery</title>
		<author>
			<persName><forename type="first">G</forename><surname>Rizzo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Amato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Fanizzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Esposito</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ESWC (1)</title>
		<title level="s">ser. Lecture Notes in Computer Science</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">10249</biblScope>
			<biblScope unit="page" from="184" to="201" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">An evolutionary approach to class disjointness axiom discovery</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">H</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G B</forename><surname>Tettamanzi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<publisher>WI. ACM</publisher>
			<biblScope unit="page" from="68" to="75" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Layering the semantic web: Problems and directions</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">F</forename><surname>Patel-Schneider</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Fensel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Semantic Web Conference</title>
		<title level="s">ser. Lecture Notes in Computer Science</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2002">2002</date>
			<biblScope unit="volume">2342</biblScope>
			<biblScope unit="page" from="16" to="29" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Grammatical evolution</title>
		<author>
			<persName><forename type="first">M</forename><surname>O'neill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ryan</surname></persName>
		</author>
		<idno type="DOI">10.1109/4235.942529</idno>
		<ptr target="http://dx.doi.org/10.1109/4235.942529" />
	</analytic>
	<monogr>
		<title level="j">Trans. Evol. Comp</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="349" to="358" />
			<date type="published" when="2001-08">Aug. 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>O'neill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Hemberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gilligan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Bartley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mcdermott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Brabazon</surname></persName>
		</author>
		<title level="m">UCD&apos;s Natural Computing Research &amp; Application group</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
	<note>GEVA -Grammatical Evolution in Java (v1.0)</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Crowding and preselection revisited</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">W</forename><surname>Mahfoud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">PPSN. Elsevier</title>
		<imprint>
			<date type="published" when="1992">1992</date>
			<biblScope unit="page" from="27" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Fuzzy sets as a basis for a theory of possibility</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Zadeh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Fuzzy Sets and Systems</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="3" to="28" />
			<date type="published" when="1978">1978</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A definition of a nonprobabilistic entropy in the setting of fuzzy sets theory</title>
		<author>
			<persName><forename type="first">A</forename><surname>De Luca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Termini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information and Control</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="301" to="312" />
			<date type="published" when="1972">1972</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
