<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Fine-Grained Modeling and Optimization for Intelligent Resource Management in Big Data Processing</title>
				<funder ref="#_KmHJRn3">
					<orgName type="full">European Research Council</orgName>
					<orgName type="abbreviated">ERC</orgName>
				</funder>
				<funder>
					<orgName type="full">China Scholarship Council</orgName>
					<orgName type="abbreviated">CSC</orgName>
				</funder>
				<funder ref="#_pcfam68">
					<orgName type="full">Alibaba Group</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Chenghao</forename><surname>Lyu</surname></persName>
							<email>chenghao@cs.umass.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Massachusetts</orgName>
								<address>
									<settlement>Amherst</settlement>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Alibaba Group</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Qi</forename><surname>Fan</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Ecole Polytechnique</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Alibaba Group</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Fei</forename><surname>Song</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Ecole Polytechnique</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Alibaba Group</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Arnab</forename><surname>Sinha</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Ecole Polytechnique</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Alibaba Group</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yanlei</forename><surname>Diao</surname></persName>
							<email>yanlei.diao@polytechnique.eduwickeychen.cw</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Massachusetts</orgName>
								<address>
									<settlement>Amherst</settlement>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Ecole Polytechnique</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Alibaba Group</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Wei</forename><surname>Chen</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Alibaba Group</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Li</forename><surname>Ma</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Alibaba Group</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yihui</forename><surname>Feng</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Alibaba Group</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yaliang</forename><surname>Li</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Alibaba Group</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Inst</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Kai</forename><surname>Zeng</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Alibaba Group</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Inst</orgName>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="department">Inst</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jingren</forename><surname>Zhou</surname></persName>
							<email>jingren.zhou@alibaba-inc.com</email>
							<affiliation key="aff2">
								<orgName type="department">Alibaba Group</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Inst</orgName>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="department">Inst</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Fine-Grained Modeling and Optimization for Intelligent Resource Management in Big Data Processing</title>
					</analytic>
					<monogr>
						<idno type="ISSN">2150-8097</idno>
					</monogr>
					<idno type="MD5">AAE80EC6AC4425425BBDFA7619B3813A</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-04-12T14:47+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Big data processing at the production scale presents a highly complex environment for resource optimization (RO), a problem crucial for meeting performance goals and budgetary constraints of analytical users. The RO problem is challenging because it involves a set of decisions (the partition count, placement of parallel instances on machines, and resource allocation to each instance), requires multi-objective optimization (MOO), and is compounded by the scale and complexity of big data systems while having to meet stringent time constraints for scheduling. This paper presents a MaxCompute based integrated system to support multi-objective resource optimization via ne-grained instance-level modeling and optimization. We propose a new architecture that breaks RO into a series of simpler problems, new ne-grained predictive models, and novel optimization methods that exploit these models to make effective instance-level RO decisions well under a second. Evaluation using production workloads shows that our new RO system could reduce 37-72% latency and 43-78% cost at the same time, compared to the current optimizer and scheduler, while running in 0.02-0.23s.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>While big data query processing has become commonplace in enterprise businesses and many platforms have been developed for this purpose <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b58">59,</ref><ref type="bibr" target="#b62">63,</ref><ref type="bibr" target="#b63">64]</ref>, resource optimization in large clusters <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b33">[34]</ref><ref type="bibr" target="#b34">[35]</ref><ref type="bibr" target="#b35">[36]</ref> has received less attention. However, we observe from real-world experiences of running large compute clusters in the Alibaba Cloud that resource management plays a vital role in meeting both performance goals and budgetary constraints of internal and external analytical users.</p><p>Production-scale big data processing presents a highly complex environment for resource optimization. We show an example through the life cycle of a submitted job in MaxCompute <ref type="bibr" target="#b27">[28]</ref>, the big data query processing system at Alibaba, as shown in Fig. <ref type="figure" target="#fig_0">1</ref>. A submitted SQL job is rst processed by a "Cost-Based Optimizer" (CBO) to construct a query plan in the form of a Directed Acyclic Graph (DAG) of operators. These operators are further grouped into several stages connected with data shu e (exchanging) dependencies. As shown in Fig. <ref type="figure" target="#fig_0">1</ref>, job 1 is composed of stage 1 (operators 1-3), stage 2 (operators 4-8), and stage 3 (operators 9-15), and their boundaries are data shu ing operations. To explore data parallelism, each stage runs over multiple machines and each machine runs an instance of a stage over a partition of the input data. To enable such parallel execution, a "History-Based Optimizer" (HBO) recommends a partition count (number of instances) for each stage and a resource plan (the number of cores and memory needed) for all instances of the stage based on previous experiences.</p><p>During job execution, a Stage Dependency Manager will maintain the stage dependencies of a job and pop out all the unblocked stages to the Fuxi scheduler <ref type="bibr" target="#b62">[63]</ref>. The scheduler treats each stage as a task to be scheduled and maintains tasks in queues. For each stage, the Fuxi scheduler uses a heuristic-based approach to recommending a placement plan that sends instances to machines. After an instance is assigned to a machine, it will be executed using the resource plan that HBO has created for this instance, which is the same for all instances of the same stage.</p><p>Challenges. MaxCompute's large, complex big data processing environment poses a number of challenges to resource optimization.</p><p>First, resource optimization involves a set of decisions that need to be made in the life cycle of a big data query: <ref type="bibr" target="#b0">(1)</ref> the partition count of a stage; <ref type="bibr" target="#b1">(2)</ref> the placement plan that maps the instances of a stage to the available machines; (3) the resource plan that determines the resources assigned to each instance on a given machine. All of these factors will have a signi cant impact on the performance of the job, e.g., its latency and computing cost. Among the three issues, the partition count is best studied in the literature <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b34">35]</ref>. But this decision alone is not enough -both decisions 2 and 3 can a ect the performance a lot. While most of the existing work neglects the placement problem, due to the use of virtualization or container technology, a service provider like Alibaba Cloud does need to solve the placement problem in the physical clusters. As for the resource plan, it is determined by HBO from past experiences, without considering the latencies of the current instances in hand. However, such solutions are far from ideal: if one underestimates the resource needs, the job can miss its deadline. On the other hand, overestimation leads to wasted resources and higher costs on (both internal and external) users. A few systems addressed the placement 10   problem <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b17">18]</ref> or the resource problem <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b45">46]</ref> in isolation and in simpler system environments. We are not aware of any solution that can address all three problems in an integrated system.</p><p>Second, the scale and complexity of our production clusters make the above decisions a challenging problem. Most notably, our clusters can easily extend to tens of thousands of machines, while all the resource optimization decisions must be made well under a second. An algorithm for the placement problem with quadratic complexity <ref type="bibr" target="#b17">[18]</ref> may work for a small cluster of 10 machines and a small number of jobs, as in the original paper, but not for the scale of 10's thousands of machines and millions of jobs. Furthermore, the optimal solutions to both the placement and resource plans depend on the workload characteristics of instances, available machines, and current system states. It is nontrivial to characterize all the data needed for resource optimization, let alone the question of using all the data to derive optimal solutions well under a second.</p><p>Third, our use cases clearly indicate that resource optimization is a multi-objective optimization (MOO) problem. A real example we encountered in production is that after the user changed to allocate 10x more resources (hence paying 10x the cost), the latency of a job was reduced only by half, which indicates ine ective use of resources and a poor tradeo between latency and cost. Over such a complex processing system, the user has no insights into how latency and cost trade-o . Therefore, there is a growing demand that the resource optimizer makes the decisions automatically to achieve the best tradeo between multiple, often competing, objectives. Most existing work on resource optimization focuses on a single objective, i.e., job latency <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b60">61]</ref>. Only a few recent systems <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b39">40]</ref> o er a multi-objective approach to the resource optimization problem. Still, their solutions do not suit the complex structure of big data queries, which we detail below.</p><p>Fourth, to enable multi-objective resource optimization, the system must have a model for each objective to predict its value under any possible solution that the optimizer would consider. Existing models, derived from domain knowledge <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b33">34]</ref> or machine learning methods <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b64">65]</ref>, have been used to improve SQL query plans <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b44">45]</ref> or to improve selectivity estimation <ref type="bibr">[11, 20, 21, 32, 33, 38, 50-54, 56, 57, 66]</ref>. But a key observation made in this work is that existing models do not suit the needs of resource optimization of big data queries because they perform only coarse-grained modeling: by capturing only end-to-end query latency or operator latency across parallel instances, these models may yield highly variable performance as they often involve large numbers of stages and parallel instances. Running optimization on highly-variable models gives undesirable results while missing opportunities for instance-level recommendations.</p><p>Example 1. Fig. <ref type="figure" target="#fig_2">2</ref>(a) and 2(b) show that in a production trace of 0.62 million jobs, there are 1.9 million stages in total, with up to 64 stages in each job, and 121 million instances, with up to 81430 instances in a stage. For a particular stage with 6716 instances, Fig. <ref type="figure" target="#fig_2">2(c</ref>) shows that the latencies of di erent instances vary a lot. If a performance model captures only the overall stage latency, i.e., the maximum instance latency, when the resource optimizer is asked to reduce latency, it will assign more resources uniformly to all the instances (as they are not distinguishable by the model). The extra resources do not contribute to the stage latency for those short-running instances, while incurring a higher cost. Instead, an optimal solution would be only to assign more resources to longrunning instances while reducing resources for short-running ones. Such decisions require ne-grained instance-level models as well as instance-speci c resource plans.</p><p>Contributions. Based on the above discussion, our work aims to support multi-objective resource optimization via ne-grained instance-level modeling and optimization, and devises new system architecture and algorithms to enable fast resource optimization decisions, well under a second, in the face of 10's thousands of machines and 10's thousands of instances per stage. By way of addressing the above challenges, our work makes the following contributions.</p><p>1. New architecture of a resource optimizer (Section 3). To enable all the resource optimization (RO) decisions of a stage within a second, we propose a new architecture that breaks RO into a series of simpler problems. The solutions to these problems leverage existing cost-based optimization (CBO) and history-based optimization (HBO), while xing their suboptimal decisions using a new Intelligent Placement Advisor (IPA) and Resource Assignment Advisor (RAA), both of which exploit ne-grained predictive models to enable e ective instance-level recommendations.</p><p>2. Fine-grained models (Section 4). To suit the complexity of our big data system, our ne-grained instance-level models capture all relevant aspects from the outputs of CBO and HBO, the hardware, and machine states, and embed these heterogenous channels of information in a number of deep neural network architectures.</p><p>3. Optimizing placement and resource plans (Section 5). We design a new stage optimizer that employs a new IPA module to derive a placement plan to reduce the stage latency, and a novel RAA model to derive instance-speci c resource plans to further reduce latency and cost in a hierarchical MOO framework. Both methods are proven to achieve optimality with respect to their own set of variables, and are optimized to run well under a second.</p><p>4. Evaluation (Section 6). Using production workloads of 0.6M jobs and 1.9M stages and a simulator of the extended MaxCompute environment, our evaluation shows promising results: (1) Our best model achieved 7-15% median error and 9-19% weighted mean absolute percentage error. (2) Compared to the HBO and Fuxi scheduler, IPA reduced the latency by 10-44% and the cloud cost by 3-12% while running in 0.04s. (3) IPA + RAA further achieved the reduction of 37-72% latency and 43-78% cost while running in 0.02-0.23s.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>ML-based query performance prediction. Recent work has developed various Machine Learning (ML) methods to predict query performance. QPPNet <ref type="bibr" target="#b25">[26]</ref> builds separate neural network models (neural units) for individual query operators and constructs more extensive neural networks based on the query plan structure. Each neural unit learns the latency of the subquery rooted in a given operator. TLSTM <ref type="bibr" target="#b36">[37]</ref> constructs a uniform feature representation for each query operator and feeds the operator representations into a TreeLSTM to learn the query latency. Both approaches, however, are tailored only for a single machine with an isolated runtime environment and xed resources. Hence, they are not directly applicable to our RO problem in large-scale complex clusters.</p><p>ModelBot2 <ref type="bibr" target="#b22">[23]</ref> trains ML models for ne-grained operating units decomposed from a DBMS architecture to enable a self-driving DBMS. However, it is designed for a local DMBS but not big data systems as in our work. GPredictor <ref type="bibr" target="#b64">[65]</ref> predicts the latency of concurrent queries in a single machine with a graph-embedding-based model. It further improves accuracy by using pro ling features such as data-sharing, data-con ict and resource competition from the local DBMS. In our work, however, concurrency information regarding the operators from di erent queries is unavailable due to the container technology in large clusters <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b58">59,</ref><ref type="bibr" target="#b63">64]</ref>.</p><p>ML-based models have been used for di erent purposes. NEO <ref type="bibr" target="#b24">[25]</ref> learns a DNN-based cost model for (sub)queries and uses it to build a value-based Reinforcement Learning (RL) algorithm for improving query execution plans. Vaidya et al. <ref type="bibr" target="#b44">[45]</ref> train ML models from query logs to improve query plans for parametric queries. Phoebe <ref type="bibr" target="#b66">[67]</ref> uses ML models for improving checkpointing decisions. Many recent works <ref type="bibr">[11, 20, 21, 32, 33, 38, 50-54, 56, 57, 66]</ref> have applied ML-based approaches to improve cardinality estimation.</p><p>A nal, yet important, comment on the above systems is that they do not address the RO problem like in our work.</p><p>Performance tuning in DBMS and big data systems. Performance tuning systems require a dedicated, often iterative, tuning session for each workload, which can take long to run (e.g., 15-45 minutes <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b60">61]</ref>). As such, they are not designed for production workloads that need to be executed on demand. In addition, they aim to optimize a single objective, e.g., query latency. Among searchbased methods, BestCon g <ref type="bibr" target="#b68">[69]</ref> searches for good con gurations by dividing high-dimensional con guration space into subspaces based on samples, but it cold-starts each tuning request. Classy-Tune <ref type="bibr" target="#b67">[68]</ref> solves the optimization problem by classi cation, which cannot be easily extended to the MOO setting. Among learningbased methods, Ottertune <ref type="bibr" target="#b45">[46]</ref> builds a predictive model for each query by leveraging similarities to past queries, and runs Gaussian Process exploration to try other con gurations to reduce latency. CDBTune <ref type="bibr" target="#b60">[61]</ref> uses Deep RL to predict the reward (a weighted sum of latency and throughput) of a given con guration and explores a series of con gurations to optimize the reward. ResTune <ref type="bibr" target="#b61">[62]</ref> uses a meta-learning model to learn the accumulated knowledge from historical workloads to accelerate the tuning process for optimizing resource utilization without violating SLA constraints.</p><p>Task scheduling in big data systems. Fuxi <ref type="bibr" target="#b62">[63]</ref> and Yarn <ref type="bibr" target="#b47">[48]</ref> make the scheduling decisions based on locality information, while Trident <ref type="bibr" target="#b12">[13]</ref> improves Yarn by considering the locality in di erent storage tiers. However, these systems treat each task as a blackbox and make scheduling decisions without knowing the task characteristics, such as the query plan structures and performance predictions. Hence, they cannot nd optimal solutions to the machine placement and/or resource allocation problems.</p><p>Resource optimization in big data systems. In cluster computing, a resource optimizer (RO) determines the optimal resource con guration on demand and with low latency as jobs are submitted. RO for parallel databases <ref type="bibr" target="#b17">[18]</ref> determines the best data partitioning strategy across di erent machines to minimize a single objective, latency. Its time complexity of solving the placement problem is quadratic to the number of machines (9 machines in <ref type="bibr" target="#b17">[18]</ref>), which is not a ordable on today's productive scale (&gt;10K machines). Morpheus <ref type="bibr" target="#b14">[15]</ref> codi es user expectations as SLOs and enforces them using scheduling methods. However, its optimization focuses on system utilization and predictability, but not minimizing the cost and latency of individual jobs as in our work. PerfOrator <ref type="bibr" target="#b33">[34]</ref> solves a single-objective (latency) optimization problem via an exhaustive search of the solution space while calling its model for predicting the performance of each solution. WiseDB <ref type="bibr" target="#b23">[24]</ref> manages cloud resources based on a decision tree trained on performance and cost features from minimum-cost schedules of sample workloads, while such schedules are not available in our case. Li et al. <ref type="bibr" target="#b18">[19]</ref> minimize end-to-end tuple processing time using deep RL and requires de ning scheduling actions and the associated reward, which is not available in our problem. Recent work <ref type="bibr" target="#b16">[17]</ref> proposes a heuristicbased model to recommend a cloud instance (e.g., EC2 instance) that achieves cost optimality for OLAP queries, which is di erent from our resource optimization problem.</p><p>CLEO <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b48">49]</ref> learns the end-to-end latency model of query operators, and based on the model, minimizes the latency of a stage (involving multiple operators) by tuning the partition count. It has a set of limitations: First, its modeling target concerns the latency of multiple instances over di erent machines, which can be highly variable (e.g., due to uneven data partitions or scheduling delays) and hard to predict. Second, CLEO's latency model does not permit instance-level recommendations for the placement and resource allocation problems. Third, its optimization supports a single objective, and determines only the partition count in a stage, but not other RO decisions. Bag et al. <ref type="bibr" target="#b1">[2]</ref> propose a plan-aware resource allocation approach to save resource usage without impacting the job latency but not minimize latency and cost like in our work.</p><p>UDAO <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b59">60]</ref> tunes Spark con gurations to optimize for multiple objectives. However, it works only on the granularity of an entire query and neglects its internal structure. Such coarse-grained modeling of latency is unlikely to be accurate for complex big data queries, which leads to poor results of optimization. TEMPO <ref type="bibr" target="#b39">[40]</ref> considers multiple Service-Level Objectives (SLOs) of SQL queries and guarantees max-min fairness when they cannot be all met. But its MOO works for entire queries, but not ne-grained MOO that suits the complex structure of big data systems.</p><p>Multi-objective optimization (MOO). MOO for SQL <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b41">[42]</ref><ref type="bibr" target="#b42">[43]</ref><ref type="bibr" target="#b43">[44]</ref> nds Pareto-optimal query plans by e ciently searching through a large set of them. The problem is fundamentally di erent from RO, including machine placement and resource tuning problems. MOO for work ow scheduling <ref type="bibr" target="#b15">[16]</ref> assigns operators to containers to minimize total running time and money cost. But its method is limited to searching through 20 possible numbers of containers and solving a constrained optimization for each option.</p><p>Theoretical MOO solutions su er from a range of performance issues when used in a RO: Weighted Sum <ref type="bibr" target="#b26">[27]</ref> is known to have poor coverage of the Pareto frontier <ref type="bibr" target="#b28">[29]</ref>. Normalized Constraints <ref type="bibr" target="#b29">[30]</ref> lacks in e ciency due to repeated recomputation to return more solutions. Evolutionary Methods <ref type="bibr" target="#b7">[8]</ref> approximately compute a Pareto set but su er from inconsistent solutions. Multi-objective Bayesian Optimization <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b11">12]</ref> explores the potential Pareto-optimal points iteratively by extending the Bayesian approach, but lacks the eciency due to a long-running time. Progressive frontier <ref type="bibr" target="#b35">[36]</ref> is the rst MOO solution suitable for RO, o ering good coverage, eciency, and consistency. However, none of them consider complex structures of big data queries and require modeling end-to-end query latency, which does not permit instance-level optimization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">SYSTEM OVERVIEW</head><p>In this section, we provide the background on MaxCompute <ref type="bibr" target="#b27">[28]</ref>, the big data query processing system at Alibaba, and our proposed extended architecture for resource optimization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Background on MaxCompute</head><p>In MaxCompute, a submitted user job is represented hierarchically using stages and operators, which will then be executed by parallel instances. As shown in Fig. <ref type="figure" target="#fig_0">1</ref>, a job is a Directed Acyclic Graph (DAG) of stages, where the edges between stages are inter-machine data exchange (shu ing) operations. A stage is a DAG of operators, where edges are intra-machine pipelines without data shu ing. The input data of each stage is partitioned over di erent machines, where each partition is run as an instance of the stage in a container.</p><p>Fig. <ref type="figure" target="#fig_0">1</ref> shows the functionality of query optimizers and the scheduler in the lifecycle of a submitted job.</p><p>Cost-Based Optimizer (CBO): MaxCompute's Cost-Based Optimizer (CBO) is a variant of the Cascades optimizer <ref type="bibr" target="#b9">[10]</ref>. It follows traditional SQL optimization based on cardinality and cost estimation and generates a Physical Operator Tree (POT), which is a DAG of stages where each stage is a DAG of operators.</p><p>History-Based Optimizer (HBO): To facilitate resource optimization, a History-Based Optimizer (HBO) gives an initial attempt to recommend a partition count (number of instances) for each stage and a resource plan (the number of cores and memory needed) for all instances of the stage based on previous experiences. This history-based approach is known to be suboptimal because it does not consider the machines to run these queries and their current states. Both hardware characteristics and system states a ect the latencies of instances, and an optimal solution should try to minimize the maximum latency of parallel instances, in addition to cost objectives. Moreover, HBO needs expensive engineering e orts, especially when workloads change and the system upgrades. We will address these issues in our new system design.</p><p>Scheduler: During job execution, the granularity of scheduling is a stage: a stage that has all dependencies met is handed over to the Fuxi scheduler <ref type="bibr" target="#b62">[63]</ref>. Fuxi uses a heuristic approach to recommending a placement plan that sends instances to machines, and each instance is assigned to a container on a machine with a previously-determined resource plan for CPU and memory. These decisions, however, are made without being aware of the latency of each instance. As such, an instance with a potential longer running time (e.g., due to larger input size) may be sent to a heavily loaded machine while another instance with less data may be sent to an idle machine, leading to overall poor stage latency (the maximum of instance latencies). A detailed example is given later in Fig. <ref type="figure">6</ref>.</p><p>Workload and Cluster complexity. Production clusters take workloads with a vast variety of characteristics. Workload A from an internal department includes 1 -64 stages in each job. A stage may involve 2 -249 operators and be executed by 1 -42K instances, where an instance could take sub-seconds up to 1.4 hours to run. Further, production clusters consist of heterogeneous machines. For example, we observed 5 di erent hardware types when executing workload A, where each hardware type includes 30 -7K machines. Moreover, the system states in each machine vary over time. Take CPU utilization for example. The average CPU utilization varies from 32%-83%, and the standard deviation ranges from 6%-23%. Finally, each machine could run multiple containers, and a container runs an instance with a quota of CPU and memory based on a resource plan. For workload A, we observed 17 di erent resource plans for containers. Since there is no perfect isolation between containers and the host OS <ref type="bibr" target="#b46">[47]</ref>, containers with the same resource plan could perform di erently on machines with di erent hardware or system states, making the running environment more complex.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">System Design for Resource Optimization</head><p>We next show our design for resource optimization by extending the MaxCompute architecture. Our work aims to support multiobjective resource optimization (MORO). Here, we can support any user objectives (as long as we can obtain training data for them). For ease of composition, our discussion below focuses on minimizing both the stage latency (maximum latency among its instances) and the cloud cost (a weighted sum of CPU-hour and memory-hour), the two common objectives of our users.</p><p>To achieve MORO, the resource optimizer needs to make three decisions: (1) the partition count of a stage; (2) the placement plan (PP) that maps the instances of a stage to the available machines;</p><p>(3) the resource plan (RP) that determines the resources (the number of cores and memory size) assigned to each instance on a given machine. Our design is guided by two principles: Job1.stage2</p><p>(1 core, 4G), 40</p><note type="other">1 2 3 4 5 6 7 8 9 10 11 12 13 14 15</note><p>A job with 3 stages A job with 3 stages Simplicity and E ciency. An optimal solution to MORO may require examining all possibilities of dividing the input data into m instances and arranging them to run on some of the n machines, each using one of the r possible resource con gurations. In production clusters, both m and n could be 10's of thousands, while all the RO decisions must be made well under a second. Hence, it is infeasible to run an exhaustive search for optimal solutions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Stage Dependency Manager</head><p>Our design principle is to break MORO into a series of simpler problems, each of which can run very fast. First, we keep the History-Based Optimizer (HBO) that uses past experiences to recommend a partition count for a stage and an initial resource plan for its instances. There is a merit of learning such con gurations from past best-performing runs of recurring jobs (which dominate production workloads). While the recommendations may not be optimal, they serve as a good initial solution. Second, given the output of HBO, we design an Intelligent Placement Advisor (IPA) that determines the placement plan (PP), mapping the instances to machines, by predicting latencies of individual instances. Third, our Resource Assignment Advisor (RAA) will ne-tune the resource plan (RP) for each instance, after it is assigned to a speci c machine, to achieve the best tradeo between stage latency and cost. We give a theoretical justifcation of our approach in Section 5.</p><p>Fine-grained Modeling and Hierarchical MOO. As discussed earlier, instance-level recommendations for PP and RP are key to minimizing latency and cost. To do so, we design a ne-grained model that predicts the latency of each instance based on the workload characteristics, hardware, machine states, and resource plan in use. The model will be used in IPA to develop the PP that minimizes the maximum instance latency (while using the same RP for all instances). The instance-level model will be further used in RAA to improve the RP by solving a hierarchical MOO problem: We rst compute the instance-level MOO solutions that minimize the latency and cost of each individual instance. We then combine the instance-level MOO solutions into stage-level MOO solutions, and recommend one of them that determines the instance-speci c resources with the best tradeo between stage latency and cost.</p><p>Fig. <ref type="figure" target="#fig_4">3</ref> presents the detailed architecture that extends MaxCompute with three new components (colored in purple): The trace collector collects runtime traces, including the query traces (the query plan and operator features such as cardinality), instancelevel traces (input row number and data size of an instance, and the resource plan assigned to it), and machine-level traces (machine system states and hardware type). The model server featurizes the collected traces and internally learns an instance-level latency model. The internal model gets updated periodically by retraining or ne-tuning when the new traces are ready. It will serve as an instance-level latency predictor for resource optimization of online queries. The Stage-level Optimizer (SO) consists of the IPA and RAA, as described above. For a stage to be scheduled, it calls the predictive model to estimate the latency of each instance on any available machine, and determines the PP by minimizing stage latency and then the RP by minimizing both stage latency and cost.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">FINE-GRAINED MODELING</head><p>In this section, we present how to build ne-grained instance-level models that can be used to minimize both latency and cost of each stage. To suit the complexity of big data systems, our models capture all relevant systems aspects to support the resource optimizer to make e ective recommendations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Multi-Channel Coverage</head><p>It is nontrivial to predict the latency of a single instance due to many factors in big data systems. Those factors include the characteristics of the (sub)query plan running in the instance, data characteristics, resources in use, current system states, and hardware properties. Each factor alone could a ect the latency of an instance, and the coe ects of multiple factors can make the latency pattern more complex. Therefore, we propose the idea of multi-channel inputs (MCI) to capture all of the above factors in our model.</p><p>Multi-channel Inputs. For a given stage, we extract features from all available runtime traces including the query plan, resource plan, instance-level metrics, hardware pro le, and system states. We design multi-channel inputs (MCI) to group the features into ve channels that characterize di erent factors. Fig. <ref type="figure" target="#fig_11">4</ref> shows the overview of the MCI design for the instances of a stage.</p><p>Channel 1: Stage-oriented features (query plan). Channel 1 introduces the stage-oriented features that are shared among instances in a stage. It captures the operator characteristics in an operator feature matrix (see Fig. <ref type="figure" target="#fig_11">4</ref>) and operator dependencies in a DAG structure (Fig. <ref type="figure" target="#fig_5">5</ref>). The characteristics of each operator are featurized by three common feature types (CT1-CT3) and the customized features (CFs). CT1 identi es the operator type and is represented as a categorical variable. CT2 captures the statistics from CBO and HBO, including the cardinality, selectivity, average row size, partition count, and cost estimation. CT3 captures the IO-related  properties, including the location of data (local disk or network) and strategies for shu ing. CFs are tailored for the unique properties of operators, and each feature belongs to one particular operator.</p><p>Channels 2-5: Instance-oriented features. Channel 2-5 characterizes individual instances (Fig. <ref type="figure" target="#fig_11">4</ref>). Channel 2, instance meta, includes an instance's input row number and input size captured from the underlying storage system after the data partition is determined. Channel 3, resource plan, featurizes properties of the container that runs an instance, in terms of the CPU cores and memory size. Channel 4, machine system states, records the CPU utilization, memory utilization, and IO activities of a machine to capture its running states. Since the system states represented by count-based measurements could be in nite, we discretize the system states to reduce the computation complexity. Channel 5, hardware type, uses the machine model to distinguish a set of hardware types.</p><p>Augmented Channel 1: Additional Instance Meta (AIM). In big data systems, CBO produces the query plan without considering the characteristics of individual instances. Therefore, instances in a stage share the same query plan features (Ch1), even though their input row numbers can di er signi cantly. A model built on such features may have di culty distinguishing those instance latencies.</p><p>To address this issue, for each instance, we seek to enrich its query plan features with instance-level characteristics. More specifically, we augment the query plan channel of an instance by adding additional instance meta (AIM) features for each operator. AIM consists of the estimated operator input/output cardinality and costs of an individual instance. It is derived using the stage-level selectivity (Ch1), the instance meta (Ch2), and the cost model in CBO.</p><p>Take job1.stage3 in Fig. <ref type="figure" target="#fig_11">4</ref> for example. We rst get the instancelevel input cardinality of operators 9 and 10 from Ch2, and calculate their output cardinality accordingly with the operator selectivity. Then, we derive the instance-level cardinality of the remaining operators according to the operator dependency. Finally, we calculate the instance-level operator costs by reusing CBO's cost model. Specifically, we substitute the stage-level cardinality with the instancelevel cardinality, set the partition count to one, and call the CBO's cost model to derive the operator costs for an instance.</p><p>Note that the above approach assumes that instances in a stage share the same selectivities. Although it may not always be true, our evaluation results show that the model built under this assumption could approximate the best performance in a realistic setting.</p><p>Finally, recent ML models of DBMSs <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b38">39]</ref> also learn data characteristics by encoding/embedding tables. In a production system, however, many tables are not reachable due to access control for security reasons, and the overhead of representing a distributed table is much higher than in a single machine. Therefore, we assume the data characteristics are already digested by CBO in our work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">MCI-based Models</head><p>Now we propose our MCI-based models to learn instance latency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MCI-based Modeling</head><p>Framework. In the output of MCI, features in the query plan channel are represented as a DAG (Fig. <ref type="figure" target="#fig_5">5</ref>), while the instance-oriented features are in the tabular form. To incorporate these heterogeneous data structures in model building, we design two model components, as shown in Fig. <ref type="figure" target="#fig_5">5</ref>. The plan embedder constructs a plan embedding of the plan features in the form of an arbitrary DAG structure. It rst encodes the operators into a uniform feature space by padding zeros for the unmatched customized features. To embed the query plan, it then applies a Graph Transformer Networks (GTN) <ref type="bibr" target="#b57">[58]</ref> due to its ability to learn the DAG context in heterogeneous graphs with di erent types of nodes. The latency predictor is a downstream model that concatenates the plan embedding with other instance-oriented features (channels 2-5) into a big vector, and feeds the vector to a Multilayer Perceptron (MLP) to predict the instance-level latency.</p><p>Modeling Tools in Our Framework. Besides GTN, our modeling framework can accommodate other models designed for DBMSs, with necessary extensions to our graph-based MCIs. Thus, we can leverage di erent models and examine their pros and cons in our system. Due to space limitations, our extensions of QPPNet <ref type="bibr" target="#b25">[26]</ref> and TLSTM <ref type="bibr" target="#b36">[37]</ref> are deferred to <ref type="bibr" target="#b21">[22]</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">STAGE-LEVEL OPTIMIZATION</head><p>In this section, we present our Stage-level Optimizer (SO) that minimizes both stage latency and cost based on instance-level models.</p><p>During execution, once a stage is handed to the scheduler, two decisions are made: the placement plan (PP) that maps instances to machines, and the resource plan (RP) that determines the CPU and memory resources of each instance on its assigned machine. The current Fuxi scheduler <ref type="bibr" target="#b62">[63]</ref> decides a PP for m instances as follows:</p><p>(1) Identify the key resource (bottleneck) in the current cluster, 3) WUN Recommendation: 3) WUN Recommendation: (3) Assign instances, in order of their instance id, to the m machines, and use the same resource plan for each instance as suggested by HBO. However, its negligence of latency variance among instances leads to suboptimal decisions for PP and RP: Example 2. Figure <ref type="figure">6</ref> shows how Fuxi gets a suboptimal placement plan in a toy example of sending a stage of 2 instances (i 1 , i 2 ) to a cluster of 3 available machines (m 1 , m 2 , m 3 ). Assume that the instance latency is proportional to its input row number on the same machine, and the current key resource in the cluster is the CPU. In this example, Fuxi rst picks m 1 and m 2 as machines with the top-2 lowest CPU utilization (watermarks) and assigns i 1 to m 1 and i 2 to m 2 respectively. The stage latency, i.e., the maximum instance latency, is 24s. However, the optimal placement plan could achieve a 16s stage latency by assigning i 1 to m 3 and i 2 to m 1 . Further, using the same resources for i 1 and i 2 is not ideal. Instead, an optimal resource plan would be adding resources to i 2 and reducing resources for i 1 so as to reduce both latency and cost, indicating the need for instance-speci c resource allocation.</p><formula xml:id="formula_0">C 1 D Stage Latency Stage Cost 0 -3 [-# ) , -) ) ] 100 25 0 -4 -# # , -) ) 150 10 0 -5 -# # , -) # 300 9</formula><formula xml:id="formula_1">E 3 C Stage Latency Stage Cost 0 -3 [-# ) , -) ) ] 100 25 0 -4 -# # , -) ) 150 10 0 -5 -# # , -) # 300 9</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">MOO Problem and Our Approach</head><p>To derive the optimal placement and resource plans, we begin by providing the mathematical de nition of multi-objective optimization (MOO) and present an overview of our approach.</p><p>Instance-level MOO. First, consider a given instance to be run on a speci c machine. We use f 1 , ..., f k to denote the set of predictive models of the k objectives and θ to denote a resource con guration available on that machine. Then the instance-level multipleobjective optimization (MOO) problem is de ned as:</p><formula xml:id="formula_2">De nition 5.1. Multi-Objective Optimization (MOO). arg min θ f θ = f (θ ) =       f 1 (θ ) ... f k (θ )       s.t . θ ∈ Σ ⊆ R d</formula><p>where Σ denotes the set of all possible resource con gurations.</p><formula xml:id="formula_3">A point f ∈ R k Pareto-dominates another point f i ∀i ∈ [1, k], f i ≤ f i and ∃j ∈ [1, k], f j &lt; f j .</formula><p>A point f * is Pareto Optimal i there does not exist another point f that Pareto-dominates it. Then, the Pareto Set f = [f θ 1 , f θ 2 , ...] includes all the Pareto optimal points in the objective space Φ ⊆ R k , under the con gurations [θ 1 , θ 2 , . . .], and is the solution to the MOO problem.</p><p>Stage-level MOO. We next consider the stage-level MOO problem over multiple instances. Consider m instances, (x 1 , ..., x m ), and n machines, ( 1 , ..., n ). We use xi to denote the characteristics of x i based on its features of Ch1 and Ch2 in its multi-channel representation, and ˜ j to denote the features of Ch4 and Ch5 of machine j . Then consider two sets of variables, B and Θ:</p><p>(1) B ∈ R m×n is the binary assignment matrix, where B i, j = 1 when x i is assigned to j , and <ref type="figure"></ref>and<ref type="figure">Σ  *</ref> i is set of possible con gurations of d resources (e.g., d = 2 for CPU and memory resources) for instance i.</p><formula xml:id="formula_4">j B i, j = 1, ∀i = 1...m. (2) Θ = [Θ 1 , Θ 2 , ..., Θ m ], denotes the collection of resource con- gurations of m instances, where ∀i ∈ [1, . . . , m], Θ i ∈ Σ * i ⊆ R d ,</formula><p>Given these variables, suppose that f is the instance-level latency prediction model, i.e., f ( xi , Θ i , ˜ j ) gives the latency when x i is running on j using the resource con guration Θ i . Then the stage-level latency can be written as, L st a e = max i, j B i, j f ( xi , Θ i , ˜ j ). The stage cost is the weighted sum of cpu-hour and memory-hour and can be written as, C st a e = i, j B i, j f ( xi , Θ i , ˜ j )(w • Θ T i ), where w is the weight vector over d resources and w • Θ T i is the dot product between w and Θ i . Other objectives can be written in a similar fashion. Then we have the Stage-level MOO Problem:</p><formula xml:id="formula_5">De nition 5.2. Stage-Level MOO Problem. arg min B,Θ       L(B, Θ) = max i, j B i, j f ( xi , Θ i , ˜ j ) C(B, Θ) = i, j B i, j f ( xi , Θ i , ˜ j )(w • Θ T i ) ...       s.t . B i, j ∈ {0, 1}, ∀i = 1...m, ∀j = 1...n j B i, j = 1, ∀i = 1...m i B i, j Θ 1 i ≤ U 1 j , . . . , i B i, j Θ d i ≤ U d j , ∀j = 1...n</formula><p>where U j ∈ R d is the d-dim resource capacities on machine j .</p><p>Existing MOO Approaches. Given the above de nition of stagelevel MOO, one approach is to call existing MOO methods <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b35">36]</ref> to solve it directly. However, this approach is facing a host of issues: (1) The parameter space is too large. In our problem setting, both m and n can reach 10's of thousands. Hence, both B ∈ R m×n and Θ</p><formula xml:id="formula_6">= [Θ 1 , Θ 2 , ..., Θ m ], ∀i, Θ i ∈ R d , involve O(mn)</formula><p>and O(md) variables, respectively, which challenge all MOO methods. (2) There are also constraints speci ed in Def. 5.2. Most MOO methods do not handle such complex constraints and hence may often fail to return feasible solutions. We will demonstrate the performance issues of this approach in our experimental study.</p><p>Our MOO Approach. To solve the complex stage-level MOO problem while meeting stringent time constraints, we devise a novel MOO approach that proceeds in two steps:</p><p>Step 1 (IPA): Take the resource con guration Θ 0 returned from the HBO optimizer as the default and assign it uniformly to all instances, Θ i = Θ 0 , ∀i ∈ [1, ..., m]. Then minimize over B in Def. 5.2 by treating Θ 0 as a constant.</p><p>Step 2 (RAA): Given the solution from step 1, B * , we now minimize over the variables Θ in Def. 5.2 by treating B * as a constant. The intuition behind our approach is that if we start with a decent choice of Θ 0 , as returned by HBO, we hope that step 1 will reduce stage latency via a good assignment of instances to machines by considering machine capacities and instance latencies. Then step 2 will ne-tune the resources assigned to each instance on a speci c machine to reduce stage latency, cost, as well as other objectives. We next present our IPA and RAA methods that implement the above steps, respectively, prove their optimality based on their respective de nitions, and optimize them to run well under a second.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Intelligent Placement Advisor (IPA)</head><p>Problem Setup. IPA determines the placement plan (PP) by way of minimizing the stage latency. We assume that the total available resources in a cluster are more than a stage's need -this assumption is likely to be met in a production system with 10's of thousands of machines 1 . We further suppose that all the instances of a stage can start running simultaneously and hence the stage latency equals the maximum instance latency.</p><p>IPA begins by feeding the MCI features of the stage to the latency model and builds a latency matrix, L ∈ R m×n , to include the predictions of running the instances on all available machines, i.e., L i, j is the latency of running instance x i on machine j . By setting L i, j = f ( xi , Θ 0 , ˜ j ), ignoring the constant Θ 0 , and focusing on the stage latency objective in Def. 5.2, we obtain:</p><formula xml:id="formula_7">arg min B L(B) = max i, j B i, j L i, j<label>(3)</label></formula><formula xml:id="formula_8">s.t. j B i, j = 1, ∀i and i B i, j ≤ β j , ∀j</formula><p>Here, we add the constraint i B i, j ≤ β j to meet the resource capacity constraint of each machine and a diverse placement preference (soft constraint) of sending instances to di erent machines to reduce resource contention. Let β j denote the maximum number of instances a machine j can take based on its capacity constraint (U j ∈ R d ) and diversity preference. We have</p><formula xml:id="formula_9">β j = min{ U 1 j /Θ 1 0 , U 2 j /Θ 2 0 , ..., U d j /Θ d 0 ,</formula><p>α } where α is a parameter that de nes the maximum number of instances each machine can take based on the diverse placement preference. Basically, a smaller α shows a stronger preference of the diverse placement for a stage and we have α &gt;= m/n . From Eq. ( <ref type="formula" target="#formula_7">3</ref>), given that the variable B is a binary matrix, it is an NP-hard Integer Linear Programming (ILP) problem <ref type="bibr" target="#b3">[4]</ref>.</p><p>IPA Method. We design a new solution to IPA based on the following intuition: Since the stage latency is the maximum instance 1 Otherwise, there is a scheduling delay for the stage with the admission control. // Init 3: BP L l is t = cal_bpl(L, X * , Y * ). 4: repeat</p><formula xml:id="formula_10">5: i t , j t = argmax(BP L l is t ) 6:</formula><p>// get the instance and machine index pairs of the largest BPL 7:</p><formula xml:id="formula_11">P = P {x it → jt }, X * = X * -{x it }, S jt = S jt -1. 8:</formula><p>if Y * is ∅ and X * is not ∅ then if</p><formula xml:id="formula_12">S jt == 0 then Y * = Y * -{ jt }, Recalculate the BP L l is t .</formula><p>12:</p><p>end if 13: until X * is empty 14: return P latency, intuitively, the placement plan wants to reduce the latency of the longer-running instances by sending them to the machines where they can run faster, potentially at the cost of compromising the latency of other short-running instances.</p><p>Our IPA method works as follows: We rst prioritize the instances by their best possible latency (BPL), where BPL is de ned as the minimum latency that an instance can achieve among all available machines. Then we keep sending the instance of the largest BPL to its matched machine and updating the BPL for instances when a machine cannot take more instances. The full procedure is given in Algorithm 1. The time complexity is O(m(m +n) +d) using parallelism and vectorized computations in the implementation.</p><p>Optimality Result. Our proof of the IPA result is based on the following column-order assumption about the latency matrix L: For a given column (machine), when we visit the cells in increasing order of latency, let s 1 , . . . , s m denote the indexes of the cells retrieved this way. Then the column-order assumption is that all columns of the L matrix share the same order, s 1 , . . . , s m . See Fig. <ref type="figure">6</ref> for an example of L matrix. This assumption is likely to hold because, in the IPA step, all machines use the same amount of resources, Θ 0 , to process each instance. Hence, the order of latency across di erent instances is strongly correlated with the size of input tuples (or tuples that pass the lter) in those instances, which is independent of the machine used. Empirically, we veri ed that this assumption holds over 88-96% stages across three large production workloads, where the violations largely arise from the uncertainty of the learned model used to generate the L matrix.</p><p>Below is our main optimality result of IPA. All proofs in this paper are left to <ref type="bibr" target="#b21">[22]</ref> due to space limitations. Theorem 5.1. IPA achieves the single-objective stage-latency optimality under the column-order assumption.</p><p>Boosting IPA with clustering. A remaining issue is that a stage can be up to 80K instances in a production workload, and the number of machines can also be tens of thousands. To further reduce the time cost, we exploit the idea that groups of machines or instances may behave similarly in the placement problem. Therefore, we boost the IPA e ciency by clustering both machines and instances without losing much stage latency performance.</p><p>While there exist many clustering methods <ref type="bibr" target="#b0">[1]</ref>, running them on many instances with large MCI features is still an expensive operation for a scheduler. This motivates us to design a customized clustering method based on MCI properties. An instance in a stage is characterized by its query plan (Ch1), instance meta (Ch2), resource plan (Ch3) and additional instance meta (AIM), where Ch1 and Ch3 are the same for all instances, hence not needed in clustering, and AIM fully depends on Ch1 and Ch2. So the key factors lie in Ch2, where the input row number and input size are correlated. Thus, we approximately characterize an instance only by its input row number and apply 1D density-based clustering <ref type="bibr" target="#b0">[1]</ref>. To represent a cluster, we choose the instance with the largest input row number to avoid latency underestimation. A machine in the cluster is characterized by its system states (Ch4) and hardware type (Ch5). We cluster them based on discretized values of Ch4 and Ch5.</p><p>Suppose that clustering yields m instance clusters and n machine clusters. Then the time complexity of IPA is O(m log m + n log n + m (m + n ) + d), where a sorting-based method is used for clustering both instances and machines. Since m m, n n, the complexity reduces to O(m log m + n log n). Compared to other packing algorithms <ref type="bibr" target="#b17">[18]</ref> that solve linear programming problems with quadratic complexity, the complexity of our algorithm is lower.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Resource Assignment Advisor (RAA)</head><p>After IPA determines the placement plan of a stage, each of its instances is scheduled to run on a speci c machine. Then we propose a Resource Assignment Advisor (RAA) to tune the resource plan of each instance to solve a MOO problem, e.g., minimizing the stage latency, cloud cost, as well as other objectives.</p><p>Stage-level MOO. Consider the stage-level MOO problem de ned in Def. 5.2. By ignoring the constant B, we can rewrite it in the following abstract form using aggregators ( 1 , ..., k ), each for one objective, to be applied to m instances:</p><formula xml:id="formula_13">arg min Θ F Θ = F (Θ) =       F 1 (Θ) = 1 (f 1 (Θ 1 ), ..., f 1 (Θ m )) ... F k (Θ) = k (f k (Θ 1 ), ..., f k (Θ m ))       (4) where Θ = [Θ 1 , Θ 2 , ..., Θ m ] = [θ l 1 1 , θ l 2 2 , ...θ l m m ]</formula><p>, θ l i i denotes the l i -th resource con guration of the i-th instance (i ∈ {1, ..., m}), and the aggregator j (j=1,...,k) is either a sum or max.</p><p>As in the instance-level, we de ne the stage-level resource con guration as</p><formula xml:id="formula_14">Θ = [θ l 1 1 , θ l 2 2 , ...θ l m m ],</formula><p>with its corresponding F Θ in the objective space Φ ⊆ R k . Then we can de ne stage-level Pareto Optimality and the Pareto Set similarly as before.</p><p>Note that we are particularly interested in two aggregators. The rst is max: the stage-level value of one objective is the maximum of instance-level objective values over all instances, e.g., latency. The second is sum: the stage-level value of one objective is the sum of instance-level objective values, e.g., cost.</p><p>Example. In Figure <ref type="figure">7</ref>, we have f 1 as the predictive model for latency ( 1 = max) and f 2 for cost ( 2 = sum). Suppose Θ = [θ 1 1 , θ 2 2 ] as the stage-level resource con guration. Then we have the stagelevel latency as F 1 (Θ) = max(150, 100) = 150, the stage-level cost as F 2 (Θ) = sum(5, 5) = 10, and the stage-level solution F Θ is [150, 10].</p><p>While one may consider using existing MOO methods to solve Eq. ( <ref type="formula">4</ref>), it is still subject to a large number (O(md)) of variables. To Algorithm 2: General Hierarchical MOO Require:</p><formula xml:id="formula_15">f j i , i ∈ [1, m], j ∈ [1, p i ],</formula><p>where p i is the number of Pareto-optimal solutions in i-th instance and [θ 1 i , ..., θ De nition 5.3. Hierarchical MOO. We solve the instance-level MOO problem for each of the m instances of a stage separately, for which we can use any existing MOO method (in practice, the Progressive Frontier (PF) algorithm <ref type="bibr" target="#b35">[36]</ref> since it is shown to be the fastest). Suppose that there are k stage-level objectives, each with its max or sum aggregator to be applied to m instances. Our goal is to e ciently nd the stage-level MOO solutions from the instance-level MOO solutions, for which we use f j i to denote the j-th Pareto-optimal solution in the i-th instance by using θ j i . General hierarchical MOO solution. Suppose that there are k user objectives, where k 1 objectives use the max and k 2 objectives use sum, k 1 + k 2 = k. We are also given instance-level Pareto sets. Then we want to select a solution for each instance such that the corresponding stage-level solution is Pareto optimal. Algorithm 2 gives the full description. After initialization, line 2 obtains the lower and upper bounds for each of the k 1 max objectives. In line 3 (f ind_all_possible_ alues), we rst nd for each max objective all the possible values as one list, and then use the Cartesian product of these lists as the candidates for the k 1 max objectives. In lines 4 to 10, given one candidate, we try to nd the corresponding Paretooptimal solution for the k 2 sum objectives. For the k 2 objectives using sum, due to the complexity of the sum operation, we can not a ord the enumeration, which grows exponentially in the number of instances, O(p m max ) where p max is the maximum number of instance-level Pareto points among m instances. To reduce the complexity, we resort to any existing MOO method, denoted by the function nd_optimal, that (in line 6) for each instance selects one Pareto-optimal solution for the k 2 objectives. At the end of the procedure, we add a lter to remove the non-optimal solutions.</p><formula xml:id="formula_16">p i i ] 1: P O Θ = [], P O F = [] 2: minMList, maxMList = nd_range(f ) 3: k1Combs = nd_all_possible_values(f , minMList, maxMList</formula><p>Example. In Figure <ref type="figure">7</ref>, we calculate the lower and upper bounds of the stage-level latency as max(55, 100) = 100 and max(150, 300) = 300 respectively. Then we enumerate all the possible stage-level latency values within the bounds (100, 150, 300) to collect potential Pareto-optimal solutions. For latency=100, there is only one feasible Θ choice (Θ = [θ 2 1 , θ </p><formula xml:id="formula_17">i , θ j i , f j i , ∀i ∈ [1, m], j ∈ [1, p i ],</formula><p>where we pre-sort f j i and θ j i in the descending order of latency for each instance. 1: Q .push((u Fast hierarchical MOO solution: RAA Path. In the case that there are only two objectives, whose aggregators are max and sum, respectively, we provide another algorithm that can resolve the Hierarchical MOO e ciently. The key idea here is that the two objectives are indeed making tradeo s. The increase in one objective often leads to a decrease in the other. With this observation, we design a new approach called "RAA path".</p><formula xml:id="formula_18">u j i = l at (f j i ), ∀i ∈ [1, m], j ∈ [1, p i ]. 2: P O Θ = [], P O F = [], λ = [1, 1, ...,</formula><formula xml:id="formula_19">λ i i , i)</formula><p>Example. Figure <ref type="figure" target="#fig_8">8</ref> shows an example of the Pareto sets of 3 instances in a stage, where each instance has 2, 4, 3 Pareto-optimal solutions, and each solution is sorted by latency in descending order. We start the procedure by selecting the rst Pareto solution for each instance, this leads to Θ λ = [θ 1  1 , θ 1 2 , θ 1 3 ] and F λ = [300, 14]. Notice that 300 is corresponding to θ 1 2 , so we replace θ 1 2 by the entry below it in the same Pareto set. Now we have the second solution as <ref type="bibr" target="#b14">15]</ref>. Continue in this manner; each time, select the θ corresponding to the instance of the max latency and replace it with the item below it in the same Pareto set. It terminates until there is no entry below the current θ .</p><formula xml:id="formula_20">Θ λ = [θ 1 1 , θ 2 2 , θ 1 3 ] and F λ = [150,</formula><p>For a formal description, we use the following notation: (1) λ = [λ 1 , λ 2 , ..., λ m ] as a state, where λ i is the index of the Pareto point in instance i with the λ i -largest latency; (2) </p><formula xml:id="formula_21">Θ λ = [θ λ 1 1 , θ λ 2 2 , ..., θ λ m m ]; (3) π i : λ → λ is a step such that the i t h dimension</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">EXPERIMENTS</head><p>This section presents the evaluation of our models and stage optimizer. Using production traces from MaxCompute, we rst analyze our models and compare them to state-of-the-art modeling techniques. We then report the end-to-end performance of our stage optimizer using a simulator of the extended MaxCompute environment (detailed in <ref type="bibr" target="#b21">[22]</ref>) by replaying the production traces. Workload Characteristics. See Table <ref type="table" target="#tab_3">1</ref> for descriptions of production workloads A-C. We give additional details in <ref type="bibr" target="#b21">[22]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Model Evaluation</head><p>To train models, we partition the traces of workloads A-C into training, validation, and test sets, and tune the hyperparameters of all models using the validation set. As the default, we train MCI+GTN over all channels augmented by AIM for the instance latency prediction. More details about training are given in <ref type="bibr" target="#b21">[22]</ref>.</p><p>We report model accuray on 5 metrics: (1) weighted mean absolute percentage error (WMAPE), (2) median error (MdErr), (3) 95 percentile Error (95%Err), (4) Pearson correlation (Corr), and (5) error of the cloud cost (GlbErr). We choose WMAPE as the primary, also the hardest, metric because it assigns more weights to long-running instances, which are of more importance in resource optimization.</p><p>Expt 1: Performance Pro ling. We report the performance of our best models for each workload in Table <ref type="table" target="#tab_8">3</ref>. First, our best model achieves 9-19% WMAPEs and 7-15% MdErrs over the three workloads with our MCI features (Ch1-Ch5 plus AIM). Second, WMAPE is a more challenging metric than MdErr. The distribution of the instance latency in production workloads is highly skewed to shortand median-running instances. Therefore, MdErr is determined by the majority of the instances but does not capture well those long-running ones. Third, the error rate of the total cloud cost is 3-4.5x smaller than WMAPEs, because individual errors of single instances could have a canceling e ect on the global resource metric, hence better model performance on the global metric.</p><p>Our breakdown analyses further show that IO-intensive operators and the dynamics of system states are the two main sources of model errors. First, by training a separate model for the instancelevel operator latency and calculating the error contribution of each operator type, we nd the top-3 most inaccurate operators as StreamLineWrite, TableScan, and MergeJoin, all involving frequent IO activities. It is likely that current traces in MaxCompute miss the features to fully characterize IO operations. As for the system dynamics, we train a separate model by augmenting system   states with the average system metrics during the lifetime of an instance (which is not realistic to get before running an instance). But this allows us to show that the model further reduces WMAPE and MdErr to 6-12% and 5-10%, pointing the error source to the system dynamics. The comprehensive results are shown in <ref type="bibr" target="#b21">[22]</ref>.</p><p>Expt 2: Multi-channel Inputs. We now investigate the importance of each channel to model performance. We train separate models with di erent input channel choices, including (1) the leave-oneout choices for a channel x (Chx_off), (2) the ve basic channels all_on, and (3) the ve basic channels and the AIM all_on+calib.</p><p>Fig. <ref type="figure" target="#fig_13">9</ref>(a) shows our results. The top-3 important features are the instance meta (Ch2), the query plan (Ch1), and the system states (Ch4). By turning o each, WMAPE gets worse by 18-66%, 16-50%, and 9-27%, respectively, compared to all_on. The e ect of the hardware type (Ch5) is not signi cant, likely because the hardware types used are all high-performance ones. The e ect of the resource plan (Ch3) is small here due to its sparsity in the feature space; e.g., only 26 di erent resource plans are observed in workload B. But their e ects will be di erent once they are tuned widely in MOO.</p><p>Expt 3: Impact of Cardinality. We train separate models by deriving AIMs from di erent cardinalities: (1) all_on+calib represents the stage-oriented cardinality from MaxCompute's CBO;</p><p>(2) all_on+simu1 applies the ground-truth stage-level cardinalities while assuming operators in di erent instances share the stage selectivities; (3) all_on+simu2 applies the ground-truth instancelevel cardinalities for operators. Fig. <ref type="figure" target="#fig_13">9</ref>(b) shows at most a 0.2% and 0.4% WAMPE can be reduced by using all_on+simu1 and all_on+simu2, respectively. This means that improving cardinality estimation alone cannot improve much latency prediction in big data systems, which is consistent with CLEO's observation <ref type="bibr" target="#b34">[35]</ref>.</p><p>Expt 4: Comparison with QPPNet <ref type="bibr" target="#b25">[26]</ref> and TLSTM <ref type="bibr" target="#b36">[37]</ref>. We next compare di erent modeling tools, including (1) original QPPNet, (2) original TLSTM, (3) our extension, MCI-based QPPNet, (4) MCIbased TLSTM, and (5) MCI+GTN (our new graph embedder). Fig. <ref type="figure" target="#fig_13">9(c)</ref> shows our results. First, QPPNet and TLSTM achieve 22-36% and 15-31% WMAPE, respectively, 2-3x larger than our best model, MCI+GTN. Second, using our MCI, MCI+QPPNet and MCI+TLSTM can improve WMAPE by 12-15% and 6-12%, respectively, while MCI+TLSTM and MCI+GTN achieve close performance. Thus, our MCI framework o ers both good modeling performance and extensibility to adapt SOTAs from a single machine to big data systems.</p><p>Expt 5: Training Overhead and Model Adaptivity. We next report on the training overhead: The data preparation stage costs ∼2 minutes every day to collect new traces from each department using MaxCompute jobs (with a parallelism of 219). We collected 5-day traces from three internal departments, totaling 0.62M jobs, 2M stages, and 407G bytes. In the training stage, we run 24h periodic retraining (retrain) at midnight of each day when the workloads are light, which takes 3.5 hours on average over 16 GPUs (including hyperparameter tuning). Optionally, we run ne-tuning every 6 hours (retrain+ netune), at the average cost of 0.9h each.</p><p>To study model adaptivity, we design two settings of workload drifts: (a) a realistic setting where queries from 5 days are injected in temporal order; (b) a hypothetical worst case where queries are injected in decreasing order of latency. In both settings, we compare a static method that trains the model using the data from rst 6 hours and never updates it afterward, with our retrain and retrain+ netune methods. Our results, as shown in Fig. <ref type="figure" target="#fig_14">10</ref> for workload C, include:</p><p>(1) The static approach can reach up to 72% WMAPE in some hours of a day, demonstrating the presence of workload drifts. (2) retrain and retrain+ netune adapt better to the workload drifts, keeping errors mostly in the range of 15-25% after days of training, while having tradeo s between them: If the workload patterns in 24h windows are highly regular, retrain works slightly better than re-train+ netune by avoiding over tting to insigni cant local changes (workload A). Otherwise, retrain+ netune works better by adapting to signi cant local changes (workload B). More discussion is in <ref type="bibr" target="#b21">[22]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Resource Optimization (RO) Evaluation</head><p>We next evaluate the end-to-end performance of our Stage Optimizer (SO) against the current HBO and Fuxi scheduler <ref type="bibr" target="#b62">[63]</ref>, as well as other MOO methods using production workloads A-C. As we cannot run experiments directly in the production clusters, we developed a simulator of the extended MaxCompute and replayed the query traces to conduct our experiments (detailed in <ref type="bibr" target="#b21">[22]</ref>).</p><p>We consider the following metrics in resource optimization (RO): (1) coverage, the ratio of stages that receive feasible solutions within 60s; (2) Lat (in) s , the average stage latency that includes the RO time;</p><p>(3) Cost s , the average cloud cost of all stages in a workload; (4) T s , the RO time cost. Below, we rst present a microbenchmark of our methods and other MOO methods using 29 subworkloads in Table <ref type="table" target="#tab_7">2</ref>, and then report our net bene t over the full dataset in   We rst turn on only the IPA module in the stage optimizer (no RAA) and compare the two options of IPA to the Fuxi scheduler over the 29 subworkloads. As shown in Tab. 2, the clustered version IPA(Cluster) reduces the stage latency (including the solving time) by 12-50% and cost by 4-14%, with the average time cost between 10-33 msec. Without clustering, IPA(Org) achieves comparable reduction rates as IPA(Cluster) but costs 2-83x more time for solving.</p><p>Expt 7: IPA+RAA. We now run IPA(Cluster) with RAA of four choices. IPA+RAA(Path) solves the resource plan by applying RAA Path over instance and machine clusters and achieves the best performance. It reduces the stage latency by 36-80% and cloud cost by 29-75%, with an average time cost between 17-156ms (including IPA). IPA+RAA(W/O_C) does RAA without clustering and su ers high overhead (up to 19s for a stage). IPA+RAA(DBSCAN) applies DBSCAN for the instance clustering, which incurs up to 937 msec for a stage, hence ine cient for production use. IPA+RAA(General) shows the performance of our general hierarchical MOO approach, which is slightly worse than IPA+RAA(Path) (in this 2D MOO problem) in running time while o ering a similar reduction of latency and cost. Details of the four choices are in <ref type="bibr" target="#b21">[22]</ref>.</p><p>Expt 8: MOO baselines. We next compare to SOTA MOO solutions, EVO <ref type="bibr" target="#b7">[8]</ref>, WS(Sample) <ref type="bibr" target="#b26">[27]</ref>, and PF(MOGD) <ref type="bibr" target="#b35">[36]</ref>, using Def. 5.2. See <ref type="bibr" target="#b21">[22]</ref> for their implementation details. As shown in the 3 red rows of Table <ref type="table" target="#tab_7">2</ref>, (1) over 29 sub-workloads, none of them guarantees to return all results within 60s; (2) their latency and cost reduction rates are all dominated by IPA+RAA(Path), and even lose to the Fuxi scheduler on some workloads; (3) their solving time is 1-2 magnitude higher than our approach, making them infeasible to be used by a cloud scheduler. As an alternative, we apply IPA to solve the B variables and these MOO methods to solve only Θ based on Eq. ( <ref type="formula">4</ref>). As shown in the last 3 blue rows in Table <ref type="table" target="#tab_7">2</ref>, they are still inferior to IPA+RAA(Path) in both latency and cost reduction and in running time (mostly taking 1-6 sec to complete).</p><p>Expt 9: Net Bene ts. We next compare our SO (IPA+RAA) against Fuxi's scheduling results by considering the model e ects: the noisefree case means that the predicted latency is the true latency, while the noisy case captures the fact that the true latency is di erent from the predicted one. We run the entire 2M stages over 3 departments in both noisy and noise-free cases. For the noisy case, we turn on the actual latency simulator (GPR model) to simulate the actual latency. Speci cally, given a predicted instance latency, GPR generates a Gaussian distribution (N (µ, σ )) of the actual latency, and samples from the distribution within µ ± 3σ . Table <ref type="table" target="#tab_5">4</ref> shows that in both noise-free and noisy settings, SO (IPA+RAA) signi cantly reduces stage latency and cloud cost compared to Fuxi, with the ne-grained MCI+GTN model for latency prediction.</p><p>Expt 10: Impact of Model Accuracy. Finally, to quantify the impact of model accuracy on resource optimization, we use GPR models pre-trained over three bootstrap models (MCI+GTN, TLSTM, QPP-Net). Note that in terms of model accuracy, we have (MCI+GTN &gt; TLSTM &gt; QPPNet), as shown in Fig. <ref type="figure" target="#fig_13">9(c</ref>). We borrow Fuxi's groundtruth placement plan and ask RAA for resource plans of each stage under the same condition of the system states as Fuxi. To be fair in the comparison, we dropped the scheduling delays for all stages and set the stage latency as the maximum instance latency. Table <ref type="table" target="#tab_5">4</ref> compares the RAA's reduction rate (RR) among di erent bootstrap models. These results show that indeed, better reduction rates are achieved by using a more accurate model, which validates the importance of having a ne-grained accurate prediction model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CONCLUSIONS</head><p>We presented a MaxCompute <ref type="bibr" target="#b27">[28]</ref> based big data system that supports multi-objective resource optimization via ne-grained instancelevel modeling and optimization. To suit the complexity of our system, we developed ne-grained instance-level models that encode all relevant information as multi-channel inputs to deep neural networks. By exploiting these models, our stage optimizer employs a new IPA module to derive a latency-aware placement plan to reduce the stage latency, and a novel RAA model to derive instancespeci c resource plans to further reduce stage latency and cost in a hierarchical MOO framework. Evaluation using production workloads shows that (1) our best model achieved 7-15% median error and 9-19% weighted mean absolute percentage error; (2) compared to the Fuxi scheduler <ref type="bibr" target="#b62">[63]</ref>, IPA+RAA achieved the reduction of 37-72% latency and 43-78% cost while running in 0.02-0.23s.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The lifecycle of a query job in MaxCompute</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Timeline of 6716 instances in a stage</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Trace overview</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Extended system architecture for resource optimization</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: MCI-based modeling framework</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fuxi=&gt; Stage latency = 24s 1 )Θ0 - 7 = 6 = 2 )</head><label>1762</label><figDesc>Figure 6: Example of IPA</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 : 2 )</head><label>72</label><figDesc>Figure 7: Example of RAA</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Example of RAA path</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Algorithm 1 :</head><label>1</label><figDesc>IPA Approach 1: L = cal_latency(model, X, Y), S = cal_max_num_inst(). 2: Y * = Y , X * = X , and P = { }.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>) 4 :</head><label>4</label><figDesc>for c in k1Combs do 5: for i in m do 6: optimal_solution, index = nd_optimal(c, [f 1 i , ...f .append(Θ), P O F .append(F (Θ)) 10: end for 11: P O F , P O Θ = lter_dominated(P O F , P O Θ ) 12: return P O F , P O Θ enable a fast algorithm, we next introduce our Hierarchical MOO approach developed in the divide-and-conquer paradigm.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>) 13 :Proposition 5 . 1 .</head><label>1351</label><figDesc>until True and [300, 9] with Θ = [θ 1 1 , θ 1 2 ]. The rst one will be ltered because it is dominated by the latter one (line 6). Finally, we further lter solutions being dominated in the chosen set (line 11) and get the stage-level MOO solutions as [[100, 25], [150, 10], [300, 9]]. For a stage-level MOO problem, Algorithm 2 guarantees to nd a subset of stage-level Pareto optimal points.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Performance of our instance-level models, compared to the state-of-the-art (SOTA) methods</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: WMAPEs over time (workload C)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>2 2 ]) and hence the only solution is [100, 25]. Similarly for latency=150, we get another solution [150, 10]. When latency=300, there are two solutions: [300, 24] with Θ = [θ 2 1 , θ 1 2 ] Algorithm 3: RAA Path policy to construct PO Θ and PO F</figDesc><table><row><cell>Require: p</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1 :</head><label>1</label><figDesc>in state λ is increased by 1. (4) p i is the number of instance-level Pareto solutions for instance i. Then Algorithm 3 gives the RAA Path algorithm. Proposition 5.2. For a stage-level MOO problem with two objectives using max and sum, respectively, RAA path guarantees to nd the full set of stage-level Pareto optimal points at the complexity of O(m • p max log(m • p max )).RAA with Clustering. For e ciency, we run both the General hierarchical MOO and RAA Path methods by clustering the instances and machines, where m is replaced by m &lt;&lt; m in the complexity. Workload statistics for 3 workload over 5 days Resource plan recommendation. After getting the stage-level Pareto set, we reuse UDAO's Weighted Utopia Nearest (WUN) strategy to recommend the resource plan, which includes a con guration for each instance of the stage. It recommends the resource plan whose objectives could achieve the smallest distance to the Utopia point, which is the hypothetical optimal in all objectives.</figDesc><table><row><cell>WL</cell><cell>Num. Jobs</cell><cell>Num. stages</cell><cell>Num. Inst</cell><cell>#stages /job</cell><cell>#insts /stage</cell><cell>#ops /stage</cell><cell>Avg Job Lat(s)</cell><cell>Avg Stage Lat(s)</cell><cell>Avg Inst Lat(s)</cell></row><row><cell cols="4">A 405K 970K 34M</cell><cell>2.40</cell><cell cols="2">35.45 3.71</cell><cell>30.97</cell><cell>14.64</cell><cell>16.85</cell></row><row><cell cols="4">B 173K 858K 36M</cell><cell>4.95</cell><cell cols="3">42.02 6.27 120.15</cell><cell>39.72</cell><cell>15.63</cell></row><row><cell cols="4">C 41K 100K 50M</cell><cell cols="4">2.42 505.51 5.31 376.83</cell><cell>181.88</cell><cell>71.08</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 .</head><label>4</label><figDesc></figDesc><table><row><cell>Coverage</cell><cell>Lat</cell><cell>(in) s</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>↓</head><label></label><figDesc>Cost s ↓ a (T s ) (ms) / max(T s ) (ms)</figDesc><table><row><cell>SO choice</cell><cell>A B C</cell><cell>A B C</cell><cell>A B C</cell><cell>A</cell><cell>B</cell><cell>C</cell></row><row><cell cols="3">IPA(Org) 100% 100% 100% 11% 20% 51%</cell><cell cols="2">5% 9% 15% 280 / 2.0K</cell><cell cols="2">17 / 18 1.4K / 1.8K</cell></row><row><cell cols="3">IPA(Cluster) 100% 100% 100% 12% 17% 50%</cell><cell>4% 7% 14%</cell><cell>15 / 24</cell><cell>10 / 10</cell><cell>33 / 36</cell></row><row><cell cols="2">IPA+RAA(W/O_C) 100% 100% 100%</cell><cell>8% 79% 58%</cell><cell cols="4">31% 76% 75% 2.5K / 19K 177 / 220 3.5K / 6.3K</cell></row><row><cell cols="3">IPA+RAA(DBSCAN) 100% 100% 100% 27% 69% 67%</cell><cell cols="4">21% 64% 74% 223 / 937 132 / 136 258 / 452</cell></row><row><cell cols="3">IPA+RAA(General) 100% 100% 100% 36% 80% 76%</cell><cell cols="2">29% 75% 75% 100 / 241</cell><cell>20 / 23</cell><cell>167 / 229</cell></row><row><cell cols="3">IPA+RAA(Path) 100% 100% 100% 36% 80% 76%</cell><cell cols="2">29% 75% 75% 98 / 226</cell><cell>17 / 18</cell><cell>156 / 224</cell></row><row><cell>EVO</cell><cell>0% 82% 0%</cell><cell>-% -36% -%</cell><cell>-% 66% -%</cell><cell>-/ -</cell><cell>21K / 24K</cell><cell>-/ -</cell></row><row><cell cols="7">WS(Sample) 90% 85% 82% -140% 48% -74% -107% 49% -52% 7.4K / 22K 465 / 753 9.8K / 12K</cell></row><row><cell cols="2">PF(MOGD) 99% 100% 98%</cell><cell>-15% 49% 65%</cell><cell cols="4">24% 56% 75% 2.7K / 4.0K 2.1K / 2.5K 1.5K / 2.3K</cell></row><row><cell cols="2">IPA+EVO 98% 100% 98%</cell><cell>-27% 69% 43%</cell><cell cols="4">22% 75% 71% 3.0K / 7.3K 2.4K / 2.6K 5.0K / 5.9K</cell></row><row><cell cols="3">IPA+WS(Sample) 100% 100% 100% -36% 76% -1%</cell><cell cols="4">-19% 72% 32% 3.5K / 10K 517 / 741 12K / 18K</cell></row><row><cell cols="3">IPA+PF(MOGD) 100% 100% 100% -0.4% 51% 69%</cell><cell cols="4">26% 56% 75% 1.6K / 2.5K 1.2K / 1.6K 1.2K / 2.2K</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 2 :</head><label>2</label><figDesc>Average Reduction Rate (RR) against Fuxi in 29 subworkloads within 60s</figDesc><table><row><cell cols="3">WL WMAPE MdErr 95%Err Corr GlbErr</cell></row><row><cell>A B C</cell><cell>8.6% 19.0% 15.1%</cell><cell>7.4% 62.4% 96.6% 1.9% 15.1% 71.5% 96.4% 5.4% 12.7% 97.3% 98.4% 5.1%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 3 :</head><label>3</label><figDesc>Modeling Performance</figDesc><table><row><cell>SO scenario</cell><cell>Stage Lat (in):</cell><cell>Cost:</cell></row><row><cell>IPA (noise-free)</cell><cell cols="2">10%, 15%, 44% 3%, 7%, 12%</cell></row><row><cell>IPA (noisy)</cell><cell>9%, 10%, 42%</cell><cell>3%, 6%, 12%</cell></row><row><cell cols="3">IPA+RAA (noise-free) 37%, 58%, 72% 43%, 56%, 78%</cell></row><row><cell>IPA+RAA (noisy)</cell><cell cols="2">37%, 55%, 72% 42%, 56%, 78%</cell></row><row><cell>Bootstrap Model</cell><cell>Stage Lat (in):</cell><cell>Cost:</cell></row><row><cell>GTN+MCI</cell><cell>34%,49%,68%</cell><cell>48%,41%,71%</cell></row><row><cell>TLSTM</cell><cell>17%,2%,65%</cell><cell>43%,31%,70%</cell></row><row><cell>QPPNet</cell><cell>34%,1%,63%</cell><cell>46%,30%,68%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 4 :</head><label>4</label><figDesc>Average RR over 2M stages Expt 6: IPA Only.</figDesc><table /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>ACKNOWLEDGMENTS</head><p>This work was partially supported by the <rs type="funder">European Research Council (ERC)</rs> <rs type="programName">Horizon 2020 research and innovation programme</rs> (grant <rs type="grantNumber">n725561</rs>), <rs type="funder">Alibaba Group</rs> through <rs type="programName">Alibaba Innovative Research Program</rs>, and <rs type="funder">China Scholarship Council (CSC)</rs>. We also thank <rs type="person">Yongfeng Chai</rs>, <rs type="person">Daoyuan Chen</rs>, <rs type="person">Xiaozong Cui</rs>, <rs type="person">Botong Huang</rs>, <rs type="person">Xiaofeng Zhang</rs>, and <rs type="person">Yang Zhang</rs> from the <rs type="affiliation">Alibaba Group</rs> for the discussion and help throughout the project.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_KmHJRn3">
					<idno type="grant-number">n725561</idno>
					<orgName type="program" subtype="full">Horizon 2020 research and innovation programme</orgName>
				</org>
				<org type="funding" xml:id="_pcfam68">
					<orgName type="program" subtype="full">Alibaba Innovative Research Program</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<ptr target="http://www.crcpress.com/product/isbn/9781466558212" />
		<title level="m">Data Clustering: Algorithms and Applications</title>
		<editor>
			<persName><forename type="first">C</forename><surname>Charu</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Chandan</forename><forename type="middle">K</forename><surname>Aggarwal</surname></persName>
		</editor>
		<editor>
			<persName><surname>Reddy</surname></persName>
		</editor>
		<imprint>
			<publisher>CRC Press</publisher>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Towards Plan-aware Resource Allocation in Serverless Query Processing</title>
		<author>
			<persName><forename type="first">Malay</forename><surname>Bag</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alekh</forename><surname>Jindal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hiren</forename><surname>Patel</surname></persName>
		</author>
		<ptr target="https://www.usenix.org/conference/hotcloud20/presentation/bag" />
	</analytic>
	<monogr>
		<title level="m">12th USENIX Workshop on Hot Topics in Cloud Computing</title>
		<editor>
			<persName><forename type="first">Amar</forename><surname>Phanishayee</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Ryan</forename><surname>Stutsman</surname></persName>
		</editor>
		<meeting><address><addrLine>HotCloud</addrLine></address></meeting>
		<imprint>
			<publisher>USENIX Association</publisher>
			<date type="published" when="2020-07-13">2020. 2020. July 13-14, 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Hyracks: A exible and extensible foundation for data-intensive computing</title>
		<author>
			<persName><forename type="first">R</forename><surname>Vinayak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Borkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raman</forename><surname>Carey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicola</forename><surname>Grover</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rares</forename><surname>Onose</surname></persName>
		</author>
		<author>
			<persName><surname>Vernica</surname></persName>
		</author>
		<idno>1151-1162</idno>
	</analytic>
	<monogr>
		<title level="m">ICDE</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Introduction to Algorithms</title>
		<author>
			<persName><forename type="first">H</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><forename type="middle">E</forename><surname>Cormen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ronald</forename><forename type="middle">L</forename><surname>Leiserson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cli</forename><surname>Rivest</surname></persName>
		</author>
		<author>
			<persName><surname>Ord Stein</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>The MIT Press</publisher>
		</imprint>
	</monogr>
	<note>Third Edition. 3rd ed.</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Di erentiable Expected Hypervolume Improvement for Parallel Multi-Objective Bayesian Optimization</title>
		<author>
			<persName><forename type="first">Maximilian</forename><surname>Samuel Daulton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eytan</forename><surname>Balandat</surname></persName>
		</author>
		<author>
			<persName><surname>Bakshy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.05078</idno>
		<ptr target="https://arxiv.org/abs/2006.05078" />
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">MapReduce: simpli ed data processing on large clusters</title>
		<author>
			<persName><forename type="first">Je</forename><surname>Rey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dean</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjay</forename><surname>Ghemawat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">OSDI&apos;04: Proceedings of the 6th conference on Symposium on Opearting Systems Design &amp; Implementation</title>
		<meeting><address><addrLine>San Francisco, CA; Berkeley, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>USENIX Association</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="10" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Optimistic Recovery for Iterative Data ows in Action</title>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Dudoladov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Schelter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Asterios</forename><surname>Katsifodimos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephan</forename><surname>Ewen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kostas</forename><surname>Tzoumas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Volker</forename><surname>Markl</surname></persName>
		</author>
		<idno type="DOI">10.1145/2723372.2735372</idno>
		<ptr target="https://doi.org/10.1145/2723372.2735372" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 ACM SIGMOD International Conference on Management of Data</title>
		<meeting>the 2015 ACM SIGMOD International Conference on Management of Data<address><addrLine>Melbourne, Victoria, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-05-31">2015. May 31 -June 4, 2015</date>
			<biblScope unit="page" from="1439" to="1443" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A Tutorial on Multiobjective Optimization: Fundamentals and Evolutionary Methods</title>
		<author>
			<persName><forename type="first">T</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">André</forename><forename type="middle">H</forename><surname>Emmerich</surname></persName>
		</author>
		<author>
			<persName><surname>Deutz</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11047-018-9685-y</idno>
		<ptr target="https://doi.org/10.1007/s11047-018-9685-y" />
	</analytic>
	<monogr>
		<title level="j">Natural Computing: an international journal</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="585" to="609" />
			<date type="published" when="2018-09">2018. Sept. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Building a HighLevel Data ow System on top of MapReduce: The Pig Experience</title>
		<author>
			<persName><forename type="first">Alan</forename><surname>Gates</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olga</forename><surname>Natkovich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shubham</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pradeep</forename><surname>Kamath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shravan</forename><surname>Narayanam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Olston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Santhosh</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Utkarsh</forename><surname>Srivastava</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PVLDB</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1414" to="1425" />
			<date type="published" when="2009">2009. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The Cascades Framework for Query Optimization</title>
		<author>
			<persName><forename type="first">Goetz</forename><surname>Graefe</surname></persName>
		</author>
		<ptr target="http://sites.computer.org/debull/95SEP-CD.pdf" />
	</analytic>
	<monogr>
		<title level="j">IEEE Data Eng. Bull</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="19" to="29" />
			<date type="published" when="1995">1995. 1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep Learning Models for Selectivity Estimation of Multi-Attribute Queries</title>
		<author>
			<persName><forename type="first">Shohedul</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saravanan</forename><surname>Thirumuruganathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jees</forename><surname>Augustine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Koudas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gautam</forename><surname>Das</surname></persName>
		</author>
		<idno type="DOI">10.1145/3318464.3389741</idno>
		<ptr target="https://doi.org/10.1145/3318464.3389741" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 International Conference on Management of Data, SIGMOD Conference 2020</title>
		<editor>
			<persName><forename type="first">David</forename><surname>Maier</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Rachel</forename><surname>Pottinger</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Anhai</forename><surname>Doan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Wang-Chiew</forename><surname>Tan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Abdussalam</forename><surname>Alawini</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Hung</forename><forename type="middle">Q</forename><surname>Ngo</surname></persName>
		</editor>
		<meeting>the 2020 International Conference on Management of Data, SIGMOD Conference 2020<address><addrLine>Portland, OR, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2020-06-14">2020. June 14-19, 2020</date>
			<biblScope unit="page" from="1035" to="1050" />
		</imprint>
	</monogr>
	<note>online conference</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Predictive Entropy Search for Multi-objective Bayesian Optimization</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Hernández-Lobato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">José</forename><surname>Miguel Hernández-Lobato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amar</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><forename type="middle">P</forename><surname>Adams</surname></persName>
		</author>
		<ptr target="http://proceedings.mlr.press/v48/hernandez-lobatoa16.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33nd International Conference on Machine Learning, ICML 2016</title>
		<editor>
			<persName><forename type="first">Maria-Florina</forename><surname>Balcan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Kilian</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</editor>
		<meeting>the 33nd International Conference on Machine Learning, ICML 2016<address><addrLine>New York City, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-06-19">2016. June 19-24, 2016</date>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="1492" to="1501" />
		</imprint>
	</monogr>
	<note>JMLR Workshop and Conference Proceedings</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Trident: Task Scheduling over Tiered Storage Systems in Big Data Platforms</title>
		<author>
			<persName><forename type="first">Herodotos</forename><surname>Herodotou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elena</forename><surname>Kakoulli</surname></persName>
		</author>
		<ptr target="http://www.vldb.org/pvldb/vol14/p1570-herodotou.pdf" />
	</analytic>
	<monogr>
		<title level="j">Proc. VLDB Endow</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1570" to="1582" />
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Parametric Query Optimization for Linear and Piecewise Linear Cost Functions</title>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Hulgeri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sudarshan</surname></persName>
		</author>
		<ptr target="http://dl.acm.org/citation.cfm?id=1287369.1287385" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Very Large Data Bases</title>
		<meeting>the 28th International Conference on Very Large Data Bases<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="167" to="178" />
		</imprint>
	</monogr>
	<note>VLDB &apos;02). VLDB Endowment</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Morpheus: Towards Automated SLOs for Enterprise Clusters</title>
		<author>
			<persName><forename type="first">Abdu</forename><surname>Sangeetha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlo</forename><surname>Jyothi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ishai</forename><surname>Curino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shravan</forename><surname>Menache</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Matthur Narayanamurthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Tumanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Yaniv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iñigo</forename><surname>Mavlyutov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Subru</forename><surname>Goiri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Janardhan</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sriram</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName><surname>Rao</surname></persName>
		</author>
		<ptr target="https://www.usenix.org/conference/osdi16/technical-sessions/presentation/jyothi" />
	</analytic>
	<monogr>
		<title level="m">12th USENIX Symposium on Operating Systems Design and Implementation, OSDI 2016</title>
		<meeting><address><addrLine>Savannah, GA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-11-02">2016. November 2-4, 2016</date>
			<biblScope unit="page" from="117" to="134" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Schedule Optimization for Data Processing Flows on the Cloud</title>
		<author>
			<persName><forename type="first">Herald</forename><surname>Kllapi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eva</forename><surname>Sitaridi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Manolis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yannis</forename><surname>Tsangaris</surname></persName>
		</author>
		<author>
			<persName><surname>Ioannidis</surname></persName>
		</author>
		<idno type="DOI">10.1145/1989323.1989355</idno>
		<ptr target="https://doi.org/10.1145/1989323.1989355" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2011 ACM SIGMOD International Conference on Management of Data</title>
		<meeting>the 2011 ACM SIGMOD International Conference on Management of Data<address><addrLine>Athens, Greece; New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="289" to="300" />
		</imprint>
	</monogr>
	<note>) (SIGMOD &apos;11)</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Towards Cost-Optimal Query Processing in the Cloud</title>
		<author>
			<persName><forename type="first">Viktor</forename><surname>Leis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maximilian</forename><surname>Kuschewski</surname></persName>
		</author>
		<ptr target="http://www.vldb.org/pvldb/vol14/p1606-leis.pdf" />
	</analytic>
	<monogr>
		<title level="j">Proc. VLDB Endow</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1606" to="1612" />
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Resource Bricolage for Parallel Database Systems</title>
		<author>
			<persName><forename type="first">Jiexing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Je</forename><surname>Rey F. Naughton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rimma</forename><forename type="middle">V</forename><surname>Nehme</surname></persName>
		</author>
		<ptr target="http://www.vldb.org/pvldb/vol8/p25-Li.pdf" />
	</analytic>
	<monogr>
		<title level="j">PVLDB</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="25" to="36" />
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Model-free Control for Distributed Stream Data Processing Using Deep Reinforcement Learning</title>
		<author>
			<persName><forename type="first">Teng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanzhi</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.14778/3184470.3184474</idno>
		<ptr target="https://doi.org/10.14778/3184470.3184474" />
	</analytic>
	<monogr>
		<title level="m">Proc. VLDB Endow</title>
		<meeting>VLDB Endow</meeting>
		<imprint>
			<date type="published" when="2018-02">2018. Feb. 2018</date>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="705" to="718" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Fauce: Fast and Accurate Deep Ensembles with Uncertainty for Cardinality Estimation</title>
		<author>
			<persName><forename type="first">Jie</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenqian</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qingqing</forename><surname>Zhou</surname></persName>
		</author>
		<ptr target="http://www.vldb.org/pvldb/vol14/p1950-liu.pdf" />
	</analytic>
	<monogr>
		<title level="j">Proc. VLDB Endow</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="1950" to="1963" />
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Pretraining Summarization Models of Structured Datasets for Cardinality Estimation</title>
		<author>
			<persName><forename type="first">Yao</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Srikanth</forename><surname>Kandula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arnd</forename><surname>Christian König</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Surajit</forename><surname>Chaudhuri</surname></persName>
		</author>
		<ptr target="http://www.vldb.org/pvldb/vol15/p414-lu.pdf" />
	</analytic>
	<monogr>
		<title level="j">Proc. VLDB Endow</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="414" to="426" />
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Fine-Grained Modeling and Optimization for Intelligent Resource Management in Big Data Processing</title>
		<author>
			<persName><forename type="first">Chenghao</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arnab</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanlei</forename><surname>Diao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yihui</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaliang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingren</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.2207.02026</idno>
		<ptr target="https://doi.org/10.48550/ARXIV.2207.02026" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">MB2: Decomposed Behavior Modeling for Self-Driving Database Management Systems</title>
		<author>
			<persName><forename type="first">Lin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wuwen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Butrovich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wan</forename><forename type="middle">Shen</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prashanth</forename><surname>Menon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Pavlo</surname></persName>
		</author>
		<idno type="DOI">10.1145/3448016.3457276</idno>
		<ptr target="https://doi.org/10.1145/3448016.3457276" />
	</analytic>
	<monogr>
		<title level="m">SIGMOD &apos;21: International Conference on Management of Data</title>
		<editor>
			<persName><forename type="first">Guoliang</forename><surname>Li</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Zhanhuai</forename><surname>Li</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Stratos</forename><surname>Idreos</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Divesh</forename><surname>Srivastava</surname></persName>
		</editor>
		<meeting><address><addrLine>Virtual Event, China</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2021-06-20">2021. June 20-25, 2021</date>
			<biblScope unit="page" from="1248" to="1261" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">WiSeDB: A Learning-based Workload Management Advisor for Cloud Databases</title>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Marcus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olga</forename><surname>Papaemmanouil</surname></persName>
		</author>
		<ptr target="http://www.vldb.org/pvldb/vol9/p780-marcus.pdf" />
	</analytic>
	<monogr>
		<title level="j">PVLDB</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="780" to="791" />
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Neo: A Learned Query Optimizer</title>
		<author>
			<persName><forename type="first">Ryan</forename><forename type="middle">C</forename><surname>Marcus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Parimarjan</forename><surname>Negi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongzi</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Alizadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Kraska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olga</forename><surname>Papaemmanouil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nesime</forename><surname>Tatbul</surname></persName>
		</author>
		<idno type="DOI">10.14778/3342263.3342644</idno>
		<ptr target="https://doi.org/10.14778/3342263.3342644" />
	</analytic>
	<monogr>
		<title level="m">Proc. VLDB Endow</title>
		<meeting>VLDB Endow</meeting>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="1705" to="1718" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Plan-Structured Deep Neural Network Models for Query Performance Prediction</title>
		<author>
			<persName><forename type="first">Ryan</forename><forename type="middle">C</forename><surname>Marcus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olga</forename><surname>Papaemmanouil</surname></persName>
		</author>
		<idno type="DOI">10.14778/3342263.3342646</idno>
		<ptr target="https://doi.org/10.14778/3342263.3342646" />
	</analytic>
	<monogr>
		<title level="j">Proc. VLDB Endow</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1733" to="1746" />
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Survey of multi-objective optimization methods for engineering</title>
		<author>
			<persName><forename type="first">Regina</forename><surname>Marler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J S</forename><surname>Arora</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Structural and Multidisciplinary Optimization</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="369" to="395" />
			<date type="published" when="2004">2004. 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<ptr target="https://www.alibabacloud.com/product/maxcompute" />
		<title level="m">Open Data Processing Service</title>
		<imprint/>
	</monogr>
	<note>MaxCompute</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">From Dubious Construction of Objective Functions to the Application of Physical Programming</title>
		<author>
			<persName><forename type="first">Achille</forename><surname>Messac</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AIAA Journal</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="155" to="163" />
			<date type="published" when="2012">2012. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">The normalized normal constraint method for generating the Pareto frontier</title>
		<author>
			<persName><forename type="first">Achille</forename><surname>Messac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amir</forename><surname>Ismailyahaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">A</forename><surname>Mattson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Structural and Multidisciplinary Optimization</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="86" to="98" />
			<date type="published" when="2003">2003. 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Naiad: A Timely Data ow System</title>
		<author>
			<persName><forename type="first">Derek</forename><forename type="middle">G</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Mcsherry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rebecca</forename><surname>Isaacs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martín</forename><surname>Abadi</surname></persName>
		</author>
		<idno type="DOI">10.1145/2517349.2522738</idno>
		<ptr target="https://doi.org/10.1145/2517349.2522738" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Fourth ACM Symposium on Operating Systems Principles (Farminton, Pennsylvania) (SOSP &apos;13)</title>
		<meeting>the Twenty-Fourth ACM Symposium on Operating Systems Principles (Farminton, Pennsylvania) (SOSP &apos;13)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="439" to="455" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Flow-Loss: Learning Cardinality Estimates That Matter</title>
		<author>
			<persName><forename type="first">Parimarjan</forename><surname>Negi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><forename type="middle">C</forename><surname>Marcus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongzi</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nesime</forename><surname>Tatbul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Kraska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Alizadeh</surname></persName>
		</author>
		<ptr target="http://www.vldb.org/pvldb/vol14/p2019-negi.pdf" />
	</analytic>
	<monogr>
		<title level="j">Proc. VLDB Endow</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="2019" to="2032" />
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Weighted Distinct Sampling: Cardinality Estimation for SPJ Queries</title>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yilei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ke</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feifei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chaoqun</forename><surname>Zhan</surname></persName>
		</author>
		<idno type="DOI">10.1145/3448016.3452821</idno>
		<ptr target="https://doi.org/10.1145/3448016.3452821" />
	</analytic>
	<monogr>
		<title level="m">SIGMOD &apos;21: International Conference on Management of Data</title>
		<editor>
			<persName><forename type="first">Guoliang</forename><surname>Li</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Zhanhuai</forename><surname>Li</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Stratos</forename><surname>Idreos</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Divesh</forename><surname>Srivastava</surname></persName>
		</editor>
		<meeting><address><addrLine>Virtual Event, China</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2021-06-20">2021. June 20-25, 2021</date>
			<biblScope unit="page" from="1465" to="1477" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">PerfOrator: eloquent performance models for Resource Optimization</title>
		<author>
			<persName><forename type="first">Dharmesh</forename><surname>Kaushik Rajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlo</forename><surname>Kakadia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Subru</forename><surname>Curino</surname></persName>
		</author>
		<author>
			<persName><surname>Krishnan</surname></persName>
		</author>
		<idno type="DOI">10.1145/2987550.2987566</idno>
		<ptr target="https://doi.org/10.1145/2987550.2987566" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventh ACM Symposium on Cloud Computing</title>
		<meeting>the Seventh ACM Symposium on Cloud Computing<address><addrLine>Santa Clara, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-10-05">2016. October 5-7, 2016</date>
			<biblScope unit="page" from="415" to="427" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Cost Models for Big Data Query Processing: Learning, Retro tting, and Our Findings</title>
		<author>
			<persName><forename type="first">Tarique</forename><surname>Siddiqui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alekh</forename><surname>Jindal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shi</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hiren</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wangchao</forename><surname>Le</surname></persName>
		</author>
		<idno type="DOI">10.1145/3318464.3380584</idno>
		<ptr target="https://doi.org/10.1145/3318464.3380584" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 International Conference on Management of Data, SIGMOD Conference 2020</title>
		<editor>
			<persName><forename type="first">David</forename><surname>Maier</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Rachel</forename><surname>Pottinger</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Anhai</forename><surname>Doan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Wang-Chiew</forename><surname>Tan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Abdussalam</forename><surname>Alawini</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Hung</forename><forename type="middle">Q</forename><surname>Ngo</surname></persName>
		</editor>
		<meeting>the 2020 International Conference on Management of Data, SIGMOD Conference 2020<address><addrLine>Portland, OR, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2020-06-14">2020. June 14-19, 2020</date>
			<biblScope unit="page" from="99" to="113" />
		</imprint>
	</monogr>
	<note>online conference</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Spark-based Cloud Data Analytics using Multi-Objective Optimization</title>
		<author>
			<persName><forename type="first">Fei</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Khaled</forename><surname>Zaouk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenghao</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arnab</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanlei</forename><surname>Diao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prashant</forename><forename type="middle">J</forename><surname>Shenoy</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICDE51399.2021.00041</idno>
		<ptr target="https://doi.org/10.1109/ICDE51399.2021.00041" />
	</analytic>
	<monogr>
		<title level="m">37th IEEE International Conference on Data Engineering, ICDE 2021</title>
		<meeting><address><addrLine>Chania, Greece</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021-04-19">2021. April 19-22, 2021</date>
			<biblScope unit="page" from="396" to="407" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">An End-to-End Learning-based Cost Estimator</title>
		<author>
			<persName><forename type="first">Ji</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guoliang</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.14778/3368289.3368296</idno>
		<ptr target="https://doi.org/10.14778/3368289.3368296" />
	</analytic>
	<monogr>
		<title level="j">Proc. VLDB Endow</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="307" to="319" />
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Learned Cardinality Estimation for Similarity Queries</title>
		<author>
			<persName><forename type="first">Ji</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guoliang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Tang</surname></persName>
		</author>
		<idno type="DOI">10.1145/3448016.3452790</idno>
		<ptr target="https://doi.org/10.1145/3448016.3452790" />
	</analytic>
	<monogr>
		<title level="m">SIGMOD &apos;21: International Conference on Management of Data</title>
		<editor>
			<persName><forename type="first">Guoliang</forename><surname>Li</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Zhanhuai</forename><surname>Li</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Stratos</forename><surname>Idreos</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Divesh</forename><surname>Srivastava</surname></persName>
		</editor>
		<meeting><address><addrLine>Virtual Event, China</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2021-06-20">2021. June 20-25, 2021</date>
			<biblScope unit="page" from="1745" to="1757" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Improved Semantic Representations From Tree-Structured Long Short-Term Memory Networks</title>
		<author>
			<persName><forename type="first">Kai</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tai</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<idno type="DOI">10.3115/v1/p15-1150</idno>
		<ptr target="https://doi.org/10.3115/v1/p15-1150" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing, ACL 2015</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing, ACL 2015<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<publisher>The Association for Computer Linguistics</publisher>
			<date type="published" when="2015-07-26">2015. July 26-31, 2015</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1556" to="1566" />
		</imprint>
	</monogr>
	<note>Long Papers</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Tempo: robust and self-tuning resource management in multi-tenant parallel databases</title>
		<author>
			<persName><forename type="first">Zilong</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shivnath</forename><surname>Babu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the VLDB Endowment</title>
		<meeting>the VLDB Endowment</meeting>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="720" to="731" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Hive -A Warehousing Solution Over a Map-Reduce Framework</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Thusoo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sen</forename><surname>Joydeep</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Namit</forename><surname>Sarma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prasad</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suresh</forename><surname>Chakka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Anthony</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pete</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raghotham</forename><surname>Wycko</surname></persName>
		</author>
		<author>
			<persName><surname>Murthy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PVLDB</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1626" to="1629" />
			<date type="published" when="2009">2009. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Approximation Schemes for Many-objective Query Optimization</title>
		<author>
			<persName><forename type="first">Immanuel</forename><surname>Trummer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christoph</forename><surname>Koch</surname></persName>
		</author>
		<idno type="DOI">10.1145/2588555.2610527</idno>
		<ptr target="https://doi.org/10.1145/2588555.2610527" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 ACM SIGMOD International Conference on Management of Data</title>
		<meeting>the 2014 ACM SIGMOD International Conference on Management of Data<address><addrLine>Snowbird, Utah, USA; New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1299" to="1310" />
		</imprint>
	</monogr>
	<note>SIGMOD &apos;14)</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Multi-objective Parametric Query Optimization</title>
		<author>
			<persName><forename type="first">Immanuel</forename><surname>Trummer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christoph</forename><surname>Koch</surname></persName>
		</author>
		<idno type="DOI">10.14778/2735508.2735512</idno>
		<ptr target="https://doi.org/10.14778/2735508.2735512" />
	</analytic>
	<monogr>
		<title level="j">Proc. VLDB Endow</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="221" to="232" />
			<date type="published" when="2014-11">2014. Nov. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">An Incremental Anytime Algorithm for Multi-Objective Query Optimization</title>
		<author>
			<persName><forename type="first">Immanuel</forename><surname>Trummer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christoph</forename><surname>Koch</surname></persName>
		</author>
		<idno type="DOI">10.1145/2723372.2746484</idno>
		<ptr target="https://doi.org/10.1145/2723372.2746484" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 ACM SIG-MOD International Conference on Management of Data</title>
		<meeting>the 2015 ACM SIG-MOD International Conference on Management of Data<address><addrLine>Melbourne, Victoria, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1941">2015. May 31 -June 4, 2015. 1941-1953</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Leveraging Query Logs and Machine Learning for Query Optimization</title>
		<author>
			<persName><forename type="first">Kapil</forename><surname>Vaidya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anshuman</forename><surname>Dutt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Vivek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Surajit</forename><surname>Narasayya</surname></persName>
		</author>
		<author>
			<persName><surname>Chaudhuri</surname></persName>
		</author>
		<ptr target="http://www.vldb.org/pvldb/vol15/p401-vaidya.pdf" />
	</analytic>
	<monogr>
		<title level="j">Proc. VLDB Endow</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="401" to="413" />
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Automatic Database Management System Tuning Through Large-scale Machine Learning</title>
		<author>
			<persName><forename type="first">Dana</forename><surname>Van Aken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Pavlo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geo</forename><surname>Rey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bohan</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.1145/3035918.3064029</idno>
		<ptr target="https://doi.org/10.1145/3035918.3064029" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 ACM International Conference on Management of Data</title>
		<meeting>the 2017 ACM International Conference on Management of Data<address><addrLine>Chicago, Illinois, USA; New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1009" to="1024" />
		</imprint>
	</monogr>
	<note>SIGMOD &apos;17)</note>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Van Steen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Tanenbaum</surname></persName>
		</author>
		<title level="m">Distributed Systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note>3 ed.</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Apache Hadoop YARN: yet another resource negotiator</title>
		<author>
			<persName><forename type="first">Vinod</forename><forename type="middle">Kumar</forename><surname>Vavilapalli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arun</forename><forename type="middle">C</forename><surname>Murthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Douglas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharad</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mahadev</forename><surname>Konar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Lowe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hitesh</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siddharth</forename><surname>Seth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bikas</forename><surname>Saha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlo</forename><surname>Curino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O'</forename><surname>Owen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjay</forename><surname>Malley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Radia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName><surname>Baldeschwieler</surname></persName>
		</author>
		<idno type="DOI">10.1145/2523616.2523633</idno>
		<ptr target="https://doi.org/10.1145/2523616.2523633" />
	</analytic>
	<monogr>
		<title level="m">ACM Symposium on Cloud Computing, SOCC &apos;13</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Guy</surname></persName>
		</editor>
		<editor>
			<persName><surname>Lohman</surname></persName>
		</editor>
		<meeting><address><addrLine>Santa Clara, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013-10-01">2013. October 1-3, 2013</date>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Query and Resource Optimization: Bridging the Gap</title>
		<author>
			<persName><forename type="first">Lalitha</forename><surname>Viswanathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alekh</forename><surname>Jindal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Konstantinos</forename><surname>Karanasos</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICDE.2018.00156</idno>
		<ptr target="https://doi.org/10.1109/ICDE.2018.00156" />
	</analytic>
	<monogr>
		<title level="m">34th IEEE International Conference on Data Engineering, ICDE 2018</title>
		<meeting><address><addrLine>Paris, France</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2018-04-16">2018. April 16-19, 2018</date>
			<biblScope unit="page" from="1384" to="1387" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">FACE: A Normalizing Flow based Cardinality Estimator</title>
		<author>
			<persName><forename type="first">Jiayi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengliang</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiabin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guoliang</forename><surname>Li</surname></persName>
		</author>
		<ptr target="http://www.vldb.org/pvldb/vol15/p72-li.pdf" />
	</analytic>
	<monogr>
		<title level="j">Proc. VLDB Endow</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="72" to="84" />
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">PostCENN: PostgreSQL with Machine Learning Models for Cardinality Estimation</title>
		<author>
			<persName><forename type="first">Lucas</forename><surname>Woltmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dominik</forename><surname>Olwig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Claudio</forename><surname>Hartmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dirk</forename><surname>Habich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wolfgang</forename><surname>Lehner</surname></persName>
		</author>
		<ptr target="http://www.vldb.org/pvldb/vol14/p2715-woltmann.pdf" />
	</analytic>
	<monogr>
		<title level="j">Proc. VLDB Endow</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="2715" to="2718" />
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Towards a Learning Optimizer for Shared Clouds</title>
		<author>
			<persName><forename type="first">Chenggang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alekh</forename><surname>Jindal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saeed</forename><surname>Amizadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hiren</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wangchao</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shi</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sriram</forename><surname>Rao</surname></persName>
		</author>
		<idno type="DOI">10.14778/3291264.3291267</idno>
		<ptr target="https://doi.org/10.14778/3291264.3291267" />
	</analytic>
	<monogr>
		<title level="j">Proc. VLDB Endow</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="210" to="222" />
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">A Uni ed Deep Model of Learning from both Data and Queries for Cardinality Estimation</title>
		<author>
			<persName><forename type="first">Peizhi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gao</forename><surname>Cong</surname></persName>
		</author>
		<idno type="DOI">10.1145/3448016.3452830</idno>
		<ptr target="https://doi.org/10.1145/3448016.3452830" />
	</analytic>
	<monogr>
		<title level="m">SIGMOD &apos;21: International Conference on Management of Data</title>
		<editor>
			<persName><forename type="first">Guoliang</forename><surname>Li</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Zhanhuai</forename><surname>Li</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Stratos</forename><surname>Idreos</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Divesh</forename><surname>Srivastava</surname></persName>
		</editor>
		<meeting><address><addrLine>Virtual Event, China</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009">2021. June 20-25, 2021. 2009-2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">BayesCard: Revitilizing Bayesian Frameworks for Cardinality Estimation</title>
		<author>
			<persName><forename type="first">Ziniu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amir</forename><surname>Shaikhha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxing</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingren</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.2012.14743</idno>
		<ptr target="https://doi.org/10.48550/ARXIV.2012.14743" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Shark: SQL and rich analytics at scale</title>
		<author>
			<persName><forename type="first">Reynold</forename><forename type="middle">S</forename><surname>Xin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Josh</forename><surname>Rosen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matei</forename><surname>Zaharia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Franklin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Shenker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ion</forename><surname>Stoica</surname></persName>
		</author>
		<idno type="DOI">10.1145/2463676.2465288</idno>
		<ptr target="https://doi.org/10.1145/2463676.2465288" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 ACM SIGMOD International Conference on Management of Data</title>
		<meeting>the 2013 ACM SIGMOD International Conference on Management of Data<address><addrLine>New York, New York, USA; New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="13" to="24" />
		</imprint>
	</monogr>
	<note>SIGMOD &apos;13)</note>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">NeuroCard: One Cardinality Estimator for All Tables</title>
		<author>
			<persName><forename type="first">Zongheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amog</forename><surname>Kamsetty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sifei</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ion</forename><surname>Stoica</surname></persName>
		</author>
		<idno type="DOI">10.14778/3421424.3421432</idno>
		<ptr target="https://doi.org/10.14778/3421424.3421432" />
	</analytic>
	<monogr>
		<title level="j">Proc. VLDB Endow</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="61" to="73" />
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Deep Unsupervised Cardinality Estimation</title>
		<author>
			<persName><forename type="first">Zongheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amog</forename><surname>Kamsetty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenggang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><forename type="middle">M</forename><surname>Hellerstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjay</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ion</forename><surname>Stoica</surname></persName>
		</author>
		<idno type="DOI">10.14778/3368289.3368294</idno>
		<ptr target="https://doi.org/10.14778/3368289.3368294" />
	</analytic>
	<monogr>
		<title level="m">Proc. VLDB Endow</title>
		<meeting>VLDB Endow</meeting>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="279" to="292" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Graph Transformer Networks</title>
		<author>
			<persName><forename type="first">Seongjun</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minbyul</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raehyun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaewoo</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyunwoo</forename><forename type="middle">J</forename><surname>Kim</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2019/hash/9d63484abb477c97640154d40595a3bb-Abstract.html" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Hanna</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Hugo</forename><surname>Wallach</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Alina</forename><surname>Larochelle</surname></persName>
		</editor>
		<editor>
			<persName><surname>Beygelzimer</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Emily</forename><forename type="middle">B</forename><surname>Florence D'alché-Buc</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Roman</forename><surname>Fox</surname></persName>
		</editor>
		<editor>
			<persName><surname>Garnett</surname></persName>
		</editor>
		<meeting><address><addrLine>NeurIPS; Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-12-08">2019. 2019. December 8-14, 2019</date>
			<biblScope unit="page" from="11960" to="11970" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Resilient distributed datasets: a fault-tolerant abstraction for in-memory cluster computing</title>
		<author>
			<persName><forename type="first">Matei</forename><surname>Zaharia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mosharaf</forename><surname>Chowdhury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tathagata</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ankur</forename><surname>Dave</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Justin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Murphy</forename><surname>Mccauley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Franklin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Shenker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ion</forename><surname>Stoica</surname></persName>
		</author>
		<ptr target="http://dl.acm.org/citation.cfm?id=2228298.2228301" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 9th USENIX conference on Networked Systems Design and Implementation</title>
		<meeting>the 9th USENIX conference on Networked Systems Design and Implementation<address><addrLine>San Jose, CA; Berkeley, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>USENIX Association</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="2" to="2" />
		</imprint>
	</monogr>
	<note>) (NSDI&apos;12)</note>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">UDAO: A Next-Generation Uni ed Data Analytics Optimizer</title>
		<author>
			<persName><forename type="first">Khaled</forename><surname>Zaouk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenghao</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arnab</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanlei</forename><surname>Diao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prashant</forename><forename type="middle">J</forename><surname>Shenoy</surname></persName>
		</author>
		<idno type="DOI">10.14778/3352063.3352103</idno>
		<ptr target="https://doi.org/10.14778/3352063.3352103" />
	</analytic>
	<monogr>
		<title level="j">PVLDB</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1934" to="1937" />
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">An End-to-End Automatic Cloud Database Tuning System Using Deep Reinforcement Learning</title>
		<author>
			<persName><forename type="first">Ji</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ke</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guoliang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhili</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiashu</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yangtao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianheng</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minwei</forename><surname>Ran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zekang</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.1145/3299869.3300085</idno>
		<ptr target="https://doi.org/10.1145/3299869.3300085" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 International Conference on Management of Data</title>
		<meeting>the 2019 International Conference on Management of Data<address><addrLine>Amsterdam, Netherlands; New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="415" to="432" />
		</imprint>
	</monogr>
	<note>SIGMOD &apos;19)</note>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">ResTune: Resource Oriented Tuning Boosted by Meta-Learning for Cloud Databases</title>
		<author>
			<persName><forename type="first">Xinyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuo</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuowei</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feifei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tieying</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Cui</surname></persName>
		</author>
		<idno type="DOI">10.1145/3448016.3457291</idno>
		<idno>2102-2114</idno>
		<ptr target="https://doi.org/10.1145/3448016.3457291" />
	</analytic>
	<monogr>
		<title level="m">SIGMOD &apos;21: International Conference on Management of Data</title>
		<editor>
			<persName><forename type="first">Guoliang</forename><surname>Li</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Zhanhuai</forename><surname>Li</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Stratos</forename><surname>Idreos</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Divesh</forename><surname>Srivastava</surname></persName>
		</editor>
		<meeting><address><addrLine>Virtual Event, China</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2021-06-20">2021. June 20-25, 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Fuxi: a Fault-Tolerant Resource Management and Job Scheduling System at Internet Scale</title>
		<author>
			<persName><forename type="first">Zhuo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yangyu</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Renyu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hong</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Xu</surname></persName>
		</author>
		<idno type="DOI">10.14778/2733004.2733012</idno>
		<ptr target="https://doi.org/10.14778/2733004.2733012" />
	</analytic>
	<monogr>
		<title level="j">Proc. VLDB Endow</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="1393" to="1404" />
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">SCOPE: parallel databases meet MapReduce</title>
		<author>
			<persName><forename type="first">Jingren</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Bruno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Chuan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Per-Ake</forename><surname>Larson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ronnie</forename><surname>Chaiken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Darren</forename><surname>Shakib</surname></persName>
		</author>
		<idno type="DOI">10.1007/s00778-012-0280-z</idno>
		<ptr target="https://doi.org/10.1007/s00778-012-0280-z" />
	</analytic>
	<monogr>
		<title level="j">The VLDB Journal</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="611" to="636" />
			<date type="published" when="2012-10">2012. Oct. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Query Performance Prediction for Concurrent Queries using Graph Embedding</title>
		<author>
			<persName><forename type="first">Xuanhe</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guoliang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianhua</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. VLDB Endow</title>
		<meeting>VLDB Endow</meeting>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
			<biblScope unit="page" from="1416" to="1428" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">FLAT: Fast, Lightweight and Accurate Method for Cardinality Estimation</title>
		<author>
			<persName><forename type="first">Rong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziniu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxing</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Pfadler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengping</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingren</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Cui</surname></persName>
		</author>
		<ptr target="http://www.vldb.org/pvldb/vol14/p1489-zhu.pdf" />
	</analytic>
	<monogr>
		<title level="j">Proc. VLDB Endow</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1489" to="1502" />
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Phoebe: A Learning-based Checkpoint Optimizer</title>
		<author>
			<persName><forename type="first">Yiwen</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matteo</forename><surname>Interlandi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhishek</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Krishnadhan</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hiren</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Malay</forename><surname>Bag</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hitesh</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alekh</forename><surname>Jindal</surname></persName>
		</author>
		<ptr target="http://www.vldb.org/pvldb/vol14/p2505-zhu.pdf" />
	</analytic>
	<monogr>
		<title level="j">Proc. VLDB Endow</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="2505" to="2518" />
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">ClassyTune: A Performance Auto-Tuner for Systems in the Cloud</title>
		<author>
			<persName><forename type="first">Yuqing</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianxun</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Cloud Computing</title>
		<imprint>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">BestCon g: tapping the performance potential of systems via automatic con guration tuning</title>
		<author>
			<persName><forename type="first">Yuqing</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianxun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mengying</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yungang</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenlong</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuoyue</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kunpeng</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yingchun</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Symposium on Cloud Computing Santa Clara California</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="338" to="350" />
			<date type="published" when="2017-09">2017. September, 2017. 2017</date>
		</imprint>
	</monogr>
	<note>SoCC</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
