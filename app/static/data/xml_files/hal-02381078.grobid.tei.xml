<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Different Flavors of Attention Networks for Argument Mining</title>
				<funder ref="#_fZJaXws">
					<orgName type="full">European Union</orgName>
				</funder>
				<funder ref="#_NRxK6rT">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Johanna</forename><surname>Frau</surname></persName>
							<email>jfrau@famaf.unc.edu.ar</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Natural Language Processing Group</orgName>
								<orgName type="institution">FAMAFyC-UNC</orgName>
								<address>
									<settlement>Córdoba</settlement>
									<country key="AR">Argentina</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Milagro</forename><surname>Teruel</surname></persName>
							<email>mteruel@unc.edu.ar</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Natural Language Processing Group</orgName>
								<orgName type="institution">FAMAFyC-UNC</orgName>
								<address>
									<settlement>Córdoba</settlement>
									<country key="AR">Argentina</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Laura</forename><forename type="middle">Alonso</forename><surname>Alemany</surname></persName>
							<email>lauraalonsoalemany@unc.edu.ar</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Natural Language Processing Group</orgName>
								<orgName type="institution">FAMAFyC-UNC</orgName>
								<address>
									<settlement>Córdoba</settlement>
									<country key="AR">Argentina</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Serena</forename><surname>Villata</surname></persName>
							<email>villata@i3s.unice.fr</email>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">Université Côte d&apos;Azur. CNRS</orgName>
								<orgName type="institution" key="instit2">Inria</orgName>
								<address>
									<region>I3S</region>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Different Flavors of Attention Networks for Argument Mining</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">DFB24E26958EF00B8C27A8B8A4BFFAEB</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-04-12T14:45+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Argument mining is a rising area of Natural Language Processing (NLP) concerned with the automatic recognition and interpretation of argument components and their relations. Neural models are by now mature technologies to be exploited for automating the argument mining tasks, despite the issue of data sparseness. This could ease much of the manual effort involved in these tasks, taking into account heterogeneous types of texts and topics. In this work, we evaluate different attention mechanisms applied over a state-of-the-art architecture for sequence labeling. We assess the impact of different flavors of attention in the task of argument component detection over two datasets: essays and legal domain. We show that attention not only models the problem better but also supports interpretability.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Argument Mining <ref type="bibr" target="#b5">(Peldszus and Stede 2013;</ref><ref type="bibr" target="#b4">Lippi and Torroni 2016;</ref><ref type="bibr" target="#b0">Cabrio and Villata 2018)</ref> tackles a very complex phenomenon, involving several levels of human communication and cognition. It has been defined as "the general task of analyzing discourse on the pragmatics level and applying a certain argumentation theory to model and automatically analyze the data at hand" <ref type="bibr" target="#b2">(Habernal and Gurevych 2017)</ref>. Due to the complexity of the task, data-driven approaches require a huge amount of data to properly characterize the phenomena and find patterns that can be exploited by a classifier. However, most of the corpora annotated for this task are small, and they cannot be used in combination because they are based on different theoretical frameworks (e.g., abstract and structured argumentation) or cover different genres (e.g., political debates, social network posts). Our primary research domain is argument mining in legal cases, where corpora are especially scarce.</p><p>In this paper, we analyze the utility of neural attention mechanisms to improve the tasks of Argument Mining. It has been shown that some kind of attention improves the performance of neural-based argument mining systems <ref type="bibr" target="#b8">Stab et al. (2018)</ref>. It seems that attention mechanisms concentrate the classifier on the most determinant elements to take into account, and thus direct the training of the classifier to a better convergence point. Moreover, attention has a promise of Copyright c 2019, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved. interpretability, since the parts of the input that receive more attention can be recognized.</p><p>We assess the impact of adding an attention mechanism to a state-of-the-art neural model (BiLSTM with character embeddings and a CRF layer) <ref type="bibr" target="#b6">(Reimers and Gurevych 2017)</ref> <ref type="foot" target="#foot_0">1</ref> .We apply different flavours of attention and compare their impact on performance. We assess the impact of performance over two very different corpora for Argument Mining: a corpus of persuasive essays <ref type="bibr" target="#b7">(Stab and Gurevych 2017)</ref> and a small corpus of the legal domain, consisting of only 8 judgments <ref type="bibr" target="#b9">(Teruel et al. 2018)</ref> of the European Court of Human Rights (ECHR).<ref type="foot" target="#foot_1">2</ref> . We focus on the argument component detection task, distinguishing claims and justifications or focusing on claim detection alone.</p><p>We show that attention mechanisms consistently improve performance. Additionally, it helps to identify words that are important for the Argument Mining task, useful for guidelines and selection of examples for annotation.</p><p>In the rest of the paper we present relevant work, describe the architecture of the attention-based system, then we detail our experimental setting and we discuss the obtained results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related work</head><p>In this section, we report on the approaches proposed in the area of Argument Mining employing recurrent neural networks and attention mechanisms. For state-of-the-art on argument mining, we refer the reader to <ref type="bibr" target="#b0">(Cabrio and Villata 2018)</ref>. Recurrent neural networks have been successfully applied to the problem of Argument Mining. In <ref type="bibr" target="#b1">Eger et al. (2017)</ref>, a single model is trained to jointly classify argumentative components and the relations of attack and support between them. The model is evaluated over the dataset of argumentative essays. <ref type="bibr" target="#b8">Stab et al. (2018)</ref> applied attention to crowd-sourced general domain argumentative essays from the Web, to measure how relevant each word on the input is, with respect to the topic of the essay.</p><p>Our final aim is to find a good system for argument mining in legal cases as the ones in the ECHR corpus. In legal documents, the length and complexity of the texts is bigger than for essays, there is no pre-defined topic and the spans to be detected are intra-sentential, which makes the task considerably more complex than for essays.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Architecture</head><p>In this section, we first describe the Bidirectional Long Short-Term Memory (BiLSTM) we adopted for our Argument Mining task, and second we present the attention mechanism we empowered the BiLSTM with.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>BiLSTM architecture</head><p>Our proposed model adds an attention mechanism over the recurrent architecture implemented by <ref type="bibr" target="#b6">(Reimers and Gurevych 2017)</ref>, which we will briefly describe in this section. The base of the model is a bidirectional recurrent layer with LSTM units. The input layer is composed of three parts: a word embedding, a character embedding and a casing embedding. The word embedding uses pre-trained weights, supporting any type of word vectors. The character embeddings generate a representation for each word by performing either a convolution operation or a recurrence operation at a character level. The objective of this type of embeddings is to generate a representation of the out-of-vocabulary (OOV) words based on their morphology. The weights for both methods are trained with the network. The casing embedding generates a very small representation of the word using features like presence of digits or all uppercase characters. Finally, the model replaces the traditional classification layer, a dense layer with a softmax activation, with a Conditional Random Field (CRF) layer <ref type="bibr" target="#b3">(Huang, Xu, and Yu 2015)</ref>. A CRF takes into account the surrounding labels to calculate the label for the current word. This mechanism should produce more consistent labels, to avoid for example starting a claim in the middle of a premise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The attention mechanism</head><p>Attention mechanisms, in simple terms, weight the output of a layer with importance scores. These scores are used to increase the values of inputs which are relevant to the prediction of the correct output, and to decrease the values of inputs that are not relevant. Different types of attention mechanisms arise depending on how we calculate the attention scores: we can use only the information from each example individually, or use the whole set of examples.</p><p>In this work, we propose a simple attention mechanism applied directly to the input words of the sentence, visualized in Figure <ref type="figure" target="#fig_0">1</ref>. The goal of placing the attention layer before the recurrent layer is to weight which words in the sequence are relevant to the task we want to solve. This type of attention, called inner attention, was firstly proposed by <ref type="bibr" target="#b10">(Wang, Liu, and Zhao 2016)</ref>.</p><p>The attention layer added to the base RNN model is composed of two parts: one that calculates the attention scores, and a merge function ⊗ that combines them with the original input. As a result of this combination, the attention scores regulate how much of the initial value of the cells in the input are passed to the following layer, increasing or alternatively turning off certain values.</p><p>The attention scores are calculated using a fully connected layer. This layer has the following parameters: a weight matrix W A , a bias b A vector, and an activation function f . Once we obtain the attention scores, the merge function (in this case multiplication) is applied pointwise. Let it be x a single sequence with n timesteps and m features (input of the network), s the attention scores, and x A the output of the attention layer, then the operations performed inside the attention layer are:</p><formula xml:id="formula_0">s = f (xW A + b A ) x A = x ⊗ s (1)</formula><p>Note that, to obtain the attention score s i (corresponding to the i-th input on the sequence), we are multiplying each row x i by the parameter W A . There is an important implication to this equation: the attention scores for each cell in the input are computed taking into account only the features in that timestep input, and not using any information present on the rest of the elements in the sequence. We call this approach word attention.</p><p>This may seem counter intuitive, as we expect the importance of a word in the sentence not to be only determined by itself, but also by the words with which it co-occurs, by the property of compositionality of sentential semantics. We have also explored a different attention mechanism, where the attention score for feature j in instance x i is calculated using the value of feature j in all timesteps of the sequence. The model, which only differs in two transpositions, is described in Equation . We call this approach context attention.</p><formula xml:id="formula_1">s = f (x T W A + b A ) x A = x ⊗ s T</formula><p>(2)</p><p>As we can see, all the operations of the two attention layers are differentiable, and we can optimize its parameters along with the training of the original model.</p><p>The selection of the activation function f has also an important impact. We propose three different options: linear activation, a sigmoid function, and an hyperbolic tangent function. If we use linear activation, the values of the attention scores are not limited to any range, and could possibly explode to huge values, become negative or vanish to nearly zeros. However, in our experiments the values remain in reasonable intervals (between 1 and 10). Furthermore, linear activation makes the attention scores a linear combination of the original values, but we expect the pointwise multiplication with the mean score of the instance to effectively add expressiveness to the model. The sigmoid function, on the other hand, introduces non linearity and forces the attention scores to the (0, 1) interval. After the application of the scores, all the input activations are decreased. This could lead to a faster vanishing of the neuron's signals as they propagate through the network. Finally, the hyperbolic tangent (tanh) function does not have this problem, as it constraints the scores to the interval (-1, 1). However, it enables negative attention, which does not have an intuitive interpretation. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experimental Setting Datasets</head><p>We applied the model on two datasets developed for the Argument Mining task of argument component detection. The essays dataset consists of 400 argumentative essays (148,000 words) written by students in response to controversial topics <ref type="bibr" target="#b7">(Stab and Gurevych 2017)</ref>. In this dataset, the argument components are separated in Claims, Major Claims and Premises, but we collapsed the distinction between Major Claims and Claims to make it homogeneous with the ECHR corpus.</p><p>Our primary research interest concerns a small corpus of the European Court of Human Rights (ECHR dataset) <ref type="bibr" target="#b9">(Teruel et al. 2018)</ref>, with 8 judgments (28,847 words). It has, 329 claims and 401 premises. As input for the attention learners, each sequence is an entire paragraph, and the classifier is expected to detect all components within the paragraph.</p><p>A major difference between these two corpora, besides their sizes, is the length of the sequences. For the essays corpus, the longest paragraph is 98 words long, while the longest paragraph in the ECHR documents is 317 words long. The maximal length determines the length of the sequences given to the classifier for each corpus. The longer the sequence, the more difficult the task of calculating context attention, as context distributes attention along the whole length of the sequence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Evaluation procedure and metrics</head><p>To evaluate our models using the ECHR dataset, we use a 8 fold cross validation where, in each fold, we leave out one entire document for testing, instead of just leaving out a subset of examples. We consider this setting to be more representative of the real case scenario of a production system, where the new instances are documents never seen before. As the essays dataset has more documents, we perform the evaluation on 9 holdout documents.</p><p>The evaluation metric used is the F1-score, the harmonic mean between precision and recall. All values reported are averaged between classes with a weighted strategy, to avoid giving too much weight to the minority B-classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Hyperparameter search</head><p>During the experiment phase, we noted that, in the ECHR dataset, models were highly sensitive to the architecture and hyperparameter selection. Contrary to the results on previous works, simpler models without character embeddings and CRF do not perform necessarily worse. As a result, we use a random search to find successful combinations of hyperparameters. The evaluated combinations are:</p><p>• BiLSTM layer with identical layers of 30, 50, 100, or 200 units.</p><p>• Mini batches sizes of 30, 50, 100, or 200.</p><p>• Dropout on the LSTM layer of 0.1, 0.2, 0.3, 0.4, or 0.5.</p><p>• Classification layer with softmax activation and with CRF activation.</p><p>• Character embeddings with convolutions and with recurrence, resulting on vectors of sizes 16, 32 or 64.</p><p>The optimizer used is Adam, with a clip norm of 1. All classifiers are trained during 50 epochs, stopping if there is no improvement for 10 epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Evaluation</head><p>In this section, we first discuss the results we obtain with our classifier for argument component detection empowered with attention, distinguishing claims and premises and focusing on claim detection alone. Claim detection is a simpler, better defined task which still retains utility in applications. Indeed, <ref type="bibr" target="#b9">Teruel et al. (2018)</ref> show that inter-annotator agreement is higher for claims than for premises. Thus the training and evaluation data for claim detection are more consistent, therefore it provides for a fair evaluation.</p><p>Secondly, we present how attention mechanisms can be used to provide an explanation of the output of the Argument Mining system, to be integrated to increase consistency of annotations (as in guidelines for annotators) or as a criterion to select examples to be annotated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Classifier performance</head><p>Tables <ref type="table" target="#tab_0">1</ref> and<ref type="table">2</ref> show the best results obtained after hyperparameter search for the essays corpus and the ECHR corpus, respectively. As could be expected, we obtain better results for the task of claim detection than when we distinguish claims, premises and non-components. Also, for this task, attention does not provide an improvement over not using attention.</p><p>The performance for the ECHR is also worse than for essays. This is probably due to various factors: the essays corpus is bigger, but also sentences in that corpus are shorter, and argument components are less complex. Concerning the configuration of the classifiers, we can see that word attention systematically obtains better results than context attention, with context attention performing worse than a system with no attention in some cases. It has to be noted that context attention has a very strong parameter, the length of the context to be used to find attention. The longer the context, the more computational resources are required. This is an interesting parameter to explore and may have a big impact in improving the performance of context attention, especially if we take into account the extensive length of paragraphs in the ECHR corpus. This will be explored in future work.</p><p>Results for the rest of hyperparameters are not conclusive. In Figure <ref type="figure" target="#fig_1">2</ref> we can see the distribution of F1-Scores on different configurations of classifiers for the task of argumentative component detection, for the ECHR and the essays dataset, respectively. These classifiers were trained for the selection of hyperparameters with random combinations, as described in the previous section.</p><p>We can see that there is a wide range of variability across results. For the essays corpus, we can see that the difference in performance between word attention and context attention falls within the range of variability of different parameters with the same kind of attention. This means that the combination of hyperparameters affects performance more importantly than the kind of attention. However, word attention tends to have slightly better performance and more stable (less variable results).</p><p>In the case of the ECHR corpus, word attention performs clearly better than context attention, beyond the impact of the rest of hyperparameters. This is probably due to the fact that sequences given to the classifier are whole paragraphs, which are thrice longer than the sequences for the essays corpus. The task of context modelling is more difficult when sequences are longer. In contrast, the formulaic, repetitive nature of legal text may be useful to concentrate attention in particular words or word sequences, which is particularly adequate for word attention mechanisms.</p><p>We cannot see an important difference in variability across activation functions: the three kinds of activation present wide ranges of variability.</p><p>However, the effects that activation have in the distribution of the attention scores are intrinsically different. For both datasets, the scores given with the sigmoid and tanh activation concentrate on values greater than 0.9, and few words obtain a low attention score. In contrast, linear activation concentrates the scores in a middle value, usually close to 3, with a gaussian-like distribution. We have observed classifiers where linear activation concentrates the attention scores close to zero, effectively shutting down the signals from irrelevant words. We are currently working on establishing a correspondence between these patterns of distribution of attention and improvements in performance or interpretability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Visualizing attention</head><p>To further explore the effects of attention, we visualize the attention scores assigned by a model. We propose a visualization where the intensity of the color over each word in the text denotes its attention. The scale of intensity is linear, normalized according to attention values. The color itself represents the label assigned by the classifier: red for claims, blue for premises, gray for the O class. Words misclassified by the model are underlined.</p><p>In Figure <ref type="figure" target="#fig_2">3</ref> we show the labels and attention scores assigned by a word attention, sigmoid classifier for claim de-  We can see that words with low attention are irrelevant to detect claims, like stopwords. In contrast, words with high attention are very relevant, as is the case of "inadmissible / admissible" or "conclude". This kind of information is very useful to include in annotation guidelines to enhance interannotator agreement, and also to speed up annotation, by focusing the attention of annotators in more important information.</p><p>Context attention provides an entirely different kind of information, as can be seen in Figure <ref type="figure" target="#fig_3">4</ref>, showing the result of a classifier with context attention in the essays corpus, distinguishing claims and premises.</p><p>Context attention weights differently the word on each occurrence. It systematically gives more importance to words at the beginning of the sentence and to the two words at the end of the sentence, because they are strong signals of the beginning of an argument component, whereas the rest of words in the sentence are not. This behaviour has its roots on the argument structure, as it is not likely that a new component will start at the end of the sentence.</p><p>Punctuation acting as sentence separators ( . ) receive a high value of attention, as they denote the start of a new component. In the case of commas ( , ) we have seen mixed results, as they can be part of a component as well. It is interesting to note that this behaviour is captured by both types of attention.</p><p>Regarding activation functions, the linear activation previously described favours more evenly distributed, nonextreme attention scores. As a result, it generates a less saturated visualization than the sigmoid activation. We consider this type of activation better suitable for explaining the results, as we can identify words in more distributed attention values.</p><p>With the word attention mechanism, we can extract the words with more attention. We show them in a wordcloud on Figure <ref type="figure" target="#fig_4">5</ref>. This visualization highlights the words that are more important to decide any classification label. In the case of the essays, where the language and the argumentation are more informal, we see mainly modal verbs, connectors and adverbs. The implication is that the model is not defining the argumentation by the semantics of its words, instead using the intention of the speaker represented at the start of the phrase. Adverbs like really and somehow exemplify the more sentimental nature of the argumentations in student essays. This information can be readily included in guidelines or to select new instances for annotation. For example, we can select instances without words for which we already have obtained high attention values in already annotated instances, thus gaining insight for new words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusions</head><p>In this paper, we present an analysis of the impact of neural attention mechanisms in the task of training Argument Mining systems for argument component detection. Argument Mining is a particularly challenging task for machine learning approaches because training corpora are typically small, and the phenomena to represent are very complex, with impact in various linguistic levels, thus it suffers from acute data sparseness. Our assumption was that attention mechanisms could help concentrate this sparse information. We have found that some attention mechanisms do improve the performance of Argument Mining systems, if only slightly. We have found that for the task of claim detection, which is more well-defined, the performance is higher, but the improvement provided by attention mechanisms is approximately the same in both tasks. We have also found that word-level attention produces consistently better results than context-level attention. However, an important parameter of context-level attention, the length of the sequence, remains to be explored. The rest of the hyperparameters do not seem to provide consistently better results, although they produce important variations in performance.</p><p>Even if the improvement in performance is small, we have gained on explanainability of the results: we can now analyze how classifiers make errors with more insight, as we have stronger indicators of the information wherefrom they make their conclusions. This helps to improve the training corpora and the classifiers that exploit them.</p><p>These results have been found to hold on two very different corpora: a reference corpus of student essays and our primary focus of research, a small corpus of judgments of the European Court of Human Rights. We expect that the interpretability provided by attention mechanisms, focused on words, will help in enhancing this corpus, by increasing the consistency of annotations, improving inter-annotator agreement. It will also be used to select examples with words for which the model has no strong attention, thus helping explore regions of the space that are not well characterized yet.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Attention mechanism applied to the Char embedding + BiLSTM + CRF architecture.</figDesc><graphic coords="4,129.60,54.00,352.79,218.10" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Distribution of F1-Scores on different architectures in the ECHR and Essays dataset, for the argumentative component classification task.</figDesc><graphic coords="6,79.20,54.00,453.61,219.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Attention scores assigned to a fragment of the ECHR corpus for the claim detection task. The model uses word attention with sigmoid activation.</figDesc><graphic coords="6,65.93,325.32,214.64,98.53" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Attention scores assigned to a fragment of the essays corpus for the component detection (claim-premise) task. The model uses context attention with linear activation.</figDesc><graphic coords="6,325.46,325.32,226.57,70.73" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Words with most attention in the essays corpus, with word attention. Size is proportional to attention score.</figDesc><graphic coords="7,71.89,54.00,202.72,109.26" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Best results obtained for claim detection (left) and component detection (right) after hyperparameter search in the essays corpus.</figDesc><table><row><cell></cell><cell cols="4">Claim Detection Claim -Premise</cell></row><row><cell></cell><cell>Acc</cell><cell>F1</cell><cell>Acc</cell><cell>F1</cell></row><row><cell>No attention</cell><cell cols="2">0.844 0.841</cell><cell cols="2">0.696 0.683</cell></row><row><cell>Word + lin</cell><cell cols="2">0.831 0.834</cell><cell cols="2">0.684 0.684</cell></row><row><cell>Word + sig</cell><cell cols="2">0.829 0.833</cell><cell cols="2">0.700 0.695</cell></row><row><cell>Word + tanh</cell><cell cols="2">0.841 0.837</cell><cell cols="2">0.717 0.704</cell></row><row><cell>Context + lin</cell><cell cols="2">0.786 0.756</cell><cell cols="2">0.709 0.699</cell></row><row><cell>Context + sig</cell><cell cols="2">0.797 0.799</cell><cell cols="2">0.703 0.691</cell></row><row><cell cols="3">Context + tanh 0.781 0.754</cell><cell cols="2">0.702 0.698</cell></row><row><cell cols="5">Table 2: Best results obtained for claim detection (left) and</cell></row><row><cell cols="5">component detection (right) after hyperparameter search in</cell></row><row><cell cols="2">the ECHR corpus.</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="4">Claim Detection Claim -Premise</cell></row><row><cell></cell><cell>Acc</cell><cell>F1</cell><cell>Acc</cell><cell>F1</cell></row><row><cell cols="3">No attention 0.805 0.793</cell><cell cols="2">0.661 0.652</cell></row><row><cell>Word + lin</cell><cell cols="2">0.824 0.816</cell><cell cols="2">0.668 0.673</cell></row><row><cell>Word + sig</cell><cell cols="2">0.822 0.810</cell><cell cols="2">0.680 0.683</cell></row><row><cell cols="3">Word + tanh 0.821 0.814</cell><cell cols="2">0.674 0.682</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>https://github.com/UKPLab/emnlp2017-bilstm-cnn-crf</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>hudoc.echr.coe.intThe Thirty-Second International Florida Artificial Intelligence Research Society Conference </p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>The authors have received funding from the <rs type="funder">European Union</rs>'s <rs type="programName">Horizon 2020 research and innovation programme</rs> under the <rs type="grantName">Marie Skodowska-Curie</rs> grant agreement No <rs type="grantNumber">690974</rs> for the project <rs type="projectName">MIREL</rs>: <rs type="projectName">MIning</rs> and REasoning with Legal texts.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_fZJaXws">
					<idno type="grant-number">690974</idno>
					<orgName type="grant-name">Marie Skodowska-Curie</orgName>
					<orgName type="project" subtype="full">MIREL</orgName>
					<orgName type="program" subtype="full">Horizon 2020 research and innovation programme</orgName>
				</org>
				<org type="funded-project" xml:id="_NRxK6rT">
					<orgName type="project" subtype="full">MIning</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Five years of argument mining: a data-driven analysis</title>
		<author>
			<persName><forename type="first">E</forename><surname>Cabrio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Villata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IJCAI 2018</title>
		<editor>
			<persName><forename type="first">J</forename><surname>Lang</surname></persName>
		</editor>
		<meeting>IJCAI 2018</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="5427" to="5433" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Neural end-to-end learning for computational argumentation mining</title>
		<author>
			<persName><forename type="first">S</forename><surname>Eger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Daxenberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Gurevych</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th ACL</title>
		<meeting>the 55th ACL</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="11" to="22" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Argumentation mining in user-generated web discourse</title>
		<author>
			<persName><forename type="first">I</forename><surname>Habernal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Gurevych</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Linguist</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="125" to="179" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Bidirectional LSTM-CRF models for sequence tagging</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<idno>CoRR abs/1508.01991</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Argumentation mining: State of the art and emerging trends</title>
		<author>
			<persName><forename type="first">M</forename><surname>Lippi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Torroni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Internet Techn</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1" to="10" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">From argument diagrams to argumentation mining in texts: A survey</title>
		<author>
			<persName><forename type="first">A</forename><surname>Peldszus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Stede</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCINI</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="31" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Reporting Score Distributions Makes a Difference: Performance Study of LSTM-networks for Sequence Tagging</title>
		<author>
			<persName><forename type="first">N</forename><surname>Reimers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Gurevych</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="338" to="348" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Parsing argumentation structures in persuasive essays</title>
		<author>
			<persName><forename type="first">C</forename><surname>Stab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Gurevych</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Linguist</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="619" to="659" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Cross-topic argument mining from heterogeneous sources using attentionbased neural networks</title>
		<author>
			<persName><forename type="first">C</forename><surname>Stab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Gurevych</surname></persName>
		</author>
		<idno>CoRR abs/1802.05758</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Increasing Argument Annotation Reproducibility by Using Inter-annotator Agreement to Improve Guidelines</title>
		<author>
			<persName><forename type="first">M</forename><surname>Teruel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Cardellino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Cardellino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Alemany</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Villata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the LREC</title>
		<meeting>the LREC</meeting>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Inner attention based recurrent neural networks for answer selection</title>
		<author>
			<persName><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th ACL</title>
		<meeting>the 54th ACL</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1288" to="1297" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
