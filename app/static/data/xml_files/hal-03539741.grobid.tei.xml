<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">RETRIEVING SPEAKER INFORMATION FROM PERSONALIZED ACOUSTIC MODELS FOR SPEECH RECOGNITION</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Salima</forename><surname>Mdhaffar</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">LIA</orgName>
								<orgName type="institution">Avignon Université</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jean-Franc</forename><surname>¸ois Bonastre</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">LIA</orgName>
								<orgName type="institution">Avignon Université</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Marc</forename><surname>Tommasi</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">UMR 9189</orgName>
								<orgName type="institution" key="instit1">Université de Lille</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
								<orgName type="institution" key="instit3">Inria</orgName>
								<orgName type="institution" key="instit4">Centrale Lille -CRIStAL</orgName>
								<address>
									<settlement>Lille</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Natalia</forename><surname>Tomashenko</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">LIA</orgName>
								<orgName type="institution">Avignon Université</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yannick</forename><surname>Estève</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">LIA</orgName>
								<orgName type="institution">Avignon Université</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">RETRIEVING SPEAKER INFORMATION FROM PERSONALIZED ACOUSTIC MODELS FOR SPEECH RECOGNITION</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">FD81F28BD3F6E32010E0429C97AA6D7C</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-04-12T14:52+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Automatic speech recognition</term>
					<term>acoustic model</term>
					<term>personalized acoustic models</term>
					<term>collaborative learning</term>
					<term>speaker information</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The widespread of powerful personal devices capable of collecting voice of their users has opened the opportunity to build speaker adapted speech recognition system (ASR) or to participate to collaborative learning of ASR. In both cases, personalized acoustic models (AM), i.e. fine-tuned AM with specific speaker data, can be built. A question that naturally arises is whether the dissemination of personalized acoustic models can leak personal information. In this paper, we show that it is possible to retrieve the gender of the speaker, but also his identity, by just exploiting the weight matrix changes of a neural acoustic model locally adapted to this speaker. Incidentally we observe phenomena that may be useful towards explainability of deep neural networks in the context of speech processing. Gender can be identified almost surely using only the first layers and speaker verification performs well when using middle-up layers. Our experimental study on the TED-LIUM 3 dataset with HMM/TDNN models shows an accuracy of 95% for gender detection, and an Equal Error Rate of 9.07% for a speaker verification task by only exploiting the weights from personalized models that could be exchanged instead of user data.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Automatic speech recognition (ASR) is now at the heart of a large number of applications used on a daily basis by a large number of users. In order to improve the performance of their ASR solutions, it is common that companies collect and centralize data to train new acoustic models. New data regulations such as the General Data Protection Regulation in the European Union change the rules in order to protect the citizen privacy <ref type="bibr" target="#b0">[1]</ref>. In order to improve the performance of ASR models by leveraging user experience without accessing their This work was supported by the French National Research Agency under project DEEP-PRIVACY (ANR-18-CE23-0018) and by the VoicePersonae project.</p><p>data, solutions such as federated learning are increasingly being proposed. They consist on exchanging personalized models, or their gradients, instead of data <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6]</ref> to preserve the user privacy. In the framework of collaborative distributed learning, a personalized model is a model that has been locally adapted to a user <ref type="bibr" target="#b6">[7]</ref>. In a very recent work <ref type="bibr" target="#b7">[8]</ref>, we presented a such approach to personalize an hybrid HMM/TDNN acoustic model <ref type="bibr" target="#b8">[9]</ref> in a context of collaborative learning.</p><p>In this paper we investigate the information contained in personalized acoustic models. Especially, we are interested in the information related to the speaker identity or the speaker gender that is retrievable from personalized acoustic models. Previous works have studied speech intermediate representations computed within neural end-to-end models for speech recognition. They illustrated the way such end-to-end models build phonetic and graphemic representations <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11]</ref>, or showed how speaker variability and noise are gradually removed as the layer goes deeper <ref type="bibr" target="#b11">[12]</ref>. To our knowledge, there is no study on the information provided by the changes in neural weight due to an acoustic model personalization. Our assumption is that the changes applied to the weights of a neural acoustic model when this model is adapted to a speaker brings information about this speaker. Thanks to our experimental protocol we expect to evaluate the level of speaker information that can be retrieved directly from these weight changes, and also highlight in which neural layers these changes are particularly discriminant for such information.</p><p>Section 2 describes the acoustic model personalization, section 3 presents the approach proposed to retrieve gender and speaker information from the personalized acoustic models, while section 4 describes the experimental set up and section 5 the experimental results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">ACOUSTIC MODEL PERSONALIZATION</head><p>In our scenario, a global acoustic model is available, trained on an initial public dataset. This global model is distributed to many devices -each device is linked to only one speaker -on which it is possible to fine-tune a local instance of the global model by locally exploiting the user data. Fine-tuning consists in continuing the training process of the generic acoustic model on a small dataset of the target speaker, by taking care on avoiding overfitting. The output of the fine-tuning process is considered as a personalized model for the local speaker. Figure <ref type="figure" target="#fig_0">1</ref> illustrates the model personalization explored in this work. Used in the context of a collaborative distributed learning, for instance federated learning, such personalized models, or their gradients, would be exchanged in order to aggregate and improve a global model without sharing user data in an iterative way. Even if local data are not transmitted, a possible leakage of personal information appears when personalized models are exchanged. Therefore, our study explores the amount of personal information that can be inferred from the model weights of the personalized models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">SPEAKER INFORMATION RETRIEVAL</head><p>During the model personalization, the weights of the neural generic model are updated. We assume that these weight updates are dependent to some speaker characteristics. We expect that it is possible to extract such speaker information by only studying these weight changes. We focus on gender and on speaker identity. In addition, we investigate in which hidden layers these changes are particularly informative to retrieve such speaker information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Gender information</head><p>In order to retrieve the gender information from the personalized models, we propose an approach based on clustering into 2 classes. We assume the two clusters corresponds to a female/male classification. We evaluate this hypothesis by calculating the purity criterion, by using the gender labels as ground truth.</p><p>We perform as many clustering as the number of layers in the models. The inputs are the weights of the layers at the same depth. The clustering algorithm is an agglomerative clustering that merges the closest pair of clusters recursively, building a hierarchy of clusters in bottom-up fashion. The distance between layers is the Euclidean distance and the Ward linkage function is used to evaluate the distance between clusters. It is based on the minimum variance method and allows to minimize the total within cluster variance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Speaker Identification</head><p>In the second part of this study, we want to evaluate the ability to identify speakers, again by only considering the changes applied to weight matrices during the personalization of an ASR acoustic model. However, such weight matrices, and even their hidden layers, are too large to characterize the speaker. Reduction dimensionality approaches like Principal Component Analysis (PCA) are a potential solution but the large reduction factor targeted, combined to a limited number of samples -one model by speaker -could result in this case in a large loss of discriminant information. In order to solve this dimension/discrimination problem, we propose to apply a method inspired from <ref type="bibr" target="#b12">[13]</ref>, that consists in learning a speaker embedding extractor. This neural network-based extractor is trained on the weight matrices of a given hidden layer from personalized neural ASR models. Training objective is a speaker discrimination task. But we have to face two difficulties: the input matrices are very large, and the training dataset is very small.</p><p>We propose to modify the training objective by using classes of speakers as classification labels, in place of speakers. This allows us to increase the number of examples per class during the training phase, and so to reduce the risk of overfitting. The classes of speakers used for the speaker embedding extractor training are issued from hierarchical clustering of i-vector of speakers present in the training data.</p><p>In order to drastically reduce the memory footprint of the extractor and overcome this difficulty, we designed a specific structure for our extractor. Starting from a classical deep neural network (DNN) classifier, we apply a multi-stream input approach. The weight matrix is split into small blocks that are separately linked to a dedicated hidden layer. A small block of the input weight matrix is composed by all the weights related to a unit neural in the hidden layer targeted in the ASR acoustic model. For instance, if the targeted hidden layer H t of the ASR acoustic model architecture contains n units, the weight matrix used as input of our speaker embedding extractor will be split into n different blocks. Next, the outputs of the hidden layer dedicated to each block are concatenated to feed the upper hidden layer of the DNN-based extractor, composed of fully connected layers followed by the final softmax layer.</p><p>The structure of the resulting embedding extractor is illus-Fig. <ref type="figure">2</ref>. Proposed DNN structure to train speaker's embedding from neural network weights of acoustic models trated in Figure <ref type="figure">2</ref>. The embedding layer is the hidden layer just below the softmax one. The resulting DNN model is able to extract speaker embeddings from speech data, including for speakers that were not present in the training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">EXPERIMENTAL FRAMEWORK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">ASR system</head><p>Our experiments target (chain) HMM/TDNN acoustic models for speech recognition <ref type="bibr" target="#b13">[14]</ref>. The ASR system is based on the Kaldi toolkit <ref type="bibr" target="#b14">[15]</ref>. The chain-TDNN setup is based on 13 layers with 512 dimensions and is trained on cepstral mean and variance normalized 40-dimensional MFCC features. I-vectors are also incorporated as auxiliary input features. The resulting context of TDNN models is 28 left and 28 right neighbour frames. The acoustic model has about 14 million parameters. For the generic model, the initial and final scheduled learning rates are equal to 0.00025 and 0.000025 respectively. Training audio samples are randomly perturbed in speed and volume during the training process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Dataset</head><p>All experiments to train generic and fine-tune acoustic models are conducted with the TED-LIUM 3 dataset <ref type="bibr" target="#b15">[16]</ref>, a large corpus of 452 hours of TED talks pronounced by 2,295 speakers. For the study presented in this paper, an original setup has to be defined. Similarly to <ref type="bibr" target="#b7">[8]</ref>, the dataset is splited into three parts and the sets of speakers in each part are pairwise disjoints. Characteristics of the three parts are reported in Table 1. The first part is called generic and has been used to train the initial acoustic model for ASR. The two other parts, called p1 and p2, are used for model personalization and evaluation.</p><p>In both subsets, the available audio material is split in order to get two sessions of 5 minutes per speaker. Each session is used to personalize a model. Table <ref type="table" target="#tab_0">1</ref> presents the statistics of the three subsets. For p1 and p2, the table presents the exact number of speakers who have pronounced enough speech to have two sessions of five minutes (463 from 650 speakers for p1 and 581 from 765 speakers for p2, respectively).</p><p>The TED-LIUM 3 dataset is provided without information about the gender. Using the website of TED conference, the annotation of the corpus in gender was done manually for p2. <ref type="foot" target="#foot_0">1</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Personalized models</head><p>The initial generic model is trained on the generic part. Personalized models are obtained by fine-tuning the generic model on the speaker's data from p1 and p2: for each speaker, we personalize the generic model twice using separately his/her two five-minutes sessions. Thus, for most of the speakers (speakers with duration &gt; 10 minutes), two different personalized models are obtained. When fine-tuning the generic model on target speaker data, we modify only the value of learning rate (the initial and final learning rates were equal to 0.000025 and 0.000015 respectively) and all hyperparameters (i.e. learning rate and local epochs number) are assumed to be homogeneous among all workers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">RESULTS AND ANALYSIS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Gender identification</head><p>There are several methods used to evaluate clustering performance. In our study, we use the purity. Purity focuses only on maximising the total number of true positive responses per cluster. Purity values range between 0 and 1 (perfect clustering). It is defined as</p><formula xml:id="formula_0">P urity = 1 N k i=1 max j |c i ∩ t j |</formula><p>where N is the number of speakers, k is the number of clusters, c i is a cluster and t j is the classification count for cluster c i .</p><p>Figure <ref type="figure">3</ref> shows the results for the different hidden layers of neural network of ASR acoustic models from which we extract weights for data in p2. We observe that it is possible to get two gender-based clusters with a purity value of 0.96 for the layer 5. Results show that gender information can be identified for the five first layers.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Speaker verification</head><p>Speaker verification is evaluated in terms of false alarm (FA) and false reject (FR) error rates and reported using equal error rate (EER), with EER = FA = FR. Fig. <ref type="figure">4</ref>. Speaker verification performance depending on the hidden layer of the acoustic model used to extract weights.</p><p>First, a speaker embedding extractor is trained using each layer of our acoustic models as input. To train the extractors, we use 926 personalized speaker models corresponding to 463+463 unique subsets from p1. The trained extractor models are then used on p2 data to extract the speaker's embedding (knowing that there is no overlap between p1 and p2 speakers). Respectively, a second experiment is conducted with p1 as a test set and p2 as a training set for the extractor. The number of target classes (issued from a hierarchical clustering of i-vectors of speakers present in the training data) used to train our extractor is fixed to 20 and the dimension of the output vectors, the speaker embeddings, is fixed to 100. We use a speaker verification task to evaluate the ability to recognize the speakers from a given layer weights. A simple cosine distance is used to compute the verification score for a trial (enrolment,test). The data of each speaker (see Section 4.2) is divided into two sessions, denoted s1 and s2. It gives one target trial, (x s1 i , x s2 i ), per speaker x i . Nontarget trials, (x s1 i , x s2 j ), are formed by crossing the first session of a given speaker with all the second sessions of the other speakers. It gives respectively 463 / 581 target trials and 213 906 / 336 980 non-target trials for p1 and p2. Figure <ref type="figure">4</ref> shows the comparative results in terms of EER.</p><p>The best performance is obtained using layer 9 (9.07% EER for p2 and 10% EER for p1), showing clearly that speaker specific information could be extracted from the weights of a personalised ASR acoustic model. For comparison purposes, we also computed the performance when the weight vectors are used directly to compute the cosine distance, without the embedding extractor. The EER is about 48% in this case for p2 (close to the random performance). This proves the effectiveness of the proposed approach to extract a speaker embeddings from the weights of personalized acoustic model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">CONCLUSION</head><p>In this study, we showed that it is possible to retrieve the gender and the identity of a speaker from the analysis of the changes applied to the weights of her/his personalized acoustic model. Experiments conducted on the TED-LIUM 3 dataset show that the gender information is mainly brought by the updates impacting the first five layers of a HMM/TDNN acoustic models composed of 13 hidden layers, when the speaker identity is mainly embedded in the middle-up hidden layers (5 to 9). To obtain the latter result, we also proposed an original way to build a speaker embedding extractor from personalized weight matrices. We obtained a gender purity of 0.96 on the five first layers and a speaker verification EER of 9% for layer 9. These results would be particularly interesting for future works focusing on distributed learning for privacy preservation. In this direction, we propose in a parallel study dedicated to attack approaches against federated learning for speech recognition, to use external speech data in order to analyze the behavior of personalized models on such data, see <ref type="bibr" target="#b16">[17]</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Model personalization: (a) A generic model is downloaded to each user device. (b) The generic model is locally fine-tuned on the user data stored on the device.</figDesc><graphic coords="3,54.43,180.43,248.50,181.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig.</head><label></label><figDesc>Fig. Clustering purity of hidden layer weights of the acoustic model using women and men labels as a reference.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="4,149.73,72.00,313.95,179.55" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1</head><label>1</label><figDesc>presents also statistics about the gender.</figDesc><table><row><cell>generic p1</cell><cell>p2</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>TED-LIUM 3 dataset</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>For the reproductibility of experimental results by research community, we will make available this annotation.</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">The GDPR &amp; Speech Data: Reflections of legal and technology communities, first steps towards a common understanding</title>
		<author>
			<persName><surname>Nautsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Els</forename><surname>Jasserand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kindt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Todisco</surname></persName>
		</author>
		<author>
			<persName><surname>Trancoso</surname></persName>
		</author>
		<author>
			<persName><surname>Evans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Interspeech. ISCA</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Federated learning for keyword spotting</title>
		<author>
			<persName><forename type="first">David</forename><surname>Leroy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alice</forename><surname>Coucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thibaut</forename><surname>Lavril</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thibault</forename><surname>Gisselbrecht</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><surname>Dureau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6341" to="6345" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Training keyword spotting models on non-iid data with federated learning</title>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Hard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kurt</forename><surname>Partridge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cameron</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niranjan</forename><surname>Subrahmanya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aishanee</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pai</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ignacio</forename><forename type="middle">Lopez</forename><surname>Moreno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rajiv</forename><surname>Mathews</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Interspeech 2020</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Training speech recognition models with federated learning: A quality/cost framework</title>
		<author>
			<persName><forename type="first">Dhruv</forename><surname>Guliani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Giovanni</forename><surname>Franc ¸oise Beaufays</surname></persName>
		</author>
		<author>
			<persName><surname>Motta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="3080" to="3084" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Federated learning in ASR: Not as easy as you think</title>
		<author>
			<persName><forename type="first">Wentao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Freiwald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sören</forename><surname>Tewes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabien</forename><surname>Huennemeyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dorothea</forename><surname>Kolossa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ITG Conference on Speech Communication</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Federated acoustic modeling for automatic speech recognition</title>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Songtao</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Kingsbury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="6748" to="6752" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Three approaches for personalization with applications to federated learning</title>
		<author>
			<persName><forename type="first">Yishay</forename><surname>Mansour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mehryar</forename><surname>Mohri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jae</forename><surname>Ro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ananda</forename><surname>Theertha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suresh</forename></persName>
		</author>
		<idno>abs/2002.10619</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Study on acoustic model personalization in a context of collaborative learning constrained by privacy preservation</title>
		<author>
			<persName><forename type="first">Salima</forename><surname>Mdhaffar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Tommasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yannick</forename><surname>Estève</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SPECOM</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A time delay neural network architecture for efficient modeling of long temporal contexts</title>
		<author>
			<persName><forename type="first">Vijayaditya</forename><surname>Peddinti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjeev</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">terspeech 2015, 16th Annual Conference of the International Speech Communication Association</title>
		<meeting><address><addrLine>Dresden, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>ISCA</publisher>
			<date type="published" when="2015">September 6-10, 2015. 2015</date>
			<biblScope unit="page" from="3214" to="3218" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Analyzing hidden representations in end-to-end automatic speech recognition systems</title>
		<author>
			<persName><forename type="first">Yonatan</forename><surname>Belinkov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Glass</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on Neural Information Processing Systems</title>
		<meeting>the 31st International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2438" to="2448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Analyzing Phonetic and Graphemic Representations in Endto-End Automatic Speech Recognition</title>
		<author>
			<persName><forename type="first">Yonatan</forename><surname>Belinkov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ahmed</forename><surname>Ali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Glass</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech 2019</title>
		<meeting>Interspeech 2019</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="81" to="85" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">What does a network layer hear? analyzing hidden representations of end-to-end asr through speech synthesis</title>
		<author>
			<persName><forename type="first">Chung-Yi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pei-Chieh</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hung-Yi</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="6434" to="6438" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">X-vectors: Robust dnn embeddings for speaker recognition</title>
		<author>
			<persName><forename type="first">David</forename><surname>Snyder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Garcia-Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><surname>Sell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjeev</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="5329" to="5333" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Purely sequencetrained neural networks for asr based on lattice-free mmi</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vijayaditya</forename><surname>Peddinti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Galvez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pegah</forename><surname>Ghahremani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vimal</forename><surname>Manohar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xingyu</forename><surname>Na</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjeev</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Interspeech</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2751" to="2755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">The Kaldi speech recognition toolkit</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arnab</forename><surname>Ghoshal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gilles</forename><surname>Boulianne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukas</forename><surname>Burget</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ondrej</forename><surname>Glembek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nagendra</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mirko</forename><surname>Hannemann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Petr</forename><surname>Motlicek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanmin</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Petr</forename><surname>Schwarz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 2011 workshop on automatic speech recognition and understanding</title>
		<imprint>
			<publisher>IEEE Signal Processing Society</publisher>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">TED-LIUM 3: twice as much data and corpus repartition for experiments on speaker adaptation</title>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Franc ¸ois Hernandez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sahar</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Natalia</forename><surname>Ghannay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yannick</forename><surname>Tomashenko</surname></persName>
		</author>
		<author>
			<persName><surname>Estève</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Speech and Computer</title>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="198" to="208" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Privacy attacks for automatic speech recognition acoustic models in a federated learning framework</title>
		<author>
			<persName><forename type="first">Natalia</forename><surname>Tomashenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Salima</forename><surname>Mdhaffar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Tommasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yannick</forename><surname>Estève</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean-Franc ¸ois</forename><surname>Bonastre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Submitted to ICASSP 2022</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
