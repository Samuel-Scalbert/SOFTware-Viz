<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Differential Privacy has Bounded Impact on Fairness in Classification</title>
				<funder ref="#_8v8w8bU">
					<orgName type="full">Région Hauts de France</orgName>
				</funder>
				<funder ref="#_musK7Je #_CkT3KTj">
					<orgName type="full">Agence Nationale de la Recherche</orgName>
					<orgName type="abbreviated">ANR</orgName>
				</funder>
				<funder ref="#_PAK5vWd #_CzzR4UM">
					<orgName type="full">unknown</orgName>
				</funder>
				<funder>
					<orgName type="full">Inria Exploratory Action FLAMED</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Paul</forename><surname>Mangold</surname></persName>
							<email>&lt;paul.mangold@inria.fr&gt;</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Univ. Lille</orgName>
								<orgName type="institution" key="instit2">Inria</orgName>
								<orgName type="institution" key="instit3">CNRS</orgName>
								<orgName type="institution" key="instit4">Centrale Lille</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">UMR</orgName>
								<address>
									<addrLine>9189 -CRIStAL</addrLine>
									<postCode>F-59000</postCode>
									<settlement>Lille</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Michaël</forename><surname>Perrot</surname></persName>
							<email>&lt;michael.perrot@inria.fr&gt;.</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Univ. Lille</orgName>
								<orgName type="institution" key="instit2">Inria</orgName>
								<orgName type="institution" key="instit3">CNRS</orgName>
								<orgName type="institution" key="instit4">Centrale Lille</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">UMR</orgName>
								<address>
									<addrLine>9189 -CRIStAL</addrLine>
									<postCode>F-59000</postCode>
									<settlement>Lille</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Aurélien</forename><surname>Bellet</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Univ. Lille</orgName>
								<orgName type="institution" key="instit2">Inria</orgName>
								<orgName type="institution" key="instit3">CNRS</orgName>
								<orgName type="institution" key="instit4">Centrale Lille</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">UMR</orgName>
								<address>
									<addrLine>9189 -CRIStAL</addrLine>
									<postCode>F-59000</postCode>
									<settlement>Lille</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Marc</forename><surname>Tommasi</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Univ. Lille</orgName>
								<orgName type="institution" key="instit2">Inria</orgName>
								<orgName type="institution" key="instit3">CNRS</orgName>
								<orgName type="institution" key="instit4">Centrale Lille</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">UMR</orgName>
								<address>
									<addrLine>9189 -CRIStAL</addrLine>
									<postCode>F-59000</postCode>
									<settlement>Lille</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Differential Privacy has Bounded Impact on Fairness in Classification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">3740DADF4FAC5C71C98D333F7B55F624</idno>
					<note type="submission">Submitted on 18 Sep 2023</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-04-12T14:50+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract xml:lang="fr">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>des établissements d'enseignement et de recherche français ou étrangers, des laboratoires publics ou privés.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The performance of machine learning models have mainly been evaluated in terms of utility, that is their ability to solve specific tasks. However, they can be used in sensitive contexts, and impact people's lives. It is thus crucial that users can trust these models. While trustworthiness encompasses multiple concepts, fairness and privacy have attracted a lot of interest in the past few years. Fairness requires models not to unjustly discriminate against specific individuals or subgroups of the population, and privacy preserves individual-level information about the training data from being inferred from the model. These two notions have been extensively studied in isolation: there exists numerous approaches to learn fair models <ref type="bibr" target="#b8">(Caton &amp; Haas, 2020;</ref><ref type="bibr" target="#b33">Mehrabi et al., 2021)</ref>, or to preserve privacy <ref type="bibr" target="#b16">(Dwork et al., 2014;</ref><ref type="bibr" target="#b28">Liu et al., 2021)</ref>. However, only few works studied the interplay between privacy and fairness. In this paper, we take a step forward in this direction, proposing a new theoretical bound on the relative impact of privacy on fairness in classification.</p><p>Fairness takes various forms (depending on the task and context), and several definitions exist. On the one hand, the goal may be to ensure that similar individuals are treated similarly. This is captured by individual fairness <ref type="bibr" target="#b15">(Dwork et al., 2012)</ref> and counterfactual fairness <ref type="bibr" target="#b27">(Kusner et al., 2017)</ref>. On the other hand, group fairness requires that decisions made by machine learning models do not unjustly discriminate against subgroups of the population. In this paper, we focus on group fairness and consider four popular definitions, namely Equalized Odds <ref type="bibr" target="#b20">(Hardt et al., 2016)</ref>, Equality of Opportunity <ref type="bibr" target="#b20">(Hardt et al., 2016)</ref>, Accuracy Parity <ref type="bibr" target="#b47">(Zafar et al., 2017)</ref>, and Demographic Parity <ref type="bibr" target="#b7">(Calders et al., 2009)</ref>. Differential privacy <ref type="bibr" target="#b14">(Dwork, 2006)</ref> has been widely adopted for controlling how much information the output of an algorithm may leak about its input data. It allows publishing machine learning models while preventing an adversary from guessing too confidently the presence (or absence) of an individual in the training data. To enforce differential privacy, one typically releases a noisy estimate of the true model <ref type="bibr" target="#b14">(Dwork, 2006)</ref>, so as to conceal any sensitive information contained in individual data points. This induces a trade-off between the strength of the protection and the utility of the learned model. While this trade-off has been extensively studied <ref type="bibr" target="#b10">(Chaudhuri et al., 2011;</ref><ref type="bibr" target="#b3">Bassily et al., 2014;</ref><ref type="bibr" target="#b28">Liu et al., 2021)</ref>, its implications for fairness are not yet well understood.</p><p>Contributions. In this work, we quantify the difference in fairness levels between private and non-private models in multi-class classification. We derive high probability bounds showing that this difference shrinks at a rate of O(</p><formula xml:id="formula_0">√ p /n),</formula><p>where n is the number of data records, and p the dimension of the model. To obtain this result, we first prove that the accuracy of a model conditioned on an arbitrary event (such as membership to a sensitive group), is pointwise Lipschitz continuous with respect to the model parameters. This property is inherited by many popular group fairness notions, such as Equalized Odds, Equality of Opportunity, Accuracy Parity and Demographic Parity. Consequently, two sufficiently close models will have similar fairness levels. We then upper-bound the distance between the optimal nonprivate model and the private models obtained with privacy preserving mechanisms like output perturbation <ref type="bibr" target="#b10">(Chaudhuri et al., 2011;</ref><ref type="bibr" target="#b31">Lowy &amp; Razaviyayn, 2021)</ref> or DP-SGD <ref type="bibr" target="#b40">(Song et al., 2013;</ref><ref type="bibr" target="#b3">Bassily et al., 2014)</ref>. These bounds hold for strongly convex empirical risk minimization formulations, potentially allowing explicit fairness-promoting convex regularization terms <ref type="bibr" target="#b5">(Bechavod &amp; Ligett, 2017;</ref><ref type="bibr" target="#b22">Huang &amp; Vishnoi, 2019;</ref><ref type="bibr" target="#b30">Lohaus et al., 2020;</ref><ref type="bibr" target="#b42">Tran et al., 2021)</ref>. Combining these two results, we derive high probability bounds on the fairness loss due to privacy. They show that, with enough training examples, (i) given an optimal non-private model, enforcing privacy will not harm fairness too much, and (ii) given a private model, the corresponding (unknown) nonprivate optimal model cannot be vastly fairer. Our results also highlight the role of the confidence margin of models in the disparate impact of differential privacy: notably, if the non-private model has high per-group confidence, then our bound on the loss in fairness due to privacy will be smaller.</p><p>Our contributions can be summarized as follows:</p><p>• We prove that group fairness is pointwise Lipschitz, with a smaller constant for models with large margins.</p><p>• We bound the distance between private and optimal models, and show that the difference in their fairness levels decreases in O( √ p /n).</p><p>• We show that this bound can be computed even when the optimal model is unknown, and numerically demonstrate that we obtain non-trivial guarantees.</p><p>Related work. The joint study of fairness and privacy in machine learning only goes back a few years, and has been the focus of a recent survey <ref type="bibr" target="#b18">(Fioretto et al., 2022)</ref>. One may identify three main research directions. First, it has been empirically observed that privacy can exacerbate unfairness <ref type="bibr" target="#b2">(Bagdasaryan et al., 2019;</ref><ref type="bibr" target="#b37">Pujol et al., 2020;</ref><ref type="bibr" target="#b17">Farrand et al., 2020;</ref><ref type="bibr" target="#b43">Uniyal et al., 2021;</ref><ref type="bibr" target="#b19">Ganev et al., 2022)</ref> and, conversely, that enforcing fairness can lead to more privacy leakage for the unprivileged group <ref type="bibr" target="#b9">(Chang &amp; Shokri, 2020)</ref>. These empirical results suggest that some properties of the dataset (such as group sizes and groupwise input norms) and the choice of the private training method may affect the extent of these disparate impacts. Unfortunately, these observations are not supported by theoretical results, and it is not clear why and when disparate impact occurs. Second, a few approaches have been proposed to learn models that are both fair and privacy preserving. However, these works either have limited theoretical guarantees on their performance <ref type="bibr" target="#b26">(Kilbertus et al., 2018;</ref><ref type="bibr" target="#b45">Xu et al., 2019;</ref><ref type="bibr" target="#b1">2020;</ref><ref type="bibr" target="#b41">Tran et al., 2020)</ref>, or learn stochastic models which might not be usable in contexts where deterministic decisions are expected <ref type="bibr" target="#b23">(Jagielski et al., 2019;</ref><ref type="bibr" target="#b35">Mozannar et al., 2020)</ref>. Finally, a few works have shown that fairness and privacy are incompatible in some settings, in the sense that there exists data distributions where enforcing one prevents the other from being satisfied <ref type="bibr" target="#b38">(Sanyal et al., 2022)</ref>, or where enforcing both implies trivial utility <ref type="bibr" target="#b12">(Cummings et al., 2019;</ref><ref type="bibr" target="#b1">Agarwal, 2020)</ref>. While appealing at first glance, these results usually consider unrealistic cases that are hardly encountered in practice. In this paper, we also study fairness and privacy jointly but rather than studying whether they may be achieved simultaneously, we investigate the relative difference in fairness level between private and non-private models.</p><p>To the best of our knowledge, the work closest to ours is the one of <ref type="bibr" target="#b42">Tran et al. (2021)</ref>. They analyze the impact of privacy on fairness in Empirical Risk Minimization, where their notion of fairness is defined as the difference between the excess risk computed on the overall population and the excess risk computed on a subgroup of the population. They study the expected behavior over the possible private models while our results are model-specific. In line with our work, their results suggest that the distance to the decision boundary plays a key role in the disparate impact of differential privacy. However, the quantity appearing in their result is based on a second-order Taylor approximations which is loose for popular classification loss functions. In contrast, the quantity appearing in our bounds is precisely the confidence margin considered in prior work on multi-class margin-based classification <ref type="bibr" target="#b11">(Cortes et al., 2013)</ref>. Finally and most importantly, loss-based fairness does not necessarily imply that the actual decisions taken by the model are fair with respect to standard group-fairness notions <ref type="bibr" target="#b30">(Lohaus et al., 2020)</ref>. In contrast, our work provides guarantees in terms of these widely-accepted group fairness definitions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Preliminaries</head><p>In this section, we present the fairness and privacy notions that will be used throughout the paper. We consider a multiclass classification setting with a feature space X , a finite set of labels Y, and a finite set S of values for the sensitive attribute. Let D be a distribution over X × S × Y, and</p><formula xml:id="formula_1">D = {(x 1 , s 1 , y 1 ), . . . , (x n , s n , y n )} be a training set of n examples drawn i.i.d. from D.</formula><p>Let H be a space of realvalued functions h : X × Y → R equipped with a norm • H . For an example x ∈ X , the predicted label is the one with the highest value, that is H(x) = arg max y∈Y h(x, y). In case of a tie, a random label among the most likely ones is predicted.</p><p>The confidence margin <ref type="bibr" target="#b11">(Cortes et al., 2013)</ref> of a model h for an example-label pair (x, y) is defined as</p><formula xml:id="formula_2">ρ(h, x, y) = h(x, y) -max y =y h(x, y ) .</formula><p>This confidence margin is positive when the example x is classified as y by h and negative otherwise. In this paper, we make the assumption that the margin is Lipschitz-continuous in the model h.</p><p>Assumption 2.1 (Lipschitzness of the margin). We assume that ρ is Lipschitz-continuous in its first argument, that is for all h, h ∈ H and (x,</p><formula xml:id="formula_3">y) ∈ X × Y, |ρ(h, x, y) -ρ(h , x, y)| ≤ L x,y h -h H ,</formula><p>where L x,y &lt; +∞ may depend on the example (x, y).</p><p>Note that this assumption is not very restrictive. Typically, it holds for any class of differentiable models with either (i) bounded gradients, or (ii) continuous gradients on a compact parameter space (e.g., generalized linear models or smooth deep neural networks <ref type="bibr" target="#b21">(Hastie et al., 2009)</ref>). We stress that L x,y does not need to hold uniformly on the data (although it must hold uniformly on H). Indeed, we will see in our results (Section 3.2) that a large L x,y can be compensated for by the small probability of x, y in the data distribution.</p><p>To illustrate Assumption 2.1, consider linear models of the form h(x, y) = W T y x where W is a real-valued matrix where each line is a vector W y of label-specific parameters. Define</p><formula xml:id="formula_4">h -h H = W -W 2 . Then, remark that |ρ(h, x, y) -ρ(h , x, y)| ≤ |h(x, y) -h (x, y)| + max y =y |h(x, y ) -h (x, y )| ≤ 2 x 2 h -h H . We thus have L x,y = 2 x 2 .</formula><p>The goal of a learning algorithm A : (X × S × Y) n → H is to find the best possible model to solve the task. In this work, the quality of a model h is evaluated through its accuracy Acc(h) = P (H(X) = Y ) but also its fairness level (as defined in Section 2.1). Furthermore, given a non-private algorithm A, our goal will be to compare the quality of its output to that of a private version A priv of A that guarantees differential privacy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Fairness</head><p>In this paper, we focus on group fairness. These definitions are based on the idea that a group of individuals should not be discriminated against, compared to the overall population. Usually, these groups are defined by the sensitive attribute from S. However, in some cases, it is necessary to consider more fine grained partitions. This is for example the case in Equalized Odds <ref type="bibr" target="#b20">(Hardt et al., 2016)</ref>, where a model is fair if its performance is the same on the overall population and on subgroups of individuals that share the same sensitive group and the same label. Thus, for the sake of generality, we assume that the data can be partitioned into K disjoint groups denoted by D 1 , . . . , D k , . . . , D K . As in Maheshwari &amp; Perrot (2022), we consider fairness definitions that, for each group k, can be written as:</p><formula xml:id="formula_5">F k (h, D) = C 0 k + K k =1 C k k P (H(X) = Y | D k ) ,<label>(1)</label></formula><p>where the C k k 's are group specific values independent of h, that typically depend on the size of the groups. In Ap-pendix A, we show that usual group fairness notions such as Demographic Parity (with binary labels) <ref type="bibr" target="#b7">(Calders et al., 2009)</ref>, Equality of Opportunity <ref type="bibr" target="#b20">(Hardt et al., 2016)</ref>, Equalized Odds <ref type="bibr" target="#b20">(Hardt et al., 2016)</ref>, and Accuracy Parity <ref type="bibr" target="#b47">(Zafar et al., 2017)</ref> can all be expressed in the form of (1). By convention, we consider that F k (h, D) &gt; 0 when the group k is advantaged by h compared to the overall population, F k (h, D) &lt; 0 when the group is disadvantaged and F k (h, D) = 0 when h is fair for group k.</p><p>In some cases, rather than measuring fairness for each group k independently, it is interesting to summarize the information with an aggregate value. For example, we will use the mean of the absolute fairness level of each group:</p><formula xml:id="formula_6">Fair(h, D) = 1 K K k=1 |F k (h, D)| ,<label>(2)</label></formula><p>which is 0 when h is fair and positive when it is unfair.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Differential Privacy</head><p>We measure the privacy of machine learning models with differential privacy (see Definition 2.2 below). Differential privacy (DP) guarantees that the outcomes of a randomized algorithm are similar when run on datasets that differ in at most one data point. It effectively preserves privacy by preventing an adversary observing the trained model from inferring the presence of an individual in the training set. A key property of differential privacy is that it still holds after post-processing of the algorithm's outcome <ref type="bibr" target="#b14">(Dwork, 2006)</ref>, as long as this post-processing is independent of the data. Let D, D ∈ (X × S × Y) n be two datasets of n elements. We say that they are neighboring (denoted by D ≈ D ) if they differ in at most one element.</p><p>Definition 2.2 (Differential Privacy - <ref type="bibr" target="#b14">Dwork (2006)</ref>). Let A priv : (X × S × Y) n → H be a randomized algorithm. We say that A priv is ( , δ)-differentially private if, for all neighboring datasets D, D ∈ (X ×S ×Y) n and all subsets of hypotheses H ⊆ H,</p><formula xml:id="formula_7">P(A priv (D) ∈ H ) ≤ exp( ) P(A priv (D ) ∈ H ) + δ .</formula><p>To design differentially private algorithms to estimate a function A : (X × S × Y) n → R p , we need to quantify how much changing one point in a dataset can impact the output of A. This is typically measured by (an upper bound on) the 2 -sensitivity of A, defined as</p><formula xml:id="formula_8">∆(A) = sup D≈D A(D) -A(D ) 2 .</formula><p>The value of A on a dataset D ∈ (X × S × Y) n can then be released privately using the Gaussian mechanism <ref type="bibr" target="#b16">(Dwork et al., 2014)</ref>. Formally, to guarantee ( , δ)-differential privacy, we add Gaussian noise to A(D), calibrated to its sensitivity and the desired level of privacy:</p><formula xml:id="formula_9">A priv (D) = A(D) + N 0, 2∆(A) 2 log(1.25/δ) 2 I p ,</formula><p>where N (0, σ 2 I p ) is a sample from the normal distribution with mean zero and variance σ 2 I p . In many cases (e.g., when the dataset D is large), A priv is computed on a random subsample of D. Assuming A priv is ( , δ)differentially private, applying A priv to a randomly selected fraction q of D satisfies (O(q ), qδ)-differential privacy, thereby amplifying privacy guarantees <ref type="bibr" target="#b24">(Kasiviswanathan et al., 2011;</ref><ref type="bibr" target="#b6">Beimel et al., 2014)</ref>. This privacy amplification by subsampling phenomenon, together with the Gaussian mechanism, serve as building blocks in more complex algorithms. In particular, they can be composed <ref type="bibr" target="#b16">(Dwork et al., 2014)</ref>, allowing the design of iterative private algorithms such as DP-SGD <ref type="bibr" target="#b3">(Bassily et al., 2014;</ref><ref type="bibr" target="#b0">Abadi et al., 2016)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Pointwise Lipschitzness and Group Fairness</head><p>Here, we show that several group fairness notions are pointwise Lipschitz with respect to the model. To this end, we first prove a more general result on the pointwise Lipschitzness of accuracy conditionally on an arbitrary event.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Pointwise Lipschitzness of Conditional Accuracy</head><p>We first relate the difference of conditional accuracy of two models to the distance that separates them. This is summarized in the next theorem.</p><p>Theorem 3.1 (Pointwise Lipschitzness of Conditional Accuracy). Let H be a set of real-valued functions with L X,Y the Lipschitz constants defined in Assumption 2.1. Let h, h ∈ H be two models, (X, Y, S) be a triple of random variables with distribution D, and E be an arbitrary event.</p><p>Assume that</p><formula xml:id="formula_10">E ( L X,Y / ρ(h , X, Y ) | E) &lt; +∞, then |P(H(X) = Y | E) -P(H (X) = Y | E)| ≤ E L X,Y |ρ(h, X, Y )| E h -h H . (Lip)</formula><p>Proof. (Sketch) The proof of this theorem is in two steps. First, we use the Lipschitzness of the margin (Assumption 2.1), the triangle inequality, and the union bound to</p><formula xml:id="formula_11">show that |P (H(X) = Y | E) -P (H (X) = Y | E)| ≤ P ( L X,Y /|ρ(h, X, Y )| ≥ 1 / h -h H | E).</formula><p>Then, applying Markov's inequality gives the desired result. The complete proof can be found in Appendix B.</p><p>Theorem 3.1 shows the pointwise lipschitzness of the function h → P (H(X) = Y | E) and underlines the importance of the confidence margin ρ(h, x, y) of the model h. Note that Lx,y /|ρ(h, x, y)| is small when the model h is confident (relatively to L x,y ) in its prediction for the true label y. This implies that, when the probability (given E) that a point has a small margin (relatively to L x,y ) is small,</p><formula xml:id="formula_12">E L X,Y |ρ(h,X,Y</formula><p>)| E is also small. This is notably the case for large margin classifiers.</p><p>Additionally, we note that data records that are unlikely do not affect the result too much. Indeed, even if Lx,y /|ρ(h, x, y)| is large, this can be compensated for by the small probability of observing x, y so that the value of</p><formula xml:id="formula_13">E L X,Y |ρ(h,X,Y )| E is not significantly affected.</formula><p>It is worth noting that the bound presented in Theorem 3.1 can be tightened (at the expense of readability) without affecting the quantities that need to be controlled, that is the margin |ρ(h, x, y)| and the distance hh H . Hence, note that given</p><formula xml:id="formula_14">(x, y) ∈ X × Y, if |ρ(h, x, y)| ≥ L x,y h -h H ,</formula><p>then it means that h's margin is large enough to ensure that h and h have the same prediction on x. The corresponding term in the expectation may then be accounted for as zero, improving the upper bound (Remark B.2). Interestingly, if all the examples are classified with such a large margin, our bound becomes 0, further hinting toward the importance of large margin classifiers. This result may be further tightened by using a Chernoff bound instead of Markov's inequality</p><formula xml:id="formula_15">(Remark B.1), yield- ing |P(H(X) = Y |E)-P(H (X) = Y |E)| ≤ β X,Y (h), with β X,Y (h) = inf t≥0 e t h-h H E e - t|ρ(h,X,Y )| L X,Y E .</formula><p>In the subsequent theoretical developments, we use the bound derived in Theorem 3.1 for the sake of readability. In the numerical experiments (Section 5), we use the version of the bound that yields the tightest results by combining both of the aforementioned techniques.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Pointwise Lipschitzness of Group Fairness Notions</head><p>We now use Theorem 3.1's general result to relate the fairness levels of two classifiers, based on their distance. In Theorem 3.2, we show that fairness notions in the form of (1) are pointwise Lipschitz. Theorem 3.2 (Pointwise Lipschitzness of Fairness). Let h, h ∈ H, and L X,Y defined as in Assumption 2.1. For any fairness notion of the form of (1), we have, for all k ∈ [K],</p><formula xml:id="formula_16">|F k (h, D) -F k (h , D)| ≤ χ k (h, D) h -h H . with χ k (h, D) = K k =1 C k k E L X,Y |ρ(h,X,Y )| D k . Simi- larly, for the aggregate measure of fairness defined in (2), |Fair(h, D)-Fair(h , D)| ≤ 1 K K k=1 χ k (h, D) h -h H .</formula><p>Proof. (Sketch) To prove the first claim, we use the triangle inequality to show that, for each group, the absolute differ-ence in fairness is bounded by a combination of absolute differences between conditional probabilities. We can then apply Theorem 3.1. The second claim follows by applying the first one to each group independently. The complete proof is provided in Appendix C.</p><p>Theorem 3.2 implies that classifiers that are sufficiently close have similar fairness levels. This has two major consequences when studying a given model. On the one hand, we have an upper bound on the harm that can be done to fairness: small variations of the model cannot make it much more unfair. On the other hand, we have a lower bound on the distance needed to make a model fair: making the model significantly more fair requires to substantially alter it. In the next corollary, we instantiate Theorem 3.2 for various popular group fairness notions, and for accuracy.</p><p>Corollary 3.3. Let h, h ∈ H, and L X,Y defined as in Assumption 2.1. The difference in fairness or accuracy between h and h can be bounded as follows.</p><p>Equalized Odds <ref type="bibr" target="#b20">(Hardt et al., 2016)</ref>: the data is divided into K = |Y × S| groups such that for all (y, r) ∈ Y × S,</p><formula xml:id="formula_17">χ (y,r) (h, D) = E L X,Y |ρ(h,X,Y )| Y = y + E L X,Y |ρ(h,X,Y )| Y = y, S = r .</formula><p>Equality of Opportunity <ref type="bibr" target="#b20">(Hardt et al., 2016)</ref>: we let Y ⊆ Y the set of desirable outcomes. The data is divided into K = |Y × S| such that for all (y, r) ∈ Y × S,</p><formula xml:id="formula_18">χ (y,r) (h, D) = E L X,Y |ρ(h,X,Y )| Y = y, S = r + E L X,Y |ρ(h,X,Y )| Y = y ,</formula><p>if y is a desired outcome, and χ (y,r) (h, D) = 0 otherwise.</p><p>Accuracy Parity <ref type="bibr" target="#b47">(Zafar et al., 2017)</ref>: the data is divided into K = |S| groups such that for all r ∈ S,</p><formula xml:id="formula_19">χ (r) (h, D) = E L X,Y |ρ(h,X,Y )| + E L X,Y |ρ(h,X,Y )| S = r .</formula><p>Demographic Parity (Binary Labels) <ref type="bibr" target="#b7">(Calders et al., 2009)</ref>:</p><formula xml:id="formula_20">the data is divided into K = |Y × S| groups such that for all (y, r) ∈ Y × S, χ (y,r) (h, D) = E L X,Y |ρ(h,X,Y )| +E L X,Y |ρ(h,X,Y )| S = r .</formula><p>Accuracy: the data is in a single group, such that</p><formula xml:id="formula_21">χ(h, D) = E L X,Y |ρ(h,X,Y )| .</formula><p>Proof. This corollary follows from Theorem 3.2 by replacing the C k k 's by their appropriate values (depending on the considered notion). See Appendix A for more details.</p><p>Corollary 3.3 shows that our results are applicable to several group fairness notions, but also to accuracy. Importantly, the pointwise Lipschitz constant χ k (h, D) depends both on the group k ∈ [K], and on the considered fairness notion. This sheds light on the fact that comparing the fairness levels of two models requires special attention, as there may be important disparities depending on the considered sensitive group or fairness notion. We will use this result in Section 4, where we bound the disparate impact of privacy by bounding the difference in fairness levels between private and non-private models.</p><p>Finite sample analysis. In practice, it is often assumed that one does not have access to the true distribution D but rather to a finite sample D = {(x 1 , s 1 , y 1 ), . . . , (x n , s n , y n )} of size n. An empirical estimate of the expectation from a finite sample is then defined as</p><formula xml:id="formula_22">E (f (X)) = 1 n n i=1 f (x i ).</formula><p>The results presented in Theorem 3.1, Theorem 3.2, and Corollary 3.3 also hold in this finite sample setting. For instance, denoting by χ k the empirical version of χ k , we have that ∀k ∈ [K],</p><formula xml:id="formula_23">F k (h, D) -F k (h , D) ≤ χ k (h, D) h -h H .</formula><p>One may then wonder whether it is possible to connect the true fairness of a model h to the empirical fairness of a second model h , that is bound F k (h, D) -F k (h , D) . In the next lemma, we show that such bound can indeed be obtained when h and h were learned on D. </p><formula xml:id="formula_24">K k =0 C k k -C k k &gt; α C ≤ B 3 exp -B 4 α 2 C n .</formula><p>Let H be an hypothesis space and d H be the Natarajan dimension of H. With probability at least 1δ over the choice of D, ∀h, h ∈ H</p><formula xml:id="formula_25">F k (h, D) -F k (h , D) ≤ χ k (h, D) h -h H + O K k =1 C k k d H + log ( K /δ) np k .</formula><p>Proof. (Sketch) This lemma follows from bounding the two terms in the following inequality:</p><formula xml:id="formula_26">F k (h , D) -F k (h, D) ≤ F k (h , D) -F k (h, D) + F k (h , D) -F k (h , D) .</formula><p>The first term can be bounded using the empirical counterpart of Theorem 3.2. The second term is then bounded with high probability using standard uniform convergence bounds <ref type="bibr" target="#b39">(Shalev-Shwartz &amp; Ben-David, 2014)</ref>. The complete proof can be found in Appendix D where the result is also extended to the simpler case where h and h are fixed rather than learned on D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Bounding the Relative Fairness of Private Models</head><p>In this section, we quantify the difference of fairness between a private model and its non-private counterpart. Let : H × X × S × Y → R be a loss function. Assume is Λ-Lipschitz, and µ-strongly-convex with respect to its first variable. Assume the norm • H is Euclidean, and that H is convex. We define the optimal model h * ∈ H as</p><formula xml:id="formula_27">h * = arg min h∈H f (h) = 1 n n i=1 (h; x i , s i , y i ) .<label>(3)</label></formula><p>Note that, since f is strongly-convex, h * is unique, and is thus a natural choice for the non-private reference model. This strong convexity assumption could be relaxed: this would require to define another reference model (e.g., the output of non-private SGD) and to bound the distance between this reference model and the private model.</p><p>Two mechanisms are commonly used to find a differentially private approximation h priv of h * : output perturbation <ref type="bibr" target="#b10">(Chaudhuri et al., 2011;</ref><ref type="bibr" target="#b31">Lowy &amp; Razaviyayn, 2021)</ref>, and DP-SGD <ref type="bibr" target="#b3">(Bassily et al., 2014;</ref><ref type="bibr" target="#b0">Abadi et al., 2016)</ref>. For both mechanisms, the distance h privh * H can be upper bounded with high probability. In this section, we recall these two mechanisms and the corresponding high probability upper bounds. We then plug these bounds in Theorem 3.2 to bound the fairness level of the private solution h priv relatively to the one of the true solution h * .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Bounding the Distance between Private and Optimal Classifiers</head><p>Output perturbation. Output perturbation computes the non-private solution h * of (3), and releases a private estimate by the Gaussian mechanism:</p><formula xml:id="formula_28">h priv = π H (h * + N (σ 2 I p )) ,</formula><p>where π H is the projection on H. Let ∆ be the sensitivity of the function D → arg min w∈H f (w; D). In our setting, we have ∆ = 2Λ /µn. Then, given 0 &lt; , δ &lt; 1, h priv is ( , δ)-differentially private as long as σ 2 ≥ 2∆ 2 log(1.25/δ) / 2 . We bound the distance between h priv and h * with high probability in Lemma 4.1. Lemma 4.1. Let h priv be the vector released by output perturbation with noise σ 2 = 8Λ 2 log(1.25/δ) /µ 2 n 2 2 , and</p><formula xml:id="formula_29">0 &lt; ζ &lt; 1, then with probability at least 1 -ζ, h priv -h * 2 2 ≤ 32pΛ 2 log(1.25/δ) log(2/ζ) µ 2 n 2 2 .</formula><p>DP-SGD. DP-SGD starts from some h 0 ∈ H and updates it using stochastic gradients. That is, with γ &gt; 0, i ∼ U([n]), and η t ∼ N (0, σ 2 I p ), we iteratively update</p><formula xml:id="formula_30">h t+1 = π H (h t -γ(∇ (h t ; x i , y i ) + η t )) .</formula><p>After T &gt; 0 iterations, we release h priv = h T . Given 0 &lt; , δ &lt; 1, h priv is ( , δ)-differentially private when σ 2 ≥ 64Λ 2 T 2 log(3T /δ) log(2/δ) /n 2 2 . We bound the distance between h priv and h * with high probability in Lemma 4.2.</p><p>Lemma 4.2. Let h priv be the vector released by DP-SGD with σ 2 = 64Λ 2 T 2 log(3T /δ) log(2/δ) /n 2 2 . Assume that the loss function is smooth in its first parameter, and that</p><formula xml:id="formula_31">σ 2 * = E i∼[n] ∇ (h * ; x i , y i ) 2 ≤ σ 2 . Let 0 &lt; ζ &lt; 1, then with probability at least 1 -ζ, h priv -h * 2 2 = O pΛ 2 log(1/δ) 2 ζµ 2 n 2 2 ,</formula><p>where O ignores logarithmic terms in n (the number of examples) and p (the number of model parameters).</p><p>We prove Lemmas 4.1 and 4.2 in Appendices E and F. These lemmas give upper bounds on the distance between the optimal model and the private models learned by output perturbation and DP-SGD. This distance decreases as the number of records n or the privacy budget increase. Conversely, it increases with the complexity of the model (i.e., with larger dimension p), and with the Lipschitz constant of the loss Λ.</p><p>Remark 4.3. For clarity of exposition in Lemma 4.2, we did not use minimal assumptions and used the simplest variant of DP-SGD. Notably, the assumption on σ * can be removed by using variance reduction schemes, and tighter bounds on σ can also be obtained using Rényi Differential Privacy <ref type="bibr" target="#b34">(Mironov, 2017)</ref>. Similarly, the assumption &lt; 1 is only used to give simple closed-form bounds. Strong convexity and smoothness assumptions can be relaxed as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Bounding the Fairness of Private Models</head><p>We now state our central result (Theorem 4.4), where we bound the fairness of h priv relatively to the one of h * .</p><p>Theorem 4.4. Let h * be the solution of (3), and h priv its private estimate obtained by output perturbation. Let h ref ∈ {h priv , h * }, and 0 &lt; ζ &lt; 1. Then, the difference of fairness of group k ∈ [K] satisfies, with probability at least 1ζ, Similarly, if h priv is estimated through DP-SGD, we have that, with probability at least 1ζ,</p><formula xml:id="formula_32">F k (h priv , D) -F k (h * , D) ≤ χ k (h ref , D)LΛ 32p log(1.25/δ) log(2/ζ) µn .</formula><formula xml:id="formula_33">F k (h priv , D) -F k (h * , D) ≤ O χ k (h ref , D)LΛ p log(1/δ) √ ζµn ,</formula><p>where O ignores logarithmic terms in n (the number of examples) and p (the number of model parameters).</p><p>Proof. By Lemma 4.1 or Lemma 4.2, we control the distance h privh * . Plugging this bound in Theorem 3.2 gives the result.</p><p>This result shows that, when learning a private model, the unfairness due to privacy vanishes at a O( √ p /n) rate. To the best of our knowledge, our result is the first to quantify this rate. Importantly, it highlights the role of the confidence margin of the classifier on the disparate impact of differential privacy. This is in line with previous empirical and theoretical work that identified the groupwise distances to the decision boundary as an important factor <ref type="bibr" target="#b42">(Tran et al., 2021;</ref><ref type="bibr" target="#b18">Fioretto et al., 2022)</ref>. However, our bounds are the first to quantify this impact through a classic notion of confidence margin studied in learning theory <ref type="bibr" target="#b11">(Cortes et al., 2013)</ref>.</p><p>Our result may be interpreted and used in various ways. A first example is the case where the private model is known but its optimal non-private counterpart is not. There, our result guarantees that, given enough examples, the fairness level of the private model is close to the one of the optimal non-private model. This allows the practitioner to give guarantees on the model, that the end user can trust. A second example is the case where the true model h * is owned by someone who cannot share it, due to privacy concerns.</p><p>Imagine that the model needs to be audited for fairness.</p><p>Then, the model owner can compute a private estimate of their model, and send it to the (honest but curious) auditing company. The bound allows to obtain fairness bounds for the true model from the inspection of the private one, and thus acts as a certificate of correctness of the audit done on the private version of the model.</p><p>Remark 4.5. The fairness guarantee for the private model given by Theorem 4.4 is relative to the fairness of the optimal model h * , which may itself be quite unfair. A standard approach to promote fair models is to use convex relaxations of fairness as regularizers to the ERM problem <ref type="bibr" target="#b5">(Bechavod &amp; Ligett, 2017;</ref><ref type="bibr" target="#b22">Huang &amp; Vishnoi, 2019;</ref><ref type="bibr" target="#b30">Lohaus et al., 2020)</ref>. Interestingly, to be able to use output perturbation, we only require the objective function of (3) to be strongly convex and Lipschitz over h ∈ H, which is the case for these relaxations when they are combined with a squared 2 -norm. For binary classification with two sensitive groups, Lohaus et al.</p><p>(2020) proved that, with a proper choice of regularization parameters, this approach can yield a fair h * (see their Theorem 1 for more details). Combined with our results, this paves the way for the design of algorithms that learn provably private and fair classifiers. However, several crucial challenges remain to make this approach work in practice, such as (i) finding the appropriate regularization parameters privately, and (ii) providing guarantees on the resulting classifiers' accuracy. We leave this for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Numerical Experiments</head><p>In this section, we numerically illustrate the upper bounds from Section 4.2. We use the celebA <ref type="bibr" target="#b29">(Liu et al., 2015)</ref> and folktables <ref type="bibr" target="#b13">(Ding et al., 2021)</ref> datasets, which respectively contain 202, 599 and 1, 664, 500 samples, with 39 and 10 features (including one sensitive attribute, sex, that is not not used for prediction), and binary labels. For each dataset, we use 90% of the records for training, and the remaining 10% for empirical evaluation of the bounds. We train 2 -regularized logistic regression models, ensuring that the underlying optimization problem is 1-strongly-convex. This allows learning private models by output perturbation, for which the bound from Theorem 4.4 holds.<ref type="foot" target="#foot_0">1</ref> </p><p>In Section 5.1, we show that we obtain non-trivial guarantees on the private model's fairness and accuracy. Then, we study the influence of the number of training samples and of the privacy budget in Section 5.2, and discuss the tightness of our result in Section 5.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Value of the Upper Bounds</head><p>In two groups) for multiple fairness and accuracy measures on two datasets. In all cases, our results give non-trivial guarantees on the difference of fairness: it is bounded by at most 0.105 for celebA and 0.0026 for folktables. This means that any (1, 1/n 2 )-DP model learned by output perturbation will, with high probability, achieve a fairness level within this margin of that of the non-private model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Influence of the Number of Training Samples and Privacy Budget</head><p>We now verify numerically the rate at which fairness and accuracy levels decrease when increasing the number of training records or privacy budget. In Figure <ref type="figure">1</ref>, we plot the optimal model's equality of opportunity and accuracy, as a function of (i) in the first line, the number of samples n used for training, or (ii) in the second line, the privacy budget (see Appendix G for results with other fairness measures).</p><p>For each value of n and , we plot Theorem 4.4's theoretical guarantees (solid blue line). With = 1, our bounds give meaningful guarantees for n ≥ 100, 000 records on both celebA and folktables datasets (Figures <ref type="figure">1a to 1d</ref>). When using all records, we obtain meaningful bounds for ≥ 1 for celebA and ≥ 0.1 for folktables (Figures 1e to 1h). Additionally, note that we obtain both upper and lower bounds on fairness and accuracy, confirming remarks from Section 3.2.</p><p>We also report the fairness and accuracy levels of 100 private models computed by output perturbation (in green in Figure <ref type="figure">1</ref>). As predicted by our theory, their fairness and accuracy converges towards the ones of their non-private counterparts as n and increase. Interestingly, our bounds seem to follow the same tendency as what we observe empirically (albeit with a larger multiplicative constant), suggesting that they capture the correct dependence in n and . We further discuss the tightness of our results in next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Tightness of the Bound</head><p>We now argue that the two major factors of looseness in our results are (i) the upper bound on h privh * and (ii) the looseness of Assumption 2.1. While these cannot be improved in general, specific knowledge of h priv and h * (that is typically not available due to privacy) can lead to tighter bounds. First, when the distance h privh * is known, we can use its actual value rather than the upper bounds of Section 4.1 (see dashed blue line in Figure <ref type="figure">1</ref>). Second, when both h priv and h * are known, Assumption 2.1 can be substantially refined (see details in Appendix G.3). We evaluate this bound for the private model that is the farthest away from the non-private one (see dotted blue line in Figure <ref type="figure">1</ref>). The resulting bound appears to be tight up to a small multiplicative constant. These two observations suggest that our bounds cannot be significantly tightened, unless one can obtain such knowledge through either private computation or additional assumptions on the data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this work, we proved that the fairness (and accuracy) costs induced by privacy in differentially private classification vanishes at a O( √ p /n) rate, where n is the number of training records, and p the number of parameters. This rate follows from a general statement on the regularity of a family of group fairness measures, that we prove to be pointwise Lipschitz with respect to the model. The pointwise Lipschitz constant explicitly depends on the confidence margin of the model, and may be different for each sensitive group. We also show it can be computed from a finite data sample. Importantly, our bounds do not require the knowledge of the optimal (non-private) model: they can thus be used in practical privacy-preserving scenarios. We numerically evaluate our bounds on real datasets, and highlight practical settings where non-trivial guarantees can be obtained.</p><p>While we illustrated our results for output perturbation and DP-SGD on strongly-convex problems, we stress that they are more general. Indeed, our Theorem 3.2 holds for any pair of models. Consequently, it would apply to DP-SGD on non-convex problems, provided that we have a bound on the distance between the models obtained with and without privacy. Deriving a tight bound on this distance is a challenging problem, that constitutes an interesting future direction.</p><p>Finally, we stress that our results do not provide fairness guarantees per se, but only bound the difference of fairness between models. It is nonetheless a first step towards a more complete understanding of the interplay between privacy, fairness, and accuracy. We believe that our results can guide the design of fairer privacy-preserving machine learning algorithms. A first promising direction in this regard is to combine our bounds with fairness-promoting convex regularizers, as discussed in Remark 4.5. Another direction is the design of methods to privately learn models with large-margin guarantees, as recently considered by <ref type="bibr" target="#b4">Bassily et al. (2022)</ref>. Our results, which explicitly depend on the confidence margin of the model, suggest that better fairness guarantees could be obtained for these methods.</p><p>This appendix provides several examples of group fairness functions compatible with our framework (Appendix A), the proofs of the main theoretical results that were omitted in the main paper for the sake of readability (Appendices B to F), and additional experiments (Appendix G).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Fairness functions</head><p>In this section we recall several well known fairness functions and show that they can be written in the form of Equation ( <ref type="formula" target="#formula_5">1</ref>).</p><p>Example 1 (Equalized Odds <ref type="bibr" target="#b20">(Hardt et al., 2016)</ref>). A model h is fair for Equalized Odds when the probability of predicting the correct label is independent of the sensitive attribute, that is, ∀(y, r) ∈ Y × S</p><formula xml:id="formula_34">F (y,r) (h, D) = P (H(X) = Y | Y = y, S = r) -P (H(X) = Y | Y = y) .</formula><p>We can then write F (y,r) (h, D) in the form of Equation (1) as</p><formula xml:id="formula_35">F (y,r) (h, D) = C 0 (y,r) + (y ,r )∈Y×S C (y ,r ) (y,r) P (H(x) = Y |Y = y , S = r ) (4) with C 0 (y,r) = 0 C (y,r) (y,r) = 1 -P (S = r | Y = y) ∀r = r, C (y,r ) (y,r) = -P (S = r | Y = y) ∀y = y, ∀r ∈ S, C (y ,r ) (y,r) = 0</formula><p>Proof. We have that</p><formula xml:id="formula_36">F (y,r) (h, D) = P (H(X) = Y | Y = y, S = r) -P (H(X) = Y | Y = y) = P (H(X) = Y | Y = y, S = r) - r ∈S P (H(X) = Y | Y = y, S = r ) P (S = r | Y = y)</formula><p>which gives the result.</p><p>Example 2 (Equality of Opportunity <ref type="bibr" target="#b20">(Hardt et al., 2016)</ref>). A model h is fair for Equality of Opportunity when the probability of predicting the correct label is independent of the sensitive attribute for the set of desirable outcomes Y ⊂ Y,</p><formula xml:id="formula_37">that is ∀(y, r) ∈ Y × S F (y,r) (h, D) = P (H(X) = Y | Y = y, S = r) -P (H(X) = Y | Y = y) if y ∈ Y , 0 otherwise.</formula><p>We can then write F (y,r) (h, D) in the form of Equation (1) as Proof. We consider the two cases. On the one hand, when y ∈ Y \ Y , then we have that</p><formula xml:id="formula_38">F (y,r) (h, D) = C 0 (y,r) + (y ,r )∈Y×S C (y ,r ) (y,r) P (H(X) = Y |Y = y , S = r ) (5) with, if y ∈ Y , C 0 (y,r) = 0 C (y,r) (y,r) = 1 -P (S = r | Y = y) ∀r = r, C</formula><formula xml:id="formula_39">F (y,r) (h, D) = 0</formula><p>which gives the first part of the result. On the other hand, when y ∈ Y , then we have that</p><formula xml:id="formula_40">F (y,r) (h, D) = P (H(X) = Y | Y = y, S = r) -P (H(X) = Y | Y = y) = P (H(X) = Y | Y = y, S = r) - r ∈S P (H(X) = Y | Y = y, S = r ) P (S = r | Y = y)</formula><p>which gives the second part of the result.</p><p>Example 3 (Accuracy Parity <ref type="bibr" target="#b47">(Zafar et al., 2017)</ref>). A model h is fair for Accuracy Parity when the probability of being correct is independent of the sensitive attribute, that is, ∀(r) ∈ S</p><formula xml:id="formula_41">F (r) (h, D) = P (H(X) = Y | S = r) -P (H(X) = Y ) .</formula><p>We can then write F (r) (h, D) in the form of Equation (1) as</p><formula xml:id="formula_42">F (r) (h, D) = C 0 (r) + (r )∈S C (r ) (r) P (H(X) = Y |S = r ) (6) with C 0 (r) = 0 C (r) (r) = 1 -P (S = r) ∀r = r, C (r ) (r) = -P (S = r )</formula><p>Proof. We have that</p><formula xml:id="formula_43">F (r) (h, D) = P (H(X) = Y | S = r) -P (H(X) = Y ) = P (H(X) = Y | S = r) - r ∈S P (H(X) = Y | S = r ) P (S = r )</formula><p>which gives the result.</p><p>Example 4 (Demographic Parity (Binary Labels) <ref type="bibr" target="#b7">(Calders et al., 2009)</ref>). A model h is fair for Demographic Parity with binary labels when the probability of predicting a label is independent of the sensitive attribute, that is, ∀(y, r) ∈ Y × S</p><formula xml:id="formula_44">F (y,r) (h, D) = P (H(X) = y | S = r) -P (H(X) = y) .</formula><p>Assuming that given a label y, the second binary label is denoted ȳ, we can then write F (y,r) (h, D) in the form of Equation (1) as</p><formula xml:id="formula_45">F (y,r) (h, D) = C 0 (y,r) + (y ,r )∈Y×S C (y ,r ) (y,r) P (H(X) = Y |Y = y , S = r ) (7) with C 0 (y,r) = P (Y = y) -P (Y = y | S = r) C (y,r) (y,r) = P (Y = y | S = r) -P (Y = y, S = r) C (ȳ,r) (y,r) = P (Y = ȳ, S = r) -P (Y = ȳ | S = r) ∀r = r, C (y,r ) (y,r) = -P (Y = y, S = r ) ∀r = r, C (ȳ,r ) (y,r) = P (Y = ȳ, S = r )</formula><p>Proof. We have that</p><formula xml:id="formula_46">F (y,r) (h, D) = P (H(X) = y | S = r) -P (H(X) = y) = P (H(X) = y | Y = y, S = r) P (Y = y | S = r) + P (H(X) = y | Y = y, S = r) P (Y = y | S = r) - r ∈S P (H(X) = y | Y = y, S = r ) P (Y = y, S = r ) + P (H(X) = y | Y = y, S = r ) P (Y = y, S = r ) = P (H(X) = y | Y = y, S = r) P (Y = y | S = r) + 1 -P (H(X) = y | Y = y, S = r) P (Y = y | S = r) - r ∈S P (H(X) = y | Y = y, S = r ) P (Y = y, S = r ) + 1 -P (H(X) = y | Y = y, S = r ) P (Y = y, S = r ) .</formula><p>Here, we only consider binary labels, y and ȳ. Hence, H(X) = y ⇔ H(X) = ȳ and Y = y ⇔ Y = ȳ. Thus, we obtain B. Proof of Theorem 3.1</p><formula xml:id="formula_47">F (y,r) (h, D) = P (H(X) = y | Y = y, S = r) P (Y = y | S = r) + (1 -P (H(X) = ȳ | Y = ȳ, S = r)) P (Y = ȳ | S = r) - r ∈S P (H(X) = y | Y = y, S = r ) P (Y = y, S = r ) + (1 -P (H(X) = ȳ | Y = ȳ, S = r )) P (Y = ȳ, S = r ) = P (H(X) = y | Y = y, S = r) [P (Y = y | S = r) -P (Y = y, S = r)] + P (H(X) = ȳ | Y = ȳ, S = r) [P (Y = ȳ, S = r) -P (Y = ȳ | S = r)] + r ∈S,r =r P (H(X) = y | Y = y, S = r ) (-P (Y = y, S = r )) + r ∈S,r =r P (H(X) = ȳ | Y = ȳ, S =</formula><p>Theorem (Pointwise Lipschitzness of Conditional Negative Predictions). Let H be a set of real vector-valued functions with L X,Y the Lipschitz constants defined in Assumption 2.1. Let h, h ∈ H be two models, (X, Y, S) be a triple of random variables having distribution D, and E be an arbitrary event. Assume that</p><formula xml:id="formula_48">E L X,Y |ρ(h,X,Y )| E &lt; +∞, then |P(H(X) = Y | E) -P(H (X) = Y | E)| ≤ E L X,Y |ρ(h, X, Y )| E h -h H .</formula><p>Proof. The proof of this theorem is in two steps. First, we use the Lipschitz continuity property associated with H, the triangle inequality, and the union bound to show that</p><formula xml:id="formula_49">|P (H(X) = Y | E) -P (H (X) = Y | E)| ≤ P L X,Y |ρ(h,X,Y )| ≤ h -h H E .</formula><p>Then, applying Markov's inequality gives the desired result.</p><p>Bounding</p><formula xml:id="formula_50">|P (H(X) = Y | E) -P (H (X) = Y | E)|. We have that P (H(X) = Y | E) -P (H (X) = Y | E) ≤ P (ρ(h, X, Y ) ≥ 0 | E) -P (ρ(h , X, Y ) &gt; 0 | E) = P (ρ(h , X, Y ) ≤ 0 | E) -P (ρ(h, X, Y ) &lt; 0 | E) = P (ρ(h , X, Y ) -ρ(h, X, Y ) + ρ(h, X, Y ) ≤ 0 | E) -P (ρ(h, X, Y ) &lt; 0 | E) = P (ρ(h, X, Y ) ≤ ρ(h, X, Y ) -ρ(h , X, Y ) | E) -P (ρ(h, X, Y ) &lt; 0 | E) ≤ P (ρ(h, X, Y ) ≤ |ρ(h, X, Y ) -ρ(h , X, Y )| | E) -P (ρ(h, X, Y ) &lt; 0 | E) ↓ Assumption 2.1. ≤ P (ρ(h, X, Y ) ≤ L X,Y h -h H | E) -P (ρ(h, X, Y ) &lt; 0 | E) = P ρ(h, X, Y ) &lt; 0 0 ≤ ρ(h, X, Y ) ≤ L X,Y h -h H E -P (ρ(h, X, Y ) &lt; 0 | E)</formula><p>↓ Union bound on disjoint events.</p><formula xml:id="formula_51">= P (0 ≤ ρ(h, X, Y ) ≤ L X,Y h -h H | E) ≤ P (|ρ(h, X, Y )| ≤ L X,Y h -h H | E) = P |ρ(h, X, Y )| L X,Y ≤ h -h H E</formula><p>Similarly, we have that</p><formula xml:id="formula_52">P (H (X) = Y | E) -P (H(X) = Y | E) ≤ P (ρ(h , X, Y ) ≥ 0 | E) -P (ρ(h, X, Y ) &gt; 0 | E) = P (ρ(h , X, Y ) + ρ(h, X, Y ) -ρ(h, X, Y ) ≥ 0 | E) -P (ρ(h, X, Y ) &gt; 0 | E) = P (ρ(h, X, Y ) ≥ -(ρ(h , X, Y ) -ρ(h, X, Y )) | E) -P (ρ(h, X, Y ) &gt; 0 | E) ≤ P (ρ(h, X, Y ) ≥ -|ρ(h , X, Y ) -ρ(h, X, Y )| | E) -P (ρ(h, X, Y ) &gt; 0 | E) ↓ Assumption 2.1 ≤ P (ρ(h, X, Y ) ≥ -L X,Y h -h H | E) -P (ρ(h, X, Y ) &gt; 0 | E) = P ρ(h, X, Y ) &gt; 0 0 ≥ ρ(h, X, Y ) ≥ -L X,Y h -h H E -P (ρ(h, X, Y ) &gt; 0 | E)</formula><p>↓ Union bound on disjoint events.</p><formula xml:id="formula_53">= P (0 ≥ ρ(h, X, Y ) ≥ -L X,Y h -h H | E) ≤ P (-|ρ(h, X, Y )| ≥ -L X,Y h -h H | E) = P |ρ(h, X, Y )| L X,Y ≤ h -h H E It implies that |P (H(X) = Y | E) -P (H (X) = Y | E)| ≤ P |ρ(h, X, Y )| L X,Y ≤ h -h H E Bounding P |ρ(h,X,Y )| L X,Y ≤ h -h H E .</formula><p>We use the Markov's Inequality and we assume that</p><formula xml:id="formula_54">E L X,Y |ρ(h,X,Y )| E &lt; +∞. Hence, we have that P |ρ(h, X, Y )| L X,Y ≤ h -h H E = P L X,Y |ρ(h, X, Y )| ≥ 1 h -h H E ↓ Markov's inequality. ≤ E L X,Y |ρ(h, X, Y )| E h -h H</formula><p>It concludes the proof.</p><p>Remark B.1. In the last step of the proof of Theorem 3.1, we can also use the Chernoff bound:</p><formula xml:id="formula_55">P |ρ(h, X, Y )| L X,Y ≤ h -h H E = P exp -t |ρ(h, X, Y )| L X,Y ≥ exp (-t h -h H ) E ≤ E exp -t |ρ(h, X, Y )| L X,Y E exp (t h -h H ) .</formula><p>A correct choice of t would lead to potentially tighter bounds than the Markov's inequality at the expense of readability.</p><p>Remark B.2. Before using Markov's inequality or Chernoff bound in Theorem 3.1, we can modify the probability as</p><formula xml:id="formula_56">P |ρ(h, X, Y )| L X,Y ≤ h -h H E = P |ρ(h, X, Y )| L X,Y h-h H ≤ h -h H E , where |ρ(h, X, Y )| L X,Y h-h H = |ρ(h,X,Y )| L X,Y if |ρ(h, X, Y )| ≤ L X,Y h -h H , +∞ otherwise .</formula><p>This essentially means that, whenever the model's margin on a data record is large enough, its precise value is no more meaningful, as its prediction will not change whatsoever. The remaining of Theorem 3.1's proof is unchanged, except</p><formula xml:id="formula_57">with |ρ(h,X,Y )| L X,Y h-h H instead of |ρ(h,X,Y )| L X,Y</formula><p>.</p><p>Note that this can lead to much tighter bounds. Notably, when distance hh H between h and h is small enough, the difference of fairness may even become zero.</p><p>C. Proof of Theorem 3.2</p><p>Theorem (Pointwise Lipschitzness of Fairness). Let h, h ∈ H, L X,Y be defined as in Assumption 2.1, and (X, S, Y ) ∼ D.</p><p>For any fairness notion of the form of Equation (1), we have:</p><formula xml:id="formula_58">∀k ∈ [K], |F k (h, D) -F k (h , D)| ≤ χ k (h, D) h -h H , with χ k (h, D) = K k =1 C k k E 1 |h(X)| D k .</formula><p>Similarly, for the aggregate measure of fairness defined in Equation (2), we have:</p><formula xml:id="formula_59">|Fair(h, D) -Fair(h , D)| ≤ 1 K K k=1 χ k (h, D) h -h H .</formula><p>Proof. The first part follows from the following derivation. For all k,</p><formula xml:id="formula_60">|F k (h, D) -F k (h , D)| = C 0 k + K k =1 C k k P (H(X) = Y |D k ) -C 0 k - K k =1 C k k P (H (X) = Y |D k ) = K k =1 C k k P (H(X) = Y |D k ) -P (H (X) = Y |D k ) ↓ Triangle inequality. ≤ K k =1 C k k |P (H(X) = Y |D k ) -P (H (X) = Y |D k )| ↓ Theorem 3.1. ≤ K k =1 C k k E L X,Y |ρ(h, X, Y )| D k h -h H .</formula><p>The second part is obtained thanks to the triangle inequality:</p><formula xml:id="formula_61">|Fair(h, D) -Fair(h , D)| = 1 K K k=1 |F k (h, D)| - 1 K K k=1 |F k (h , D)| ↓ Triangle inequality. ≤ 1 K K k=1 ||F k (h, D)| -|F k (h , D)|| ↓ Reverse triangle inequality. ≤ 1 K K k=1 |F k (h, D) -F k (h , D)| ,</formula><p>which gives the claim when combined with the first part of the theorem. </p><formula xml:id="formula_62">K k =0 C k k -C k k &gt; α C ≤ B 3 exp -B 4 α 2 C n .</formula><p>Let H be an hypothesis space and d H be the Natarajan dimension of H.</p><p>• Assuming that h and h are independent of D. With probability 1δ over the choice of D,</p><formula xml:id="formula_63">F k (h, D) -F k (h , D) ≤ χ k (h, D) h -h H + O K k =1 C k k log ( K /δ) np k</formula><p>• Assuming that h and h are dependent of D. With probability 1δ over the choice of D, ∀h, h ∈ H,</p><formula xml:id="formula_64">F k (h, D) -F k (h , D) ≤ χ k (h, D) h -h H + O K k =1 C k k d H + log ( K /δ) np k</formula><p>Proof. First of all, notice that we have</p><formula xml:id="formula_65">F k (h, D) -F k (h , D) = F k (h, D) -F k (h, D) + F k (h, D) -F k (h , D) ≤ F k (h, D) -F k (h, D) + F k (h, D) -F k (h , D) ↓ Theorem 3.2 ≤ F k (h, D) -F k (h, D) + χ k (h , D) h -h H</formula><p>Hence it remains to bound the first term. By definition of our fairness notions, notice that we have the following.</p><formula xml:id="formula_66">F k (h, D) -F k (h, D) = C 0 k + K k =1 C k k P (H(X) = Y |D k ) -C 0 k - K k =1 C k k P (H(X) = Y |D k ) ≤ C 0 k -C 0 k + K k =1 C k k P (H(X) = Y |D k ) -C k k P (H(X) = Y |D k ) ≤ C 0 k -C 0 k + K k =1 C k k P (H(X) = Y |D k ) -C k k P (H(X) = Y |D k ) + C k k P (H(X) = Y |D k ) -C k k P (H(X) = Y |D k ) ≤ C 0 k -C 0 k + K k =1 C k k P (H(X) = Y |D k ) -C k k P (H(X) = Y |D k ) + K k =1 C k k P (H(X) = Y |D k ) -C k k P (H(X) = Y |D k ) ≤ C 0 k -C 0 k + K k =1 C k k P (H(X) = Y |D k ) -P (H(X) = Y |D k ) + K k =1 C k k -C k k P (H(X) = Y |D k ) ≤ C 0 k -C 0 k + K k =1 C k k P (H(X) = Y |D k ) -P (H(X) = Y |D k ) + K k =1 C k k -C k k ≤ K k =0 C k k -C k k + K k =1 C k k P (H(X) = Y |D k ) -P (H(X) = Y |D k )</formula><p>We now need to consider two cases, depending on whether h depends on D or not.</p><p>Assuming that h is independent of D. In this case, our goal is to upper bound</p><formula xml:id="formula_67">P D∼D n F k (h, D) -F k (h, D) &gt; α C + K k =1 C k k α k</formula><p>Notice that, using the same trick that <ref type="bibr" target="#b44">Woodworth et al. (2017)</ref> used to prove their Equation (38), we have that</p><formula xml:id="formula_68">P D∼D n F k (h, D) -F k (h, D) &gt; α C + K k =1 C k k α k ≤ P D∼D n K k =0 C k k -C k k + K k =1 C k k P (H(X) = Y |D k ) -P (H(X) = Y |D k ) &gt; α C + K k =1 C k k α k ≤ P D∼D n K k =0 C k k -C k k &gt; α C K k =1 P (H(X) = Y |D k ) -P (H(X) = Y |D k ) &gt; α k ≤ P D∼D n K k =0 C k k -C k k &gt; α C + K k =1 P D∼D n P (H(X) = Y |D k ) -P (H(X) = Y |D k ) &gt; α k ≤ P D∼D n K k =0 C k k -C k k &gt; α C + K k =1 n i=0 P D∼D n P (H(X) = Y |D k ) -P (H(X) = Y |D k ) &gt; α k |D k | = i P D∼D n (|D k | = i) ↓ Let p k = P (D k ) ≤ P D∼D n K k =0 C k k -C k k &gt; α C + K k =1 np k /2-1 i=0 P D∼D n P (H(X) = Y |D k ) -P (H(X) = Y |D k ) &gt; α k |D k | = i P D∼D n (|D k | = i) + K k =1 n i= np k /2 P D∼D n P (H(X) = Y |D k ) -P (H(X) = Y |D k ) &gt; α k |D k | = i P D∼D n (|D k | = i) ≤ P D∼D n K k =0 C k k -C k k &gt; α C + K k =1 np k /2-1 i=0 P D∼D n (|D k | = i) + K k =1 n i= np k /2 P D k ∼D i k P (H(X) = Y |D k ) -P (H(X) = Y |D k ) &gt; α k P D∼D n (|D k | = i) ≤ P D∼D n K k =0 C k k -C k k &gt; α C + K k =1 P D∼D n |D k | &lt; np k 2 + K k =1 n i= np k /2 P D k ∼D i k P (H(X) = Y |D k ) -P (H(X) = Y |D k ) &gt; α k P D∼D n (|D k | = i)</formula><p>Using Hoeffding's inequality, we can show that</p><formula xml:id="formula_69">P D k ∼D n k k P (H(X) = Y |D k ) -P (H(X) = Y |D k ) &gt; β ≤ 2 exp -2β 2 n k which implies P D∼D n F k (h, D) -F k (h, D) &gt; α C + K k =1 C k k α k ↓ 2 exp -2β 2 i ≥ 2 exp -2β 2 (i + 1) . ≤ P D∼D n K k =0 C k k -C k k &gt; α C + K k =1 P D∼D n |D k | &lt; np k 2 + K k =1 2 exp -α 2 k np k ↓ By assumption, P D∼D n K k =0 C k k -C k k &gt; αC ≤ B3 exp -B4α 2 C n . ≤ B 3 exp -B 4 α 2 C n + K k =1 P D∼D n |D k | &lt; np k 2 + K k =1 2 exp -α 2 k np k ↓ Chernoff multiplicative bound. ≤ B 3 exp -B 4 α 2 C n + K k =1 exp - np k 8 + K k =1 2 exp -α 2 k np k Now, by assumption that n ≥ 8 log( 2K+1 δ ) min k p k and setting α C = log B3(2K+1) δ B 4 n α k = log 2(2K+1) δ np k yields that, with probability at least 1 -δ, F k (h, D) -F k (h, D) ≤ log B3(2K+1) δ B 4 n + K k =1 C k k log 2(2K+1) δ np k</formula><p>Assuming that h is dependent of D. In this case, our goal is to bound</p><formula xml:id="formula_70">P D∼D n sup h∈H F k (h, D) -F k (h, D) &gt; α C + K k =1 C k k α k</formula><p>Using similar arguments that in the independent case, we have that</p><formula xml:id="formula_71">P D∼D n sup h∈H F k (h, D) -F k (h, D) &gt; α C + K k =1 C k k α k ≤ P D∼D n K k =0 C k k -C k k &gt; α C + K k =1 P D∼D n |D k | &lt; np k 2 + K k =1 n i= np k /2 P D k ∼D i k sup h∈H P (H(X) = Y |D k ) -P (H(X) = Y |D k ) &gt; α k P D∼D n (|D k | = i)</formula><p>Using the Multiclass Fundamental Theorem (Shalev-Shwartz &amp; Ben-David, 2014, Theorem 29.3, Lemma 29.4) with d H the Natarajan dimension of H, we have that</p><formula xml:id="formula_72">P D k ∼D n k k sup h∈H P (H(X) = Y |D k ) -P (H(X) = Y |D k ) &gt; β ≤ 8n d H k |Y| 2d H exp - n k β 2 32 which implies P D∼D n sup h∈H F k (h, D) -F k (h, D) &gt; α C + K k =1 C k k α k ≤ P D∼D n K k =0 C k k -C k k &gt; α C + K k =1 P D∼D n |D k | &lt; np k 2 + K k =1 8 np k 2 d H |Y| 2d H exp - np k 2 α 2 k 32 Now, by assumption that n ≥ 8 log( 2K+1 δ ) min k p k and setting α C = log B3(2K+1) δ B 4 n α k = 64 log 8( np k 2 ) d H |Y| 2d H (2K+1) δ np k = 64 d H log np k 2 + 2 log (|Y|) + log 8(2K+1) δ np k yields that, with probability at least 1 -δ, ∀h ∈ H F k (h, D) -F k (h, D) ≤ log B3(2K+1) δ B 4 n + K k =1 C k k 64 d H log np k 2 + 2 log (|Y|) + log 8(2K+1) δ np k .</formula><p>This concludes the proof.</p><p>E. Bound for Output Perturbation (Proof of Lemma 4.1)</p><p>Lemma. Let h priv be the vector released by output perturbation with noise σ 2 = 8Λ 2 log(1.25/δ) /µ 2 n 2 2 , and 0 &lt; ζ &lt; 1, then with probability at least 1ζ,</p><formula xml:id="formula_73">h priv -h * 2 2 ≤ 32pΛ 2 log(1.25/δ) log(2/ζ) µ 2 n 2 2 .</formula><p>Proof. We prove this lemma in two steps. First, we show that for a given sensitivity, the distance h privh * is bounded. Second, we estimate the sensitivity.</p><p>Bounding the Error. Let ∆ be the sensitivity of the function D → arg min w∈C f (w; D). Its value can be released under ( , δ) differential privacy <ref type="bibr" target="#b10">(Chaudhuri et al., 2011;</ref><ref type="bibr" target="#b31">Lowy &amp; Razaviyayn, 2021)</ref> as follows:</p><formula xml:id="formula_74">h priv = h * + N (0, σ 2 I p ) ,<label>(8)</label></formula><p>where σ 2 = 2∆ 2 log(1.25/δ) 2 and h * = arg min h∈C f (h). Then, Chernoff's bound gives, for t, α &gt; 0,</p><formula xml:id="formula_75">P( h priv -h * 2 ≥ α) ≤ exp(-tα) E(exp(t h priv -h * 2 ))<label>(9)</label></formula><p>= exp(-tα)</p><formula xml:id="formula_76">p j=1 E(exp(t(h priv j -h * j ) 2 )) ,<label>(10)</label></formula><p>by independence of the noise's p coordinates. Since h priv j h * j is a Gaussian random variable of mean 0 and variance σ 2 , we can compute E(exp(t(h priv j h * j ))) = (1 -2tσ 2 ) -1/2 . We then obtain</p><formula xml:id="formula_77">P( h priv -h * 2 ≥ α) ≤ exp(-tα)(1 -2tσ 2 ) -p/2 . (<label>11</label></formula><formula xml:id="formula_78">) Let t = 1/4pσ 2 , then it holds that 1 -2tσ 2 = 1 -1/2p ≤ 1 and (1 -2tσ 2 ) -p/2 = exp - p 2 log(1 - 1 2p ) ≤ exp 1 2(1 -1 p ) ≤ exp(1/2) ≤ 2 ,<label>(12)</label></formula><p>By reorganizing, we obtain -2γ ∇f</p><formula xml:id="formula_79">(h t ), h t -h * ≤ -2γ(f (h t ) -f (h * )) -γµ h t -h * 2 , which gives E h t+1 -h * 2 ≤ (1 -γµ) h t -h * 2 -2γ(f (h t ) -f (h * )) + 2γ 2 E g t 2 + 2γ 2 E η t 2 . (24) Finally, remark that if f = 1 n n i=1 f i with each f i being β-smooth and E f i = f , we have, for i ∼ [n], E ∇f i (h t ) 2 = E ∇f i (h t ) -∇f i (h * ) + ∇f i (h * ) 2 (25) ≤ E(2 ∇f i (h t ) -∇f i (h * ) 2 + 2 ∇f i (h * ) 2 ) (26) ≤ E(4β(f i (h t ) -f i (h * ) -∇f i (h * ), h t -h * ) + 2 ∇f i (h * ) 2 ) (27) = 4β(f (h t ) -f (h * )) + 2 E ∇f i (h * ) 2 , (<label>28</label></formula><formula xml:id="formula_80">)</formula><p>since f i is β-smooth, which implies, for all w, v ∈ R p ,</p><formula xml:id="formula_81">∇f i (w) -∇f i (v) 2 ≤ 2β(f i (w) -f i (v) -∇f i (v), w -v ,<label>(29)</label></formula><p>and E ∇f i (h * ) = 0. Combined with the fact that E ∇f i (h * ) 2 ≤ σ 2 * and E η t 2 = pσ 2 , we obtained</p><formula xml:id="formula_82">E h t+1 -h * 2 ≤ (1 -γµ) h t -h * 2 + (4βγ 2 -2γ)(f (h t ) -f (h * )) + 2γ 2 (σ 2 * + σ 2 ) (30) ≤ (1 -γµ) h t -h * 2 + 4γ 2 σ 2 , (<label>31</label></formula><formula xml:id="formula_83">)</formula><p>since γ ≤ 1/2β, which implies 4βγ 2 -2γ ≤ 0 and σ * ≤ σ. By induction, we obtain that, after T iterations,</p><formula xml:id="formula_84">E h T -h * 2 ≤ (1 -γµ) T h 0 -h * 2 + 4γ 2 T -1 t=0 (1 -γµ) T -t σ 2 (32) ≤ (1 -γµ) T h 0 -h * 2 + 4γσ 2 µ .<label>(33)</label></formula><p>Now, recall that DP-SGD is ( , δ)-differentially private for σ 2 = 64Λ 2 T log(3T /δ) log(2/δ)</p><formula xml:id="formula_85">n 2 2</formula><p>(following from the Gaussian mechanism, advanced composition theorem and amplification by subsampling). Thus, taking γ = 1/2β, and setting T = 2β µ log(µβ h 0h * 2 /2M 2 ), where M 2 = 64Λ 2 log(2/δ)</p><formula xml:id="formula_86">n 2 2</formula><p>, yields</p><formula xml:id="formula_87">E h T -h * 2 ≤ 2(T log(3T /δ) + 1)M 2 βµ ≤ 8M 2 µ 2 log µβ h 0 -h * 2 2M 2 log 6β log( µβ h 0 -h * 2 2M 2 ) µδ .<label>(34)</label></formula><p>Using Markov inequality, we obtain</p><formula xml:id="formula_88">P   h T -h * 2 ≥ 8M 2 ζµ 2 log µβ h 0 -h * 2 2M 2 log 6β log( µβ h 0 -h * 2 2M 2 ) µδ   ≤ ζ . (<label>35</label></formula><formula xml:id="formula_89">)</formula><p>This results in the following upper bound, with probability at least 1ζ,</p><formula xml:id="formula_90">h T -h * 2 ≤ 512Λ 2 log(2/δ) ζµ 2 n 2 2 log µβ h 0 -h * 2 2M 2 log 6β log( µβ h 0 -h * 2 2M 2 ) µδ (36) = O G 2 log(1/δ) ζµ 2 n 2 2 , (<label>37</label></formula><formula xml:id="formula_91">)</formula><p>which is the result of our lemma.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. Additional Experimental Details</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.1. Experimental Setup</head><p>The celebA dataset <ref type="bibr" target="#b29">(Liu et al., 2015)</ref> is a face attributes dataset, that can be downloaded at http://mmlab.ie.cuhk. edu.hk/projects/CelebA.html, and the folktables dataset <ref type="bibr" target="#b13">(Ding et al., 2021)</ref> is derived from US Census, and can be downloaded using a Python package available here https://github.com/zykls/folktables.</p><p>On each dataset, for each value of n, we train a 2 -regularized logistic regression model using scikit-learn <ref type="bibr" target="#b36">(Pedregosa et al., 2011)</ref>. Private models are then learned using the output perturbation mechanism as described in Section 4.1. We then compute our bounds using the non-private model as reference, over a test set containing 10% of the data, that has not been used for training (containing 20, 260 records for celebA and 166, 450 records for folktables). The value of the bound is computed by minimizing the experession given by the Chernoff bound using the golden section search algorithm <ref type="bibr" target="#b25">(Kiefer, 1953)</ref>. The code is in the supplementary, and will be made public.</p><p>For the plots with different number of training records, we train 20 non-private models with a number of records logarithmically spaced between 10 and the number of records in the complete training set (that is, 182, 339 for celebA and 1, 498, 050 for folktables). For the plots with different privacy budgets, we use 20 values logarithmically spaced between 10 -3 and 10 for both datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.2. Results for Other Fairness Measures</head><p>Our bounds also hold for accuracy parity, demographic parity and equalized odds. The same plots as those presented in Figure <ref type="figure">1</ref> for these fairness notions are in Figure <ref type="figure" target="#fig_3">2</ref> and Figure <ref type="figure">3</ref>. The comments from Section 5 on equality of opportunity and accuracy also hold for these three notions of fairness.  G.3. Refined Bounds with Additional Knowledge of h priv and h *</p><p>In Assumption 2.1, we use a uniform Lipschitz bound for all h, h ∈ H. Let's consider the class H of linear models, where, for h ∈ H, we denote by h y the parameters of h associated with the label y, that is h(x, y) = h T y x. For linear models, we</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Lemma 3.4. Let D be a finite sample of n ≥ 8 log( 2K+1 δ ) min k p k examples drawn i.i.d. from D, where p k is the true proportion of examples from group k . Assume that P D∼D n</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>r) = -P (S = r | Y = y) ∀y = y, ∀r ∈ S, C (y ,r ) (y,r) = 0 and, if y ∈ Y \ Y , ∀y ∈ Y, ∀r ∈ S, C (y ,r ) (y,r) = 0.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>r ) P (Y = ȳ, S = r ) + P (Y = ȳ | S = r) -P (Y = ȳ) = P (H(X) = Y | Y = y, S = r) [P (Y = y | S = r) -P (Y = y, S = r)] + P (H(X) = Y | Y = ȳ, S = r) [P (Y = ȳ, S = r) -P (Y = ȳ | S = r)] + r ∈S,r =r P (H(X) = Y | Y = y, S = r ) (-P (Y = y, S = r )) + r ∈S,r =r P (H(X) = Y | Y = ȳ, S = r ) P (Y = ȳ, S = r ) + P (Y = y) -P (Y = y | S = r) which gives the result.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 .</head><label>2</label><figDesc>Figure2. Fairness and accuracy levels for optimal non-private model and random private ones as a function of the number n of training samples. For each value of n, we sample 100 private models and take their minimum and maximum fairness/accuracy values to mark the area of attainable values. The solid blue line and the dashed one give our guarantees, respectively from Theorem 4.4 with Lemma 4.1's bounds and with an empirical evaluation of h priv -h * .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Upper bound, with 99% probability, on the difference of fairness between private and non-private models for different fairness measures and accuracy. Privacy parameters are = 1 and δ = 1/n 2 where n is the number of samples in the training data.</figDesc><table><row><cell>Dataset</cell><cell cols="5">Equality of Opportunity Equalized Odds Demographic Parity Accuracy Parity Accuracy</cell></row><row><cell>celebA (n = 182, 339)</cell><cell>0.1044</cell><cell>0.0975</cell><cell>0.0975</cell><cell>0.0975</cell><cell>0.0487</cell></row><row><cell>folktables (n = 1, 498, 050)</cell><cell>0.0017</cell><cell>0.0026</cell><cell>0.0026</cell><cell>0.0026</cell><cell>0.0013</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Table 1, we compute the value of Theorem 4.4's bounds.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="10">Differential Privacy has Bounded Impact on Fairness in Classification</cell></row><row><cell></cell><cell></cell><cell cols="3">Theoretical Upper Bound Theoretical Upper Bound</cell><cell cols="5">Bound with Empirical Distance Bound with Empirical Distance</cell><cell cols="3">Improved Bound Knowing both Models Improved Bound Knowing both Models</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Non-private Model Fairness</cell><cell></cell><cell></cell><cell cols="3">Private Models Fairness</cell></row><row><cell></cell><cell>0.4</cell><cell></cell><cell></cell><cell>1.0</cell><cell></cell><cell></cell><cell></cell><cell>0.10</cell><cell></cell><cell></cell><cell></cell><cell>0.74</cell></row><row><cell>Equal. Opp.</cell><cell>0.1 0.2 0.3</cell><cell></cell><cell>Accuracy</cell><cell>0.6 0.8</cell><cell></cell><cell></cell><cell>Equal. Opp.</cell><cell>0.02 0.04 0.06 0.08</cell><cell></cell><cell></cell><cell></cell><cell>Accuracy</cell><cell>0.66 0.68 0.70 0.72</cell></row><row><cell></cell><cell>0.0</cell><cell cols="2">10 2 Number of training samples 10 4</cell><cell>0.4</cell><cell cols="2">10 2 Number of training samples 10 4</cell><cell></cell><cell>0.00</cell><cell cols="3">10 2 Number of training samples 10 4 10 6</cell><cell>10 2 Number of training samples 10 4 10 6</cell></row><row><cell cols="4">(a) Equal. Opp. (celebA)</cell><cell cols="3">(b) Accuracy (celebA)</cell><cell cols="5">(c) Equal. Opp. (folktables)</cell><cell>(d) Accuracy (folktables)</cell></row><row><cell></cell><cell>0.15</cell><cell></cell><cell></cell><cell>0.90</cell><cell></cell><cell></cell><cell></cell><cell>0.04</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Equal. Opp.</cell><cell>0.05 0.10</cell><cell></cell><cell>Accuracy</cell><cell>0.80 0.85</cell><cell></cell><cell></cell><cell>Equal. Opp.</cell><cell>0.01 0.02 0.03</cell><cell></cell><cell></cell><cell></cell><cell>0.72 0.74</cell></row><row><cell></cell><cell>0.00</cell><cell>10 -2 Value of</cell><cell>10 0</cell><cell>0.75</cell><cell>10 -2 Value of</cell><cell>10 0</cell><cell></cell><cell>0.00</cell><cell></cell><cell>10 -2 Value of</cell><cell>10 0</cell><cell>0.70</cell><cell>Value of 10 -2</cell><cell>10 0</cell></row><row><cell cols="4">(e) Equal. Opp. (celebA)</cell><cell cols="3">(f) Accuracy (celebA)</cell><cell cols="5">(g) Equal. Opp. (folktables)</cell></row></table><note><p><p><p>We learn a non-private 2 -regularized logistic regression model, and use it to compute the bounds (averaged over the Accuracy (h) Accuracy (folktables) Figure</p>1</p>. Equality of opportunity (Equal. Opp.) and Accuracy levels for optimal non-private model and random private ones as a function of the number of training records n (first line, with = 1 and δ = 1/n 2 ) and of the privacy budget (second line, using all available training records). For each value of n and , we sample 100 private models and take their minimum and maximum fairness/accuracy values to mark the area of attainable values. The solid blue line gives the theoretical guarantees from Theorem 4.4, while the dashed and dotted line give finer bounds when more information is available (see Section 5.3 for details).</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>examples drawn i.i.d. from D, where p k is the true proportion of examples from group k . Assume that P D∼D n</figDesc><table><row><cell>D. Proof of Lemma 3.4</cell><cell></cell><cell></cell></row><row><cell>Lemma (Finite Sample analysis). Let D be a finite sample of n ≥</cell><cell>8 log( 2K+1 δ min k p k</cell><cell>)</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>The code is available at https://github.com/ pmangold/fairness-privacy.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>This work was supported by the <rs type="funder">Inria Exploratory Action FLAMED</rs>, by the <rs type="funder">Région Hauts de France</rs> (<rs type="programName">Projet STaRS Equité en apprentissage décentralisé respectueux de la vie privée</rs>), and by the <rs type="funder">French National Research Agency (ANR)</rs> through the grants <rs type="grantNumber">ANR-20-CE23-0015</rs> (Project <rs type="projectName">PRIDE</rs>), and <rs type="grantNumber">ANR 22-PECY-0002</rs> <rs type="projectName">IPOP</rs> (<rs type="projectName">Interdisciplinary Project on Privacy)</rs> project of the <rs type="projectName">Cybersecurity PEPR</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_8v8w8bU">
					<orgName type="program" subtype="full">Projet STaRS Equité en apprentissage décentralisé respectueux de la vie privée</orgName>
				</org>
				<org type="funded-project" xml:id="_musK7Je">
					<idno type="grant-number">ANR-20-CE23-0015</idno>
					<orgName type="project" subtype="full">PRIDE</orgName>
				</org>
				<org type="funded-project" xml:id="_CkT3KTj">
					<idno type="grant-number">ANR 22-PECY-0002</idno>
					<orgName type="project" subtype="full">IPOP</orgName>
				</org>
				<org type="funded-project" xml:id="_PAK5vWd">
					<orgName type="project" subtype="full">Interdisciplinary Project on Privacy)</orgName>
				</org>
				<org type="funded-project" xml:id="_CzzR4UM">
					<orgName type="project" subtype="full">Cybersecurity PEPR</orgName>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>1-1/2p ≥ -1 2 . Let 0 &lt; ζ &lt; 1, t = 1/4pσ 2 and α = 4pσ 2 log(2/ζ), we have proven</p><p>The error obtained by output perturbation is thus upper bounded by h privh * 2 ≤ 4pσ 2 log(2/ζ) = 8p∆ 2 log(1.25/δ) log(2/ζ) 2 with probability at least 1ζ.</p><p>Estimating the Sensitivity. Define g(h) = 1 n n i=1 (w; d i ) with d i ∈ X × Y such that d i = d i for all i = 1. By strong convexity, the two following inequalities hold for h, h ,</p><p>Summing these two inequalities give ∇f</p><p>Now, optimality conditions give</p><p>Combined with ( <ref type="formula">16</ref>), this shows that the sensitivity of arg min h∈C f (h) is ∆ = 2Λ nµ , which concludes the proof.</p><p>F. Convergence of DP-SGD (Proof of Lemma 4.2)</p><p>Lemma. Let h priv be the vector released by DP-SGD with</p><p>where O ignores logarithmic terms in n (the number of examples) and p (the number of model parameters).</p><p>Proof. We start by recalling that in DP-SGD,</p><p>Since h * ∈ H, and H is convex, we have</p><p>where we developed the square and used a</p><p>Taking the expectation with respect to the stochastic gradient computation and noise, we obtain</p><p>since E(η t ) = 0 and E(g t ) = ∇f (h t ). Now recall that, by strong-convexity of f , we have  (f) Equality of Opportunity (folktables)</p><p>Figure <ref type="figure">3</ref>. Fairness and accuracy levels for optimal non-private model and random private ones as a function of privacy budget . For each value of , we sample 100 private models and take their minimum and maximum fairness/accuracy values to mark the area of attainable values. The solid blue line and the dashed one respectively give our guarantees, respectively from Theorem 4.4 with Lemma 4.1's bounds and with an empirical evaluation of h priv -h * .</p><p>derived the bound ρ(h, x, y)ρ(h , x, y) H ≤ 2 x 2 hh H , as derived in Section 2. Note that this inequality can be very loose whenever x and h yh y (for y ∈ Y) are (close to) orthogonal. When they are orthogonal, this bounds only gives 0 = (h yh y ) T x ≤ h yh y 2 x 2 . We can thus improve the inequality by remarking that we have |ρ(h, x, y)ρ(h , x, y)| ≤ |h(x, y)h (x, y)| + max</p><p>where p hy-h y (x) is the projection of x on the axis defined by h yh y . We can thus define a variant of L X,Y which depends on</p><p>Replacing Assumption 2.1 by this inequality in the proof of Theorem 3.1, we end up with the inequality</p><p>where the probability is over (X, S, Y ) ∼ D. We obtained the same bound as Theorem 3.1, except with L h-h X,Y instead of L X,Y . Note that even if this gives a much tighter bound, this can generally not be computed, as one of h or h is typically not known.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Deep Learning with Differential Privacy</title>
		<author>
			<persName><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">B</forename><surname>Mcmahan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Mironov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Talwar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communications Security, CCS &apos;16</title>
		<meeting>the 2016 ACM SIGSAC Conference on Computer and Communications Security, CCS &apos;16<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machin</publisher>
			<date type="published" when="2016-10">October 2016</date>
			<biblScope unit="page" from="308" to="318" />
		</imprint>
	</monogr>
	<note>ery. 1130 citations (Crossref. 2022-08-19</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Trade-offs between fairness and privacy in machine learning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Agarwal</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Differential privacy has disparate impact on model accuracy</title>
		<author>
			<persName><forename type="first">E</forename><surname>Bagdasaryan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Poursaeed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Shmatikov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 32</title>
		<meeting><address><addrLine>Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
			<biblScope unit="page" from="15453" to="15462" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Private Empirical Risk Minimization: Efficient Algorithms and Tight Error Bounds</title>
		<author>
			<persName><forename type="first">R</forename><surname>Bassily</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Thakurta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 55th Annual Symposium on Foundations of Computer Science</title>
		<meeting><address><addrLine>Philadelphia, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014-10">2014. October 2014</date>
			<biblScope unit="page" from="464" to="473" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><surname>Bassily</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mohri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">T</forename><surname>Suresh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.10376</idno>
		<title level="m">Differentially private learning with margin guarantees</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Bechavod</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ligett</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.00044</idno>
		<title level="m">Penalizing unfairness in binary classification</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Bounds on the sample complexity for private learning and private data release</title>
		<author>
			<persName><forename type="first">A</forename><surname>Beimel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Brenner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">P</forename><surname>Kasiviswanathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Nissim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="2014-03">March 2014</date>
			<biblScope unit="page" from="401" to="437" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Building classifiers with independency constraints</title>
		<author>
			<persName><forename type="first">T</forename><surname>Calders</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Kamiran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pechenizkiy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Data Mining Workshops</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009">2009. 2009</date>
			<biblScope unit="page" from="13" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Fairness in machine learning: A survey</title>
		<author>
			<persName><forename type="first">S</forename><surname>Caton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Haas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.04053</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">On the privacy risks of algorithmic fairness</title>
		<author>
			<persName><forename type="first">H</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Shokri</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.03731</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Differentially Private Empirical Risk Minimization</title>
		<author>
			<persName><forename type="first">K</forename><surname>Chaudhuri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Monteleoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>Sarwate</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<idno type="ISSN">1533-7928</idno>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">29</biblScope>
			<biblScope unit="page" from="1069" to="1109" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Multi-class classification with maximum margin multiple kernel</title>
		<author>
			<persName><forename type="first">C</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mohri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rostamizadeh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="46" to="54" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">On the compatibility of privacy and fairness</title>
		<author>
			<persName><forename type="first">R</forename><surname>Cummings</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kimpara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Morgenstern</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Adjunct Publication of the 27th Conference on User Modeling, Adaptation and Personalization</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="309" to="315" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Retiring adult: New datasets for fair machine learning</title>
		<author>
			<persName><forename type="first">F</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Schmidt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Differential privacy</title>
		<author>
			<persName><forename type="first">C</forename><surname>Dwork</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Encyclopedia of Cryptography and Security</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Fairness through awareness</title>
		<author>
			<persName><forename type="first">C</forename><surname>Dwork</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Pitassi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Reingold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd innovations in theoretical computer science conference</title>
		<meeting>the 3rd innovations in theoretical computer science conference</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="214" to="226" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">The algorithmic foundations of differential privacy</title>
		<author>
			<persName><forename type="first">C</forename><surname>Dwork</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foundations and Trends in Theoretical Computer Science</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">3-4</biblScope>
			<biblScope unit="page" from="211" to="407" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><surname>Farrand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Mireshghallah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Trask</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.06389</idno>
		<title level="m">Neither private nor fair: Impact of data imbalance on utility and fairness in differential privacy</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Differential privacy and fairness in decisions and learning tasks: A survey</title>
		<author>
			<persName><forename type="first">F</forename><surname>Fioretto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">V</forename><surname>Hentenryck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Artificial Intelligence (IJCAI)</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="5470" to="5477" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Robin hood and matthew effects: Differential privacy has disparate impact on synthetic data</title>
		<author>
			<persName><forename type="first">G</forename><surname>Ganev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Oprisanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">D</forename><surname>Cristofaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Equality of opportunity in supervised learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Hardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Srebro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="3315" to="3323" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">The Elements of Statistical Learning</title>
		<author>
			<persName><forename type="first">T</forename><surname>Hastie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Tibshirani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Friedman</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-0-387-84858-7</idno>
		<ptr target="http://link.springer.com/10.1007/978-0-387-84858-7" />
	</analytic>
	<monogr>
		<title level="s">Springer Series in Statistics</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>Springer</publisher>
			<pubPlace>New York, NY</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Stable and fair classification</title>
		<author>
			<persName><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Vishnoi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2879" to="2890" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Differentially private fair learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Jagielski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kearns</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Oprea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sharifi-Malvajerdi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ullman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3000" to="3008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">What Can We Learn Privately</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">P</forename><surname>Kasiviswanathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Nissim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Raskhodnikova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Computing</title>
		<imprint>
			<biblScope unit="page" from="793" to="826" />
			<date type="published" when="2011-01">January 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Sequential minimax search for a maximum</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kiefer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the American mathematical society</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="502" to="506" />
			<date type="published" when="1953">1953</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Blind justice: Fairness with encrypted sensitive attributes</title>
		<author>
			<persName><forename type="first">N</forename><surname>Kilbertus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gascón</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kusner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Veale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Gummadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Weller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2630" to="2639" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Counterfactual fairness</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Kusner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Loftus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Silva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">When machine learning meets privacy: A survey and outlook</title>
		<author>
			<persName><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shaham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Rahayu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Farokhi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys (CSUR)</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1" to="36" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Deep learning face attributes in the wild</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Computer Vision (ICCV)</title>
		<meeting>International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2015-12">December 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Too relaxed to be fair</title>
		<author>
			<persName><forename type="first">M</forename><surname>Lohaus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Perrot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Von</forename><surname>Luxburg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="6360" to="6369" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Lowy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Razaviyayn</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.04704</idno>
		<title level="m">Output perturbation for differentially private convex optimization with improved population loss bounds, runtimes and applications to private adversarial training</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Fairness aware gradient descent</title>
		<author>
			<persName><forename type="first">G</forename><surname>Maheshwari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Perrot</surname></persName>
		</author>
		<author>
			<persName><surname>Fairgrad</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2206.10923</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A survey on bias and fairness in machine learning</title>
		<author>
			<persName><forename type="first">N</forename><surname>Mehrabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Morstatter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lerman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Galstyan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys (CSUR)</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1" to="35" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Renyi Differential Privacy</title>
		<author>
			<persName><forename type="first">I</forename><surname>Mironov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 30th Computer Security Foundations Symposium (CSF)</title>
		<meeting><address><addrLine>August</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Fair learning with private demographic data</title>
		<author>
			<persName><forename type="first">H</forename><surname>Mozannar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ohannessian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Srebro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="7066" to="7075" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Scikit-learn: Machine learning in Python</title>
		<author>
			<persName><forename type="first">F</forename><surname>Pedregosa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Varoquaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gramfort</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Thirion</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Grisel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Blondel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Prettenhofer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Dubourg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Vanderplas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Passos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cournapeau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Brucher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Perrot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Duchesnay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2825" to="2830" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Fair decision making using privacy-protected data</title>
		<author>
			<persName><forename type="first">D</forename><surname>Pujol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mckenna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kuppam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Machanavajjhala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Miklau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency, FAT* &apos;20</title>
		<meeting>the 2020 Conference on Fairness, Accountability, and Transparency, FAT* &apos;20<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="189" to="199" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Sanyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2206.03985</idno>
		<title level="m">How unfair is private learning?</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Understanding machine learning: From theory to algorithms</title>
		<author>
			<persName><forename type="first">S</forename><surname>Shalev-Shwartz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ben-David</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<publisher>Cambridge university press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Stochastic gradient descent with differentially private updates</title>
		<author>
			<persName><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chaudhuri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>Sarwate</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2013 IEEE Global Conference on Signal and Information Processing</title>
		<meeting><address><addrLine>Austin, TX, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-12">December 2013</date>
			<biblScope unit="page" from="245" to="248" />
		</imprint>
	</monogr>
	<note>IEEE. 145 citations (Crossref) [2022-08-19</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Fioretto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Van Hentenryck</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.12562</idno>
		<title level="m">Differentially private and fair deep learning: A lagrangian dual approach</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Differentially private empirical risk minimization under the fairness lens</title>
		<author>
			<persName><forename type="first">C</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dinh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Fioretto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="27555" to="27565" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Uniyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Naidu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kotti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Kenfack</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Mireshghallah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Trask</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.12576</idno>
		<title level="m">Dp-sgd vs pate: Which has less disparate impact on model accuracy?</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Learning non-discriminatory predictors</title>
		<author>
			<persName><forename type="first">B</forename><surname>Woodworth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gunasekar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Ohannessian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Srebro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Learning Theory</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1920" to="1953" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Achieving differential privacy and fairness in logistic regression</title>
		<author>
			<persName><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Companion Proceedings of The 2019 World Wide Web Conference</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="594" to="599" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Removing disparate impact of differentially private stochastic gradient descent on model accuracy</title>
		<author>
			<persName><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.03699</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Fairness beyond disparate treatment &amp; disparate impact: Learning classification without disparate mistreatment</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">B</forename><surname>Zafar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Valera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gomez Rodriguez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">P</forename><surname>Gummadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th international conference on world wide web</title>
		<meeting>the 26th international conference on world wide web</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1171" to="1180" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
