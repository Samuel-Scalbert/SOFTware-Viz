<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A ML-LLM pairing for better code comment classification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Hanna</forename><forename type="middle">Abi</forename><surname>Akl</surname></persName>
							<email>hanna.abi-akl@dsti.institute</email>
							<affiliation key="aff0">
								<orgName type="department">Data ScienceTech Institute (DSTI)</orgName>
								<address>
									<addrLine>4 Rue de la Collégiale</addrLine>
									<postCode>75005</postCode>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">Université Côte d&apos;Azur</orgName>
								<orgName type="institution" key="instit2">Inria</orgName>
								<orgName type="institution" key="instit3">CNRS</orgName>
								<address>
									<region>I3S</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A ML-LLM pairing for better code comment classification</title>
					</analytic>
					<monogr>
						<idno type="ISSN">1613-0073</idno>
					</monogr>
					<idno type="MD5">6E8733471C9C1771914F0447FF7116A5</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-04-12T14:46+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Natural Language Processing</term>
					<term>Machine Learning</term>
					<term>Information Retrieval</term>
					<term>Large Language Models</term>
					<term>Code Comprehension</term>
					<term>Comment Quality</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The "Information Retrieval in Software Engineering (IRSE) 1 " at FIRE 2023 shared task introduces code comment classification, a challenging task that pairs a code snippet with a comment that should be evaluated as either useful or not useful to the understanding of the relevant code. We answer the code comment classification shared task challenge by providing a two-fold evaluation: from an algorithmic perspective, we compare the performance of classical machine learning systems and complement our evaluations from a data-driven perspective by generating additional data with the help of large language model (LLM) prompting to measure the potential increase in performance. Our best model, which took second place in the shared task, is a Neural Network with a Macro-F1 score of 88.401% on the provided seed data and a 1.5% overall increase in performance on the data generated by the LLM.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>In software development, code and documentation go hand-in-hand. Writing code is crucial to maintaining existing code bases, developing new features and fixing bugs. Documentation helps developers make sense of the logic behind written code and provides a steady set of guidelines to iterate over it <ref type="bibr" target="#b0">[1]</ref>. Code commenting is a form of documentation whereby comments written in natural language are inserted in the code <ref type="bibr" target="#b0">[1]</ref>. The advantage of this method is that it helps clarify parts of the code without affecting performance since comments are ignored by compilers <ref type="bibr" target="#b0">[1]</ref>. It also provides an easy way to reflect updates on code changes without having to modify the entire documentation <ref type="bibr" target="#b0">[1]</ref>.</p><p>From the practice of writing code comments comes the challenge of identifying useful comments <ref type="bibr" target="#b1">[2]</ref>. Writing comments is not always accurate science, and some comments can be outdated or ambiguous <ref type="bibr" target="#b1">[2]</ref>. This can be problematic for developers who rely on these comments to understand and alter the code. There is then a real need for code comment checking. This need has framed the task of collecting code comments from real projects in a code-comment database to aid in the task of classifying useful versus not useful comments <ref type="bibr" target="#b2">[3]</ref>. Code comment classification is still a relatively new task that explores the possibility of accurately discriminating between comments that bring added value to the corresponding code and comments that are not pertinent with respect to the surrounding code <ref type="bibr" target="#b3">[4]</ref>.</p><p>Recent research has aimed to answer this challenge by compiling a semantic code-comment base by scraping and collecting code and surrounding comments from real projects in C <ref type="bibr" target="#b2">[3]</ref>. Researchers have also explored applying machine and deep learning techniques to solve this binary classification problem by considering useful comments (i.e., informative of the surrounding code) as a class and non-useful comments (i.e., redundant, uninformative or ambiguous) as another <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6]</ref>.</p><p>On the other hand, the rise of large language models (LLM) <ref type="bibr" target="#b6">[7]</ref> and their ability to pose as a jack-of-all-trades by solving a wide range of machine learning and deep learning problems, coupled with their wealth of training data, make them an interesting entry point for the code comment classification task <ref type="bibr" target="#b7">[8]</ref>. Based on the Transformers model <ref type="bibr" target="#b8">[9]</ref>, they are able to create robust embeddings from text, which helps them tackle problems based on natural language <ref type="bibr" target="#b9">[10]</ref>. Another recent breakthrough in LLMs is in generative artificial intelligence, where users combine pre-trained models with different prompting techniques to generate output data (e.g., text) <ref type="bibr" target="#b10">[11]</ref>. This prompting ability is at the heart of prompt engineering, a method that can redirect a LLM into focusing its generation on a specific need. This need can be in the form of answering specific questions, solving certain tasks (e.g., a classification problem) or even producing data in a pre-defined format <ref type="bibr" target="#b11">[12]</ref>. The latter use case plays a detrimental role in data augmentation, whereby users can couple the power of LLMs with a pre-existing dataset to enrich it and overcome data scarcity <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14]</ref>.</p><p>The IRSE at FIRE 2023 shared task proposes to measure the effects of leveraging LLMs in the context of solving the code comment classification problem <ref type="bibr" target="#b14">[15]</ref>. Specifically, challengers are asked to use the generative capabilities of LLMs to enrich an existing dataset of code comments and compare the performance of classical machine learning models on the classification task before and after data augmentation <ref type="bibr" target="#b14">[15]</ref>.</p><p>In this paper, we show how prompting LLMs effectively can increase model performance on the code comment classification problem. The rest of the paper is organized as follows. In section 2, we discuss some of the related work. In section 3, we present the experimental setup. In section 4, we discuss the results. Finally, we present our conclusions in section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>This section discusses some of the proposed strategies in the literature to classify code comments by quality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Baseline models for code comment classification</head><p>Paul <ref type="bibr" target="#b15">[16]</ref> leveraged classical machine learning models to solve the code comment classification task on a C language dataset of code and comments. They extracted text-level features like comment length and comment position within the source code and found a comparable performance between a logistic regression and a support vector machine binary classifier <ref type="bibr" target="#b15">[16]</ref>. Das and Chatterjee <ref type="bibr" target="#b16">[17]</ref> studied the performance of deep learning models by proposing a fusion transformer system based on BERT and CodeBERT. Their system combined text-based features with dense embeddings and outperformed all other baseline models on the code comment classification task <ref type="bibr" target="#b16">[17]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Embedding techniques for code comment classification</head><p>Basu et al. <ref type="bibr" target="#b17">[18]</ref> compared both classical machine learning models and transformer-based models with different embedding techniques and found that the bag-of-word representation can outperform transformer-based embeddings on the code comment classification problem. Their findings could not be generalized and were limited by the size of the dataset they used for their runs <ref type="bibr" target="#b17">[18]</ref>. Majumdar et al. <ref type="bibr" target="#b18">[19]</ref> examined the effects of using embeddings to tackle the code comment pair classification challenge by developing and training a low-dimensional contextualized word embeddings model based on masked language models. The resulting model captured semantic code concepts better and resulted in a boost in their binary classification systems when compared to vanilla word embeddings models <ref type="bibr" target="#b18">[19]</ref>. Other areas of research suggest an inclination toward specializing software engineering terms and building a domain vocabulary to produce more representative word models. Mishra and Sharma <ref type="bibr" target="#b19">[20]</ref> proposed a methodology for crawling and scraping Wikipedia as a base for collecting software engineering terms. Gonzalez-Perez and Henderson-Sellers <ref type="bibr" target="#b20">[21]</ref> laid the groundwork for the construction of such an ontology in terms of completeness, clarity, generalizability and extensibility. Simmons and Dillon <ref type="bibr" target="#b21">[22]</ref> proposed an open-source architecture designed to act as both an ontology and a knowledge base meta-model for software development semantics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Experiments</head><p>This section describes the framework of our experiments in terms of data, models and training process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Dataset description</head><p>The dataset considered for this shared task is divided in two parts: a seed data provided by the task organizers and a LLM-generated dataset to complement it. We introduce both datasets in the following subsections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1.">Seed Data</head><p>The data provided by the task organizers consists of 11452 pairs of code and comments written in C, labeled as either Useful or Not Useful. The data contains 7063 Useful rows and 4389 Not Useful rows. The comments and surrounding code snippets are extracted from GitHub. For every comment, the label (Useful or Not Useful) was generated by a team of 14 annotators. Each comment was annotated by 2 annotators. Cohen's metric was used for inter-annotator agreement with a kappa score of 0.734. The annotation process was supervised by weekly meetings and peer review sessions. Sample data is shown in Figure <ref type="figure" target="#fig_0">1</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2.">Data Augmentation</head><p>Participants are required to generate an additional dataset to complement the provided seed data. The generated dataset consists of code and comment pairs with labels generated using a LLM model of choice. In our experiments, we chose ChatGPT as our LLM and prompted it to generate data that aligns with the criteria of the given dataset, i.e., the generated code snippets should be written in the C programming language and the corresponding comments should be a mixture of useful and not useful. Additionally, we asked ChatGPT<ref type="foot" target="#foot_0">1</ref> to label each code-comment pair with the corresponding class (Useful or Not Useful). The ablation study performed on the LLM-generated dataset can be found in the Appendix. Using this method, we were able to generate 421 new code-comment pairs with 411 being labeled as Useful and 10 labeled as Not Useful by ChatGPT. Figure <ref type="figure" target="#fig_1">2</ref> shows an example output from ChatGPT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">System description</head><p>This section introduces the methodology used in our experimental runs. It describes the machine learning models as well as the features employed in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1.">Model Choice</head><p>Since the challenge explicitly limits participants to classical machine learning models (including neural networks but not extending to recurrent neural networks or more modern architectures like large language models), we based our experiments on 3 systems: Random Forest (RF), Voting Classifier (VC) and Neural Network (NN).</p><p>For the Random Forest model, we configured the following parameters: number of estimators = 100, criterion = gini, minimum samples split = 2, minimum samples leaf = 1, maximum features = sqrt, and bootstrap = True. The last model is a standalone Neural Network with the following configuration: number of hidden layers = 2, hidden layers sizes = (20,10), activation = relu, solver = adam, alpha = 0.0001, learning rate = constant, initial learning rate = 0.001, maximum iterations = 200, shuffle = True, tolerance = 0.0001, momentum = 0.9, nesterov's momentum = True, beta 1 = 0.9, beta 2 = 0.999, epsilon = 0.00000001.</p><p>All models are implemented using the scikit-learn<ref type="foot" target="#foot_1">2</ref> package in Python.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2.">Features</head><p>For the feature engineering phase, we concatenate code-comment pairs and embed the resulting input strings. We use the flax-sentence-embeddings/st-codesearch-distilroberta-base<ref type="foot" target="#foot_2">3</ref> model trained with the Hugging Face sentence-transformers<ref type="foot" target="#foot_3">4</ref> library on the CodeSearchNet<ref type="foot" target="#foot_4">5</ref> dataset compiled from code and documentation strings in the Go, Java, Javascript, PHP, Python and Ruby programming languages <ref type="bibr" target="#b22">[23]</ref>. The result is one 768 dimensional embedding vector for every code-comment input string. These embeddings constitute our final feature set and are fed to the different models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3.">Experimental Setup</head><p>We divide our experiment in two phases: seed data run and seed + LLM data run. The setup is identical for both phases and the only difference is the input data used. In the seed data run, only the seed data provided by the task organizers is used to assess model performance. In the seed + LLM data run, the data generated by ChatGPT is added to the seed data and the resulting augmented dataset is used as the input for our models.</p><p>In both phases, analyzing the data at our disposal shows a class imbalance where the Useful class is over-represented at 61.6% in the seed data and 97.6% in the LLM-generated data. We use the SMOTE <ref type="bibr" target="#b23">[24]</ref> technique to balance the datasets and restore class parity by synthetically generating rows of Not Useful data to achieve a 50-50 percent class distribution.</p><p>Next, we split our data using the scikit-learn Repeated Stratified K-Fold cross validator<ref type="foot" target="#foot_5">6</ref> with 10 folds and 3 allowed repetitions. We use the Accuracy, Precision, Recall and F1 scores as metrics for evaluating our models. All experiments are performed on a Dell G15 Special Edition 5521 hardware with 14 CPU Cores, 32 GB RAM and NVIDIA GeForce RTX 3070 Ti GPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Results</head><p>Table <ref type="table" target="#tab_0">1</ref> demonstrates the performance of each model on the seed data. For each scoring metric, the best score is marked in bold for both the Useful (U) and Not Useful (NU) classes. On the majority of the scoring metrics, the Neural Network outclasses the Random Forest and the Voting Classifier models. The Voting Classifier outperforms the Neural Network on the Recall of the Useful class and the Precision of the Not Useful class which shows that applying different non-linear models together can compensate for the shortcomings of one model's blind spots and classify more instances of Useful and Not Useful data correctly.</p><p>The results of Table <ref type="table" target="#tab_1">2</ref> are consistent with these findings. The Neural Network model is the overall best model since it outperforms the other systems in 5 scoring metrics out of 8 over both classes, while the Voting Classifier retains the best scores in F1 (U), Recall (U) and Precision (UN). We also note that the scores are consistently high for both classes, which is in large part helped by the SMOTE data augmentation technique.</p><p>Having balanced both classes in our experiments allows us to have a better baseline when measuring the impact of the additional data generated by ChatGPT. By comparing the scores of Tables <ref type="table" target="#tab_0">1</ref> and<ref type="table" target="#tab_1">2</ref>, we see that fixing the models and augmenting the data yields a 1.5% increase in scores overall. Particularly, this solidifies the claim that the data generated by the LLM aligns with the data expected for this challenge and can further aid in solving it. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model Macro-F1 (U) Precision Recall Accuracy Macro-F1 (NU) Precision</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this shared task, we evaluate the impact of generating LLM data to improve model performance. We explore the effects of this data generation by augmenting the existing code comment dataset and measuring the increase in the model classification scores. In the future, we plan to incorporate other data generation mechanisms such as ontology or knowledge graph integration into our LLM prompting technique to further our study of the impact of a refined data augmentation pipeline on classification performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Appendix</head><p>This section presents the ablation study done on the ChatGPT prompt to understand its inherent data generation mechanisms. Overall, we asked ChatGPT to provide 3000 additional lines of code-comment data split equally between Useful and Not Useful.</p><p>From the generated dataset, we've had to trim down rows for duplicates (i.e., the same code-comment pairs generated multiple times), incompleteness (i.e., the generation was missing parts of the code snippet or comment) and ambiguity (i.e., the code was not written in proper C language or the comment was not in proper English). These rules ensure the generated dataset mimics as faithfully as possible the original seed data with human supervision.</p><p>Additionally, we've attempted to retro-engineer ChatGPT to explain the process of its generated data. The following extract transcribes the code-comment generation mechanism adopted by the LLM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>User: can you explain how you came up with these examples?</head><p>ChatGPT: Certainly! The examples were generated programmatically using a combination of randomization and predefined patterns. Here's a breakdown of the process:</p><p>1. Randomization: To create random lines of C code, I utilized random number generators available in programming languages. For this task, I used the built-in random number generator provided by the programming environment. The generator is initialized with a seed (which may be the system time), and it generates random numbers within a specified range. This allowed me to create random integers for variables, loop limits, and other numerical values.</p><p>2. Predefined Patterns: I used predefined patterns to structure the C code snippets. These patterns include simple variable declarations, conditional statements, loops, arithmetic operations, function calls, and string operations. By combining these patterns with the randomly generated numbers, I formed meaningful C code lines.</p><p>3. Comments: For the comments, I used a mixture of random phrases and predefined descriptions. Half of the comments were matched with the corresponding lines of code to provide meaningful explanations of the code's purpose or functionality. The other half of the comments were randomly generated phrases, ensuring a mix of relevant and unrelated comments. It's important to note that these examples are randomly generated and may not necessarily represent valid or complete programs. The goal was to provide a mix of code snippets and comments to showcase various C language constructs and coding patterns, as well as to demonstrate the use of comments for code documentation. The examples are intended for illustrative purposes and should not be used in a production environment without proper validation and testing.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Example of Seed Data</figDesc><graphic coords="5,89.29,123.23,416.69,121.81" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Example of ChatGPT Gnenerated Data</figDesc><graphic coords="6,89.29,84.19,416.70,349.79" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>4 .</head><label>4</label><figDesc>Iteration: To generate 3000 examples, I repeated the randomization and predefined patterns generation process 3000 times. Each iteration produced a unique line of C code along with a comment, resulting in a diverse set of examples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1</head><label>1</label><figDesc>Model performance comparison on seed data</figDesc><table><row><cell>Recall Accuracy</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2</head><label>2</label><figDesc>Model performance comparison on seed + LLM data</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>https://chat.openai.com/share/6538a7f4-0a19-4e54-b5d8-d246dac3781a</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>https://scikit-learn.org/stable/index.h</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>https://huggingface.co/flax-sentence-embeddings/st-codesearch-distilroberta-base</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"><p>https://huggingface.co/sentence-transformers</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4"><p>https://huggingface.co/datasets/code_search_net</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5"><p>https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RepeatedStratifiedKFold.html</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Clean Code: A Handbook of Agile Software Craftsmanship</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Martin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
			<publisher>Prentice Hall PTR</publisher>
			<pubPlace>USA</pubPlace>
		</imprint>
	</monogr>
	<note>1 ed</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Analyzing code comments to boost program comprehension</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Shinyama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Arahori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Gondow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 25th Asia-Pacific Software Engineering Conference (APSEC)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="325" to="334" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Comment-mine-a semantic search approach to program comprehension from code comments</title>
		<author>
			<persName><forename type="first">S</forename><surname>Majumdar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Papdeja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">P</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Ghosh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advanced Computing and Systems for Security: Volume Twelve</title>
		<imprint>
			<biblScope unit="page" from="29" to="42" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m">Proceedings of the 14th Annual Meeting of the Forum for Information Retrieval Evaluation</title>
		<meeting>the 14th Annual Meeting of the Forum for Information Retrieval Evaluation<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note>FIRE &apos;22</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Can we predict useful comments in source codes?-analysis of findings from information retrieval in software engineering track@ fire 2022</title>
		<author>
			<persName><forename type="first">S</forename><surname>Majumdar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bandyopadhyay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">P</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Clough</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chattopadhyay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Majumder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th Annual Meeting of the Forum for Information Retrieval Evaluation</title>
		<meeting>the 14th Annual Meeting of the Forum for Information Retrieval Evaluation</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="15" to="17" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">K</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Senapati</surname></persName>
		</author>
		<title level="m">Information retrieval in software engineering utilizing a pre-trained bert model</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-Y</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-R</forename><surname>Wen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2303.18223</idno>
		<title level="m">A survey of large language models</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Chatgpt: Jack of all trades, master of none</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kocoń</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Cichecki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Kaszyca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kochanek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Szydło</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Baran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bielaniewicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gruza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Janz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kanclerz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Fusion</title>
		<imprint>
			<biblScope unit="page">101861</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.03762</idno>
		<title level="m">Attention is all you need</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Delangue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Moi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Cistac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Rault</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Louf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Funtowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Davison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shleifer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Von Platen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Plu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">L</forename><surname>Scao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gugger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Drame</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Lhoest</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.03771</idno>
		<title level="m">Huggingface&apos;s transformers: State-of-the-art natural language processing</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hayashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Neubig</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.13586</idno>
		<title level="m">Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">I</forename><surname>Muresanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Paster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Pitis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2211.01910</idno>
		<title level="m">Large language models are human-level prompt engineers</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><surname>Whitehouse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Choudhury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Aji</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.14288</idno>
		<title level="m">Llm-powered data augmentation for enhanced crosslingual performance</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2303.16756</idno>
		<title level="m">Large language models for healthcare data augmentation: An example on patient-trial matching</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Generative ai for software metadata: Overview of the information retrieval in software engineering track at fire 2023</title>
		<author>
			<persName><forename type="first">S</forename><surname>Majumdar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bandyopadhyay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Dave</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chattopadhyay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">P</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">D</forename><surname>Clough</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Majumder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Forum for Information Retrieval Evaluation</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Source code comment classification using logistic regression and support vector machine</title>
		<author>
			<persName><forename type="first">S</forename><surname>Paul</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Exploring transformer-based models for automatic useful code comments detection</title>
		<author>
			<persName><forename type="first">M</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chatterjee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><surname>Basu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2308.06144</idno>
		<title level="m">Identification of the relevance of comments in codes using bag of words and transformer based models</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">An effective lowdimensional software code representation using bert and elmo</title>
		<author>
			<persName><forename type="first">S</forename><surname>Majumdar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Varshney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">P</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">D</forename><surname>Clough</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chattopadhyay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 22nd International Conference on Software Quality, Reliability and Security (QRS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2022">2022. 2022</date>
			<biblScope unit="page" from="763" to="774" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Crawling wikipedia pages to train word embeddings model for software engineering domain</title>
		<author>
			<persName><forename type="first">S</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sharma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">14th Innovations in Software Engineering Conference (formerly known as India Software Engineering Conference)</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">An ontology for software development methodologies and endeavours</title>
		<author>
			<persName><forename type="first">C</forename><surname>Gonzalez-Perez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Henderson-Sellers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Ontologies for software engineering and software technology</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="123" to="151" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Towards an ontology for open source software development</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">L</forename><surname>Simmons</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Dillon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IFIP International Conference on Open Source Systems</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="65" to="75" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><surname>Husain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Gazit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Allamanis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Brockschmidt</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.09436</idno>
		<title level="m">Codesearchnet challenge: Evaluating the state of semantic code search</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">SMOTE: Synthetic minority over-sampling technique</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">V</forename><surname>Chawla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">W</forename><surname>Bowyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">O</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">P</forename><surname>Kegelmeyer</surname></persName>
		</author>
		<idno type="DOI">10.1613/jair.953</idno>
		<ptr target="https://doi.org/10.1613%2Fjair.953.doi:10.1613/jair.953" />
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="321" to="357" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
