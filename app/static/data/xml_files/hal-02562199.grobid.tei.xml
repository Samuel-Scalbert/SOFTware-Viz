<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Introducing the VoicePrivacy Initiative</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">N</forename><surname>Tomashenko</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">LIA</orgName>
								<orgName type="institution">University of Avignon</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">B</forename><forename type="middle">M L</forename><surname>Srivastava</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Inria</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">X</forename><surname>Wang</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">NII</orgName>
								<address>
									<settlement>Tokyo</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">E</forename><surname>Vincent</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">Université de Lorraine</orgName>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="institution" key="instit1">CNRS</orgName>
								<orgName type="institution" key="instit2">Inria</orgName>
								<orgName type="institution" key="instit3">LORIA</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">A</forename><surname>Nautsch</surname></persName>
							<affiliation key="aff5">
								<orgName type="institution">EURECOM</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">J</forename><surname>Yamagishi</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">NII</orgName>
								<address>
									<settlement>Tokyo</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
							<affiliation key="aff6">
								<orgName type="institution">University of Edinburgh</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">N</forename><surname>Evans</surname></persName>
							<affiliation key="aff5">
								<orgName type="institution">EURECOM</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">J</forename><surname>Patino</surname></persName>
							<affiliation key="aff5">
								<orgName type="institution">EURECOM</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">J.-F</forename><surname>Bonastre</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">LIA</orgName>
								<orgName type="institution">University of Avignon</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">P.-G</forename><surname>Noé</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">LIA</orgName>
								<orgName type="institution">University of Avignon</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">M</forename><surname>Todisco</surname></persName>
							<affiliation key="aff5">
								<orgName type="institution">EURECOM</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Introducing the VoicePrivacy Initiative</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">0064C9DCDE7397A40CBEBD99B4231B5A</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-04-12T14:48+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>privacy</term>
					<term>anonymization</term>
					<term>speech synthesis</term>
					<term>voice conversion</term>
					<term>speaker verification</term>
					<term>automatic speech recognition</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The VoicePrivacy initiative aims to promote the development of privacy preservation tools for speech technology by gathering a new community to define the tasks of interest and the evaluation methodology, and benchmarking solutions through a series of challenges. In this paper, we formulate the voice anonymization task selected for the VoicePrivacy 2020 Challenge and describe the datasets used for system development and evaluation. We also present the attack models and the associated objective and subjective evaluation metrics. We introduce two anonymization baselines and report objective evaluation results.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Recent years have seen mounting calls for the preservation of privacy when treating or storing personal data. This is not least the result of the European general data protection regulation (GDPR). While there is no legal definition of privacy <ref type="bibr" target="#b0">[1]</ref>, speech data encapsulates a wealth of personal information that can be revealed by listening or by automated systems <ref type="bibr" target="#b1">[2]</ref>. This includes, e.g., age, gender, ethnic origin, geographical background, health or emotional state, political orientations, and religious beliefs, among others <ref type="bibr">[3, p. 62</ref>]. In addition, speaker recognition systems can reveal the speaker's identity. It is thus of no surprise that efforts to develop privacy preservation solutions for speech technology are starting to emerge. The VoicePrivacy initiative aims to gather a new community to define the tasks of interest and the evaluation methodology, and to benchmark these solutions through a series of challenges.</p><p>Current methods fall into four categories: deletion, encryption, distributed learning, and anonymization. Deletion methods <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5]</ref> are meant for ambient sound analysis. They delete or obfuscate any overlapping speech to the point where no information about it can be recovered. Encryption methods <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7]</ref> such as fully homomorphic encryption <ref type="bibr" target="#b7">[8]</ref> and secure multiparty computation <ref type="bibr" target="#b8">[9]</ref>, support computation upon data in the encrypted domain. They incur significant increases in computational complexity, which require special hardware. Decentralized or federated learning methods aim to learn models from distributed data without accessing it directly <ref type="bibr" target="#b9">[10]</ref>. The derived data used for learning (e.g., model gradients) may still leak information about the original data, however <ref type="bibr" target="#b10">[11]</ref>.</p><p>Anonymization refers to the goal of suppressing personally identifiable attributes of the speech signal, leaving all other attributes intact<ref type="foot" target="#foot_0">1</ref> . Past and recent attempts have focused on noise addition <ref type="bibr" target="#b11">[12]</ref>, speech transformation <ref type="bibr" target="#b12">[13]</ref>, voice conversion <ref type="bibr" target="#b13">[14]</ref><ref type="bibr" target="#b14">[15]</ref><ref type="bibr" target="#b15">[16]</ref><ref type="bibr" target="#b16">[17]</ref>, speech synthesis <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19]</ref>, or adversarial learning <ref type="bibr" target="#b19">[20]</ref>. In contrast to the above categories of methods, anonymization appears to be more flexible since it can selectively suppress or retain certain attributes and it can easily be integrated within existing systems. Despite the appeal of anonymization and the urgency to address privacy concerns, a formal definition of anonymization and attacks against it is missing. Furthermore, the level of anonymization offered by existing solutions is unclear and not meaningful because there are no common datasets, protocols and metrics.</p><p>For these reasons, the VoicePrivacy 2020 Challenge focuses on the task of speech anonymization. This paper is intended as a general reference about the Challenge for researchers, engineers and privacy professionals. Details for participants are provided in the evaluation plan <ref type="bibr" target="#b20">[21]</ref> and on the challenge website <ref type="foot" target="#foot_1">2</ref> .</p><p>The paper is structured as follows. The anonymization task and the attack models, the datasets, and the metrics are described in Sections 2, 3, and 4, respectively. The two baseline systems and the corresponding objective evaluation results are presented in Section 5. We conclude in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Anonymization task and attack models</head><p>Privacy preservation is formulated as a game between users who publish some data and attackers who access this data or data derived from it and wish to infer information about the users <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b22">23]</ref>. To protect their privacy, the users publish data that contain as little personal information as possible while allowing one or more downstream goals to be achieved. To infer personal information, the attackers may use additional prior knowledge.</p><p>Focusing on speech data, a given privacy preservation scenario is specified by: (i) the nature of the data: waveform, features, etc., (ii) the information seen as personal: speaker identity, traits, spoken contents, etc., (iii) the downstream goal(s): human communication, automated processing, model training, etc., (iv) the data accessed by the attackers: one or more utterances, derived data or model, etc., (v) the attackers' prior knowledge: previously published data, privacy preservation method applied, etc. Different specifications lead to different privacy preservation methods from the users' point of view and different attacks from the attackers' point of view.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Privacy preservation scenario</head><p>VoicePrivacy 2020 considers the following scenario, where the terms "user" and "speaker" are used interchangeably. Speakers want to hide their identity while still allowing all other downstream goals to be achieved. Attackers have access to one or more utterances and want to identify the speakers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Anonymization task</head><p>To hide his/her identity, each speaker passes his/her utterances through an anonymization system. The resulting anonymized utterances are referred to as trial data. They sound as if they had been uttered by another speaker called pseudo-speaker, which may be an artificial voice not corresponding to any real speaker.</p><p>The task of challenge participants is to design this anonymization system. In order to allow all downstream goals to be achieved, this system should: (a) output a speech waveform, (b) hide speaker identity as much as possible, (c) distort other speech characteristics as little as possible, (d) ensure that all trial utterances from a given speaker appear to be uttered by the same pseudo-speaker, while trial utterances from different speakers appear to be uttered by different pseudo-speakers <ref type="foot" target="#foot_2">3</ref> .</p><p>Requirement (c) is assessed via utility metrics: automatic speech recognition (ASR) decoding error rate using a model trained on original, i.e., unprocessed data and subjective speech intelligibility and naturalness (see Section 4). Requirement (d) and additional downstream goals including ASR training will be assessed in a post-evaluation phase (see Section 6).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Attack models</head><p>The attackers have access to: (a) one or more anonymized trial utterances, (b) possibly, original or anonymized enrollment utterances for each speaker. They do not have access to the anonymization system applied by the user. The protection of personal information is assessed via privacy metrics, including objective speaker verifiability and subjective speaker verifiability and linkability. These metrics assume different attack models.</p><p>The objective speaker verifiability metrics assume that the attackers have access to a single anonymized trial utterance and several enrollment utterances. Two sets of metrics are used for original vs. anonymized enrollment data (see Section 4.1). In the latter case, we assume that the trial and enrollment utterances of a given speaker have been anonymized using the same system, but the corresponding pseudo-speakers are different.</p><p>The subjective speaker verifiability metric (Section 4.2) assumes that the attackers have access to a single anonymized trial utterance and a single original enrollment utterance. Finally, the subjective speaker linkability metric (Section 4.2) assumes that the attackers have access to several anonymized trial utterances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Datasets</head><p>Several publicly available corpora are used for the training, development and evaluation of speaker anonymization systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Training set</head><p>The training set comprises the 2,800 h VoxCeleb-1,2 speaker verification corpus <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b24">25]</ref> and 600 h subsets of the LibriSpeech <ref type="bibr" target="#b25">[26]</ref> and LibriTTS <ref type="bibr" target="#b26">[27]</ref> corpora, which were initially designed for ASR and speech synthesis, respectively. The selected subsets are detailed in Table <ref type="table" target="#tab_0">1</ref> (top).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Development set</head><p>The development set comprises LibriSpeech dev-clean and a subset of the VCTK corpus <ref type="bibr" target="#b27">[28]</ref> denoted as VCTK-dev (see Ta- </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Utility and privacy metrics</head><p>Following the attack models in Section 2.3, we consider objective and subjective privacy metrics to assess anonymization performance in terms of speaker verifiability and linkability. We also propose objective and subjective utility metrics to assess whether the requirements in Section 2.2 are fulfilled.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Objective metrics</head><p>For objective evaluation, we train two systems to assess speaker verifiability and ASR decoding error. The first system denoted ASVeval is an automatic speaker verification (ASV) system, which produces log-likelihood ratio (LLR) scores. The second system denoted ASReval is an ASR system which outputs a word error rate (WER). Both are trained on LibriSpeech trainclean-360 using Kaldi <ref type="bibr" target="#b28">[29]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1.">Objective speaker verifiability</head><p>The ASVeval system for speaker verifiability evaluation relies on x-vector speaker embeddings and probabilistic linear discriminant analysis (PLDA) <ref type="bibr" target="#b29">[30]</ref>. Three metrics are computed: the equal error rate (EER) and the LLR-based costs Cllr and C min llr . Denoting by Pfa(θ) and Pmiss(θ) the false alarm and miss rates at threshold θ, the EER corresponds to the threshold θEER at which the two detection error rates are equal, i.e., EER = Pfa(θEER) = Pmiss(θEER). Cllr is computed from PLDA scores as defined in <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b31">32]</ref>. It can be decomposed into a discrimination loss (C min llr ) and a calibration loss (Cllr -C min llr ). C min llr is estimated by optimal calibration using monotonic transformation of the scores to their empirical LLR values.</p><p>As shown in Fig. <ref type="figure" target="#fig_0">1</ref>, these metrics are computed and compared for: (1) original trial and enrollment data, (2) anomymized trial data and original enrollment data, (3) anomymized trial and enrollment data. The number of target and impostor trials is given in Table <ref type="table" target="#tab_1">2</ref>.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Test trials Enrollment</head><note type="other">.</note></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2.">ASR decoding error</head><p>ASReval is based on the state-of-the-art Kaldi recipe for Lib-riSpeech involving a factorized time delay neural network (TDNN-F) acoustic model (AM) <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b33">34]</ref> and a trigram language model. As shown in Fig. <ref type="figure" target="#fig_1">2</ref>, the (1) original and (2) anonymized trial data is decoded using the provided pretrained ASReval model and the corresponding WERs are calculated. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Subjective metrics</head><p>Subjective metrics include speaker verifiability, speaker linkability, speech intelligibility, and speech naturalness. They will be evaluated using listening tests carried out by the organizers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1.">Subjective speaker verifiability</head><p>To evaluate subjective speaker verifiability, listeners are given pairs of one anonymized trial utterance and one distinct original enrollment utterance of the same speaker. Following <ref type="bibr" target="#b34">[35]</ref>, they are instructed to imagine a scenario in which the anonymized sample is from an incoming telephone call, and to rate the similarity between the voice and the original voice using a scale of 1 to 10, where 1 denotes 'different speakers' and 10 denotes 'the same speaker' with highest confidence. The performance of each anonymization system will be visualized through detection error tradeoff (DET) curves.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2.">Subjective speaker linkability</head><p>The second subjective metric assesses speaker linkability, i.e., the ability to cluster several utterances into speakers. Listeners are asked to place a set of anonymized trial utterances from different speakers in a 1-or 2-dimensional space according to speaker similarity. This relies on a graphical interface, where each utterance is represented as a point in space and the distance between two points expresses subjective speaker dissimilarity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3.">Subjective speech intelligibility</head><p>Listeners are also asked to rate the intelligibility of individual samples (anonymized trial utterances or original enrollment utterances) on a scale from 1 (totally unintelligible) to 10 (totally intelligible). The results can be visualized through DET curves.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.4.">Subjective speech naturalness</head><p>Finally, the naturalness of the anonymized speech will be evaluated on a scale from 1 (totally unnatural) to 10 (totally natural).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Baseline software and results</head><p>Two anonymization baselines are provided. 4 We briefly introduce them and report the corresponding objective results below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Anonymization baselines</head><p>The primary baseline shown in Fig. <ref type="figure" target="#fig_2">3</ref> is inspired from <ref type="bibr" target="#b17">[18]</ref> and comprises three steps: (1) extraction of x-vector <ref type="bibr" target="#b29">[30]</ref>, pitch (F0) and bottleneck (BN) features; (2) x-vector anonymization; (3) speech synthesis (SS) from the anonymized x-vector and the original F0+BN features. In Step 1, 256-dimensional BN features encoding spoken content are extracted using a TDNN-F ASR AM trained on LibriSpeech train-clean-100 and trainother-500 using Kaldi. A 512-dimensional x-vector encoding the speaker is extracted using a TDNN trained on VoxCeleb-1,2 with Kaldi. In Step 2, for every source x-vector, an  anonymized x-vector is computed by finding the N farthest xvectors in an external pool (LibriTTS train-other-500) according to the PLDA distance, and by averaging N * randomly selected vectors among them <ref type="foot" target="#foot_3">5</ref> . In Step 3, an SS AM generates Mel-filterbank features given the anonymized x-vector and the F0+BN features, and a neural source-filter (NSF) waveform model <ref type="bibr" target="#b35">[36]</ref> outputs a speech signal given the anonymized xvector, the F0, and the generated Mel-filterbank features. The SS AM and NSF models are both trained on LibriTTS trainclean-100. See <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b36">37]</ref> for further details. The secondary baseline is a simpler, formant-shifting approach provided as additional inspiration <ref type="bibr" target="#b37">[38]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Objective evaluation results</head><p>Table <ref type="table" target="#tab_2">3</ref> reports the values of objective speaker verifiability metrics obtained before/after anonymization with the primary baseline. <ref type="foot" target="#foot_4">6</ref> The EER and C min llr metrics behave similarly, while interpretation of Cllr is more challenging due to non-calibration <ref type="foot" target="#foot_5">7</ref> . We hence focus on the EER below. On all datasets, anonymization of the trial data greatly increases the EER. This shows that the anonymization baseline effectively increases the users' privacy. The EER estimated with original enrollment data (47 to 58%), which is comparable to or above the chance value (50%), suggests that full anonymization has been achieved. However, anonymized enrollment data result in a much lower EER (26 to 37%), which suggests that F0+BN features retain some information about the original speaker. If the attackers have access to such enrollment data, they will be able to re-identify users almost half of the time. Note also that the EER is larger for females than males on average. This further demonstrates that failing to define the attack model or assuming a naive attack model leads to a greatly overestimated sense of privacy <ref type="bibr" target="#b22">[23]</ref>.</p><p>Table <ref type="table" target="#tab_3">4</ref> reports the WER achieved before/after anonymization with the primary baseline. While the absolute WER stays below 7% on LibriSpeech and 16% on VCTK, anonymization incurs a large WER increase of 19 to 67% relative.</p><p>The results achieved by the secondary baseline are inferior and detailed in <ref type="bibr" target="#b20">[21]</ref>. Overall, there is substantial potential for challenge participants to improve over the two baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusions</head><p>The VoicePrivacy initiative aims to promote the development of private-by-design speech technology. Our initial event, the VoicePrivacy 2020 Challenge, provides a complete evaluation protocol for voice anonymization systems. We formulated the voice anonymization task as a game between users and attackers, and highlighted three possible attack models. We also designed suitable datasets and evaluation metrics, and we released two open-source baseline voice anonymization systems. Future work includes evaluating and comparing the participants' systems using objective and subjective metrics, computing alternative objective metrics relating to, e.g., requirement (d) in Section 2.2, and drawing initial conclusions regarding the best anonymization strategies for a given attack model. A revised, stronger evaluation protocol is also expected as an outcome.</p><p>In this regard, it is essential to realize that the users' downstream goals and the attack models listed above are not exhaustive. For instance, beyond ASR decoding, anonymization is extremely useful in the context of anonymized data collection for ASR training <ref type="bibr" target="#b19">[20]</ref>. It is also known that the EER becomes lower when the attackers generate anonymized training data and retrains ASVeval on this data <ref type="bibr" target="#b22">[23]</ref>. In order to assess these aspects, we will ask volunteer participants to share additional data with us and run additional experiments in a post-evaluation phase.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Acknowledgment</head><p>VoicePrivacy was born at the crossroads of projects VoicePersonae, COMPRISE (https://www.compriseh2020. eu/), and DEEP-PRIVACY. Project HARPOCRATES was designed specifically to support it. The authors acknowledge support by ANR, JST, and the European Union's Horizon 2020 Research and Innovation Program, and they would like to thank Md Sahidullah and Fuming Fang. Experiments presented in this paper were partially carried out using the Grid'5000 testbed, supported by a scientific interest group hosted by Inria and including CNRS, RENATER and several Universities as well as other organizations (see https://www.grid5000.fr).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: ASV evaluation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: ASR decoding evaluation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>4Figure 3 :</head><label>3</label><figDesc>Figure 3: Primary baseline anonymization system.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Number of speakers and utterances in the VoicePrivacy 2020 training, development, and evaluation sets.</figDesc><table><row><cell cols="2">Subset</cell><cell cols="3">Female Male Total</cell><cell>#Utter.</cell></row><row><cell></cell><cell>VoxCeleb-1,2</cell><cell cols="4">2,912 4,451 7,363 1,281,762</cell></row><row><cell>Training</cell><cell>LibriSpeech train-clean-100 LibriSpeech train-other-500 LibriTTS train-clean-100</cell><cell cols="4">125 126 251 564 602 1,166 148,688 28,539 123 124 247 33,236</cell></row><row><cell></cell><cell>LibriTTS train-other-500</cell><cell cols="4">560 600 1,160 205,044</cell></row><row><cell>Development</cell><cell>LibriSpeech Enrollment dev-clean Trial Enrollment VCTK-dev Trial (common) Trial (different)</cell><cell>15 20 15</cell><cell>14 20 15</cell><cell>29 40 30</cell><cell>343 1,978 600 695 10,677</cell></row><row><cell>Evaluation</cell><cell>LibriSpeech Enrollment test-clean Trial Enrollment VCTK-test Trial (common) Trial (different)</cell><cell>16 20 15</cell><cell>13 20 15</cell><cell>29 40 30</cell><cell>438 1,496 600 70 10,748</cell></row></table><note><p><p><p><p><p>ble 1, middle). With the above attack models in mind, we split them into trial and enrollment subsets. For LibriSpeech devclean, the speakers in the enrollment set are a subset of those in the trial set. For VCTK-dev, we use the same speakers for enrollment and trial and we consider two trial subsets, denoted as common and different. The common trial subset is composed of utterances #1 -24 in the VCTK corpus that are identical for all speakers. This is meant for subjective evaluation of speaker verifiability/linkability in a text-dependent manner. The enrollment and different trial subsets are composed of distinct utterances for all speakers.</p>3.3. Evaluation set</p>Similarly, the evaluation set comprises LibriSpeech test-clean and a subset of VCTK called VCTK-test (see Table</p>1</p>, bottom).</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Number of speaker verification trials.</figDesc><table><row><cell cols="2">Subset</cell><cell>Trials</cell><cell>Female</cell><cell>Male</cell><cell>Total</cell></row><row><cell>Development</cell><cell cols="5">LibriSpeech Target dev-clean Impostor VCTK-dev Target (common) Target (different) Impostor (common) Impostor (different) 13,219 12,985 26,204 704 644 1,348 14,566 12,796 27,362 344 351 695 1,781 2,015 3,796 4,810 4,911 9,721</cell></row><row><cell></cell><cell cols="2">LibriSpeech Target</cell><cell>548</cell><cell>449</cell><cell>997</cell></row><row><cell>Evaluation</cell><cell>test-clean VCTK-test</cell><cell>Impostor Target (common) Target (different) Impostor (common)</cell><cell cols="3">11,196 9,457 20,653 346 354 700 1,944 1,742 3,686 4,838 4,952 9,790</cell></row><row><cell></cell><cell></cell><cell cols="4">Impostor (different) 13,056 13,258 26,314</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Speaker verifiability achieved by the pretrained ASVeval model. The primary baseline is used for anonymization.</figDesc><table><row><cell>Dataset</cell><cell>Gender</cell><cell cols="2">Anonymization Enroll Trial</cell><cell cols="2">Development EER (%) C min llr</cell><cell>C llr</cell><cell>EER (%)</cell><cell>Test C min llr</cell><cell>C llr</cell></row><row><cell>LibriSpeech</cell><cell>Female Male</cell><cell>original anonymized original anonymized</cell><cell>original anonymized original anonymized</cell><cell>8.67 50.14 36.79 1.24 57.76 34.16</cell><cell>0.304 0.996 0.894 0.034 0.999 0.867</cell><cell>42.86 144.11 16.35 14.25 168.99 24.72</cell><cell>7.66 47.26 32.12 1.11 52.12 36.75</cell><cell>0.183 0.995 0.839 0.041 0.999 0.903</cell><cell>26.79 151.82 16.27 15.30 166.66 33.93</cell></row><row><cell>VCTK (different)</cell><cell>Female Male</cell><cell>original anonymized original anonymized</cell><cell>original anonymized original anonymized</cell><cell>2.86 49.97 26.11 1.44 53.95 30.92</cell><cell>0.100 0.989 0.760 0.052 1.000 0.839</cell><cell>1.13 166.03 8.41 1.16 167.51 23.80</cell><cell>4.89 48.05 31.74 2.07 53.85 30.94</cell><cell>0.169 0.998 0.847 0.072 1.000 0.834</cell><cell>1.50 146.93 11.53 1.82 167.82 23.84</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>ASR decoding error achieved by the pretrained ASReval model. The primary baseline is used.</figDesc><table><row><cell>Dataset</cell><cell cols="3">Anonymization Dev. WER (%) Test WER (%)</cell></row><row><cell>LibriSpeech</cell><cell>original anonymized</cell><cell>3.83 6.39</cell><cell>4.15 6.73</cell></row><row><cell>VCTK</cell><cell>original</cell><cell>10.79</cell><cell>12.82</cell></row><row><cell>(comm.+diff.)</cell><cell>anonymized</cell><cell>15.38</cell><cell>15.23</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>In the legal community, the term "anonymization" means that this goal has been achieved. Here, it refers to the task to be addressed, even when the method being evaluated has failed. We expect the VoicePrivacy initiative to lead to the definition of new, unambiguous terms.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>https://www.voiceprivacychallenge.org/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>This is akin to "pseudonymization", which replaces each user's identifiers by a unique key. We do not use this term here, since it often refers to the distinct case when the identifiers are tabular data and the data controller stores the correspondence table linking users and keys.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_3"><p>In the baseline, we use N = 200 and N * = 100.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_4"><p>Results on VCTK (common) are omitted due to space constraints.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_5"><p>In particular, C llr &gt; 1 is not a problem, since we care more about discrimination metrics than score calibration metrics in the first edition.</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">The GDPR &amp; speech data: Reflections of legal and technology communities, first steps towards a common understanding</title>
		<author>
			<persName><forename type="first">A</forename><surname>Nautsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Jasserand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Kindt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Todisco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Trancoso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Evans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Interspeech</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3695" to="3699" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Preserving privacy in speaker and speech characterisation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Nautsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jimenez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Treiber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kolberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Jasserand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Kindt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Delgado</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Speech and Language</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="page" from="441" to="480" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName><surname>Comprise</surname></persName>
		</author>
		<ptr target="https://www.compriseh2020.eu/files/2019/06/D5.1.pdf" />
		<title level="m">Deliverable Nº5.1: Data protection and GDPR requirements</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Voice anonymization in urban sound recordings</title>
		<author>
			<persName><forename type="first">A</forename><surname>Cohen-Hadria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cartwright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mcfee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Bello</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE International Workshop on Machine Learning for Signal Processing (MLSP)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Privacy aware acoustic scene synthesis using deep spectral feature inversion</title>
		<author>
			<persName><forename type="first">F</forename><surname>Gontier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lagrange</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lavandier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-F</forename><surname>Petiot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="886" to="890" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Privacypreserving speech processing: cryptographic and string-matching frameworks show promise</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Raj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">D</forename><surname>Rane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Smaragdis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="62" to="74" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A framework for secure speech recognition</title>
		<author>
			<persName><forename type="first">P</forename><surname>Smaragdis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shashanka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1404" to="1413" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Encrypted speech recognition using deep polynomial networks</title>
		<author>
			<persName><forename type="first">S.-X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="5691" to="5695" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">VoiceGuard: Secure and private speech processing</title>
		<author>
			<persName><forename type="first">F</forename><surname>Brasser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Frassetto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Riedhammer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A.-R</forename><surname>Sadeghi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Weinert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Interspeech</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1303" to="1307" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Federated learning for keyword spotting</title>
		<author>
			<persName><forename type="first">D</forename><surname>Leroy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Coucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lavril</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Gisselbrecht</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dureau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6341" to="6345" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Geiping</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bauermeister</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Dröge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Moeller</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.14053</idno>
		<title level="m">Inverting gradients -how easy is it to break privacy in federated learning?</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Privacy-preserving sound to degrade automatic speaker verification performance</title>
		<author>
			<persName><forename type="first">K</forename><surname>Hashimoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yamagishi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Echizen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE International Conference Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="5500" to="5504" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Voicemask: Anonymize and sanitize voice input on mobile devices</title>
		<author>
			<persName><forename type="first">J</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X.-Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Deng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.11460</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Speaker deidentification via voice transformation</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Toth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Schultz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">W</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ASRU</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Online speaker de-identification using voice transformation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Pobar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Ipšić</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">37th International Convention on Information and Communication Technology, Electronics and Microelectronics</title>
		<meeting><address><addrLine>MIPRO</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1264" to="1267" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Convolutional neural network based speaker de-identification</title>
		<author>
			<persName><forename type="first">F</forename><surname>Bahmaninezhad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Hansen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Odyssey</title>
		<imprint>
			<biblScope unit="page" from="255" to="260" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Reversible speaker de-identification using pre-trained transformation functions</title>
		<author>
			<persName><forename type="first">C</forename><surname>Magariños</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lopez-Otero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Docio-Fernandez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Rodriguez-Banga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Erro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Speech &amp; Language</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="page" from="36" to="52" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Speaker anonymization using x-vector and neural waveform models</title>
		<author>
			<persName><forename type="first">F</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yamagishi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Echizen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Todisco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-F</forename><surname>Bonastre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Speech Synthesis Workshop</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="155" to="160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Voiceindistinguishability: Protecting voiceprint in privacy-preserving speech data release</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yoshikawa</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.07442</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Privacy-preserving adversarial representation learning in ASR: Reality or illusion?</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">M L</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bellet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tommasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Vincent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Interspeech</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3700" to="3704" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<author>
			<persName><forename type="first">N</forename><surname>Tomashenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">M L</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Nautsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yamagishi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Evans</surname></persName>
		</author>
		<ptr target="https://www.voiceprivacychallenge.org/docs/VoicePrivacy_2020_Eval_Plan_v1_3.pdf" />
		<title level="m">The VoicePrivacy 2020 Challenge evaluation plan</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Towards privacy-preserving speech data publishing</title>
		<author>
			<persName><forename type="first">J</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X.-Y</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE Conference on Computer Communications (INFOCOM)</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1079" to="1087" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Evaluating voice conversion-based privacy protection against informed attackers</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">M L</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Vauquier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sahidullah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bellet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tommasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Vincent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="2802" to="2806" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">VoxCeleb: a largescale speaker identification dataset</title>
		<author>
			<persName><forename type="first">A</forename><surname>Nagrani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Interspeech</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2616" to="2620" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">VoxCeleb2: Deep speaker recognition</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Nagrani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Interspeech</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1086" to="1090" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Lib-riSpeech: an ASR corpus based on public domain audio books</title>
		<author>
			<persName><forename type="first">V</forename><surname>Panayotov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="5206" to="5210" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">LibriTTS: A corpus derived from LibriSpeech for text-to-speech</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Dang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Interspeech</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1526" to="1530" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">CSTR VCTK corpus: English multi-speaker corpus for CSTR voice cloning toolkit (version 0.92)</title>
		<author>
			<persName><forename type="first">C</forename><surname>Veaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yamagishi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Macdonald</surname></persName>
		</author>
		<ptr target="https://datashare.is.ed.ac.uk/handle/10283/3443" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">The Kaldi speech recognition toolkit</title>
		<author>
			<persName><forename type="first">D</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ghoshal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Boulianne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Burget</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Glembek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Goel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">X-vectors: Robust DNN embeddings for speaker recognition</title>
		<author>
			<persName><forename type="first">D</forename><surname>Snyder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Garcia-Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="5329" to="5333" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Application-independent evaluation of speaker detection</title>
		<author>
			<persName><forename type="first">N</forename><surname>Brümmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Du</forename><surname>Preez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Speech and Language</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">2-3</biblScope>
			<biblScope unit="page" from="230" to="275" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Cross-entropy analysis of the information in forensic speaker recognition</title>
		<author>
			<persName><forename type="first">D</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gonzalez-Rodriguez</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
	<note>in Odyssey</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Semi-orthogonal low-rank matrix factorization for deep neural networks</title>
		<author>
			<persName><forename type="first">D</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yarmohammadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Interspeech</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="3743" to="3747" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A time delay neural network architecture for efficient modeling of long temporal contexts</title>
		<author>
			<persName><forename type="first">V</forename><surname>Peddinti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Interspeech</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="3214" to="3218" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">The Voice Conversion Challenge 2018: Promoting development of parallel and nonparallel methods</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lorenzo-Trueba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yamagishi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Toda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Villavicencio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kinnunen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Odyssey</title>
		<imprint>
			<biblScope unit="page" from="195" to="202" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Neural harmonic-plus-noise waveform model with trainable maximum voice frequency for text-tospeech synthesis</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yamagishi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Speech Synthesis Workshop</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Design choices for x-vector based speaker anonymization</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">M L</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Tomashenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yamagishi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Maouche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bellet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tommasi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note>in Interspeech</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Speaker anonymisation using the McAdams coefficient</title>
		<author>
			<persName><forename type="first">J</forename><surname>Patino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Todisco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Nautsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Evans</surname></persName>
		</author>
		<ptr target="http://www.eurecom.fr/publication/6190" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">Tech. Rep. EURECOM+6190</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
