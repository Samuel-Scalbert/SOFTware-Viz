<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Asteroid: the PyTorch-based audio source separation toolkit for researchers</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Manuel</forename><surname>Pariente</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Université de Lorraine</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
								<orgName type="institution" key="instit3">Inria</orgName>
								<orgName type="institution" key="instit4">LORIA</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Samuele</forename><surname>Cornell</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Università Politecnica delle Marche</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Joris</forename><surname>Cosentino</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Université de Lorraine</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
								<orgName type="institution" key="instit3">Inria</orgName>
								<orgName type="institution" key="instit4">LORIA</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Sunit</forename><surname>Sivasankaran</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Université de Lorraine</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
								<orgName type="institution" key="instit3">Inria</orgName>
								<orgName type="institution" key="instit4">LORIA</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Efthymios</forename><surname>Tzinis</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jens</forename><surname>Heitkaemper</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">Universität Paderborn</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Michel</forename><surname>Olvera</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Université de Lorraine</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
								<orgName type="institution" key="instit3">Inria</orgName>
								<orgName type="institution" key="instit4">LORIA</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Fabian-Robert</forename><surname>Stöter</surname></persName>
							<affiliation key="aff4">
								<orgName type="laboratory">Inria and LIRMM</orgName>
								<orgName type="institution">University of Montpellier</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Mathieu</forename><surname>Hu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Université de Lorraine</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
								<orgName type="institution" key="instit3">Inria</orgName>
								<orgName type="institution" key="instit4">LORIA</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Juan</forename><forename type="middle">M</forename><surname>Martín-Doñas</surname></persName>
							<affiliation key="aff5">
								<orgName type="institution">Universidad de Granada</orgName>
								<address>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">David</forename><surname>Ditter</surname></persName>
							<affiliation key="aff6">
								<orgName type="institution">Universität Hamburg</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ariel</forename><surname>Frank</surname></persName>
							<affiliation key="aff7">
								<orgName type="institution">Technion -Israel Institute of Technology</orgName>
								<address>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Antoine</forename><surname>Deleforge</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Université de Lorraine</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
								<orgName type="institution" key="instit3">Inria</orgName>
								<orgName type="institution" key="instit4">LORIA</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Emmanuel</forename><surname>Vincent</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Université de Lorraine</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
								<orgName type="institution" key="instit3">Inria</orgName>
								<orgName type="institution" key="instit4">LORIA</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Asteroid: the PyTorch-based audio source separation toolkit for researchers</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">C1BADF903ACF07EE29092D75E8874388</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-04-12T14:44+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>source separation</term>
					<term>speech enhancement</term>
					<term>opensource software</term>
					<term>end-to-end</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes Asteroid, the PyTorch-based audio source separation toolkit for researchers. Inspired by the most successful neural source separation systems, it provides all neural building blocks required to build such a system. To improve reproducibility, Kaldi-style recipes on common audio source separation datasets are also provided. This paper describes the software architecture of Asteroid and its most important features. By showing experimental results obtained with Asteroid's recipes, we show that our implementations are at least on par with most results reported in reference papers. The toolkit is publicly available at github.com/mpariente/asteroid.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Audio source separation, which aims to separate a mixture signal into individual source signals, is essential to robust speech processing in real-world acoustic environments <ref type="bibr">[1]</ref>. Classical open-source toolkits such as FASST <ref type="bibr">[2]</ref>, HARK <ref type="bibr" target="#b0">[3]</ref>, ManyEars <ref type="bibr" target="#b1">[4]</ref> and openBliSSART <ref type="bibr" target="#b2">[5]</ref> which are based on probabilistic modelling, non-negative matrix factorization, sound source localization and/or beamforming have been successful in the past decade. However, they are now largely outperformed by deep learning-based approaches, at least on the task of single-channel source separation <ref type="bibr" target="#b3">[6]</ref><ref type="bibr" target="#b4">[7]</ref><ref type="bibr" target="#b5">[8]</ref><ref type="bibr" target="#b6">[9]</ref><ref type="bibr" target="#b7">[10]</ref>.</p><p>Several open-source toolkits have emerged for deep learning-based source separation. These include nussl (Northwestern University Source Separation Library) <ref type="bibr" target="#b8">[11]</ref>, ONSSEN (An Open-source Speech Separation and Enhancement Library) <ref type="bibr" target="#b9">[12]</ref>, Open-Unmix <ref type="bibr" target="#b10">[13]</ref>, and countless isolated implementations replicating some important papers 1 .</p><p>We would like to thank Hervé Bredin for fruitful discussions about software design and Kaituo Xu for open-sourcing his Conv-Tasnet implementation.</p><p>Experiments presented in this paper were partially carried out using the Grid'5000 testbed, supported by a scientific interest group hosted by Inria and including CNRS, RENATER and several Universities as well as other organizations (see https://www.grid5000.fr).</p><p>High Performance Computing resources were partially provided by the EXPLOR centre hosted by the University de Lorraine 1 kaituoxu/TasNet, kaituoxu/Conv-TasNet, yluo42/TAC, JusperLee/Conv-TasNet, JusperLee/Dual-Path-RNN-Pytorch, tky1117/DNN-based source separation ShiZiqiang/dual-path-RNNs-DPRNNs-based-speech-separation Both nussl and ONSSEN are written in PyTorch <ref type="bibr" target="#b11">[14]</ref> and provide training and evaluation scripts for several state-of-the art methods. However, data preparation steps are not provided and experiments are not easily configurable from the command line. Open-Unmix does provide a complete pipeline from data preparation until evaluation, but only for the Open-Unmix model on the music source separation task. Regarding the isolated implementations, some of them only contain the model, while others provide training scripts but assume that training data has been generated. Finally, very few provide the complete pipeline. Among the ones providing evaluation scripts, differences can often be found, e.g., discarding short utterances or splitting utterances in chunks and discarding the last one. This paper describes Asteroid (Audio source separation on Steroids), a new open-source toolkit for deep learning-based audio source separation and speech enhancement, designed for researchers and practitioners. Based on PyTorch, one of the most widely used dynamic neural network toolkits, Asteroid is meant to be user-friendly, easily extensible, to promote reproducible research, and to enable easy experimentation. As such, it supports a wide range of datasets and architectures, and comes with recipes reproducing some important papers. Asteroid is built on the following principles:</p><p>1. Abstract only where necessary, i.e., use as much native</p><p>PyTorch code as possible. 2. Allow importing third-party code with minimal changes.</p><p>3. Provide all steps from data preparation to evaluation. 4. Enable recipes to be configurable from the command line.</p><p>We present the audio source separation framework in Section 2. We describe Asteroid's main features in Section 3 and their implementation in Section 4. We provide example experimental results in Section 5 and conclude in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">General framework</head><p>While Asteroid is not limited to a single task, single-channel source separation is currently its main focus. Hence, we will only consider this task in the rest of the paper. Let x be a single channel recording of J sources in noise:</p><formula xml:id="formula_0">x(t) = J j=1 sj(t) + n(t),<label>(1)</label></formula><p>where {sj}j=1..J are the source signals and n is an additive noise signal. The goal of source separation is to obtain source estimates { sj}j=1..J given x.</p><p>Most state-of-the-art neural source separation systems follow the encoder-masker-decoder approach depicted in Fig. 1 <ref type="bibr" target="#b5">[8,</ref><ref type="bibr" target="#b6">9,</ref><ref type="bibr" target="#b12">15,</ref><ref type="bibr" target="#b13">16]</ref>. The encoder computes a short-time Fourier transform (STFT)-like representation X by convolving the timedomain signal x with an analysis filterbank. The representation X is fed to the masker network that estimates a mask for each source. The masks are then multiplied entrywise with X to obtain sources estimates { Sj}j=1..J in the STFT-like domain. The time-domain source estimates { sj}j=1..J are finally obtained by applying transposed convolutions to { Sj}j=1..J with a synthesis filterbank. The three networks are jointly trained using a loss function computed on the masks or their embeddings <ref type="bibr" target="#b3">[6,</ref><ref type="bibr" target="#b14">17,</ref><ref type="bibr" target="#b15">18]</ref>, on the STFT-like domain estimates <ref type="bibr" target="#b4">[7,</ref><ref type="bibr" target="#b12">15,</ref><ref type="bibr" target="#b16">19]</ref>, or directly on the time-domain estimates <ref type="bibr" target="#b13">16,</ref><ref type="bibr" target="#b17">20]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mixture waveform Separated waveforms</head><p>Encoder Decoder STFT-like rep. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Masked rep. Masker</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Functionality</head><p>Asteroid follows the encoder-masker-decoder approach, and provides various choices of filterbanks, masker networks, and loss functions. It also provides training and evaluation tools and recipes for several datasets. We detail each of these below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Analysis and synthesis filterbanks</head><p>As shown in <ref type="bibr" target="#b17">[20]</ref><ref type="bibr" target="#b18">[21]</ref><ref type="bibr" target="#b19">[22]</ref><ref type="bibr" target="#b20">[23]</ref>, various filterbanks can be used to train end-to-end source separation systems. A natural abstraction is to separate the filterbank object from the encoder and decoder objects. This is what we do in Asteroid. All filterbanks inherit from the Filterbank class. Each Filterbank can be combined with an Encoder or a Decoder, which respectively follow the nn.Conv1d and nn.ConvTranspose1d interfaces from PyTorch for consistency and ease of use. Notably, the STFTFB filterbank computes the STFT using simple convolutions, and the default filterbank matrix is orthogonal.</p><p>Asteroid supports free filters <ref type="bibr" target="#b5">[8,</ref><ref type="bibr" target="#b6">9]</ref>, discrete Fourier transform (DFT) filters <ref type="bibr" target="#b16">[19,</ref><ref type="bibr" target="#b18">21]</ref>, analytic free filters <ref type="bibr" target="#b19">[22]</ref>, improved parameterized sinc filters <ref type="bibr" target="#b19">[22,</ref><ref type="bibr" target="#b21">24]</ref> and the multi-phase Gammatone filterbank <ref type="bibr" target="#b20">[23]</ref>. Automatic pseudo-inverse computation and dynamic filters (computed at runtime) are also supported. Because some of the filterbanks are complex-valued, we provide functions to compute magnitude and phase, and apply magnitude or complex-valued masks. We also provide interfaces to NumPy <ref type="bibr" target="#b22">[25]</ref> and torchaudio 2 . Additionally, Griffin-2 github.com/pytorch/audio Lim <ref type="bibr" target="#b23">[26,</ref><ref type="bibr" target="#b24">27]</ref> and multi-input spectrogram inversion (MISI) <ref type="bibr" target="#b25">[28]</ref> algorithms are provided.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Masker network</head><p>Asteroid provides implementations of widely used masker networks: TasNet's stacked long short-term memory (LSTM) network <ref type="bibr" target="#b5">[8]</ref>, Conv-Tasnet's temporal convolutional network (with or without skip connections) <ref type="bibr" target="#b6">[9]</ref>, and the dual-path recurrent neural network (DPRNN) in <ref type="bibr" target="#b13">[16]</ref>. Open-Unmix <ref type="bibr" target="#b10">[13]</ref> is also supported for music source separation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Loss functions -Permutation invariance</head><p>Asteroid supports several loss functions: mean squared error, scale-invariant signal-to-distortion ratio (SI-SDR) <ref type="bibr" target="#b6">[9,</ref><ref type="bibr" target="#b26">29]</ref>, scale-dependent SDR <ref type="bibr" target="#b26">[29]</ref>, signal-to-noise ratio (SNR), perceptual evaluation of speech quality (PESQ) <ref type="bibr" target="#b27">[30]</ref>, and affinity loss for deep clustering <ref type="bibr" target="#b3">[6]</ref>.</p><p>Whenever the sources are of the same nature, a permutation-invariant (PIT) loss shall be used <ref type="bibr" target="#b4">[7,</ref><ref type="bibr" target="#b28">31]</ref>. Asteroid provides an optimized, versatile implementation of PIT losses. Let s = [sj(t)] t=0...T j=1...J and s = [ sj(t)] t=0...T j=1...J be the matrices of true and estimated source signals, respectively. We denote as sσ = [ s σ(j) (t)] t=0...T j=1...J a permutation of s by σ ∈ SJ , where SJ is the set of permutations of [1, ..., J]. A PIT loss LPIT is defined as</p><formula xml:id="formula_1">LPIT(θ) = min σ∈S J L( sσ, s),<label>(2)</label></formula><p>where L is a classical (permutation-dependent) loss function, which depends on the network's parameters θ through sσ.</p><p>We assume that, for a given permutation hypothesis σ, the loss L( sσ, s) can be written as L( sσ, s) = G F( s σ(1) , s1), ..., F( s σ(J) , sJ )</p><p>where sj = [sj(0), . . . , sj(T )], sj = [ sj(0), . . . , sj(T )], F computes the pairwise loss between a single true source and its hypothesized estimate, and G is the reduce function, usually a simple mean operation. Denoting by F the J × J pairwise loss matrix with entries F( si, sj), we can rewrite (2) as</p><formula xml:id="formula_3">LPIT(θ) = min σ∈S J G F σ(1)1 , ..., F σ(J)J<label>(4)</label></formula><p>and reduce the computational complexity from J! to J 2 by precomputing F's terms. Taking advantage of this, Asteroid provides PITLossWrapper, a simple yet powerful class that can efficiently turn any pairwise loss F or permutation-dependent loss L into a PIT loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Datasets</head><p>Asteroid provides baseline recipes for the following datasets: wsj0-2mix and wsj0-3mix <ref type="bibr" target="#b3">[6]</ref>, WHAM <ref type="bibr" target="#b29">[32]</ref>, WHAMR <ref type="bibr" target="#b30">[33]</ref>, LibriMix <ref type="bibr" target="#b31">[34]</ref> FUSS <ref type="bibr" target="#b32">[35]</ref>, Microsoft's Deep Noise Suppression challenge dataset (DNS) <ref type="bibr" target="#b33">[36]</ref>, SMS-WSJ <ref type="bibr" target="#b34">[37]</ref>, Kinect-WSJ <ref type="bibr" target="#b35">[38]</ref>, and MUSDB18 <ref type="bibr" target="#b36">[39]</ref>. Their characteristics are summarized and compared in </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Training</head><p>For training source separation systems, Asteroid offers a thin wrapper around PyTorch-Lightning <ref type="bibr" target="#b37">[40]</ref> that seamlessly enables distributed training, experiment logging and more, without sacrificing flexibility. Regarding the optimizers, we also rely on native PyTorch and torch-optimizer 3 .</p><p>PyTorch provides basic optimizers such as SGD and Adam and torch-optimizer provides state-of-the art optimizers such as RAdam, Ranger or Yogi.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.">Evaluation</head><p>Evaluation is performed using pb bss eval 4 , a sub-toolkit of pb bss 5 <ref type="bibr" target="#b38">[41]</ref> written for evaluation. It natively supports most metrics used in source separation: SDR, signal-to-interference ratio (SIR), signal-to-artifacts ratio (SAR) <ref type="bibr" target="#b39">[42]</ref>, SI-SDR <ref type="bibr" target="#b26">[29]</ref>, PESQ <ref type="bibr" target="#b40">[43]</ref>, and short-time objective intelligibility (STOI) <ref type="bibr" target="#b41">[44]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Implementation</head><p>Asteroid follows Kaldi-style recipes <ref type="bibr" target="#b42">[45]</ref>, which involve several stages as depicted in Fig. <ref type="figure" target="#fig_1">2</ref>. These recipes implement the entire pipeline from data download and preparation to model training and evaluation. We show the typical organization of a recipe's directory in Fig. <ref type="figure" target="#fig_2">3</ref>. The entry point of a recipe is the run.sh script which will execute the following stages:</p><p>• Stage 0: Download data that is needed for the recipe. In the first stage, necessary data is downloaded (if available) into a storage directory specified by the user. We use the official scripts provided by the dataset's authors to generate the data, and optionally perform data augmentation. All the information required by the dataset's DataLoader such as filenames and paths, utterance lengths, speaker IDs, etc., is then gathered into text files under data/. The training stage is finally followed by the evaluation stage. Throughout the recipe, log files are saved under logs/ and generated data is saved under exp/.</p><p>3 github.com/jettify/pytorch-optimizer 4 pypi.org/project/pb bss eval 5 github.com/fgnt/pb bss  As can be seen in Fig. <ref type="figure" target="#fig_4">4</ref>, the model class, which is a direct subclass of PyTorch's nn.Module, is defined in model.py. It is imported in both training and evaluation scripts. Instead of defining constants in model.py and train.py, most of them are gathered in a YAML configuration file conf.yml. An argument parser is created from this configuration file to allow modification of these values from the command line, with run.sh passing arguments to train.py. The resulting modified configuration is saved in exp/ to enable future reuse. Other arguments such as the experiment name, the number of GPUs, etc., are directly passed to run.sh.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Example results</head><p>To illustrate the potential of Asteroid, we compare the performance of state-of-the-art methods as reported in the corresponding papers with our implementation. We do so on two common source separation datasets: wsj0-2mix <ref type="bibr" target="#b3">[6]</ref> and WHAMR <ref type="bibr" target="#b30">[33]</ref>. wsj0-2mix consists of a 30 h training set, a 10 h validation set, and a 5 h test set of single-channel two-speaker mixtures without noise and reverberation. Utterances taken from the Wall Street Journal (WSJ) dataset are mixed together at random SNRs between -5 dB and 5 dB. Speakers in the test set are different from those in the training and validation sets. WHAMR  Table <ref type="table" target="#tab_3">2</ref> reports SI-SDR improvements (SI-SDRi) on the test set of wsj0-2mix for several well-known source separation systems. In Table <ref type="table">3</ref>, we reproduce Table <ref type="table" target="#tab_3">2</ref> from <ref type="bibr" target="#b30">[33]</ref> which reports the performance of an improved TasNet architecture (more recurrent units, overlap-add for synthesis) on the four main tasks of WHAMR: anechoic separation, noisy anechoic separation, reverberant separation, and noisy reverberant separation. On all four tasks, Asteroid's recipes achieved better results than originally reported, by up to 2.6 dB.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Reported Using Asteroid</head><p>Deep Clustering <ref type="bibr" target="#b43">[46]</ref> 9  <ref type="table">3</ref>: SI-SDRi (dB) on the four WHAMR tasks using the improved TasNet architecture in <ref type="bibr" target="#b30">[33]</ref>.</p><p>In both Tables <ref type="table" target="#tab_3">2</ref> and<ref type="table">3</ref>, we can see that our implementations outperform the original ones in most cases. Most often, the aforementioned architectures are trained on 4-second segments. For the architectures requiring a large amount of memory (e.g., Conv-TasNet and DPRNN), we reduce the length of the training segments in order to increase the batch size and stabilize gradients. This, as well as using a weight decay of 10 -5 for recurrent architectures increased the final performance of our systems.</p><p>Asteroid was designed such that writing new code is very simple and results can be quickly obtained. For instance, starting from stage 2, writing the TasNet recipe used in Table <ref type="table">3</ref> took less than a day and the results were simply generated with the command in Fig. <ref type="figure" target="#fig_6">5</ref>, where the GPU ID is specified with the --id argument.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this paper, we have introduced Asteroid, a new open-source audio source separation toolkit designed for researchers and practitioners. Comparative experiments show that results obtained with Asteroid are competitive on several datasets and for several architectures. The toolkit was designed such that it can quickly be extended with new network architectures or new benchmark datasets. In the near future, pre-trained models will be made available and we intend to interface with ESPNet to enable end-to-end multi-speaker speech recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">References</head><p>[1] E. Vincent, T. Virtanen, and S. Gannot, Audio Source Separation and Speech Enhancement, 1st ed. Wiley, 2018.</p><p>[2] Y. Salaün, E. Vincent, N. Bertin, N. Souviraà-Labastie, X. Jaureguiberry, D. T. Tran, and F. Bimbot, "The Flexible Audio Source Separation Toolbox Version 2.0," ICASSP Show &amp; Tell, 2014.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Typical encoder-masker-decoder architecture.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Typical recipe flow in Asteroid.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Typical directory structure of a recipe.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Simplified code example.</figDesc><graphic coords="5,236.72,87.46,75.18,140.12" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>n=0 for task in clean noisy reverb reverb_noisy do ./run.sh --stage 3 --task $task --id $n n=$(($n+1)) done</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Example command line usage.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1</head><label>1</label><figDesc></figDesc><table><row><cell>. wsj0-2mix and MUSDB18</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Datasets currently supported by Asteroid.</figDesc><table /><note><p>* White sensor noise. ** Background environmental scenes.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>•</head><label></label><figDesc>Stage 1: Generate mixtures with the official scripts, optionally perform data augmentation. • Stage 2: Gather data information into text files expected by the corresponding DataLoader. • Stage 3: Train the source separation system. • Stage 4: Separate test mixtures and evaluate.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>SI-SDRi (dB) on the wsj0-2mix test set for several architectures. ks stands for for kernel size, i.e., the length of the encoder and decoder filters.</figDesc><table><row><cell></cell><cell>.6</cell><cell>9.8</cell></row><row><cell>TasNet [8]</cell><cell>10.8</cell><cell>15.0</cell></row><row><cell>Conv-TasNet [9]</cell><cell>15.2</cell><cell>16.2</cell></row><row><cell>TwoStep [15]</cell><cell>16.1</cell><cell>15.2</cell></row><row><cell>DPRNN (ks = 16) [16]</cell><cell>16.0</cell><cell>17.7</cell></row><row><cell>DPRNN (ks = 2) [16]</cell><cell>18.8</cell><cell>19.3</cell></row><row><cell>Wavesplit [10]</cell><cell>20.4</cell><cell>-</cell></row><row><cell cols="3">Reported Using Asteroid</cell></row><row><cell>Noise Reverb</cell><cell>[33]</cell><cell></cell></row><row><cell></cell><cell>14.2</cell><cell>16.8</cell></row><row><cell></cell><cell>12.0</cell><cell>13.7</cell></row><row><cell></cell><cell>8.9</cell><cell>10.6</cell></row><row><cell></cell><cell>9.2</cell><cell>11.0</cell></row><row><cell>Table</cell><cell></cell><cell></cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">An open source software system for robot audition HARK and its evaluation</title>
		<author>
			<persName><forename type="first">K</forename><surname>Nakadai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">G</forename><surname>Okuno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Nakajima</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Hasegawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Tsujino</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Humanoids</title>
		<imprint>
			<biblScope unit="page" from="561" to="566" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The ManyEars open framework</title>
		<author>
			<persName><forename type="first">F</forename><surname>Grondin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Létourneau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Ferland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Rousseau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Michaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Autonomous Robots</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="217" to="232" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Blind enhancement of the rhythmic and harmonic sections by nmf: Does it help?</title>
		<author>
			<persName><forename type="first">B</forename><surname>Schuller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lehmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Weninger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Eyben</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Rigoll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICA</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="361" to="364" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deep clustering: discriminative embeddings for segmentation and separation</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Hershey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Le</forename><surname>Roux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="31" to="35" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Permutation invariant training of deep models for speaker-independent multi-talker speech separation</title>
		<author>
			<persName><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kolbaek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jensen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="241" to="245" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">TasNet: Time-domain audio separation network for real-time, single-channel speech separation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Mesgarani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="696" to="700" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Conv-TasNet: Surpassing ideal time-frequency magnitude masking for speech separation</title>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Trans. Audio, Speech, Lang. Process</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1256" to="1266" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Wavesplit: End-to-end speech separation by speaker clustering</title>
		<author>
			<persName><forename type="first">N</forename><surname>Zeghidour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Grangier</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.08933</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The Northwestern University Source Separation Library</title>
		<author>
			<persName><forename type="first">E</forename><surname>Manilow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Seetharaman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Pardo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISMIR</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="297" to="305" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Onssen: an open-source speech separation and enhancement library</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Mandel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.00982</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Open-Unmix -a reference implementation for music source separation</title>
		<author>
			<persName><forename type="first">F.-R</forename><surname>Stöter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Uhlich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Liutkus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Mitsufuji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Open Source Soft</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">41</biblScope>
			<biblScope unit="page">1667</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.01703</idno>
		<title level="m">Py-Torch: An imperative style, high-performance deep learning library</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Two-step sound source separation: Training on learned latent targets</title>
		<author>
			<persName><forename type="first">E</forename><surname>Tzinis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Venkataramani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Subakan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Smaragdis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="31" to="35" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Dual-path RNN: Efficient long sequence modeling for time-domain single-channel speech separation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Yoshioka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="46" to="50" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Single-channel multi-speaker separation using deep clustering</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Isik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Le Roux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Hershey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Interspeech</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="545" to="549" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep attractor network for single-microphone speaker separation</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Mesgarani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="246" to="250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Demystifying TasNet: A dissecting approach</title>
		<author>
			<persName><forename type="first">J</forename><surname>Heitkaemper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Jakobeit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Boeddeker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Drude</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Haeb-Umbach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="6359" to="6363" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A comprehensive study of speech separation: Spectrogram vs waveform separation</title>
		<author>
			<persName><forename type="first">F</forename><surname>Bahmaninezhad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Interspeech</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4574" to="4578" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Universal sound separation</title>
		<author>
			<persName><forename type="first">I</forename><surname>Kavalerov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wisdom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Erdogan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Patton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Le</forename><surname>Roux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Hershey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">WASPAA</title>
		<imprint>
			<biblScope unit="page" from="175" to="179" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Filterbank design for end-to-end speech separation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Pariente</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Cornell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Deleforge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Vincent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="6364" to="6368" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A multi-phase gammatone filterbank for speech separation via TasNet</title>
		<author>
			<persName><forename type="first">D</forename><surname>Ditter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Gerkmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="36" to="40" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Speaker recognition from raw waveform with SincNet</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ravanelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SLT</title>
		<imprint>
			<biblScope unit="page" from="1021" to="1028" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">The NumPy array: A structure for efficient numerical computation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Van Der Walt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>Colbert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Varoquaux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computing in Science and Engineering</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="22" to="30" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Signal estimation from modified shorttime Fourier transform</title>
		<author>
			<persName><forename type="first">D</forename><surname>Griffin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Acoust., Speech, Signal Process</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="236" to="243" />
			<date type="published" when="1984">1984</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A fast Griffin-Lim algorithm</title>
		<author>
			<persName><forename type="first">N</forename><surname>Perraudin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Balazs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Søndergaard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">WASPAA</title>
		<imprint>
			<biblScope unit="page" from="1" to="4" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Iterative phase estimation for the synthesis of separated sources from single-channel mixtures</title>
		<author>
			<persName><forename type="first">D</forename><surname>Gunawan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Sen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Process. Letters</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="421" to="424" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">SDRhalf-baked or well done?</title>
		<author>
			<persName><forename type="first">J</forename><surname>Le Roux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wisdom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Erdogan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Hershey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="626" to="630" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A deep learning loss function based on the perceptual evaluation of the speech quality</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Martín-Doñas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Peinado</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Process. Letters</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1680" to="1684" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Multitalker speech separation with utterance-level permutation invariant training of deep recurrent neural networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kolbaek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z.-H</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jensen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Trans. Audio, Speech, Lang. Process</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1901" to="1913" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">WHAM!: extending speech separation to noisy environments</title>
		<author>
			<persName><forename type="first">G</forename><surname>Wichern</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Antognini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Flynn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">R</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Mcquinn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Crow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Manilow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Le</forename><surname>Roux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Interspeech</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1368" to="1372" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">WHAMR!: Noisy and reverberant single-channel speech separation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Maciejewski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wichern</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Mcquinn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Le</forename><surname>Roux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="696" to="700" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">LibriMix: An open-source dataset for generalizable speech separation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Cosentino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Cornell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pariente</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Deleforge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Vincent</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.11262</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">What&apos;s all the fuss about free universal sound separation data?</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wisdom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Erdogan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P W</forename><surname>Ellis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Serizel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Turpault</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Fonseca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Salamon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Seetharaman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Hershey</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note>in preparation</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">K A</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Beyrami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Dubey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Gopal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cheng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.08662</idno>
		<title level="m">The Interspeech 2020 deep noise suppression challenge: Datasets, subjective speech quality and testing framework</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">SMS-WSJ: Database, performance measures, and baseline recipe for multi-channel source separation and recognition</title>
		<author>
			<persName><forename type="first">L</forename><surname>Drude</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Heitkaemper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Boeddeker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Haeb-Umbach</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.13934</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Analyzing the impact of speaker localization errors on speech separation for automatic speech recognition</title>
		<author>
			<persName><forename type="first">S</forename><surname>Sivasankaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Fohr</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">The MUSDB18 corpus for music separation</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Rafii</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Liutkus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F.-R</forename><surname>Stöter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">I</forename><surname>Mimilakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bittner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<author>
			<persName><forename type="first">W</forename><surname>Falcon</surname></persName>
		</author>
		<ptr target="https://github.com/PytorchLightning/pytorch-lightning" />
		<title level="m">Pytorch lightning</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Tight integration of spatial and spectral features for BSS with deep clustering embeddings</title>
		<author>
			<persName><forename type="first">L</forename><surname>Drude</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Haeb-Umbach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Interspeech</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2650" to="2654" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Performance measurement in blind audio source separation</title>
		<author>
			<persName><forename type="first">E</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Gribonval</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Févotte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Trans. Audio, Speech, Lang. Process</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1462" to="1469" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Perceptual evaluation of speech quality (PESQ) -a new method for speech quality assessment of telephone networks and codecs</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">W</forename><surname>Rix</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">G</forename><surname>Beerends</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">P</forename><surname>Hollier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Hekstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP</title>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="749" to="752" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">An algorithm for intelligibility prediction of time-frequency weighted noisy speech</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>Taal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Hendriks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Heusdens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jensen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Trans. Audio, Speech, Lang. Process</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="2125" to="2136" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">The Kaldi speech recognition toolkit</title>
		<author>
			<persName><forename type="first">D</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ghoshal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Boulianne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Burget</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Glembek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ASRU</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Alternative objective functions for deep clustering</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Le</forename><surname>Roux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Hershey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="686" to="690" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
