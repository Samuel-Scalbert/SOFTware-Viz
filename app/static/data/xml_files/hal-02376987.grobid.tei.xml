<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">MaxHedge: Maximising a Maximum Online</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Stephen</forename><surname>Pasteris</surname></persName>
							<email>s.pasteris@cs.ucl.ac.uk</email>
						</author>
						<author>
							<persName><forename type="first">Fabio</forename><surname>Vitale</surname></persName>
							<email>fabio.vitale@inria.fr</email>
						</author>
						<author>
							<persName><forename type="first">Kevin</forename><surname>Chan</surname></persName>
							<email>kevin.s.chan.civ@mail.mil</email>
						</author>
						<author>
							<persName><forename type="first">Shiqiang</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Mark</forename><surname>Herbster</surname></persName>
							<email>m.herbster@cs.ucl.ac.uk</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">University College London</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">Sapienza University Italy</orgName>
								<orgName type="institution" key="instit2">INRIA Lille</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Army Research Lab Adelphi</orgName>
								<address>
									<region>MD</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">IBM Research Yorktown Heights</orgName>
								<address>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="institution">University College London</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">MaxHedge: Maximising a Maximum Online</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">8CA2D2058A779BE2ACF3164C04480063</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-04-12T14:49+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We introduce a new online learning framework where, at each trial, the learner is required to select a subset of actions from a given known action set. Each action is associated with an energy value, a reward and a cost. The sum of the energies of the actions selected cannot exceed a given energy budget. The goal is to maximise the cumulative profit, where the profit obtained on a single trial is defined as the difference between the maximum reward among the selected actions and the sum of their costs. Action energy values and the budget are known and fixed. All rewards and costs associated with each action change over time and are revealed at each trial only after the learner's selection of actions. Our framework encompasses several online learning problems where the environment changes over time; and the solution trades-off between minimising the costs and maximising the maximum reward of the selected subset of actions, while being constrained to an action energy budget. The algorithm that we propose is efficient and general that may be specialised to multiple natural online combinatorial problems.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In this paper we propose a novel online framework where learning proceeds in a sequence of trials and the goal is to select, at each trial, a subset of actions maximising a profit while taking into account a certain constraint. More precisely, we are given a finite set of actions enumerated from 1 to n. Each action is associated with three values: (i) a cost, (ii) an amount of energy (both of which are required to perform the given action), as well as (iii) a reward. Both the cost and the reward associated with each action may change over time, are unknown at the beginning of each trial, and their values are all revealed after the learner's selection. The energy associated with each action is instead known by the learner and does not change over time. At each trial, the learner is required to select a subset of actions such that the sum of their energies does not exceed a fixed energy budget. The goal of the learner is to maximise the cumulative profit, where the profit obtained on a single trial is defined as the difference between the maximum reward among the selected actions and the sum of their costs. We denote by T the total number of trials.</p><p>Our framework is general and flexible in the sense that it encompasses several online learning problems. In the general case, the main challenge lies in the fact that the rewards are not known at the beginning of each trial, and the learner's profit depends only on the maximum reward among the selected subset of actions, instead of the sum of all their rewards. In particular, it is worth mentioning three different problems which can be seen as special cases of our online learning framework; these are variants of the Facility Location problem, the 0-1 Knapsack problem, and the Knapsack Median problem. When all action energy values are equal to 0, we obtain an online learning variant of the Facility Location problem (see, e.g., <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b17">[18]</ref>). The goal of this specific problem may be viewed as selecting and opening a subset of facilities at each given trial, to service a sequence of users which arrive one at a time. At any given trial t, each action's cost may be interpreted as the cost for opening a facility for the t-th user. The reward associated with a given facility represents what the user potentially gains when it is opened. More specifically, the rewards can be seen as quantities dependent on the distance between the user and the facilities in some metric space. In this context, it is reasonable to assume that the profit obtained on a single trial depends only on the maximum reward among the opened facilities taking into account the facility costs. This represents a natural setting for the Facility Location problem, because in practical scenarios users may arrive sequentially and each arrival requires a connection to an open facility. Online versions of the Facility Location problem have been studied in several contexts (see, e.g., <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b19">[20]</ref>). However, as far as we are aware, our work is the first study of this specific online setting for the Facility Location problem. Moreover, this dynamic model is natural and interesting within this context 1 , because in practice the location of the next user, which in turn determines the reward associated with each facility, is often unknown to the learner. At each trial, after the user's location is known, the rewards are then revealed, because disclosing the user's location enables to compute all distances between the facilities and the user, which are previously unknown. In fact, this corresponds to assuming all rewards are revealed at the end of the trial.</p><p>When, instead, all rewards are equal to 0 and all costs are negative, the problem can be seen as an online learning variant of the 0-1 Knapsack problem (see, e.g., <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b18">[19]</ref>). In this case, each action corresponds to an item whose weight is equal to the associated energy, the energy budget represents the knapsack capacity, and the absolute value of each action cost can be viewed as the corresponding item value. Our formulation makes the problem challenging especially because the item values are revealed only after the learner's selection.</p><p>Finally, when all costs are equal to 0, we obtain an online learning variant of the Knapsack Median problem ( <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b14">[15]</ref>). In this problem (which is a generalization of the k-median problem -see, e.g., <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b9">[10]</ref>), we are given a set of clients and facilities, as well as a distance metric. The goal is to open a subset of facilities such that the total connection cost (distance to nearest open facility) of all clients is minimized, while the sum of the open facility weights is limited by a budget threshold.</p><p>1 Similar arguments hold also for motivating the other two special cases mentioned in this section.</p><p>In our framework, at each trial the rewards can express the closeness of each facility, and the action energy budget represents therefore the threshold of sum of the opened facility weights. This problem has practical applications in multiple domains. In computer networks, network administrators may want to understand where to place network monitors or intrusion detection systems. Network packets or malicious attacks are related to the events playing a crucial role in this scenario, and a limited amount of network resources are available to detect or observe network behavior. Another class of application examples include municipal emergency services. A service center needs to deploy responders (police, paramedics, fire rescue), and with limited resources, personnel must be deployed sparingly. A further application is related to deploying program instances in a distributed computing environment (e.g. distributed cloud). These systems must respond to user requests and are subject to constraints. Both costs and rewards may be unknown in practical real-world scenarios at the beginning of each trial.</p><p>For our general problem we propose and rigorously analyse a very scalable algorithm called MaxHedge based on the complex interplay between satisfying the energy budget constraint and bounding the profit by a concave function, which in turn is related to the online gradient descent algorithm. Moreover, the total time required per trial by our learning strategy is quasi-linear in n. We measure the performance of proposed solution with respect to the difference between its cumulative profit and a discounted cumulative profit of the best fixed subset of actions.</p><p>In summary, our framework captures several real-world problems, where the environment changes over time and the solutions trade-off between minimising the costs and maximising the maximum reward among the selected subset of actions, while being constrained to an action energy budget. The framework is very general and the proposed algorithm is very efficient and may be specialised to several natural online combinatorial problems. Finally we provide a guarantee on the rewards achieved and costs incurred as compared to the best fixed subset of actions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Related Work</head><p>The closest work to our online learning framework is perhaps addressed in <ref type="bibr" target="#b12">[13]</ref>, where the authors describe an online learning algorithm for structured concepts that are formed by components. Each component and each concept can be respectively seen as an action and a feasible subset of actions. Despite several similarities, the algorithm they proposed cannot be used when the rewards are non-zero, because we focus on the maximum reward among the selected subsets of actions, whereas in <ref type="bibr" target="#b12">[13]</ref> the profit corresponds to the sum of the rewards of all selected components/actions. When all rewards are instead equal to 0, then we have the online variant of the 0-1 Knapsack problem described above as one of the three special cases of our general problem. In Appendix C we prove that if the algorithm presented in <ref type="bibr" target="#b12">[13]</ref> could handle the Knapsack problem, then the classical version of the Knapsack problem could be solved in polynomial time, which therefore implies that it cannot address this problem unless P = N P .</p><p>The Hedge Algorithm described in <ref type="bibr" target="#b6">[7]</ref> obtains a regret linear in n. However, as we have exponentially many possible subsets of selected actions, a vanilla application of Hedge would require an exponential amount of time and space to solve our problem.</p><p>Another class of problems and algorithms that are not far from ours is represented by online decision problems where efficient strategies use, as a subroutine, an approximation algorithm for choosing a concept to maximise an inner product ( <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b10">[11]</ref>). Again, these learning strategies cannot handle the case of nonzero reward. They also have a significantly higher time complexity than MaxHedge in the case of zero reward.</p><p>In <ref type="bibr" target="#b2">[3]</ref> , <ref type="bibr" target="#b8">[9]</ref>, and <ref type="bibr" target="#b21">[22]</ref>, the authors address the problem of online maximisation of non-negative monotone submodular functions. Although our profit is submodular, it is not necessarily either non-negative or monotone. However, as we shall show in Appendix D, we could, for the facility location special case, combine much of the mechanics of our paper with the second algorithm of <ref type="bibr" target="#b2">[3]</ref>, essentially doing gradient ascent with the exact expected profit instead of an approximate expected profit (as is done in MaxHedge). Even though this new algorithm could be as efficient as the one presented in our paper, its theoretical guarantees are worse. Note that in the Knapsack and Knapsack Median special cases, the profit is indeed monotone and non-negative. For these special cases, the approach presented in <ref type="bibr" target="#b21">[22]</ref> comes close to solving the problem, but only guarantees that the expectation of the total energy does not exceed the budget rather than the actual total energy. In addition, for the Knapsack Median problem, <ref type="bibr" target="#b21">[22]</ref> uses quadratic time and space. As far as we are aware, there does not exist any trivial reduction to use <ref type="bibr" target="#b2">[3]</ref> or <ref type="bibr" target="#b8">[9]</ref> for these special cases.</p><p>Finally, in <ref type="bibr" target="#b15">[16]</ref> the authors address a problem of online minimisation of submodular function. Our paper maximises, instead of minimising, a submodular function (the profit). This is a very different problem. Some of the techniques behind the development of MaxHedge were inspired by <ref type="bibr" target="#b24">[25]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Preliminaries and Problem Setup</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Preliminaries</head><p>Vectors in R n will be indicated in bold. Given a vector v ∈ R n we define v i to be its i-th component. Given vectors v, x ∈ R n define v, x := n i=1 v i x i . Define P to be the set of n-dimensional real vectors in which every component is non-negative. Given a closed convex set C ⊆ R n and a vector x ∈ R n , we define Π C (x) as the projection (under the Euclidean norm) of x onto C. Let N be the set of the positive integers. For l ∈ N define [l] = {1, 2, 3, ..., l}. Given an event E let ¬E be the event that E does not occur. Let P (E) be the probability that event E occurs. For a random variable Y , let E (Y ) be the expected value of Y . Given a differentiable function h : R n → R and a vector x ∈ R n let ∇h(x) be the derivative of h evaluated at x. Let ∂ i h(x) be the i-th component of ∇h(x).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Problem Setup</head><p>In this section we formally define our problem. We have a set of n actions enumerated from 1 to n. Each action i has an energy z i ∈ [0, β] for some β &lt; 1, and on each trial t each action i has a cost c t i ∈ R (which can be negative) and a reward r t i ∈ R + . The learner knows z, but c t and r t are revealed to the learner only at the end of trial t. On each trial t the learner has to select a set X t ⊆ [n] of actions, such that the total energy i∈X t z i of the selected actions is no greater than 1. In selecting the set X t , the learner pays a cost equal to i∈X t c t i . On each trial t the learner then receives the maximum reward max i∈X t r t i , over all actions selected (defined as equal to zero if X t is empty). Hence, the profit obtained by the learner on trial t is equal to max i∈X t r t i -i∈X t c t i .</p><p>Formally, this online problem can be defined as follows: We have a vector z ∈ P known to the learner. On trial t:</p><p>1. Nature selects vectors c t ∈ R n and r t ∈ P (but does not reveal these vectors to learner)</p><formula xml:id="formula_0">2. Learner selects a set X t ⊆ [n] with i∈X t z i ≤<label>1</label></formula><p>3. Learner obtains profit:</p><formula xml:id="formula_1">µ t (X t ) := max i∈X t r t i - i∈X t c t i</formula><p>4. c t and r t are revealed to the learner.</p><p>In this paper we write, for a trial t, the cost vector c t as the sum c t+ + c t-where c t+ i := max{0, c t i } and c t- i := min{0, c t i }, i.e. c t+ and c t-are the positive and negative parts of the cost vector, respectively.</p><p>In order to bound the cumulative profit of our algorithm we, for some α, δ ∈ [0, 1], some set S ⊆ [n] and some trial t, define the (α, δ)-discounted profit μt α,δ (S) as:</p><formula xml:id="formula_2">μt α,δ (S) := α max i∈S r t i -α i∈S c t- i -δ i∈S c t+ i</formula><p>which would be the profit obtained on trial t if we selected the subset of actions S, and all the rewards and negative costs were multiplied by α and all positive costs were multiplied by δ.</p><p>In this paper we provide a randomised quasi-linear time (per trial) algorithm MaxHedge that, for any set S ⊆ [n] with i∈S z i ≤ 1, obtains an expected cumulative profit bounded below by:</p><formula xml:id="formula_3">E T t=1 µ t (X t ) ≥ T t=1 μt α,δ (S) -n √ 2T δ(r + ĉ)</formula><p>where</p><formula xml:id="formula_4">δ := 1 - √ β 2 , α := 1 -exp -1 - √ β 2 , r := max t∈[T ],i∈[n] r t i and ĉ := max t∈[T ],i∈[n] |c t i |.</formula><p>This paper is structured as follows. In Section 3 we introduce the algorithms that define MaxHedge. In Section 4 we prove that the sets of actions selected by MaxHedge are feasible. In Section 5 we prove the above bound on the cumulative profit. In Section 6 we give special cases of the general problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Algorithms</head><p>We now present our learning strategy MaxHedge, describing the two subroutines "Algorithm 1" and "Algorithm 2" (see the pseudocode below). MaxHedge maintains a vector ω ∈ C where C := {x ∈ [0, 1] n : x, z ≤ 1}. We define ω t to be the vector ω at the start of trial t. We initialise ω 1 ← 0. On trial t MaxHedge (randomly) constructs X t from ω t using Algorithm 1. After receiving r t and c t MaxHedge updates ω (from ω t to ω t+1 ) using Algorithm 2. Algorithm 2 also uses a "learning rate" ηt which is defined from ηt-1 . We define η0 := ∞.</p><p>Algorithm 1 operates with a partition of all possible actions and, for each set in this partition, Algorithm 1 draws a certain subset of actions from it. Given a set in the partition, the number of actions drawn from it and the probability distribution governing the draws depends on β and ω t . The subset, X t , of actions selected by Algorithm 1 satisfies the following three crucial properties (proved in sections 4 and 5):</p><p>• The total energy of all actions selected is no greater than 1.</p><p>• Given an arbitrary set Z ⊆ [n], the probability that X t and Z intersect is lower bounded by 1exp -δ i∈Z ω t i .</p><p>• Given an action i, the probability that it is selected on trial t is upper bounded by δω t i .</p><p>In the analysis we shall construct, from r t and c t , a convex function h t : C → R. Using the second and third properties (given above) of Algorithm 1, we show that h t (ω t ) is an upper bound on the negative of the expected profit on trial t. Algorithm 2 computes the gradient g t := ∇h t (ω t ) and updates ω using online gradient descent on C.</p><p>The last line of Algorithm 2 requires us to project (with Euclidean distance) the vector y t onto the set C, i.e. we must compute the x that minimises the value xy t subject to x ∈ C. Note that minimising xy t is equivalent to minimising xy t 2 = x, x -2 y t , x + y t , y t , which is in turn equivalent to minimising x, x -2 y t , x . The constraints defining the set C then imply that this projection is a case of the continuous bounded quadratic knapsack problem which can be solved in linear time (see, e.g., <ref type="bibr" target="#b13">[14]</ref>).</p><p>The bottleneck of the algorithms is hence the ordering step in Algorithm 2 which takes a time of O(n log(n))</p><formula xml:id="formula_5">Algorithm 1 Constructing X t 1: C ← {x ∈ [0, 1] n : x, z ≤ 1} 2: β ← max i∈[n] z i 3: τ ← 1 - √ β 4: δ ← 1 - √ β 2 . 5: Γ ← {q ∈ N : ∃i ∈ [n] with τ q β &lt; z i ≤ τ q-1 β} 6: For all q ∈ Γ set Ω q ← {i ∈ [n] : τ q β &lt; z i ≤ τ q-1 β}. 7: Input: ω t ∈ C 8: For all q ∈ Γ set π t q ← i∈Ωq ω t i 9:</formula><p>For all q ∈ Γ set ζ t q ← δπ t q 10: For all q ∈ Γ and for all k ≤ ζ t q draw ξ t q,k randomly from Ω q such that ξ t q,k ← i with probability ω t i /π t q 11: For all q ∈ Γ and for k := ζ t q +1 draw ξ t q,k randomly from Ω q ∪ {0} such that, for i ∈ Ω q we have ξ t q,k ← i with probability (δπ t q -δπ t q )ω t i /π t q , and we have ξ t q,k ← 0 with probability δπ t q + 1 -δπ t q . NB: In the case that π t q = 0 we define 0/0 = 0</p><formula xml:id="formula_6">12: Output: X t ← {ξ t q,k : q ∈ Γ, k ≤ ζ t q + 1} \ {0} Algorithm 2 Computing ω t+1 1: C ← {x ∈ [0, 1] n : x, z ≤ 1} 2: β ← max i∈[n] z i 3: δ ← 1 - √ β 2 4: Input: ω t ∈ C, c t ∈ R n r t ∈ P 5: Order [n] as [n] = {σ(t, 1), σ(t, 2), ...σ(t, n)} such that r t σ(t,j) ≥ r t σ(t,j+1) for all j ∈ [n -1] 6: For all j ∈ [n] set t j ← exp -δ j k=1 ω t σ(t,k) 7: For all j ∈ [n] set: λ t j ← r t σ(t,n) t n + n-1 k=j r t σ(t,k) -r t σ(t,k+1) t k 8: For all j ∈ [n] set: g t σ(t,j) ← δ c t+ σ(t,j) + c t- σ(t,j) exp -δω t σ(t,j) -λ t j 9: ηt ← min ηt-1 , √ n/ g t 10: η t ← ηt / √ 2t 11: y t ← ω t -η t g t 12: Output: ω t+1 ← Π C (y t )</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">The Feasibility of X t</head><p>In this section we show that the the total energy of the actions selected by Algorithm 1 is no greater than 1, as required in our problem definition (see Section 2.2). We first introduce the sets and quantities used in the selection of the actions. Definition 4.1. We define the following:</p><formula xml:id="formula_7">• τ := 1 - √ β and δ := 1 - √ β 2</formula><p>• For all q ∈ N we define</p><formula xml:id="formula_8">Ω q := {i ∈ [n] : τ q β &lt; z i ≤ τ q-1 β} • Γ := {q ∈ N : Ω q = ∅}. Note that {Ω q : q ∈ Γ} is a partition of [n]</formula><p>• On trial t, for all q ∈ Γ define π t q := i∈Ωq ω t i and ζ t q := δπ t q .</p><p>Algorithm 1 works by drawing actions {ξ t q,k : q ∈ Γ, k ∈ [ζ t q + 1]} randomly, where the number of actions (including a "null action" 0) drawn (with replacement) from Ω q is equal to ζ t q + 1 and the probability distribution of the draws is dependent on ω t .</p><p>The following theorem ensures that the choice of X t made by our method satisfies our problem's energy constraint. Theorem 4.2. On trial t we have i∈X t z i ≤ 1.</p><p>Proof. See Appendix A</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Bounding the Cumulative Profit</head><p>In this section we bound the cumulative profit of MaxHedge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">The Probability of Intersection</head><p>In this subsection we first bound below the probability that, on a trial t, an arbitrary set Z intersects with X t . We start with the following lemma: Lemma 5.1. Given a set D of independent draws from |D|-many probability distributions, such that the probability of an event E happening on draw d ∈ D is ρ d , then the probability of event E happening on either of the draws is lower bounded by:</p><formula xml:id="formula_9">1 -exp - d∈D ρ d Proof. See Appendix A</formula><p>We now bound the probability of intersection: Theorem 5.2. For any trial t and any subset Z ⊆</p><formula xml:id="formula_10">[n] we have P (Z ∩ X t = ∅) ≥ 1 -exp -δ i∈Z ω t i .</formula><p>Proof. See Appendix A</p><p>We now bound the probability that some arbitrary action is selected on trial t:</p><p>Theorem 5.3. Given some action i ∈ [n] and some trial t ∈ [T ] we have 1-exp(-δω t i ) ≤ P (i ∈ X t ) ≤ δω t i .</p><p>Proof. See Appendix A</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Approximating the Expected Profit</head><p>In this subsection we define a convex function h t : C → R and show that the expected profit on trial t is bounded below by -h t (ω t ). </p><formula xml:id="formula_11">for all j ∈ [n -1].</formula><p>We also define σ(t, n + 1) := 0 and r t 0 := 0. Definition 5.5. Given a trial t and a number j ∈ [n] we define the function f t j : P → R by:</p><formula xml:id="formula_12">f t j (γ) := r t σ(t,j) -r t σ(t,j+1) 1 -exp -δ j k=1 γ σ(t,k)</formula><p>We also define the function h t : P → R as:</p><formula xml:id="formula_13">h t (γ) = c t+ , γ + n i=1 c t- i (1 -exp(-δγ i )) - n j=1 f t j (γ)</formula><p>Theorem 5.6. For all t ∈ [T ], the function h t is convex.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proof. See Appendix A</head><p>The rest of this subsection proves that the expected profit on trial t is bounded below by -h t (ω t ).</p><p>Lemma 5.7. On trial t we have</p><formula xml:id="formula_14">max i∈X t r t i = n j=1 r t σ(t,j) -r t σ(t,j+1) I (∃k ≤ j : σ(t, k) ∈ X t ).</formula><p>Proof. See Appendix A Lemma 5.8. On trial t we have E (max</p><formula xml:id="formula_15">i∈X t r t i ) = n j=1 r t σ(t,j) -r t σ(t,j+1) P (∃k ≤ j : σ(t, k) ∈ X t ).</formula><p>Proof. Direct from lemma 5.7 using linearity of expectation.</p><p>Theorem 5.9. On trial t we have E (µ t (X t )) ≥ -h t (ω t ).</p><p>Proof. See Appendix A</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">The Gradient</head><p>In this subsection we show how to construct the gradient of h t and bound its magnitude. We start with the following definitions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Definition 5.10. On any trial t and for any j ∈ [n]</head><p>we define:</p><formula xml:id="formula_16">• t j := exp -δ j k=1 ω t σ(t,k) • λ t j := n k=j r t σ(t,k) -r t σ(t,k+1) t k • g t σ(t,j) := δ c t+ σ(t,j) + c t- σ(t,j) exp -δω t σ(t,j) -λ t j</formula><p>We first show that g t is the gradient of h t evaluated at ω t .</p><p>Theorem 5.11. On any trial t we have g t = ∇h t (ω t ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proof. See Appendix A</head><p>We now bound the magnitude of the gradient.</p><p>Lemma 5.12. For any trial t we have g t 2 ≤ nδ 2 (r + ĉ) 2 .</p><p>Proof. Since ω t σ(t,k) ≥ 0 for all k ∈ [n] we have t j ∈ [0, 1] for all j ∈ [n]. This gives us, for all j ∈ [n], that δλ t j ≤ δr t σ(t,n) +δ n-1 k=j (r t σ(t,k) -r t σ(t,k+1) ) = δr t σ(t,j) ≤ δr. Since also λ t j ≥ 0 this implies that -g t i ≤ δr + δĉ and that g t i ≤ δĉ so (g t i ) 2 ≤ (δr + δĉ) 2 . This then implies the result.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Online Gradient Descent</head><p>In this subsection we show that Algorithm 2 corresponds to the use of online gradient descent over C with convex functions {h t : t ∈ [T ]} and we use the standard analysis of online gradient descent to derive a lower bound on the cumulative profit.</p><p>From here on we compare the performance of our algorithm against any fixed set S of actions such that i∈S z i ≤ 1. We define φ as the vector in R n such that, for all i ∈ [n], we have φ i := 0 if i / ∈ S and</p><formula xml:id="formula_17">φ i := 1 if i ∈ S. It is clear that φ ∈ C.</formula><p>Definition 5.13. Our learning rates are defined as follows:</p><formula xml:id="formula_18">• ηt := min t ≤t ( √ n/ g t ) • η t := ηt / √ 2t</formula><p>The next result follows from the standard analysis of online gradient descent.</p><p>Theorem 5.14. We have:</p><formula xml:id="formula_19">T t=1 (h t (ω t ) -h t (φ)) ≤ R 2 2η T + 1 2 T t=1 η t g t 2</formula><p>where R := max x,y∈C x -y .</p><p>Proof. For all trials t: From Theorem 5.6 we have that h t is a convex function. From Theorem 5.11 we have that g t = ∇h t (ω t ). We also have that η t+1 ≤ η t so since, by Algorithm 2, we have ω t+1 = Π C (ω t -η t g t ) and φ ∈ C, the standard analysis of online gradient descent (see, e.g., <ref type="bibr" target="#b23">[24]</ref>) gives us the result.</p><p>We now bound the right hand side of the equation in Theorem 5.14. </p><formula xml:id="formula_20">(h t (ω t ) -h t (φ)) ≤ n √ 2T δ(r + ĉ) Proof. See Appendix A</formula><p>The next result bounds h t (φ).</p><p>Lemma 5.19. On trial t we have μt α,δ (S) ≤ -h t (φ) where α := 1 -e -δ .</p><p>Proof. Let j = argmax j∈[n]:σ(t,j)∈S r t σ(t,j) which is equal to the minimum j such that σ(t, j) ∈ S. Note that for all j ≥ j we have j k=1 φ σ(t,j) ≥ 1 and hence</p><formula xml:id="formula_21">f t j (φ) ≥ r t σ(t,j) -r t σ(t,j+1) (1-e -δ ) so n j=1 f t j (φ) ≥ n j=j f t j (φ) ≥ n j=j r t σ(t,j) -r t σ(t,j+1) (1 -e -δ ) = r t σ(t,j ) (1 -e -δ ) = (1 -e -δ ) max i∈S r t i .</formula><p>Also note that δ c t+ , φ = δ i∈S c t+ i and that</p><formula xml:id="formula_22">n i=1 c t- i (1 -exp(-δφ i )) = i∈S c t- i (1 -e -δ</formula><p>). Combining with the above gives us the result.</p><p>Putting together we obtain the main result: Theorem 5.20. We have:</p><formula xml:id="formula_23">E T t=1 µ t (X t ) ≥ T t=1 μt α,δ (S) -n √ 2T δ(r + ĉ)</formula><p>where</p><formula xml:id="formula_24">δ := 1 - √ β 2 , α := 1 -exp -1 - √ β 2 , r := max t∈[T ],i∈[n] r t i and ĉ := max t∈[T ],i∈[n] |c t i |.</formula><p>Proof. Let α := 1 -e -δ . By Theorem 5.9 we have, for all t ∈ [T ], that E (µ t (X t )) ≥ -h t (ω t ). By Lemma 5.19 we have, for all t ∈ [T ], that μt α,δ (S) ≤ -h t (φ). Hence we have that μt α,δ (S) -E (µ t (X t )) ≤ h t (ω t ) -h t (φ). By Theorem 5.18 we than have:</p><formula xml:id="formula_25">T t=1 μt α,δ (S) -E µ t (X t ) ≤ T t=1 (h t (ω t ) -h t (φ)) ≤n √ 2T δ(r + ĉ)</formula><p>Rearranging gives us:</p><formula xml:id="formula_26">T t=1 E µ t (X t ) ≥ T t=1 μt α,δ (S) -n √ 2T δ(r + ĉ)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Special Cases</head><p>The following online variants of classic computer science problems are special cases of the general problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Facility Location Problem</head><p>The (inverted) facility location problem is defined by a vector c ∈ P and vectors r 1 , r In our online variant of the (inverted) facility location problem, learning proceeds in trials. On trial t:</p><p>1. For all sites i, the cost, c t i of opening a facility on site i is revealed to the learner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">The learner chooses a set X t of sites in which to</head><p>open facilities on.</p><p>3. User t requests the use of a facility, revealing r t to the learner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Learner incurs profit: max</head><formula xml:id="formula_27">i∈X t r t i -i∈X t c t i</formula><p>The objective is to maximise the cumulative profit. Note that this is the special case of our problem when, for all i ∈ [n] and t ∈ [T ] we have z i = 0 and c t i ≥ 0. Given some set S the expected cumulative profit of MaxHedge is then bounded below by:</p><formula xml:id="formula_28">T t=1 (1 -1/e) max i∈S r t i - i∈S c t i -n √ 2T (r + ĉ)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Knapsack Median Problem</head><p>The (inverted) knapsack median problem is defined by a vector z ∈ P and vectors</p><formula xml:id="formula_29">r 1 , r 2 , • • • , r T ∈ P. A feasible solution is any X ⊆ [n] with i∈X z i ≤ 1.</formula><p>The aim is to maximise the objective function T t=1 max i∈X r t i . An example of the problem is as follows. There are n sites and T users, all located in some metric space. We have to choose a set X of sites to open a facility on. Opening a facility on site i has a fee of z i and we have a budget of 1 to spend on opening facilities. Each user pays us a reward based on how near it is to the closest open facility. If the nearest open facility to user t is at site i then user t rewards us r t i . The objective is to maximise the total reward.</p><p>In our online variant of the (inverted) knapsack median problem, learning proceeds in trials. The learner has knowledge of the fee z i for every site i. On trial t:</p><p>1. The learner chooses a set X t of sites in which to open facilities on. The total fee, i∈X t z i , can't exceed 1.</p><p>2. User t requests the use of a facility, revealing r t to the learner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Learner incurs reward: max i∈X t r t i</head><p>The objective is to maximise the cumulative reward. Note that this is the special case of our problem when, for all i ∈ [n] and t ∈ [T ] we have c t i = 0. Given some set S with i∈S z i ≤ 1 the expected cumulative reward of MaxHedge is then bounded below by:</p><formula xml:id="formula_30">(1 -exp (-δ)) T t=1 max i∈S r t i -δn √ 2T r</formula><p>where</p><formula xml:id="formula_31">δ := (1 - √ max i∈[n] z i ) 2</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">0-1 Knapsack Problem</head><p>The knapsack problem is defined by a vector z ∈ P and a vector v ∈ P. A feasible solution is any X ⊆ [n] with</p><formula xml:id="formula_32">i∈X z i ≤ 1.</formula><p>The aim is to maximise the objective function i∈X v i .</p><p>An example of the problem is as follows. We have n items. Item i has a value v i and a weight z i . The objective is to place a set X ⊆ [n] of items in the knapsack that maximises the total value of all items in the knapsack subject to their total weight being no greater than 1.</p><p>In our online variant of the knapsack problem, learning proceeds in trials. The learner has knowledge of the weight z i for every item i. On trial t:</p><p>1. The learner chooses a set X t of items to place in the knapsack. The total weight, i∈X t z i , can't exceed 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2.</head><p>For each item i, the value v t i , of item i on this trial is revealed to the learner 3. Learner incurs profit:</p><formula xml:id="formula_33">i∈X t v t i</formula><p>The objective is to maximise the cumulative profit. Note that this is the special case of our problem when, for all i ∈ [n] and t ∈ [T ] we have r t i = 0 and c t i ≤ 0 (noting that c t i = -v t i ). Given some set S with i∈S z i ≤ 1 the expected cumulative profit of MaxHedge is then bounded below by:</p><formula xml:id="formula_34">(1 -exp (-δ)) T t=1 i∈S v t i -δn √ 2T ĉ where δ := (1 - √ max i∈[n] z i ) 2</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusions and Ongoing Research</head><p>We presented and investigated in depth a novel online framework, capable of encompassing several online learning problems and capturing many practical problems in the real-world. The main challenge of the general version of this problem lies in the fact that the learner's profit depends on the maximum reward of the selected actions, instead of the sum of all their rewards. We proposed and rigorously analysed a very scalable and efficient learning strategy MaxHedge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Current ongoing research includes:</head><p>• Deriving a lower bound on the achievable profit.</p><p>• Complementing our results with a set of experiments on synthetic and real-world datasets.</p><p>• Several real systems usually have a switching cost for turning on/off services, which translates in our framework to the cost incurred whenever an action selected at any given trial is not selected in the preceding one. This represents an interesting direction for further research, which is certainly motivated by practical problems. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Missing Proofs</head><p>Proof of Theorem 4.2</p><p>Proof. Define z 0 := 0. We have X t = {ξ t q,k : q ∈ Γ, k ≤ ζ t q + 1} \ {0} so:</p><formula xml:id="formula_35">i∈X t z i ≤ q∈Γ,k≤ζ t q +1 z ξ t q,k = q∈Γ k≤ζ t q +1 z ξ t q,k ≤ q∈Γ k≤ζ t q +1 τ q-1 β = q∈Γ (ζ t q + 1)τ q-1 β ≤ q∈Γ (δπ t q + 1)τ q-1 β = β q∈Γ τ q-1 + βδ q∈Γ π t q τ q-1 ≤ β ∞ q=1 τ q-1 + βδ q∈Γ π t q τ q-1 = β 1 -τ + βδ q∈Γ π t q τ q-1 = β 1 -τ + βδ q∈Γ i∈Ωq ω t i τ q-1 = β 1 -τ + δ q∈Γ i∈Ωq ω t i τ -1 (τ q β) ≤ β 1 -τ + δ q∈Γ i∈Ωq ω t i τ -1 z i ≤ β 1 -τ + δ τ q∈Γ i∈Ωq ω t i z i ≤ β 1 -τ + δ τ i∈[n] ω t i z i ≤ β 1 -τ + δ τ = β + (1 -β) = 1</formula><p>Proof of Lemma 5.1</p><p>Proof. Given some d ∈ D let E d be the event that E happens on draw d. Given some set D ⊆ D let E D be the probability that E happens on either of the draws in D . We prove the result by induction on |D|. The inductive hypothesis clearly holds for</p><formula xml:id="formula_36">|D| = 0 as then P (E D ) = 0 = 1 -1 = 1 -exp(0) = 1 -exp -d∈D ρ d .</formula><p>Now suppose the inductive hypothesis holds for |D| = l for some l. We now show that it holds for |D| = l + 1 which will complete the proof. To show this choose some d ∈ D and set D = D \ {d} We have:</p><formula xml:id="formula_37">P (¬E D ) = P (¬E D ∧ ¬E d ) = P (¬E D ) P (¬E d ) = P (¬E D ) (1 -P (E d )) (1) = P (¬E D ) (1 -ρ d ) ≤ P (¬E D ) exp(-ρ d ) ≤ (1 -P (E D )) exp(-ρ d ) ≤ exp - d∈D ρ d exp(-ρ d )<label>(2)</label></formula><p>= exp -</p><formula xml:id="formula_38">d∈D ρ d</formula><p>Where Equation 1 is due to the independence of the draws and Equation 2 is from the inductive hypothesis (noting |D | = l). We now have</p><formula xml:id="formula_39">P (E D ) = 1 -P (¬E D )<label>(3)</label></formula><formula xml:id="formula_40">≥ 1 -exp - d∈D ρ d (4)</formula><p>which proves the inductive hypothesis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proof of Theorem 5.2</head><p>Proof. Note that the event Z ∩X t = ∅ happens if either of {ξ t q,k : q ∈ N, k ≤ ζ t q } are in Z. For every q ∈ N and k ∈ [ζ t q ] we have:</p><formula xml:id="formula_41">P ξ t q,k ∈ Z = i∈Z∩Ωq P ξ t q,k = i = i∈Z∩Ωq ω t i π t q</formula><p>and for k = ζ t q + 1 we similarly have:</p><formula xml:id="formula_42">P ξ t q,k ∈ Z = (δπ t q -δπ t q ) i∈Z∩Ωq ω t i π t q</formula><p>So letting ρ (q,k) := P ξ t q,k ∈ Z we have:</p><formula xml:id="formula_43">ζ t q +1 k=1 ρ (q,k) = ζ t q k=1 i∈Z∩Ωq ω t i π t q + (δπ t q -δπ t q ) i∈Z∩Ωq ω t i π t q</formula><p>Stephen Pasteris, Fabio Vitale, Kevin Chan, Shiqiang Wang, Mark Herbster</p><formula xml:id="formula_44">=ζ t q i∈Z∩Ωq ω t i π t q + (δπ t q -δπ t q ) i∈Z∩Ωq ω t i π t q =(ζ t q + δπ t q -δπ t q ) i∈Z∩Ωq ω t i π t q =δπ t q i∈Z∩Ωq ω t i π t q =δ i∈Z∩Ωq ω t i</formula><p>So, plugging into Lemma 5.1 with D := {(q, k) : q ∈ N, k ∈ [ζ t q + 1]}, we get:</p><formula xml:id="formula_45">P Z ∩ X t = ∅ = P ∃i ∈ Z : i ∈ X t ≥ 1 -exp - d∈D ρ d = 1 -exp   - ∞ q=1 ζ t q +1 k=1 ρ (q,k)   = 1 -exp   -δ ∞ q=1 i∈Z∩Ωq ω t i   = 1 -exp -δ i∈Z ω t i</formula><p>Proof of Theorem 5.3</p><p>Proof. From Theorem 5.2 we have P (i ∈ X t ) = P ({i} ∩ X t = ∅) ≥ 1 -exp(-δω t i ). Choosing q ∈ N such that i ∈ Ω q we also have:</p><formula xml:id="formula_46">P i ∈ X t ≤ ζ t q +1 k=1 P ξ t q,k = i = ζ t q ω t i /π t q + (δπ t q -δπ t q )ω t i /π t q = δπ t q ω t i /π t q = δω t i Proof of Theorem 5.<label>7</label></formula><p>Proof. For all j ∈ [n -1] the function δ j k=1 γ σ(t,k) is concave and the function (1 -exp(-x)) is concave and monotonic increasing which implies their combination, (1 -exp -δ j k=1 γ σ(t,k) ), is concave. Hence, as r t σ(t,j) -r t σ(t,j+1) ≥ 0, f t j is concave. Similarly, as all components of c t-are negative, we have that n i=1 c t- i (1 -exp(-δγ i )) is convex. Since also the function c t+ • γ is convex we then have that h t is a positive sum of convex functions and is therefore convex.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proof of Lemma 5.6</head><p>Proof. Let l := min{j ∈ [n] : σ(t, j) ∈ X t }. Note that r t σ(t,l) = max i∈X t r t i . From the definition of l we have:</p><formula xml:id="formula_47">• For all j &lt; l, I (∃k ≤ j : σ(t, k) ∈ X t ) = 0 • For all j ≥ l, I (∃k ≤ j : σ(t, k) ∈ X t ) = 1</formula><p>This implies that:</p><formula xml:id="formula_48">n j=1 r t σ(t,j) -r t σ(t,j+1) I ∃k ≤ j : σ(t, k) ∈ X t = n j=l r t σ(t,j) -r t σ(t,j+1) =r t σ(t,l) = max i∈X t r t i</formula><p>Proof of Theorem 5.9</p><p>Proof. For all j ∈ [n], Theorem 5.2 with Z := {σ(t, k) : k ≤ j} implies that:</p><formula xml:id="formula_49">P ∃k ≤ j : σ(t, k) ∈ X t =P {σ(t, k) : k ≤ j} ∩ X t = ∅ ≥1 -exp -δ j k=1 ω t σ(t,k)</formula><p>Lemma 5.8 then gives us:</p><formula xml:id="formula_50">E max i∈X t r t i ≥ n j=1 r t σ(t,j) -r t σ(t,j+1) 1 -exp -δ j k=1 ω t σ(t,k) = n j=1 f t j (ω t )</formula><p>By Theorem 5.3 we also have:</p><formula xml:id="formula_51">i∈[n]:c t i &lt;0 c t i P i ∈ X t ≤ i∈[n]:c t i &lt;0 c t i (1 -exp(-δω t i )) = i∈[n] c t- i (1 -exp(-δω t i ))</formula><p>and:</p><formula xml:id="formula_52">i∈[n]:c t i &gt;0 c t i P i ∈ X t ≤ i∈[n]:c t i &gt;0 c t i δω t i = i∈[n] c t+ i δω t i =δ c t+ , ω t so: n i=1 c t i P i ∈ X t ≤ δ c t+ , ω t + i∈[n] c t- i (1-exp(-δω t i ))</formula><p>Hence, we have:</p><formula xml:id="formula_53">E µ t (X t ) =E max i∈X t r t i - i∈X t c t i =E max i∈X t r t i - n i=1 c t i I i ∈ X t =E max i∈X t r t i - n i=1 c t i E I i ∈ X t (5) =E max i∈X t r t i - n i=1 c t i P i ∈ X t ≥ n j=1 f t j (ω t ) - n i=1 c t i P i ∈ X t ≥ n j=1 f t j (ω t ) -δ c t+ , ω t - i∈[n] c t- i (1 -exp(-δω t i )) = -h t (ω t )</formula><p>where Equation <ref type="formula">5</ref>is due to linearity of expectation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proof of Theorem 5.11</head><p>Proof. Suppose we have some l ∈ [n]. For j &lt; l we have ∂ σ(t,l) f t j (ω t ) = 0 and for j ≥ l we have</p><formula xml:id="formula_54">∂ σ(t,l) f t j (ω t ) =δ r t σ(t,j) -r t σ(t,j+1) exp -δ j k=1 ω t σ(t,k) =δ r t σ(t,j) -r t σ(t,j+1) t j</formula><p>This implies that: Proof of Theorem 5.18</p><formula xml:id="formula_55">∂ σ(t,l) n j=1 f t j (ω t ) = δ n j=l r t σ(t,j) -</formula><p>Proof. From Lemma 5.16 we have:</p><formula xml:id="formula_56">T t=1 η t g t 2 ≤ T t=1 n s/(2t) = n s/2 T t=1 1 √ t ≤ n s/2 1 + T t=1 1 √ t = n s/2(2 √ T -1) ≤ 2n sT /2</formula><p>So using Lemma 5.17 and plugging into Theorem 5.14 we have:</p><formula xml:id="formula_57">T t=1 (h t (ω t ) -h t (φ)) ≤ 1 2 n √ 2sT + n sT /2 = 2n sT /2 = n √ 2sT</formula><p>By Definition 5.15 and Lemma 5.12 we have s ≤ δ 2 (r + ĉ) 2 . Plugging this into the above gives us the result.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B When max i∈[n] z i is large</head><p>When max i∈[n] z i is large we construct X t from ω t i in the following way:  We defer the analysis.</p><formula xml:id="formula_58">1. Define A = {i ∈ [n] : z i ∈ [1/2,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C On the Knapsack Problem and Component Hedge</head><p>In this section we show that Component Hedge cannot solve the online knapsack problem special case in polynomial time unless P = N P .</p><p>The Component Hedge algorithm described in <ref type="bibr" target="#b12">[13]</ref> requires a set of concepts X ⊆ {0, 1} n . In the knapsack problem, X is the subset of all x that fit in the knapsack, i.e. those x ∈ {0, 1} d with x, z ≤ 1. For Component Hedge, the convex hull H of X must have a number of constraints polynomial in n and that any y ∈ H can, in polynomial time, be decomposed into a convex combination of n + 1 concepts. We now show that if this is true then we can solve the knapsack problem in polynomial time:</p><p>Let c be the vector defining the objective function of the knapsack problem (i.e. we seek the x ∈ X that maximises c, x ). Now, since H has polynomial constraints we can efficiently choose the maximiser of c, y (for y ∈ H) via linear programming. Let this maximiser be y. Now decompose y into a convex combination, n+1 i=1 (m i x i ), of n + 1 concepts x i (where n+1 i=1 m i = 1). Now suppose, for contradiction, that c, x j &lt; c, y for some j with m j &gt; 0. Without loss of generality let j = n + 1. Hence, for all j with m j &gt; 0 we have c, x j = c, y so the maximising x is found in polynomial time. This contradicts the assumption that P = N P .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Using Submodular Maximisation</head><p>We consider the facility location special case (i.e, z := 0 and all costs are positive). In this case it is possible to obtain our bound by selecting, on a trial t, each action i with probability ω t i . We can then use some definitions in our paper to define the following functions on a trial t:</p><p>• For all j ∈ [n] define f t j (γ) := r t σ(t,j) -r t σ(t,j+1)</p><p>1 -j k=1 1 -γ σ(t,k)</p><p>• Define ĥt (γ) = n j=1 f t j (γ) -c t , γ</p><p>We can then use the arguments in our paper to write the expected profit as: E (µ t (X t )) = ĥt (ω t ) which is continuous submodular. We can use a slight modification of our method of computing the gradient of h t to compute the gradient of ĥt in quasi-linear time. With this in hand we can then plug the gradient into one of the algorithms of <ref type="bibr" target="#b2">[3]</ref>:</p><p>• The first algorithm requires the submodular function to be monotone and non-negative whereas ours, in general, is neither. Even if our expected profit was monotone and non-negative, for their algorithm to have a regret linear in √ T their per-trial time complexity bound becomes Ω(n √ T ) which is much larger than ours.</p><p>• In the gradient ascent based second algorithm, the submodular function also needs to be monotone and non-negative but the cost vector can be separated from the reward vector in the analysis allowing us to use the expected maximum reward which is monotone non-negative. The result of this analysis is similar to ours expect that for them α = 1/2 instead of the better α = 1 -e -1 that we have (NB: α is the discount on the comparator selection S).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>5 .</head><label>5</label><figDesc>If tails define β := 1/2 and run Algorithm 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Then, since for all x ∈ X with have c, x ≤ c, y (as X ⊂ H) we have: c, y = c, (n+1 i=1 m i x i ) = n+1 i=1 m i c, x i = n i=1 m i c, x i + m n+1 c, x n+1 &lt; n i=1 m i c, x i + m n+1 c, y ≤ n i=1 m i c, y + m n+1 c, y = n+1 i=1 m i c, y = c,y which is a contradiction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>2 , • • • , r T ∈ P. A feasible solution is any X ⊆ [n]. The aim is to maximise the objective function: An example of the problem is as follows. There are n sites and T users, all located in some metric space. We have to choose a set X of sites to open a facility on. Opening a facility on site i costs us c i . Each user pays us a reward based on how near it is to the closest open facility. If the nearest open facility to user t is at site i then user t rewards us r t i . The objective is to maximise the total profit.</figDesc><table><row><cell>T</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>t=1</cell><cell>max i∈X</cell><cell>r t i -</cell><cell>i∈X</cell><cell>c i</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>Proceedings of the 22nd International Conference on Artificial Intelligence and Statistics (AISTATS) 2019, Naha, Okinawa, Japan. PMLR: Volume 89. Copyright 2019 by the author(s).</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A constant-factor approximation algorithm for the kmedian problem</title>
		<author>
			<persName><forename type="first">M</forename><surname>Charikar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Guha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Tardos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Shmoys</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Symposium on Theory of Computing (STOC)</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Improved combinatorial algorithms for the facility location and k-median problems</title>
		<author>
			<persName><forename type="first">M</forename><surname>Charikar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Guha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Foundations of Computer Science</title>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="378" to="388" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Online Continuous Submodular Maximization</title>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hassani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Karbasi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics, AISTATS</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">The uncapacitated facility location problem</title>
		<author>
			<persName><forename type="first">G</forename><surname>Cornuejols</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">L</forename><surname>Nemhauser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Wolsey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">itors, Discrete Location Theory</title>
		<editor>
			<persName><forename type="first">B</forename><surname>Pitu</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Richard</forename><forename type="middle">L</forename><surname>Mirchandani</surname></persName>
		</editor>
		<editor>
			<persName><surname>Francis</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="page" from="119" to="171" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Online Facility Location with Deletions</title>
		<author>
			<persName><forename type="first">M</forename><surname>Cygan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Czumaj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mucha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sankowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual European Symposium on Algorithms</title>
		<imprint>
			<publisher>ESA</publisher>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Discrete variable extremum problems</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">B</forename><surname>Dantzig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Operations Research</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="266" to="277" />
			<date type="published" when="1957">1957</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A decision-theoretic generalization of on-line learning and an application to boosting</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Freund</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Schapire</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computer and System Sciences</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="page" from="119" to="139" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">E Takimoto Combinatorial Online Prediction via Metarounding</title>
		<author>
			<persName><forename type="first">T</forename><surname>Fujita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hatano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Algorithmic Learning Theory</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="68" to="82" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Online Submodular Maximization under a Matroid Constraint with Application to Learning Assignments In Technical report</title>
		<author>
			<persName><forename type="first">D</forename><surname>Golovin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Streeter</surname></persName>
		</author>
		<idno>arXiv</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Approximation algorithms for metric facility location and k-median problems using the primal-dual schema and Lagrangian relaxation</title>
		<author>
			<persName><forename type="first">K</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vazirani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. ACM</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="274" to="296" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Efficient algorithms for online decision problems</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kalai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Vempala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computer and System Sciences</title>
		<imprint>
			<biblScope unit="volume">71</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="291" to="307" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Playing games with approximation algorithms</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kakade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kalai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ligett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Symposium on the Theory of Computing</title>
		<imprint>
			<publisher>STOC</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="546" to="555" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Hedging structured concepts</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">M</forename><surname>Koolen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>Warmuth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kivinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Learning Theory</title>
		<imprint>
			<publisher>Omnipress</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="239" to="254" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Breakpoint searching algorithms for the continuous quadratic knapsack problem</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">C</forename><surname>Kiwiel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Math. Program</title>
		<imprint>
			<biblScope unit="volume">112</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="473" to="491" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Constant factor approximation algorithm for the knapsack median problem</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM-SIAM Symposium on Discrete Algorithms (SODA)</title>
		<imprint>
			<publisher>SIAM</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="824" to="832" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Online submodular minimization</title>
		<author>
			<persName><forename type="first">E</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kale</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Online Learning of Combinatorial Objects via Extended Formulation</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Helmbold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">V N</forename><surname>Vishwanathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Algorithmic Learning Theory</title>
		<meeting><address><addrLine>ALT</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Distributed Placement of Service Facilities in Large-Scale Networks</title>
		<author>
			<persName><forename type="first">N</forename><surname>Laoutaris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Smaragdakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Oikonomou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Stavrakakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bestavros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE INFOCOM</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="2144" to="2152" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">An upper bound for the zeroone knapsack problem and a branch and bound algorithm</title>
		<author>
			<persName><forename type="first">S</forename><surname>Martello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Toth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">European Journal of Operational Research</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="169" to="175" />
			<date type="published" when="1977">1977</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Online Facility Location</title>
		<author>
			<persName><forename type="first">A</forename><surname>Meyerson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Symposium on Foundations of Computer Science</title>
		<imprint>
			<publisher>FOCS</publisher>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Approximation algorithms for facility location problems</title>
		<author>
			<persName><forename type="first">D</forename><surname>Shmoys</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Tardos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Symposium on Theory of Computing (STOC 1997)</title>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="265" to="274" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">An Online Algorithm for Maximizing Submodular Functions</title>
		<author>
			<persName><forename type="first">M</forename><surname>Streeter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Golovin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Neural Information Processing Systems, NIPS</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Online Convex Optimization with Stochastic Constraints</title>
		<author>
			<persName><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Neely</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Neural Information Processing Systems, NIPS</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Online convex programming and generalized infinitesimal gradient ascent</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zinkevich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Service Placement with Provable Guarantees in Heterogeneous Edge Computing Systems</title>
		<author>
			<persName><forename type="first">S</forename><surname>Pasteris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Herbster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE INFOCOM</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
