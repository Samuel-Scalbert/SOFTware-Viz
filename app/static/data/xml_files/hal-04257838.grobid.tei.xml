<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">FactSpotter: Evaluating the Factual Faithfulness of Graph-to-Text Generation</title>
				<funder ref="#_v6bmYXC #_2d2Pa7u">
					<orgName type="full">Agence Nationale de la Recherche</orgName>
					<orgName type="abbreviated">ANR</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Kun</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Inria</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Institut Polytechnique de Paris</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Oana</forename><surname>Balalau</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Inria</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Institut Polytechnique de Paris</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ioana</forename><surname>Manolescu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Inria</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Institut Polytechnique de Paris</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">FactSpotter: Evaluating the Factual Faithfulness of Graph-to-Text Generation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">8DF3FE867662EDCDA70BD650A4A16C12</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-04-12T14:51+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Graph-to-text (G2T) generation takes a graph as input and aims to generate a fluent and faithful textual representation of the information in the graph. The task has many applications, such as dialogue generation and question answering. In this work, we investigate to what extent the G2T generation problem is solved for previously studied datasets, and how proposed metrics perform when comparing generated texts. To help address their limitations, we propose a new metric that correctly identifies factual faithfulness, i.e., given a triple (subject, predicate, object), it decides if the triple is present in a generated text. We show that our metric FactSpotter achieves the highest correlation with human annotations on data correctness, data coverage, and relevance. In addition, FactSpotter can be used as a plug-in feature to improve the factual faithfulness of existing models. Finally, we investigate if existing G2T datasets are still challenging for state-of-the-art models. Our code is available online:</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Graph-to-text (G2T) generation is an important task in natural language generation, as it renders graphs, and in particular knowledge graphs, accessible to non-technical users in downstream applications such as question answering <ref type="bibr" target="#b17">(Gu et al., 2021)</ref>, <ref type="bibr" target="#b32">(Romero and Razniewski, 2020)</ref>, knowledge-grounded dialogue generation <ref type="bibr" target="#b43">(Zhou et al., 2018)</ref>, and document summarization <ref type="bibr" target="#b15">(Fan et al., 2019)</ref>. In recent years, there have been several datasets <ref type="bibr" target="#b16">(Gardent et al., 2017;</ref><ref type="bibr" target="#b25">Nan et al., 2021)</ref> and methods proposed for G2T generation <ref type="bibr" target="#b19">(Ke et al., 2021;</ref><ref type="bibr" target="#b30">Ribeiro et al., 2021)</ref>, in addition to G2T competitions <ref type="bibr" target="#b35">(Shimorina et al., 2018;</ref><ref type="bibr" target="#b5">Castro Ferreira et al., 2020)</ref>. Evaluating text generation is a challenging task in itself <ref type="bibr" target="#b6">(Celikyilmaz et al., 2020)</ref>; moreover, in the context of G2T generation, we are concerned not only with the fluency of the gener-ated text, but also with its faithfulness to the input graph. While recent models such as T5 and GPT models are very fluent, they have been criticized for their factual accuracy, a problem commonly referred to as hallucination <ref type="bibr" target="#b18">(Ji et al., 2023;</ref><ref type="bibr" target="#b24">Liu et al., 2022)</ref>. Hallucinations are a serious drawback in G2T, where the generated text should only contain facts mentioned in the input graph.</p><p>In this work, we focus on measuring and improving the factual accuracy of G2T generative models. More precisely, our contributions are as follows: i) We introduce a novel metric FactSpotter for detecting if G2T generations are faithful to the facts present in the input graph; ii) We show how FactSpotter can be used in the inference step of any G2T model to improve its generations; iii) We analyze the difficulty of existing G2T datasets and determine which are (resp., are no longer) challenging for state-of-the-art models. FactSpotter can be extended to other data-to-text tasks via methods for transforming a relational dataset into RDF, such as the R2RML language<ref type="foot" target="#foot_0">1</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Graph-to-text (G2T) generation</head><p>In <ref type="bibr" target="#b30">(Ribeiro et al., 2021)</ref>, the authors investigate the potential of pretrained language models (PLM) on the G2T task. They consider two Transformerbased models <ref type="bibr" target="#b37">(Vaswani et al., 2017)</ref> with an encoder-decoder structure: T5 <ref type="bibr" target="#b28">(Raffel et al., 2020)</ref>, and Bart <ref type="bibr" target="#b22">(Lewis et al., 2020)</ref>. The models receive in input a linearized (serialized) version of the input graph, in which they add the tags ⟨H⟩, ⟨R⟩, and ⟨T ⟩ before the head entity, the relation, and tail entity of a triple, respectively.</p><p>The potential of large language models is further investigated in <ref type="bibr" target="#b20">(Keymanesh et al., 2022)</ref> on the DART dataset <ref type="bibr" target="#b25">(Nan et al., 2021)</ref>, a data-totext dataset constructed from tables, available in a triple-to-sentence format. The dataset is constructed from a combination of manual and automatic techniques. The authors empirically evaluate the GPT2 model <ref type="bibr" target="#b27">(Radford et al., 2019)</ref> and the T5 model on the dataset by varying the amount of supervision a model receives. They also investigate the potential of adding predicate descriptors in the prompt and re-ranking generations. In a small-scale human evaluation, they find that their best model, T5 large, outperforms the reference text regarding hallucinations and fluency, underlining that existing datasets suffer from poor human annotations, which we also observe and discuss in Section 7.</p><p>The authors of <ref type="bibr" target="#b19">(Ke et al., 2021)</ref> propose modifying the Transformer model by adding extra attention and pooling layers to improve G2T generation. In addition, the model's pretraining has three steps given the input graph and the expected output text: 1) reconstructing the text sequence given the complete subgraph, 2) predicting the masked entities and relations in the corrupted subgraph, given the complete text, and, 3) aligning the embedding vectors of the knowledge graph and the text.</p><p>Most state-of-the-art G2T models are based on Transformers, and they can generally generate fluent texts related to given graphs. Although various baselines designed neural networks to encode both global and local information <ref type="bibr" target="#b31">(Ribeiro et al., 2020)</ref>, they cannot guarantee that generated texts are factual faithful to the given graphs. It's also not clear whether current G2T datasets are still challenging.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Evaluation metrics for text generation</head><p>Alongside the significant improvements that models for G2T generation underwent and, in general, the improvement of language models, new metrics to assess the generations' quality have been proposed <ref type="bibr" target="#b6">(Celikyilmaz et al., 2020;</ref><ref type="bibr" target="#b33">Sai et al., 2022)</ref>. G2T generation belongs to the broader field of natural language generation, including tasks such as machine translation, automatic summarization, question answering, and more. Each task has specific requirements, which might entail using some metrics over others. For example, in machine translation, the translation should match the ground-truth text as closely as possible, while in chatting or summarization, adding or removing some information is acceptable or even desirable.</p><p>Evaluation metrics are split into three categories: human-centric metrics, untrained automatic metrics, and machine-learned metrics. Human evalu-ation is the most important of these metrics. It consists of asking users to evaluate the quality of a text in a specific category, such as fluency or correctness. Unfortunately, human evaluation sometimes suffers from a low inter-annotator agreement <ref type="bibr" target="#b6">(Celikyilmaz et al., 2020)</ref>, <ref type="bibr" target="#b2">(Belz and Reiter, 2006)</ref> as different people might have different notions of what makes a text fluent or correct, or the instructions they receive for annotation might lack sufficient clarity. Also, human annotation is time and money-consuming; it can represent a bottleneck in the iterative process of improving a model. Hence the need for automatic metrics such as BLEU <ref type="bibr" target="#b26">(Papineni et al., 2002)</ref>, ROUGE <ref type="bibr" target="#b23">(Lin, 2004)</ref>, METEOR <ref type="bibr" target="#b1">(Banerjee and Lavie, 2005)</ref>, BERTScore <ref type="bibr" target="#b41">(Zhang et al., 2019)</ref>, BLEURT <ref type="bibr" target="#b34">(Sellam et al., 2020)</ref> BARTScore <ref type="bibr" target="#b40">(Yuan et al., 2021</ref><ref type="bibr">), In-foLM (Colombo et al., 2022)</ref>, among others. Overlap measures based on n-grams, such as BLEU, ROUGE, and METEOR, have been widely used in the literature, while recently proposed metrics based on word embeddings, such as BERTScore, BLEURT, BARTScore are gaining traction. The embedding-based measures have been shown to correlate better with human evaluation than the ngrams metrics. In addition, some metrics, such as BLEURT, have been trained on human annotations. As these automatic metrics are based on distances or similarities to ground-truth texts, they rely on the quality of annotated sentences.</p><p>Apart from the works mentioned above, a few prior studies assessed factual faithfulness in graphto-text generation. In <ref type="bibr" target="#b14">(Faille et al., 2021)</ref> the authors introduce a metric for verifying whether entities in input graphs are represented in the generated texts. However, this work does not evaluate the quality of the predicates in the generated texts, which is a much more difficult task. In <ref type="bibr" target="#b29">(Rebuffel et al., 2021)</ref>, a question generation (QG) and question answering (QA) framework is employed to evaluate the quality of the generated text by determining if each question posed by the QG module can be addressed by the QA module. We believe our contribution can further advance the state-ofthe-art as: i) FactSpotter requires significantly less computational resources; ii) FactSpotter is self supervised, thus it does not requires additional data to the G2T model; item FactSpotter can be pluged into a G2T to improve its generation.</p><p>A knowledge graph (KG) consists of facts (or data triples) of the form ⟨subject, relation, object/literal⟩ and/or an ontology describing the properties that hold between classes and/or relations.</p><p>Graph-to-text. A G2T tool takes in input a graph and outputs a textual representation of the graph. G2T inputs are often subgraphs of larger real-world graphs, such as knowledge graphs. The textual representation should be fluent and should contain all the facts present in the input graph. For example, given a subgraph consisting of the single DBpedia <ref type="bibr" target="#b0">(Auer et al., 2007)</ref> fact ⟨The Myth of Sisyphus, author, Albert Camus⟩, we would like to generate the sentence The Myth of Sisyphus was written by Albert Camus. This work primarily focuses on creating textual representations of KG subgraphs.</p><p>Factual faithfulness. The following human criteria have been proposed to evaluate the factual quality of a generated text <ref type="bibr" target="#b5">(Castro Ferreira et al., 2020)</ref>: data correctness, data coverage, and relevance. Given a graph G and a machine-generated text T , the generated text is characterized by: 1. Data coverage or recall: are all the descriptions presented in the graph G included in the text T ? i) Covered predicates: does T contain all the predicates from G? ii) Covered entities: does T contain all the entities from G?</p><p>2. Relevance or precision: i) Relevant predicates: does T only contain relevant predicates? ii) Relevant entities: does T only contain relevant entities?</p><p>3. Correctness: are predicates correctly mentioned and adequately introduced in the data?</p><p>Research questions. We focus on the following three research questions:</p><p>RQ1 What metric would better correlate with the factual faithfulness of generated text?</p><p>RQ2 Can we improve factual faithfulness of G2T?</p><p>RQ3 Is the G2T task solved on existing datasets?</p><p>4 FactSpotter: An explainable metric for factual faithfulness</p><p>In this section, we introduce a new metric for factual faithfulness. A good metric should be interpretable, that is: given in input a fact f of the form ⟨subject, predicate, object⟩ and a text T , it assigns a score between 0 and 1, where a score close to 0 signifies that the text that does not correctly represent the fact, and a score close to 1 rewards a factual faithful textual representation of f in T . Such a metric can be used to compare different generation systems and, in addition, assess a single system on a given dataset, to determine how close the system is to representing the input graphs correctly.</p><p>The intuition of our score is the following. We train a model to perform a task simpler than G2T generation: only detecting whether facts exist in a sentence and whether they are well expressed. This simpler model can then be used as a plug-in feature by any existing G2T model, to aid it perform the more complex task of language generation.</p><p>Our metric, FactSpotter, is trained as a binary classifier. Given in input a fact ⟨subject, predicate, object⟩ and a sentence, it should predict 1 if the fact is well expressed in the sentence, or 0 otherwise. Thus, FactSpotter is inherently interpretable. We leverage as classification models recent large language models, capable of detecting semantic closeness, even if different words, e.g., synonyms, are used. This approach is similar to the one taken to compute metrics such as BertScore and BartScore. Given an input G2T dataset D, with a training (train) set, a development (dev) set, and a test set, we create the training set as follows:</p><p>Positive samples. Given an instance of the training set of the form (graph G, ground truth text T ), for each fact (triple) in G, we create a positive sample of the form (fact f , ground truth text T ).</p><p>Negative samples. Given an instance of the training set of the form (graph G, ground truth text T ), for each fact f ∈ G, we create negative samples as follows: i) Type I: we perturb the fact f : we change its predicate, or an entity (subject or object), or both, while the ground truth text T remains unchanged. ii) Type II: we perturb the ground truth text T : we drop one or both entities related to f from the text, or drop the n-grams most similar to the predicate of f , or we apply simultaneously several modifications, keeping the fact unchanged.</p><p>For example, given the fact ⟨The Myth of Sisyphus, author, Albert Camus⟩ and its associated text "The Myth of Sisyphus was written by Albert Camus", a Type I negative sample alters the fact (⟨The Myth of Sisyphus, author, Simone de Beauvoir⟩, "The Myth of Sisyphus was written by Albert Ca-mus"), while for Type II yields the sample ( ⟨The Myth of Sisyphus, author, Albert Camus⟩, "The Myth of Sisyphus was written"). We associate probabilities to each perturbation, and control the generation such that for each positive sample, we only generate one negative sample. To allow our classifier to learn from different negative samples and avoid over fitting <ref type="bibr" target="#b7">(Chen et al., 2020)</ref>, for each training epoch, we use a newly generated set of negative samples. The development set is built in the same way. Through evaluation on a fixed test set (Appendix A.4.1) we find that the model which best detects factual faithfulness is the one with the highest probability of perturbing the fact (90%) and 10% the probability of perturbing the ground truth.</p><p>Above, we have described FactSpotter as a (trained) classifier. To use it as a score (metric), we take the output of the model after the softmax layer. The final score of a generated text T given an input graph G is the average over the scores for each pair (fact f ∈ G, generated text T ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Parameters and performance of FactSpotter.</head><p>As we aim to add our metric, FactSpotter, in the inference step of graph-to-text generation, we prefer small language models. Hence, we select the small Electra model <ref type="bibr" target="#b9">(Clark et al., 2020)</ref>. We have experimented with other small models, such as DistilBERT and DistilRoBERTa, but we did not observe an improvement. We train our classifier for 16 epochs, with a learning rate of 5 • 10 -5 , and the AdamW optimizer. We describe in the Appendix A.4.1 how we chose the percentages of negative samples for FactSpotter.</p><p>The performance of FactSpotter on the test splits across multiple datasets is detailed in Table <ref type="table" target="#tab_0">1</ref>, with accuracy and F1 score. We also report numbers of true positives/negatives (TP/TN), and false positives/negatives (FP/FN) in the </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Evaluating graph-to-text generation</head><p>We investigate our first research question (RQ1): which metrics would better correlate with the fac-tual faithfulness of the generated text? For this question, we compare FactSpotter with: BLEU <ref type="bibr" target="#b26">(Papineni et al., 2002)</ref>, METEOR <ref type="bibr" target="#b1">(Banerjee and Lavie, 2005)</ref>, BERTScore <ref type="bibr" target="#b41">(Zhang et al., 2019)</ref>, BLEURT <ref type="bibr" target="#b34">(Sellam et al., 2020)</ref>, BARTScore <ref type="bibr" target="#b40">(Yuan et al., 2021)</ref>. The only metric that is not normalized is BARTScore. This metric has a variant specifically adapted for factual faithfulness, BARTScorefaithful. Further details of these metrics are provided in Appendix A.1. We calculate the system-level correlation between automatic metrics and human annotations. Given S systems under evaluation, for a certain dimension, e.g., fluency, we compute the correlation between the system-level automatic metric scores [M (S 1 ), . . . M (S S )] and the corresponding system-level human scores [H(S 1 ), . . . H(S S )], where M (S i ) is the score of the automatic metric on the texts from system S i , and H(S i ) is the score of the human annotation on the same result. Similarly to <ref type="bibr" target="#b10">(Colombo et al., 2022)</ref>, we compute three correlation metrics: Pearson correlation (r), Spearman correlation (ρ), and Kendall's Tau (τ ). To test if a metric M 1 has a higher correlation with human annotations than M 2 , we use the bootstrapping technique proposed in (Wilcox, 2016), which we describe in the Appendix A.2. We also report the sentence-level correlation in Appendix A.5. In addition to automatic measures, we report the correlation between one annotator and the average score of the remaining annotators, which should be a upper bound on the correlation we can obtain using automatic measures.</p><p>WebNLG 2017. In the WebNLG 2017 challenge <ref type="bibr" target="#b35">(Shimorina et al., 2018)</ref>, the organizers annotated 9 submissions on semantic adequacy (the text correctly represents the meaning in the data), text structure (as above, referred in the original paper as grammar) and fluency (as above). This annotation has carried over 223 samples.</p><p>WebNLG 2020. After the WebNLG 2020 Challenge <ref type="bibr" target="#b5">(Castro Ferreira et al., 2020)</ref>, the organizers annotated the 16 participating systems on data correctness (the predicates found in the data are correctly mentioned together with their subject and object), data coverage (the text includes descriptions of all predicates presented in the data), and relevance (the text describes only those predicates with related subjects and objects which are in the data), in addition to text structure (the text is grammati-  cal, well structured, and written in good English) and fluency (the text progresses naturally, forms a coherent whole and is easy to understand). 178 generations of each system were annotated.</p><p>Results. On WebNLG 2020, Table <ref type="table" target="#tab_1">2</ref> shows that FactSpotter has the best performance on factual faithfulness, significantly improving relevance. BLEU, METEOR, BERTScore and BLEURT reach similar fluency and text stucture scores. For the results on WebNLG 2017 in Table <ref type="table" target="#tab_2">3</ref>, FactSpotter has the highest performance on semantic adequacy, which is the only dimension related to factual faithfulness. For text structure and fluency, BLEURT obtains the best results, although the results are not statistically significant. Overall, previous metrics are better on text structure and fluency, which are generally considered as no longer a bottleneck for large language models. FactSpotter is the best suited metric on factual faithfulness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Improving the factual faithfulness of graph-to-text generation</head><p>In this section, we investigate the answer to our third research question: can we improve graph-totext generation on factual faithfulness (RQ2)? For this, we first explain how to improve the inference step of any G2T model using FactSpotter, and then we present the results of this technique on the stateof-the-art models for G2T generation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Improving models' factual faithfulness</head><p>Let M be a neural network G2T (seq2seq) model that, given an input sequence x = x 1 , x 2 , . . . x M , produces an output sequence y = y 1 , y 2 , . . . , y N , where x i ∈ V x , y i ∈ V y , and V x , V y are the vocabularies from where we select items in the input, respectively output sequence. In the inference step, the model generates the sequence y that maximizes:</p><formula xml:id="formula_0">P (y|x) = N i=1 P (y i |y &lt;i , x)</formula><p>In practice, for computational efficiency, the log of the probabilities are typically utilized in beam search. We use the following method to improve factual faithfulness in G2T inference without retraining the model.</p><p>Given:i) a graph-to-text generation model M, ii) our factual faithfulness classifier, i.e., FactSpotter, iii) a subgraph G composed of F facts, we encourage factual generations by modifying the prediction step as follows:</p><formula xml:id="formula_1">log(P f (y i |y &lt;i , x)) = λ F j=1</formula><p>(1 -P f act j (y &lt;i-1 )) log P f act j (y &lt;i ) + log(P (y i |y &lt;i , x)) (1) where: i) P f (y i |y &lt;i , x) is the probability of generating token y i given the factual classifier; ii) P f act j (y &lt;i-1 ) is the probability of correctly representing the fact j in the previously generated tokens y 0 , ..., y i-1 , computed by FactSpotter. iii) P (y i |y &lt;i , x) is the probability for generating the next token based on previous i -1 tokens.</p><p>In our equation, a fact j is encouraged only if we have not observed in the text generated until step i, according to P f act j (y &lt;i-1 ). Then adding token y i would increase the probability of the text including the fact j, P f act j (y &lt;i ). When P f act j (y &lt;i-1 ) is small, the equation encourages the selection of words belonging to the fact j. As P is large and 1 -P f act j (y &lt;i-1 ) tends to 0, then we can consider that the fact j already appears in the text, and words that satisfy fact j will no longer be encouraged. The weight λ controls the influence of the FactSpotter on the prediction of the following words. A high λ might yield a factual text, but not necessarily a fluent one. We generate tokens till we have generated text S = y 0 , ..., y k for which ∀j ∈ F, P f act j (S) &gt; 0.5, i.e., the probability of each fact j in G is verbalized in the text S is over 0.5, the standard positive threshold.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Models</head><p>We consider for our evaluation state-of-the-art models for G2T genetation, PLM <ref type="bibr" target="#b30">(Ribeiro et al., 2021)</ref> and JointGT <ref type="bibr" target="#b19">(Ke et al., 2021)</ref>. The former investigates how a standard seq2seq model can perform on G2T, given a carefully constructed representation of a graph. The latter proposes a more complex neural network, with built-in attention layers specialized on graph structured inputs. Both are initialized with the pretrained weights of a language model. Similar to the authors, we consider in this work T5 <ref type="bibr" target="#b28">(Raffel et al., 2020)</ref> for both G2T models. For simplicity we refer to the first model as T5, and to the second model as JointGT. We refer to the models modified as explained in Section 6.1 as FactT5 and FactJointGT. For T5 and FactT5, we initialize the weights with T5 small, and T5 base. For JointGT and FactJointGT we initialize the weights with a pretrained T5 base model offered by the authors of JointGT. For the fine-tuning step (each model is fine-tuned on the training split of the datasets), we train the small models with a learning rate of 10 -4 and a batch size of 32, while the base models are trained with a learning rate of 5 • 10 -5 and a batch of 16. We use a beam size of 5 and the AdamW optimizer for both sizes of models. For Equation 1, we fix the weight λ = 0.15 (parameter tuning in Appendix A.4.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Datasets</head><p>To evaluate G2T performance, we need (graph, text) pairs datasets. The graphs can be directly extracted from an underlying knowledge graph or adapted to have richer semantics, such as query graphs <ref type="bibr" target="#b39">(Yih et al., 2015)</ref>. The text associated to the graph should be a sentence or a paragraph verbalizing all the information contained in the subgraph.</p><p>Several datasets are proposed in the literature for the G2T task, such as WebNLG <ref type="bibr" target="#b16">(Gardent et al., 2017)</ref>. Many question-answering (QA, in short) datasets are also in the form (graph, text). Since question answering datasets can be very large and cover many KG predicates <ref type="bibr" target="#b17">(Gu et al., 2021)</ref>, we also leverage such datasets. To ensure that FactSpotter has never encountered the test data, it is trained exclusively on the training set and evaluated it on the validation split.</p><p>SimpleQuestions <ref type="bibr" target="#b4">(Bordes et al., 2015)</ref> is a QA dataset built on Freebase <ref type="bibr" target="#b3">(Bollacker et al., 2008)</ref>. The dataset contains 108K (triple, question) pairs, where the question corresponds to the subject and predicate from the triple, and the answer is the object of the triple. For example, given the triple (Gulliver's Travels, book/written_work/author, Dean Swift), the question is Who wrote Gulliver's Travels?, with the answer Dean Swift. The dataset covers 76 domains, 741 classes, 89K entities and 2K relations. A Freebase domain is a general area of knowledge such as business, politics, economics, etc. We created our own split for this dataset, where the test set is zero shot: we have not seen the predicates during training. FactSpotter can be trained to correctly classify if a question refers to a triple, even if the object or subject is missing from the question, as we replace the entity with its type.</p><p>GrailQA <ref type="bibr" target="#b17">(Gu et al., 2021)</ref> is also a QA dataset that uses Freebase, created using human annotation. The original dataset contains 64K (triple, question) pairs, however, the test set is not released as the authors have created a QA challenge 2 , hence we use the development set as a test set. The remaining data (training and development) consists of 51K pairs. The authors propose three levels of generalization and split the development and test as follows. 50% of the pairs from held-out domains (Freebase assigns to each entity and predicate a domain, such as music, sports, etc.) are not covered in training: this is the zero-shot split. 25% of the pairs correspond to graphs where the combination of ontology items (classes and predicates) were not covered in training: this is the compositional split. Finally, the remaining 25% are randomly sampled from the same distribution as the training dataset: the i.i.d. split. The i.i.d. and compositional subsets contain only ontology items (classes and predicates) covered in training. For the zero-shot subset, five domains are held out for validation.</p><p>WebNLG <ref type="bibr" target="#b16">(Gardent et al., 2017;</ref><ref type="bibr" target="#b5">Castro Ferreira et al., 2020)</ref> is a text generation dataset on DBPedia, created via human annotation. The dataset consists of (graph, paragraph) pairs, where the graph is a set of DBPedia facts, and the paragraph consists of one or several sentences that describe the graph. We use the 2017 (2.1) and 2020 (3.0) versions of the dataset 3 . The 2017 version of the dataset contains 42K graph-text pairs, and it has two splits, the standard version and the constrained version. In the constrained version, the test set does not contain that a triple occurring in train/dev. In this work we considered only the constrained split, as it is more challenging. The WebNLG 2020 dataset has 40K pairs, which comprises 10 categories that were previously seen and utilized in WebNLG 2017, as well as 5 categories that were unseen in WebNLG 2017 are now incorporated into the seen data of the WebNLG 2020 dataset. It also introduces a new category of data: company.</p><p>DART <ref type="bibr" target="#b25">(Nan et al., 2021)</ref> is a data-to-text dataset based on Wikipedia tables. Since the tables are represented as (subject, predicate, object) triples, it also suits our evaluation. Besides creating tableto-text annotations, the authors also use existing datasets: the QA dataset WikiSQL <ref type="bibr" target="#b42">(Zhong et al., 2017)</ref>, the cleaned E2E <ref type="bibr" target="#b12">(Dušek et al., 2019)</ref> (entityto-entity relations in the restaurant domain), and the original release of the WebNLG dataset for the 2017 challenge <ref type="bibr" target="#b35">(Shimorina et al., 2018)</ref>. The au-2 https://dki-lab.github.io/GrailQA/ 3 https://gitlab.com/shimorina/webnlg-dataset thors align the predicates such that predicates with the same meaning have the same representation. The dataset has 82K instances. We excluded the WikiSQL split as it has been generated automatically and after we performed a manual verification, we observed many low quality ground truth texts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Evaluation</head><p>In this section, we consider (RQ2): Can we improve G2T generation on factual faithfulness?</p><p>Table <ref type="table">4</ref> shows the results on the SimpleQuestions dataset. We generally have a high FactSpotter score, indicating that models are already good at relaying factual information. We can improve the factual faithfulness with F-T5 and FGT without significant compromise on other metrics, implying maintained fluency of texts. Table <ref type="table" target="#tab_3">5</ref> has the highest FactSpotter score from all datasets, which means that we observe the most factual generations on WebNLG 2017, with F-T5 and FGT having slightly higher scores.</p><p>In Table <ref type="table" target="#tab_4">6</ref>, the FactSpotter scores are lower for the WebNLG 2020 test split, although we achieve scores comparable to WebNLG 2017 on its validation split. This discrepancy may be attributed to the difference in distribution between the test and training splits of WebNLG 2020. F-T5B can achieve higher FactSpotter score than T5B without compromising fluency. We observe the same trends for the DART dataset in Table <ref type="table" target="#tab_5">7</ref>.</p><p>In Table <ref type="table" target="#tab_6">8</ref>, all the metrics are higher for the IID split of GrailQA, and in particular FactSpotter can reach 99.5%, hence the models learn to reproduce triples seen in training. For Zero-shot and Compositional splits, larger models are better, and our factual inference improves the score of FactSpotter. We illustrate some improved samples of Zero-shot and Compositional splits in Appendix A.3, and in Appendix A.6, we investigate the impact of varying numbers of triples in the input subgraphs on the quality of the generated text.</p><p>To validate that indeed generations improve using FactSpotter in inference, we select the best FGT-T5 model and we analyse the top 20 phrases where the FactSpotter score improved the most compared to the JGT-T5 generations.</p><p>For the SimpleQuestion datasets, we have 15 generations that are more factual, and 5 generations less factual. For the Zero-shot split of GrailQA, 14 generations are more factual. For its Composi-tional split, we have 13 improved generations, but also 5 that are less factual. For the IID split, 4 generations are improved, others are all rephrased texts. Only 7 samples of IID improve over 0.01 for FactSpotter, so this split is not challenging. For the DART dataset, 6 texts are more factual. DART dataset has samples that ground-truth sentences do not match with graphs, so FactSpotter trained on DART has false positives. For WebNLG 2017 dataset, 11 generations are more factual, others are rephrased texts. WebNLG 2017 is only has 16 generations improve over 0.1 for FactSpotter, whose baseline is very high. For WebNLG 2020, 12 generations are more factual, and 3 are rephrased texts. 5 generations in WebNLG 2020 have higher FactSpotter than baseline generations, but they're still not factual enough.</p><p>For the cases where a generation becomes less factual, this is a consequence of the accuracy of FactSpotter, which we present in Section 4. Given that our metric does not correlate strongly with fluency, we perform a second analysis on generations to observe if there is a decrease in fluency.</p><p>To answer this question, we study the top 20 sentences for which the BLEURT decreased the most in comparison with the original generated question. We do not observe an obvious decrease in fluency on any dataset, the decrease in BLEURT score is due to several other factors: BLEURT has difficulties identifying rephrased sentences, in a few cases the factual faithfulness decreased, and in the remaining cases the generations are more faithful to the input graph than the ground truth sentences, however BLEURT cannot identify it. Hence, we can conclude that adding FactSpotter as a plugin in generation can improve G2T generations on factual faithfulness and does not affect the fluency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Remaining challenges in G2T task</head><p>Finally, we consider (RQ3): is G2T task solved on existing datasets? We have observed high FactSpotter scores in Section 6 on the performances of models. We use FactSpotter to do a more detailed analysis: we investigate what is the percentage of generations in each dataset which had at least a fact considered missing by FactSpotter. A fact is considered as missing if the score of the pair (fact, generated sentence) is lower than 0.5. We obtain the following statistics: 1.94% of texts miss a fact in SimpleQuestions; 7.27% of texts miss at least a fact in DART; 5.79% of WebNLG 2017 texts miss at least one fact, and 12.64% for WebNLG 2020; For GrailQA we have 5.8% for the zero shot split, 4.36% for the compositional split and 1.13 for the IID split. According to the observations, WebNLG 2020 is the most challenging dataset, followed by DART, the zero shot split of GrailQA, and the WebNLG 2017 dataset. In Appendix A.7, taking GrailQA and WebNLG 2017 as examples, we analysed the difficulty of G2T on datasets from different knowledge graphs, by looking into how often predicates and entity names are rephrased or expressed exactly as in the input graph.</p><p>We perform a second evaluation, this time by manually analyzing the output of the models. We consider the worst 20 sentences according to BLEURT and FactSpotter, hence 40 examples per dataset or split. On SimpleQuestions, the generations are fluent, however 22/40 have an incorrect predicate. For GrailQA, in the IID split the predicates are correctly generated, but the models still have difficulties in generating some entity names (16/40). For the zero-shot split, generations suffer from wrong entities and predicates (22/40). The compositional split has several samples with wrong ground truth (6/40 of the worst generations) and 19 out of 40 incorrect generations. For DART, 24 generations are not correct. On WebNLG 2017, from the worst 40 generations, only two might benefit from improved fluency, while in many examples, the generated sentence was more fluent than the ground truth (14/40). Regarding correctness, only 2 out of 40 generations had a missing triple, while two generations incorrectly used a predicate. On WebNLG2020, only one instance exhibits room for improvement in fluency, but 24 instances either omit factual information or contain incorrect facts. Among the 20 outputs with the lowest BLEURT scores, 9 are rephrased texts with correctly explained facts. In contrast, among the 20 outputs with the lowest FactSpotter, only 4 instances fall into this category.</p><p>Based on our manual annotations, we observed that models are able to produce correct generations. However, when the generation is rephrased in respect to the ground truth sentence, metrics like BLEURT, which measure if two sentences are equivalent, struggle to assign high scores. We recall that BLEURT, a normalized metric, gives a score of 1 to equivalent sentences. On the dataset WebNLG 2017, our metric assigns a very high score to the models, while the highest average BLEURT score is 73.44%. The BLEURT scores of the generations vary from 0.46 to 0.99; more than 50% of the test set generations score less than 0.80. We sampled 40 generations with BLEURT score &lt; 0.8 and note that 35 generations are correct, which are rephrasing the ground truth, while 2 out of 35 that are better than the ground truth. Hence, we observe that BLEURT score cannot be used to determine if we have achieved a good performance on a dataset, it can only be used to compare different models. This issue has also been pointed out by the authors<ref type="foot" target="#foot_1">4</ref> .</p><p>FactSpotter answers whether a fact is present in text; it does not have to address the much harder task of deciding if two sentences are equivalent. Besides being more reliable because it is solving a simpler task, it is also more interpretable as we can investigate the exact triples that are classified as negative, instead of doing a complete comparison between a subgraph and a sentences or between two sentences. This is especially useful for long input graphs and long generations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion</head><p>In this work, we have presented a new metric for measuring factual faithfulness in G2T, FactSpotter. This metric can be trained in a self supervised fashion, using the same annotations as a G2T model. We have shown it achieves the highest correlation with humans on factual faithfulness and it can be used as a plug-in feature for G2T models. Finally, we have used our metric to analyze the difficulty of existing datasets. We have observed that models perform very well on these datasets, hence new datasets should be proposed. Such datasets could cover more difficult input graphs, for example triple from tables. In addition, through the initiative of governments to release tabular data related to public interest<ref type="foot" target="#foot_2">5</ref> , tools trained to express in natural language the content of tables could be used as user friendly interfaces for citizens.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">Limitations</head><p>Our work has the following limitations:</p><p>• FactSpotter cannot be used to determine the precise nature of the error in the generated sentence. It was trained to predict whether a fact is presented in the text or not, not if we the sentence has a wrong predicate or a wrong subject or object. This problem can be solved by a second classification step for whether predicates or entities are incorrectly verbalized in the text, to make FactSpotter more interpretable.</p><p>• The input of FactSpotter is the concatenation of a fact f represented in triple and a natural language text T , i.e., it has limited input format. With such input, the advantage is that it is easier to construct both positive and negative samples for its self-supervised training. However, it is also difficult to use it to check the factual faithfulness on other text generation tasks, since high quality structured knowledge graphs are hard to generate. However, we will investigate in the future the use of open information extraction models <ref type="bibr" target="#b36">(Upadhyay et al., 2023)</ref> for extracting facts from sentences.</p><p>• Although the accuracy and F1 score of our classification model on the test splits of various datasets in Table <ref type="table" target="#tab_0">1</ref> are high, there still exist some false positive and false negative samples. Hence, our FactSpotter generally reflects factual faithfulness, but it might still be biased on some hard samples, especially when predicates in knowledge graphs are distant to their natural language representations in the vector space of language models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Appendix</head><p>A.1 Metrics for the evaluation of G2T BLEU <ref type="bibr" target="#b26">(Papineni et al., 2002)</ref> This is one of the oldest sentence similarity metrics. BLEU is computed as a weighted geometric mean of n-gram precision scores, that is: it rewards inputs that share many common substrings (or n-grams, usually n is 4). The score ranges between 0 and 1.</p><p>METEOR <ref type="bibr" target="#b1">(Banerjee and Lavie, 2005)</ref> This similarity metric is based on the harmonic mean of the 1-gram precision and recall. Its recall has a higher weight than precision. METEOR scores range between 0 and 1.</p><p>BERTScore <ref type="bibr" target="#b41">(Zhang et al., 2019)</ref> This leverages the pre-trained contextual embeddings of BERT, and matches words in candidate and reference sentences by cosine similarity. The score can be used to compute precision, recall, and F1. BERTScore is also a similarity measure, ranging from 0 to 1.</p><p>BLEURT <ref type="bibr" target="#b34">(Sellam et al., 2020)</ref> This is a finetuned Bert model on synthetically generated texts.</p><p>It provides a similarity score learned from evaluation scores such as BLEU or ROUGE, and from human ratings; 1 represents a perfect match, while a value close to 0 means no similarity. We consider the authors' latest released model, BleuRT-20<ref type="foot" target="#foot_3">6</ref> .</p><p>BARTScore <ref type="bibr" target="#b40">(Yuan et al., 2021)</ref> This metric is based on the BART seq2seq language model: the score represents the weighted log probability of one text y, given another text x. We compute two versions of the BARTScore, based on the indications of the paper: in the first version of the score, we compare the generated sentence with the gold standard (we refer to it as BARTScore, the default version provided by the authors), and a second version in which we compare the generated text with the input graph (referred in the paper as the faithfulness method, BARTScore-faithful in our experiments). The second score should encourage the metric to consider the factual faithfulness of a generation. BARTScore is not normalized.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Bootstrapping</head><p>Given two metrics M 1 , M 2 , we need to assess which one better correlates with human annotations. We have at our disposal a set of G2T systems S 1 , . . . , S S , and a set A of G2T samples. Given a set D of dimensions, e.g., fluency, factfulness, etc., we have at our disposal the human annotation</p><formula xml:id="formula_2">H d (S i (a)), d ∈ D 1 ≤ i ≤ S, a ∈ A:</formula><p>this is the score that human users assigned to the G2T generation of system S i , on the sample a, along dimension d. Similarly, each automated metric M computes M d (S i (a)), the M score of S i on sample a along dimension d. We note however that automated metric give generally just one score for all dimensions.</p><p>Based on the H d (S i (a)), M d (S i (a)) over all systems, samples, and dimensions, we assess how well metric M correlates with human judgment H in a statistically sound way. Since each dimension is of independent interest, we study the correlation of M and H separately for each dimension, and omit d from the notations below, for simplicity.</p><p>We compute the three correlation coefficients (r, ρ and τ ) between vectors of per-system scores [M (S 1 ), . . . M (S S )] and [H(S 1 ), . . . H(S S )]. One aspect remains to settle: for a given S i , how to aggregate information from all the annotated samples A? It is important to do this in a way that is robust to the divergence and noise sometimes found in human annotations.</p><p>As is proposed in <ref type="bibr" target="#b11">(Dhingra et al., 2019)</ref>, we apply bootstrapping, as follows. We generate a number (we used 1000, which is same as <ref type="bibr" target="#b11">(Dhingra et al., 2019)</ref>) of "variants" A 1 , . . . , A 1000 of the annotated samples A, by sampling with replacement from A. Each A i has the same size as A, but it may contain some A samples several times, and others not at all. We then compute correlation coefficients between the vectors [M (S 1 ), . . . M (S S )] and [H(S 1 ), . . . H(S S )] over each A i , then represent each of the resulting 1000 correlation values by their mean, together a confidence interval. The relatively large number of "variants" acts as insulation against divergence and noise in the annotations. Then for two metrics M 1 , M 2 : i) If the mean of M 1 is higher than M 2 and their confidence intervals do not overlap, it means M 1 better correlates with human judgment, than M 2 .</p><p>ii) If the confidence intervals of M 1 , M 2 overlap, we need to check whether a statistically significant conclusion can be drawn about the metrics. This is the case only if the confidence interval of the difference in correlation between the metrics is above 0 (Wilcox, 2016), in which the metric with higher mean correlates better with human judgment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Improved Samples of G2T Generation</head><p>Table <ref type="table" target="#tab_8">9</ref> shows the improvement of factual faithfulness in G2T generation task after integrating FactSpotter into G2T inference. The samples are from the hardest zero-shot and compositional splits of GrailQA dataset. Each sample in the table includes a description of a subgraph, a ground truth text, a text generated by the best baseline model (JointGT), and a text generated by our FactJointGT.</p><p>Although the texts generated by the baseline model, JointGT, are generally fluent, we observe hallucinations in its generations: i) Some predicates are interpreted with different meanings in the generated texts; ii) Some entities are generated wrongly; iii) Some facts in subgraphs are lost in the generated texts. After integrating FactSpotter into the inference of G2T generation, without retraining the G2T model, our FactJointGT can generate texts that verbalize facts in subgraphs more correctly and completely. Hence the factual faithfulness of G2T task is improved by FactSpotter, but the other metrics do not necessary improve, since rephrased high quality texts might also be punished.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 Parameter tuning</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4.1 Percentages in negative samples for</head><p>FactSpotter training FactSpotter receives in the training set two types of negative samples. In the first type of negative sample (type I) we modify the fact from the graph, while the second type of negative sample (type II) we modify the text. To determine the best balance between negative samples of type I, respectively, type II, we compute the correlations between the score of FactSpotter and a subset of 50 annotations per system from the 2020 WebNLG challenge. As shown in Table <ref type="table" target="#tab_9">10</ref>, the best results for detecting factual accuracy and fluency are using either a ratio 0.9-0.1 or 0.8-0.2. In the remaining of the paper, we use FactSpotter with the parameters 0.9-0.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4.2 λ tuning for factual generations</head><p>We recall that in Equation 1, we have defined a weight λ that quantifies the importance that we   assign to the probability that the next word is predicted such that we increase the probability of having a fact in the generated text, versus the default probability of choosing a word based on previously generated tokens. In Table <ref type="table" target="#tab_0">11</ref>, we vary the value of λ on the different splits of GrailQA on the T5 small model. The BleuRT score does not vary significantly, meaning that the fluency of the G2T model should not be affected. We fix λ to 0.15 for the experimental evaluation in Section 6, as it gives the best performance on the zero shot split, which is the most challenging, and gives very good results also for the other splits. More precisely, we report the results of FactT5 and FactJointGT with λ = 0.15 on the datasets introduced in Section 6.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.5 Sentence level correlation</head><p>We explain two distinct definitions of sentencelevel correlation provided by literature and report the correlations on the WebNLG human annotations.</p><p>The first definition of sentence-level correlation <ref type="bibr" target="#b10">(Colombo et al., 2022)</ref> is outlined as follows:</p><p>• Construct a pair containing the automated metric scores [M (sys 1 ), . . . , M (sys S )] and the corresponding system-level human scores [H(sys 1 ), . . . , H(sys S )] for a given sentence.</p><p>• The Pearson correlation amidst these pairs is computed. If the correlation is significant (adopting p &lt; 0.05 as the threshold), this correlation is preserved.</p><p>• Then, report the average over all significant correlations (average over at most the total number of annotated sentences).</p><p>When there is a large number of significant pairs with high correlation value, it can show if a metric can be used to compare different verbalisations of the same input triples. We report the human correlation results computed as this definition on the WebNLG 2017 and 2020 annotations in Tables <ref type="table" target="#tab_10">12</ref> and<ref type="table" target="#tab_11">13</ref> respectively. We note that only FactSpotter is significantly larger than the other metrics on data coverage and relevance for WebNLG 2020, and semantic adequacy for WebNLG 2017, which are metrics about factual faithfulness. Please note that here it is important to report on how many sentences the correlation was significant with p &lt; 0.05. Because we compute if a metric has a higher score than a second metric using the bootstrapping technique presented in the Annexes, different samples might have different number of sentences with correlation having p &lt; 0.05, hence we report the average number of sentences over all the samples. For WebNLG2017, out of 223 human annotated sentences, we have for semantics    The second definition of sentence-level correlation <ref type="bibr" target="#b1">(Banerjee and Lavie, 2005)</ref> is computed between the vector containing all the automatic scores for each sentence by a system S given by a metric M , and the vector containing the human metrics for each sentence. The sentence-level correlation of a metric M is computed as an average over all the correlation of each system S 1 , . . . , S S . This measure determines if a metric can be used to rank verbalisations of different input graphs. The human correlation results computed as the second definition on the WebNLG 2017 and 2020 annotations are reported in Tables <ref type="table" target="#tab_12">14</ref> and<ref type="table" target="#tab_13">15</ref> respectively.</p><p>On WebNLG 2017 dataset, three metrics achieve similar scores for semantic adequacy, with no results significantly larger than the others (computed  <ref type="bibr">, 1996)</ref> correlation guidelines, where a value between 0.6 to 0.79 is "strong correlation" and 0.8 to 1.0 is "very strong", we have "strong" correlation for the type of sentence correlation to the annotation. However, on WebNLG 2020 dataset, all metrics demonstrate a "moderate" level of correlation, given that the scores hover between 0.3 and 0.49. We note that the two sentence correlation interpretations presented above are more sensitive to noise in the human annotation, as we consider each individual sentence score, not aggregates. is evident also from the lower Human-Human correlation we observe on sentences, in comparison with the high correlation for the system level correlation. In Tables <ref type="table" target="#tab_4">16</ref> and<ref type="table" target="#tab_5">17</ref>, we present the interannotator agreement for WebNLG 2017 and 2020 using Krippendorff's alpha <ref type="bibr" target="#b21">(Krippendorff, 2018)</ref>. It can be observed that both datasets exhibit generally low consistency among annotators, with the 2020 dataset showing less agreement than its 2017 version. However, on WebNLG 2017 we observe a higher agreement on semantic adequancy, for which we also observed a high correlation with our metric at sentence level.</p><p>We believe that these results show the need for better human annotation guidelines for this task. In particular, on WebNLG2020, the annotators were asked to give a score from 0 to 100 to a sentence for a given dimension such as correctness. Such a fine-grained decision is very difficult to take, hence the low agreement score. We note that mapping these scores to scores from 1 to 10 or 1 to 5 does not improve agreement. In the 2017 challenge, the annotators gave scores from 1 to 3, but we believe that those scores were not sufficiently described such that the annotators could choose them accordingly. We recommend that future annotations use a Likert scale annotation, but with clearer guidelines and examples for each score. For the WebNLG 2017 dataset, we can observe that for the small G2T model, T5S, the triples of size 4, 5 and 7 are more challenging, as is shown in Table <ref type="table" target="#tab_18">19</ref>. The integration of FactSpotter enhances the factuality of the results (as evident in F-T5S). We also observed a distinct improvement across various numbers of triples in the WebNLG 2020 dataset, especially when handling sentences comprising multiple triples.</p><p>As for the DART dataset in Table <ref type="table" target="#tab_22">23</ref>, inputs ranging from 1 to 5 triples witness improved results with FactSpotter's addition. Remarkably high FactSpotter scores are observed for inputs containing 6 or 7 triples. It's worth noting that of the inputs with 6 triples, 569 out of 598 are sourced from the E2E split, and for those with 7 triples, 320 out of 342 hail from the same E2E split. Hence, the E2E split of DART seems to be less challenging.</p><p>The results of GrailQA dataset are illustrated in Tables <ref type="table" target="#tab_25">25,</ref><ref type="table" target="#tab_29">29</ref>, and 27. Regarding the GrailQA zero-shot split, single-triple verbalization consistently achieves superior scores with FactSpotter. Beginning with two triples, the incorporation of FactSpotter offers a discernible boost in model performance. In the compositional split, mirroring the trend observed in the zero-shot scenario, FactSpotter scores remain high for single-triple inputs. Furthermore, there is a marked improvement for inputs with two or more triples upon the addition of FactSpotter. In the IID split, while the baseline scores are impressively high, enhancements is noticeable for small model like T5S.   We consider the intricacy in verbalizing a given triple is predominantly influenced by:</p><p>1. The distance between entity names in the knowledge graph and those in natural text.</p><p>2. The difference between KG predicates and their equivalent natural language phrases.   Entity Generation Regarding the difficulty of generating correct entities, we have the following statistics on the WebNLG 2017 dataset (v2.1):</p><p>• 87% of input graph entities appear identical in its corresponding ground-truth texts.</p><p>• 7% of differences arise from special characters, e.g., "Motherwell F.C." in the input graph becomes "Motherwell FC" in the groundtruth, both being correct.</p><p>• 2% are due to variations in date or number formats, as observed with "2006-12-31" in the KG interpreted to be "December 31, 2006" in the ground-truth text.</p><p>• 1% emerge from linguistic differences between the source language and English, as with "Atatürk" in KG being written as "Ataturk" in reference text.</p><p>• Other variances often regard alternative entity designations, such as "United States" in the KG being abbreviated as "US" in specific ground-truth texts.</p><p>For the GrailQA dataset, rooted in Freebase, 99% of input graphs retain consistent entity names in their ground-truth sentences. Accurate entity verbalization is relatively straightforward for this dataset, but we observe hallucinations in the texts generated by baseline models.</p><p>Considering that most differences between KG and natural language entity names result from formatting nuances, we can easily address its consistency challenge using constrained beam search to ensure all entity names appear in the generated text. For beam sizes over 10, the text generation becomes accurate without compromising other metrics. However, for beam sizes under 10, such constraints tend to impede the fluency.</p><p>Predicate Generation In the WebNLG 2017 dataset, 49% of predicates appear the same as in the KB in their corresponding ground-truth sentences. Using SBert, we computed an average similarity of 84% between each predicate and its nearest ngrams in the ground-truth.</p><p>For the GrailQA dataset, only 27% of predicates align perfectly with their KG representations in the ground-truth. The average similarity stands at 64%, highlighting a larger difference between FreeBase predicates and their natural language phrases. Summary of Difficulty Datasets like WebNLG (from DBPedia) and GrailQA (from FreeBase) present challenges on different fronts. The difficulty from datasets based on FreeBase is the distance between knowledge graph and natural language is much higher. However, WebNLG and DART datasets have more complex input subgraphs, which has more number of triples, while GrailQA only has input subgraphs with up to 4 triples.</p><p>We consider that promoting accurate predicate generation is more challenging problem than promoting the generation of correct entities, because predicates are much more often rephrased in sentences, which is harder to evaluate. Inspired by the efficacy of Constrained Beam Search in ensuring accurate entity generation, we designed FactSpotter to enhance the accurate production of rephrased facts, especially rephrased predicates to be correctly generated.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>table. Performance of FactSpotter on test splits.</figDesc><table><row><cell>Dataset</cell><cell>Acc.</cell><cell>F1</cell><cell>TP/TN</cell><cell>FP/FN</cell></row><row><cell>GrailQA</cell><cell cols="2">96.85 96.85</cell><cell>2272/2643</cell><cell>64/96</cell></row><row><cell>SimpleQ.</cell><cell cols="4">95.41 95.41 10260/10438 419/575</cell></row><row><cell>DART</cell><cell cols="4">97.62 97.62 26613/26352 607/684</cell></row><row><cell cols="3">WebNLG17 99.10 99.09</cell><cell>8594/8575</cell><cell>93/63</cell></row><row><cell cols="3">WebNLG20 95.21 95.21</cell><cell>9637/9853</cell><cell>317/663</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Correlation at the system level with human judgment on correctness, data coverage, relevance, fluency and text structure for the 2020 WebNLG task. For tables here and below, BERTF1 stands for BERTScore-F1, BARTS for BARTScore, BARTS-F for BARTScore-faithful, FactS for FactSpotter. We highlight the best result and we mark it with an asterisk when it is statistically significantly larger than any other metric (excluding Human to Human correlation). FactSpotter performs the best on correctness, data coverage, and relevance. We put a value for correlation if the pvalue p &lt; 0.05.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Correct.</cell><cell></cell><cell></cell><cell cols="2">D. Cover.</cell><cell></cell><cell></cell><cell>Relev.</cell><cell></cell><cell></cell><cell>Fluency</cell><cell></cell><cell></cell><cell>T. Struct.</cell></row><row><cell>Metric</cell><cell>r</cell><cell>ρ</cell><cell></cell><cell>τ</cell><cell>r</cell><cell></cell><cell>ρ</cell><cell>τ</cell><cell>r</cell><cell>ρ</cell><cell>τ</cell><cell>r</cell><cell>ρ</cell><cell>τ</cell><cell>r</cell><cell>ρ</cell><cell>τ</cell></row><row><cell>Correct.</cell><cell>1.0</cell><cell cols="2">1.0</cell><cell>1.0</cell><cell cols="4">0.96 0.81 0.66</cell><cell>0.97</cell><cell cols="7">0.81 0.66 0.80 0.77 0.60 0.79 0.76 0.59</cell></row><row><cell>D. Cover.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>1.0</cell><cell></cell><cell>1.0</cell><cell>1.0</cell><cell>0.93</cell><cell cols="7">0.80 0.64 0.71 0.56 0.43 0.69 0.55 0.42</cell></row><row><cell>Relev.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>1.0</cell><cell>1.0</cell><cell>1.0</cell><cell cols="5">0.76 0.63 0.48 0.76 0.62 0.47</cell></row><row><cell>Fluency</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>1.0</cell><cell>1.0</cell><cell>1.0</cell><cell cols="2">0.98 0.97 0.91</cell></row><row><cell>T. Struct.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>1.0</cell><cell>1.0</cell><cell>1.0</cell></row><row><cell>Human</cell><cell cols="8">0.96 0.80 0.65 0.93 0.83 0.68</cell><cell>0.96</cell><cell cols="7">0.74 0.59 0.95 0.93 0.80 0.93 0.91 0.77</cell></row><row><cell>BLEU</cell><cell cols="8">0.59 0.64 0.48 0.53 0.53 0.40</cell><cell>0.56</cell><cell cols="7">0.60 0.45 0.87 0.84 0.68 0.86 0.84 0.68</cell></row><row><cell cols="9">METEOR 0.72 0.75 0.60 0.65 058 0.44</cell><cell>0.70</cell><cell cols="7">0.64 0.50 0.88 0.89 0.74 0.86 0.88 0.72</cell></row><row><cell>BERTF1</cell><cell cols="8">0.83 0.77 0.60 0.74 0.58 0.43</cell><cell>0.81</cell><cell cols="7">0.65 0.50 0.90 0.93 0.80 0.88 0.92 0.78</cell></row><row><cell>BLEURT</cell><cell cols="8">0.93 0.82 0.67 0.86 0.65 0.50</cell><cell>0.91</cell><cell cols="7">0.69 0.55 0.90 0.92 0.78 0.89 0.91 0.76</cell></row><row><cell>BARTS</cell><cell cols="8">0.90 0.83 0.67 0.86 0.71 0.53</cell><cell>0.88</cell><cell cols="7">0.71 0.56 0.77 0.81 0.63 0.75 0.80 0.62</cell></row><row><cell cols="9">BARTS-F 0.67 0.54 0.41 0.68 0.61 0.46</cell><cell>0.68</cell><cell cols="3">0.59 0.45 0.51</cell><cell>-</cell><cell>-</cell><cell>0.52</cell><cell>-</cell><cell>-</cell></row><row><cell>FactS</cell><cell cols="16">0.94 0.80 0.64 0.91 0.87 0.71 0.96  *  0.79 0.64 0.74 0.59 0.45 0.72 0.59 0.45</cell></row><row><cell></cell><cell cols="2">Sem. Adeq.</cell><cell></cell><cell></cell><cell>T. Struct.</cell><cell></cell><cell></cell><cell>Fluency</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Metric</cell><cell>r</cell><cell>ρ</cell><cell>τ</cell><cell>r</cell><cell>ρ</cell><cell>τ</cell><cell>r</cell><cell>ρ</cell><cell>τ</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Sem. Adeq.</cell><cell>1.0</cell><cell>1.0</cell><cell>1.0</cell><cell cols="6">0.73 0.65 0.52 0.71 0.66 0.52</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>T. Struct.</cell><cell></cell><cell></cell><cell></cell><cell>1.0</cell><cell>1.0</cell><cell>1.0</cell><cell cols="3">0.98 0.95 0.88</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Fluency</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>1.0</cell><cell>1.0</cell><cell>1.0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Human</cell><cell>0.99</cell><cell cols="8">0.98 0.94 0.98 0.92 0.84 0.98 0.89 0.79</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>BLEU</cell><cell>0.76</cell><cell cols="8">0.71 0.56 0.86 0.71 0.57 0.83 0.72 0.57</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>METEOR</cell><cell>0.86</cell><cell cols="8">0.83 0.67 0.85 0.70 0.57 0.80 0.70 0.56</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>BERTF1</cell><cell>0.70</cell><cell cols="8">0.78 0.63 0.71 0.70 0.57 0.69 0.70 0.57</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>BLEURT</cell><cell>0.90</cell><cell cols="8">0.88 0.72 0.84 0.69 0.56 0.79 0.68 0.56</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>BARTS</cell><cell>0.90</cell><cell cols="3">0.87 0.78 0.71</cell><cell>-</cell><cell>-</cell><cell>0.68</cell><cell>-</cell><cell>-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>FactS</cell><cell cols="4">0.97  *  0.93 0.85 0.67</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table /><note><p>Correlation at the system level with human judgment on semantic adequacy, grammar, and fluency, for the 2017 WebNLG dataset.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 5 :</head><label>5</label><figDesc>Results on G2T on WebNLG 2017 Const.</figDesc><table><row><cell>Model</cell><cell cols="6">BLEU METEOR BERTF1 BLEURT BARTS FactS</cell></row><row><cell>T5S</cell><cell>37.97</cell><cell>36.50</cell><cell>93.43</cell><cell>67.85</cell><cell>-2.45</cell><cell>96.80</cell></row><row><cell>F-T5S</cell><cell>37.85</cell><cell>36.06</cell><cell>93.42</cell><cell>67.86</cell><cell>-2.45</cell><cell>98.17</cell></row><row><cell>T5B</cell><cell>38.73</cell><cell>36.43</cell><cell>93.61</cell><cell>68.48</cell><cell>-2.43</cell><cell>95.09</cell></row><row><cell>F-T5B</cell><cell>38.73</cell><cell>36.42</cell><cell>93.56</cell><cell>68.42</cell><cell>-2.43</cell><cell>97.14</cell></row><row><cell cols="2">JGT-T5 39.35</cell><cell>36.82</cell><cell>93.65</cell><cell>68.50</cell><cell>-2.42</cell><cell>95.40</cell></row><row><cell cols="2">FGT-T5 39.24</cell><cell>36.78</cell><cell>93.64</cell><cell>68.48</cell><cell>-2.42</cell><cell>97.25</cell></row><row><cell cols="7">Table 4: Results on G2T on SimpleQuestions. Here and</cell></row><row><cell cols="7">below, T5S stands for T5 small, T5B for T5 base, F-T5S</cell></row><row><cell cols="7">for FactT5 small, F-T5B for FactT5 base, JGT-T5 for</cell></row><row><cell cols="6">JointGT-T5, and FGT-T5 for FactJointGT-T5.</cell><cell></cell></row><row><cell>Model</cell><cell cols="6">BLEU METEOR BERTF1 BLEURT BARTS FactS</cell></row><row><cell>T5S</cell><cell>66.24</cell><cell>47.80</cell><cell>96.72</cell><cell>73.16</cell><cell>-1.41</cell><cell>98.67</cell></row><row><cell>F-T5S</cell><cell>66.27</cell><cell>47.89</cell><cell>96.73</cell><cell>73.21</cell><cell>-1.41</cell><cell>99.25</cell></row><row><cell>T5B</cell><cell>67.04</cell><cell>48.35</cell><cell>96.81</cell><cell>73.22</cell><cell>-1.40</cell><cell>99.44</cell></row><row><cell>F-T5B</cell><cell>67.04</cell><cell>48.22</cell><cell>96.80</cell><cell>73.26</cell><cell>-1.40</cell><cell>99.71</cell></row><row><cell cols="2">JGT-T5 67.08</cell><cell>48.34</cell><cell>96.76</cell><cell>73.44</cell><cell>-1.39</cell><cell>99.09</cell></row><row><cell cols="2">FGT-T5 66.89</cell><cell>48.19</cell><cell>96.84</cell><cell>73.42</cell><cell>-1.39</cell><cell>99.67</cell></row><row><cell>Model</cell><cell cols="6">BLEU METEOR BERTF1 BLEURT BARTS FactS</cell></row><row><cell>T5S</cell><cell>52.30</cell><cell>40.82</cell><cell>93.43</cell><cell>-1.75</cell><cell>65.80</cell><cell>90.75</cell></row><row><cell>F-T5S</cell><cell>52.44</cell><cell>41.02</cell><cell>93.45</cell><cell>-1.74</cell><cell>65.92</cell><cell>93.45</cell></row><row><cell>T5B</cell><cell>54.29</cell><cell>41.66</cell><cell>93.65</cell><cell>-1.69</cell><cell>66.43</cell><cell>93.60</cell></row><row><cell>F-T5B</cell><cell>54.72</cell><cell>41.70</cell><cell>93.61</cell><cell>-1.69</cell><cell>66.46</cell><cell>95.14</cell></row><row><cell>JGT</cell><cell>54.23</cell><cell>41.49</cell><cell>93.47</cell><cell>-1.72</cell><cell>66.23</cell><cell>91.26</cell></row><row><cell cols="2">FGT-T5 54.45</cell><cell>41.52</cell><cell>93.49</cell><cell>-1.72</cell><cell>66.31</cell><cell>93.16</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 6 :</head><label>6</label><figDesc>Results on G2T on the WebNLG 2020 dataset.</figDesc><table><row><cell>Model</cell><cell cols="6">BLEU METEOR BERTF1 BLEURT BARTS FactS</cell></row><row><cell>T5S</cell><cell>46.22</cell><cell>39.96</cell><cell>94.69</cell><cell>66.62</cell><cell>-2.03</cell><cell>95.47</cell></row><row><cell>F-T5S</cell><cell>46.31</cell><cell>40.07</cell><cell>94.74</cell><cell>66.66</cell><cell>-2.02</cell><cell>97.29</cell></row><row><cell>T5B</cell><cell>48.47</cell><cell>40.74</cell><cell>95.04</cell><cell>67.49</cell><cell>-1.97</cell><cell>96.65</cell></row><row><cell>F-T5B</cell><cell>48.37</cell><cell>40.72</cell><cell>95.05</cell><cell>67.43</cell><cell>-1.97</cell><cell>97.60</cell></row><row><cell cols="2">JGT-T5 47.51</cell><cell>40.43</cell><cell>94.92</cell><cell>67.33</cell><cell>-2.01</cell><cell>95.86</cell></row><row><cell cols="2">FGT-T5 47.39</cell><cell>40.32</cell><cell>94.92</cell><cell>67.26</cell><cell>-2.00</cell><cell>97.25</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 7 :</head><label>7</label><figDesc>Results on G2T on the DART dataset.</figDesc><table><row><cell cols="7">Model/Split BLEU METEOR BERTF1 BLEURT BARTS FactS</cell></row><row><cell>IID</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>T5S</cell><cell>44.51</cell><cell>41.80</cell><cell>93.23</cell><cell>69.53</cell><cell>-2.37</cell><cell>97.98</cell></row><row><cell>F-T5S</cell><cell>44.64</cell><cell>41.88</cell><cell>93.25</cell><cell>69.63</cell><cell>-2.36</cell><cell>98.47</cell></row><row><cell>T5B</cell><cell>45.95</cell><cell>42.71</cell><cell>93.50</cell><cell>70.66</cell><cell>-2.29</cell><cell>99.43</cell></row><row><cell>F-T5B</cell><cell>46.10</cell><cell>42.67</cell><cell>93.52</cell><cell>70.73</cell><cell>-2.29</cell><cell>99.50</cell></row><row><cell>JGT-T5</cell><cell>43.68</cell><cell>41.65</cell><cell>93.21</cell><cell>69.41</cell><cell>-2.37</cell><cell>98.62</cell></row><row><cell>FGT-T5</cell><cell>43.61</cell><cell>41.65</cell><cell>93.19</cell><cell>69.41</cell><cell>-2.37</cell><cell>99.12</cell></row><row><cell>Zero</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>T5S</cell><cell>30.30</cell><cell>36.91</cell><cell>91.74</cell><cell>62.87</cell><cell>-2.74</cell><cell>93.27</cell></row><row><cell>F-T5S</cell><cell>30.30</cell><cell>36.90</cell><cell>91.75</cell><cell>63.01</cell><cell>-2.73</cell><cell>94.60</cell></row><row><cell>T5B</cell><cell>32.20</cell><cell>37.35</cell><cell>91.92</cell><cell>63.84</cell><cell>-2.72</cell><cell>94.77</cell></row><row><cell>F-T5B</cell><cell>32.39</cell><cell>37.39</cell><cell>91.92</cell><cell>63.94</cell><cell>-2.71</cell><cell>95.61</cell></row><row><cell>JGT-T5</cell><cell>32.94</cell><cell>37.69</cell><cell>92.02</cell><cell>64.18</cell><cell>-2.68</cell><cell>94.15</cell></row><row><cell>FGT-T5</cell><cell>32.46</cell><cell>37.55</cell><cell>91.93</cell><cell>64.00</cell><cell>-2.68</cell><cell>94.95</cell></row><row><cell>Comp.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>T5S</cell><cell>30.38</cell><cell>35.32</cell><cell>92.09</cell><cell>63.99</cell><cell>-2.74</cell><cell>94.94</cell></row><row><cell>F-T5S</cell><cell>30.14</cell><cell>35.21</cell><cell>92.08</cell><cell>63.72</cell><cell>-2.75</cell><cell>96.58</cell></row><row><cell>T5B</cell><cell>31.75</cell><cell>35.64</cell><cell>92.24</cell><cell>63.99</cell><cell>-2.72</cell><cell>94.84</cell></row><row><cell>F-T5B</cell><cell>31.66</cell><cell>35.72</cell><cell>92.24</cell><cell>64.10</cell><cell>-2.71</cell><cell>96.53</cell></row><row><cell>JGT-T5</cell><cell>31.46</cell><cell>36.08</cell><cell>92.39</cell><cell>64.92</cell><cell>-2.67</cell><cell>95.26</cell></row><row><cell>FGT-T5</cell><cell>31.25</cell><cell>36.21</cell><cell>92.43</cell><cell>65.12</cell><cell>-2.65</cell><cell>97.10</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 8 :</head><label>8</label><figDesc>Results on G2T on the GrailQA dataset.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 9 :</head><label>9</label><figDesc>Texts with improved factual faithfulness after integrating FactSpotter into G2T inference. Red stands for the contents that are presented in subgraphs, but are missed in the baseline generations. Blue stands for the exact contents correctly presented in ground-truth texts or FactJointGT generations.</figDesc><table><row><cell>Type I /</cell><cell cols="3">Data Coverage</cell><cell></cell><cell>Relevance|</cell><cell></cell><cell></cell><cell>Fluency</cell><cell></cell></row><row><cell>Type II</cell><cell>r</cell><cell>ρ</cell><cell>τ</cell><cell>r</cell><cell>ρ</cell><cell>τ</cell><cell>r</cell><cell>ρ</cell><cell>τ</cell></row><row><cell cols="10">0.9 / 0.1 0.85 0.84 0.68 0.93 0.88 0.75 0.91 0.79 0.63</cell></row><row><cell cols="10">0.8 / 0.2 0.83 0.85 0.63 0.92 0.93 0.79 0.89 0.79 0.61</cell></row><row><cell cols="10">0.7 / 0.3 0.77 0.84 0.66 0.85 0.89 0.73 0.66 0.63 0.50</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 10 :</head><label>10</label><figDesc>The influence of the ratio on FactSpotter.</figDesc><table><row><cell></cell><cell>IID</cell><cell></cell><cell cols="2">Zero-shot</cell><cell cols="2">Compositional</cell></row><row><cell>λ</cell><cell cols="6">BleuRT FactS BleuRT FactS BleuRT FactS</cell></row><row><cell>0.0</cell><cell>69.53</cell><cell>97.98</cell><cell>62.27</cell><cell>93.27</cell><cell>63.99</cell><cell>94.94</cell></row><row><cell>0.05</cell><cell>69.76</cell><cell>98.15</cell><cell>62.94</cell><cell>94.15</cell><cell>63.92</cell><cell>95.69</cell></row><row><cell>0.1</cell><cell>69.76</cell><cell>98.48</cell><cell>63.00</cell><cell>94.56</cell><cell>63.71</cell><cell>95.83</cell></row><row><cell>0.15</cell><cell>69.63</cell><cell>98.47</cell><cell>63.01</cell><cell>94.60</cell><cell>63.72</cell><cell>96.58</cell></row><row><cell>0.2</cell><cell>69.47</cell><cell>98.38</cell><cell>62.95</cell><cell>94.79</cell><cell>63.61</cell><cell>96.19</cell></row><row><cell cols="7">Table 11: BleuRT vs. FactSpotter when varying the λ</cell></row><row><cell cols="7">parameter (importance of the fact classifier for generat-</cell></row><row><cell cols="4">ing a sentence, see Equation 1).</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 12 :</head><label>12</label><figDesc>The result of sentence-level correlation with the first definition on the WebNLG 2017 annotation.</figDesc><table><row><cell>:</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 13 :</head><label>13</label><figDesc>The result of sentence-level correlation with the first definition on the WebNLG 2020 annotation.</figDesc><table><row><cell cols="4">Metrics Sem. Adeq. T. Structure Fluency</cell></row><row><cell>Human</cell><cell>0.56</cell><cell>0.44</cell><cell>0.50</cell></row><row><cell>BertF1</cell><cell>0.64</cell><cell>0.54</cell><cell>0.57</cell></row><row><cell>BleuRT</cell><cell>0.69</cell><cell>0.55</cell><cell>0.59</cell></row><row><cell>BartS</cell><cell>0.59</cell><cell>0.47</cell><cell>0.45</cell></row><row><cell>FactS</cell><cell>0.65</cell><cell>0.43</cell><cell>0.43</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 14 :</head><label>14</label><figDesc>The result of sentence-level correlation with the second definition on the WebNLG 2017 annotation.</figDesc><table><row><cell cols="6">Metrics Correct. D. Cover. Fluency Relev. T. Structure</cell></row><row><cell>Human</cell><cell>0.38</cell><cell>0.37</cell><cell>0.29</cell><cell>0.35</cell><cell>0.30</cell></row><row><cell>BertF1</cell><cell>0.42</cell><cell>0.38</cell><cell>0.42</cell><cell>0.37</cell><cell>0.40</cell></row><row><cell>BleuRT</cell><cell>0.45</cell><cell>0.43</cell><cell>0.47</cell><cell>0.38</cell><cell>0.43</cell></row><row><cell>BartS</cell><cell>0.39</cell><cell>0.38</cell><cell>0.32</cell><cell>0.35</cell><cell>0.31</cell></row><row><cell>FactS</cell><cell>0.38</cell><cell>0.40</cell><cell>0.29</cell><cell>0.37</cell><cell>0.27</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 15 :</head><label>15</label><figDesc>The result of sentence-level correlation with the second definition on the WebNLG 2020 annotation.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head>Table 18 :</head><label>18</label><figDesc>The number and the percentage of (graph, sentence) pairs for the test set of WebNLG 2017 Const</figDesc><table><row><cell>Triple Num</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell><cell>7</cell></row><row><cell>T5S</cell><cell cols="6">99.87 99.45 99.13 97.62 98.09 97.53</cell></row><row><cell>F-T5S</cell><cell cols="6">99.75 99.65 99.22 98.74 99.20 99.93</cell></row><row><cell>T5B</cell><cell cols="6">99.65 99.69 99.35 99.15 99.52 99.93</cell></row><row><cell>F-T5B</cell><cell cols="6">99.81 99.70 99.69 99.85 99.53 99.93</cell></row><row><cell>JGT-T5</cell><cell cols="6">99.69 99.73 98.52 98.63 99.30 99.93</cell></row><row><cell>FGT-T5</cell><cell cols="6">99.40 99.89 99.50 99.89 99.60 99.93</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head>Table 19 :</head><label>19</label><figDesc>FactSpotter by the number of triples for the test set of WebNLG 2017 Const</figDesc><table><row><cell>Triple Num</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell><cell>6</cell><cell>7</cell></row><row><cell>Pair Num</cell><cell>369</cell><cell>698</cell><cell cols="3">1050 1220 1065</cell><cell>684</cell><cell>553</cell></row><row><cell cols="8">Percentage 9.09 17.19 25.86 30.04 26.21 16.84 13.62</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_19"><head>Table 20 :</head><label>20</label><figDesc>The number and the percentage of (graph, sentence) pairs for the test set of WebNLG 2020 .13 95.58 94.07 92.21 91.01 90.54 T5B 95.21 96.33 94.51 93.00 92.29 92.30 93.36 F-T5B 96.27 96.74 95.69 94.41 94.72 94.89 94.09 JGT-T5 94.74 94.71 92.71 92.16 88.16 89.10 88.46 FGT-T5 94.26 95.85 93.85 93.93 90.90 91.43 92.48</figDesc><table><row><cell>Triple Num</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell><cell>6</cell><cell>7</cell></row><row><cell>T5S</cell><cell cols="7">93.71 93.91 92.24 90.71 89.95 88.68 86.12</cell></row><row><cell>F-T5S</cell><cell cols="2">95.05 95</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_20"><head>Table 21 :</head><label>21</label><figDesc>FactSpotter by the triple number for the test set of WebNLG 2020</figDesc><table><row><cell>Triple Num</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell><cell>6</cell><cell>7</cell></row><row><cell>Pair Num</cell><cell>848</cell><cell>797</cell><cell>821</cell><cell>869</cell><cell>822</cell><cell>598</cell><cell>342</cell></row><row><cell cols="8">Percentage 16.64 15.64 16.11 17.05 16.13 11.73 6.71</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_21"><head>Table 22 :</head><label>22</label><figDesc>The number and the percentage of (graph, sentence) pairs for DART test set</figDesc><table><row><cell>Triple Num</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell><cell>6</cell><cell>7</cell></row><row><cell>T5S</cell><cell cols="7">98.47 93.55 93.35 92.45 94.01 99.57 99.82</cell></row><row><cell>F-T5S</cell><cell cols="7">99.71 97.21 96.41 94.82 96.68 99.88 99.82</cell></row><row><cell>T5B</cell><cell cols="7">98.08 95.53 94.54 94.44 96.03 99.88 99.99</cell></row><row><cell>F-T5B</cell><cell cols="7">99.54 96.81 96.32 96.16 97.19 99.74 99.87</cell></row><row><cell>JGT-T5</cell><cell cols="7">98.49 94.36 93.61 93.44 94.33 99.82 99.80</cell></row><row><cell>FGT-T5</cell><cell cols="7">99.51 96.43 95.83 94.78 95.82 99.77 99.89</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_22"><head>Table 23 :</head><label>23</label><figDesc>FactSpotter by triple number for DART A.7 Further analysis on the difficulty of graph-to-text generation</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_24"><head>Table 24 :</head><label>24</label><figDesc>The number and the percentage of (graph, sentence) pairs for GrailQA zero-shot split</figDesc><table><row><cell>Triple Num</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell></row><row><cell>T5S</cell><cell cols="4">96.41 89.63 92.71 93.62</cell></row><row><cell>F-T5S</cell><cell cols="4">97.08 91.30 94.52 99.39</cell></row><row><cell>T5B</cell><cell cols="4">96.49 93.03 92.84 94.98</cell></row><row><cell>F-T5B</cell><cell cols="4">97.48 93.31 94.28 99.68</cell></row><row><cell>JGT-T5</cell><cell cols="4">96.96 90.77 93.50 91.28</cell></row><row><cell>FGT-T5</cell><cell cols="4">98.02 91.63 95.95 93.81</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_25"><head>Table 25 :</head><label>25</label><figDesc>FactSpotter by the number of triples for for GrailQA Zero-shot split</figDesc><table><row><cell>Triple Num</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell></row><row><cell>Pair Num</cell><cell>388</cell><cell>292</cell><cell>39</cell><cell>38</cell></row><row><cell cols="5">Percentage 51.25 38.57 5.15 5.01</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_26"><head>Table 26 :</head><label>26</label><figDesc>The number and the percentage of (graph, sentence) pairs for GrailQA Compositional split</figDesc><table><row><cell>Triple Num</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell></row><row><cell>T5S</cell><cell cols="4">98.47 94.89 95.53 88.15</cell></row><row><cell>F-T5S</cell><cell cols="4">99.14 95.77 98.12 94.69</cell></row><row><cell>T5B</cell><cell cols="4">98.64 96.23 77.78 93.27</cell></row><row><cell>F-T5B</cell><cell cols="4">99.25 96.35 93.80 94.36</cell></row><row><cell>JGT-T5</cell><cell cols="4">98.84 95.59 89.73 91.02</cell></row><row><cell>FGT-T5</cell><cell cols="4">98.92 96.62 98.79 94.85</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_27"><head>Table 27 :</head><label>27</label><figDesc>FactSpotter by the number of triples for GrailQA Compositional split</figDesc><table><row><cell>Triple Num</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell></row><row><cell>Pair Num</cell><cell>257</cell><cell>465</cell><cell>63</cell><cell>12</cell></row><row><cell cols="5">Percentage 32.65 58.34 7.90 1.50</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_28"><head>Table 28 :</head><label>28</label><figDesc>The number and the percentage of (graph, sentence) pairs for GrailQA IID split</figDesc><table><row><cell>Triple Num</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell></row><row><cell>T5S</cell><cell cols="4">97.75 97.98 98.82 96.96</cell></row><row><cell>F-T5S</cell><cell cols="4">97.96 98.73 98.83 99.20</cell></row><row><cell>T5B</cell><cell cols="4">99.55 99.31 99.38 99.77</cell></row><row><cell>F-T5B</cell><cell cols="4">99.55 99.47 99.38 99.77</cell></row><row><cell>JGT-T5</cell><cell cols="4">98.09 99.12 98.31 99.77</cell></row><row><cell>FGT-T5</cell><cell cols="4">98.72 99.35 99.31 99.77</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_29"><head>Table 29 :</head><label>29</label><figDesc>FactSpotter by the number of triples for GrailQA IID split</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>http://www.w3.org/TR/r2rml/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_1"><p>https://github.com/google-research/bleurt/ issues/1</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_2"><p>For example, Eurostat, https://ec.europa.eu</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_3"><p>https://github.com/google-research/bleurt</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgment. This work was performed using HPC resources from <rs type="funder">GENCI-IDRIS</rs> (Grant <rs type="grantNumber">2023-AD011014244</rs>). The authors were partially funded by the <rs type="grantNumber">ANR-20-CHIA-0015</rs> project.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_v6bmYXC">
					<idno type="grant-number">2023-AD011014244</idno>
				</org>
				<org type="funding" xml:id="_2d2Pa7u">
					<idno type="grant-number">ANR-20-CHIA-0015</idno>
				</org>
			</listOrg>

			<div type="availability">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>https: //github.com/guihuzhang</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Dbpedia: A nucleus for a web of open data</title>
		<author>
			<persName><forename type="first">Sören</forename><surname>Auer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Bizer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georgi</forename><surname>Kobilarov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jens</forename><surname>Lehmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Cyganiak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zachary</forename><surname>Ives</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Semantic Web: 6th International Semantic Web Conference, 2nd Asian Semantic Web Conference, ISWC 2007+ ASWC 2007</title>
		<meeting><address><addrLine>Busan, Korea</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2007-11-11">2007. November 11-15, 2007</date>
			<biblScope unit="page" from="722" to="735" />
		</imprint>
	</monogr>
	<note>Proceedings</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">METEOR: An automatic metric for MT evaluation with improved correlation with human judgments</title>
		<author>
			<persName><forename type="first">Satanjeev</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alon</forename><surname>Lavie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization</title>
		<meeting>the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="65" to="72" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Comparing automatic and human evaluation of NLG systems</title>
		<author>
			<persName><forename type="first">Anja</forename><surname>Belz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ehud</forename><surname>Reiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">11th Conference of the European Chapter of the Association for Computational Linguistics</title>
		<meeting><address><addrLine>Trento, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="313" to="320" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Freebase: A collaboratively created graph database for structuring human knowledge</title>
		<author>
			<persName><forename type="first">Kurt</forename><surname>Bollacker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Colin</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Praveen</forename><surname>Paritosh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Sturge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jamie</forename><surname>Taylor</surname></persName>
		</author>
		<idno type="DOI">10.1145/1376616.1376746</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2008 ACM SIGMOD International Conference on Management of Data, SIGMOD &apos;08</title>
		<meeting>the 2008 ACM SIGMOD International Conference on Management of Data, SIGMOD &apos;08<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1247" to="1250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Large-scale simple question answering with memory networks</title>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.02075</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The 2020 bilingual, bi-directional WebNLG+ shared task: Overview and evaluation results (WebNLG+ 2020)</title>
		<author>
			<persName><forename type="first">Thiago</forename><surname>Castro Ferreira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Claire</forename><surname>Gardent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikolai</forename><surname>Ilinykh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Van Der Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Mille</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diego</forename><surname>Moussallem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anastasia</forename><surname>Shimorina</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd International Workshop on Natural Language Generation from the Semantic Web (WebNLG+)</title>
		<meeting>the 3rd International Workshop on Natural Language Generation from the Semantic Web (WebNLG+)<address><addrLine>Dublin, Ireland (Virtual)</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="55" to="76" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Evaluation of text generation: A survey</title>
		<author>
			<persName><forename type="first">Asli</forename><surname>Celikyilmaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elizabeth</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.14799</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ternational conference on machine learning</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1597" to="1607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title/>
		<author>
			<persName><surname>Pmlr</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Electra: Pre-training text encoders as discriminators rather than generators</title>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.10555</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">InfoLM: A new metric to evaluate summarization &amp; data2text generation</title>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Jean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Colombo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chloé</forename><surname>Clavel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pablo</forename><surname>Piantanida</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="10554" to="10562" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Handling divergent reference texts when evaluating table-to-text generation</title>
		<author>
			<persName><forename type="first">Bhuwan</forename><surname>Dhingra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manaal</forename><surname>Faruqui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ankur</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dipanjan</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Cohen</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1483</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4884" to="4895" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Semantic noise matters for neural natural language generation</title>
		<author>
			<persName><forename type="first">Ondřej</forename><surname>Dušek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">M</forename><surname>Howcroft</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Verena</forename><surname>Rieser</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W19-8652</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th International Conference on Natural Language Generation</title>
		<meeting>the 12th International Conference on Natural Language Generation<address><addrLine>Tokyo, Japan</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="421" to="426" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>James</surname></persName>
		</author>
		<author>
			<persName><surname>Evans</surname></persName>
		</author>
		<title level="m">Straightforward statistics for the behavioral sciences</title>
		<imprint>
			<publisher>Thomson Brooks/Cole Publishing Co</publisher>
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Entity-based semantic adequacy for data-to-text generation</title>
		<author>
			<persName><forename type="first">Juliette</forename><surname>Faille</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Albert</forename><surname>Gatt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Claire</forename><surname>Gardent</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.findings-emnlp.132</idno>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: EMNLP 2021</title>
		<meeting><address><addrLine>Punta Cana, Dominican Republic</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1530" to="1540" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Using local knowledge graph construction to scale Seq2Seq models to multi-document inputs</title>
		<author>
			<persName><forename type="first">Angela</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Claire</forename><surname>Gardent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chloé</forename><surname>Braud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1428</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4186" to="4196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Creating training corpora for NLG micro-planners</title>
		<author>
			<persName><forename type="first">Claire</forename><surname>Gardent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anastasia</forename><surname>Shimorina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shashi</forename><surname>Narayan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laura</forename><surname>Perez-Beltrachini</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P17-1017</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="179" to="188" />
		</imprint>
	</monogr>
	<note>: Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Beyond i.i.d.: Three levels of generalization for question answering on knowledge bases</title>
		<author>
			<persName><forename type="first">Yu</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sue</forename><surname>Kase</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michelle</forename><surname>Vanni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Sadler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xifeng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Su</surname></persName>
		</author>
		<idno type="DOI">10.1145/3442381.3449992</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Web Conference 2021, WWW &apos;21</title>
		<meeting>the Web Conference 2021, WWW &apos;21<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="3477" to="3488" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Survey of hallucination in natural language generation</title>
		<author>
			<persName><forename type="first">Ziwei</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nayeon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rita</forename><surname>Frieske</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tiezheng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Etsuko</forename><surname>Ishii</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ye</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Jin</forename><surname>Bang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Madotto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pascale</forename><surname>Fung</surname></persName>
		</author>
		<idno type="DOI">10.1145/3571730</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Comput. Surv</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">12</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">JointGT: Graph-text joint representation learning for text generation from knowledge graphs</title>
		<author>
			<persName><forename type="first">Pei</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haozhe</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Ran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linfeng</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoyan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minlie</forename><surname>Huang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.findings-acl.223</idno>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2526" to="2538" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">What makes data-to-text generation hard for pretrained language models</title>
		<author>
			<persName><forename type="first">Moniba</forename><surname>Keymanesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adrian</forename><surname>Benton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Dredze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd Workshop on Natural Language Generation, Evaluation, and Metrics (GEM)</title>
		<meeting>the 2nd Workshop on Natural Language Generation, Evaluation, and Metrics (GEM)<address><addrLine>Abu Dhabi, United Arab Emirates</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="539" to="554" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Content analysis: An introduction to its methodology</title>
		<author>
			<persName><forename type="first">Klaus</forename><surname>Krippendorff</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>Sage publications</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension</title>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marjan</forename><surname>Ghazvininejad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abdelrahman</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.703</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="7871" to="7880" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">ROUGE: A package for automatic evaluation of summaries</title>
		<author>
			<persName><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Text summarization branches out</title>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="74" to="81" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A token-level reference-free hallucination detection benchmark for free-form text generation</title>
		<author>
			<persName><forename type="first">Tianyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yizhe</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Brockett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhifang</forename><surname>Sui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weizhu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bill</forename><surname>Dolan</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2022.acl-long.464</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 60th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="6723" to="6737" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">DART: Opendomain structured data record to text generation</title>
		<author>
			<persName><forename type="first">Linyong</forename><surname>Nan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dragomir</forename><surname>Radev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amrit</forename><surname>Rau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhinand</forename><surname>Sivaprasad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chiachun</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangru</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aadit</forename><surname>Vyas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neha</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yangxiaokang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nadia</forename><surname>Irwanto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jessica</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Faiaz</forename><surname>Rahman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ahmad</forename><surname>Zaidi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mutethia</forename><surname>Mutuma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yasin</forename><surname>Tarabar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ankit</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Chern Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xi</forename><surname>Victoria Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nazneen</forename><surname>Fatema</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rajani</forename></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.naacl-main.37</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="432" to="447" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">BLEU: a method for automatic evaluation of machine translation</title>
		<author>
			<persName><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei-Jing</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th annual meeting of the Association for Computational Linguistics</title>
		<meeting>the 40th annual meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">OpenAI blog</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="5485" to="5551" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Data-questeval: A referenceless metric for data-totext semantic evaluation</title>
		<author>
			<persName><forename type="first">Clement</forename><surname>Rebuffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Scialom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laure</forename><surname>Soulier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Piwowarski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sylvain</forename><surname>Lamprier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacopo</forename><surname>Staiano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Scoutheeten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Gallinari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2021 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="8029" to="8036" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Investigating pretrained language models for graph-to-text generation</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">R</forename><surname>Leonardo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Ribeiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hinrich</forename><surname>Schmitt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iryna</forename><surname>Schütze</surname></persName>
		</author>
		<author>
			<persName><surname>Gurevych</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.nlp4convai-1.20</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd Workshop on Natural Language Processing for Conversational AI</title>
		<meeting>the 3rd Workshop on Natural Language Processing for Conversational AI</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="211" to="227" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Modeling global and local node contexts for text generation from knowledge graphs</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">R</forename><surname>Leonardo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Ribeiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Claire</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iryna</forename><surname>Gardent</surname></persName>
		</author>
		<author>
			<persName><surname>Gurevych</surname></persName>
		</author>
		<idno type="DOI">10.1162/tacl_a_00332</idno>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="589" to="604" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Inside quasimodo: Exploring construction and usage of commonsense knowledge</title>
		<author>
			<persName><forename type="first">Julien</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Razniewski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th ACM International Conference on Information &amp; Knowledge Management</title>
		<meeting>the 29th ACM International Conference on Information &amp; Knowledge Management</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="3445" to="3448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A survey of evaluation metrics used for nlg systems</title>
		<author>
			<persName><forename type="first">Akash</forename><surname>Ananya B Sai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mitesh</forename><forename type="middle">M</forename><surname>Kumar Mohankumar</surname></persName>
		</author>
		<author>
			<persName><surname>Khapra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys (CSUR)</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1" to="39" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">BLEURT: Learning robust metrics for text generation</title>
		<author>
			<persName><forename type="first">Thibault</forename><surname>Sellam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dipanjan</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ankur P</forename><surname>Parikh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.04696</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">WebNLG challenge: Human evaluation results</title>
		<author>
			<persName><forename type="first">Anastasia</forename><surname>Shimorina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Claire</forename><surname>Gardent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shashi</forename><surname>Narayan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laura</forename><surname>Perez-Beltrachini</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
		<respStmt>
			<orgName>Loria &amp; Inria Grand Est</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Open information extraction with entity focused constraints</title>
		<author>
			<persName><forename type="first">Prajna</forename><surname>Upadhyay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oana</forename><surname>Balalau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ioana</forename><surname>Manolescu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2023.findings-eacl.95</idno>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: EACL 2023</title>
		<meeting><address><addrLine>Dubrovnik, Croatia</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="1285" to="1296" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Comparing dependent robust correlations</title>
		<author>
			<persName><surname>Rand R Wilcox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">British Journal of Mathematical and Statistical Psychology</title>
		<imprint>
			<biblScope unit="volume">69</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="215" to="224" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Semantic parsing via staged query graph generation: Question answering with knowledge base</title>
		<author>
			<persName><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<idno type="DOI">10.3115/v1/P15-1128</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1321" to="1331" />
		</imprint>
	</monogr>
	<note>Long Papers</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Bartscore: Evaluating generated text as text generation</title>
		<author>
			<persName><forename type="first">Weizhe</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengfei</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="27263" to="27277" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Bertscore: Evaluating text generation with bert</title>
		<author>
			<persName><forename type="first">Tianyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Varsha</forename><surname>Kishore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felix</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kilian</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoav</forename><surname>Artzi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.09675</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Seq2sql: Generating structured queries from natural language using reinforcement learning</title>
		<author>
			<persName><forename type="first">Victor</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.00103</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Commonsense knowledge aware conversation generation with graph attention</title>
		<author>
			<persName><forename type="first">Hao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minlie</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haizhou</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingfang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoyan</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4623" to="4629" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
