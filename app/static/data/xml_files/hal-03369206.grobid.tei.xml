<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Study on acoustic model personalization in a context of collaborative learning constrained by privacy preservation</title>
				<funder ref="#_P4DCEty">
					<orgName type="full">Agence Nationale de la Recherche</orgName>
					<orgName type="abbreviated">ANR</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Salima</forename><surname>Mdhaffar</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Marc</forename><surname>Tommasi</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">UMRCRIStAL</orgName>
								<orgName type="institution" key="instit1">Université de Lille</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
								<orgName type="institution" key="instit3">Inria</orgName>
								<orgName type="institution" key="instit4">Centrale Lille</orgName>
								<address>
									<postCode>9189</postCode>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yannick</forename><surname>Estève</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">LIA -Avignon Université</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Study on acoustic model personalization in a context of collaborative learning constrained by privacy preservation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">4CE0A5345D61B8BBC2E03D8DFA2FB81E</idno>
					<idno type="DOI">10.1007/978-3-030-87802-3_39</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-04-12T14:49+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>automatic speech recognition</term>
					<term>privacy-protection</term>
					<term>collaborative learning</term>
					<term>acoustic models</term>
					<term>personalization</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper investigates different approaches in order to improve the performance of a speech recognition system for a given speaker by using no more than 5 minutes of speech from this speaker, and without exchanging data from other users/speakers. Inspired by the federated learning paradigm, we consider speakers that have access to a personalized database of their own speech, learn an acoustic model and collaborate with other speakers in a network to improve their model. Several local personalizations are explored depending on how aggregation mechanisms are performed. We study the impact of selecting, in an adaptive way, a subset of speakers's models based on a notion of similarity. We also investigate the effect of weighted averaging of fine-tuned and global models. In our approach, only neural acoustic model parameters are exchanged and no audio data is exchanged. By avoiding communicating their personal data, the proposed approach tends to preserve the privacy of speakers. Experiments conducted on the TEDLIUM 3 dataset show that the best improvement is given by averaging a subset of different acoustic models fine-tuned on several user datasets. Our approach applied to HMM/TDNN acoustic models improves quickly and significantly the ASR performance in terms of WER (for instance in one of our two evaluation datasets, from 14.84% to 13.45% with less than 5 minutes of speech per speaker)</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>User interface of modern electronic and personal devices are more and more based on voice interaction and this tendency would probably continue increasing during the next years. Automatic speech recognition (ASR) is the technology at the core of voice interaction systems. To attain a satisfactory level of usability, ASR models need to be trained with a huge amount of training data costly to be collected and annotated. But an important issue with the data collection is privacy preservation. Indeed, some users are now very reluctant to use software solutions that do not preserve their privacy. Efforts towards privacy have been made by European states and the General Data Protection Regulation (GDPR) for instance constraints the way to realize data collection. Speech signals can be considered as sensitive information because in addition to the linguistic content, speech also brings information about the speaker: identity, gender, age, health, emotion... <ref type="bibr" target="#b8">[8]</ref> Different levels of privacy preservation can be defined according to the private information to preserve. Two main approaches have been proposed depending on the information to hide. In the Interspeech VoicePrivacy Challenge <ref type="bibr" target="#b13">[13]</ref>, the aim of privacy preservation consisted in modifying the speech representation features, trying to remove the speaker identity without removing the linguistic content. In this scenario, the data is first anonymized, and then collected. Even if very promising results have been reached with these contributions, data anonymization is still imperfect and negatively impacts the performance of ASR systems. Another approach consists in avoiding to share data: data is only used locally, on the user device, to personalize the model to this user. Then the models are exchanged, assuming that adapted models contain less sensitive information than the data itself. Such approaches have been used in different works for speech recognition <ref type="bibr" target="#b7">[7]</ref>, mainly through the use of distributed learning to speech the acoustic model train process <ref type="bibr" target="#b16">[16]</ref>.</p><p>Instead of targeting the improvement of a single general model by sharing anonymized data or applying a distributed learning approach, we propose in this paper to focus on the personalization of an initial model to each user. Inspired by widespread of powerful personal devices, we consider speakers that have access to a personalized database of their own speech. In this scenario, closely related to personalized federated learning <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b14">14]</ref>, it is possible to both locally fine-tune an acoustic model and collaborate with other speakers in a network to improve their own model.</p><p>The paper is organised along the following lines. Section 2 presents related works. Section 3 details the model adaptation. The experimental setup are described in Section 4. The experimental results are presented in Section 5 before concluding and giving some perspectives in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>In this study the term 'personalization' can be interpreted as 'speaker adaptation', more used in the speech community. A nice overview of speaker adaptation techniques for neural acoustic models has been presented in <ref type="bibr" target="#b0">[1]</ref>, that classifies adaptation techniques into three categories: embedding-based approaches that relies to the use of auxiliary speaker-dependent features like i-vectors <ref type="bibr" target="#b2">[3]</ref>, modelbased approaches that relies to speaker data to update the neural weights, and data augmentation approaches 'which attempt to synthetically generate additional training data with a close match to the target speaker, by transforming the existing training data'. No approach based on the use of collaborative train-ing was mentioned in this paper for speaker adaptation, but collaborative training has already been investigated for acoustic model training. In <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b7">7]</ref>, federated learning was applied to improve a general shared acoustic model with the goal of privacy preservation, but no speaker adaptation was targeted. Federated learning was also experimented in <ref type="bibr" target="#b3">[4]</ref> to speed up the training process and improve the shared general acoustic model performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Model adaptation</head><p>Our objective is to locally improve the acoustic model for a target speaker by taking advantage from both local pre-existing data and from pre-existing models specific to other users. In our scenario, a global acoustic model is available, trained on the initial corpus. This global model is distributed to all the devices, on which it is possible to fine-tune a local instance of the global model by exploiting locally the user data. These local models can be shared in order to indirectly take benefit from the local data used to their adaptation, through a model averaging.</p><p>Since the number of speakers (i.e devices) can be very high, and so the number of adapted models, it seems relevant to propose a strategy to better select these local models that could be use to adapt the model of a target speaker. In a classical hybrid HMM/DNN speech recognition acoustic features like MFCC (Mel-Frequency Cepstral Coefficients) are generally augmented with additional speaker-specific features like i-vectors <ref type="bibr" target="#b6">[6]</ref> or x-vectors <ref type="bibr" target="#b12">[12]</ref> that can capture information about the speaker. In this work, we assume that this kind of information cannot be exchanged between the different devices since we want to avoid to share explicit knowledge about the speaker and the linguistic content present in the data. To select the best candidate models from the other speakers, we suggest to consider the euclidean distance between candidate models and the model fine-tuned on the target user data.</p><p>Our study explores different ways to adapt a HMM/DNN acoustic model through the use of model averaging, local fine-tuning, or a combination of these approaches. The aim of this adaptation is to modify the parameters of the (generic) neural network involved in the HMM/DNN architecture. Fine-tuning consists in continuing the training process of the generic acoustic model on a small dataset of the target speaker, by taking care on avoiding overfitting. Model averaging consists on computing a model whose each weight is the average of the weights extracted from a set of models that share the same neural topology.</p><p>Figure <ref type="figure" target="#fig_0">1</ref> illustrates the adaptation approach explored in this work. In this framework, no user data is shared: the fine-tuning is made locally, only adapted models can be exchanged.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental setup</head><p>This section describes the ASR system, the experimental methodology and the datasets used for the experiments on speaker adaptation through fine-tuning and </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">ASR system</head><p>The ASR system is based on the Kaldi toolkit <ref type="bibr" target="#b9">[9]</ref>. Acoustic models are based on a chain-TDNN approach <ref type="bibr" target="#b10">[10]</ref>. The chain-TDNN setup is based on 13 layers with dimension 512 and is trained on cepstral mean and variance normalized 40-dimensional MFCC features concatenated with 9 left and 9 right neighbor frames. We also incorporates i-vectors as an additional input features. The acoustic model has about 14 million parameters. The initial and final learning rates were equal to 0.00025 and 0.000025 respectively. Training audio samples were randomly perturbed in speed and volume during the training process. This approach is commonly called audio augmentation.</p><p>When fine-tuning the generic model on target speaker data, we modify only the value of learning rate (the initial and final learning rates were equal to 0.000025 and 0.000015 respectively) and all hyperparameters (i.e. learning rate and local epochs number) are assumed to be homogeneous among all workers.</p><p>We make available complete recipes for building the generic acoustic model and the fine-tuned models 3 .</p><p>As described below, the TEDLIUM 3 dataset was used to train the acoustic models. Data used to train the model is not a part of the TEDLIUM 3 data and is described in <ref type="bibr" target="#b11">[11]</ref>. The language model used in our experiment is a 4-gram model, which was pruned to 10 million n-grams.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Experimental Methodology</head><p>The experiments are conducted in the following way. We start by building a generic ASR model using a large set of utterances from many speakers. Then, every speaker is associated with a worker that fine-tunes the generic model with a fresh set of utterances of the given speaker. We obtain a set of fine-tuned ASR models. Then we try to collaboratively improve these fine-tuned models in different ways.</p><p>Let us consider in the following a set of n different speakers. Let us denote by G the generic model, P s the fine-tuned model of speaker s. We call an average model of a given set of models, the model defined by the average of model parameters component wise. We denote by Ps the per-speaker average of all fine-tuned models P s except P s : i.e Ps = 1 n-1 s =s P s . For a given speaker s and an integer k, we denote by B≤k s the average of the k best models, measured by the WER on the set of utterances of s used to fine-tune P s .</p><p>We also performed a hierarchical clustering on personal models represented by the vector of their weights of the first layer only. This choice of the first layer as a representative layer comes from a preliminary study we made. This study showed us that the word error rate obtained by using an acoustic model fine tuned on a non-target speaker and the Euclidean distance between the first layer of this model and the first layer of the model fine-tuned on the target speaker data is the most correlated, in comparison to the use of other layers. This is illustrated by Figure <ref type="figure" target="#fig_1">2</ref>. For the hierarchical clustering, we use the Numpy library 4 with the ward linkage function. Let us recall that the principle of the hierarchical clustering is to build a hierarchy of clusters in bottom-up fashion. The Euclidean distance between weight vectors is used to compute the distance between neural network models. In an iterative process, the two closest clusters are successively merged until only one remains. The output can be represented by a dendrogram. The Ward linkage function <ref type="bibr" target="#b15">[15]</ref> is used to evaluate the distance between clusters. It is based on minimum variance method and allows to minimize the total withincluster variance.</p><p>Using this dendrogram, for a given speaker s and an integer k, we compute D≤k s the average of k closest models to P s (in terms of distance within the dendrogram). <ref type="foot" target="#foot_1">5</ref>We perform different kinds of aggregation of the fine-tuned models using a weighted average combined to:</p><p>1. the generic model G, 2. or the per-speaker average of all fine-tuned models Ps 3. or the k-best fine-tuned models B≤k s (models that have the lowest WER on data from speaker s) 4. or the k-nearest neighbours models D≤k s where the similarity is given by a dendrogram of Euclidean distance between model weights 5. or the average of k models taken at random among all fine-tuned models Rk s .</p><p>The weighted average of P s with one these models M ∈ {G, Ps , B≤k s , D≤k s , R k s } is computed by αP s + (1 -α)M , i.e. in a component-wise convex combination of the weights.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Datasets</head><p>All experiments to train acoustic models were conducted with the TEDLIUM 3 dataset, a large corpus of 452 hours of TED talks given by 2,295 speakers. The dataset is ready for training ASR systems but also dedicated to speaker adaptation tasks. We processed the dataset in an original way for this set of experiments. We split it into three parts so that the sets of speakers in each part are pairwise disjoints. Characteristics of the three parts are reported in Table <ref type="table" target="#tab_0">1</ref>. The first part is called generic and has been used to train an initial acoustic model for ASR. The two other parts called perso1 and perso2 are used for 2 distinct trials of model personalization and evaluation. In each part p ∈ {perso1, perso2}, for each speaker s, we consider a small subset of 5 minutes of speech data called train s p to fine-tune a per-speaker model and the remaining is called test s p and used for evaluation. These datasets are never shared (or merged) with other data. We consider them as personal and private datasets belonging to speakers. The average duration of test s p data is presented in the third line in Table <ref type="table" target="#tab_0">1</ref>. For the reproductibility of experimental results by research community, we give the list of the new division of the dataset The values of average WER for the different base models computed as explained above are reported in Table <ref type="table">3</ref> for the values of α = 0, when the private data of the given speaker has not been used and α = 1/2 when the fine-tuned model and the other model equally contribute to the averaged model. A graphical representation of the results is given in Figure <ref type="figure" target="#fig_2">3</ref> for α varying between 0 and 1.  <ref type="table">3</ref>. Results of collaborative learning with (α = 0.5) and without (α = 0) finetuned model of the target speaker. The significance of our results is measured using WER and using the 95% confidence interval. Confidence interval for the perso1 is 0.08 and 0.07 for perso2. Confidence interval means that if the improvement in WER exceed this value, we can consider it as significant improvement. The generic model G is considered as a good model as it is trained by mixing all the training data such that training is carried out on public dataset. WER of G evaluated on the test part of the two datasets perso1 and perso2 are presented in line 1 in Table <ref type="table" target="#tab_1">2</ref>. We observe that the values are significantly different. The second line in Table <ref type="table" target="#tab_1">2</ref> presents results for the speaker model (the local model trained by fine-tuning using only 5 minutes of speech from the target speaker). Compared to the generic model in Table <ref type="table" target="#tab_1">2</ref>, speaker model improves the WERs for the two sets perso1 and perso2. It is also treated as a good model since it is trained using data from the target speaker.</p><formula xml:id="formula_0">α = 0 α = 0.</formula><p>Table <ref type="table">3</ref> shows results obtained under various aggregation of acoustic model. Results are given for two configurations: with and without the local fine-tuned model of the target speaker.</p><p>Collaborative learning without the local fine-tuned model of the target speaker (α = 0): The second line presents results of aggregation of all fine-tuned models except the target speaker. An improvement is shown compared to the generic model (from 15.43% to 15.21% for perso1 and from 14.84% to 14.62% for perso2). This improvement is significant since it exceeds the confidence interval value (the absolute gain for perso1 is 0.22 (0.22&gt;0.08) and the same for perso2).</p><p>The third line in Table <ref type="table">3</ref> presents results of aggregation of k-nearest neighbours models. The selection of k-nearest neighbours models does not improve results compared to the generic model and the model of all speakers (15.49% WER for perso1 and 14.81% WER for perso2). This selection is compared to a random selection. Results are presented in the line 4 in Table <ref type="table">3</ref>. Results of random selection are better than the results of dendrogram-based selection. Surprisingly, the benefit of using close models is not empirically demonstrated. This may be due to several factors. Either the distance is not reflecting a notion of usefulness or some amount of diversity is necessary to obtain models that behave well on new data. It should be noted that the first layer has maybe a too large number of parameters to compute a meaningful distance. We also tried to reduce the dimension of this vector, but the impact on the correlations with the WER (computed in a similar way than in Fig 2 ) was not observable.</p><p>Collaborative learning with the local fine-tuned model of the target speaker (α = 0.5): As shown in Table <ref type="table" target="#tab_1">2</ref>, the fine-tuned model using a small local dataset improves WER compared to the generic model. So, we decide to take benefit from this improvement by aggregating models obtained with α = 0 with the fine-tuned model of the target speaker (note that this model is trained with a very small dataset). Results in Table <ref type="table">3</ref> shows a significant improvement in WER for all kinds of aggregation.</p><p>Acoustic models are prone to overfitting when the training dataset is limited. This could explain why the speaker model P s cannot get very high performance, since speakers' models are trained using only 5 minutes. Averaging speakers' models with the target speaker model allows us to produce a more accurate model than the target speaker model. The weight value used to combine the target speaker model P s and an aggregated model has an influence on the resulting model combination. This is illustrated in Figure <ref type="figure" target="#fig_2">3</ref>. Results in Figure <ref type="figure" target="#fig_2">3</ref> show also that with a good aggregated model, there is less need of the fine-tuned model P s to get better results. This is particularly visible when combining P s to the B≤10 s aggregated model, made by averaging the ten models fine-tuned on other speakers that got the lowest WERs when applied to the target speaker data. This final combined model outperforms all the other ones, but its usage seems unrealistic since a local decoding process on the target user data is necessary for all the available non-target speaker models P s . However, these results provide good indications to continue this work on acoustic model collaborative personalization with privacy preservation constraints.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we investigate a collaborative learning algorithm to locally improve the performance of an automatic speech recognition system, without sharing data (but models). In this purpose, we suggest to take benefit from acoustic models that have been separately fine-tuned for each user, in addition to the local fine-tuning on the target speaker data. Two kinds of local personalizations are explored, based on a fine-tuning processed on local data, and model averaging. Significant improvements are observed when these two local personalizations are combined through a weighted average. We also observed that a random selection of non-target speaker models gives better results than a non-naive approach. In a scenario where computations are not limited, a selection of non-target speaker models based on their performances -in terms of WER -on the target speaker data gives the best results.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Model personnalization for a target speaker</figDesc><graphic coords="5,134.77,115.83,352.40,167.60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Pearson correlation between WER and Euclidian distance in function of the layer order on the perso1 dataset, described in Section 4.3</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. WER according to different weighted average of Ps with four different aggregated models: all fine-tuned models Ps, generic model G, 10-best (WER) fine-tuned models B≤10 s , 50 random models R≤50 s on perso1 (top) and perso2 (bottom).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>6 . TEDLIUM3 dataset5 Experimental resultsIn our experiments, we take k equals to 50, except for B≤k s , where k = 10. We measure the average of WER of different models on the test s p data. In a more formal way, if we denote by WER(M, S) the word error rate of model M on the dataset S, then we compute averages in the following way. For each part p ∈ {perso1, perso2}, and for a base model M s in {G, Ps , B≤k s , D≤k s , R k s }, we compute the average WER on part p as 1 n n s=1 WER(αP s + (1 -α) Ms , test s p ). The word error rate of the generic model G and the fine-tuned model P s are given in Table 2. Generic model G 15.43 14.84 Speaker model Ps 15.04 14.63</figDesc><table><row><cell>generic perso1 perso2</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Word Error Rate of the generic and the fine-tuned models.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_0"><p>https://docs.scipy.org/doc/scipy/reference/generated/scipy.cluster. hierarchy.linkage.html</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_1"><p>Note that this set may not be unique and we build it iteratively, starting from the closest cluster and choosing models uniformly at random in the last iteration.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_2"><p>https://github.com/mdhaffar/Acoustic model personalisation</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="7">Acknowledgements</head><p>This work was supported by the <rs type="funder">French National Research Agency</rs> under project <rs type="projectName">DEEP-PRIVACY</rs> (<rs type="grantNumber">ANR-18-CE23-0018</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_P4DCEty">
					<idno type="grant-number">ANR-18-CE23-0018</idno>
					<orgName type="project" subtype="full">DEEP-PRIVACY</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Adaptation algorithms for neural network-based speech recognition: An overview</title>
		<author>
			<persName><forename type="first">P</forename><surname>Bell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Fainberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Klejch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Renals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Swietojanski</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.06580</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Federated acoustic modeling for automatic speech recognition</title>
		<author>
			<persName><forename type="first">X</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kingsbury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="6748" to="6752" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Front-end factor analysis for speaker verification</title>
		<author>
			<persName><forename type="first">N</forename><surname>Dehak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Kenny</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Dehak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dumouchel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ouellet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="788" to="798" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A federated approach in training acoustic models</title>
		<author>
			<persName><forename type="first">D</forename><surname>Dimitriadis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kumatani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Gmyr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gaur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">E</forename><surname>Eskimez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Personalized federated learning with theoretical guarantees: A model-agnostic meta-learning approach</title>
		<author>
			<persName><forename type="first">A</forename><surname>Fallah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mokhtari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ozdaglar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>Balcan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Lin</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="3557" to="3568" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title/>
		<author>
			<persName><surname>Curran Associates</surname></persName>
		</author>
		<author>
			<persName><surname>Inc</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2020/file/24389bfe4fe2eba8bf9aa9203a44cdad-Paper.pdf" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">I-vector-based speaker adaptation of deep neural networks for french broadcast audio transcription</title>
		<author>
			<persName><forename type="first">V</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kenny</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ouellet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Stafylakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2014 IEEE international conference on acoustics, speech and signal processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="6334" to="6338" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Federated learning for keyword spotting</title>
		<author>
			<persName><forename type="first">D</forename><surname>Leroy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Coucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lavril</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Gisselbrecht</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dureau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6341" to="6345" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Preserving privacy in speaker and speech characterisation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Nautsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jiménez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Treiber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kolberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Jasserand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Kindt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Delgado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Todisco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Hmani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mtibaa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Speech &amp; Language</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="page" from="441" to="480" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The kaldi speech recognition toolkit</title>
		<author>
			<persName><forename type="first">D</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ghoshal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Boulianne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Burget</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Glembek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hannemann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Motlicek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Schwarz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 2011 workshop on automatic speech recognition and understanding. No. CONF</title>
		<imprint>
			<publisher>IEEE Signal Processing Society</publisher>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Purely sequence-trained neural networks for asr based on lattice-free mmi</title>
		<author>
			<persName><forename type="first">D</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Peddinti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Galvez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ghahremani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Manohar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Na</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Interspeech</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2751" to="2755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Enhancing the ted-lium corpus with selected data for language modeling and more ted talks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Rousseau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Deléglise</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Esteve</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">LREC</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="3935" to="3939" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">X-vectors: Robust dnn embeddings for speaker recognition</title>
		<author>
			<persName><forename type="first">D</forename><surname>Snyder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Garcia-Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="5329" to="5333" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">N</forename><surname>Tomashenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">M L</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Nautsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yamagishi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Patino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Bonastre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">G</forename><surname>Noé</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.01387</idno>
		<title level="m">Introducing the voiceprivacy initiative</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mathews</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kiddon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Eichner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Beaufays</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ramage</surname></persName>
		</author>
		<title level="m">Federated evaluation of on-device personalization</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Hierarchical grouping to optimize an objective function</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Ward</surname><genName>Jr</genName></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American statistical association</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">301</biblScope>
			<biblScope unit="page" from="236" to="244" />
			<date type="published" when="1963">1963</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Distributed deep learning strategies for automatic speech recognition</title>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Finkler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kingsbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Saon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Picheny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="5706" to="5710" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
