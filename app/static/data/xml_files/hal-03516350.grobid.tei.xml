<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Embedding Knowledge Graphs Attentive to Positional and Centrality Qualities</title>
				<funder ref="#_nWhwA6r">
					<orgName type="full">MLwin</orgName>
				</funder>
				<funder ref="#_TvNnpDH">
					<orgName type="full">EU</orgName>
				</funder>
				<funder>
					<orgName type="full">Federal Ministry of Education and Research of Germany</orgName>
				</funder>
				<funder ref="#_CqDUfgr">
					<orgName type="full">Federal Ministry for Economic Affairs and Energy (BMWi)</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Afshin</forename><surname>Sadeghi</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Smart Data Analytics Group</orgName>
								<orgName type="institution">University of Bonn</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Fraunhofer IAIS</orgName>
								<address>
									<settlement>Sankt Augustin</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Diego</forename><surname>Collarana</surname></persName>
							<idno type="ORCID">0000-0001-7185-7210</idno>
							<affiliation key="aff1">
								<orgName type="institution">Fraunhofer IAIS</orgName>
								<address>
									<settlement>Sankt Augustin</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">Universidad Privada Boliviana</orgName>
								<address>
									<country>Bolivia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Damien</forename><surname>Graux</surname></persName>
							<idno type="ORCID">0000-0002-2583-0778</idno>
							<affiliation key="aff2">
								<orgName type="institution" key="instit1">Inria</orgName>
								<orgName type="institution" key="instit2">Université Côte d&apos;Azur</orgName>
								<orgName type="institution" key="instit3">CNRS</orgName>
								<address>
									<region>I3S</region>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Jens</forename><surname>Lehmann</surname></persName>
							<email>jens.lehmann@iais.fraunhofer.dedamien.graux@inria.fr</email>
							<idno type="ORCID">0000-0003-3392-3162</idno>
							<affiliation key="aff0">
								<orgName type="laboratory">Smart Data Analytics Group</orgName>
								<orgName type="institution">University of Bonn</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Fraunhofer IAIS</orgName>
								<address>
									<settlement>Sankt Augustin</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Embedding Knowledge Graphs Attentive to Positional and Centrality Qualities</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">FEC226EAEC23DE57D71DDDF1C151B613</idno>
					<idno type="DOI">10.1007/978-3-030-86520-7_34</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-04-12T14:44+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Knowledge graphs embeddings (KGE) are lately at the center of many artificial intelligence studies due to their applicability for solving downstream tasks, including link prediction and node classification. However, most Knowledge Graph embedding models encode, into the vector space, only the local graph structure of an entity, i.e., information of the 1-hop neighborhood. Capturing not only local graph structure but global features of entities are crucial for prediction tasks on Knowledge Graphs. This work proposes a novel KGE method named Graph Feature Attentive Neural Network (GFA-NN) that computes graphical features of entities. As a consequence, the resulting embeddings are attentive to two types of global network features. First, nodes' relative centrality is based on the observation that some of the entities are more "prominent" than the others. Second, the relative position of entities in the graph. GFA-NN computes several centrality values per entity, generates a random set of reference nodes' entities, and computes a given entity's shortest path to each entity in the reference set. It then learns this information through optimization of objectives specified on each of these features. We investigate GFA-NN on several link prediction benchmarks in the inductive and transductive setting and show that GFA-NN achieves on-par or better results than state-of-the-art KGE solutions.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Knowledge graphs (KGs) are capable of integrating heterogeneous data sources under the same graph data model. Thus KGs are at the center of many artificial intelligence studies. KG nodes represent concepts (entities), and labeled edges represent the relation between these entities <ref type="foot" target="#foot_0">1</ref> . KGs such as Wikidata, WordNet, Freebase, and Nell include millions of entities and relations representing the current knowledge about the world. KGs in combination with Machine Learning models are used for refining the Knowledge Graph itself and for downstream tasks, like link prediction and node classification. However, to use KGs in Machine Learning methods, we need to transform graph representation into a vector space presentation, named Knowledge Graph embeddings (KGE).</p><p>KGE have many applications including analysis of social networks and biological pathways. Thus, many approaches have been proposed ranging from translation methods, e.g., Trans* family <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b28">29]</ref>; Rotation-based methods, e.g., RotatE <ref type="bibr" target="#b19">[20]</ref>; Graph Convolutional methods, e.g., R-GCN <ref type="bibr" target="#b18">[19]</ref>, COMPGCN <ref type="bibr" target="#b24">[25]</ref>, and TransGCN <ref type="bibr" target="#b3">[4]</ref>; and Walk-based methods , e.g., RDF2Vec <ref type="bibr" target="#b15">[16]</ref>. Traditional graph embedding methods, however, rely exclusively on facts (triples) that are explicitly present in a Knowledge Graph. Therefore, their prediction ability is limited to a set of incomplete facts. A means of improvement is to incorporate complementary information in the embeddings. A class of methods applies external knowledge such as entity text descriptions <ref type="bibr" target="#b29">[30]</ref> and text associations related to entities <ref type="bibr" target="#b25">[26]</ref> into the KG modeling. In contrast, intrinsic methods extract complementary knowledge from the same KG. For example, the algorithms that derive logical rules from a KG and combine them with embeddings of the KG <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b27">28]</ref>. Analogously recent studies <ref type="bibr" target="#b34">[35]</ref> consider graph structural features as an intrinsic aspect of KGs in the embedding.</p><p>We motivate our model by addressing a challenge of most KGE models; These methods independently learn the existence of relation from an entity to its hop-1 neighborhood. This learning strategy neglects the fact that entities located at a distance can still affect an entity's role in the graph. Besides that, the location of the entities in the network can be useful to distinguish nodes. Figure <ref type="figure" target="#fig_0">1</ref> illustrates such an example where the goal is to learn embeddings for e 1 and e 2 entities in the KG. Distinguishing between the two candidates, i.e., George W. Bush and George H. W. Bush, is challenging for previous methods since e 1 and e 2 have almost the same neighbors, except George W. Bush graduated from Harvard University while George H. W. Bush did not.</p><p>However, If we compare e 1 to e 2 using their eigenvector centrality, we can easily distinguish them. e 1 has a greater centrality than e 2 since e 1 is connected to Harvard that has a high eigenvector centrality. Analogously, if we consider the shortest path of e 1 and e 2 to e 3 that belongs to set of reference node S, their distance to e 3 is different. Intuitively, if a model could beforehand know the centrality and distance to e 3 as additional knowledge, it can more easily model e 1 and e 2 and rank them correctly.</p><p>With a new view to Knowledge Graph embeddings, we propose GFA-NN<ref type="foot" target="#foot_1">2</ref> , an approach that learns both the local relations between the entities and their global properties in one model. In order to efficiently encode entity indicators in Knowledge Graph modeling, we focus on learning node centrality and positional indicators, (e.g., the degree, Katz, or eigenvalue centrality of entities in the graph) as well as the Knowledge Graph structure.</p><p>For this purpose, we fuse the modeling of each entity indicator in the style of Multiple Distance Embedding (MDE) <ref type="bibr" target="#b16">[17]</ref> where distinct views to Knowledge Graphs are modeled through independent embedding weights.</p><p>GFA-NN extracts positional information and four centrality indicators of nodes from the KG and defines a learning function for each one. Then GFA-NN scores their aggregation with MDE.</p><p>Previously, different leanings were applied to embedding models using constraints in the loss function. Now that MDE has broken the limitation of using more than one objective function on independent embeddings, we directly add new extracted information about the entities as aggregated objective functions.</p><p>Centrality values and position of nodes in graphs are global measurements for nodes across the whole graph. If we use a local assignment, for example the number of paths between specific nodes, this measurement may have different wights based on what portion of the network is considered in the calculation.</p><p>Despite the exciting recent advancements, most of the previous works fail to learn the relation between entities regarding the whole graph. Therefore, we define relative position attentive and relative centrality attentive functions for embedding the relative importance of nodes and their position relative to the whole network. In the following section, we discuss the relation between our work and the current state-of-the-art. Later in Section 3, we introduce the preliminaries and notations required to explain our chosen method. We outline in Section 4 the idea of centrality and positional qualities learning and explain our approach. In Section 5, we mention the model's theoretical analysis; and we continue with experiments that evaluate our model in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>A large and growing body of literature has investigated KGE models. A typical KGE model consists of three main elements: (1) entities and relations representation in a continuous vector space, (2) a scoring function to measure KG's facts plausibility, and (3) a loss function that allows learning KGE in a supervised manner. Based on this formulation, we classify KGE models in: latent distance approaches, tensor factorization and multiplicative models, and neural networks.</p><p>Latent Distance Models, e.g., Trans* <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b28">29]</ref> family, measure a fact's plausibility by scoring the distance between the two entities, usually after a translation carried out by the relation. RotatE <ref type="bibr" target="#b19">[20]</ref> combines translation and rotation. Ro-tatE models relations as rotations from head to tail entities in the complex space and uses the Hadamard product in the score function to do these rotations.</p><p>Tensor factorization and multiplicative approaches define the score of triples via pairwise multiplication of embeddings. DistMult <ref type="bibr" target="#b32">[33]</ref>, for example, multiplies the embedding vectors of a triple element by element (h, r, t) as the objective function. However, DistMult fails to distinguish displacement of head relation and tail entities, and therefore, it cannot model anti-symmetric relations. Com-plEx <ref type="bibr" target="#b22">[23]</ref> solves DistMult's issue.</p><p>Unlike previous methods, the neural network-based methods learn KGE by connecting artificial neurons in different layers. Graph Neural Network (GNN) aggregate node formation using a message-passing architecture. Recently, hybrid neural networks such as CompGCN <ref type="bibr" target="#b23">[24]</ref> and MDE nn <ref type="bibr" target="#b16">[17]</ref> have raised. These methods benefit from neural network architectures to model relations with (anti)symmetry, inversion, and composition patterns.</p><p>Several studies have investigated the benefits of using graph features to bridge the graph structure gap and the numeric vector space. Muzzamil et al. <ref type="bibr" target="#b13">[14]</ref> defined a Fuzzy Multilevel Graph Embedding (FMGE), an embedding of attributed graphs with many numeric values. P-GNN <ref type="bibr" target="#b34">[35]</ref> incorporates positional information by sampling anchor nodes and calculating their distance to a given node (see Section 5.1 for an in-depth comparison with GFA-NN). Finally, it learns a non-linear distance weighted aggregation scheme over the anchor nodes. This effort's main difference with previous approaches is in the message passing mechanism. Traditionally in GNNs, approaches learn just nodes' local features (similar to the modeling schema of KGEs) while focusing on neighbor nodes; here, our approach also learns nodes' features regarding the whole graph, known as global graph properties.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Background and Notation</head><p>A Knowledge graph KG, is comprised of a set of entities e ∈ E and a set of relations r ∈ R. A fact in a Knowledge Graph is a triple of the form (h, r, t) in which h (head) and t (tail) are entities and r is a relation. A KG is a subset of all true facts KG ⊂ ξ. A KG can be conceived as a multi-relational graph. An entity in such formulation is equivalent to a node in graph theory, and an edge represents a relation. In this study, we use Node and Entity interchangeably. We use the term "Node" to emphasize its graphical properties. We use the term "Entity" to highlight the entity's concept.</p><p>Link prediction on Knowledge Graphs is made by a Siamese classifier that embeds KG's entities and relations into a low-dimensional space. Thus, a Knowledge Graph embedding model is a function f : E, R → Z, that maps entities E and relations R to d-dimensional vectors Z = {z 1 , . . . , z n }, z i ∈ R.</p><p>Centrality value of a node designates the importance of the node with regard to the whole graph. For instance, degree is a centrality attribute of a node that indicates the number of links incident upon it. When we consider degree as centrality value, the higher the degree of a node is, the greater is its importance in a graph. We provide a generalization of the position-aware embedding definition <ref type="bibr" target="#b34">[35]</ref> that distinguishes our method from the previous works.</p><p>Structure-based Embedding: A KG embedding z i = f : E, R → Z is attentive to network structure if it is a function of entities and relations such that it models the existence of a neighborhood of an entity e i using relations r i and other entities e j ∈ E. Most Knowledge Graph embedding methods like QuatE and RotatE compute embeddings using the information describing connections between entities and, therefore, structure-based.</p><p>Property-Attentive Embedding: A KG embedding z i = f : E, R → Z is attentive to network properties of an entity if there exists a function g p (., ., ...) such that d p (v i , v j , ...) = g p (z i , z j ), where d p (, ) is a graphical property in G. This definition includes both the property of a sole node such as its centrality and the properties that describe the inter-relation of two nodes such as their shortest path. Examples of Property-Attentive Embedding are P-GNNs and RDF2Vec, which their objective function incorporates the shortest path between nodes into embedding computation.</p><p>We show that current KGE methods cannot recover global graph properties, such as path distances between entities and centrality of nodes, limiting the performance in tasks where such information is beneficial. Principally, structureaware embeddings cannot be mapped to property-aware embeddings. Therefore, only using structure-aware embeddings as input is not sufficient when the learning task requires node property information. This work focuses on learning KGEs capturing both entities' local network structures conjointly with the global network properties. We validate our hypothesis that a trait between local and global network features is crucial for link prediction and node classification tasks. A KGE is attentive to node network properties if the embedding of two entities and their relation can be used to approximately estimate their network feature, e.g., their degree relative to other entities in the network.</p><p>You et al. <ref type="bibr" target="#b34">[35]</ref> show for position attentive networks, there exists a mapping g that maps structure-based embeddings</p><formula xml:id="formula_0">f st (v i ), ∀ v i ∈ V to position attentive embeddings f p (v i ), ∀ v i ∈ V , if</formula><p>and only if no pair of nodes have isomorphic local q-hop neighborhood graphs. This proposition justifies the good performance of KGE models in tasks requiring graphical properties and their under-performance in real-world graphs such as biological and omniscience KGs (e.g., Freebase, DBpedia), in which the structure of local neighborhoods are quite common. This proposition, however, does not hold for centrality attentive embeddings. The reason is that if no pair of nodes have isomorphic local q-hop neighborhood graphs, it is still possible for them to have the same centrally attentive embeddings. For example, two nodes with the same number of neighbors consisting of different nodes have the same degree; however, their neighborhoods are non-isometric. We show in Section 4 how we address this challenge for centrality learning. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Method</head><p>This Section details our proposed method for generating entity network properties attentive embeddings from Knowledge Graphs. We generalize the concept of Knowledge Graph embeddings with a primary insight that incorporating centrality and distance values enables KGE models to compute embeddings with respect to the graphical proprieties of entities relative to the whole network instead of only considering the direct local neighbors (Figure <ref type="figure" target="#fig_1">2</ref>, left side).</p><p>When modeling the positional information, instead of letting each entity model the information independently and selecting a new reference set per iteration, we keep a set of reference entities through training iterations and across all the networks in order to create comparable embeddings. This design choice enables the model to learn the position of nodes with respect to the spectrum of different reference node positions and makes each embedding attentive to position (Figure <ref type="figure" target="#fig_1">2</ref>, top left). GFA-NN models each graphical feature with a dedicated objective function, meaning that the information encrypted in centrality attentive embeddings does not interfere with the embedding vectors that keep the positional information (Figure <ref type="figure" target="#fig_1">2</ref>, top right).</p><p>Centrality for nodes are individual values. While positional values are calculated relative to a set of nodes in a graph, only one centrality per entity is extracted. Still, learning this information is valuable because the centrality value of a node is meaningful despite the absence of a large portion of the network. This trait is particularly beneficial in inductive relation prediction tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Model Formulation</head><p>The components of GFA-NN are as follows:</p><p>-Random set of reference nodes for distance calculations.</p><p>-Matrix M of distances to random entities, where each row i is a set of shortest distance of an entities to the selected set of random nodes. -Structure-attentive objective functions f st 1 (v i ), . . . , f st k (v i ) that model the relatedness information of two entities with their local network, which is indicated by triples that consist of head and tail nodes (entities) connected by an edge (relation).</p><p>-Position-attentive objective function F s that models the position of a node (entity) in the graph with respect to its distance to other nodes. This objective considers these distances as a factor of relatedness of entities. -Centrality attentive objective functions F c that model the relatedness information of two entities according to centrality properties of nodes (entities).</p><p>In this setting, the global importances of nodes are learned relatively to the centrality of other nodes. -Trainable aggregation function f 1×1 is a 1×1 convolution <ref type="bibr" target="#b11">[12]</ref> that fuses the modeling of the structure-based connectivity information of the entities and relations with their position aware and centrality attentive scoring. -Trainable vectors r d , h d , t d that project distance matrix M to a lower dimensional embedding space z ∈ R k .</p><p>Our approach consists of several centrality and position-attentive phases that each of which learns an indicator in a different metric of the status for entities relative to the network.</p><p>In the first phase, GFA-NN performs two types of computation to determine the position status and the centrality status of entities. The unit for centrality status computes the relative significance of entities as a vector of length one c j i , where j represents each of the centrality metrics. The unit for position status embedding samples n random reference-entities S n , and computes an embedding for entities. Each dimension i of the embedding is obtained by a function F that computes the shortest path to the i-th reference entity relative to the maximum shortest path in the network.</p><p>Then objective functions F s , F 1 c , ..., F 4 c apply an entity interaction model to enforce the property features e s i into entity embeddings e i , which in the next phase makes a 1 × 1 convolution <ref type="bibr" target="#b11">[12]</ref> over the scores via weights w ∈ R r and non-linear transformation Tanhshrink.</p><p>Specifically, each entity earns an embedding per attribute that includes values that reveal the relative status information from input entity network properties information. Calculation of the centrality for all nodes in the network leads to a vector representation of the graph for each measure, while the distances to the reference nodes S generate a dense matrix representation.</p><p>The network property attentive modeling functions are the same class of functions as used by existing translational KGEs plus a modeling function of embeddings that we extended to be performed in 3D using rotation matrix. In the following, we further elaborate on the design choices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Centrality-Attentive embedding:</head><p>As shown in Section 3, the centrality values are not canonical. Therefore, the model learns their difference in a normal form, in which the equality of their norm does not mean they are equal. Degree centrality is defined as :</p><formula xml:id="formula_1">C d (n) = deg(n).</formula><p>Katz centrality <ref type="bibr" target="#b7">[8]</ref> extends degree centrality from counting neighbor nodes to nodes that can be connected through a path, where the contribution of distant nodes are reduced:</p><formula xml:id="formula_2">C k (n) = ∞ k=1 N j=1 α k A k j,i</formula><p>where A is the adjacency matrix and α is attenuation factor in the range (0, 1). Another included centrality measure is PageRank with the following formulation:</p><formula xml:id="formula_3">C p (n) = α j a j,i C p (j L(j) + 1 -α N</formula><p>where N is |V |, the number of nodes in the graph, and L(j) is the degree of node j. Relative eingenvector centrality score of a node n is defined as:</p><formula xml:id="formula_4">C e i(n) = 1 λ m∈KG a m,n x m</formula><p>where A = (a v,t ) is the adjacency matrix such that a v,t = 1 if node n is linked to node m, and a v,t = 0 otherwise. λ is a constant which fulfils the eingenvector formulation Ax = λx. Note that the method in first phase normalizes each of the centrality values. The normalization occurs with respect to minimum and the maximum value for nodes in the network and makes attributes relative to the whole network. For example, degree centrality is normalized as follows:</p><formula xml:id="formula_5">C d i =</formula><p>degree(i) -degree min degree max -degree min</p><p>The centrality-attentive modeling embeddings functions are the same class of dissimilarity functions used by existing KGEs plus a penalty we define on the difference of the entity embeddings as:</p><formula xml:id="formula_6">F c d = h i -t i 2 - cos(log(C d h )) -cos(log(C d t )) 2<label>(1)</label></formula><p>where the function is normalized with the l 2 norm, h i and t i represent the vector representation of head and tail in a triple and lastly, C d h and C d t respectively denote the centrality values of the head and tail entities in that triple.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Position-Attentive embedding:</head><p>GFA-NN models the neighborhood structure using rotations in 3D space and a penalty that forces the method to encode the difference of distances of entities to the reference nodes. The formulation for the structure-attentive part is:</p><formula xml:id="formula_7">F rot = v h -v r ⊗ v t 2<label>(2)</label></formula><p>where ⊗ represents a rotation using a rotation matrix of Euler angles with the formulation of direction cosine matrix (DCM):</p><p>cos θ cos ψ -cos φ sin ψ + sin φ sin θ cos ψ sin φ sin ψ + cos φ sin θ cos ψ cos θ sin ψ cos φ cos ψ + sin φ sin θ sin ψ -sin φ cos ψ + cos φ sin θ sin φ -sin θ sin φ cos θ cos φ cos θ</p><p>where φ, θ and ψ are Euler angles. The modeling of positional information is performed by a score function made from rotation matrices and a penalty:</p><formula xml:id="formula_9">F p = F rot -cos(S h i ) -cos(S t i )) 2<label>(4)</label></formula><p>where S i C is the calculated distance from the head and tail nodes to the reference nodes. Hence, the score enforces to learn structure-attentive embeddings with a penalty that is the normalized scalar difference of distance to reference nodes. Here we use the l 2 norm to regularize the F i score functions and apply negative adversarial sampling <ref type="bibr" target="#b19">[20]</ref>. We utilise Adam <ref type="bibr" target="#b8">[9]</ref> for optimization.</p><p>Reference-set selection relies on a Gaussian random number generator to select normally distributed random reference nodes from the network. GFA-NN keeps a fixed set of reference nodes during the training of different entities through different iterations to generate embeddings attentive to the position that are in the same space and, hence, comparable to each other.</p><p>Multiple Property aware scores can be naturally fused to achieve higher expressive power. This happens in f 1×1 .</p><p>Since canonical position-attentive embeddings do not exist, GFA-NN also computes structure-attentive embeddings h v via the common distance-based modelings of MDE. These scores are aggregated with attribute attentive scores, and then the model using a linear combination of these scores forms a 1×1 convolution to produce only one value that contains both properties. The output of this layer is then fed into the nonlinear activation function.</p><p>It is notable that independent weights in MDE formulation allow restricting solution space without limiting the learnability power of the model. Note also that the method is still Semi-supervised learning, where the train and test data are disjoint, and the centrality and path information computation do not consider the portion of the unknown network to the model and only exist in the test data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Theoretical analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Connection to Preceding KGE Methods</head><p>GFA-NN generalizes the existing Knowledge Graph embedding models. Taking the definition for the structure-aware and node properties attentive models into perspective, existing knowledge embedding models use the same information of connecting entities through different relations techniques, but use different neighborhood selection scoring function and sampling strategies, and they only output the structure-aware embeddings.</p><p>GFA-NN shares the score function aggregate training with MDE <ref type="bibr" target="#b16">[17]</ref>. There, a linear combination of scores f 1×1 = w i F i is trained, where w i weights are learnt together with the embeddings in the score functions F i . GFA-NN also shares the concept of training independent embeddings with MDE. The direction cosine matrix used in modeling positional information is convertible into a fourelement unit quaternion vector (q 0 , q 1 , q 2 , q 3 ). The quaternions are the center of the structure-based model QuatE <ref type="bibr" target="#b35">[36]</ref>, where the relations are models as rotations in the quaternion space. Here, besides modeling rotation, we formulated the score to include a translation as well. RotatE <ref type="bibr" target="#b19">[20]</ref> similarly, formulates the relations with a rotation and reduction in v h • v r -v t , however RotatE models rotation in the complex space. In the branch of Graph neural networks, the aggregate information of a node's neighborhood in one-hop <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b24">25]</ref> or nodes in the higher hops <ref type="bibr" target="#b31">[32]</ref> is used in message passing mechanism. P-GNN <ref type="bibr" target="#b34">[35]</ref> explicitly learns the shortest path of random nodes for simple graphs. However, it takes a new set of reference nodes in each iteration, which makes the learning of shortest paths local and incremental. In addition, it makes it difficult to retain the structural information from positional embedding. GFA-NN generalizes positional learning by learning the distances to a fixed set of random nodes through the whole network, which makes the positional embedding vectors globally comparable. From the point of view of graph type, GFA-NN generalizes the positional learning to multi-relational graphs to support KGs.</p><p>GFA-NN not only learns a weight for each of the network features, but it also associates it with the existing relation types between the two entities that their features are being learned. By including the relation type into position-attentive embeddings, the position also is encoded into relation vectors that connect the entities. Note that relation type learning is sub-optimal for learning centrality values because the dimension of relation types is much more higher than dimension of the the node property values (one integer value), which makes the centrality value differentiation diminish when learnt together with the association information belonging to relations. Another aspect that GFA-NN generalize the existing graph learning algorithms is that this method learns several centrality aspect and positional information at the same time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Expressive Power</head><p>In this Section we explain how GFA-NN generalizes the expressive power of Knowledge Graph embedding methods in the perspective of a broader Inductive bias. Generally, inductive bias in a learning algorithm allows it to better prioritize one solution over another, independent of the observed data <ref type="bibr" target="#b1">[2]</ref>.</p><p>Assuming that a labeling function y labels a triple (h, r, t) as d r y (h, t), we predict y r , similar to <ref type="bibr" target="#b34">[35]</ref> from the prospective of representation learning, which is by learning an embedding function f , where v h = f (v, G) and f computes the entity embeddings for v h , v r and v t . Thus, the objective becomes the task of maximizing the probability of the conditional distribution p(y|v h , v r , v t ). This probability can be designated by a distance function d v (v h , v r , v t ) in the embedding space, which usually is an l p norm of the objective function of the model.</p><p>A KGE model, with a goal to predict the existence of an unseen triple (h, r, t) learns embeddings weights v h and v t for the entities h and t and v r for a relation r that lies between them. In this formulation, the embedding for an entity e is computed based on its connection through its one-hop neighborhood, which we express that by structural information S e , and optimization over the objective function f θ (e, S e ). Hereby, the neighborhood information of two entities S e1 and S e2 is computed independently. However, the network feature attentive objective function f φ in GFA-NN poses a more general inductive bias that takes in the distance from a random shared set of reference-nodes, which are common across all entities, and the centrality values, which are relative to all nodes. In this setting, any pair of entity embeddings are correlated through the referenceset and the spectrum of relative centrality and therefore are not independent anymore. We call this feature attentive information I.</p><p>Accordingly, we define a joint distribution p(w e1 , w e2 ) over node embeddings, where w ei = f φ (e i , I). We formalize the problem of KG representation learning by minimizing the expected value of the likelihood of the objective function in margin-based ranking setting, in the following for a structure base KGE:</p><formula xml:id="formula_10">min θ E e1,e2,e3,Se 1 ,Se 2 ,Se 3 L(d + v (f θ (e 1 , S e1 ), f θ (e 2 , S e2 )) -d - v (f θ (e 1 , S e1 ), f θ (e 3 , S e3 )) -m)<label>(5)</label></formula><p>and in GFA-NN:</p><formula xml:id="formula_11">min θ E e1,e2,e3,I L(d + v (f φ (e 1 , I), f φ (e 2 , I)) -d - v (f φ (e 1 , I), f φ (e 3 , I)) -m)<label>(6)</label></formula><p>where d + v is the similarity metric determined by the objective function for a positive triple, indicating existing a predicate between entities and by optimizing converges to the target label function d y (e 1 , e 2 ) = 0 for positive samples(existing triples) and d y (e 1 , e 3 ) = m on negative samples. Here, m is the margin value in the margin ranking loss optimization setting. Note that the representations of entities are calculated using joint and marginal distributions, respectively.</p><p>Similar to the proof of expressive power in <ref type="bibr" target="#b34">[35]</ref>, considering the selection of entities e 1 , ..., e i ∈ G as random variables to form any triples, the mutual information between the joint distribution of entity embeddings and any Y = d y (e 1 , e 2 ) is greater than that between the marginal distributions. Y : I(Y ; X joint ) ≥ I(Y ; X marginal ). Where,</p><formula xml:id="formula_12">X joint = (f φ (e 1 , S e1 ), f φ (e 2 , S e2 )) ∼ p(f φ (e 1 , S e1 ), f φ (e 2 , S e2 )) X marginal = (f θ (e 1 , I), f θ (e 2 , I))</formula><p>Because the gap of mutual information is large when the targeted task is related to positional and centrality information of the network, we deduce that KGE embedding based on the joint distribution of distances to reference nodes and relative centrality values have more expressive power than the current structure-based KGE models. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Complexity Analysis</head><p>Next, we explain the complexity of the method and show its complexity compared to the structure-based models. When the shortest paths are calculated on the fly, the learning complexity is added up by O(b log(b)) for finding the shortest paths on b entities in each batch, and similarly, the centrality computation aggregates to the complexity. We, therefore, pre-calculate this information to separate them from the learning complexity. The complexity of each of the objective functions on a batch with size b is O(b), and suppose n property attentive features and m structure-aware scores be involved, the overall complexity becomes O((n + m) b). Note that the larger number here is b and the complexity increases by b times when a graphical feature is involved in the learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experiments</head><p>We evaluate the performance of our model with two link prediction experiments; First, the traditional transductive ranking evaluation, which is originally introduced in <ref type="bibr" target="#b2">[3]</ref>, and second, inductive relation prediction experiment. In the inductive setting, the experiment evaluates a models ability to generalize the link prediction task to unseen entities. Table <ref type="table" target="#tab_0">1</ref> shows the statistics of the datasets used in the experiments. Metrics and Implementation: We evaluate the link prediction performance by ranking the score of each test triple against all possible derivable negative samples by once replacing its head with all entities and once by replacing its tail. We then calculate the hit at N (Hit@N), mean rank (MR), and mean reciprocal rank (MRR) of these rankings. We report the evaluations in the filtered setting. We determine the hyper-parameters by using grid search. We select the testing models which give the best results on the validation set. In general, we fix the learning rate on 0.0005 and search the embedding size amongst {200, 300, 400, 500}. We search the batch size from {250, 300, 500, 800, 1000}, and the number of negative samples amongst {10, 100, 200, 400, 600, 800, 1000}. We perform experiments on three benchmark datasets: WN18RR <ref type="bibr" target="#b4">[5]</ref>, FB15k-237 <ref type="bibr" target="#b21">[22]</ref>, and ogbl-biokg <ref type="bibr" target="#b6">[7]</ref>, which is comparably a sizeable Knowledge Graph assembled from a large number of biomedical repositories.</p><p>Baselines: We compare our model with several state-of-the-art structurebased embedding approaches. Our baselines include RotatE <ref type="bibr" target="#b19">[20]</ref>, TuckER <ref type="bibr" target="#b0">[1]</ref>, ComplEx-N3 <ref type="bibr" target="#b10">[11]</ref>, QuatE <ref type="bibr" target="#b35">[36]</ref>, MDE <ref type="bibr" target="#b16">[17]</ref> and the recent graph neural network CompGCN <ref type="bibr" target="#b24">[25]</ref>. We report results of each method on WN18RR and FB15k-237 from their respective papers, while the results of the other models in ogbl-biokg are from <ref type="bibr" target="#b6">[7]</ref>. For RotatE, we report its best results with self-adversarial negative sampling, and for QuatE, we report the results with N3 regularization. For our model, we use the same self-adversarial negative sampling introduced in RotatE. This negative sampling schema is also applied to all the other models in the ogbl-biokg benchmark.</p><p>Results and Discussion: Table <ref type="table" target="#tab_1">2</ref> and Table <ref type="table" target="#tab_2">3</ref> summarize the performance of GFA-NN and other KGE models in the transductive link prediction task. We observe that GFA-NN outperforms other state-of-the-art KGEs on WN18RR and is producing competitive results on FB15k-237.</p><p>Our analysis shows that the standard deviation of different positional and centrality measures through the network in WN18RR is ≈0.009, while in FB15k-237, it is ≈0.002, which is 4.5 times smaller. This comparison indicates that in WN18RR, these features are more diversified, but in FB15k237, they are close to each other. This analysis suggests the crucial impact of learning centrality and positional-attentive embeddings on the superiority of the GFA-NN on the WN18RR benchmark. While the result on the FB15k-237 is still very compet- itive to the state-of-the-art, as a lesson learned, we can declare it as a fixed procedure to perform the standard deviation analysis on a dataset before determining how much the network property attentive embedding learning method would be beneficial.</p><p>Table <ref type="table" target="#tab_2">3</ref> shows the MRR evaluation results on the comparably large biological dataset named as ogbl-biokg. In this benchmark, the number of entity and training samples is much larger than the WN18rr and FB15k-237 datasets. The capability of learning feature attentive embeddings is crucial in this transductive link prediction task. While the best KGEs can only achieve the M RR of 0.8105 on the validation and 0.8095 on the test dataset, GFA-NN reaches 0.901 on both datasets, improving state-of-the-art by 9 percent. This wide gap between the results supports the assumption that property-attentive embeddings surpass prior methods in larger-scale real-world networks. This improvement in such a small-world structured network is because of its significant entity-to-relation ratio, which causes a large standard deviation of positional and centrality qualities. As indicated earlier, this feature is beneficial to the efficiency of the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Inductive link prediction experiment</head><p>Datasets: For evaluations in the inductive setting, we select four variant datasets which Komal et al. <ref type="bibr" target="#b20">[21]</ref> extracted from WN18RR and NELL-995 <ref type="bibr" target="#b30">[31]</ref>.</p><p>Baselines: Inductive baselines include GraIL <ref type="bibr" target="#b20">[21]</ref>, which uses sub-graph reasoning for inductive link prediction. RuleN <ref type="bibr" target="#b14">[15]</ref> that applies a statistical rule mining method, and two differentiable methods of rule learning NeuralLP <ref type="bibr" target="#b33">[34]</ref> and DRUM <ref type="bibr" target="#b17">[18]</ref>. We report the results of these state-of-the-art models from Komal et al. <ref type="bibr" target="#b20">[21]</ref>.</p><p>Results: Table <ref type="table" target="#tab_3">4</ref> summarizes the GFA-NN's Hit@10 ranking performance against methods specified on the inductive link prediction task. Although we did not explicitly design GFA-NN for this task, we observe GFA-NN performs very competitively in this setting and outperforms the best inductive learning models in most cases. This result supports our hypothesis that the Knowledge Graph embeddings attentive to positional and centrality qualities are beneficial for prediction tasks in challenging settings, i.e., inductive link prediction task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>In this article, with a new view to the relational learning algorithms, we propose to learn the structural information of the network conjointly with the learning of the centrality and positional properties of the Knowledge Graph entities in one model. We provide theoretical analyses and empirical evaluations to identify the improvements and constraints in the expressive power for this class of KGEs. In particular, we demonstrate that with proper formulation, the learning of these global features is beneficial to the link prediction task, given that GFA-NN performs highly efficiently in a variety of benchmarks and often outperforms current state-of-the-art solutions in both inductive and transductive settings. Since GFA-NN is efficient on networks with a higher entity-to-relation ratio, applications of the approach can be considered on biological, chemical, and social networks in future works.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Example Knowledge Graph in which nodes e1 and e2 are difficult to distinguish by a KGE model only using their neighborhood information.</figDesc><graphic coords="3,215.55,116.83,184.23,120.82" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig.2. Architecture of GFA-NN. GFA-NN first pre-computes the centrality property of nodes and their distance to a set of to randomly selected reference nodes (Left). Then, node centrality and position embeddings attentive to position zv m are computed via scores F1, ..., F k from the distance between a given node vi and the reference-sets Si which are shared across all the entities (Top-middle). To compute the embedding zv 1 for node v1, a score of GFA-NN first computes via function Fi and then aggregates the Fi scores via 1×1 convolution and an activation function over obtains a vector of final scores. Inside 1 × 1 a vector w learned, which is used to reduce scores into one centrality and position-aware score and produces embeddings zv 1 which is the output of the GFA-NN (Right).</figDesc><graphic coords="7,165.95,116.83,283.45,171.85" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Statistics of the data sets used in the Experiments.</figDesc><table><row><cell>Dataset</cell><cell cols="5">#entities #relations #train #validation #test</cell></row><row><cell>WN18RR</cell><cell>40943</cell><cell>11</cell><cell>86835</cell><cell>3034</cell><cell>3134</cell></row><row><cell>FB15k-237</cell><cell>14541</cell><cell>237</cell><cell>272115</cell><cell>17535</cell><cell>20466</cell></row><row><cell>ogbl-biokg</cell><cell>45085</cell><cell>51</cell><cell cols="3">4762678 162886 162870</cell></row><row><cell>WN18RR-v3-ind</cell><cell>5084</cell><cell>11</cell><cell>6327</cell><cell>538</cell><cell>605</cell></row><row><cell>WN18RR-v4-ind</cell><cell>7084</cell><cell>9</cell><cell>12334</cell><cell>1394</cell><cell>1429</cell></row><row><cell>NELL-995-v1-ind</cell><cell>225</cell><cell>14</cell><cell>833</cell><cell>101</cell><cell>100</cell></row><row><cell cols="2">NELL-995-v4-ind 2795</cell><cell>61</cell><cell>7073</cell><cell>716</cell><cell>731</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Results on WN18RR and FB15k-237. Best results are in bold.</figDesc><table><row><cell></cell><cell></cell><cell>WN18RR</cell><cell></cell><cell></cell><cell>FB15k-237</cell></row><row><cell>Model</cell><cell>MR</cell><cell>MRR</cell><cell cols="2">Hit@10 MR</cell><cell>MRR</cell><cell>Hit@10</cell></row><row><cell cols="2">ComplEx-N3 -</cell><cell>0.48</cell><cell>0.57</cell><cell>-</cell><cell>0.37</cell><cell>0.56</cell></row><row><cell>QuatE 2</cell><cell>-</cell><cell>0.482</cell><cell cols="2">0.572 -</cell><cell>0.366</cell><cell>0.556</cell></row><row><cell>TuckER</cell><cell>-</cell><cell>0.470</cell><cell cols="2">0.526 -</cell><cell>0.358</cell><cell>0.544</cell></row><row><cell cols="2">CompGCN 3533</cell><cell>0.479</cell><cell cols="2">0.546 197</cell><cell>0.355</cell><cell>0.535</cell></row><row><cell>RotatE</cell><cell>3340</cell><cell>0.476</cell><cell cols="2">0.571 177</cell><cell>0.338</cell><cell>0.533</cell></row><row><cell>MDE</cell><cell>3219</cell><cell>0.458</cell><cell cols="2">0.536 203</cell><cell>0.344</cell><cell>0.531</cell></row><row><cell cols="2">GFA-NN 3390</cell><cell>0.486</cell><cell cols="2">0.575 186</cell><cell>0.338</cell><cell>0.522</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>MRR Results for ogbl-biokg. (Results of previous models are from<ref type="bibr" target="#b6">[7]</ref>.)</figDesc><table><row><cell cols="3">Method Validation Test</cell></row><row><cell>TransE</cell><cell>0.7456</cell><cell>0.7452</cell></row><row><cell>DistMult</cell><cell>0.8055</cell><cell>0.8043</cell></row><row><cell>ComplEx</cell><cell>0.8105</cell><cell>0.8095</cell></row><row><cell>RotatE</cell><cell>0.7997</cell><cell>0.7989</cell></row><row><cell>GFA-NN</cell><cell cols="2">0.9011 0.9011</cell></row><row><cell cols="3">6.1 Transductive link prediction experiment</cell></row><row><cell>Datasets:</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>Hit@10 results for inductive datasets. (Other models' results are from<ref type="bibr" target="#b20">[21]</ref>.)</figDesc><table><row><cell cols="5">Model WN18RR-v3-ind WN18RR-v4-ind NELL-995-v1-ind NELL-995-v4-ind</cell></row><row><cell>NeuralLP</cell><cell>0.4618</cell><cell>0.6713</cell><cell>0.4078</cell><cell>0.8058</cell></row><row><cell>DRUM</cell><cell>0.4618</cell><cell>0.6713</cell><cell>0.5950</cell><cell>0.8058</cell></row><row><cell>RuleN</cell><cell>0.5339</cell><cell>0.7159</cell><cell>0.5950</cell><cell>0.6135</cell></row><row><cell>GraiL</cell><cell>0.5843</cell><cell>0.7341</cell><cell>0.5950</cell><cell>0.7319</cell></row><row><cell>GFA-NN</cell><cell>0.5893</cell><cell>0.7355</cell><cell>0.9500</cell><cell>0.7722</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>E.g. (Berlin, CapitalOf, Germany) is a fact stating Berlin is the capital of Germany.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>Source code is available at: https://github.com/afshinsadeghi/GFA-NN</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgments: First author thanks <rs type="person">Firas Kassawat</rs> for related discussions. This study was supported by <rs type="funder">MLwin</rs> project grant <rs type="grantNumber">01IS18050F</rs> of the <rs type="funder">Federal Ministry of Education and Research of Germany</rs>, the <rs type="funder">EU</rs> <rs type="programName">H2020 Projects</rs> <rs type="projectName">Opertus Mundi</rs> (<rs type="grantNumber">GA 870228</rs>), and the <rs type="funder">Federal Ministry for Economic Affairs and Energy (BMWi)</rs> project <rs type="projectName">SPEAKER</rs> (<rs type="grantNumber">FKZ 01MK20011A</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_nWhwA6r">
					<idno type="grant-number">01IS18050F</idno>
				</org>
				<org type="funded-project" xml:id="_TvNnpDH">
					<idno type="grant-number">GA 870228</idno>
					<orgName type="project" subtype="full">Opertus Mundi</orgName>
					<orgName type="program" subtype="full">H2020 Projects</orgName>
				</org>
				<org type="funded-project" xml:id="_CqDUfgr">
					<idno type="grant-number">FKZ 01MK20011A</idno>
					<orgName type="project" subtype="full">SPEAKER</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Tucker: Tensor factorization for knowledge graph</title>
		<author>
			<persName><forename type="first">I</forename><surname>Balazevic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Allen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP-IJCNLP</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="5185" to="5194" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Relational inductive biases, deep learning, and graph networks</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">W</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Hamrick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Bapst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sanchez-Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Zambaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tacchetti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Raposo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Faulkner</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.01261</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">Preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Translating embeddings for modeling multi-relational data</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Garcia-Duran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Yakhnenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="2787" to="2795" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Transgcn: Coupling transformation assumptions with graph convolutional networks for link prediction</title>
		<author>
			<persName><forename type="first">L</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Mai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Janowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zhu</surname></persName>
		</author>
		<editor>K-CAP</editor>
		<imprint>
			<date type="published" when="2019">2019</date>
			<publisher>ACM</publisher>
			<biblScope unit="page" from="131" to="138" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Convolutional 2D knowledge graph embeddings</title>
		<author>
			<persName><forename type="first">T</forename><surname>Dettmers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Minervini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Stenetorp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Riedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1811" to="1818" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Knowledge graph embedding with iterative guidance from soft rules</title>
		<author>
			<persName><forename type="first">S</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4816" to="4823" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Open graph benchmark: Datasets for machine learning on graphs</title>
		<author>
			<persName><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Catasta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<editor>NeurIPS.</editor>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A new status index derived from sociometric analysis</title>
		<author>
			<persName><forename type="first">L</forename><surname>Katz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychometrika</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="39" to="43" />
			<date type="published" when="1953">1953</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<editor>Bengio, Y., LeCun, Y.</editor>
		<imprint>
			<date type="published" when="2015">2015</date>
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Canonical tensor decomposition for knowledge base completion</title>
		<author>
			<persName><forename type="first">T</forename><surname>Lacroix</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Obozinski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2863" to="2872" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Network in network</title>
		<author>
			<persName><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.4400</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">Preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning entity and relation embeddings for knowledge graph completion</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2181" to="2187" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Fuzzy multilevel graph embedding</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Luqman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Y</forename><surname>Ramel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Llads</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brouard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="551" to="565" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Fine-grained evaluation of rule-and embedding-based systems for knowledge graph completion</title>
		<author>
			<persName><forename type="first">C</forename><surname>Meilicke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fink</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ruffinelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Gemulla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Stuckenschmidt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISWC</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="3" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Rdf2vec: RDF graph embeddings for data mining</title>
		<author>
			<persName><forename type="first">P</forename><surname>Ristoski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Paulheim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISWC</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="498" to="514" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">MDE: Multiple distance embeddings for link prediction in knowledge graphs</title>
		<author>
			<persName><forename type="first">A</forename><surname>Sadeghi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Graux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Shariat Yazdi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lehmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECAI</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">DRUM: end-to-end differentiable rule mining on knowledge graphs</title>
		<author>
			<persName><forename type="first">A</forename><surname>Sadeghian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Armandpour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="page" from="15321" to="15331" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Modeling relational data with graph convolutional networks</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Schlichtkrull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bloem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Van Den Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ESWC</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">RotatE: Knowledge graph embedding by relational rotation in complex space</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">H</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Y</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Inductive relation prediction by subgraph reasoning</title>
		<author>
			<persName><forename type="first">K</forename><surname>Teru</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Denis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hamilton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="9448" to="9457" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Observed versus latent features for knowledge base and text inference</title>
		<author>
			<persName><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVSC</title>
		<imprint>
			<biblScope unit="page" from="57" to="66" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Complex embeddings for simple link prediction</title>
		<author>
			<persName><forename type="first">T</forename><surname>Trouillon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Welbl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">É</forename><surname>Gaussier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Bouchard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2071" to="2080" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Composition-based multirelational graph convolutional networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Vashishth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sanyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Nitin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Talukdar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Composition-based multirelational graph convolutional networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Vashishth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sanyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Nitin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">P</forename><surname>Talukdar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Unsupervised embedding enhancements of knowledge graphs using textual associations</title>
		<author>
			<persName><forename type="first">N</forename><surname>Veira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Keng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Padmanabhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Veneris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Graph attention networks</title>
		<author>
			<persName><forename type="first">P</forename><surname>Velickovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Liò</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Learning first-order logic embeddings via matrix factorization</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2132" to="2138" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Knowledge graph embedding by translating on hyperplanes</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Representation learning of knowledge graphs with entity descriptions</title>
		<author>
			<persName><forename type="first">R</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2659" to="2665" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">DeepPath: A reinforcement learning method for knowledge graph reasoning</title>
		<author>
			<persName><forename type="first">W</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="564" to="573" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Representation learning on graphs with jumping knowledge networks</title>
		<author>
			<persName><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Sonobe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kawarabayashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
		<editor>Dy, J.G., Krause, A.</editor>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>ICML</publisher>
			<biblScope unit="page" from="5449" to="5458" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Embedding entities and relations for learning and inference in knowledge bases</title>
		<author>
			<persName><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Differentiable learning of logical rules for knowledge base reasoning</title>
		<author>
			<persName><forename type="first">F</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="page" from="2319" to="2328" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Position-aware graph neural networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="7134" to="7143" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Quaternion knowledge graph embeddings</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="page" from="2731" to="2741" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
