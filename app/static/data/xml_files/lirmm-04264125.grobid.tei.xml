<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Subset Modelling: A Domain Partitioning Strategy for Data-efficient Machine-Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Vitor</forename><surname>Ribeiro</surname></persName>
							<email>victorr@posgrad.lncc.br</email>
							<affiliation key="aff0">
								<orgName type="laboratory">National Laboratory for Scientific Computing (LNCC)</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Eduardo</forename><forename type="middle">H M</forename><surname>Pena</surname></persName>
							<email>eduardopena@utfpr.edu.brraphael.de-freitas-saldanha</email>
							<affiliation key="aff3">
								<orgName type="institution">Federal University of Technology (UTFPR)</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Raphael</forename><surname>Saldanha</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Institut national de recherche en sciences et technologies du numérique</orgName>
								<orgName type="institution">(INRIA</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Reza</forename><surname>Akbarinia</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Institut national de recherche en sciences et technologies du numérique</orgName>
								<orgName type="institution">(INRIA</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Patrick</forename><surname>Valduriez</surname></persName>
							<email>patrick.valduriez@inria.fr</email>
							<affiliation key="aff2">
								<orgName type="department">Institut national de recherche en sciences et technologies du numérique</orgName>
								<orgName type="institution">(INRIA</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Falaah</forename><forename type="middle">Arif</forename><surname>Khan</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Center for Data Science</orgName>
								<orgName type="institution">New York University (NYU</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Julia</forename><surname>Stoyanovich</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Center for Data Science</orgName>
								<orgName type="institution">New York University (NYU</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Fabio</forename><surname>Porto</surname></persName>
							<email>fporto@lncc.br</email>
							<affiliation key="aff0">
								<orgName type="laboratory">National Laboratory for Scientific Computing (LNCC)</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Subset Modelling: A Domain Partitioning Strategy for Data-efficient Machine-Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">7AC81C090F5CB2CA2925F11574EA69BB</idno>
					<idno type="DOI">10.5753/sbbd.2023.232829</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-04-12T14:47+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The success of machine learning (ML) systems depends on data availability, volume, quality, and efficient computing resources. A challenge in this context is to reduce computational costs while maintaining adequate accuracy of the models. This paper presents a framework to address this challenge. The idea is to identify "subdomains" within the input space, train local models that produce better predictions for samples from that specific subdomain, instead of training a single global model on the full dataset. We experimentally evaluate our approach on two real-world datasets. Our results indicate that subset modelling (i) improves the predictive performance compared to a single global model and (ii) allows data-efficient training.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The remarkable success of machine learning systems (MLS) in various domains has been largely attributed to the availability of vast amounts of data and high-performance computing infrastructures <ref type="bibr" target="#b8">[Zhang et al. 2022]</ref>. Such resources come at a significant cost, making them accessible mainly to larger organizations with substantial financial capabilities. A second challenge is that MLS do not perform uniformly well on all parts of the input space, despite showing good overall performance. This issue is pervasive across all MLS: disparities in performance across demographic subgroups have been the focus of fair machine learning <ref type="bibr" target="#b1">[Chouldechova and Roth 2020]</ref>.</p><p>This paper attempts to tackle both challenges simultaneously by considering "subdomains" within the input space. As a motivating example, consider the task of predicting the incidence of dengue in Brazil. In principle, we could build a single model that forecasts dengue prevalence in all Brazilian cities However, different cities experience variations in transmission patterns that may be hard to capture by a single model <ref type="bibr" target="#b0">[Cabrera and et al 2022]</ref>. On the other hand, building a separate (local) model for each municipality is also challenging. First, there may be relatively few dengue cases in some areas, resulting in limited data for model training, and, thus, in models that will not generalize well when deployed. Second, training and maintaining separate models for each municipality demands significant data management effort and computing resources. The approach of this paper interpolates between these two extremes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Summary of contributions.</head><p>We propose a framework that accounts for shared characteristics and regional variations across different cities, notably at low training costs and good prediction capability. The idea is simple: first identify subsets within the dataset, and then train subset models. At inference time, assign the incoming sample to one of the subset, and use the corresponding model to make a prediction. It is important to mention that the clustering process may help us to achieve algorithmic fairness. We present our framework in Section 2, and show its effectiveness empirically in Section 3 using two tasks: dengue forecasting in Brazil and predicting unemployment in the US based on Census data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Partition Modelling Framework</head><p>Preliminaries. Let a given dataset be denoted as D(X, Y ), where X represents the attribute set and Y is the target variable for the prediction task. We assume the existence of a function f such that f (X) : X → Y . A learner in a machine learning context aims to construct a model f (X) : X → Ŷ that can approximate f such that f (X) ≈ f (X). The quality of the prediction is determined by an error metric µ such that |f (x i ) -f (x i )| ≤ ϵ, where ϵ is a real value computed according to µ. The empirical error of a model is computed over a set of samples in D according to the Empirical Risk Minimization formulae [Shalev-Shwartz and Ben-David 2014], whereas E s ( f ) represents an average measure over a set of observed input samples, meaning that the approximation f (X) can perform better for some examples than others.</p><formula xml:id="formula_0">E s ( f ) = |i ∈ [m] : f (x i ) -f (x i )| m , [m] = 1, 2, ..., m, ∀x 1 ∈ X.<label>(1)</label></formula><p>Our framework. We consider a dataset D whose domain can be one of two classes: time-series and independent samples. The first class D r is interpreted as a set of spatially localized time-series (ts), ordered by time, and having some measurement values at each time instant. The second dataset D c has a set of features X and a class label Y . The dataset D is composed of a set of samples D = {s 1 , s 2 , ..., s n }. The problem we want to solve is to select partitions of D,</p><formula xml:id="formula_1">P i = {S 1 , S 2 , .., S k }, with S j = {s j1 , s j2 , .., s jn }, S j ∩ S t = ∅, if j ̸ = t, D = ∪ j=1.</formula><p>.k S j , ∀P i , such that we train k machine learning models, using a given learner algorithm. Each machine-learning model m j in M , 1 ≤ j ≤ k, is built on the samples of the corresponding subset S j ∈ P i . We define P i as a partition of dataset D. We want to find a partition P i of the dataset D, such that when used for training produces models in M with higher accuracy than a global model trained on samples of the complete dataset D</p><formula xml:id="formula_2">P i := argmin P i ∈P j=1..k E s ((ts l ∈ S ij ) : m ij (ts l ))<label>(2)</label></formula><p>Solving equation 2 is not practical for large datasets due to the following combined reasons:(i) for D r the number of windows computed on the pre-processing step for preparing the data for model training can considerably multiply the original dataset size, (ii) building and evaluating models for all subsets in all partitions (i.e. the power set of D r ) has complexity with lower bound exponential in the sizes of D r .</p><p>We consider that the partition of the dataset D we are looking for should strike a balance between similarity among the samples of a subset, and some variation that would enable the learner to generalize beyond observed trained data. In this paper, we argue that by adopting non-supervised clustering algorithms, such as k-means or k-shape the partition therein obtained offers the desired properties, approximating a solution for the Equation 2 And thus it becomes: P i = clustering algorithm(D, k) (3). Once the set of M models has been built, they are ready to be used for inference. The latter happens in the following way. Given an input sample s x to which an inference is to be computed, we want to use a model m i ∈ M that is the best fit for s x . The chosen model m i is the one built on the samples of the partition S i , whose representative is closest in a metric distance to s x .</p><p>Related work. <ref type="bibr" target="#b3">[Khan and Stoyanovich 2023]</ref> explore the existence of subdomains in the data through the lens of algorithmic fairness. Here, subdomains correspond to minority groups.The authors use this framing to motivate the use of group-specific models for improved performance on socially disadvantaged groups. In comparison, we compute subsets that offer no specific semantic guarantee; rather, they are identified based on data distributions similarity. The Coreset approach aims to select a subset of a dataset that produces models of comparable accuracy to a model built on the full dataset <ref type="bibr" target="#b4">[Mirzasoleiman et al. 2020</ref>]. This work differs from ours because we aim at improving the model accuracy by training subset models on subsets of the original dataset.Our framework addresses the data subset selection problem. This problem is known to be NP-hard since both the training set and the model parameters work as optimization variables <ref type="bibr" target="#b7">[Wei et al. 2015</ref>]. Still, our empirical results in Section 3 show that subset modeling works well in practice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Experimental Evaluation</head><p>The Dengue dataset: The first task evaluates the subset modelling procedure on a time series regression task to forecast the number of weekly dengue virus (DENV) cases in each city in Brazil. DENV-suspected cases are tracked nationwide using a health information system. For this research, we computed the number of dengue cases each week by city from 2011 to 2020. We only considered confirmed cases. Further, due to the data sparsity, we only considered cities with more than 100,000 inhabitants. The dataset has 400 measurements of dengue cases in 312 cities in total.</p><p>The ACSEmployment dataset: The second task we consider is to predict a person's employment status, based on social and demographic information collected by the US Census. Folktables <ref type="bibr" target="#b2">[Ding and et al 2021]</ref> is a benchmark dataset in fair machine learning, constructed from census data from 50 US states for the years 2014-2018. We report results on the ACSEmployment task: a binary classification task of predicting whether an individual is employed. We report our results on data from Georgia from 2018. The dataset has 16 features, including age, schooling, and disability status, and contains about 200k samples, which we sub-sample down to 10k samples for computational feasibility.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Experimental set-up</head><p>Dengue: As a training feature, we considered the time series on the number of dengue cases. We computed a sliding window of length (w) and stride 1 on the complete training sequence, producing n -(w + 1) subsequences as input for the training process.</p><p>For each window of size w, the prediction infers the next measurement value. We first pre-processed the data to normalize it, and then split it into training and test, using the first (temporally) 320 measurements for training (80%) and testing on the remaining 80 (20%). We compared two training approaches: a single global model trained on all 312 municipalities vs. a subset-based approach. For the subset approach, we first partition the training dataset using k-means clustering for k ∈ [4, 7], with dynamic time warping (DTW) as the distance metric. We then trained three learners: RandomForestRegressor (rf), LGBMRegressor (lgbm) and LinearSVR (lsvr), with hyperparameter tuning using Optuna. This procedure were repeated 100 times for each learner algorithm. During testing, for each municipality, we select for prediction the subset model trained on the subset containing the time-series for that municipality. We also use the same time-series as input for inference with the global model. Since the Dengue task is a time-series regression, we use the RMSE as the evaluation metric, and report results in Figures <ref type="figure" target="#fig_1">1a</ref> and<ref type="figure" target="#fig_3">2a</ref>.</p><p>ACSEmployment: We used all 16 features as input for the binary classification task. We one-hot-encoded the categorical features, and performed standard scaling on the numerical ones. Next, we split the dataset into training and test sets (80:20). We ran k-means clustering using all the features, and partitioned the dataset into partitions based on cluster assignment. For each partition, we trained three learners: Random Forest Classifier (rf), Gradient-boosting machine (lgbm) and Logistic Regression (lr). We performed 3-fold cross validation using RandomizedSearchCV from scikit-learn to tune the hyperparameters of each subset model. We applied this methodology for k ∈ [2, 8]. To capture variance in the results, for each choice of k we repeated the experiment with 3 different random train-test splits. In each setting, we also trained a global model on the full dataset.</p><p>During testing, we first predict the cluster assignment for each test sample, and then use the corresponding subset model (trained on samples only from the same cluster) to do inference, making predictions for each point using the global model. We report accuracy as the evaluation metric for this binary classification task, in Figures <ref type="figure" target="#fig_1">1b</ref> and<ref type="figure" target="#fig_3">2b</ref>. To evaluate training efficiency, we report training dataset sizes for each subset model in Figure <ref type="figure" target="#fig_5">3</ref>. For both experimental cases we decided to fix the learning algorithmns and their  hyper-parameters, trying to establish a simpler correlation between cluster composition and prediction quality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Results</head><p>Figure <ref type="figure" target="#fig_1">1</ref> provides evidence that partitioning the data to guide the construction of subset models can improve (lower) RMSE. For any choice of k &gt; 2, we see an improvement in performance on Dengue, in terms of lower root mean square error (RMSE). For ACSEmployment, we see some improvement over the global model, for some values of k (e.g., k = 2), in terms of higher accuracy. For both tasks, as the number of partitions becomes larger, the variance of the accuracy metric (shown by the width of the box and whiskers plot) increases. This is expected, since the number of training samples per cluster will decrease, leading to higher variance. This results may indicate the need for considering some constraint regarding the classification task.</p><p>In Figure <ref type="figure" target="#fig_3">2</ref>, we present performance for a fixed k, with k = 7 for Dengue and k = 5 for ACSEmployment. We see that all of subset models outperform the global model on Dengue (lower RMSE). On ACSEmployment, some subset models (such as the rf and lr on partition 2) outperform the global models, indicating improved performance on some subset of the input space. Figure <ref type="figure" target="#fig_5">3</ref> ilustrates the training time and dataset size for different subset models, as a function of k, to evaluate the efficiency gains from this methodology. Figure <ref type="figure" target="#fig_5">3a</ref>,Presentst the elapsed time as the maximum time to train a subset model for Dengue. Note that we only considered time dedicated with training time, disregarding clustering time.</p><p>Observe that, for all values of k, training time decreases as the number of partitions increases, reinforcing the possibility that subset models can expedite the training process. Further, note that considering each partition as a disjoint dataset brings the training problem a natural parallelism. Figure <ref type="figure" target="#fig_5">3b</ref> reports the fraction of samples assigned to each cluster for ACSEmployment. As expected, larger number of partitions decreases the fraction of samples per subset model. This causes a larger variance in performance of subset models for k &gt; 7 on both tasks, indicating the existence of a trade-off between performance and efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Conclusion</head><p>The machine learning literature provides several pieces of evidence indicating that data selection can have an impact on training time while maintaining prediction quality. Typ-  ically, this techniques support the construction of a single model. In this paper, we propose a novel training approach that extends the data selection problem, providing support for constructing multiple models. Our experimental findings suggest that a subset approach can improve predictive performance, as well as training efficiency, bounded by an accuracy-efficiency trade-off.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Performance of subset models for different learners; k is the number of partitions, k = 1 corresponds to the global model.</figDesc><graphic coords="5,85.04,102.04,212.59,106.30" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>for Dengue, k = 7, lower is better. (b) Accuracy for ACSEmployment, k = 5, higher is better.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Performance of subset models vs. the global model, for a fixed k.</figDesc><graphic coords="6,85.04,105.60,212.59,106.30" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Computational efficiency, for different k (number of clusters/domains), measured by training time for Dengue (a) and dataset fraction for ACSEmployment (b).</figDesc><graphic coords="7,85.04,99.21,212.60,118.11" type="bitmap" /></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Dengue prediction in latin america using machine learning and the one health perspective: A literature review</title>
		<author>
			<persName><forename type="first">M</forename><surname>Cabrera</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Tropical Medicine and Infectious Disease</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page">322</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A snapshot of the frontiers of fairness in machine learning</title>
		<author>
			<persName><forename type="first">A</forename><surname>Chouldechova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="82" to="89" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Retiring adult: New datasets for fair machine learning</title>
		<author>
			<persName><forename type="first">F</forename><surname>Ding</surname></persName>
		</author>
		<editor>Ranzato, M., Beygelzimer, A., Dauphin, Y. N., Liang, P., and Vaughan, J. W.</editor>
		<imprint>
			<date type="published" when="2021-12-06">2021. 2021. December 6-14, 2021</date>
			<biblScope unit="page" from="6478" to="6490" />
			<pubPlace>NeurIPS; virtual</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">A</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Stoyanovich</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2302.08704</idno>
		<title level="m">The unbearable weight of massive privilege: Revisiting bias-variance trade-offs in the context of fair prediction</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Coresets for data-efficient training of machine learning models</title>
		<author>
			<persName><forename type="first">B</forename><surname>Mirzasoleiman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Bilmes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML 2020</title>
		<imprint>
			<date type="published" when="2020-07">2020. July 2020</date>
			<biblScope unit="volume">119</biblScope>
			<biblScope unit="page" from="6950" to="6960" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title/>
		<author>
			<persName><surname>Pmlr</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Understanding machine learning: From theory to algorithms</title>
		<author>
			<persName><forename type="first">S</forename><surname>Shalev-Shwartz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ben-David</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Submodularity in data subset selection and active learning</title>
		<author>
			<persName><forename type="first">K</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Iyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bilmes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on International Conference on Machine Learning</title>
		<meeting>the 32nd International Conference on International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="1954" to="1963" />
		</imprint>
	</monogr>
	<note>ICML&apos;15</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">The ai index 2022 annual report</title>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Maslej</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Brynjolfsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Etchemendy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lyons</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Manyika</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ngo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Niebles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sellitto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Sakhaee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shoham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Perrault</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
