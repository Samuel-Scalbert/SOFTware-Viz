<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Future Generation Computer Systems</title>
				<funder>
					<orgName type="full">CAPES</orgName>
				</funder>
				<funder>
					<orgName type="full">FAPERJ</orgName>
				</funder>
				<funder ref="#_UR2RCy7">
					<orgName type="full">EU</orgName>
				</funder>
				<funder>
					<orgName type="full">CNPq</orgName>
				</funder>
				<funder ref="#_pEBTvhr">
					<orgName type="full">Inria</orgName>
				</funder>
				<funder ref="#_KJ9jNpC">
					<orgName type="full">MCTI/RNP-Brazil</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Renan</forename><surname>Souza</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">COPPE/Federal University of Rio de Janeiro</orgName>
								<address>
									<country key="BR">Brazil</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">IBM Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">VÃ­tor</forename><surname>Silva</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">COPPE/Federal University of Rio de Janeiro</orgName>
								<address>
									<country key="BR">Brazil</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jose</forename><forename type="middle">J</forename><surname>Camata</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Federal University of Juiz de Fora</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Alvaro</forename><forename type="middle">L G A</forename><surname>Coutinho</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">COPPE/Federal University of Rio de Janeiro</orgName>
								<address>
									<country key="BR">Brazil</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Patrick</forename><surname>Valduriez</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">Inria and LIRMM</orgName>
								<address>
									<settlement>Montpellier</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Marta</forename><surname>Mattoso</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">COPPE/Federal University of Rio de Janeiro</orgName>
								<address>
									<country key="BR">Brazil</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Future Generation Computer Systems</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">461656ED6E5DA595019F449779490971</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-04-12T14:47+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Parameter Tuning</term>
					<term>Computational Steering</term>
					<term>Provenance Data</term>
					<term>Dynamic Workflows</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In long-lasting scientific workflow executions in HPC machines, computational scientists (the users in this work) often need to fine-tune several workflow parameters. These tunings are done through user steering actions that may significantly improve performance (e.g., reduce execution time) or improve the overall results. However, in executions that last for weeks, users can lose track of what has been adapted if the tunings are not properly registered. In this work, we build on provenance data management to address the problem of tracking online parameter fine-tuning in dynamic workflows steered by users. We propose a lightweight solution to capture and manage provenance of the steering actions online with negligible overhead. The resulting provenance database relates tuning data with data for domain, dataflow provenance, execution, and performance, and is available for analysis at runtime. We show how users may get a detailed view of the execution, providing insights to determine when and how to tune. We discuss the applicability of our solution in different domains and validate its ability to allow for online capture and analyses of parameter finetunings in a real workflow in the Oil and Gas industry. In this experiment, the user could determine which tuned parameters influenced simulation accuracy and performance. The observed overhead for keeping track of user steering actions at runtime is less than 1% of total execution time.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>In typical High-Performance Computing (HPC) scientific workflows, or workflows for short, computational scientists (the users in this work) need to set-up several configuration parameters. These users are specialists in computational models or simulations to solve complex physical problems. They select initial values for the parameters based on their domain expertise. These parameters include solver options, tolerances, and error thresholds. Because of the exploratory nature of those computations, it is hard to determine, before the execution, which configuration values will work best, even for the most experienced users. For this reason, dynamic workflows (i.e., workflows that can be changed at runtime) allow for fine-tunings of specific parameters <ref type="bibr" target="#b0">[1]</ref>. These workflow dynamic adaptations are known as user steering actions. After the initial setups, the user starts the computation and, based on online intermediate data analysis, fine-tunes data. Online intermediate data analysis is supported by monitoring tools <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref> and user steering involves several actions, such as</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Motivating Case Study</head><p>The case study explored in this paper is based on a real Computational Fluid Dynamics application in the Oil and Gas domain, called libMesh-sedimentation <ref type="bibr" target="#b17">[18]</ref>. It is a simulation solver, implemented in C++ with source code available on GitHub <ref type="bibr" target="#b18">[19]</ref>, built on top of a widely used parallel fine element framework, libMesh <ref type="bibr" target="#b19">[20]</ref>, which supports parallel simulation of multiscale, multiphysics applications. libMesh interfaces with several libraries for Computational Science and Engineering applications (e.g., PeTSc, Metis, Parmetis, LAPACK). Also, scientific visualization tools like ParaView <ref type="bibr" target="#b20">[21]</ref>, are typically used in these applications to gain insight from the computations. In this class of applications, users need to set-up the goals of the computation, and parameters for the numerical methods. Examples of parameters are tolerances for linear and nonlinear solvers, number of levels for mesh adaptation, tolerances for space and time error estimates, etc. These parameters have a direct influence on the accuracy and simulation costs, and bad choices may lead to inaccuracies and even to a simulation crash. As an example, the number of finite elements predicted by the mesh adaptation procedure may exceed the memory available in a processor, and the simulation is halted with an error message. In simulations with complex dynamics, it is often very difficult to set-up a priori a maximum number of finite elements per core that will guarantee the necessary accuracy without exhausting the available resources. Thus, Quantities of Interest (QoIs) like number of finite elements predicted must be tracked and analyzed during execution. The resulting application can be seen as an iterative workflow, as illustrated in Figure <ref type="figure" target="#fig_0">1</ref>. In libMesh-sedimentation, users identify a workflow within the simulation code. They </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Provenance Management in Computational Steering</head><p>In-situ visualization o instrument the code to capture monitoring data, which are relevant data for online analysis, and add a steering point (after the for loop in time &lt; tmax, in Figure <ref type="figure" target="#fig_0">1</ref>). Monitoring data captured are stored in a provenance database that follows W3C PROV standards and, in this work, we introduce the track of steering actions to be registered and properly related in a provenance database.</p><p>To be able to accomplish this, we present a methodology that describes the steps needed to register and evaluate steering actions. Some of these steps occur offline, before the execution starts, whereas others occur online, during the execution. The offline steps are mainly related to invoking DfAdapter services at the user code, which is a common practice in scientific applications <ref type="bibr" target="#b1">[2]</ref>. The user code works as a script, which automates the execution of tasks and often does calls to parallel libraries or other services. We extended our previous methodology <ref type="bibr" target="#b21">[22]</ref> to add user steering support. Figure <ref type="figure" target="#fig_3">2</ref> summarizes all high-level steps for enabling workflow steering.</p><p>Methodology. In Step 1, users identify inputs and outputs of relevant parts of their code to form a workflow of chained activities with a dataflow between activities. These inputs and outputs are often domain-specific relevant data (like QoIs) for the users so they can monitor the evolution of the simulation, analyze intermediate data, and understand partial results during the long run. Also, users specify the initial settings and input datasets. In Step 2, users insert service calls in the workflow code to add monitoring points. In monitoring points, input and output data elements in the dataflow are specified so they can be captured for monitoring and online data analysis. In Step 3, users identify parts of the code that can be dynamically modified at runtime and add steering points in those parts. Steering points should be added in safe points of the code to avoid execution or data inconsistencies. Usually, users know where to add steering points. A typical example occurs in iterative workflows where each new iteration is an opportunity to redefine parameters or input datasets preset beforehand. In this case, a steering point is added in the beginning of the iteration. Each iteration is often executed as a whole. When a user steers, the steering will take effect only at the next iteration, rather than changing values during an iteration. This helps to make data and execution consistent to what the user decided to steer during the iteration.</p><p>After these three initial offline steps, the workflow is submitted to parallel execution in an HPC machine. In Step 4, those monitoring data specified at Step 2 are captured and can be analyzed online. In Step 5, based on the analyses, users may decide to execute a steering action. In Step 6, the system tracks steering actions and relates to the data being captured. Finally, in <ref type="bibr">Step 7</ref>, users analyze the consequences of their actions relating to domain-specific relevant data and execution data (e.g., time taken to execute a processing). We highlight that,  to the best of our knowledge, Steps 6 and 7 are not supported in Computational Steering systems <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Related Work in Computational Steering in Scientific Workflows</head><p>In a recent survey <ref type="bibr" target="#b22">[23]</ref>, the authors discuss past, present, and future of scientific workflows. As a challenge, they argue that "monitoring and logging will be enhanced with more interactive components for intermediate stages of active workflows." We did not find any work that registers steering actions in dynamic workflows in logs or provenance databases. Thus, there is no related work on tracking steering data and querying workflow data considering steering actions. Therefore, we initially analyze the main issues on computational steering, and then the related work on HPC computational steering, afterwards steering in application-specific scenarios, and finally we discuss steering support specifically in Parallel Workflow Management Systems (WMSs) and science gateways. The capability to track steering actions we are proposing is complementary in systems that already provide steering support.</p><p>Mattoso et al. <ref type="bibr" target="#b0">[1]</ref> investigate six aspects of computational steering in large-scale workflows: interactive analysis, monitoring, human adaptation, notification, interface for interaction, and computing model. Despite the importance of each of them individually, the first three are essential for online analysis, tracking and evaluation steering actions (the challenges addressed in this work). Users will know how to adapt the workflow if they can analyze intermediate data during a long-term execution. Online provenance data management is an essential asset for interactive intermediate data analyses and monitoring, which are important ways to help gaining insights from the data being generated during execution. For monitoring, users set up monitoring analyses and wait for the results to be generated. Results might be presented as graphical dashboards or three-dimensional in-situ data visualizations. As users gain insights from monitoring results, new data exploration through interactive analysis can be done, and the monitoring can be adapted <ref type="bibr" target="#b14">[15]</ref>. Human adaptation is the most important aspect of computational steering. There are several types of expertise in humans that are involved in a long-lasting workflow execution <ref type="bibr" target="#b14">[15]</ref>. Domain scientists (e.g., biologists, geologists) are experts in defining the hypothesis behind the experiment and interpretation of the results. Computational scientists (e.g., bioinformaticians, numerical analysts) are experts in programming the computational models that do the simulations or in using programs that require HPC. They usually also have a good knowledge of the application domain. Computational scientists are the users responsible for computational steering <ref type="bibr" target="#b0">[1]</ref><ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref>.</p><p>Data-oriented solutions for workflows facilitate online human adaptation. When a user adapts the dataflow, new data (user steering data) are generated, and thus their provenance must be registered. Not tracking may negatively influence results reliability, validation, and reproducibility.</p><p>For example, if a user removes subsets of a dataset (data reduction), the tasks (execution data) that would consume them will not need to be processed <ref type="bibr" target="#b14">[15]</ref>. If a user fine-tunes parameters of a program, the overall result may be changed. In addition to reliability and reproducibility, having such data enables users to learn from their own adaptations: for example, they may find that when they tune certain parameters to a given range of values, the convergence of the linear equation solver improves by a certain amount. Finally, these adaptation data allow for building AI-based systems that help users while they are steering simulations <ref type="bibr" target="#b23">[24]</ref>, as they can extend their training database with provenance of adaptations.</p><p>Computational steering in parallel applications is a necessity for HPC users for decades <ref type="bibr" target="#b5">[6]</ref>. Several systems provide steering support. Examples are SCIRun <ref type="bibr" target="#b24">[25]</ref>, CUMULVS <ref type="bibr" target="#b25">[26]</ref>, ParaView Catalyst Live <ref type="bibr" target="#b20">[21]</ref>, GRASPARC <ref type="bibr" target="#b26">[27]</ref>, Cactus <ref type="bibr" target="#b27">[28]</ref>, RealityGrid <ref type="bibr" target="#b28">[29]</ref>, and many others <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b29">[30]</ref><ref type="bibr" target="#b30">[31]</ref><ref type="bibr" target="#b31">[32]</ref><ref type="bibr" target="#b32">[33]</ref><ref type="bibr" target="#b33">[34]</ref>. Similarly to our approach, these systems also allow for monitoring and parameter tuning, and require code instrumentation via libraries or API calls, which is an approach often adopted in Computational Science and Engineering <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b31">32]</ref> applications. Nevertheless, tracking of steering actions is not provided in any of those systems.</p><p>In addition to those systems, there are steering solutions for specific applications or domains. Often, examples that need user steering come from parallel scientific applications in the Oil and Gas industry <ref type="bibr" target="#b34">[35]</ref>. For instance, BSIT <ref type="bibr" target="#b11">[12]</ref> is a platform tailored for seismic applications that supports adaptations in parameters, programs, datasets, but it does not register provenance or allow for adaptation data analysis. Other examples are applications for Computational Fluid Dynamics <ref type="bibr" target="#b35">[36]</ref>.</p><p>With respect to WMSs, only a few <ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref> support user steering. Pegasus <ref type="bibr" target="#b36">[37]</ref> provide a database with execution data to help debugging. Lee et al. <ref type="bibr" target="#b6">[7]</ref> propose an extension to Pegasus to execute scientific workflows adaptively based on the analysis of Pegasus' database. However, in case a replacement on a data transformation occurs, because the adaptation is not registered, analyzing the average of execution time of a data transformation might give inconsistent results. Likewise, OpenMole <ref type="bibr" target="#b7">[8]</ref> is a WMS for simulation models that need continuous adaptation and improvement. Users can replace programs in the workflow during its execution. Due to the lack of registering when, what, and who did the replacement, a different user may choose an already tested configuration disregarding previous efforts. Both Pegasus with its extensions <ref type="bibr" target="#b6">[7]</ref> and OpenMole could benefit from our approach to register their supported steering actions.</p><p>FireWorks is a WMS <ref type="bibr" target="#b8">[9]</ref> that uses a DBMS-driven workflow execution engine. It has a JSON-based approach for state management and uses MongoDB to query JSON documents to monitor workflow execution. However, no other steering action is supported. Copernicus WMS <ref type="bibr" target="#b9">[10]</ref> also allows for dynamic workflow steering via parameter tuning, sharing similar motivations to ours. It also aims at analyzing data to steer exploration towards undiscovered regions of a solution space. Typical parameters tuned by users are initial seeds, number of samples, and parameters specific to the analysis method. These systems evidence the need for tracking and querying steering actions like we propose in this work.</p><p>Chiron WMS enables users to change filter values and adapt loop conditions of iterative workflows <ref type="bibr" target="#b37">[38]</ref>, and reduce input datasets <ref type="bibr" target="#b14">[15]</ref>. These works show that online adaptations significantly reduce overall execution time, since users can identify a satisfactory result before reaching the programmed number of iterations. However, tracking the adaptation has not been addressed in Chiron.</p><p>WorkWays <ref type="bibr" target="#b3">[4]</ref> is a powerful science gateway that enables users to dynamically adapt the workflow by reducing the range of some parameters. It uses Nimrod/K as its underlying workflow engine, which is an extension of the Kepler workflow system <ref type="bibr" target="#b38">[39]</ref>. It presents several tools for user interaction in human-in-the-loop workflows, such as graphic user interfaces, data visualization, and interoperability among others. Such graphical functionalities can highly benefit the user experience with the steering solution, and hence could be incorporated to DfAdapter for future work.</p><p>WINGS <ref type="bibr" target="#b39">[40]</ref> is a WMS concerned with workflow composition and its semantics. WINGS facilitates the iterative process of designing workflows. This is complementary to our solution, as we need to identify the dataflow behind a computational model or simulation before the execution starts. WINGS also focuses on assisting users in automatic data discovery. It helps generating and executing multiple combinations of workflows based on user constraints, selecting appropriate input data, and eliminating workflows that are not viable. However, it does not allow for online parameter tuning, nor does it record the provenance of adaptations at runtime.</p><p>While WMSs and science gateways provide for efficient parallel workflow execution, this can be an issue when the workflow is already a parallel application. Simulations that use highly parallel libraries or adaptive parallel algorithms, already implement parallel execution control and scheduling on HPC machines. Often, this application parallelism conflicts with the scheduling and parallel execution of WMSs or science gateways.</p><p>Therefore, none of these systems, WMS, science gateways or other systems with steering support, provide steering action data tracking. The steering action definitions in Section 4 and the system design principles presented in Section 5 to track steering actions give directions that may complement current approaches to add the track of steering actions for dataflow analysis integrated with dynamic steering, following data provenance standards, all with negligible performance overhead.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">User Steering Actions Definitions</head><p>Two data categories should be analyzed online to support human adaptation: domain dataflow and workflow execution <ref type="bibr" target="#b0">[1]</ref>. While workflow execution is often associated to the control of task flow between chained activities <ref type="bibr" target="#b13">[14]</ref>, dataflows are often associated to datasets being transformed by the chaining of data transformations <ref type="bibr" target="#b15">[16]</ref>. Each data transformation operates on input datasets and transforms it into output datasets. In parallel executions, elements in the datasets are mapped to workflow activity tasks. Domain dataflow. The datasets that are produced or generated in the flow between data transformations are part of the domain application data that compose the domain dataflow. To analyze intermediate data with its context, domain dataflow must be available for interactive analysis and monitoring, while the workflow runs. Keeping track of the raw data files while keeping their context and relating their content to provenance improves online data analyses <ref type="bibr" target="#b15">[16]</ref>.</p><p>Workflow execution data. Data related to the workflow execution performance is very helpful in interactive data analysis, monitoring, and debugging. Users may monitor the execution data to control the amount of computational resources being used. Users are frequently interested in knowing how long tasks are taking or how much memory or CPU they are consuming. This information can deliver interesting insights when linked to domain dataflow data. For example, users can investigate which values are making a task consume more memory than expected.</p><p>Scientific workflows are data-centric and so are steering actions. Therefore, we follow a dataflow approach as opposed to a workflow control-based approach. Inspired by dataflow concepts proposed by Ikeda et al. <ref type="bibr" target="#b40">[41]</ref>, in previous works <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16]</ref> we proposed a conceptualization for the flow of data elements in running workflows. In the present work, we refine and extend such concepts aiming to add semantics to data elements and to define steering actions. These semantics represent the role of data elements in the steered execution, for example, a parameter and a loop condition. Next, we define these concepts formally. Definition 5: Dataflow. A dataflow is represented by ð·ð = (ð, ð, â), where ð is the set of all data transformations participating in the dataflow, ð is the set of all datasets consumed or produced by the data transformations, and â is the set of all data dependencies between the data transformations (adapted from background work <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b40">41]</ref>). Definition 6: Semantics of attributes. We further group each attribute ð , â ð¢(ð·ð) by its semantics Î£(ð·ð), so that: Î£(ð¼ 56 ) = {ð¹ K , ð K , ð K , ð¿ K } and Î£(ð 56 ) = {ð¹ O , ð O , ð¶ O , ð¿ Q }, where:</p><p>â¢ ð¹ K and ð¹ Q contain attributes that represent pointers to input and output files, respectively.</p><p>â¢ ð K and ð Q contain attributes for extracted data or metadata from input and output files, respectively. â¢ ð K contains attributes for general purpose input parameter values of the data transformation. â¢ ð¿ K contains attributes used by in iteration loop, i.e., used for data transformations that evaluate a loop. â¢ ð¿ Q contains output values especially related to an iteration in case of data transformations that evaluate a loop. â¢ ð¶ Q contains attributes for any output values that are explicit data transformation results.</p><p>Such added semantics improves the data modeling of the dataflow and allows specifying which attributes of a ð·ð are parameters to be steered. Parameters ð K are the main target of fine tunings. For example, parameters are numerical solver configurations, thresholds, or any other parameter that can be adjusted.</p><p>ð¹ K and ð¹ Q are often large raw (textual, imagery, matrices, binary data, etc.) scientific data files in a wide variety of formats depending on the scientific domain (e.g., FITS for astronomy, SEG-Y for seismic, NetCDF for computational fluid dynamics simulations). These data are typically not tuned, but are important for data analyses.</p><p>In the case of output data, examples are QoI. Some applications write calculated values, like the QoI results of a data transformation into files and they often need to be tracked. ð Q represents these special resulting extracted data, which are often scalars, useful for domain data analyses <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b17">18]</ref>. ð K and ð Q can be seen as views over the actual large raw data files, as users can have a big picture of the content of the files through them.</p><p>Besides large scientific data files produced by data transformations, they may produce explicit output results, ð¶ Q , often scalar values or simple arrays that are very meaningful for the overall result. Since they may be of high interest for the user, these values are typical provenance data that need to be registered.</p><p>Moreover, the semantics of a dataset ð·ð may not be applicable to all attributes. For example, if a data transformation does not evaluate a loop, ð´(ð·ð) of this data transformation does not contain ð¿ K or ð¿ Q . Examples of ð¿ K are loop-stop conditions (e.g., "max" in case of "while counter &lt; max" loops or "threshold" in case of "while error &gt; threshold" loops), or any other parameter used inside the iterations. In data transformations that evaluate loops, each iteration may be modeled as a loop evaluation execution and produces ð¿ Q . They are attribute values that contain current values being used to evaluate a loop, which are updated at each iteration (e.g. "counter" or "error"). Definition 7: Steering action. A steering action ðð´ is an interaction between a user who analyzes or monitors or dynamically adapts one or more elements of ð·:</p><formula xml:id="formula_0">ð·â² â ðð´ D (ð·)</formula><p>where ð· is a ð·ð or a ð·ð or a ð·ð and ð¼ is a steering action clause that defines the analysis or monitoring or adaptation that result in ð·â².</p><p>For example, when ð· is a ð·ð, users might need to monitor (or analyze or adapt) the composition of data transformations of the dataflow. When ð· is a ð·ð, users might need to monitor (or analyze or adapt) the ð·ð structure. In case of ð·ð, users might need to monitor (or analyze or adapt) data elements in the ð·ð. Depending on the operand ð·, ð¼ specifies which elements the user will interact. When ðð´ is monitoring or analysis, ð·â² contains the result of the monitoring query or analysis. When ðð´ is adaptation, ð·â² contains the resulting data modified by the user. Definition 8: User steering data. To register a steering action ðð´, user steering data need to be tracked. User steering data is denoted by (ð·, ð¼, ð· W , ð, T), where ð·, ð·â² and ð¼ are the data and the clause, respectively, involved in ðð´; ð contains data about the user who performed ðð´; and T is a set of data transformation executions related to ðð´. Any other data that benefit the register of the steering action ðð´ can optionally be tracked and associated to ðð´. For example, the current wall time at which the ðð´ occurred, or textual annotations informed by the user at the moment of ðð´ can benefit its register.</p><p>Considering that parameter fine tuning is the main action within adaptations in a ðð´, we define a special case of ðð´, named ðð¢ðð. Definition 9: Tune. ðð¢ðð is a steering action for parameter tuning as follows:</p><formula xml:id="formula_1">ð¼ 56 W â ðð¢ðð (Z,[) (ð¼ 56 )</formula><p>where the operand ð¼ 56 contains old values of attributes being tuned into ð¼ 56 W with the new values. ð¼ 56 W follows the same schema ð¢(ð¼ 56 ) and semantics Î£(ð¼ 56 ). (ð, ð¶) is the steering action clause. ð is a set of ordered pairs (ð, ð£), where ð â ð K is the parameter being tuned and ð£ is its new value. ð¶ expresses a predicate to address a specific data element that will have its parameters tuned. In case of an ð¼ 56 that contains a single data element, ð¶ is optional.</p><p>To register a ðð¢ðð operation, the user steering data tracked are: (ð¼ 56 , ð, ð¶, ð· W , ð, T, ð), where ð is an optional argument that contain useful data related to the steering action context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">DfAdapter</head><p>In this section, we present DfAdapter, a lightweight solution aimed at capturing provenance of online steering actions in dataflows and storing the related dataflow provenance to enable understanding of the impacts of the action. Section 5.1 shows the main system design principles followed by DfAdapter. Section 5.2 shows how steering actions are captured in different workflow execution models. Section 5.3 presents the system architecture. Section 5.4 provides a general overview of how to use DfAdapter. Sections 5.5 and 5.6 present the provenance data model and its implementation using the relational data model, respectively. Section 5.7 provides a formalism to calculate DfAdapter's overhead.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">System Design Principles</head><p>In this section, we explain the core system design principles followed by DfAdapter. Asynchronous service calls. DfAdapter is coupled to adaptable applications, like systems that support computational steering <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref> or adaptable simulations as our case study. In either case, an API for DfAdapter is used so it can be called from the adaptable application. Provenance capture calls are placed in monitoring points in the workflow code to capture provenance of the dataflow and execution of data transformations. Similarly, to capture provenance of steering actions, DfAdapter calls placed in the steering points, allowing DfAdapter to track the steering actions in data transformations at runtime.</p><p>Attaining low performance overhead is a basic requirement in DfAdapter, otherwise computational scientists, used to high performance systems, will not use the tool. For this, calls to DfAdapter are asynchronous, meaning that when the user adapts the running workflow, the track of steering actions is done in such a way that the main computational process will not wait until the track finishes. The same approach is valid for any added monitoring data tracking in the code. In addition, the most computationally costly components in DfAdapter, such as the ones that store steering data in the provenance database during workflow execution, are deployed in separate hardware, different from where the main computational process runs, but in same internal network (e.g., the nodes in the cluster has local access to the node that runs DfAdapter's provenance server) to reduce communication costs, following in-situ and in-transit approaches <ref type="bibr" target="#b1">[2]</ref>. This avoids making DfAdapter and the main computational process compete for resources. Following these principles, the utilization of DfAdapter attains low added performance overhead for provenance of steering actions, such as less than 1% in our case study (Section 7).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Adapter service and communication between DfAdapter interface and the running workflow.</head><p>Adding steering points in an adaptable workflow means that in those points there will be data communication between the running workflow and DfAdapter, so that the data flowing in the workflow can be modified. To represent this communication between the front end (from which the user sends steering commands) and the back end (which receives the commands and effectuates an adaptation in the running workflow), we use the notion of an adapter service. The adapter service in an adaptable workflow has the communication protocol capable of adapting a running application. The basic idea is that the user uses DfAdapter interface that communicates with the front end of the adapter service, which sends steering commands to the back end of the adapter service that does the adaptation in the running workflow, and finally DfAdapter registers the provenance of the steering action. There are different ways to implement such data communication between the back and front ends of an adapter service <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6]</ref>. DfAdapter can be coupled to any of these implementations. These implementations are the following.</p><p>(i) File-based checks. This is a simple yet widely used way to implement data communication <ref type="bibr" target="#b1">[2]</ref>. In this case, there are files in a storage resource that are accessible both by the front and by the back ends of the adapter service. In that case, when the user uses DfAdapter interface to steer the workflow, the front end of the adapter service modifies a file according to the user inputs. When the program pointer in the running workflow reaches a steering point, the back end of the adapter service verifies if files were modified and, in case of modification, the adaptation is carried out and DfAdapter is called to register the adaptation. Although file-based checks are a simple approach, it is widely used especially by users that implement their own ad-hoc way to make their simulation steerable, as in our case study. However, it requires that front and back ends share access to files in a storage resource, which may not be always possible.</p><p>(ii) Message passing. It is another way to implement data communication. In this case, when the user uses DfAdapter interface, the adapter service's front end sends a message to its back end in the running workflow. When the steering point is achieved, the adapter service's back end verifies if a message has arrived and effectuates the adaptation accordingly, and DfAdapter is called to register the adaptation. MPI, sockets, or RESTful HTTP messages can be used as communication protocol to implement this. Many systems with steering support use message passing to implement data communication <ref type="bibr" target="#b28">[29]</ref><ref type="bibr" target="#b29">[30]</ref><ref type="bibr" target="#b30">[31]</ref>. This is an alternative to file-based checks, as it does not require files to be shared in a storage resource by the adapter's front and back ends.</p><p>(iii) DBMS-driven. It is an alternative to message passing and file-based checks. It is similar to file-based checks in the sense that there is a DBMS that is accessible both by the DfAdapter interface (via the front end in the adapter service) and the running workflow (via the back end). It is similar to message passing in the sense that it does not require files to be shared in a storage resource. Rather, data that can be modified at runtime are managed by the DBMS that can even run in-memory, depending on the DBMS. In this implementation, when the user adapts using DfAdapter, the adapter front end modifies data in the DBMS. When the program pointer achieves the steering point in the running workflow, the steered end checks if the data have been modified, carries the adaptation accordingly, and DfAdapter is called to register the adaptation. We implemented the data communication and steering action tracking in a synthetic workflow example using the parallel frameworks Apache Spark and Redis, a lightweight in-memory Key Value store, as the DBMS between the workflow and DfAdapter. The source code is available on GitHub <ref type="bibr" target="#b41">[42]</ref>.</p><p>DBMS and data model for the Provenance Database. DfAdapter needs a DBMS to manage the provenance database. Several data models can be used for provenance databases, such as graph and relational data models. The usage pattern in DfAdapter is that the running workflow only produces insertions to the provenance database, while the user typically runs provenance queries for online data analyses to support decision-making, i.e., OLAP queries. This usage pattern, both by the workflow system and by the user, is benefited from columnoriented relational DBMSs, as shown in some of our previous works <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b17">18]</ref>. Moreover, since there may be many appends to this database during execution, the DBMS must be able to handle parallel requests from clients. Thus, DfAdapter follows this principle and uses a DBMS, called MonetDB â  , which has these characteristics.</p><p>Provenance data modeling. Provenance data management is at the core of DfAdapter. Instead of creating new standards, DfAdapter follows the well-stablished W3C PROV recommendation, and extends to add the specific parts for the track of parameter tuning. By adhering to W3C PROV standard, DfAdapter aims at allowing for interoperability among provenance databases. In addition, another important principle in DfAdapter is that the provenance data model is abstract and flexible enough to be used in different domains or applications. Our previous works show that similar provenance data modeling used by DfAdapter has been shown useful to capture relevant domain-specific data as well as generic dataflow provenance in other applications, such as in the Oil and Gas domain <ref type="bibr" target="#b14">[15]</ref> and Astronomy <ref type="bibr" target="#b15">[16]</ref>. The provenance data model (Section 5.5) is thereafter implemented in a relational data model (Section 5.6).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Keeping Track of Parameter Tuning in Different Workflow Execution Models</head><p>Workflow execution models are acyclic or cyclic. Acyclic model is the most commonly supported in workflows (often modeled as a Directed Acyclic Graph), although the cyclic model need to be supported for extreme-scale workflows <ref type="bibr" target="#b13">[14]</ref>. We design our solution for tracking user steering actions to support both models.</p><p>In case of acyclic models, after the user tunes attributes in an ð¼ 56 , provenance data collectors register which attributes were modified with their old and new values, and execution data of tasks that were running at the time of the adaptation.</p><p>Both sequential and concurrent execution models can be iterated in a cyclic model. Thus, at runtime, when the workflow is running in a specific cycle, a user can tune parameters. Cyclic execution models can be further distinguished between (i) loops without dependencies between iterations, also known as parameter sweeps; and (ii) loops with dependencies. Examples of loops with dependencies are counting loops, such as for i=0; i &lt; max; i++, or conditional loop, such as "while error &gt; threshold". Additionally, Dias et al. proposed "external steering" loops, where the user adapts loop-stop conditions <ref type="bibr" target="#b37">[38]</ref>.</p><p>For (i) parameter sweep loops, the user may want to modify parameters that are to be swept. For (ii) loops with dependencies, the current iteration counter is an important value to be tracked. In those cases, the evaluation of a loop can be modeled as a data transformation <ref type="bibr" target="#b37">[38]</ref> in a dataflow. Thus, the ðð¢ðð operation represents the tuned attributes that will be used inside the cycle in ð¿ K ; and ð (optionally tracked when ðð¢ðð occurs) contain the current iteration counter. Thus, ð is tracked with the current iteration counter of the loop, alongside with ð, ð, ð, as in previous execution models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">DfAdapter Architecture and Details</head><p>In this section, we present details about DfAdapter architecture following the design principles previously presented. DfAdapter controls the front end of the adapter service. That is, when the user submits a steering command using DfAdapter interface, it registers the beginning of the steering action and makes the front end of the adapter service call its back end. DfAdapter's API method to be inserted in the steering points of the workflow code implements the ðð¢ðð operator. When the back end of the adapter service effectuates the adaptation, an API call to DfAdapter is executed, which tracks the steering action and stores in the provenance database. Figure <ref type="figure" target="#fig_4">3</ref> shows DfAdapter system architecture and Figure <ref type="figure" target="#fig_5">4</ref> shows an UML sequence diagram that represents the steps that occur when the user issues a steering command.  The sequence of steps that occur when a user steers using DfAdapter are as follows: First, during workflow execution, (0) monitoring data specified in monitoring points are sent to the Provenance Server via Monitoring data tracker API calls. Then, (1) Provenance Server stores monitoring provenance in the Provenance Database. While the workflow runs, user can use Query Interfaces and Dashboards to follow the intermediate data results and decide for a steering action. If the user decides for a steering action, (2) the user sends a steering command using DfAdapter's Steering Command Interface, which (3) calls the Adapter Service front end , which (4) calls the Steering data tracker API method to (5) register the beginning of a steering intention. The Adapter Service front end reacts to DfAdapter's call and (6) communicates with the Adapter Service back end, which <ref type="bibr" target="#b6">(7)</ref> has adapters that are able to effectuate the adaptation. When <ref type="bibr" target="#b7">(8)</ref>  adaptation occurred (e.g., it verifies that a file or a data structure, depending on the adapter service implementation, has been changed because of a steering action), the (9) Steering data tracker API method inserted in the steering point is called. <ref type="bibr" target="#b9">(10)</ref> Provenance Server receives the calling and stores steering provenance in the Provenance Database. After that, the workflow continues to run normally together with monitoring data that are continuously tracked and stored, and (11) the user can run user steering action analysis.</p><p>The ðð¢ðð operator is implemented in DfAdapter system in the Steering data tracker API method inserted in the steering point (9th step in the sequence diagram Figure <ref type="figure" target="#fig_5">4</ref>). It is implemented as shown in Algorithm 1, where we denote as ð the set of ordered pairs with the old values for the tuned parameters. Algorithm 1 is responsible to register new domain data that were modified in the adaptation as well as register their corresponding old values. It tracks current execution data, iteration counter values (in case of iterative workflows), and user data. Then, it stores user steering data relating to all other data being continuously tracked during workflow execution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">DfAdapter Utilization</head><p>To describe how DfAdapter is used, we resort to the methodology listed in Figure <ref type="figure" target="#fig_3">2</ref>. We explain how it can be added to dynamic workflows before execution and how it can be used to track steering actions.</p><p>Before execution. The user identifies a workflow by specifying parts of the code that can be modeled as data transformations and their datasets, and the data dependencies. Monitoring and steering points are added, as in Figure <ref type="figure" target="#fig_0">1</ref> of our case study. Then, DfAdapter API calls are inserted in the workflow code to capture provenance of the steering actions. Figure <ref type="figure">5</ref> shows an example using an excerpt of our case study workflow code. In Line 6, a method calls the DfAdapter API that implements Algorithm 1. The remainder provenance methods (Lines 3, 10, 13, 16) contain library calls inserted in the user code for monitoring so the user will know how to steer during execution. During execution. DfAdapter wraps the front end of the adapter service. When users use DfAdapter interface to adapt the running workflow, the provenance of the steering actions are captured. The interface is command line-based, to be used in a terminal connected to the HPC machine where the workflow runs. In the command line, users only need to inform the input dataset ð¼ 56 to be adapted, and a simple key-value data structure containing the parameters and their new values. For flexibility, the key-value data structure can be passed directly using the argument --p or write it in a file and pass its path as the argument. We add an optional argument --reason to allow users to annotate that specific steering action. Keeping the interface simple helps computational scientists to adhere to DfAdapter utilization. Figure <ref type="figure">6</ref> shows an example of DfAdapter's command line interface.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">W3C PROV for the Provenance of Parameter Tuning</head><p>In this section, we propose a data provenance representation of parameter tunings. We build on our previous PROV-DfA <ref type="bibr" target="#b2">[3]</ref>, a representation for provenance of steering actions in dataflows, which is an extension of the W3C recommendation PROV-DM <ref type="bibr" target="#b42">[43]</ref>. In this paper, we specialize PROV-DfA for parameter tuning in user-steered dataflows. In Figure <ref type="figure" target="#fig_6">7</ref>, we use a class diagram to present the provenance data representation for parameter tuning. The classes in white background represent prospective provenance and in gray background represent retrospective provenance. The main added class is ParameterTuning. Parameter tunings at runtime are registered as retrospective provenance as they occurred while the workflow is in execution.</p><p>ParameterTuning represents provenance of a ðð¢ðð operation (c.f. Definition 9). It has two relationships (WasInfluencedBy) with AttributeValue. The first one is to relate to the new values of parameters being tuned. Values of parameters are modeled as AttributeValue (derived from the prospective entity Attribute), part of a DataElement of a Dataset (the ð¼ 56 having its parameters tuned). Using W3C PROV relationships, we model the new attribute value of a parameter being tuned as a revision of (WasRevisionOf) the old parameter value, which is also an attribute value; hence the auto-relationship in AttributeValue. Thus, these relationships are for representing the new and old values for the parameters tuned, i.e., ð and ð. The second WasInfluencedBy relationship between ParameterTuning and AttributeValue is to relate the tuning with ð values, which are also modeled as AttributeValues.</p><p>To relate the ParameterTuning with ð, we add the relationship WasInfluencedBy between ParameterTuning and ExecuteDataTransformation, which is the most representative class for workflow execution data (Section 4). For user data ð, we relate ParameterTuning with Person, via the added WasSteeredBy relationship. We also create a new class, Adapter, which is a PROV SoftwareAgent, to store data about the program or service that can effectively adapt the dataset. We relate the tuning with the Adapter class via WasAssociatedWith to explicitly represent which Adapter call was used to tune the parameters. For example, one could use this relationship to store the arguments used by the service call to adapt the dataflow. Finally, ParameterTuning can be further extended for any other data that the user may find relevant, such as descriptions for the tuning or the criteria ð¶ used to select the data element that will be tuned.</p><p>Therefore, with this W3C PROV-extended provenance data model, we can represent provenance of online parameter fine-tunings in dataflows steered by users. In the next section, we present one possible implementation of this provenance model using the relational data model. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6">Implementing the Provenance Database Schema for DfAdapter</head><p>We use the relational data model to represent the W3C PROV-extended provenance model presented in Section 5.5. An excerpt of the relational database schema is in Figure <ref type="figure" target="#fig_7">8</ref>, whereas a complete figure can be found on GitHub <ref type="bibr" target="#b41">[42]</ref>. Whenever a user issues a steering command to tune parameters, a new instance of parameter tuning action is stored in the ParameterTuning table. Since a parameter tuning may modify one or many attributes, and the same attribute may be modified by many steering actions, there is a many-to-many relationship between ParameterTuning and Attribute tables. The associative table, ParameterTuned, has fields to store old and new values. The ð¼ 56 is a specialization of the table Dataset. Each tuple in Dataset table is a data element. Each ParameterTuning instance may directly affect one or many data elements in ð¼ 56 and a same data element in ð¼ 56 may be affected by many parameter tuning actions, hence there is a many-to-many relationship between ParameterTuning and Dataset tables, via the ModifiedDataElement associative table. Moreover, as ð 56 is also specialization of Dataset, we use InfluencedDataElement associative table between another many-to-many relationship between ParameterTuning and Dataset tables to store output data elements directly influenced by a tuning, such as iteration counter data in case of parameter tunings in data transformations that evaluate loops. Finally, we relate execution data about the current state of the execution when a tuning action happened via the associative table InfluencedTask. Tasks are directly mapped to ExecuteDataTransformation in the provenance model, and execution data are further extended with performance data via the relationship between Task and Performance tables. The person who steered and the adapter program used in that specific tuning are related and stored to ParameterTuning. Thus, because of these entities and relationships being populated during execution of the workflow in a user-accessible database, users can drive their analyses and decisions at runtime using these data. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.7">DfAdapter Overhead Analysis</head><p>The adoption of DfAdapter depends on how much execution overhead it implies. The overhead depends on data needed for monitoring and steering. For monitoring, it depends on the workflow data identified in the simulation code that needs to be tracked. That is, which input and output data values, for each data transformation, should be monitored during execution. For steering, which steering points should be added and how many steering actions actually happened during execution. In both cases, the overhead will depend on data collected for monitoring and for steering actions, always based on user decisions.</p><p>We use the dataflow concepts presented in Section 4 to express the overhead. Whenever a task ð is executed to perform a data transformation ð·ð e , the execution cost of ð, ð(ð), is given by its actual computational cost ðððð(ð) (i.e., the inherent cost of executing ð·ð e ) plus the introduced overhead ð(ð). Let the overhead ð(ð) of a task ð be expressed as a function of monitoring ð(ð) and steering ð (ð) overhead as in</p><formula xml:id="formula_2">ð(ð) = ð(ð) + ð (ð)<label>(1)</label></formula><p>The overall overhead is given by the sum of ð(ð) for all tasks ð, of all data transformations ð·ð e in ð·ð. Next, we detail the monitoring and steering components.</p><p>Analyzing monitoring overhead. Monitoring overhead ð(ð) is defined by the provenance data tracking overhead ðððð£(ð) and raw data extractions ðð¥ð¡(ð) during each data transformation execution identified by the user as relevant for monitoring, as in</p><formula xml:id="formula_3">ð(ð) = ðððð£(ð) + ðð¥ð¡(ð)<label>(2)</label></formula><p>where ðð¥ð¡(ð) = 0 if there are no extracted data values in the execution of ð.</p><p>Provenance tracking overhead ðððð£(ð) depend on the number of data values of each data element tracked at a task execution ð. Each execution ð of a data transformation ð·ð e consumes input data elements in ð¼ e and produces output data elements in ð e . In DfAdapter, data elements are stored at once in the beginning (input data elements) and at the end (output data elements) of each task ð. Provenance tracking overhead is due to preparing provenance tuples to be sent to the provenance database. Since provenance management services and the database system run in a separate computing resource and sending provenance data to be stored occurs asynchronously, provenance tracking overhead account only for preparing tuples to be sent. This represents a very low overhead, in the order of few milliseconds per task.</p><p>The raw data extraction overhead ðð¥ð¡(ð) depends on the number of data values the user wants to extract from raw data files at each execution of a ð·ð e . Let ð l be the set of all data values extracted when ð is executed. Each extracted data value ð£ , â ð l has an associated data attribute ð , in ð K or in ð Q , depending on if ð£ , is in a data element in ð¼ e or ð e , respectivelyc.f. Definition 6. ðð¥ð¡(ð) for each ð to execute a ð·ð e is therefore given by the summation of costs to extract each ð£ , â ð l :</p><formula xml:id="formula_4">ðð¥ð¡(ð) = m ðð¥ð¡(ð£ , ) n o âp q (3)</formula><p>The cost to extract a data value will depend on application-specific raw data extractors <ref type="bibr" target="#b15">[16]</ref>.</p><p>Extracting data values from raw data files to store in a provenance database for monitoring is done synchronously. Depending on the amount of data and how the raw data extractor is implemented, overhead may not be negligible, as we show in previous works <ref type="bibr" target="#b17">[18]</ref>. Analyzing steering overhead. The steering overhead occur in data transformations that have a steering point. Steering overhead also depend on when a steering action happens. When a steering action happens, all those operations presented in the sequence diagram of Figure <ref type="figure" target="#fig_5">4</ref> are triggered. Let ð be the subset of all data transformations ð·ð e in ð·ð that have steering points. For example, in our case study, the data transformation that evaluates the time loop has a steering point. Thus, ð (ð) = ð  rO,st (ð) + ð  uvt,Os (ð)</p><p>where ð  rO,st (ð) is the overhead associated to adding steering points to ð·ð e , and ð  uvt,Os (ð) is the overhead associated to DfAdapter to compute that a steering action happened. ð  uvt,Os (ð) = 0 if no steering action has been associated to the task ð and ð  rO,st (ð) = ð  uvt,Os (ð) = 0, âð·ð e â ð. The overhead ð  rO,st (ð) is a simple check to verify if a data structure has been modified during execution. Such simple verifications are nearly constant and milliseconds-long.</p><p>Putting it all together. The overall cost ð(ð·ð) to compute a dataflow ð·ð is given by the sum of costs to compute the actual computation, ðððð(ð·ð), provenance tracking, ðððð£(ð·ð), raw data extractions ðð¥ð¡(ð·ð), steering points ð  rO,st (ð·ð), and steering actions ð  uvt,Os (ð·ð), that is, ð(ð·ð) = ðððð(ð·ð) + ð(ð·ð) = ðððð(ð·ð) + ð(ð·ð) + ð (ð·ð) = ðððð(ð·ð) + ðððð£(ð·ð) + ðð¥ð¡(ð·ð) + ð  rO,st (ð·ð) + ð  uvt,Os (ð·ð)</p><p>where ð(ð·ð) = â ð(ð), ððð ððð ð¡ðð ðð  ð, ððð ððð ð·ð e l in ð·ð. Analogously, all components of ð(ð·ð) can be obtained by the summation of each individual component for all tasks. That is, ðððð£(ð·ð) = â ðððð£(ð) l , ðð¥ð¡(ð·ð) = â ðð¥ð¡(ð) l , and so on. Therefore, the overall cost of a dataflow depends on the number of workflow tasks and the overall DfAdapter overhead depends on the number of tracked tasks and raw data extraction. We may consider that raw data extraction is a powerful support for data analysis, but not necessarily for fine-tuning, which may rely on provenance data monitoring. The raw data cost will depend on how much the user is willing to pay for data analysis. Therefore, we may separate the raw data extraction overhead from the remaining overhead costs.</p><p>We observe, on the scientific domain, that ð is often a complex task, where its ðððð(ð) takes at least a few seconds, but often minutes long <ref type="bibr" target="#b43">[44]</ref>. By analyzing the individual elapsed time of the components, ðððð£, ð  rO,st , ð  uvt,Os of ð(ð), we observe that, on average, they are close to constant and typically milliseconds-long. Therefore, we can assume that in scientific applications ðððð(ð) &gt;&gt; ð(ð), which leads to the negligible overhead of tracking user steering actions. In addition, because such operations occur asynchronously and in a different computing resource, the time for the individual components of ð(ð) is "hidden" by the actual computation, which is significantly higher. This contributes to reduce the impact on the workflow execution performance.</p><p>If we consider ðð¥ð¡(ð·ð), which depends on the user settings, it is still typically very much smaller than all raw data that is being generated and stored on files. As we show in our real case study, the overall ð(ð·ð), including the costs for ðð¥ð¡(ð·ð), is less than 2%, which is still negligible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">DfAdapter in Action: Montage and Sedimentation workflows</head><p>In this section, we present two real-world workflows modeled using the dataflow concepts and illustrated with steering actions. The parameter tuning cases are presented in increasingly order of complexity. First (Section 6.1), we illustrate the ðð¢ðð operation applied to Montage <ref type="bibr" target="#b44">[45]</ref>, a well-known workflow with a parameter sweep execution model. Montage exemplifies the applicability of our solution in a simple, yet typical case. Second (Section 6.2), we apply ðð¢ðð to our case study workflow, libMesh-sedimentation <ref type="bibr" target="#b17">[18]</ref>, showing the impact of fine-tuning in the performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Steering in Montage</head><p>Montage <ref type="bibr" target="#b44">[45]</ref> is a toolkit for assembling Flexible Image Transport System (FITS) files into custom mosaics, used for identifying potential objects of interest in the sky. It has been used for large-scale data analyses in the astronomy domain since 2005. Montage provides a service to build mosaics, according to common astronomy coordinate systems, arbitrary image sizes and rotations, and all World Coordinate System (WCS) map projections. It uses algorithms to maintain the calibration and positional fidelity of images to provide mosaics based on user-defined parameters of projection, coordinates, and spatial scale. It has independent modules for analyzing the geometry of images, and for creating and managing mosaics.</p><p>Before executing the workflow in the HPC machine, the user prepares the input data to be processed. Montage's mArchiveList module can be used for downloading FITS files, which are the inputs of this workflow. Each execution of mArchiveList has the input parameters: survey (represent source of the astronomy repository -possible values are 2MASS, DSS, etc.), band (the band or filter of the downloaded images -possible values are j, h, k, dss1, dss1b, etc.), location (name or coordinate of a mosaic region), and width and height (size of the area of interest, in degrees). These parameters represent regions in the sky and can be used to drive the analyses, as certain regions may be less or more likely to contain interesting celestial objects or, depending on these values, the assembled mosaic figure may have a better or worse resolution in a specific region of interest. See mArchiveList module â¡ for further details on each of these parameters. Furthermore, the output of a mArchiveList execution is a file containing a list of URLs of FITS files that can be downloaded. Then, we download each of those FITS files and compress them in a zip file. The input parameters used to execute mArchiveList are modeled as ð K attributes in an ð¼ 56 named I_List_FITS. The parameter values used in each mArchiveList execution, to download a list of FITS files (compressed in a zip file) compose a data element in I_List_FITS. Thus, the parameter values in one data element in I_List_FITS identify one zip file. Figure <ref type="figure" target="#fig_8">9</ref> shows a small subset of I_List_FITS. â¡ http://montage.ipac.caltech.edu/docs/mArchiveList.html Then, data transformations (mapped to Montage modules) in this dataflow are modeled as follows. The first data transformation (List FITS) extracts each of those zip files. Each input FITS file has 20 types of domain-specific values (modeled as ð K ). The second data transformation (Projection) computes the projection of these astronomy-positioning references into a specific plane (extraction of 21 ð Q attributes). After that, Select Projections joins FITS projection files that are associated to the same mosaic (two ð¹ Q attributes). Create Uncorrected Mosaic creates a mosaic without overlap interferences and color corrections and, as a result, it creates a JPG image (one ð¹ Q attribute, the JPG file). The other data transformations from the Montage dataflow are defined to consider overlap interferences and color corrections to create a corrected custom mosaic.</p><p>Furthermore, Figure <ref type="figure" target="#fig_8">9</ref> gives details of the first data transformation, List FITS. List FITS data transformation uses the values in the data elements of I_List_FITS to get the zip file (stored in the directory informed in fits_dir attribute) to be processed in that execution. The workflow can be executed in a parameter sweep fashion (cyclic) with acyclic concurrent tasks, where the concurrent execution of each data element in I_List_FITS (going from this first data transformation until the last data transformation) represents a cycle in the parameter sweep (with attributes in I_List_FITS as the parameters to be swept). Then, for each FITS file in this extracted zip file, the data transformation List FITS creates a new data element in the output dataset (named O_List_FITS, which is input for the Projection data transformation). Each data element contains the file set it came from (FILE_SET) and a FITS file identifier (CNTR), allowing for tracing back, and two extracted elements from the input FITS files (CRVAL1 and CRVAL2, modeled as ð K attributes) that represent two coordinate values to determine a position in the native image coordinate system (e.g., RA, Dec), and the FITS file (modeled as ð¹ K attribute for Projection data transformation). Other attributes are extracted at runtime in these data transformation and in all others throughout the dataflow, allowing for more online data analyses.</p><p>With such online data analytical support, the user is able to better understand the status of the execution and steer it. Users analyze the generated output mosaic images to investigate for interesting celestial objects in each analyzed region. Leaving the workflow to process the entire I_List_FITS dataset with no steering actions takes a long time, even though not all parameter values need to be processed. As the users gain insights from the online data analyses, they verify that certain values of the parameters in I_List_FITS (e.g., width, height, survey) will not lead to finding interesting celestial objects. A bad choice for those parameters result in specific regions in the mosaic image with bad resolution or quality. If that region had an interesting object, it would be hard to identify. Thus, the user needs to tune the input parameters that are to be processed in order to change the region of interest. The execution may last for long hours. The user may decide on what is considered of interest several times. The situation may get worse when they tuned parameters that identify a region (with the objective of improving the quality of the resulting image), but the resulting mosaic does not have the expected quality or image resolution to validate her scientific hypothesis (the existence of a specific celestial object). Then, they need to tune the parameters in that region again to try to get better quality or resolution. Tracking such steering actions facilitates this process, allows for online data analyses of the steering actions, and improves reproducibility.</p><p>In Figure <ref type="figure" target="#fig_8">9</ref>, the user steers the dataflow during execution by tuning the parameters band, width, and height to specify a different file set to try to obtain mosaics with different resolutions in the region of interest. The ðð¢ðð operation transforms I_List_FITS into I`_List_FITS with the modified parameters, and the dataflow continues normally as if no tune happened.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Steering a Sedimentation Simulation</head><p>libMesh-sedimentation workflow is our motivating case study (Section 2). Users typically set-up the QoIs and several parameters for the numerical methods. Examples of parameters are tolerances for linear and nonlinear solvers, number of levels for mesh adaptation, tolerances for space and time error estimates, etc.</p><p>Using the dataflow concepts, the QoIs in libMesh-sedimentation and the numerical solvers' parameters are modeled as data elements in datasets flowing in the dataflow. Moreover, function calls and other parts of the simulation source code are identified as data transformations. The dataflow has two acyclic setup data transformations, then the simulation enters in a time loop, configuring a cyclic execution model with loops with dependencies. There are five data transformations in the loop, including the solvers. Each solver runs in parallel, using all computing resources available in the HPC machine. The dataflow is modeled so that at each data transformation execution that evaluates the time loop, the parameters may be modified as the user steers. In Figure <ref type="figure" target="#fig_10">10</ref>, we show the dataflow with an excerpt of the ð¼ 56 (named I_Iteration_Params in the dataflow) that contains input parameters used inside the loops (i.e., ð¿ K attributes). Also, at each iteration, the t_step and time are captured, which are ð¿ Q attributes in the O_Iteration_Params, which is the ð 56 for the time loop. Although only the datasets for the time loop are magnified in the figure, each arrow representing the original dataflow represents datasets for the data transformations, thus their data elements are being captured and stored in the provenance database.</p><p>In this scenario, we show the ðð¢ðð operator being used while the user steers (dashed lines in the figure) the dataflow by changing parameter values online. We can see that the original dataflow is modified by the user when the old value for the flow solver linear tolerance was tuned from 10 -8 to 10 -6 , generating I`_Iteration_Params. When the user requests an adaptation, the ðð¢ðð operation will trigger the adapter to carry the adaptation and collect and relate steering data in the provenance database. In this case, as an iterative execution model with loops with dependencies, ðð¢ðð also relates the tuning with values of attributes in ð¿ Q of the ð 56 of this loop evaluation data transformation. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Experimental Analysis</head><p>This section presents the tracking of online parameter fine-tunings in a real workflow from the Oil &amp; Gas industry. We show that keeping a structured history of the steering actions supports the interpretability and validation of the results (Challenge 2). Also, we introduce how users can evaluate, at runtime, the impact of adaptations, through adaptation-aware online data analysis relating to provenance, domain, and execution data (Challenge 3). In Section 7.1, we present details of using DfAdapter in the case study and the experimental setup. In Section 7.2, we present a small-scale experiment from the same domain, highlighting different uses of our solution, and then a large-scale experiment in Section 7.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Implementation Details in a Numerical Solver and Experimental Setup</head><p>Implementation in a Numerical Solver. We conduct the experimental evaluation on libMesh-sedimentation workflow, shown in Figure <ref type="figure" target="#fig_0">1</ref>, which provides a real and rich case for parameter tuning. First, it is an HPC simulation with over 70 parameters, which may be modified by the user for better performance and accuracy of results <ref type="bibr" target="#b17">[18]</ref>. Second, as this simulation may last for weeks, the user does several tunings and there is no tracking for them. Third, there is a strong potential for richer online data analyses with user steering data by correlating the steering data to domain-specific values (mainly QoIs) and other data in the </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dynamically Inserted Data Transformation</head><p>Prov. data capture excerpt of O_Iteration_Params solver_params.in provenance database. libMesh-sedimentation is implemented in C++ and its code with instrumentation for analysis and steering is available on GitHub <ref type="bibr" target="#b18">[19]</ref>.</p><p>The first step to use DfAdapter is modeling libMesh-sedimentation simulation as a workflow and identifying monitoring and steering points. Application-specific data are modeled as new tables of the relational database schema for the provenance database, and related to the existing ones accordingly (Figure <ref type="figure" target="#fig_7">8</ref>). The main ð¼ 56 that the user adapts is the input for the loop evaluation data transformation, named I_Iteration_Params, which contains input parameters for the numerical solvers. The users specify parameters in a setup configuration file. The simulation code checks, at every time step, if any modification has been made to this file. If a modification occurred, the parameters are redefined according to the new values. That is, libMesh-sedimentation workflow implements a file-based checks approach for the adapter service (Section 5.1). Modifications in this file are implemented as an adapter service front end, which basically receives parameters and new values, and modifies the file according to the inputs. Then, its execution is controlled by DfAdapter interface. The last step is to insert DfAdapter API calls in the steering points. In libMeshsedimentation code, it is inserted immediately after the parameters are reloaded when there is a modification in the configuration file. Finally, when the user steers using DfAdapter interface, it captures provenance, domain and steering data every time it detects online user steering actions.</p><p>Experimental Setup. For the large-scale test, we use 480 cores from Lobo Carneiro cluster, an SGI ICE X with 252 nodes, each with a 24-core processor and 64 GB RAM, summing 6,048 cores and 16 TB RAM. The nodes are interconnected via FDR InfiniBand and share a Lustre file system with 500 TB. In this experiment, the provenance server and MonetDB are deployed in a separate node in the cluster, different from the ones used by the main computational process for libMesh-sedimentation. For the small-scale test, we use a Dell precision T3610 workstation, 8 cores, 16 GB RAM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Small-scale experiment</head><p>The small-scale experiment is used by scientists as a benchmark to evaluate sedimentation solvers. It simulates the laboratory test carried out by de Rooij and Dalziel <ref type="bibr" target="#b45">[46]</ref> with a lock-exchange configuration. The objective of this experiment is to show the data analytical potential of our solution, how we record structured parameter-tunings, and how users can query the user steering data to enhance their analyses.</p><p>The computational setup used in this test case consists of a plane channel with dimensions 20 x 2 filled with sediments in suspension and clear fluid at rest. In the laboratory, a lockgate is used to separate the fluids before the beginning of the experiment. When the gate is removed, a mutual intrusion flow develops in which the particle-laden front travels along the bottom to the right. In this simulation, the lock-gate is located at x = 0.75. The nondimensional parameters used are Grashof number = 5.0Ã10 -6 , Schmidt number = 1.0, and settling velocity 0.02. Adaptive mesh refinement is used to track the interface between sediments concentration and clear water. Figure <ref type="figure" target="#fig_12">11</ref> shows the concentration of sediments in suspension and the adapted mesh at simulation time t = 10. In this simulation, the user is interested in analyzing possible performance gains when the number of nonlinear and linear (in this case, GMRES) iterations is tuned at runtime. Specific fine-tunings on different input parameters may impact the solvers and hence the simulation time considerably. During the execution, the user submits analytical queries to DfAdapter, addressing Challenge 1. Based on the analyses of nonlinear and GMRES iterations, the user decides to fine-tune the solver's parameters. In total, the user chooses to do six fine-tunings in 10 hours of simulation. Query 1 (whose description and tabular results are in Figure <ref type="figure" target="#fig_3">12</ref>) shows the provenance of the adaptation. It lists all the parameters tuned by a user (say, Bob), correlated to the time steps. By using Query 1, other researchers are aware that Bob adapted this workflow execution six times. The times and values are well-structured and recorded in the provenance database by DfAdapter, thus addressing Challenge 2. To inspect the consequences of adaptations, a more sophisticated analytical query is needed. Query 2 (whose description and tabular results are in Figure <ref type="figure" target="#fig_14">13</ref>) shows the average values of strategic quantities ten iterations before and after each of the four fine-tunings. The results include nonlinear and linear (GMRES) iterations, which are output values of the solver, and the number of finite elements, which is an output of the mesh refinement process and depends on other inputs of the solver. This query shows an integration of provenance of the domain dataflow, performance data (average of elapsed times in 10 iterations), and the new fine-tuning data introduced in this paper. The results of Query 2 (we highlight the main findings) show that the Tunes #3, #4, and #6 impacted the average elapsed time and the average number of GMRES iterations, which are of high interest to the user. Tune #5 barely changed the other values but reduced the number of mesh elements by about 11.15%, while keeping the overall simulation accuracy. This reduction is important because when there are too many elements, out-of-memory errors may happen (see next experiment). In Figure <ref type="figure" target="#fig_13">14</ref>, we plot the evolution of these variables over time and annotate the tunings (Tune #1 to Tune #6) so the user can evaluate the adaptations, addressing Challenge 3. Based on the adaptation-aware online data analyses, the user can evaluate decide whether or not new tunes are needed, also supporting the Challenge 3. Moreover, suppose a scenario where another research team analyzes the provenance of the results. The team sees abrupt changes in the results and can correlate these results with Bob's adaptation through SQL queries in the provenance database. They can check if sudden changes are related to one of the adaptations Bob did. Thus, they will have a better understanding of the results, thus  addressing Challenges 2 and 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">Large-scale experiment</head><p>In this experiment, the user sets up the libMesh-sedimentation workflow with a simulation of the deposition of sediments carried by a turbidity current over a real experimental channel. A mixture of sediments is continuously injected into a channel that deposits sediments in the tank. The tank has length = 135, width = 40, and height = 50 (dimensionless units).</p><p>The dimensionless simulation parameters are settling velocity = 5.36Ã10 -6 , Grashof number = 3.42Ã10 7 , Schmidt number = 1.0, and fixed time step = 0.01. It uses a 3D simulation with a spatial discretization using an initial unstructured mesh with 1.2 million tetrahedra. AMR/C is employed and three levels of uniform refinement are applied before the time loop. The user specifies input parameter values for the sedimentation solver (i.e., linear and non-linear tolerances, maximum number of linear iterations, tolerances for AMR/C error estimation and refinement and coarsening fractions) aiming at attaining a highfidelity simulation. One strategic simulation data that quantifies such level of detail is the number of elements obtained in the mesh refinement data transformation (second one in the time loop). Although a large number of elements in the mesh means high level of details, it also means more memory and time consumed by the simulation. Depending on the parameter values specified for the solver, the simulation might run out of memory. Thus, the user does not know beforehand which range of parameters is best for a good level of detail with acceptable memory consumption.</p><p>To support the user in following the evolution of strategic values, we use the approach presented in <ref type="bibr" target="#b14">[15]</ref> to monitor results using provenance and domain data. We set up several monitoring queries to plot simulation data at each time step. One query shows linear and nonlinear iterations, residual norms, and the number of elements in the mesh at each time step. Additionally, ParaView Catalyst is set up to plot 3D visualizations of the channel and the sediment deposits over time. Then, the user sees, for example, that the number of elements generated by the AMR/C is close to a maximum preset number of elements. At that rate, the simulation may crash, running out of memory. The user knows that by changing some of the solver parameters, the number of elements tend to decrease. Thus, the user issues a command to adapt the solver parameters and DfAdapter automatically tracks and registers this tuning.</p><p>In Figure <ref type="figure" target="#fig_0">15</ref>, we show the plot of the monitoring query for the number of elements. We see how the number is increasing when the user decided to fine-tune the input parameters online aiming at reducing the number of elements. This action prevented the simulation to result in an out-of-memory error, which would interrupt the simulation, requiring offline tunings and job resubmission to the HPC machine.</p><p>In Figure <ref type="figure" target="#fig_0">16</ref>, we show the 3D visualizations and the evolution of the strategic values and how the sediments flow in the channel over time. Then, the user can run analytical queries to analyze the consequences of the fine-tunings, like Queries 1 and 2. In Table <ref type="table" target="#tab_8">1</ref>, we show a small excerpt of these results, where we can see that the simulation time is cut down to 17 days, thanks to the fine-tunings. If we consider the average solver time by iteration before the fine-tunings, the simulation time would be approximately 27 days, i.e., a reduction of 37%. Analyzing the computational time in details. We use the concepts and equations presented in Section 5.7 to analyze computational time of libMesh-sedimentation in this experiment and added overhead due to provenance capture, data extraction, and steering capabilities. Results are in Table <ref type="table" target="#tab_10">2</ref>. To obtain them, we first calculate each overhead component per task applying the Equations 1 to 4 using DfAdapter's logging data joining with tasks' performance data in the provenance database. Finally, we sum each contribution to the overall computational time as in Equation <ref type="formula" target="#formula_6">5</ref>.</p><p>For monitoring, provenance-tracking overhead account for 0.3% caused by preparing the tuples to be sent to the provenance server. libMesh-sedimentation workflow has a steering point in the beginning of the time loop iteration. Raw data extractors extract convergence values from raw data files written as XDMF/HDF5 so the user can monitor and detect possible misbehavior of nonlinear and linear solvers. In total, these raw data extractions account for 1.49% of the total computation time. For steering, since libMesh-sedimentation uses a file-based checks implementation, it verifies if a file has been modified at each new time iteration. This file verification is synchronous because the simulation code must verify if a change has happened before it can continue. In total, this check at each new iteration adds 0.03% overhead. When a steering action happens, the internal data structure that contains the solver parameters is reloaded and steering data are tracked and sent to the provenance database. Since the user steered 6 times during execution of this workflow, the overhead for steering action tracking is close to 0%.</p><p>Such reduced overhead is due to our system design principles related to asynchronicity and to the fact that the most costly data tracking operations occur in a separate node. Also, because libMesh-sedimentation tasks are seconds-long on average (Figure <ref type="figure" target="#fig_14">13</ref>), the distributed CPUs spend significantly more time computing the application tasks than computing provenance or steering functions. Therefore, considering approximately 17 days (about 1.4Ã10 6 s) of total execution time, provenance and steering tracking together account for less than 1% overhead, whereas summing with raw data extractions, the total overhead is less than 2%. Any overhead caused by this solution is greatly compensated by the benefits we make available to the user. For example, keeping the registry of the adaptations related to the provenance of the results benefits reproducibility, validation, and interpretation (Challenge 2). Also, observing at runtime that the adaptation reduced the execution time in ten days (Challenge 3) is relevant for further online tunings and result analyses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Conclusion</head><p>In this paper, we proposed a solution for keeping track of user steering actions in dynamic workflows. We provided a formal definition for steering action and the tracking of parameter tuning in dataflows of workflows. We extended a W3C PROV provenance model for data representation of fine-tuning of parameters, which is very frequent steering action available in several computational steering systems. We also presented DfAdapter, a tool to facilitate scientists to fine-tune parameters online while managing provenance of steering actions for the tunes. DfAdapter works in the same way as visualization libraries like ParaView are used in workflows. Strategic calls to DfAdapter tracking services are inserted at the adaptation service invocations of the user workflow. DfAdapter captures provenance of steering actions and stores in its database, relating with data for workflow execution, dataflow provenance, and especially with strategic domain values, like QoI. The database is available for online data analyses via structured query or graphic interfaces.</p><p>We developed a case study of DfAdapter using a real sedimentation simulation dynamic workflow in the Oil and Gas industry, using large and small-scale experiments. By using data captured by DfAdapter, the user could verify which parameters contributed to a reduction of simulation time. Also, the steering data registry enabled the user to verify that tuning specific parameters made it finish successfully. The user could run, for example, Query 2, which integrates data for provenance, performance, and the new fine-tuning data introduced in this paper. The user was able to perform steering actions based on the analysis of the impact of each previous tune as registered by DfAdapter. Without DfAdapter support, fine-tuning could be error prone and compromise the reliability of the results. We also observed that the added overhead for DfAdapter for provenance and steering accounted for less than 1% of total simulation time. Therefore, in this work we contributed to provenance management and online analysis of user steering actions in the context of putting humans in the loop of dynamic workflows, which is considered a challenge. Thus, in addition to typical uses of provenance data (i.e., result reproducibility, reliability, and validation), we exploit them for online data analysis, supporting users in their decision-making process. In future work, we plan to explore other types of steering actions in workflows.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Keeping track of user steering in the libMesh-sedimentation (adapted from previous work [18]).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>int t_step = init_tstep; (t_step &lt; n_time_steps) &amp;&amp; (time &lt; tmax); t_step++) { parameters = reload_parameters(); provenance.initTimeIteration(); ... for (unsigned int nonlinear_step = 0, AMR parameters ) ( maximum time, flow linear tolerance, flow non-linear tolerance) ( final linear residual, final non-linear residual ) ( final linear residual, final non-linear residual ) ( before active elements, after active elements )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>1</head><label></label><figDesc>Identify a workflow in the user code Before execution 2 Add monitoring points in the code 3 Add steering points in the code 4</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Methodology for workflow steering.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. DfAdapter system architecture.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Sequence diagram for the track of steering actions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 .</head><label>7</label><figDesc>Figure 7. PROV-DfA [3] with Parameter Tuning classes.</figDesc><graphic coords="17,85.05,475.10,437.00,174.25" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 .</head><label>8</label><figDesc>Figure 8. Excerpt of the database schema.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 9 .</head><label>9</label><figDesc>Figure 9. User steering the dataflow in Montage workflow.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 10 .</head><label>10</label><figDesc>Figure 10. User steering the dataflow in sedimentation simulation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head></head><label></label><figDesc>%&amp;'(%)*+_,('(-., 01*2 1)+ %*1, 10 56</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 11 .</head><label>11</label><figDesc>Figure 11. 2D visualization of the tank and the concentration of sediments. This figure was generated at simulation time t = 10 using ParaView.</figDesc><graphic coords="26,90.45,38.11,500.86,123.98" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 14 .</head><label>14</label><figDesc>Figure 14. Plots of monitoring queries for number of GMRES iterations, non-linear iterations, and mesh elements over time. We highlight the tune actions.</figDesc><graphic coords="27,85.05,292.85,441.90,312.25" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 13 .</head><label>13</label><figDesc>Figure 13. Query 2 results.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 15 .Figure 16 .Flow</head><label>1516</label><figDesc>Figure 15. Plot of monitoring query showing number of elements over time. Steering at t = 33.52 TIME NUMBER OF ELEMENTS (in millions)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Definition 1: Dataset, data elements, and data values. A</head><label></label><figDesc>dataset ð·ð is composed of data elements, i.e., ð·ð = {ð &amp; , â¦ , ð ) }. Each data element ð , , 1 â¤ ð â¤ ð, is composed of data values, i.e., ð , = {ð£ &amp; , â¦ , ð£ 3 }. Datasets are further specialized into Input Datasets (ð¼ 56 ) and Output Datasets (ð 56 ).</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Definition 2: Data schema and attributes. Data</head><label></label><figDesc>elements in a dataset ð·ð have a data schema ð¢(ð·ð) = {ð &amp; , â¦ , ð 3 }, where each element data value ð£ &lt; has an attribute ð &lt; , 1 â¤ ð â¤ ð¢. Thus, an element data value can also be represented as a set of ordered pairs {(ðð¡ð¡ðððð¢ð¡ð, ð£ððð¢ð)}, s.t., ð , = {(ð &amp; , ð£ &amp; ), â¦ , (ð 3 , ð£ 3 )}. Moreover, attributes have a data type (e.g., numeric, textual, array, etc.).</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Definition 3: Data transformation. A</head><label></label><figDesc>data transformation is characterized by the consumption of one or more input data sets ð¼ 56 and the production of one or more output data sets ð 56 . A data transformation is represented by ð·ð, where ð 56 = ð·ð(ð¼ 56 ).</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Definition 4: Data dependency. Let</head><label></label><figDesc>ð·ð D and ð·ð E be data transformations and let {ð} â ð·ð be a set of data elements produced in an output dataset ð·ð generated by ð·ð D . If ð·ð E consumes {ð}, then ð·ð is also an input dataset of ð·ð E . In this case, there is a data dependency between ð·ð D and ð·ð E through {ð} â ð·ð. A data dependency is represented as j = ({ð}, ð·ð D , ð·ð E ).</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>the running workflow notices that an</figDesc><table><row><cell></cell><cell>User side</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Server side</cell></row><row><cell cols="3">Steering command interface Steering data tracker API Adapter Service (front end)</cell><cell cols="3">Provenance Server Monitoring data tracker Steering data tracker Adapter service data communication</cell><cell cols="2">Provenance API Monitoring data tracker API Steering data tracker API</cell><cell>Workflow runningâ¦ Monitoring points Steering points</cell></row><row><cell cols="3">Query Interfaces and Dashboards</cell><cell></cell><cell cols="2">Provenance Database</cell><cell>Adapter Service (back end)</cell></row><row><cell>User</cell><cell cols="2">Steering command interface</cell><cell></cell><cell>Adapter service (front end)</cell><cell>Adapter service (back end)</cell><cell>Running Workflow</cell><cell>Provenance Server</cell><cell>Provenance Database</cell></row><row><cell></cell><cell>2. steering command</cell><cell cols="2">3. Call adapter</cell><cell cols="2">intention 4. initiate steering</cell><cell cols="2">0. monitoring data tracking</cell><cell>1. store monitoring provenance</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>6. Communicate adaptation</cell><cell></cell><cell>7. adapt</cell><cell>5. store steering provenance</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>8. notice adaptation</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">9. steering data tracking</cell><cell>10. store steering provenance</cell></row><row><cell></cell><cell cols="2">11. steering action and dataflow analysis</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Algorithm 1: DfAdapter using the Tune operator. Input: ð¼</head><label></label><figDesc></figDesc><table><row><cell></cell><cell>if p â keys(ð) then</cell></row><row><cell>11.</cell><cell>ð[p] â current_value</cell></row><row><cell>12.</cell><cell>if p â attribute_semantics[ð¿ K ] and ð = â then //tuning a loop attribute. Get iteration data</cell></row><row><cell>13.</cell><cell>ð â prov.get_current_iteration_data(ð¼ 56 )</cell></row><row><cell>14.</cell><cell>end if</cell></row><row><cell>15.</cell><cell>end if</cell></row><row><cell>16.</cell><cell></cell></row></table><note><p>56 : The ð¼ 56 in the dataflow containing the parameters to be tuned. ð: key-value data structure containing the parameters and their new values. ð¶: (optional) criteria to specify the data element that will be adapted. 1. prov â get_provenance_server() //programming interface to the Provenance Server 2. ð â â 3. ð â â 4. T â prov.get_running_execution_data() 5. ð â prov.get_user() 6. ð¡ â get_current_wall_time() 7. current_data_element â prov.get_element(ð¼ 56 , ð¶) //if C is null, get the only data element in ð¼ 5c5 8. attribute_semantics â prov.get_semantics(ð¼ 56 ) 9. for all key-value pairs (p, current_value) in current_data_element do 10.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>end for 17</head><label></label><figDesc>. prov.store_steering_data(ð¼ 56 , ð, ð¶, ð, T, ð, ð¡, ð)</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Command lines to call DfAdapter.</head><label></label><figDesc></figDesc><table><row><cell cols="2">1. ... 2. for (unsigned int t_step = p.init_tstep; (t_step &lt; p.n_time_steps) &amp;&amp; (time &lt; p.tmax); t_step++) {</cell></row><row><cell>3.</cell><cell>provenance.initTimeLoop();</cell></row><row><cell>4.</cell><cell>if ( parameters_modified() ) {</cell></row><row><cell>5.</cell><cell>p = reload_parameters();</cell></row><row><cell>6.</cell><cell>provenance.steeringTimeLoop();</cell></row><row><cell>7.</cell><cell>}</cell></row><row><cell>8.</cell><cell>...</cell></row><row><cell>9.</cell><cell>for (unsigned int nonlinear_step = 0; nonlinear_step &lt; p.max_nonlinear_steps; ++nonlinear_step) {</cell></row><row><cell>10.</cell><cell>provenance.initFluidSolver();</cell></row><row><cell>11.</cell><cell>flow_system.solve();</cell></row><row><cell>12.</cell><cell>...</cell></row><row><cell>13.</cell><cell>provenance.finalizeFluidSolver();</cell></row><row><cell>14.</cell><cell>}</cell></row><row><cell>15.</cell><cell>...</cell></row><row><cell>16.</cell><cell>provenance.finalizeTimeLoop();</cell></row><row><cell>17. }</cell><cell></cell></row><row><cell>18. ...</cell><cell></cell></row><row><cell></cell><cell>Figure 5. Excerpt of libMesh-sedimentation code with provenance and steering calls.</cell></row><row><cell></cell><cell>1. $&gt; ./DfAdapter --user='Bob'</cell></row><row><cell></cell><cell>2. $&gt; ./DfAdapter --tune</cell></row><row><cell></cell><cell>--dataset='I_Iteration_Params'</cell></row><row><cell></cell><cell>--p='{"max_linear_iterations":500}'</cell></row><row><cell></cell><cell>--reason="checking how linear iterations affects</cell></row><row><cell></cell><cell>convergence"</cell></row><row><cell></cell><cell>3. $&gt; echo '{</cell></row><row><cell></cell><cell>"flow_initial_linear_solver_tolerance": 1.0e-6,</cell></row><row><cell></cell><cell>"amr/c_fraction": 1.0e-5</cell></row><row><cell></cell><cell>}' &gt; new-values.json</cell></row><row><cell></cell><cell>4. $&gt; ./DfAdapter --tune</cell></row><row><cell></cell><cell>--dataset='I_Iteration_Params'</cell></row><row><cell></cell><cell>--p='new-values.json'</cell></row><row><cell></cell><cell>Figure 6.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Query 1 :</head><label>1</label><figDesc>List all user tunings correlating with time step. This query does a join on tables: ParameterTuning, ParameterTuned, InfluencedDataElement, and Attribute, filtering by tunings made by 'Bob'. The result is:</figDesc><table><row><cell>Parameter Tuning</cell><cell>t_step</cell><cell>Parameter Tuned</cell><cell>Old Val</cell><cell>New Val</cell></row><row><cell>1</cell><cell>1401</cell><cell>flow_initial_linear_solver_tolerance</cell><cell>1e-8</cell><cell>1e-6</cell></row><row><cell>2</cell><cell>1474</cell><cell>minimum_linear_solver_tolerance</cell><cell>1e-8</cell><cell>1e-6</cell></row><row><cell>3</cell><cell>1484</cell><cell>flow_initial_linear_solver_tolerance</cell><cell>1e-6</cell><cell>1e-4</cell></row><row><cell>4</cell><cell>1755</cell><cell>max_linear_iterations</cell><cell>500</cell><cell>300</cell></row><row><cell>5</cell><cell>10061</cell><cell>amr/c_fraction</cell><cell>0.01</cell><cell>0.05</cell></row><row><cell>6</cell><cell>10128</cell><cell>max_linear_iterations</cell><cell>300</cell><cell>200</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Figure 12. Query 1 results.</head><label></label><figDesc></figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Query 2 :</head><label>2</label><figDesc>Average of domain values (QoIs) and simulation time estimate 10 iterations before and after the tunings. This query does a join on tables ParameterTuning, ParameterTuned, Attribute, InfluencedDataElement, O_Sedimentation_Solver, O_Fluid_Solver, Task, and Performance. It also does an average on the output values of O_Sedimentation_Solver and O_Fluid_Solver, and on Execution Time in Performance table. The result is:</figDesc><table><row><cell>Parameter Tuning</cell><cell>Avg Time (s) Bef</cell><cell>Avg Time (s) Aft</cell><cell>Avg nonlin. Bef</cell><cell>Avg nonlin Aft</cell><cell>Avg gmres Bef</cell><cell>Avg gmres Aft</cell><cell>Avg Elems Bef</cell><cell>Avg Elems Aft</cell></row><row><cell>1</cell><cell>17.3</cell><cell>18.5</cell><cell>3.8</cell><cell>3.9</cell><cell>2.03e3</cell><cell>2e3</cell><cell>5.32e3</cell><cell>5.38e3</cell></row><row><cell>2</cell><cell>16.9</cell><cell>18.1</cell><cell>4.1</cell><cell>4.3</cell><cell>2.05e3</cell><cell>2.03e3</cell><cell>5.44e3</cell><cell>5.41e3</cell></row><row><cell>3</cell><cell>17.4</cell><cell>13.2</cell><cell>4.2</cell><cell>4.3</cell><cell>2.02e3</cell><cell>1.54e3</cell><cell>5.45e3</cell><cell>5.43e3</cell></row><row><cell>4</cell><cell>12.7</cell><cell>9.6</cell><cell>3.9</cell><cell>4.2</cell><cell>1.49e3</cell><cell>1.01e3</cell><cell>5.51e3</cell><cell>5.49e3</cell></row><row><cell>5</cell><cell>14.4</cell><cell>14.8</cell><cell>4.3</cell><cell>4.0</cell><cell>1.06e3</cell><cell>1.01e3</cell><cell>6.28e3</cell><cell>5.58e3</cell></row><row><cell>6</cell><cell>15.6</cell><cell>11.2</cell><cell>4.05</cell><cell>4.1</cell><cell>647</cell><cell>445</cell><cell>5.72e3</cell><cell>5.62e3</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 1 . Results of parameter-tuning.</head><label>1</label><figDesc></figDesc><table><row><cell></cell><cell>Before</cell><cell>After</cell><cell>Reduction</cell></row><row><cell>Avg. Solver Time by iteration</cell><cell>3.82 min</cell><cell>2.21 min</cell><cell>42.14%</cell></row><row><cell>Avg. Number of elements</cell><cell>2.4x10 6</cell><cell>1.7x10 6</cell><cell>29.24%</cell></row><row><cell>Total execution time</cell><cell>(expected) ~27 days</cell><cell>(real) ~17 days</cell><cell>37%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 2 . Provenance and steering overhead account for less than 1%, whereas data extraction account for 1.49% overhead. Total CPU time (sec) Total time (%)</head><label>2</label><figDesc></figDesc><table><row><cell></cell><cell>Application</cell><cell></cell><cell></cell></row><row><cell></cell><cell>computation</cell><cell>1,407,967.18</cell><cell>98.18%</cell></row><row><cell></cell><cell>ðððð(ð·ð)</cell><cell></cell><cell></cell></row><row><cell>Monitoring</cell><cell>Provenance ðððð£(ð·ð) Data extraction ðð¥ð¡(ð·ð)</cell><cell>4,259.18 21,367.60</cell><cell>0.3% 1.49%</cell></row><row><cell>Steering</cell><cell>Steering point ð  rO,st (ð·ð) Steering action ð  uvt,Os (ð·ð)</cell><cell>473.24 2.44</cell><cell>0.03% 1.7e-5%</cell></row><row><cell></cell><cell>Total ð(ð·ð)</cell><cell>1,434,069.64</cell><cell>100%</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>â  https://www.monetdb.org</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>ACKNOWLEDGMENTS</head><p>This work was partially funded by <rs type="funder">CAPES</rs>, <rs type="funder">CNPq</rs>, <rs type="funder">FAPERJ</rs> and <rs type="funder">Inria</rs> (<rs type="projectName">MUSIC</rs> and <rs type="projectName">SciDISC</rs> projects), <rs type="funder">EU</rs> <rs type="programName">H2020 Programme</rs> and <rs type="funder">MCTI/RNP-Brazil</rs> (<rs type="grantName">HPC4E</rs> grant no. <rs type="grantNumber">689772</rs>), and performed (for <rs type="person">P. Valduriez</rs>) in the context of the <rs type="institution">Computational Biology Institute</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_pEBTvhr">
					<orgName type="project" subtype="full">MUSIC</orgName>
				</org>
				<org type="funded-project" xml:id="_UR2RCy7">
					<orgName type="project" subtype="full">SciDISC</orgName>
					<orgName type="program" subtype="full">H2020 Programme</orgName>
				</org>
				<org type="funding" xml:id="_KJ9jNpC">
					<idno type="grant-number">689772</idno>
					<orgName type="grant-name">HPC4E</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Dynamic steering of HPC scientific workflows: a survey</title>
		<author>
			<persName><forename type="first">M</forename><surname>Mattoso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">A C S</forename><surname>OcaÃ±a</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Ogasawara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Costa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Horta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>De Oliveira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Future Generation Computer Systems</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="page" from="100" to="113" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">In situ methods, infrastructures, and applications on high performance computing platforms</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">H</forename><surname>Ahrens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Childs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Geveci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Klasky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Moreland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>O'leary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vishwanath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="577" to="597" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Provenance of dynamic adaptations in user-steered dataflows</title>
		<author>
			<persName><forename type="first">R</forename><surname>Souza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mattoso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Provenance and Annotation Workshop</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="16" to="29" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">WorkWays: interacting with scientific workflows</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">A</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Abramson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kipouros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Janke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Galloway</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Concurrency and Computation: Practice &amp; Experience</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="4377" to="4397" />
			<date type="published" when="2015-11">2015. Nov.</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A survey of computational steering environments</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Mulder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Van Wijk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Van Liere</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Future Generation Computer Systems</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="119" to="129" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">R</forename><surname>Van Liere</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Mulder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Van Wijk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational steering, Future Generation Computer Systems</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="441" to="450" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Utility functions for adaptively executing concurrent workflows</title>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">W</forename><surname>Paton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sakellariou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A A</forename><surname>Fernandes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Concurrency and Computation: Practice &amp; Experience</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="646" to="666" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">OpenMOLE, a workflow engine specifically tailored for the distributed exploration of simulation models</title>
		<author>
			<persName><forename type="first">R</forename><surname>Reuillon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Leclaire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Rey-Coyrehourcq</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Future Generation Computer Systems</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1981" to="1990" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">FireWorks: a dynamic workflow system designed for high-throughput applications</title>
		<author>
			<persName><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">P</forename><surname>Ong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Medasani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kocher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Brafman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Petretto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G.-M</forename><surname>Rignanese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Concurrency and Computation: Practice &amp; Experience</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">17</biblScope>
			<biblScope unit="page" from="5037" to="5059" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Copernicus, a hybrid dataflow and peer-to-peer scientific computing platform for efficient large-scale ensemble sampling</title>
		<author>
			<persName><forename type="first">I</forename><surname>Pouya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Pronk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lundborg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Lindahl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Future Generation Computer Systems</title>
		<imprint>
			<biblScope unit="volume">71</biblScope>
			<biblScope unit="page" from="18" to="31" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Towards a human-in-the-loop library for tracking hyperparameter tuning in deep learning development</title>
		<author>
			<persName><forename type="first">R</forename><surname>Souza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Neves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Azeredo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Luiz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Tady</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Cavalin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mattoso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">VLDB Workshops: Latin American Data Science</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="84" to="87" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Using HPC Software Frameworks for Developing BSIT: A Geophysical Imaging Tool</title>
		<author>
			<persName><forename type="first">M</forename><surname>Hanzich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Rodriguez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Gutierrez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Puente</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Cela</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">World Congress on Computational Mechanics</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="181" to="189" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">The future of scientific workflows</title>
		<author>
			<persName><forename type="first">E</forename><surname>Deelman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Peterka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Altintas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Carothers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kleese Van Dam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Moreland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Parashar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ramakrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Taufer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of HPC Applications</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="159" to="175" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A characterization of workflow management systems for extreme-scale applications</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">F</forename><surname>Da Silva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Filgueira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Pietri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sakellariou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Deelman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Future Generation Computer Systems</title>
		<imprint>
			<biblScope unit="volume">75</biblScope>
			<biblScope unit="page" from="228" to="238" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Data reduction in scientific workflows using provenance monitoring and user steering</title>
		<author>
			<persName><forename type="first">R</forename><surname>Souza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L G A</forename><surname>Coutinho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Valduriez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mattoso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Future Generation Computer Systems</title>
		<imprint>
			<biblScope unit="page" from="1" to="34" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Raw data queries during data-intensive parallel workflow execution</title>
		<author>
			<persName><forename type="first">V</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leite</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Camata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Oliveira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L G A</forename><surname>Coutinho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Valduriez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mattoso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Future Generation Computer Systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Tracking of online parameter finetuning in scientific workflows</title>
		<author>
			<persName><forename type="first">R</forename><surname>Souza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Camata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Coutinho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Valduriez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mattoso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM/IEEE Supercomputing workshops: Workflows in Support of Large-Scale Science</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<author>
			<persName><forename type="first">J</forename><surname>Camata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Valduriez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mattoso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L G A</forename><surname>Coutinho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">situ visualization and data analysis for turbidity currents simulation</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">110</biblScope>
			<biblScope unit="page" from="23" to="31" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<ptr target="https://github.com/hpcdb/workflow-sedimentation" />
		<title level="m">Workflow Sedimentation GitHub Repository</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">libMesh : a C++ library for parallel adaptive mesh refinement/coarsening simulations</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">S</forename><surname>Kirk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Peterson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">H</forename><surname>Stogner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">F</forename><surname>Carey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Engineering with Computers</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">3-4</biblScope>
			<biblScope unit="page" from="237" to="254" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">ParaView Catalyst: enabling in situ data analysis and visualization</title>
		<author>
			<persName><forename type="first">U</forename><surname>Ayachit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Geveci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>O'leary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Moreland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Fabian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mauldin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Situ Infrastructures for Enabling Extreme-scale Analysis and Visualization</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="25" to="29" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Capturing provenance for runtime data analysis in computational science and engineering applications</title>
		<author>
			<persName><forename type="first">V</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Souza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Camata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L G A</forename><surname>Coutinho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Valduriez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mattoso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Provenance and Annotation Workshop</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="183" to="187" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Scientific workflows: Past, present and future</title>
		<author>
			<persName><forename type="first">M</forename><surname>Atkinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gesing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Montagnat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Future Generation Computer Systems</title>
		<imprint>
			<biblScope unit="volume">75</biblScope>
			<biblScope unit="page" from="216" to="227" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">JobPruner: A machine learning assistant for exploring parameter spaces in HPC applications</title>
		<author>
			<persName><forename type="first">B</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A S</forename><surname>Netto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L F</forename><surname>Cunha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Future Generation Computer Systems</title>
		<imprint>
			<biblScope unit="volume">83</biblScope>
			<biblScope unit="page" from="144" to="157" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">SCIRun: a scientific programming environment for computational steering</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">G</forename><surname>Parker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">R</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM/IEEE conference on Supercomputing</title>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="52" to="71" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Cumulvs: Interacting with high-performance scientific simulations for visualization, steering and fault tolerance</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Kohl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wilde</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Bernholdt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The International Journal of High Performance Computing Applications</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="255" to="285" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">GRASPARC: a problem solving environment integrating computation and visualization</title>
		<author>
			<persName><forename type="first">K</forename><surname>Brodlie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Poon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Brankin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Banecki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Visualization</title>
		<imprint>
			<date type="published" when="1993">1993</date>
			<biblScope unit="page" from="102" to="109" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">The Cactus Framework and Toolkit: Design and Applications</title>
		<author>
			<persName><forename type="first">T</forename><surname>Goodale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Allen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Lanfermann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>MassÃ³</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Radke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Seidel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shalf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th International Conference on High Performance Computing for Computational Science</title>
		<meeting>the 5th International Conference on High Performance Computing for Computational Science</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="197" to="227" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A practical toolkit for computational steering</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Pickles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Haines</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Pinning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Porter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Philosophical Transactions. Series A, Mathematical, Physical, and Engineering Sciences</title>
		<imprint>
			<biblScope unit="volume">363</biblScope>
			<biblScope unit="page" from="1843" to="1853" />
			<date type="published" when="2005-08">2005. Aug.</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">CS_LITE: A lightweight computational steering system</title>
		<author>
			<persName><forename type="first">S</forename><surname>Figueira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Parallel and Distributed Computing and Networks</title>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Interactive computing framework for engineering applications</title>
		<author>
			<persName><forename type="first">J</forename><surname>Knezevic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Frisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R.-P</forename><surname>Mundani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Rank</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computer Science</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">591</biblScope>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Toward a computational steering environment based on CORBA</title>
		<author>
			<persName><forename type="first">O</forename><surname>Coulaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dussere</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Esnard</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="151" to="158" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">System Design and Algorithmic Development for Computational Steering in Distributed Environments</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">S V</forename><surname>Rao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Parallel Distrib. Syst</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="438" to="451" />
			<date type="published" when="2010-04">2010. Apr.</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Computational steering of complex flow simulations</title>
		<author>
			<persName><forename type="first">A</forename><surname>Atanasov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-J</forename><surname>Bungartz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Frisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mehl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R.-P</forename><surname>Mundani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Rank</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Van Treeck</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2010. 2009</date>
			<biblScope unit="page" from="63" to="74" />
			<pubPlace>Garching/Munich</pubPlace>
		</imprint>
		<respStmt>
			<orgName>High Performance Computing in Science and Engineering</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Live Programming in scientific simulation</title>
		<author>
			<persName><forename type="first">B</forename><surname>Swift</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sorensen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Decyk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Supercomputing Frontiers and Innovations: an International Journal</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="4" to="15" />
			<date type="published" when="2015-03">2015. Mar.</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Computational steering of CFD simulations using a grid computing environment</title>
		<author>
			<persName><forename type="first">M</forename><surname>GarcÃ­a</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Duque</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Boulanger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Figueroa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal on Interactive Design and Manufacturing (IJIDeM)</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="235" to="245" />
			<date type="published" when="2015-08">2015. Aug.</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Pegasus, a workflow management system for science automation</title>
		<author>
			<persName><forename type="first">E</forename><surname>Deelman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Vahi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Juve</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rynge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Callaghan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Maechling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mayani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ferreira Da Silva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Future Generation Computer Systems</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="page" from="17" to="35" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Data-centric iteration in dynamic workflows</title>
		<author>
			<persName><forename type="first">J</forename><surname>Dias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Guerra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Rochinha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L G A</forename><surname>Coutinho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Valduriez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mattoso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Future Generation Computer Systems</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">C</biblScope>
			<biblScope unit="page" from="114" to="126" />
			<date type="published" when="2015-05">2015. May.</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Nimrod/K: Towards massively parallel dynamic grid workflows</title>
		<author>
			<persName><forename type="first">D</forename><surname>Abramson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Enticott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Altinas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Supercomputing</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="1" to="24" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Wings: intelligent workflow-based design of computational experiments</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Gil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Ratnakar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Gonzalez-Calero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Groth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Moody</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Deelman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Intelligent Systems</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="62" to="72" />
			<date type="published" when="2011-01">2011. Jan.</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Logical provenance in data-oriented workflows?</title>
		<author>
			<persName><forename type="first">R</forename><surname>Ikeda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Das</forename><surname>Sarma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Widom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Data Engineering</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="877" to="888" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">DfAdapter Repository</title>
		<ptr target="https://github.com/hpcdb/DfAdapter" />
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>DfAdapter GitHub</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">PROV-DM: The PROV Data Model</title>
		<author>
			<persName><forename type="first">L</forename><surname>Moreau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Missier</surname></persName>
		</author>
		<ptr target="https://www.w3.org/TR/prov-dm/" />
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Many-Task Computing for Grids and Supercomputers</title>
		<author>
			<persName><forename type="first">I</forename><surname>Raicu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">T</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM/IEEE Supercomputing workshops: Many-Task Computing on Grids and Supercomputers</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Montage: a grid portal and software toolkit for science-grade astronomical image mosaicking</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Jacob</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Katz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">B</forename><surname>Berriman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Good</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Laity</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Deelman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kesselman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-H</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computational Science and Engineering (IJCSE)</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="73" to="87" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Time-and space-resolved measurements of deposition under turbidity currents</title>
		<author>
			<persName><forename type="first">F</forename><surname>De Rooij</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">B</forename><surname>Dalziel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Particulate Gravity Currents</title>
		<editor>
			<persName><forename type="first">W</forename><surname>Mccaffrey</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><surname>Kneller</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Peakall</surname></persName>
		</editor>
		<meeting><address><addrLine>Oxford, UK</addrLine></address></meeting>
		<imprint>
			<publisher>Blackwell Publishing Ltd</publisher>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="207" to="215" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
