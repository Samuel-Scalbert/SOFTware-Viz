<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Parallel Techniques for Variable Size Segmentation of Time Series Datasets</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Lamia</forename><surname>Djebour</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Inria</orgName>
								<orgName type="institution" key="instit2">University of Montpellier</orgName>
								<orgName type="institution" key="instit3">CNRS</orgName>
								<orgName type="institution" key="instit4">LIRMM</orgName>
								<address>
									<settlement>Montpellier</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Reza</forename><surname>Akbarinia</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Inria</orgName>
								<orgName type="institution" key="instit2">University of Montpellier</orgName>
								<orgName type="institution" key="instit3">CNRS</orgName>
								<orgName type="institution" key="instit4">LIRMM</orgName>
								<address>
									<settlement>Montpellier</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Florent</forename><surname>Masseglia</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Inria</orgName>
								<orgName type="institution" key="instit2">University of Montpellier</orgName>
								<orgName type="institution" key="instit3">CNRS</orgName>
								<orgName type="institution" key="instit4">LIRMM</orgName>
								<address>
									<settlement>Montpellier</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Parallel Techniques for Variable Size Segmentation of Time Series Datasets</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">08AF8E0E63C3B720E0775F1C13D4F64A</idno>
					<idno type="DOI">10.1007/978-3-031-15740-0_12</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-04-12T14:49+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Time Series</term>
					<term>Representations</term>
					<term>Information Retrieval</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Given the high data volumes in time series applications, or simply the need for fast response times, it is usually necessary to rely on alternative, shorter representations of these series, usually with loss. This incurs approximate comparisons of time series where precision is a major issue.In this paper, we propose a new parallel approach for segmenting time series before their transformation into symbolic representations. It can reduce significantly the error incurred by possible splittings at different steps of the representation calculation, by taking into account the sum of squared errors (SSE). This is particularly useful for time series similarity search, which is the core of many data analytics tasks. We provide theoretical guarantees on the lower bound of similarity measures, and our experiments illustrate that our technique can improve significantly the time series representation quality.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Time series have attracted an increasing interest due to their wide applications in many domains. The continuous flow of emitted data may concern personal activities (e.g., through smart-meters or smart-plugs for electricity or water consumption) or professional activities (e.g., for monitoring heart activity or through the sensors installed on plants by farmers). This results in the production of large and complex data, usually in the form of time series <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b1">2]</ref> that challenges knowledge discovery.</p><p>As a consequence of the high data volumes in such applications, similarity search can be slow on raw data. One of the issues that hinder the analysis of such data is the high dimensionality. This is why time series approximation is often used as a means to allow fast similarity search. SAX <ref type="bibr" target="#b7">[8]</ref> is one of the most popular time series representations, allowing dimensionality reduction on the classic data mining tasks. SAX constructs symbolic representations by splitting the time domain into segments of equal size where the mean values of segments represent the time series intervals (PAA approach). This approximation technique is effective for time series having a uniform and balanced distribution over the time domain. However, we observe that, in the case of time series having high variation over given time intervals, this division into segments of fixed length is not efficient. Our main contribution is to provide an adaptive interval distribution, rather than an equal distribution in time. However, the number of possible segmentations of k segments with n can be very high. Furthermore, when searching for the best variable-size segmentation, a large number of computation is involved in case of large sets of time series. Therefore, we propose efficient parallel techniques using GPUs for improving the execution time of our segmentation algorithm. In this paper, we make the following contributions:</p><p>-We propose a new representation technique, called ASAX SSE, that allows obtaining a variable-size segmentation of time series with better precision in retrieval tasks thanks to its lower information loss. Our representation is based on SSE measurement for detecting what time intervals should be split. -We propose a lower bounding method that allows approximating the distance between the original time series based on their representations in ASAX SSE. -We propose efficient parallel algorithms for improving the execution time of our segmentation approach using GPUs. -We implemented our approach and conducted empirical experiments using more than 120 real world datasets. The results suggest that ASAX SSE can obtain significant performance gains in terms of precision for similarity search compared to SAX. They illustrate that the more the data distribution in the time domain is unbalanced (non-uniform), the greater is the precision gain of ASAX SSE. For example, for the ECGFiveDays dataset that has a non-uniform distribution in the time domain, the precision of ASAX SSE is 93% compared to 55% for SAX.</p><p>The rest of the paper is organized as follows. In Section 2, we define the problem we address. In Section 3, we describe the details of ASAX SSE representation, and in Section 4 we present parallel versions of ASAX SSE. In Section 5, we present the experimental evaluation of our approach. Finally, we discuss the related work in Section 6 and give our conclusion in Section 7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Problem Definition and Background</head><p>We first present the background about SAX representation, and then define the problem we address. A time series X is a sequence of values X = {x 1 ,...,x n }. We assume that every time series has a value at every timestamp t=1,2,...,n. The length of X is denoted by |X|. SAX allows a time series T of length n to be reduced to a string of arbitrary length w.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">SAX Representation</head><p>Given two time series X ={x 1 ,...,x n } and Y ={y 1 ,...,y n }, the Euclidean distance between X and Y is defined as <ref type="bibr" target="#b5">[6]</ref>:</p><formula xml:id="formula_0">ED(X,Y )= n i=1 (x i -y i ) 2 .</formula><p>The Euclidean distance is one of the most popular similarity measurement methods used in time series analysis.</p><p>The SAX representation is based on the PAA representation <ref type="bibr" target="#b7">[8]</ref> which allows for dimensionality reduction while providing the important lower bounding property as we will show later. The idea of PAA is to have a fixed segment size, and minimize dimensionality by using the mean values on each segment. Example 1 gives an illustration of PAA. Fig. <ref type="figure">1:</ref> A time series X is discretized by obtaining a PAA representation and then using predetermined break-points to map the PAA coefficients into SAX symbols.</p><p>Here, the symbols are given in binary notation, where 00 is the first symbol, 01 is the second symbol, etc. The time series of Figure <ref type="figure">1a</ref> in the representation of Figure <ref type="figure">1c</ref> is [first, first, second, fourth] (which becomes [00,00,01,11] in binary).</p><p>Example 1. Figure <ref type="figure">1b</ref> shows the PAA representation of X, the time series of Figure <ref type="figure">1a</ref>. The representation is composed of w =|X|/l values, where l is the segment size. For each segment, the set of values is replaced with their mean. The length of the final representation w is the number of segments (and, usually, w &lt;&lt;|X|).</p><p>By transforming the original time series X and Y into PAA representations X = {x 1 ,...,x w } and Y = {y 1 ,...,y w }, the lower bounding approximation of the Euclidean distance for these two representations can be obtained by:</p><formula xml:id="formula_1">DR f (X,Y )= n w w i=1 (x i -y i ) 2</formula><p>The SAX representation takes as input the reduced time series obtained using PAA. It discretizes this representation into a predefined set of symbols, with a given cardinality, where a symbol is a binary number. Example 2 gives an illustration of the SAX representation.</p><p>Example 2. In Figure <ref type="figure">1c</ref>, we have converted the time series X to SAX representation with size 4, and cardinality 4 using the PAA representation shown in Figure <ref type="figure">1b</ref>. We denote SAX(X) = [00, 00, 01, 11].</p><p>The lower bounding approximation of the Euclidean distance for SAX representation X = {x 1 ,...,x w } and Ŷ = {ŷ 1 ,...,ŷ w } of two time series X and Y is defined as: 2 where the function dist(x i ,ŷ i ) is the distance between two SAX symbols xi and xi . The lower bounding condition is formulated as: MINDIST f ( X, Ŷ )≤ED(X,Y )</p><formula xml:id="formula_2">MINDIST f ( X, Ŷ )= n w w i=1 (dist(x i ,ŷ i ))</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Similarity Queries</head><p>The problem of similarity queries is one of the main problems in time series analysis and mining. In information retrieval, finding the k nearest neighbors (k-NN) of a query is a fundamental problem. Let us define exact and approximate k nearest neighbors. Definition 1. (Exact k nearest neighbors) Given a query time series Q and a set of time series D, let R=ExactkNN(Q,D) be the set of k nearest neighbors of Q from D. Let ED(X,Y ) be the Euclidean distance between two time series X and Y , then the set R is defined as follows:</p><p>(R⊆D)∧(|R|=k)∧(∀a∈R,∀b∈(D-R),ED(a,Q)≤ED(b,Q))</p><p>Definition 2. (Approximate k nearest neighbors) Given a set of time series D, a query time series Q, and ϵ &gt; 0. We say that</p><formula xml:id="formula_3">R = AppkNN(Q,D) is the approximate k nearest neighbors of Q from D, if ED(a,Q)≤(1+ϵ)ED(b,Q),</formula><p>where a is the k th nearest neighbor from R and b is the true k th nearest neighbor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Problem Statement</head><p>The SAX representation proceeds to an approximation by minimizing the dimensionality: the original time series are divided into segments of equal size. This representation does not depend on the time series values, but on their length. It allows SAX to perform the segmentation in O(n) where n is the time series length. However, for a given reduction in dimensionality, the modeling error may not be minimal since the model does not adapt to the information carried by the series.</p><p>Our goal is to propose a variable-size segmentation of the time domain that minimizes the loss of information in the time series representation. Formally, the problem we address is stated as follows. Given a database of time series D and a number w, divide the time domain into w segments of variable size such that the representation of the times series based on that segmentation lowers the error of similarity queries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Adaptive SAX based on the representation's Sum of Squared Errors (ASAX SSE)</head><p>We propose ASAX SSE, a variable-size segmentation technique for time series representation. To create a segmentation with minimum information loss on time series approximation, ASAX SSE divides the time domain based on the Sum of squared errors (SSE) value of the representation.</p><p>In the rest of this section, we first describe the notion of Sum of Squared Errors (SSE) for the time series representation. Then, we describe our algorithm for creating the variable-size segments based on this measurement. Finally, we present our method for measuring the lower bound distance between time series in the proposed representation. This lower bounding is useful for efficient evaluation of kNN queries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Sum of Squared Errors (SSE)</head><p>In Statistics, Sum of Squared Errors (SSE) is defined as the sum of the squares of the errors. In other words, SSE is the sum of the squared differences between the actual and the estimated values. Formally, SSE is defined as follows.</p><p>Definition 3. Given a vector X of n elements and a vector X being the estimated values generated from X, SSE of the estimation is given by: SSE(X, X)=</p><formula xml:id="formula_4">n i=1 (x i -x i ) 2</formula><p>In our context, we calculate the SSE on the PAA representation obtained from the transformation of the original time series of a dataset according to a given segmentation. The SSE computed on this representation allows to measure the approximation error on the time series by the PAA representation compared to the original time series. The lower the SSE, the closer is the PAA representation to original data.</p><p>By transforming a time series X = {x 1 , ..., x n } into a PAA representation X ={x 1 ,...,x w }, X is reduced to the PAA representation composed of w segments. For each segment, the set of values is replaced with their mean. We can compute the SSE for each segment, that is in this case, the sum of the squared differences between each value (actual value) and its segment's mean (estimated value). In the next subsections, we show how to compute the SSE of a PAA representation considering only one segment (called LSSE) or all segments (called GSSE). As shown by experiments, using these two different SSE measurements may lead to different results in terms of precision and execution time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">SSE of PAA Representation Considering One Segment (LSSE)</head><p>Let X be the PAA representation of X with w segments. The LSSE (local SSE) of X for a particular segment is the sum of the squared errors for the time series values in this segment. Formally, LSSE of X for a segment s i is computed as: 2 where s i is the selected segment, LB(s i ) and UB(s i ) are the start and end time points of s i respectively.</p><formula xml:id="formula_5">LSSE(s i ,x i ) = UB(si) j=LB(si) (x j -x i )</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">SSE of PAA Representation Considering All Segments (GSSE)</head><p>The global SSE (GSSE), is computed by taking into account all segments of the PAA representation X: GSSE(X,X)= w i=1 UB(si) j=LB(si) (x j -x i ) 2 where LB(s i ) and UB(s i ) are the start and end time points of the segment s i respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Variable-Size Segmentation Based on SSE Measurement</head><p>Given a database of time series D, and a number w, our goal is to find the k variable size segments that minimize the loss of information in time series representations by minimizing the approximation error of these representations.</p><p>Intuitively, our algorithm works as follows. Based on a starting segment size value size, it firstly splits the time domain into k segments of length size. The default value of size is 2. The algorithm performs k-w iterations, and in each iteration it finds the two adjacent segments s i and s i+1 whose merging gives the minimum SSE (MSSE) on the representations, and merges them. By doing this, in each iteration the two selected segments are merged to form a single segment which replaces them in the set of segments, reducing the number of segments by one. This continues until having w segments.</p><p>Let us now describe our algorithm in more details. The pseudocode is shown in Algorithm 1. It first sets the current number of segments, denoted as k, to n size . Then, it splits the time domain into k segments of length size that are included to the set segments (Line 2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1: variable-size segmentation</head><p>Input: D: time series database; n: the length of time series; size: the starting size of segments; w: the required number of segments Output: w variable-size segments Afterwards, in a loop, until the number of segments is more than w the algorithm proceeds as follows. For each segment s i (i from 1 to k -1), s i is merged with segment s i+1 to form a single segment denoted as s (Line 7). Then, a temporary set of segments tempSegments is created including the new segment and all previously created segments except s i and s i+1 i.e., except the two that have been merged (Lines 8, 9). Then, for each time series ts in the database D, the algorithm generates its PAA representation and calculates the corresponding SSE (Line 13) calling either GSSE function in the case that the entire PAA representation is considered for the error calculation, or LSSE function if the error is computed on segment s. Then, it adds the result of the computed SSE to sse (Line 13). After having calculated the sum of the SSE for the PAA representation of all the time series contained in D, if the SSE is less than the MSSE (minimum SSE) obtained so far, the algorithm sets i as the segment to be merged with the next one, and keeps the SSE of the representation (Lines 15, 16). This procedure continues by trying the merging of every two adjacent segments of segments at each time, and computing the SSE. The algorithm selects the merging whose SSE is the lowest, and updates the set of the segments by removing the selected segments, and inserting its merging to segments (Lines 17-19). Then, k, which stands for the number of current segments, is decremented by one (Line 20). The algorithm ends when k gets equal to the required number, i.e., w.</p><formula xml:id="formula_6">1 k = ⌈ n size ⌉ 2 segments = { k-1 i=0 [size×i,size×(i+1)-1]} //</formula><p>Let us illustrate the principle of our algorithm using an example. For simplicity, we consider a dataset containing only a single time series and we calculate the approximation error on the entire time series representation using GSSE approach.</p><p>Example 3. Let us apply our algorithm on the time series X in Figure <ref type="figure" target="#fig_2">2</ref> by taking the initial size of 2 for the segments. The algorithm starts by dividing the time domain into 4 segments of size 2. The next step is to reduce the number of segments from 4 to 3. To this purpose, the algorithm tests the merging of every two adjacent segments of the 4 existing segments, in order to find the one that has the minimum SSE. Three different scenarios are possible: Scenario 1: The first scenario is shown in Figure <ref type="figure">3a</ref> where s 1 and s 2 of the initial segmentation (shown in figure <ref type="figure" target="#fig_2">2</ref>) are merged into one segment. We generate the PAA representation of X using the 3 segments, and then compute the SSE of this representation that is SSE 1 (X,X)≈1.167.</p><p>Scenario 2: This scenario is shown in Figure <ref type="figure">3b</ref> in which s 2 and s 3 of the initial segmentation are merged. As for Scenario 1, we generate the PAA representation of X using the current segmentation. Here, SSE 2 (X,X)≈1.915.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Scenario 3:</head><p>The last scenario is shown in Figure <ref type="figure">3b</ref>, where we merge s 3 and s 4 . For this segmentation, SSE 3 (X,X)≈1.745.</p><p>We have calculated the SSE for the three scenarios. Since we aim to minimize the SSE, we have to choose the minimum SSE value (MSSE), that is MSSE = 1.167 corresponding to the segmentation generated in Scenario 1. The latter is chosen for this iteration of our algorithm and we continue the next iterations, until the number of segment reaches w. (c) Scenario 3</p><p>Fig. <ref type="figure">3</ref>: The three different scenarios of ASAX SSE segmentation with 3 segments. Scenario 1 is the one chosen because it provides the MSSE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Lower Bounding of the Similarity Measure</head><p>SAX <ref type="bibr" target="#b8">[9]</ref> defines a distance measure on the PAA representation of time series as described in Section 2.1. Given the representation of two time series, the DR f function allows obtaining a lower bounding approximation of the Euclidean distance between the original time series. By the following theorem, we propose a lower bounding approximation formula for the case of variable size segmentation in ASAX SSE.</p><p>Theorem 1. Let X and Y be two time series. Suppose that by using ASAX SSE we create a variable size segmentation with w segments, such that the size of the i th segment is l i . Let X and Y be the representations of X and Y in ASAX SSE. Then, DR v (X,Y ) gives a lower bounding approximation of the Euclidean distance between</p><formula xml:id="formula_7">X and Y : DR v (X,Y )= w j=1 ((x j -y j ) 2 ×l j )</formula><p>Proof: The proof has been removed due to lack of space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Parallel Versions of ASAX SSE</head><p>We propose efficient parallel techniques using GPUs for improving the execution time of our segmentation algorithm. In our approach, the CPU controls the main loop of the segmentation computation process and does light operations, while the time-consuming tasks are parallelized on GPU, particularly the SSE computation on a dataset for a given segmentation. We propose two parallel versions of the algorithm using CUDA framework to provide a fast computation of the variable-size segmentation over long time series and/or large number of time series: 1) ASAX DP that performs the parallelization on data; 2) ASAX SP that makes the parallelization on segments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Parallelization on Data</head><p>The main idea of our first parallel algorithm, called ASAX DP (ASAX Data Parallel), is to divide the dataset into blocks (partitions), and to assign the SSE computation for the time series of each block to a core of the GPU. Let us describe the proposed algorithm. Initially, the host (CPU) sends the whole dataset D to the GPU (this data transfer between the CPU and the GPU is done only once). Then, the host creates the initial segmentation segments by splitting the time domain into the k starting segments. Afterwards, in a loop, until the number of segments is more than w, it generates a candidate segmentation by merging 2 segments of the last validated segmentation. For each candidate segmentation, the GPU is used for computing SSE on D. For this, the host calls the GPU kernel that computes SSE in parallel operating on different time series of the different dataset blocks. In the kernel, each thread calculates the SSE on the time series of its block and stores the result in a shared array, called sseArray, that is sent back to the CPU. The host calculates the sum of the received results to get the SSE on D, and updates the MSSE (minimum SSE) if the SSE obtained in this iteration is less than the MSSE obtained until now.</p><p>After testing all possible segmentations, it chooses the one that has the minimum SSE, updates the set of segments segments and decrements the current number of segments k by one. This process continues until k reaches the required number of segments w.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Parallelization on Segments</head><p>Here, we propose ASAX SP (ASAX Segment in Parallel), a parallel algorithm in which the computations related to each possible merging of segments is done by a different GPU core. As shown by our experiments, this algorithm can be more efficient than the one presented previously in the cases where the time series are long (e.g., more than 1000 values per time series). The initialization of this algorithm is the same as the algorithm presented in the previous subsection. The host starts by sending the dataset D to the GPU, and dividing the time domain into k starting segments to form the set segments.</p><p>Then, until the number of segments has not reached w, the host calls the GPU kernel to compute SSE on D of each possible segmentation in parallel. The number of launched threads is equal to the number of possible segmentations obtained when reducing the number of segments from k to k-1. In the kernel, each thread calculates its segmentation by merging two segments s i and s i+1 where i is the thread position. The thread computes the SSE of the segmentation on the dataset D and stores the result in a shared array, called sseArray, according to its position. The result array is sent back to the CPU. Each element of the array represents the SSE for a candidate segmentation. The host selects the one having the lowest SSE value, and then updates segments and k. This process continues until k reaches w.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>In this section, we present the experimental evaluation of ASAX SSE. We first describe the experimental setup. Then, in Subsection 5.2, we compare the precision of ASAX SSE representation with that of the existing SAX representation. Finally, in Subsection 5.3, we evaluate the performance of the parallel versions of ASAX SSE by measuring the execution time of the variable-size segmentation using GPUs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Setup</head><p>All approaches are implemented with Python programming language. ASAX SSE and SAX implementations use Numba JIT compiler to optimize machine code at runtime. The GPU-based part of the parallel algorithms is written in Numba 1 .</p><p>The ASAX SSE and SAX experiments were conducted on a machine using Ubuntu 18.04.5 LTS operating system with 20 Gigabytes of main memory, and an Intel Xeon(R) 3,10 GHz processor with 4 cores. The parallel experimental evaluation was conducted on an NVIDIA GeForce RTX 2080 Ti GPU card, equipped with 4 352 CUDA cores and 11 GB of memory installed in the same machine. We compare the proposed ASAX SSE and SAX in terms of precision on all the real-world datasets available in the UCR Time Series Classification Archive 2 . We evaluate the performance of the parallel algorithms on two datasets taken from the same archive, the size of the datasets is increased to reach 1M by repeating the contained time series 1 Our code is available at: https://github.com/lamiad/ASAX_SSE 2 https://www.cs.ucr.edu/ ~eamonn/time_series_data_2018/ multiple times. For each approach, the length w of the approximate representations is reduced to 10% of the original time series length and the variable-size segmentation algorithms are initialized by splitting the time domain into segments of length 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Precision of k-Nearest Neighbor Search</head><p>We compare the quality of ASAX SSE and SAX representation on all 128 datasets of the UCR Time Series Classification Archive. For each dataset, we measure the precision of the approximate k-NN search as the average precision for a set of arbitrary random queries taken from this dataset. The search precision for each query Q from a dataset D is calculated as :</p><formula xml:id="formula_8">p= |AppkNN(Q,D)∩ExactkNN(Q,D)| k</formula><p>where AppkNN(Q,D) and ExactkNN(Q,D) are the sets of approximate k nearest neighbors and exact k nearest neighbors of Q from D, respectively. AppkNN(Q,D) is obtained using the DR f distance measure for SAX and DR v for the ASAX SSE representation, while ExactkNN(Q,D) contains the exact k-NN results of Q using the euclidean distance ED. AppkNN(Q,D) and ExactkNN(Q,D) use a linear search that consists in computing the distance from the query point Q to every other point in D, keeping track of the "best so far" result.</p><p>The precision results are reported in Figure <ref type="figure" target="#fig_3">4</ref> where the precision gain/loss (as percentage) for ASAX SSE compared to SAX precision is measured for each dataset. The integer part of the obtained precision is taken into consideration to compare the two methods. Figure <ref type="figure" target="#fig_3">4a</ref> shows the precision results for ASAX GSSE (i.e., ASAX SSE using GSSE) and Figure <ref type="figure" target="#fig_3">4b</ref> those for ASAX LSSE (i.e., ASAX SSE using LSSE). The results are illustrated using a scatter chart where the horizontal axis represents the dataset number and the vertical axis shows the precision gain/loss obtained. We observe a gain in precision for the large majority of datasets. We obtained a gain in precision for 80% of the datasets with ASAX GSSE and 84% with ASAX LSSE. The distribution of time series over the time domain varies from one dataset to another. There are some for which the distribution is quite balanced, those which undergo some variations and others whose variation increases a lot. Figure <ref type="figure" target="#fig_3">4</ref> does not allow explaining the precision gain or loss since we need to have the visualisation of the time series for each datasets, for this, an analysis is done regarding the precision results obtained and the shape of data. We have noticed that the more the distribution of the data is unbalanced the more the gain is important. The maximum gain achieved is a significant 38% for both ASAX GSSE and ASAX LSSE methods, obtained for the ECGFiveDays dataset. This high gain is due to the unbalanced data distribution over the time domain on this dataset. We were able to achieve a precision of 93% for ASAX SSE while it is 55% for SAX, because ASAX SSE performed a better distribution of the segments according to information gain by creating several segments in the parts that undergo a significant variation that produces more accurate times series representations leading to a better result for the approximate k-NN search. We can see that for some datasets the computed gain is zero meaning equivalent precision for ASAX SSE and SAX due to the balanced shape of the time series over the time domain. Regarding the few datasets where we obtain lower precision, the loss is relatively low (mostly near zero). Globally, our results suggest the effectiveness of our approach and its advantage over the state of the art when applied to time series especially those with unbalanced distribution over the time domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Scalability</head><p>This subsection presents the time cost of the variable-size segmentation for our proposed algorithms. We measure the variable-size segmentation time costs of the parallel algorithms ASAX DP and ASAX SP, and compare them to that of the variable-size segmentation for the sequential algorithm ASAX SSE. The percentage of precision gain computed in the experiments described in the previous subsection shows that the gain obtained with the ASAX GSSE approach is less than the one obtained with ASAX LSSE. Furthermore, the evaluation of the time cost for ASAX GSSE approach (sequential and parallel methods) showed that this approach is more time consuming than ASAX LSSE. For these reasons, we present the results of our parallel algorithms, ASAX DP and ASAX SP, only using the LSSE measurement.</p><p>Figure <ref type="figure">5</ref> and Figure <ref type="figure">6</ref> report the performance gains of our parallel approaches compared to the sequential version of ASAX LSSE. Figure <ref type="figure">5</ref> reports the variablesize segmentation time for the ASAX DP and ASAX LSSE with varying dataset size. The computation time increases with the number of time series for both algorithms. But, it is much lower in the case of ASAX DP than that of the sequential ASAX LSSE. The performance gains vary significantly depending on the number of time series. As seen, the gain reaches ×45 for 1M of time series. Figure <ref type="figure">6</ref> reports the computation time of variable-size segmentation for the ASAX SP and ASAX LSSE. Here we vary the time series length. The running time increases with the length of time series and, as one could expect, the sequential ASAX LSSE takes much more time than ASAX SP. Depending on time series length, ASAX SP shows performance gains that can reach ×24 for 1000 time series of length 2700. Figure <ref type="figure">7</ref> and Figure <ref type="figure">8</ref> compare the parallel segmentation computation time of our approaches. In Figure <ref type="figure">7</ref>, we evaluate the two approaches with varying dataset size (number of time series) and fixed time series length. For this case, we observe that ASAX DP is always faster than ASAX SP. The results show that using ASAX DP is advantageous in the case of databases of many small time series. In Figure <ref type="figure">8</ref>, we vary the time series length and we fix the dataset size for the evaluation. We notice that when time series length n=100, ASAX DP is a little faster than ASAX SP, but when the length of time series increases, ASAX SP becomes faster than ASAX DP. The performance gain reaches ×7.5 for time series of length 1000. ASAX SP allows better performance gains when the database consists of few and long time series. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Related Work</head><p>Several techniques have been yet proposed to reduce the dimensionality of time series.</p><p>Examples of such techniques that can significantly decrease the time and space required for similarity search are: singular value decomposition (SVD) <ref type="bibr" target="#b5">[6]</ref>, the discrete Fourier transformation (DFT) <ref type="bibr" target="#b0">[1]</ref>, discrete wavelets transformation (DWT) <ref type="bibr" target="#b3">[4]</ref>, piecewise aggregate approximation (PAA) <ref type="bibr" target="#b6">[7]</ref>, random sketches <ref type="bibr" target="#b4">[5]</ref>, Adaptive Piecewise Constant Approximation (APCA) <ref type="bibr" target="#b2">[3]</ref>, and symbolic aggregate approXimation (SAX) <ref type="bibr" target="#b8">[9]</ref>. SAX <ref type="bibr" target="#b8">[9]</ref> is one of the most popular techniques for time series representation. It uses a symbolic representation that segments all time series into equi-length segments and symbolizes the mean value of each segment. Some extensions of SAX have been proposed for improving the similarity search performance via indexing <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b1">2]</ref>. For example, iSAX <ref type="bibr" target="#b10">[11]</ref> is an indexable version of SAX designed for indexing large collections of time series. iSAX 2.0 <ref type="bibr" target="#b1">[2]</ref> proposes a new mechanism and also algorithms for efficient bulk loading and node splitting policy, which is not supported by iSAX index.</p><p>There have been SAX extensions designed to improve the representation of each segment, while using the SAX fixed-size segmentation, e.g., <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b14">15]</ref>. For example, SAX TD improves the representation of each segment by taking into account the trend of the time series. It uses the values at the starting and ending points of the segments to measure the trend. TFSA <ref type="bibr" target="#b13">[14]</ref> and SAX CP <ref type="bibr" target="#b12">[13]</ref> are other trend-based SAX representation methods. TFSA proposes a representation method for long time series based on the trend, and SAX CP considers abrupt change points while generating the symbols in order to capture time series' trends.</p><p>To increase the quality of time series approximation, we propose an adaptive approach ASAX SSE based on variable-length segmentation of time series by taking into account the sum of absolute error. Our approach is complementary to the existing SAX extensions, e.g., in indexing based techniques or those that use the trend for representing the segments. For example, our variable-size segmentation can be used in iSAX, SAX TD and SAX CP for segmenting the time series.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>We addressed the problem of approximating time series, and proposed ASAX SSE, a new technique for segmenting time series before their transformation into symbolic representations. ASAX SSE can reduce significantly the error incurred by possible splittings at different steps of the representation calculation, by taking into account the sum of squared errors (SSE). We also proposed two parallel algorithms for improving the execution time of ASAX SSE using GPUs. We evaluated the performance of our segmentation approach through experimentation using more than 120 real world datasets. The results suggest that the more the data distribution in the time domain is unbalanced (non-uniform), the greater is the precision gain of ASAX SSE. For example, for the ECGFiveDays dataset that has a non-uniform distribution in the time domain, the precision of ASAX SSE is 93% compared to 55% for SAX. Furthermore, the results illustrate the effectiveness of our parallel algorithms, e.g., up to ×45 faster than the sequential algorithm for 1M time series.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>representation of X, with 4 segments and cardinality 4, [00,00,01,11]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>do 4 segmentsT oMerge = null 5 msse = ∞ 6 for i=1 to k-1 do 7 s 9 tempSegments = tempSegments s 10 /</head><label>4567910</label><figDesc>split time domain into k segments of size size 3 while k ̸ = w = merge (si,si+1) 8 tempSegments = segments-{si,si+1} /merge segment i and segment i+1 in tempSegments 11 sse = 0 12 foreach ts in D do 13 sse = sse + SSE(ts) 14 if sse &lt; msse then 15 segmentsT oMerge = i 16 msse = sse 17 s = merge (s segmentsT oMerge ,s segmentsT oMerge+1 ) 18 segments = segments-{s segmentsT oMerge ,s segmentsT oMerge+1 } 19 segments = segments s 20 k = k-1 21 return segments</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 :</head><label>2</label><figDesc>Fig. 2: PAA representation of a time series X of length 8 with 4 segments.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 :</head><label>4</label><figDesc>Fig.4: The precision gain for ASAX GSSE and ASAX LSSE compared to SAX. The obtained gain is up to 38% for both methods</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 :Fig. 6 :</head><label>56</label><figDesc>Fig. 5: Variable-size segmentation time for ASAX DP and ASAX LSSE as a function of dataset size. The original time series are of length 130.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>7 :Fig. 8 :</head><label>78</label><figDesc>Fig. 8: Comparison of parallel segmentation time using ASAX DP and ASAX SP, as a function of time series length. The dataset size is fixed to 10 000.</figDesc></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Efficient similarity search in sequence databases</title>
		<author>
			<persName><forename type="first">R</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Faloutsos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Swami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 4th Int. Conf. on FODO</title>
		<meeting>of the 4th Int. Conf. on FODO</meeting>
		<imprint>
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Beyond one billion time series: indexing and mining very large time series collections with i SAX2+</title>
		<author>
			<persName><forename type="first">A</forename><surname>Camerra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shieh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Palpanas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Rakthanmanon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">J</forename><surname>Keogh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowl. Inf. Syst</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Locally adaptive dimensionality reduction for indexing large time series databases</title>
		<author>
			<persName><forename type="first">K</forename><surname>Chakrabarti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Keogh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mehrotra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pazzani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Database Syst</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="188" to="228" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Efficient time series matching by wavelets</title>
		<author>
			<persName><forename type="first">K</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">W</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the ICDE</title>
		<meeting>of the ICDE</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Fast window correlations over uncooperative time series</title>
		<author>
			<persName><forename type="first">R</forename><surname>Cole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Shasha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD Conf</title>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="743" to="749" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Fast subsequence matching in time-series databases</title>
		<author>
			<persName><forename type="first">C</forename><surname>Faloutsos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ranganathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Manolopoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the SIGMOD</title>
		<meeting>of the SIGMOD</meeting>
		<imprint>
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Dimensionality reduction for fast similarity search in large time series databases</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">J</forename><surname>Keogh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chakrabarti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Pazzani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mehrotra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowl. Inf. Syst</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="263" to="286" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">A symbolic representation of time series, with implications for streaming algorithms</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Keogh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lonardi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Chiu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003">2003</date>
			<publisher>SIGMOD</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Experiencing sax: A novel symbolic representation of time series</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Keogh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lonardi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Data Min. Knowl. Discov</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">New time series data representation esax for financial applications</title>
		<author>
			<persName><forename type="first">B</forename><surname>Lkhagva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Suzuki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kawagoe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDE Workshops</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">isax: Indexing and mining terabyte sized time series</title>
		<author>
			<persName><forename type="first">J</forename><surname>Shieh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Keogh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD Conf</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="623" to="631" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">An improvement of symbolic aggregate approximation distance measure for time series</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">138</biblScope>
			<biblScope unit="page" from="189" to="198" />
			<date type="published" when="2014-08">08 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A novel trend based sax reduction technique for time series</title>
		<author>
			<persName><forename type="first">H</forename><surname>Yahyaoui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Al-Daihani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Expert Systems with Applications</title>
		<imprint>
			<biblScope unit="volume">130</biblScope>
			<biblScope unit="issue">04</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Symbolic representation based on trend features for knowledge discovery in long time series</title>
		<author>
			<persName><forename type="first">H</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Q</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">Q</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">D</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">M</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers of Information Technology Electronic Engineering</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="744" to="758" />
			<date type="published" when="2015-09">09 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Entropy-based symbolic aggregate approximation representation method for time series</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Joint Int. Information Technology and Artificial Intelligence Conference (ITAIC)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="905" to="909" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
