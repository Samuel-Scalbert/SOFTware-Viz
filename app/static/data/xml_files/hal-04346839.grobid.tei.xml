<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Predicting the Acceptability of Atomic Candidate OWL Class Axioms</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Ali</forename><surname>Ballout</surname></persName>
							<email>ali.ballout@inria.fr</email>
							<affiliation key="aff0">
								<orgName type="department">I3S</orgName>
								<orgName type="institution" key="instit1">Université Côte d&apos;Azur</orgName>
								<orgName type="institution" key="instit2">Inria</orgName>
								<address>
									<settlement>Sophia Antipolis</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Célia</forename><surname>Da Costa Pereira</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">I3S</orgName>
								<orgName type="institution" key="instit1">Université Côte d&apos;Azur</orgName>
								<orgName type="institution" key="instit2">CNRS Sophia Antipolis</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Andrea</forename><forename type="middle">G B</forename><surname>Tettamanzi</surname></persName>
							<email>andrea.tettamanzi@unice.fr</email>
							<affiliation key="aff2">
								<orgName type="department">I3S</orgName>
								<orgName type="institution" key="instit1">Université Côte d&apos;Azur</orgName>
								<orgName type="institution" key="instit2">Inria</orgName>
								<address>
									<settlement>Sophia Antipolis</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Predicting the Acceptability of Atomic Candidate OWL Class Axioms</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">1BFA4E90697337526BDE2F5FC79B3A2C</idno>
					<idno type="DOI">10.1109/WI-IAT59888.2023.00055</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-04-12T14:53+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Ontology Learning</term>
					<term>OWL Class Axioms</term>
					<term>Concept Similarity</term>
					<term>Reasoning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The task of evaluating the fitness of a candidate axiom against known facts or data is known as candidate axiom scoring. Being able to accurately score candidate axioms is a prerequisite for automatic schema or ontology induction, but can also be useful for ontology and/or knowledge graph validation. Accurate axiom scoring heuristics are often heavy to compute, which is a big problem if one wants to exploit them in iterative search methods like level-wise generate-and-test or evolutionary algorithms, where large numbers of candidate axioms need to be scored. We tackle the challenge of learning a predictive model as a surrogate to reasoning, that predicts the acceptability of candidate class axioms, that is fast to execute yet accurate enough to be used in such settings. For this purpose, we leverage a semantic similarity measure extracted from the subsumption hierarchy of an ontology. We prove that the method proposed in this paper is able to learn the acceptability labels of candidate OWL class axioms with high accuracy and that it can do so for multiple types of OWL class axioms.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Ontologies play a crucial role in organizing digital information by categorizing entities and defining their relationships <ref type="bibr" target="#b0">[1]</ref>. Despite their importance, creating and maintaining ontologies is resource-intensive, often requiring domain experts. Ontology learning aims to mitigate this by automating ontology construction and enrichment <ref type="bibr" target="#b1">[2]</ref>.</p><p>Ontology learning is a multidisciplinary field, incorporating techniques from:</p><p>• Natural Language Processing (NLP) for data preprocessing and term extraction <ref type="bibr" target="#b2">[3]</ref>. • Data-driven methods like data mining for identifying terms and their relationships <ref type="bibr" target="#b3">[4]</ref>. • Inductive Logic Programming (ILP) to generate hypotheses based on existing knowledge <ref type="bibr" target="#b4">[5]</ref>. These techniques contribute to various stages of ontology learning, including preprocessing, term extraction, and relation identification <ref type="bibr" target="#b2">[3]</ref>. However, they come with limitations like high computational cost and reliance on potentially inaccurate data.</p><p>Our work introduces a new approach that overcomes these limitations by reducing computational costs and improving accuracy. Specifically, our method predicts labels for a range of axiom types without relying on error-prone statistics.</p><p>The remainder of this paper is organized as follows: Section II discusses related work, Section III provides essential background, Section IV describes our methodology, and Section V presents our experiments and findings. We conclude with notes and observations. II. RELATED WORK Data-driven ontology learning methods such as the possibilistic heuristic by Tettamanzi et al. <ref type="bibr" target="#b5">[6]</ref> focus primarily on statistical analyses using RDF data. Although these methods achieve high accuracy, they are time-consuming and susceptible to errors. An attempt to mitigate the time issue through time-capping resulted in a slight increase in the error rate <ref type="bibr" target="#b6">[7]</ref>.</p><p>Inductive Logic Programming (ILP) offers alternative approaches, exemplified by the SoftFOIL method <ref type="bibr" target="#b7">[8]</ref>. While effective in automating the induction of fuzzy axioms, it suffers from limitations such as a greedy search algorithm and the risk of entering infinite loops.</p><p>Emerging hybrid methods aim to combine the benefits of both ILP and statistical learning. One example is the work by Malchiodi et al. <ref type="bibr" target="#b8">[9]</ref>, which modified the support vector clustering algorithm to predict the possibilistic scores for OWL axioms. These methods show promise but are resourceintensive.</p><p>For OWL class axiom labeling, frameworks like that developed by Ballout et al. <ref type="bibr" target="#b9">[10]</ref> employ semantic similarity for training models focused on the binary classification of axioms. These approaches have demonstrated efficacy but are not standalone solutions <ref type="bibr" target="#b10">[11]</ref>.</p><p>In our current work, we aim for efficient and accurate binary classification of candidate axioms. Our method is designed to function either as a standalone labeler or as an extension to existing methods like DL-Learner <ref type="bibr" target="#b11">[12]</ref>. We benchmark our approach against state-of-the-art description logic reasoners using DBpedia to validate its efficacy and efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. BACKGROUND</head><p>Our primary goal is to create a model for predicting the acceptability of axioms, independent of potentially error-prone instance data. This necessitates a labeled training set of axioms and a method for determining their semantic relationships as model features.</p><p>Semantic similarity serves as a measure of the semantic closeness between ontological concepts, often based on the subsumption (rdfs:subClassOf) relation <ref type="bibr" target="#b12">[13]</ref>. This is distinct from semantic relatedness; for instance, a train and an airplane are semantically similar as both are vehicles <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>. Such similarity measures are often based on path lengths and node depths in a subsumption hierarchy, or on information content derived from ontology and corpus statistics <ref type="bibr" target="#b14">[15]</ref>.</p><p>Corby et al. proposed a semantic similarity measure based on the ontological distance between concepts in an inheritance hierarchy <ref type="bibr" target="#b13">[14]</ref>. The ontological distance is calculated using the lengths of subsumption paths and the depths of concepts in the hierarchy. This measure has been implemented in the Corese software platform <ref type="bibr" target="#b15">[16]</ref>, which we employ to compute concept similarity in our method.</p><formula xml:id="formula_0">DH (t1, t2) = mint(lH (⟨t1, t⟩) + lH (⟨t2, t⟩)) = mint {x∈&lt;t 1 ,t&gt;,x̸ =t 1 } 1/2 d H (x) + {x∈&lt;t 2 ,t&gt;,x̸ =t 2 } 1/2 d H (x) . (1)</formula><p>In summary, the ontological distance D H (t 1 , t 2 ) in hierarchy H is the minimum sum of subsumption path lengths between t 1 and t 2 and a common supertype. The path length from a type t 1 to its direct supertype t is 1/2 d H (t) , where d H (t) is the depth of t in H. This measure is implemented in the Corese<ref type="foot" target="#foot_0">1</ref> software platform <ref type="bibr" target="#b15">[16]</ref>, which we use for computing concept similarity in our approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. METHOD</head><p>In an OWL ontology containing an inheritance hierarchy of concepts formed by the subsumption axiom rdfs:subClassOf, our aim is to predict if a candidate atomic axiom (consisting of a single named class on each side) is accepted or not. We do this by training a model on a set of previously labeled axioms of the same type (one of: subsumption, disjointness, equivalence) and their similarity weights. In the absence of a scored set our method creates one using explicit axioms available in the ontology. To measure the similarity between (candidate) axioms, we construct a similarity measure by extending the ontological distance discussed in Section III, which is defined among classes, not axioms. This enables the model to predict the label of any new atomic candidate axiom of the same type. To this end, we consider the following steps:</p><p>1) OWL ontology closure reasoning: This step involves using DL-Reasoners the likes of HermiT <ref type="bibr" target="#b16">[17]</ref> and Pellet <ref type="bibr" target="#b17">[18]</ref> to infer all the axioms (binary relations between named classes) that can be inferred from the knowledge available in the ontology.</p><p>In this paper, we use it when testing the ability of our model to accept axioms that are not entailed by a reasoner, or to reject ones that are entailed by a reasoner. 2) Axiom extraction and labeling: This step constitutes the creation of the set of accepted and rejected labeled axioms of a certain type to be learned. One approach can be querying existing axioms and labeling them as accepted/rejected. Another is to use a scorer to label a set of generated candidate axioms to learn.</p><p>3) Semantic measure retrieval and assignment: This step involves the retrieval of concepts used in our set of axioms, and their ontological distance from the ontology, followed by extending that similarity to those axioms. This was done by calculating a single value that represents the similarity between each pair of axioms, by applying a function such as Average to the ontological distances of concepts in those axioms. 4) Axiom base vector space modeling: This step focuses on using axiom similarity measures as weights, each axiom can be represented as a vector in an axiom based vector space. 5) Binary Classification: This step is dedicated to training a Machine Learning model with the data set (vector space model in addition to the extracted labels) and predicting if new candidate axioms are accepted or rejected.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. OWL Ontology Closure Reasoning</head><p>The optional first step in our methodology involves inferring all entailed axioms from a target OWL ontology, particularly useful when the ontology is rich in explicit axioms. This serves two purposes:</p><p>1) Enlarging the training set of axioms, enhancing the experimental accuracy and facilitating performance comparison with DL-reasoners. 2) Enabling the filtering of new labeled or scored axioms against our ontology closure, allowing us to assess the model's performance on axioms not entailed by reasoners.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Axiom Extraction and Labeling</head><p>In this work, we used two strategies to create a set of labeled axioms. The first approach utilizes existing scoring methods or previously scored axioms from literature. The second, a faster method, is what we term the "type and counter-type technique."</p><p>1) Scoring Method: We used multiple sets of scored axioms for the ontology DBpedia, one set<ref type="foot" target="#foot_1">2</ref> of axioms generated and scored by Nguyen et al. <ref type="bibr" target="#b18">[19]</ref> using the possibilistic heuristic <ref type="bibr" target="#b5">[6]</ref>. The rest of teh sets we scored ourselves using the same heuristic. The scorer though accurate, is extremely slow limiting the size of the sets scored. The scorer currently supports DBpedia and is publicly available <ref type="foot" target="#foot_2">3</ref> . We utilized the version detailed in the work of Felin et al. <ref type="bibr" target="#b19">[20]</ref>. These scored data sets are what we use for our evaluation and comparison with reasoners in the case of DBpedia. The possibilistic heuristic scores are considered our ground truth and baseline. For experiments using other ontologies we employed the second approach.</p><p>2) Type and Counter Type Technique: In this technique, we query an ontology for existing axioms of a certain type, thereby obtaining a set of axioms labeled as accepted. We then query axioms of the counter type. For instance, subClassOf and disjointWith can be considered counter types to each other. Axioms of the counter type are assigned the label rejected. Since existing disjointWith axioms can be considered false or rejected subClassOf, we can construct a sample containing both labels to train a model. We consider disjointWith to be a counter-type for subClassOf and equivalenntClass. While subClassOf and equivalenntClass are counter-types of disjointWith. However, before querying an ontology for existing axioms, it is advisable to apply a reasoner to obtain the closure of the ontology. The reasoner can be any suitable choice, such as HermiT or Pellet, and not necessarily the reasoner included in the engine used. By applying a reasoner to obtain the closure of an OWL ontology, we ensure that we obtain a complete and consistent set of labeled axioms for our model.</p><p>The axiom type designated as accepted corresponds to the type that the model will be labeling. For instance, if we assign the accepted label to disjointWith axioms, it implies that disjointWith is the axiom type that the model is designed to address. While this may initially appear as a limitation, the efficiency of the process mitigates this concern. As demonstrated in Table <ref type="table" target="#tab_2">II</ref>, which details the time consumption, the speed of dataset preparation and model training is sufficiently rapid. This allows us to learn a model for each type of axiom within a relatively short time frame.</p><p>We employ Query 1 to extract and label existing axioms of both the type and counter-type. Following the extraction, we ensure an equal representation of both type and countertype axioms. The SPARQL endpoint utilized in this process is Corese <ref type="bibr" target="#b15">[16]</ref>. We have selected DBpedia as our test case to illustrate a real-world application. While other ontologies could be employed, DBpedia offers several advantages. It has been widely used in related work and aligns well with the scoring heuristic we employ for evaluating our candidate axioms. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Semantic Measure Retrieval and Assignment</head><p>To be able to assign similarity measures between axioms, we need to retrieve the ontological distances between all classes a construct the concept similarity matrix (CSM). sing Corese in which the ontological distance metric is implemented, this translates into a function added to the SPARQL query. Query 2 retrieves three columns, the first two contain the combination of all classes with the third containing the ontological distance denoted by similarity. Blank nodes are ignored.</p><p>After retrieving the table of similarities it is pivoted to construct a symmetric n×n matrix where the first column and the first row are the classes and the cells are the similarities From the CSM, we can derive the axiom similarity matrix (ASM) with labels illustrated in Matrix 1, as explained in the work of Ballout et al. <ref type="bibr" target="#b10">[11]</ref> which also highlights that the function used to calculate the similarity S between a pair of axioms can be either Average or Minimum.</p><p>We define then a vector-space model to represent axioms as vectors. Indeed, we represent each axiom as a vector whose elements are the kernels representing the similarity value between the considered axiom and the other axioms present in the labeled dataset T A . We have been inspired by the Kernel trick in Support Vector Machine (SVM) which is used to deal with nonlinear classification <ref type="bibr" target="#b20">[21]</ref>.</p><p>The number of dimensions m of our vector space corresponds to the number of axioms we have in T A . Each axiom is then represented as a vector V in this m-dimensional space, and corresponds to a row in the axiom similarity Matrix 1.</p><p>The axiom similarity matrix is updated whenever an axiom is generated or a new candidate axiom is suggested.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Binary Classification</head><p>It is now possible to apply classification machine learning methods using our labeled dataset of axioms represented as vectors. We used a range of methods throughout our experiments, including but not limited to trees, random forests, kNN, Support vector classifier, gradient boosting, and neural networks. Trees and random forests were used to check that there was no bias during the classification. The reason for this choice is that such methods allow us to interpret our predictive model easily and visually analyse the decisions. kNN instead was used to test the effect of the similarity measure as a distance (the metric used was Manhattan, due to the large number of dimensions and the weight was distances). To avoid information leakage since the matrix is symmetric, considering the size of the matrix is m × m, all predicting models were trained using an m ′ × m ′ sub-matrix, with m ′ &lt; m, of the axiom similarity matrix with the labels, and then tested on the (m -m ′ ) remaining axioms. Our goal is then to predict the  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. EXPERIMENTS AND RESULTS</head><p>For the following experiments, we used this configuration:</p><p>• Dual CPUs: 2 × Intel(R) Xeon(R) CPU E5-2689 0 @ 2.60GHz total of 16 cores and 32 threads. • A total of 32 GB of RAM memory @ 1600 MHz.</p><p>• Code and ontologies used available in our repository. <ref type="foot" target="#foot_3">4</ref></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Dataset Preparation</head><p>As outlined in Section IV-E, our experiments spanned various classification methods, ontologies, and axiom types. For illustrative purposes, we will focus on disjointness axioms to demonstrate the absence of leakage or bias from using the subClassOf hierarchy. Our initial step involved constructing the datasets as follows:</p><p>1) Ontology Selection: We selected ontologies of varying sizes and domains for our experiments, with details summarized in Table <ref type="table" target="#tab_1">I</ref>. The chosen ontologies include:</p><p>• NTNames<ref type="foot" target="#foot_4">5</ref> : A small, hand-crafted ontology focusing on New Testament names. • Pizza<ref type="foot" target="#foot_5">6</ref> : A medium-sized ontology from the University of Manchester, widely recognized in the field. • MatOnto<ref type="foot" target="#foot_6">7</ref> : A material science ontology used by the Ontology Alignment Evaluation Initiative<ref type="foot" target="#foot_7">8</ref> . • DBpedia<ref type="foot" target="#foot_8">9</ref> : A large, real-world knowledge base derived from Wikipedia, selected for its complexity and size. This ontology was also used for DL-reasoner application and further experimentation.</p><p>2) Axiom Extraction and Labeling: Each ontology was loaded into Corese. Since in some of the ontologies the explicit number of axioms was not big enough for any meaningful experiment, we applied different reasoning methods to obtain the maximum number of deduced axioms. For all ontologies, we used Corese reasoner, which is a rule-based engine that can handle RDF(S) and OWL 2 RL profiles. For DBpedia specifically, we additionaly used Hermit and Pellet reasoners to compute the closure, a step which took two hours for each run using Protegé. <ref type="foot" target="#foot_9">10</ref> Additionally, we used the disjointness axioms generated and scored by Nguyen et al. <ref type="bibr" target="#b18">[19]</ref> for DBpedia, as explained in Section IV-B. After that, we extracted a balanced set of Accepted and Rejected axioms from each ontology using SPARQL queries and the technique of type and counter type explained in Section IV-B2. For instance, for subsumption in DBpedia, we selected 4300 Accepted and 4300 disjointWith axioms representing Rejected subClassOf axioms, resulting in a total of 8600 axioms for T A , which is our axiom dataset for this scenario.</p><p>Explicit axiom extraction by querying an ontology using a SPARQL endpoint needs minimal time, as for generating a set of labeled axioms as in <ref type="bibr" target="#b18">[19]</ref> the time cost would depend on the scorer and the method used to generate candidate axioms.</p><p>3) Concept Similarity Matrix: After preparing the set of axioms, we have to produce the concept similarity matrix (CSM), as explained in Section IV-C. The processing time for creating the CSM depends on the number of concepts. In our tested dataset, MatOnto had 847 concepts and creating its CSM took 2.84 seconds. In Table <ref type="table" target="#tab_2">II</ref> we present the timings for the worst-case scenario, which consists of applying our method to the most dense axiom type that is subsumption for comparison reasons. Other axiom types are naturally much faster due to their lesser population.</p><p>4) Axiom Similarity Matrix and Vector Space: The next step is constructing the axiom similarity Matrix 1 (ASM), and encoding each of the axioms as vectors V in our vector space.</p><p>Table <ref type="table" target="#tab_2">II</ref> details the time required to complete these operations, all the values presented are the mean average of five consecutive runs. As observed the time needed to complete the task of building the ASM significantly increases as the number of axioms processed increases, but we also know that it depends on the size of the CSM as well from the time needed to encode a single axiom into a vector. As the number of concept increases the CSM being searched increases in size as it has the shape n concepts × n concepts and the number of cells n 2 . We would like to note that the test scenario presented in Table II is extreme, especially the case of DBpedia, as the number of axioms needed for training a model does not need to be as large to achieve peak accuracy/results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Training and Testing</head><p>Using the dataset obtained as described in the previous section, the classifiers we tested were Decision Trees (DT), Random Forests (RF), kNN, support vector classifier (SVC), Neural Networks (NN) and Gradient boosting (GB). Experiments were performed over every class axiom type, where the axiom population was enough, for every ontology in our dataset some results are shown in Figure <ref type="figure" target="#fig_2">1</ref>. We also experimented with both Average and Minimum functions to   We would also like to note that the model training times mentioned in Table <ref type="table" target="#tab_2">II</ref>, are the average for the above mentioned methods except for the extreme case of DBpedia subsumption using gradient Boosting, where it may take up to 136 seconds to train the model on a set of 6, 000 axioms.</p><p>In order to prove that the proposed kernel-based representation of axioms is advantageous for addressing the task of predicting the acceptability of candidate axioms, we performed an additional experiment. For this experiment we used a simple naive kNN method using our axiom similarity measure directly to check the n nearest axioms to a candidate axiom and then assigning that candidate axiom the label that occurs most in the n neighbors. To make things fair, we compare the performance of this naive approach to our proposed kernel-based vectorspace representation of axioms, using kNN. We perform the comparison using DBpedia's subClassOf axioms as a dataset. We used as a training set 200 axioms, and for the test set 4000 axioms; both sets are balanced in terms of labels.</p><p>Table <ref type="table" target="#tab_4">IV</ref> presents the F1 scores of each of the methods as a function of the number n of neighbors. From the results we can see that the naive method performs best when the number of neighbors is 1, this is because the method does not learn anything and simply retrieves the closest axioms based on the similarity measure. Our proposed method on the other hand maintains performance since it uses the similarity matrix as a kernel to map the neighbors to a plane of dimensions equal to the number of axioms used in training. Our method outperforms the naive one by a significant margin every time. This corroborates the hypothesis that the proposed kernelbased vector-space representation of axioms is capable of capturing useful features of the semantics of candidate axioms, this offering a clear advantage over the simple and direct use of the axiom similarity matrix. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Proof of Concept</head><p>In order to prove that the proposed kernel-based representation of axioms is advantageous for addressing the task of predicting the acceptability of candidate axioms, we performed an additional experiment. For this experiment we used a simple naive kNN method using our axiom similarity measure directly to check the n nearest axioms to a candidate axiom and then assigning that candidate axiom the label that occurs most in the n neighbors. To make things fair, we compare the performance of this naive approach to our proposed kernel-based vectorspace representation of axioms, using kNN. We perform the comparison using DBpedia's subClassOf axioms as a dataset. We used as a training set 200 axioms, and for the test set 4000 axioms; both sets are balanced in terms of labels.</p><p>Table <ref type="table" target="#tab_4">IV</ref> presents the F1 scores of each of the methods as a function of the number n of neighbors. From the results we can see that the naive method performs best when the number of neighbors is 1, this is because the method does not learn anything and simply retrieves the closest axioms based on the similarity measure. Our proposed method on the other hand maintains performance since it uses the similarity matrix as a kernel to map the neighbors to a plane of dimensions equal to the number of axioms used in training. Our method outperforms the naive one by a significant margin every time. This corroborates the hypothesis that the proposed kernelbased vector-space representation of axioms is capable of capturing useful features of the semantics of candidate axioms, this offering a clear advantage over the simple and direct use of the axiom similarity matrix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Comparing With Reasoners</head><p>In this experiment, we compare the performance of our model with HermiT, Pellet, and Corese in accepting or rejecting candidate OWL class axioms. For this experiment, we prepare a dataset generated and scored by the possibilistic heuristic using DBpedia ontology which is widely used and represents our real-world scenario. The generation and scoring required seven days of processing time. The dataset contains: 811 accepted subClassOf axioms and 745 rejected ones, as well as 1276 accepted disjointWith axioms and 823 rejected ones. We split the dataset into training and test sets. The split was as follows: the subClassOf training set contains 399 accepted and 398 rejected, while the test set contains 412 accepted and 347 rejected. As for disjointWith, the training set contains 524 accepted and 596 rejected axioms, while the test set contains 752 accepted and 227 rejected.</p><p>We evaluated our model and the reasoners on test sets, employing two training scenarios for our model. In the first scenario, we trained our model using a set extracted from  the closure of DBpedia, produced by the type counter type method. In the second scenario, we used a scored training set independent of the reasoners. We evaluated the reasoners by checking if they could entail the candidate axioms from DBpedia. If a candidate is entailed then it is labeled as accepted otherwise it is labeled rejected. We assessed precision, recall, and F1-score with our ground truth being the labels produced by the possibilistic scorer. The results presented in Table <ref type="table" target="#tab_5">V</ref>. We noticed that the results of our model were worse when training it using the closure, and almost perfect when using the scored training set. Therefore, we hypothesized that the labels of the training set from the closure (reasoners product) could have wrong labels, while the labels from the scorer were more accurate. To test this hypothesis, we ran the scored training set through the reasoners to see how they would would label it. Indeed, the reasoners were entailing axioms labeled as rejected by the scorer, which appear to have a great impact on how the model labeled the test set. A sample of these axioms can be seen in Table <ref type="table" target="#tab_6">VI</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Key Findings and Observations</head><p>In our experiments, various models trained on the same dataset showed similar results, consistently achieving high accuracy levels averaging between 90-95%. Key observations from our experiments are outlined below:</p><p>1) Axiom Similarity Function: Throughout our experiments, we have observed a consistent trend where most ASMs that were constructed using the Average function to obtain S gave better results than those that were built using the Minimum function. This can be observed in Table <ref type="table" target="#tab_3">III</ref>.</p><p>The models are scored based on their performance on the test set (i.e., when classifying axiom candidates never seen before).</p><p>We note that Neural Networks were the only model to break this trend. It performed better when the Minimum function was used as seen in Table <ref type="table" target="#tab_3">III</ref>. However, despite that, it had the worst performance out of all the models presented using both functions, even though different configurations were tested. 11  2) Classifier Of Choice: Tree-based models consistently achieved the highest scores. This fact is very evident in Table <ref type="table" target="#tab_3">III</ref>, where the leading models when using the better similarity function Average were, in order: Gradient Boosting, Random Forest, and Tree. For this reason, and since all models score close to each other with an accuracy greater than 95%, we find Random Forests combined with the Average function for S to be the classifier of choice when creating the model. We chose Random Forest over Gradient Boost since the time for training Random Forest models is faster while giving up at most 0.1% accuracy.</p><p>3) Model Perfomance Dealing With Variables : We chose the confusion matrix metric to summarize the results while showing the support, and how the method performs with different sizes of datasets and ontologies as a whole. Figure <ref type="figure" target="#fig_2">1</ref> presents the performance of the method on three different ontologies with different sizes as well as different types of axioms and populations. It is very clear how well the method performs across all those variables. The figure shows results only from the test set, i.e., when the model predicts labels for candidate axioms it has never seen before. This brings us to the conclusion that the method is performing as expected with F1 score between 95% to 99%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSION</head><p>We presented a method for accurately predicting the acceptability of various OWL class axioms, using a semantic similarity measure based on ontological distance. The method's robustness was confirmed through extensive testing across different ontologies.</p><p>Our approach achieved high accuracy for all OWL axiom types, making it suitable for integration with existing methods like DL-Learner <ref type="bibr" target="#b11">[12]</ref> or Grammatical evolution approaches <ref type="bibr" target="#b21">[22]</ref>. The effectiveness is attributed to the semantic similarity measure's ability to capture the axioms' modeltheoretic semantics, suggesting avenues for further research into the relationship between similarity measures and axiom truthfulness.</p><p>Future research directions include:</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>0</head><label></label><figDesc>SELECT ?class1 ?class2 ?label WHERE { 1 { ?class1 a owl:Class . ?class2 a owl:Class . ?class1 rdfs:subClassOf ?class2 2 filter (!isBlank(?class1) &amp;&amp; !isBlank(?class2)) 3 filter (?class1 != ?class2) 4 bind(1.0 as ?label) } 5 Union{ ?class1 a owl:Class . ?class2 a owl:Class . ?class1 owl:disjointWith ?class2 6 filter (!isBlank(?class1) &amp;&amp; !isBlank(?class2)) 7 filter (?class1 != ?class2) 8 bind(0.0 as ?label) }} Query 1: Axiom extraction</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Performance by axiom type for investigated ontologies</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I :</head><label>I</label><figDesc>Ontology statistics. Reasoner used: Corese built in reasoner: C , Pellet: P , Hermit: H .</figDesc><table><row><cell>Ontology</cell><cell cols="4">Classes subClassOf disjointWith equivalentClass</cell></row><row><cell>NTNames C</cell><cell>47</cell><cell>278</cell><cell>10</cell><cell>50</cell></row><row><cell>Pizza C</cell><cell>101</cell><cell>651</cell><cell>5</cell><cell>101</cell></row><row><cell>MatOnto C</cell><cell>847</cell><cell>853</cell><cell>158</cell><cell>9</cell></row><row><cell>DBpedia H P C</cell><cell>866</cell><cell>7334</cell><cell>70669</cell><cell>268</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE II :</head><label>II</label><figDesc>Time cost per step for investigated ontology in seconds for subClassOf. The labels are binary, where 1 represents the label Accepted and 0 represents the label Rejected.</figDesc><table><row><cell>Ontology</cell><cell>CSM</cell><cell>Axioms</cell><cell>ASM</cell><cell>Single Axiom</cell><cell>Model</cell></row><row><cell></cell><cell></cell><cell>processed</cell><cell></cell><cell cols="2">vector encoding training</cell></row><row><cell>NTNames</cell><cell>0.02</cell><cell>556</cell><cell>22</cell><cell>0.03</cell><cell>0.2</cell></row><row><cell>Pizza</cell><cell>0.04</cell><cell>1302</cell><cell>105</cell><cell>0.05</cell><cell>0.4</cell></row><row><cell>MatOnto</cell><cell>2.84</cell><cell>1706</cell><cell>212</cell><cell>0.09</cell><cell>0.6</cell></row><row><cell>DBpedia</cell><cell>2.17</cell><cell>8600</cell><cell>2497</cell><cell>0.08</cell><cell>3.9</cell></row><row><cell cols="6">label of those axioms which have not been used during the</cell></row><row><cell cols="2">training period.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE III :</head><label>III</label><figDesc>F1 scores using DBpedia owl:disjointWith, average and minimum as similarity functions. The results of such experimentation can be found in TableIIIwhere models were trained using 813 axioms split into 406 Rejected and 407 Accepted axioms, and tested on 349 axioms split into 175 Rejected and 174 Accepted.</figDesc><table><row><cell>Function</cell><cell>GB</cell><cell>kNN</cell><cell>NN</cell><cell>RF</cell><cell>SVC</cell><cell>DT</cell></row><row><cell>F1 (AVG)</cell><cell>0.988</cell><cell cols="5">0.976 0.953 0.985 0.976 0.982</cell></row><row><cell>F1 (MIN)</cell><cell>0.977</cell><cell cols="5">0.974 0.962 0.976 0.962 0.968</cell></row><row><cell cols="5">get the similarity between axioms S.</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE IV :</head><label>IV</label><figDesc>F1 scores for the naive nearest neighbor method and our proposed method using K-nearest neighbor as the machine learning model, as a function of number n of neighbors.</figDesc><table><row><cell>Model</cell><cell>n 1</cell><cell>n 3</cell><cell>n 15</cell></row><row><cell>Naive</cell><cell cols="3">0.59 0.49 0.33</cell></row><row><cell>kNN</cell><cell cols="3">0.86 0.87 0.86</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE V :</head><label>V</label><figDesc>Performance of our proposed model when trained using a scored training set and a training set extracted from the closure of DBpedia as a product of HermiT, Pellet and Corese, compared to reasoners in accepting or rejecting axioms of different types.</figDesc><table><row><cell></cell><cell></cell><cell cols="3">Model scored training set</cell><cell cols="3">Model closure training set</cell><cell></cell><cell>Reasoners</cell><cell></cell></row><row><cell>Axiom type</cell><cell>Label</cell><cell cols="2">Precision Recall</cell><cell>F1</cell><cell cols="2">Precision Recall</cell><cell>F1</cell><cell cols="2">Precision Recall</cell><cell>F1</cell></row><row><cell>subClassOf</cell><cell>Accepted Rejected</cell><cell>1.00 0.90</cell><cell>0.91 1.00</cell><cell>0.95 0.95</cell><cell>0.66 1.00</cell><cell>1.00 0.48</cell><cell>0.79 0.65</cell><cell>1.00 1.00</cell><cell>1.00 1.00</cell><cell>1.00 1.00</cell></row><row><cell>disjointWith</cell><cell>Accepted Rejected</cell><cell>0.97 0.83</cell><cell>0.94 0.92</cell><cell>0.96 0.87</cell><cell>0.27 0.99</cell><cell>0.99 0.20</cell><cell>0.43 0.34</cell><cell>0.00 0.23</cell><cell>0.00 1.00</cell><cell>0.00 0.37</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE VI :</head><label>VI</label><figDesc>subClassOf axioms rejected by the scorer and entailed</figDesc><table><row><cell>by the reasoners</cell></row><row><cell>SubClassOf(Tax Genre)</cell></row><row><cell>SubClassOf(Organ Aircraft)</cell></row><row><cell>SubClassOf(Guitar Locomotive)</cell></row><row><cell>SubClassOf(Treadmill ArchitecturalStructure)</cell></row><row><cell>SubClassOf(Quote Work)</cell></row><row><cell>SubClassOf(Instrument MilitaryVehicle)</cell></row><row><cell>SubClassOf(Guitar Aircraft)</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>https://project.inria.fr/corese/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>https://bitbucket.org/RDFMiner/classdisjointnessaxioms/src/ master/Results/ClassDisjointnessAxioms/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>https://github.com/RemiFELIN/RDFMining</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"><p>https://github.com/ali-ballout/axiom-acceptability</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4"><p>https://semanticbible.com/index.html</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5"><p>https://protege.stanford.edu/ontologies/pizza/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_6"><p>https://github.com/inovexcorp/MatOnto-Ontologies</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_7"><p>http://oaei.ontologymatching.org/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_8"><p>http://downloads.dbpedia.org/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10" xml:id="foot_9"><p>https://protege.stanford.edu/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_10"><p>• Extending the method to complex and property axioms.• Exploring ensemble approaches incorporating active learning.11 Can be found in keratest.py in the repository</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Ontology, archetypes and the definition of &apos;mineral species</title>
		<author>
			<persName><forename type="first">F</forename><surname>Hawthorne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mills</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Hatert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rumsey</surname></persName>
		</author>
		<idno type="DOI">10.1180/mgm.2021.21</idno>
		<ptr target="https://doi.org/10.1180/mgm.2021.21" />
	</analytic>
	<monogr>
		<title level="j">Mineralogical Magazine</title>
		<imprint>
			<biblScope unit="volume">85</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="125" to="133" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Web mining based framework for ontology learning</title>
		<author>
			<persName><forename type="first">C</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">K</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Govardhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Science &amp; Information Technology</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">13</biblScope>
			<biblScope unit="page" from="43" to="56" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A survey of ontology learning techniques and applications</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">N</forename><surname>Asim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wasim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">U G</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Mahmood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">M</forename><surname>Abbasi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Database</title>
		<imprint>
			<biblScope unit="volume">2018</biblScope>
			<biblScope unit="page" from="1" to="24" />
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Inductive learning of disjointness axioms</title>
		<author>
			<persName><forename type="first">D</forename><surname>Fleischhacker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Völker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">On the Move to Meaningful Internet Systems: OTM 2011 -Confederated International Conferences: CoopIS, DOA-SVI, and ODBASE 2011</title>
		<title level="s">Proceedings, Part II, ser. Lecture Notes in Computer Science</title>
		<meeting><address><addrLine>Hersonissos, Crete, Greece</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2011">October 17-21, 2011. 2011</date>
			<biblScope unit="volume">7045</biblScope>
			<biblScope unit="page" from="680" to="697" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">DL-FOIL concept learning in description logics</title>
		<author>
			<persName><forename type="first">N</forename><surname>Fanizzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Amato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Esposito</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Inductive Logic Programming, 18th International Conference, ILP 2008</title>
		<title level="s">Proceedings, ser. Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">F</forename><surname>Zelezný</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Lavrac</surname></persName>
		</editor>
		<meeting><address><addrLine>Prague, Czech Republic</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008">September 10-12, 2008. 2008</date>
			<biblScope unit="volume">5194</biblScope>
			<biblScope unit="page" from="107" to="121" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Possibilistic testing of OWL axioms against RDF data</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Tettamanzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Faron-Zucker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Gandon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Approximate Reasoning</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Dynamically timecapped possibilistic testing of SubClassOf axioms against RDF data to enrich schemas</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Tettamanzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">F</forename><surname>Zucker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Gandon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th International Conference on Knowledge Capture, K-CAP 2015</title>
		<meeting>the 8th International Conference on Knowledge Capture, K-CAP 2015</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A logic-based computational method for the automated induction of fuzzy ontology axioms</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">A</forename><surname>Lisi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Straccia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Fundamenta Informaticae</title>
		<imprint>
			<biblScope unit="volume">124</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="503" to="519" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Predicting the possibilistic score of OWL axioms through modified support vector clustering</title>
		<author>
			<persName><forename type="first">D</forename><surname>Malchiodi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Tettamanzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM Symposium on Applied Computing</title>
		<meeting>the ACM Symposium on Applied Computing</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1984" to="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning to classify logical formulas based on their semantic similarity</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ballout</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Da</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Costa</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G B</forename><surname>Tettamanzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">PRIMA 2022: Principles and Practice of Multi-Agent Systems -24th International Conference</title>
		<title level="s">Proceedings, ser. Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">R</forename><surname>Aydogan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Criado</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Lang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">V</forename><surname>Sánchez-Anguix</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Serramia</surname></persName>
		</editor>
		<meeting><address><addrLine>Valencia, Spain</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">November 16-18, 2022. 2022</date>
			<biblScope unit="volume">13753</biblScope>
			<biblScope unit="page" from="364" to="380" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Predicting the score of atomic candidate owl class axioms</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ballout</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G B</forename><surname>Tettamanzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Da</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Costa</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2022 IEEE/WIC/ACM International Joint Conference on Web Intelligence and Intelligent Agent Technology</title>
		<meeting><address><addrLine>WI-IAT</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="72" to="79" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Dl-learner-a framework for inductive learning on the semantic web</title>
		<author>
			<persName><forename type="first">L</forename><surname>Bühmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lehmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Westphal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Web Semantics</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="15" to="24" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">An evaluative baseline for geo-semantic relatedness and similarity</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ballatore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bertolotto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">C</forename><surname>Wilson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">GeoInformatica</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="747" to="767" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Searching the semantic web: Approximate query processing based on ontologies</title>
		<author>
			<persName><forename type="first">O</forename><surname>Corby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Dieng-Kuntz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Faron-Zucker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Gandon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Intell. Syst</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="20" to="27" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A cluster-based approach for semantic similarity in the biomedical domain</title>
		<author>
			<persName><forename type="first">H</forename><surname>Al-Mubaid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">A</forename><surname>Nguyen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Engineering in Medicine and Biology</title>
		<imprint>
			<biblScope unit="page" from="2713" to="2717" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Querying the semantic web with corese search engine</title>
		<author>
			<persName><forename type="first">O</forename><surname>Corby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Dieng-Kuntz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Faron-Zucker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECAI</title>
		<imprint>
			<publisher>IOS Press</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="705" to="709" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">HermiT: An OWL 2 reasoner</title>
		<author>
			<persName><forename type="first">B</forename><surname>Glimm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Horrocks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Motik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Stoilos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="page" from="245" to="269" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Pellet: A practical owl-dl reasoner</title>
		<author>
			<persName><forename type="first">E</forename><surname>Sirin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Parsia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">C</forename><surname>Grau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kalyanpur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Katz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">software Engineering and the Semantic Web</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="51" to="53" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">An evolutionary approach to class disjointness axiom discovery</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">H</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Tettamanzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings -2019 IEEE/WIC/ACM International Conference on Web Intelligence</title>
		<meeting>-2019 IEEE/WIC/ACM International Conference on Web Intelligence<address><addrLine>WI</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Optimizing the computation of a possibilistic heuristic to test owl subclassof axioms against rdf data</title>
		<author>
			<persName><forename type="first">R</forename><surname>Felin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Corby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Faron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G B</forename><surname>Tettamanzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2022 IEEE/WIC/ACM International Joint Conference on Web Intelligence and Intelligent Agent Technology</title>
		<meeting><address><addrLine>WI-IAT</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="80" to="87" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Kernel-based svm</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">N</forename><surname>Murty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Raghava</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Support Vector Machines and Perceptrons: Learning, Optimization, Classification, and Application to Social Networks</title>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="57" to="67" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning class disjointness axioms using grammatical evolution</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">H</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G B</forename><surname>Tettamanzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Genetic Programming -22nd European Conference, EuroGP 2019, Held as Part of EvoStar 2019</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Sekanina</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Hu</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Lourenço</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Richter</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>García-Sánchez</surname></persName>
		</editor>
		<meeting><address><addrLine>Leipzig, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">April 24-26, 2019. 2019</date>
			<biblScope unit="volume">11451</biblScope>
			<biblScope unit="page" from="278" to="294" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
