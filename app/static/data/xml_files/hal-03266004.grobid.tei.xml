<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Constrained Differentially Private Federated Learning for Low-bandwidth Devices</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Raouf</forename><surname>Kerkouche</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Privatics team</orgName>
								<orgName type="institution" key="instit1">Univ. Grenoble Alpes</orgName>
								<orgName type="institution" key="instit2">Inria</orgName>
								<address>
									<postCode>38000</postCode>
									<settlement>Grenoble</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Gergely</forename><surname>Ács</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory" key="lab1">Crysys Lab</orgName>
								<orgName type="laboratory" key="lab2">BME-HIT 3 Tyrex team</orgName>
								<orgName type="institution" key="instit1">Univ. Grenoble Alpes</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
								<orgName type="institution" key="instit3">Inria</orgName>
								<address>
									<settlement>Grenoble</settlement>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="laboratory">INP</orgName>
								<orgName type="institution">LIG</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Claude</forename><surname>Castelluccia</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Privatics team</orgName>
								<orgName type="institution" key="instit1">Univ. Grenoble Alpes</orgName>
								<orgName type="institution" key="instit2">Inria</orgName>
								<address>
									<postCode>38000</postCode>
									<settlement>Grenoble</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Pierre</forename><surname>Genevès</surname></persName>
						</author>
						<title level="a" type="main">Constrained Differentially Private Federated Learning for Low-bandwidth Devices</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">C979BE83DC84B1C191506A358422713C</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-04-12T14:47+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Federated learning becomes a prominent approach when different entities want to learn collaboratively a common model without sharing their training data. However, Federated learning has two main drawbacks. First, it is quite bandwidth inefficient as it involves a lot of message exchanges between the aggregating server and the participating entities. This bandwidth and corresponding processing costs could be prohibitive if the participating entities are, for example, mobile devices. Furthermore, although federated learning improves privacy by not sharing data, recent attacks have shown that it still leaks information about the training data. This paper presents a novel privacy-preserving federated learning scheme. The proposed scheme provides theoretical privacy guarantees, as it is based on Differential Privacy. Furthermore, it optimizes the model accuracy by constraining the model learning phase on few selected weights. Finally, as shown experimentally, it reduces the upstream and downstream bandwidth by up to 99.9% compared to standard federated learning, making it practical for mobile systems.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>In Machine Learning, different entities may want to collaborate in order to improve their local model accuracy. In traditional machine learning, such collaboration requires to first store all entities' data on a centralized server and then to train a model on it. Such data centralization might be problematic when the data are sensitive and data privacy is required. In order to mitigate this problem, Federated learning, which allows different entities to learn collaboratively a common model without sharing their data, was introduced <ref type="bibr">[Shokri and</ref><ref type="bibr">Shmatikov, 2015, McMahan et al., 2016]</ref>. In-stead of sharing the training data, Federated Learning shares the model parameters between a server, which plays the role of aggregator, and the participating entities. Although Federated Learning improves privacy, model parameters can leak information about the training data. Indeed, <ref type="bibr" target="#b50">Zhu et al. [2019]</ref>, <ref type="bibr" target="#b49">Zhao et al. [2020]</ref>, <ref type="bibr" target="#b21">Geiping et al. [2020]</ref> presented some attacks that allow an adversary to reconstruct pieces of the training data of some entities. <ref type="bibr" target="#b40">Nasr et al. [2019]</ref> define a membership attack that allows to infer if a particular record is included in the data of a specific entity. Similarly, <ref type="bibr" target="#b37">Melis et al. [2018]</ref> define an attack which aims at inferring if a subgroup of people with a specific property, like for example skin color or ethnicity, is included in the dataset of a particular participating entity. A solution to prevent these attacks and provide theoretical guarantees in to use a privacy model called Differential Privacy <ref type="bibr" target="#b17">[Dwork and Roth, 2014]</ref>. Differential Privacy has been applied to federated learning in order to protect either each record included in the dataset of any entity (record-level guarantee), or the whole dataset of any entity (client-level guarantee). Unfortunately, it is well-known that Differential Privacy drastically degrades the accuracy of the global model as it requires to add random noise to the gradients (record-level) or to the updates (client-level) of each client. Recent work by <ref type="bibr" target="#b27">Kerkouche et al. [2020]</ref> shows that this accuracy penalty can be reduced if the model is compressed, as compression reduces the required amount of noise. Furthermore, <ref type="bibr" target="#b27">Kerkouche et al. [2020]</ref> show that accuracy can be further improved by adding noise only to the largest update's values as adding noise on values close to 0 is likely to lead to random update values.</p><p>Following up on these results, we propose a novel differentially private federated learning solution that improves the model accuracy (1) by updating only a fixed subset of the model weights, and (2) by maintaining the other weights constant. The proposed scheme provides theoretical privacy guarantees, as it is based on Differential Privacy. Furthermore, it optimizes the model accuracy by constraining the model learning phase on a few selected weights. As all participants always update the same set of weights and transfer them to the server for aggregation, the proposal can be easily integrated with secure aggregation <ref type="bibr" target="#b10">[Bonawitz et al., 2016]</ref>, which allows parties to add less noise than other decentralized perturbation approaches such as randomized response <ref type="bibr" target="#b18">[Erlingsson et al., 2014]</ref> used in local differential privacy. Moreover, it also reduces the upstream and downstream bandwidth by a factor of 1000 compared to standard federated learning, making it practical for mobile systems. The paper is structured as follows: In Section 2 we introduce the necessary background to understand the proposal, in Section 3 we define our solution called FL-TOP and in Section 3.2 its private extension called FL-TOP-DP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">BACKGROUND 2.1 FEDERATED LEARNING (FL-STD)</head><p>In federated learning <ref type="bibr">[Shokri and</ref><ref type="bibr">Shmatikov, 2015, McMahan et al., 2016]</ref>, multiple parties (clients) build a common machine learning model from union of their training data without sharing them with each other. At each round of the training, a selected set of clients retrieve the global model from the parameter server, update the global model based on their own training data, and send back their updated model to the server. The server aggregates the updated models of all clients to obtain a global model that is re-distributed to some selected parties in the next round.</p><p>In particular, a subset K of all N clients are randomly selected at each round to update the global model, and C = |K|/N denotes the fraction of selected clients. At round t, a selected client k ∈ K executes T gd local gradient descent iterations on the common model w t-1 using its own training data D k (D = ∪ k∈K D k ), and obtains the updated model w k t , where the number of weights is denoted by n (i.e., |w k t | = |∆w k t | = n for all k and t). Each client k submits the update ∆w k t = w k t -w k t-1 to the server, which then updates the common model as follows:</p><formula xml:id="formula_0">w t = w t-1 + ∑ k∈K |D k | ∑ j |D j | ∆w k t ,</formula><p>where |D k | is known to the server for all k (a client's update is weighted with the size of its training data). The server stops training after a fixed number of rounds T cl , or when the performance of the common model does not improve on a held-out data.</p><p>Note that each D k may be generated from different distributions (i.e., Non-IID case), that is, any client's local dataset may not be representative of the population distribution <ref type="bibr">[McMahan et al., 2016]</ref>. This can happen, for example, when not all output classes are represented in every client's training data. The federated learning of neural networks is summarized in Alg. 4. In the sequel, each client is assumed to use the same model architecture.</p><p>The motivation of federated learning is three-fold: first, it aims to provide confidentiality of each participant's training data by sharing only model updates instead of potentially sensitive training data. Second, in order to decrease communication costs, clients can perform multiple local SGD iterations before sending their update back to the server. Third, in each round, only a few clients are required to perform local training of the common model, which further diminishes communication costs and makes the approach especially appealing with large number of clients. However, several prior works have demonstrated that model updates do leak potentially sensitive information <ref type="bibr" target="#b40">[Nasr et al., 2019</ref><ref type="bibr" target="#b37">, Melis et al., 2018]</ref>. Hence, simply not sharing training data per se is not enough to guarantee their confidentiality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">DIFFERENTIAL PRIVACY</head><p>Differential privacy allows a party to privately release information about a dataset: a function of an input dataset is perturbed, so that any information which can differentiate a record from the rest of the dataset is bounded <ref type="bibr" target="#b17">Dwork and Roth [2014]</ref>.</p><p>Definition 1 (Privacy loss). Let A be a privacy mechanism which assigns a value Range(A ) to a dataset D. The privacy loss of A with datasets D and D at output</p><formula xml:id="formula_1">O ∈ Range(A ) is a random variable P(A , D, D , O) = log Pr[A (D)=O] Pr[A (D )=O]</formula><p>where the probability is taken on the randomness of A .</p><p>Definition 2 ((ε, δ )-Differential Privacy <ref type="bibr" target="#b17">[Dwork and Roth, 2014]</ref>). A privacy mechanism A guarantees (ε, δ )differential privacy if for any database D and D , differing on at most one record,</p><formula xml:id="formula_2">Pr O∼A (D) [P(A , D, D , O) &gt; ε] ≤ δ .</formula><p>Intuitively, this guarantees that an adversary, provided with the output of A , can draw almost the same conclusions (up to ε with probability larger than 1δ ) about any record no matter if it is included in the input of A or not. That is, for any record owner, a privacy breach is unlikely to be due to its participation in the dataset.</p><p>Moments Accountant. Differential privacy maintains composition; the privacy guarantee of the k-fold adaptive composition of A 1:k = A 1 , . . . , A k can be computed using the moments accountant method <ref type="bibr" target="#b1">Abadi et al. [2016]</ref>. In particular, it follows from Markov's inequality that Pr</p><formula xml:id="formula_3">[P(A , D, D , O) ≥ ε] ≤ E[exp(λ P(A , D, D , O))]/ exp(λ ε) for any out- put O ∈ Range(A ) and λ &gt; 0. A is (ε, δ )-DP with δ = min λ exp(α A (λ ) -λ ε), where α A (λ ) = max D,D log E O∼A (D) [exp(λ P(A , D, D , O))]</formula><p>is the log of the moment generating function of the privacy loss. The privacy guarantee of the composite mechanism A 1:k can be computed using that α A</p><formula xml:id="formula_4">1:k (λ ) ≤ ∑ k i=1 α A i (λ ) Abadi et al. [2016].</formula><p>Gaussian Mechanism. A fundamental concept of all DP sanitization techniques is the global sensitivity of a function <ref type="bibr" target="#b17">[Dwork and Roth, 2014]</ref>.</p><p>Definition 3 (Global L p -sensitivity). For any function f : The Gaussian Mechanism <ref type="bibr" target="#b17">[Dwork and Roth, 2014]</ref> consists of adding Gaussian noise to the true output of a function. In particular, for any function f : D → R n , the Gaussian mechanism is defined as adding i.i.d Gaussian noise with variance (∆ 2 f • σ ) 2 and zero mean to each coordinate value of f (D).</p><formula xml:id="formula_5">D → R n , the L p -sensitivity of f is ∆ p f = max D,D || f (D) - f (D )|| p ,</formula><p>Recall that the pdf of the Gaussian distribution with mean</p><formula xml:id="formula_6">µ and variance ξ 2 is pdf G (µ,ξ ) (x) = 1 √ 2πξ exp -(x-µ) 2 2ξ 2 .</formula><p>In fact, the Gaussian mechanism draws vector values from a multivariate spherical (or isotropic) Gaussian distribution which is described by random variable</p><formula xml:id="formula_7">G ( f (D), ∆ 2 f • σ I n ),</formula><p>where n is omitted if its unambiguous in the given context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">FEDERATED PRUNING</head><p>In the standard federated learning scheme (FL-STD, in Section 2), the server sends the latest updated model to a randomly selected set of clients (downstream), and each client sends back its complete model update after local training to the server (upstream) at each round. Knowing that a model has on average millions of parameters (each is a floating point value represented on 32 bits), the network can suffer from large traffic both upstream and downstream.</p><p>Our solution, called FL-TOP, aims to reduce the large amount of network traffic by reducing both downstream and upstream traffic. Moreover, a privacy-preserving extension of this scheme, called FL-TOP-DP, is also proposed, which provides Differential Privacy for the whole training data of every client.</p><p>In what follows, we first describe the non-private scheme FL-TOP and then the privacy-preserving FL-TOP-DP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">FL-TOP: FEDERATED PRUNING FOR COMPRESSION</head><p>FL-TOP is inspired by the pruning techniques proposed in <ref type="bibr" target="#b23">Han et al. [2016]</ref> (see Section 5 for more details), and it aims to reduce the amount of parameters exchanged downstream (from the server to the participating entities) and upstream (from the participating entities to the server). In our scheme, each client updates only a small subset, Top-K, of the model parameters (weights) at each round. Only the K weight values of these Top-K parameters are updated during training, and neither the clients nor the server need to transfer the values of the remaining n -K parameters, where n is the total number of parameters. The set of Top-K parameters do not change over the whole training and are identical for all clients. We experimentally show in Section 4 that, if these K parameters are chosen carefully, the performance penalty is negligible even if K = 0.005 • n, that is, 99.5% of the model parameters are pruned. Note that unlike standard pruning techniques, where the set of pruned weights are re-selected after each SGD iteration <ref type="bibr" target="#b23">[Han et al., 2016]</ref>, our scheme always updates the same K parameters.</p><p>These Top-K parameters are selected by the server at the beginning of the protocol. More specifically, the server initializes the model and trains that with some public data that have a similar distribution as the clients' training data. After a few SGD iterations, the server selects the K parameters which values changed the most.</p><p>FL-TOP is described in Alg. 1. First, the server uses public data to identify the set T of the Top-K parameters (K = |T|), before starting federated learning. In particular, starting from a public model w 0 , it accumulates the absolute value of gradients per parameter over T init SGD iterations, and selects the K parameters with the largest accumulated gradients.</p><p>After that, the values/updates<ref type="foot" target="#foot_0">1</ref> of these parameters are the only ones exchanged during the rest of the training between the server and the clients.</p><p>At each round, each selected client k uses the K updated weights ŵt-1 received from the server to create a new weight vector w k t-1 of size n, such that w k t-1 is composed from the compressed vector ŵk t-1 of size K ≤ n (with coordinates in T) and n -K weights from the initialization vector w 0 . w 0 is identical for all participants and can be generated from a shared seed. Note that when K = |T| = n, the scheme is equivalent to FL-STD. The weight vector w k t-1 is used to train the client's model. However, only the weights in T are updated while the remaining ones are kept fixed. To do that, the weights not in T are reinitialized after each SGD iteration to w 0 . The server receives only the values from w k t -w k t-1 at coordinates T, denoted by C (w k t -w k t-1 ) for short, from every client k, and updates the common model w t with the average of these compressed updates (in Line 12).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">FL-TOP-DP: DIFFERENTIALLY PRIVATE FEDERATED PRUNING</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Privacy Model</head><p>We consider an adversary, or a set of colluding adversaries, who can access any update vector sent by the server or any clients at each round of the protocol. A plausible adversary is a participating entity, i.e. a malicious client or server, that wants to infer the training data used by other participants. The adversary is passive (i.e., honest-but-curious), that is, it follows the learning protocol faithfully.</p><p>Different privacy requirements can be considered depending on what information the adversary aims to infer. In general, private information can be inferred about:</p><p>• any record (user) in any dataset of any client (recordlevel privacy),</p><p>• any client/party (client-level privacy).</p><p>To illustrate the above requirements, suppose that several banks build a common model to predict the creditworthiness of their customers. A bank certainly does not want other banks to learn the financial status of any of their customers (record privacy) and perhaps not even the average income of all their customers (client privacy).</p><p>Record-level privacy is a standard requirement used in the privacy literature and is usually weaker than client-level privacy. Indeed, client-level privacy requires to hide any information which is unique to a client including perhaps all its training data.</p><p>We aim at developing a solution that provides client-level privacy and is also bandwidth efficient. For example, in the scenario of collaborating banks, we aim at protecting any information that is unique to each single bank's training data. The adversary should not be able to learn from the received model or its updates whether any client's data is involved in the federated run (up to ε and δ ). We believe that this adversarial model is reasonable in many practical applications when the confidential information spans over multiple samples in the training data of a single client (e.g., the presence of a group a samples, such as people from a certain race). Differential Privacy guarantees plausible deniability not only to any groups of samples of a client but also to any client in the federated run. Therefore, any negative privacy impact on a party (or its training samples) cannot be attributed to their involvement in the protocol run.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Operation</head><p>FL-TOP-DP is described in Alg. 3 is very similar to FL-TOP except that each client adds Gaussian noise to its Top-K model updates to guarantee client-level DP, and applies secure aggregation allowing the server to learn only the aggregated (and noisy) model update. More specifically, each client first calculates its compressed model update</p><formula xml:id="formula_8">∆w k t = C (w k t -w k t-1 ) (in Line 25) which is then clipped (in Line 26) to obtain ∆ ŵk t with L 2 -norm at most S. After that, random noise z k ∼ G (0, Sσ I/ √ K) is added to ∆ ŵk t such that the sum ∑ k∈K (∆ ŵk t + z k ) = ∑ k∈K ∆ ŵk t + G (0, Sσ I)</formula><p>as the sum of Gaussian random variables also follows Gaussian distribution<ref type="foot" target="#foot_1">2</ref> and then differential privacy is satisfied where ε and δ can be computed using the moments accountant described in Section 2.2. Recall that the Top-K coordinates in T are selected and distributed by the server, which is honest-but-curious by assumption.</p><p>However, as the noise is inversely proportional to √ K, z k is likely to be small if |K| is too large. Therefore, the adversary accessing an individual update ∆ ŵk t + z k can almost learn a non-noisy update since z k is small. Hence, each client uses secure aggregation to encrypt its individual update before sending it to the server. Upon reception, the server sums the encrypted updates as:</p><formula xml:id="formula_9">∑ k∈K c k t = ∑ k∈K Enc K k (∆ ŵk t + z k ) = ∑ k∈K ∆ ŵk t + ∑ k∈K z k = ∑ k∈K ∆ ŵk t + G (0, Sσ I)<label>(1)</label></formula><p>where <ref type="bibr" target="#b2">Ács and Castelluccia [2011]</ref>, <ref type="bibr" target="#b10">Bonawitz et al. [2016]</ref> for details). Here the modulo is taken element-wise and</p><formula xml:id="formula_10">Enc K k (∆ ŵk t + z k ) = ∆ ŵk t + z k + K k mod p and ∑ k K k = 0 (see</formula><formula xml:id="formula_11">p = 2 log 2 (max k ||∆ ŵk t +z k || ∞ |K|) . Let γ k t = 1/ max 1, ||∆w k t || 2 S . Then, ∑ k∈K ∆ ŵk t = ∑ k∈K γ k t ∆w k t = ∑ k∈K γ k t C (w k t -w k t-1 , T) = C ( ∑ k∈K γ k t (w k t -w k t-1 ), T)<label>(2)</label></formula><p>where the last equality comes from the linearity of the compression operation. Indeed, recall that each client selects the values of the same Top-K coordinates from T. Plugging Eq. (2) into Eq. ( <ref type="formula" target="#formula_9">1</ref>). we get that</p><formula xml:id="formula_12">∑ k∈K c k t = C ( ∑ k∈K γ k t (w k t -w k t-1 ), T) + G (0, Sσ I)</formula><p>Privacy analysis: The server can only access the noisy aggregate which is sufficiently perturbed to ensure differential privacy; any client-specific information that could be inferred from the noisy aggregate is tracked and quantified by the moments accountant, described in Section 2.2, as follows.</p><p>Let</p><formula xml:id="formula_13">η 0 (x|ξ ) = pdf G (0,ξ ) (x) and η 1 (x|ξ ) = (1 -C)pdf G (0,ξ ) (x) + Cpdf G (1,ξ ) (x) where C is the sampling probability of a single client in a single round. Let α(λ |C) = log max(E 1 (λ , ξ ,C), E 2 (λ , ξ ,C)) where E 1 (λ , ξ ,C) = R η 0 (x|ξ ,C) • η 0 (x|ξ ,C) η 1 (x|ξ ,C) λ dx and E 2 (λ , ξ ,C) = R η 1 (x|ξ ,C) • η 1 (x|ξ ,C) η 0 (x|ξ ,C) λ dx. Theorem 1 (Privacy of FL-TOP-DP). FL-TOP-DP is (min λ (T cl • α(λ |C) -log δ )/λ , δ )-DP.</formula><p>Given a fixed value of δ , ε is computed numerically as in <ref type="bibr" target="#b1">Abadi et al. [2016]</ref>, <ref type="bibr" target="#b38">Mironov et al. [2019]</ref>. </p><formula xml:id="formula_14">for each client k in K do c k t = Client k (C (w t-1 , T)) end w t = w 0 j = 1 for each coordinate i in T do w t [i] = w t-1 [i] + ∑ k c k t [ j] |K| j = j + 1 end end Output: Global model w t</formula><p>Client k ( ŵk t-1 ):</p><formula xml:id="formula_15">w k t-1 = w 0 j = 1 for each coordinate i in T do w k t-1 [i] = ŵk t-1 [ j] j = j + 1 end w k t = Top k SGD(D k , w k t-1 , w 0 , T gd , T) Output: Model update C (w k t -w k t-1 , T)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Remarks</head><p>The magnitude of the added Gaussian noise is proportional to the clipping threshold S, which is in turn calibrated to the norm of the model update. However, the norm of the model update increases if the model size increases <ref type="bibr" target="#b51">[Zhu et al., 2020]</ref>, and hence S should be chosen sufficiently large to guarantee fast convergence with large accuracy. On the other hand, too large S also increases the perturbation error caused by the added noise.</p><p>FL-TOP aims to diminish this perturbation error by reducing S via compression which also increases the L 2 -norm of the compressed update vector. This is illustrated in Figure <ref type="figure" target="#fig_1">1</ref>, which shows that the norm of the Top-K coordinates with FL-TOP tend to be larger than with FL-STD (i.e., when all coordinates get updated not only the Top-K). Therefore, besides decreasing the magnitude of the added noise, FL-TOP also decreases the relative error on the retained parameters. These together decrease the perturbation error caused by the added noise.</p><p>Notice that there exist other alternatives to identify the Top-K coordinates in a privacy-preserving manner than using a public dataset. For example, every client can select the Top-K parameters with the largest magnitude during the first More specifically, each client creates a parameter vector with size n, where the Top-K coordinates are set to 1 while the rest are kept 0. Then, these binary vectors are noised and aggregated by the server like in Section 3.2.2. In the rest of the training, all participants exchange only the updates and weights of the these Top-K parameters like in FL-TOP. However, aside from consuming more privacy budget, this approach also has lower accuracy than our proposal according to our tests. Moreover, it has larger communication cost in the initialization phase when the Top-K parameters are identified and the whole binarized parameter vector is sent for aggregation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTAL RESULTS</head><p>The goal of this section is to evaluate the performance of our proposed schemes FL-TOP and FL-TOP-DP on a benchmark dataset and a realistic in-hospital mortality prediction scenario. We aim at evaluating their performance with different levels of compression and comparing them with the performance of the following learning protocols<ref type="foot" target="#foot_2">3</ref> :</p><p>• FL-STD: The Standard Federated Learning scheme as described in Section 2.1 (see Alg. 4).</p><p>• FL-BASIC: A Federated Learning scheme that updates a random subset of parameters instead of the Top-K parameters at each SGD iteration. This subset is reselected at the beginning of each new round. The nk non-selected parameters are still reinitialized after each SGD update as in FL-TOP.</p><p>• FL-CS: A Federated Learning scheme that uses Compressive sensing (CS) to compress model updates from <ref type="bibr" target="#b27">Kerkouche et al. [2020]</ref>. See Section 5 for more details.</p><p>Note that all compression operators in the baselines are linear (just like FL-TOP-DP), and hence they can also be used with secure aggregation. Similarly to FL-TOP-DP, the </p><formula xml:id="formula_16">for each client k in K do c k t = Client k (C (w t-1 , T)) end w t = w 0 j = 1 for each coordinate i in T do w t [i] = w t-1 [i] + ∑ k c k t [ j] |K| j = j + 1 end end Output: Global model w t</formula><p>Client k ( ŵk t-1 ):</p><formula xml:id="formula_17">w k t-1 = w 0 j = 1 for each coordinate i in T do w k t-1 [i] = ŵk t-1 [ j] j = j + 1 end w k t = Top k SGD(D k , w k t-1 , w 0 , T gd , T) ∆w k t = C (w k t -w k t-1 , T) ∆ ŵk t = ∆w k t / max 1, ||∆w k t || 2 S Output: Enc K k (G (∆ ŵk t , SIσ / |K|))</formula><p>private extensions (i.e., FL-STD-DP, FL-BASIC-DP and FL-CS-DP) also clip and then noise the compressed updates.</p><p>We evaluate the above learning algorithms on the wellknown Fashion-MNIST dataset <ref type="bibr" target="#b48">[Xiao et al., 2017]</ref> and on the Premier Healthcare Database, which is a real-world medical dataset of 1.2 millions of US hospital patients 4 . More details can be found in Appendix A.1 and Appendix B.1.</p><p>Recall that the Top-K weights are selected before starting the federated learning process using public data. For Fashion-MNIST, we randomly select a batch with size 10 from MNIST dataset [LeCun and Cortes, 2010] described in Appendix B.2. For the medical dataset, we did not find any public dataset with the same features as ours, and for this reason, we selected randomly from the dataset a batch of 356 patients 5 . This set is used only by the server and never by any client. Afterwards, the server performs T init SGD iterations starting from the model parameters w 0 on the same batch to identify the Top-K weights. We experi-4 https://www.premierinc.com/newsroom/education/premierhealthcare-database-whitepaper 5 Reduced to 24 patients when we train via downsampling with 12 patients for each class mentally show later that even these small batches are enough for the server to find a good set of Top-K weights.</p><p>In order to select the clipping threshold S, the server executes a single training round locally, which is composed of T gd SGD iterations starting from the model parameters w 0 , using the batch from the public data. The clipping threshold S is set to the L 2 -norm of the Top-K weight update obtained for this single training round. For FL-BASIC-DP, the same steps are repeated for 100 times, where a new random set of trainable weights with size K are selected each time, which yields 100 L 2 -norm values. S is set to the median of these L 2 -norm values. We think that this approach is more fair, because the set of trainable weights is re-selected at each round in FL-BASIC-DP. The computed values of S can be found in Table <ref type="table" target="#tab_7">6</ref> and Table <ref type="table" target="#tab_8">7</ref> for Fashion-MNIST and Medical dataset, respectively. More information about the model architectures and the hyper-parameter selection can be found in Appendix A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">RESULTS</head><p>Figure <ref type="figure" target="#fig_1">1</ref> displays the distribution of the Top-K updated weights for FL-TOP and FL-STD at the end of the training. We select the weights when each scheme reached the best accuracy over 200 and best balanced accuracy<ref type="foot" target="#foot_3">6</ref> over 100 rounds for fashion-MNIST and the medical dataset, respectively. We choose the smallest compression ratio r that leads to the best accuracy for the FL-TOP-DP scheme. Table <ref type="table" target="#tab_1">1</ref> shows that FL-TOP-DP reaches the best accuracy, 0.81, when r = 0.5% on fashion-MNIST and reaches the best accuracy, 0.69, when r = 0.1% on the medical dataset. Both figures validate the intuition that by constraining the model to update only a small set K of the total weights, these Top-K become more important and reach larger values. This result is important when differential privacy is used as it leads to larger value-to-noise level and therefore better performance.</p><p>Table <ref type="table" target="#tab_1">1</ref> represents the best accuracy over 200 rounds for each scheme on the Fashion-MNIST dataset. Round corresponds to the round when the best accuracy is reached and Cost is the average bandwidth consumption calculated as: r × n × 32 × Round ×C, where 32 is the number of bits necessary to represent a float value, n is the uncompressed model size, r = |T| n , |T| is the compressed model size, C is the sampling probability of a client, and Round is the round when we get the the best accuracy.   These tables show that the proposed non-private scheme FL-TOP has similar accuracy than the standard scheme FL-STD but reduces the bandwidth cost significantly. For example, with the Fashion-MNIST dataset, the FL-TOP accuracy reaches 0.85 when the compression ratio r = 10%. In comparison, the standard FL-STD scheme reaches an accuracy of 0.86% but consumes 10 times more bandwidth. Furthermore, although FL-CS reaches the same accuracy than FL-TOP and consumes slightly less bandwidth upstream (9% less), its required downstream bandwidth is about 10 times larger (See Table <ref type="table" target="#tab_1">1</ref> for more details). The results on the medical dataset are quite similar. In fact, FL-TOP achieves its best balanced accuracy (0.74) and AUROC (0.82) when r = 10% while the FL-STD scheme obtains similar performance but required about 11 times more upsteam and downstream bandwidth cost. FL-CS achieves similarly accuracy at r = 10% as FL-TOP but its downstream required bandwidth is about 11 times larger (see Table <ref type="table" target="#tab_0">2</ref> for more details).</p><p>The results also show that not only our privacy-preserving solution FL-TOP-DP provides strong privacy guarantee (with ε values smaller than 1) but that it outperforms the other schemes in term of accuracy and bandwidth, for both datasets. For example, with Fashion-MNIST, our scheme achieves an accuracy of 0.81 when r = 0.5% while the baseline scheme, FL-BASIC-DP, achieves an accuracy of 0.79 when r = 10% and requires 189 times more downstream bandwidth and 18 times more upstream bandwidth. With the medical dataset, FL-TOP-DP reaches the best balanced accuracy 0.69 and best AUROC 0.76 for a compression ratio of r = 0.1% while FL-BASIC-DP and FL-CS-DP achieves the same performance at r = 5%. Note that FL-STD-DP performs very poorly as noise has to be added to the all weights of the model and the sensitivity is large (see Table <ref type="table" target="#tab_0">2</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">RELATED WORK</head><p>Privacy of Federated Learning: The concept of Clientbased Differential Privacy has been introduced in McMahan et al. <ref type="bibr">[2018]</ref> and <ref type="bibr" target="#b22">Geyer et al. [2017]</ref>, where the goal is to hide any information that is specific to a single client's training data. These algorithms noise the contribution of a single client instead of a single record in the client's dataset. The noise is added by the server, hence, unlike our solution, these works assume that the server is trusted.</p><p>Recently, <ref type="bibr" target="#b32">Liu et al. [2020]</ref> also proposed to add noise only to the update of the Top-K model parameters a la local-DP.</p><p>In local-DP, each client adds larger noise that what is necessary to guarantee DP for the aggregated model update without using secure aggregation. Therefore, the common model is less accurate than with our scheme. In addition, <ref type="bibr" target="#b32">Liu et al. [2020]</ref> uses two epsilon budgets; one for selecting Top-K parameters per client, and the second for perturbing these selected Top-K parameters. By contrast, we select the Top-K parameters via public data without sacrificing any privacy budget. Finally, their solution is also less bandwidth efficient than ours: as the Top-K parameters differ for each client and at each round, the client cannot send only the Top-K parameters values because the server will not be able to identify which value corresponds to which Top-K parameter.</p><p>For this reason, the client has to send a sparse vector with only Top-K perturbed values and all remaining parameters set to 0. Therefore, the quantization of the non-Top-K parameters is performed only during the upstream (from client to server) without compressing any downstream traffic. As opposed to this, in our solution, only the weights/updates of the Top-K parameters are transferred downstream/upstream.</p><p>Recently, <ref type="bibr" target="#b27">Kerkouche et al. [2020]</ref> proposed to use Compressive sensing (CS) in the context of federated learning in order to compress model updates meanwhile providing client-level DP. Assuming that the model update is already sparse in the time domain, the noise is added to its largest Fourier coefficients in a distributed manner, and the noisy aggregate is reconstructed with standard optimization techniques. Likewise our solution, this work also uses secure aggregation by exploiting the linearity of CS. However, the reconstruction process can be slow for large models, and therefore our solution is more scalable. Moreover, it can only compress the upstream traffic.</p><p>Bandwidth Optimization in Federated Learning: Different quantization methods have been proposed to save the bandwidth and reduce the communication costs in federated learning. They can be divided into two main groups: unbiased and biased methods. The unbiased approximation techniques use probabilistic quantization schemes to compress the stochastic gradient and attempt to approximate the true gradient value as much as possible <ref type="bibr" target="#b4">[Alistarh et al., 2016</ref><ref type="bibr" target="#b47">, Wen and al., 2017</ref><ref type="bibr">, Wang et al., 2018</ref><ref type="bibr" target="#b28">, Konecný et al., 2016]</ref>. However, biased approximations of the stochastic gradient can still guarantee convergence both in theory and practice <ref type="bibr" target="#b9">[Bernstein et al., 2018</ref><ref type="bibr" target="#b31">, Lin et al., 2018</ref><ref type="bibr" target="#b44">, Seide et al., 2014]</ref>. SignSGD <ref type="bibr" target="#b9">Bernstein et al. [2018]</ref> a quantization protocol allows to compress during downstream and upstream traffic but requires the use of all the clients at each round which is not realistic in the context of federated settings because each client is available only during few rounds <ref type="bibr" target="#b26">Kairouz et al. [2019]</ref>.</p><p>A different line of works exploit the sparsity of model updates to compress them. <ref type="bibr">Amiri and Gündüz [2019a,b]</ref> proposed to use a compressive sensing for federated learning in order to compress model updates without privacy guarantees. However, they assume that all clients participate in each round (as they maintain an error accumulation vector at each client due to the compression scheme), but as discussed in <ref type="bibr" target="#b26">Kairouz et al. [2019]</ref> this assumption is not always realistic. Sketching was adapted to federated learning for the purpose of compressing model updates in <ref type="bibr" target="#b25">Ivkin et al. [2019]</ref> and <ref type="bibr" target="#b43">Rothchild et al. [2020]</ref>. The authors proposed to use Count-Sketch from <ref type="bibr" target="#b12">Charikar et al. [2002]</ref> to retrieve the largest weights in the update vector on the server side. However, it is unclear how these works can be extended with privacy guarantees. Moreover, unlike our technique, they do not compress downstream traffic.</p><p>Constraining the weights to have a specific distribution has already been studied. In <ref type="bibr" target="#b23">Han et al. [2016]</ref>, for example, the authors use pruning techniques to create a sparse model at the end of the training. After each SGD iteration, the authors zero-out all the weights with an absolute value smaller than a threshold. Iterating the process leads to a sparse model with only some absolute weight values larger than 0. Similarly, <ref type="bibr" target="#b15">Courbariaux et al. [2016]</ref> aim to create a model with binary weights such that at the end of the training all the weights are close to 1 or -1. After each SGD update, the authors take the sign of the weights before the next update. After some iterations, the weight values become close to the interval limits -1 and 1.</p><p>In <ref type="bibr" target="#b20">Frankle and Carbin [2018]</ref>, a new hypothesis claims that there exists a sub-network which, if trained separately, can achieve similar performance as the complete network model which contains that. To find such a sub-network, one has to follow a simple iterative procedure: train the complete network, prune the smallest weights, and then reinitialize the remaining weights to their original values. These steps are repeated iteratively. This approach was extended to federated learning in <ref type="bibr" target="#b30">Li et al. [2020]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>This paper presents a novel privacy-preserving federated learning scheme that reduces bandwidth, latency and therefore power consumption. The proposed scheme is based on Differential Privacy and therefore provides theoretical privacy guarantees. Furthermore, it optimizes the model accuracy by constraining the model learning phase on few selected weights. We show experimentally, using a public dataset called Fashion-MNIST and a real world medical dataset of 1.2 millions of US hospital patients, that it reduces the upstream and downstream bandwidth by up to 99.9% compared to standard federated learning, making it practical for constrained and mobile devices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A MEDICAL DATA: DATA PRE-PROCESSING &amp; EXPERIMENTAL SETUP DETAILS</head><p>This section describes our medical dataset and the experimental setting which is used to evaluate the accuracy and the privacy of our proposals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 MEDICAL DATASET</head><p>A.1.1 The In-hospital Mortality Prediction Scenario</p><p>The ability to accurately predict the risks in the patient's perspectives of evolution is a crucial prerequisite in order to adapt the care that certain patients receive <ref type="bibr" target="#b19">[Fejza et al., 2018]</ref>.</p><p>We consider the scenario where several hospitals are collaborating to train models for in-hospital mortality prediction using our Federated Learning schemes. This well-studied real-world problem consists in trying to precisely identify the patients who are at risk of dying from complications during their hospital stay <ref type="bibr" target="#b7">[Avati et al., 2018</ref><ref type="bibr" target="#b42">, Rajkomar and al., 2018</ref><ref type="bibr" target="#b19">, Fejza et al., 2018]</ref>. As commonly found in the literature <ref type="bibr" target="#b19">[Fejza et al., 2018]</ref>, for such predictions, we focus on hospital admissions of adults hospitalized for at least 3 days, excluding elective admissions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1.2 The Premier Healthcare Database</head><p>We used EHR data from the Premier healthcare database<ref type="foot" target="#foot_4">7</ref> which is one of the largest clinical databases in the United States, collecting information from millions of patients over a period of 12 months from 415 hospitals in the USA <ref type="bibr" target="#b19">[Fejza et al., 2018]</ref>. These hospitals are supposedly representative of the United States hospital experience <ref type="bibr" target="#b19">[Fejza et al., 2018]</ref>. Each hospital in the database provides discharge files that are dated records of all billable items (including therapeutic and diagnostic procedures, medication, and laboratory usage) which are all linked to a given patient's admission <ref type="bibr">[Fejza et al., 2018, Makadia and</ref><ref type="bibr" target="#b33">Ryan, 2014]</ref>.</p><p>The initial snapshot of the database used in our work (before pre-processing step) comprises the EHR data of 1,271,733 hospital admissions. Electronic Health Record (EHR) is a digital version of a patient's paper chart readily available in hospitals. For developing supervised learning and specifically deep learning models, we focus on a specific set of features from EHR data. The features of interest that capture the patients information are summarized in Table <ref type="table" target="#tab_4">3</ref>. There is a total of 24,428 features per patient, mainly due to the variety of drugs possibly served. As in <ref type="bibr" target="#b7">Avati et al. [2018]</ref>,</p><p>we also removed all the features which appear on less than</p><p>The Medication regimen complexity index (MRCI) <ref type="bibr" target="#b34">[Mcdonald et al., 2012]</ref> is an aggregate score computed from a total of 65 items, whose purpose is to indicate the complexity of the patient's situation. The minimum MRCI score for a patient is 1.5, which represents a single tablet or capsule taken once a day as needed (single medication). However the maximum is not defined since the number of medications increases the score <ref type="bibr" target="#b34">[Mcdonald et al., 2012]</ref>. In our case, after statistical analysis of our dataset, we consider the MRCI score as ranging from 2 to 60.</p><p>Most real datasets like ours are generally imbalanced with a skewed distribution between the classes. In our case, the positive cases (patients who die during their hospital stay) represent only 3% of all patients. Table <ref type="table" target="#tab_5">4</ref> gives more details about this distribution after the pre-processing step which is discussed in A.2. To deal with this well-known problem, we have decided to use downsampling technique <ref type="bibr">[More, 2016, He and</ref><ref type="bibr" target="#b24">Garcia, 2009]</ref>, a standard solution used for this purpose, as used in <ref type="bibr" target="#b27">Kerkouche et al. [2020]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 PREPROCESSING</head><p>1. Features normalization: we extract from the dataset the values of each feature represented in Table <ref type="table" target="#tab_4">3</ref>. For gender, we use one-hot encoding: Male, Female and Unknown. Similarly, for admission type we use 4 features: Emergency, Urgent, Trauma Center, and Unknown<ref type="foot" target="#foot_5">8</ref> . For drugs, we extract 24,419 features which correspond to the different drugs (name and dosage). A given patient receives only a few of the possible drugs served, resulting in a very sparse patient's record. We use a MinMax normalization for age and MRCI in order to rescale the values of these features between 0 and 1 (using MinMaxScaler class of scikit-learn<ref type="foot" target="#foot_6">9</ref> ). The labels that we consider are boolean: true means that the patient died during his hospital stay while false means she survived.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Patients filtering:</head><p>We consider patient and drug information of the first day at the hospital so that we can make predictions 24 hours after admission (as commonly found in the literature <ref type="bibr" target="#b42">[Rajkomar and</ref><ref type="bibr" target="#b19">al., 2018, Fejza et al., 2018]</ref>). We filter out the pregnant and new-born patients because the medication types and admission services are not the same for theses two categories of patients. Our model prediction is built without patients' historical medical data. This has the advantage to require minimum patient's information and to work for new patients.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Hospitals filtering:</head><p>The dataset contains 415 hospitals for a total size of 1,271,733 records. We split randomly the dataset into disjoint training and testing data (80% and 20% respectively). The final dataset for testing contains 254,347 patients, with 7,882 deceased patients and 246,465 non-deceased patients (see Table <ref type="table" target="#tab_5">4</ref>). Using Client-Level differential privacy requires to add more noise than Record-Level differential privacy, because the privacy purposes are not the same as detailled in Section 2. To reduce the noise (when ε is fixed) and then improve the utility, we have to reduce the number of iterations or to reduce the sampling probability which are the parameters used to compute ε.</p><p>We therefore have two options to reduce the sampling probability:</p><p>-Reducing the number of clients selected at each round |K|. However this option also decreases the amount of data, and hence have a negative impact on the utility. We therefore preferred to use the next option. -Increasing the total number of clients N: we created more hospitals by splitting randomly the training data over 5010 "virtual" hospitals. We also, took care to have at least one in-hospital dead patient per hospital. Each hospital contains 203 patients. 356 patients are used as public dataset to define the Top-K updated weights. We created 5010 hospitals in order to have approximately the same number of patients per hospital, each of them with some in-hospital dead patients.</p><p>In practise, Client-Level differential privacy is more adapted to an environment with a large set of clients as explained in <ref type="bibr" target="#b36">McMahan et al. [2018]</ref>, <ref type="bibr" target="#b22">Geyer et al. [2017]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 IMBALANCED DATA</head><p>The dataset of each hospital is imbalanced because the proportion of patients that leave the hospital alive is, fortunately, much larger than in-hospital dead patients. To deal with this well-known problem, we have decided to use downsampling technique <ref type="bibr">[More, 2016, He and</ref><ref type="bibr" target="#b24">Garcia, 2009]</ref>, a standard solution used for this purpose. 10</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 PERFORMANCE METRICS</head><p>We use the following metrics:</p><p>10 We have also tested weighted loss function and oversampling techniques. But, we noticed experimentally that downsampling technique outperforms the other techniques for all the schemes.</p><p>• Balanced accuracy <ref type="bibr" target="#b11">[Brodersen et al., 2010</ref><ref type="bibr" target="#b8">, Bekkar et al., 2013]</ref> is computed as 1/2 • ( TP P + TN N ) =</p><p>TPR +TNR 2 and is mainly used with imbalanced data. True Positive Rate (TPR ) and True Negative Rate (TNR ): TPR = TP P and TNR = TN N , where P and N are the number of positive and negative instances, respectively, and TP and TN are the number of true positive and true negative instances. We note that traditional ("non-balanced") accuracy metrics such as TP +TN P +N can be misleading for very imbalanced data <ref type="bibr" target="#b3">Akosa [2017]</ref>: in our dataset, the minority class has only 3% of all the training samples (see Table <ref type="table" target="#tab_5">4</ref>), which means that a biased (and totally useless) model always predicting the majority class would have a (non-balanced) accuracy of 97%.</p><p>• The area under the ROC curve (AUROC ) is also a frequently used accuracy metric. The ROC curve is calculated by varying the prediction threshold from 1 to 0, when TPR and FPR are calculated at each threshold. The area under this curve is then used to measure the quality of the predictions. A random guess has an AUROC value of 0.5, whereas a perfect prediction has the largest AUROC value of 1.</p><p>A.5 EVALUATION METHOD.</p><p>First, we split randomly the dataset of each hospital into disjoint training and testing data (80% and 20% respectively).</p><p>An entire federated run is executed with this split, and all the metrics are evaluated in every round on the union of all clients' testing data. All metric values of the round with the best balanced metric are recorded.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.5.1 Model architecture</head><p>As in <ref type="bibr" target="#b7">Avati et al. [2018]</ref>, <ref type="bibr" target="#b27">Kerkouche et al. [2020]</ref>, we use a fully connected neural network model with the following architecture: two hidden layers of 200 units, which use a Relu activation function followed by an output layer of 1 unit with sigmoid activation function and a binary cross entropy loss function. This results in 1,496,601 parameters in total. We tune η from 0.01 to 0.5 with an increment value of 0.005. As in <ref type="bibr" target="#b27">Kerkouche et al. [2020]</ref>, we fix the momentum parameter ρ to 0.9 and the global learning rate η G to 1.0. The number of chunks is set to P = 100 (refers to <ref type="bibr" target="#b27">Kerkouche et al. [2020]</ref> for details). The hyperparameters used by each of the considered schemes are summarized in </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3 PREPROCESSING</head><p>The pixel of each image is an unsigned integer in the range between 0 and 255. We rescale them to the range [0,1] instead.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.4 MODEL ARCHITECTURE</head><p>For Fashion-MNIST, we use a model <ref type="bibr">McMahan et al. [2016]</ref>, <ref type="bibr" target="#b27">Kerkouche et al. [2020]</ref> with the following architecture: a convolutional neural network (CNN) with two 5x5 convolution layers (the first with 32 filters, the second with 64, each followed with 2x2 max pooling), a fully connected layer with 512 units and ReLu activation, and a final softmax output layer. This results in 1,663,370 parameters in total. We tune η from 0.01 to 0.5 with an increment value of 0.005. As in <ref type="bibr" target="#b27">Kerkouche et al. [2020]</ref>, we fix the momentum parameter ρ to 0.9 and the global learning rate η G to 0.35. Same for the number of chunks used P = 200 (refers to <ref type="bibr" target="#b27">Kerkouche et al. [2020]</ref> for more details). The hyperparameters used by each of the considered schemes are summarized in Table <ref type="table" target="#tab_3">5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C COMPUTATIONAL ENVIRONMENT</head><p>Our experiments were performed on a server running Ubuntu 18.04 LTS equipped with a Intel(R) Xeon(R) Silver 4114 CPU @ 2.20GHz, 192GB RAM, and two NVIDIA Quadro P5000 GPU card of 16 Go each. We use Keras 2.2.0 <ref type="bibr">Chollet et al. [2015a]</ref> with a TensorFlow backend 1.12.0 <ref type="bibr" target="#b0">Abadi et al. [2015]</ref> and Numpy 1.14. <ref type="bibr">3 Oliphant [2006]</ref> to implement our models and experiments. We use Python 3.6.5 and our code runs on a Docker container to simplify reproducibility.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D FURTHER EXPERIMENTS</head><p>The goal of this section is to compare the performance of our proposed schemes FL-TOP and FL-TOP-DP with several baselines according to different compression ratios. More specifically, we consider the following additional baselines:</p><p>• FL-BAS-2: As in FL-BASIC, only a randomly selected set of parameters are selected and sent to the server at each round. Importantly, none of the parameters are reinitialized during training.</p><p>• FL-BAS-3: This baseline is the same as FL-BASIC, except that the set of random parameters is fixed over all the rounds.</p><p>• FL-BAS-4: Same as FL-BAS-2, except that the set of random parameters is the same over all the rounds.</p><p>• FL-TOP-BIS: Similarly to FL-TOP, it uses the same Top-K parameters over the whole training. The only difference is that the n -K non-Top-K parameters are not re-initialized after each SGD iteration. As in FL-TOP, after T gd SGD iterations, clients send the update of the Top-K parameters to the server.</p><p>Note that all compression operators in the new baselines are still linear (just like FL-TOP-DP), and hence they can also be used with secure aggregation. Their private extensions (i.e., FL-BAS-2-DP, FL-BAS-3-DP, FL-BAS-4-DP and FL-TOP-BIS-DP) also clip and then noise the compressed updates as in FL-TOP-DP. The selection of sensitivity S happens similarly to FL-TOP-DP and FL-BASIC-DP using the public data as described in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.1 RESULTS</head><p>Table <ref type="table" target="#tab_9">8</ref> shows the best accuracy over 200 rounds for each scheme on the Fashion-MNIST dataset. Round corresponds to the round when the best accuracy is achieved and Cost is the average bandwidth consumption calculated as: r × n × 32 × Round ×C, where 32 is the number of bits necessary to represent a float value, n is the uncompressed model size, r = |T| n , |T| is the compressed model size, C is the sampling probability of a client, and Round is the round when we get the the best accuracy.</p><p>Table <ref type="table">9</ref> and Table <ref type="table" target="#tab_1">10</ref> display the best balanced accuracy over 100 rounds for each scheme on the Medical dataset. AUROC corresponds to the AUROC value when the best balanced accuracy is reached, Round is the round when we get the best balanced accuracy, and finally, Cost is the average bandwidth consumption calculated as for the Fashion-MNIST dataset described above.</p><p>On the medical data (see Table <ref type="table">9</ref> and 10), our schemes FL-TOP and FL-TOP-DP reach 0.64 of balanced accuracy and 0.70 of AUROC for r = 0.01%, while FL-TOP-Bis and FL-TOP-Bis-DP, which are the best baselines, have 8% less of balanced accuracy and 10% less of AUROC for identical compression ratios. Furthermore, for larger compression ratios, FL-TOP and FL-TOP-DP have similar results to that of FL-TOP-Bis and FL-TOP-Bis-DP. However, above r = 1%, FL-TOP outperforms FL-TOP-BIS. The same holds for FL-TOP-DP, which outperforms FL-TOP-Bis-DP when r is more than 0.05%.</p><p>On Fashion-MNIST, FL-TOP performs better than other schemes below r = 10%. For r = 10%, FL-CS and FL-TOP have the same accuracy of 0.85. FL-TOP-DP is the best DP scheme independently of the compression ratio r.</p><p>Notice the the larger the compression ratio r is the smaller the performance gap between our schemes and the baselines FL-BAS-1, FL-BAS-3. The same holds for their DP counterparts. This is mainly due to the fact that the larger r is the more likely that all schemes update the same Top-K parameters.</p><p>FL-CS and FL-CS-DP fail to improve their model accuracy when r = 0.01% on the medical dataset. The same holds for FL-BAS-3-DP when r = 0.1% on the Fashion-MNIST dataset.</p><p>On Fashion-MNIST, there is a decrease of accuracy for each of FL-TOP-DP, FL-TOP-BIS-DP and FL-CS-DP from r = 5% to r = 10%. Indeed, as suggested in <ref type="bibr" target="#b27">Kerkouche et al. [2020]</ref>, it may be due to the increase of sensitivity S which will also increase the noise and therefore its negative impact on convergence.       </p><formula xml:id="formula_18">k t = Client k (w t-1 ) 7 end 8 w t = w t-1 + ∑ k |D k | ∑ j |D j | ∆w k t end Output: Global model w t 10 11 Client k (w k t-1 ): 12 w k t = SGD(D k , w k t-1 , T gd ) Output: Model update (w k t -w k t-1 )<label>Algorithm</label></formula><formula xml:id="formula_19">∆ wk t = Client k (w t-1 ) 7 end 8 w t = w t-1 + 1 |K| ∑ k ∆ wk t 9 end 10 Client k (w k t-1 ): 11 ∆w k t = SGD(D k , w k t-1 , T gd ) -w k t-1 12 ∆ ŵk t = ∆w k t / max 1, ||∆w k t || 2 S Output: Enc K k (G (∆ ŵk t , SIσ / |K|))</formula></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>for all D, D differing in at most one record, where || • || p denotes the L p -norm.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Algorithm 1 :</head><label>1</label><figDesc>FL-TOP: Federated Learning Server: Initialize common model w 0 Select set T of Top-K updated weights' coordinates via public dataset for t = 1 to T cl do Select K clients uniformly at random</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Algorithm 2 :</head><label>2</label><figDesc>Top k -Stochastic Gradient Descent Input: D : training data, T gd : local epochs, w : weights, w 0 : first weights' initialization, T : set of Top-K values coordinates . 1 for t = 1 to T gd do 2 Select batch B from D randomly 3 u = -η∇ f (B; w) 4 for each coordinate i in T do 5 w[i] = w[i] + u[i] Model w rounds locally, and send them to the server for aggregation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Algorithm 3 :</head><label>3</label><figDesc>FL-TOP-DP: Federated Learning Server: Initialize common model w 0 Select set T of Top-K updated weights' coordinates via public dataset for t = 1 to T cl do Select K clients uniformly at random</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Distributions of the Top-K weight values (after convergence) for both FL-TOP and FL-STD schemes with the Fashion-MNIST dataset (left) and the medical dataset (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>1 to T cl do 4 Select K clients uniformly at random 5 for each client k in K do 6 ∆w</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>5: Stochastic Gradient Descent Input: D : training data, T gd : local epochs, w : weights 1 for t = 1 to T gd do 2 Select batch B from D randomly 3 w = w -η∇ f (B;</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fashion-MNIST dataset C</head><label>dataset</label><figDesc>= 1/60; N = 6000; T cl = 200; T gd = 5; |B| = 10; |D k | = 10; n = 1, 663, 370; δ = 10 -5 ; SGD(η = 0.215); η G = 0.35; ρ = 0.9; P = 200; σ = 1.54; T init = 5 Medical dataset C = 100/5010; N = 5010; T cl = 100; T gd = 40; n = 1, 496, 601; δ = 10 -5 ; SGD(η = 0.1); η G = 1.0; ρ = 0.9; P = 100; σ = 1.49; T init = 40</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 2</head><label>2</label><figDesc>represents the best balanced accuracy over 100 rounds for each scheme on the Medical dataset. AUROC (area under the receiver operating characteristic curve -see Appendix A.4) corresponds to the AUROC value when the best balanced accuracy is reached.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Summary of results on Fashion-MNIST dataset.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Performance</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Performance</cell><cell></cell><cell></cell></row><row><cell>r</cell><cell>Algorithms</cell><cell cols="2">Accuracy Round</cell><cell cols="2">Downstream Upstream Cost Cost</cell><cell>ε</cell><cell>r</cell><cell>Algorithms</cell><cell cols="3">Bal_Acc AUROC Round</cell><cell cols="2">Downstream Upstream Cost Cost</cell><cell>ε</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>(Kilobyte)</cell><cell>(Kilobyte)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>(Kilobyte)</cell><cell>(Kilobyte)</cell><cell></cell></row><row><cell></cell><cell>FL-BASIC</cell><cell>0.65</cell><cell>193</cell><cell>21402.03</cell><cell>107</cell><cell>N/A</cell><cell></cell><cell>FL-BASIC</cell><cell>0.51</cell><cell>0.51</cell><cell>99</cell><cell>11829.42</cell><cell>11.82</cell><cell>N/A</cell></row><row><cell></cell><cell>FL-CS</cell><cell>0.57</cell><cell>185</cell><cell>20514.9</cell><cell>102.56</cell><cell>N/A</cell><cell></cell><cell>FL-CS</cell><cell>0.53</cell><cell>0.55</cell><cell>100</cell><cell>11948.91</cell><cell>11.94</cell><cell>N/A</cell></row><row><cell></cell><cell>FL-TOP</cell><cell>0.82</cell><cell>200</cell><cell>110.88</cell><cell>110.88</cell><cell>N/A</cell><cell></cell><cell>FL-TOP</cell><cell>0.69</cell><cell>0.76</cell><cell>68</cell><cell>8.12</cell><cell>8.12</cell><cell>N/A</cell></row><row><cell>0.5%</cell><cell>FL-BASIC-DP FL-CS-DP</cell><cell>0.59 0.53</cell><cell>200 200</cell><cell>22178.27 22178.27</cell><cell>110.88 110.88</cell><cell>1 1</cell><cell>0.1%</cell><cell>FL-BASIC-DP FL-CS-DP</cell><cell>0.50 0.51</cell><cell>0.49 0.51</cell><cell>100 99</cell><cell>11948.91 11829.42</cell><cell>11.94 11.82</cell><cell>1 1</cell></row><row><cell></cell><cell>FL-TOP-DP</cell><cell>0.81</cell><cell>200</cell><cell>110.88</cell><cell>110.88</cell><cell>1</cell><cell></cell><cell>FL-TOP-DP</cell><cell>0.69</cell><cell>0.76</cell><cell>85</cell><cell>10.15</cell><cell>10.15</cell><cell>0.97</cell></row><row><cell></cell><cell>FL-BASIC</cell><cell>0.78</cell><cell>196</cell><cell>21734.70</cell><cell>1086.73</cell><cell>N/A</cell><cell></cell><cell>FL-BASIC</cell><cell>0.72</cell><cell>0.80</cell><cell>100</cell><cell>11948.91</cell><cell>597.45</cell><cell>N/A</cell></row><row><cell></cell><cell>FL-CS</cell><cell>0.82</cell><cell>200</cell><cell>22178.27</cell><cell>1108.91</cell><cell>N/A</cell><cell></cell><cell>FL-CS</cell><cell>0.73</cell><cell>0.81</cell><cell>98</cell><cell>11709.93</cell><cell>585.5</cell><cell>N/A</cell></row><row><cell></cell><cell>FL-TOP</cell><cell>0.84</cell><cell>200</cell><cell>1108.91</cell><cell>1108.91</cell><cell>N/A</cell><cell></cell><cell>FL-TOP</cell><cell>0.72</cell><cell>0.80</cell><cell>95</cell><cell>567.57</cell><cell>567.57</cell><cell>N/A</cell></row><row><cell>5%</cell><cell>FL-BASIC-DP FL-CS-DP</cell><cell>0.76 0.78</cell><cell>195 160</cell><cell>21623.81 17742.61</cell><cell>1081.18 887.13</cell><cell>0.99 0.94</cell><cell>5%</cell><cell>FL-BASIC-DP FL-CS-DP</cell><cell>0.69 0.69</cell><cell>0.76 0.76</cell><cell>100 100</cell><cell>11948.91 11948.91</cell><cell>597.45 597.45</cell><cell>1 1</cell></row><row><cell></cell><cell>FL-TOP-DP</cell><cell>0.81</cell><cell>152</cell><cell>842.77</cell><cell>842.77</cell><cell>0.92</cell><cell></cell><cell>FL-TOP-DP</cell><cell>0.68</cell><cell>0.75</cell><cell>23</cell><cell>137.41</cell><cell>137.41</cell><cell>0.79</cell></row><row><cell></cell><cell>FL-BASIC</cell><cell>0.81</cell><cell>196</cell><cell>21734.70</cell><cell>2173.47</cell><cell>N/A</cell><cell></cell><cell>FL-BASIC</cell><cell>0.74</cell><cell>0.81</cell><cell>100</cell><cell>11948.91</cell><cell>1194.89</cell><cell>N/A</cell></row><row><cell></cell><cell>FL-CS</cell><cell>0.85</cell><cell>182</cell><cell>20182.22</cell><cell>2018.22</cell><cell>N/A</cell><cell></cell><cell>FL-CS</cell><cell>0.74</cell><cell>0.82</cell><cell>100</cell><cell>11948.91</cell><cell>1194.89</cell><cell>N/A</cell></row><row><cell></cell><cell>FL-TOP</cell><cell>0.85</cell><cell>199</cell><cell>2206.74</cell><cell>2206.74</cell><cell>N/A</cell><cell></cell><cell>FL-TOP</cell><cell>0.74</cell><cell>0.82</cell><cell>90</cell><cell>1075.40</cell><cell>1075.40</cell><cell>N/A</cell></row><row><cell>10%</cell><cell>FL-BASIC-DP FL-CS-DP</cell><cell>0.79 0.72</cell><cell>189 167</cell><cell>20958.46 18518.85</cell><cell>2095.85 1851.89</cell><cell>0.98 0.95</cell><cell>10%</cell><cell>FL-BASIC-DP FL-CS-DP</cell><cell>0.69 0.69</cell><cell>0.76 0.76</cell><cell>99 96</cell><cell>11829.42 11470.95</cell><cell>1182.94 1147.09</cell><cell>1 0.99</cell></row><row><cell></cell><cell>FL-TOP-DP</cell><cell>0.80</cell><cell>157</cell><cell>1740.99</cell><cell>1740.99</cell><cell>0.93</cell><cell></cell><cell>FL-TOP-DP</cell><cell>0.68</cell><cell>0.74</cell><cell>23</cell><cell>274.82</cell><cell>274.82</cell><cell>0.79</cell></row><row><cell>100%</cell><cell>FL-STD FL-STD-DP</cell><cell>0.86 0.56</cell><cell>200 60</cell><cell>22178.27 6653.48</cell><cell cols="2">22178.27 N/A 6653.48 0.76</cell><cell>100%</cell><cell>FL-STD FL-STD-DP</cell><cell>0.74 0.66</cell><cell>0.82 0.72</cell><cell>99 62</cell><cell>11829.42 7408.32</cell><cell cols="2">11829.42 N/A 7408.32 0.91</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Summary of results on Medical dataset.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 5 .</head><label>5</label><figDesc></figDesc><table><row><cell>B FASHION-MNIST DATA: DATA</cell></row><row><cell>PRE-PROCESSING &amp;</cell></row><row><cell>EXPERIMENTAL SETUP DETAILS</cell></row><row><cell>B.1 DATA DESCRIPTION</cell></row><row><cell>Fashion-MNIST database of fashion articles consists of</cell></row><row><cell>60,000 28x28 grayscale images of 10 fashion categories,</cell></row><row><cell>along with a test set of 10,000 images Xiao et al. [2017]</cell></row><row><cell>Chollet et al. [2015b].</cell></row><row><cell>B.2 PUBLIC DATA DESCRIPTION</cell></row><row><cell>The MNIST database of handwritten digits. It consists of</cell></row><row><cell>28 x 28 grayscale images of digit items and has 10 output</cell></row><row><cell>classes. The training set contains 60,000 data samples while</cell></row><row><cell>the test/validation set has 10,000 samples LeCun and Cortes</cell></row><row><cell>[2010] Chollet et al. [2015b].</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Descriptions of featuresFeatures DescriptionsAge Value in the range of 15 and 89 Gender Male, Female or Unknown Admission type Emergency, Urgent, Trauma Center: visits to a trauma center/hospital or Unknown MRCI Medication regimen complexity index score (ranging from 2 to 60)Drugs and ICD9 codesDrugs given to the patient on the 1 st day of hospitalization. The ICD9 codes are composed of procedures and diagnosis codes, the first gives details about the medical procedures performed on the patient and the second about the doctor's diagnosis of the patient. There is a total of 24,419 possible drugs and ICD9 codes<ref type="bibr" target="#b16">[CUADRADO, 2019]</ref>.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Number of instances for our case study. The Medical dataset contains in total 1,271,733 records.</figDesc><table><row><cell cols="4">Data Positive cases Negative cases Ratio</cell><cell>Total</cell></row><row><cell>Train</cell><cell>32,106</cell><cell>985,280</cell><cell cols="2">3.16% 1,017,386</cell></row><row><cell>Test</cell><cell>7,882</cell><cell>246,465</cell><cell>3.10%</cell><cell>254,347</cell></row><row><cell cols="2">Datasets</cell><cell></cell><cell cols="2">Common Parameters</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Common environment between the schemes. ρ, η G and P are only used with FL-CS and FL-CS-DP.</figDesc><table><row><cell>Algorithms</cell><cell cols="3">Compression ratio (r) 0.1% 0.5% 1% 5% 10%</cell></row><row><cell>FL-BASIC-DP</cell><cell>0.05</cell><cell cols="2">0.12 0.16 0.34 0.45</cell></row><row><cell>FL-BAS-2-DP</cell><cell>0.07</cell><cell cols="2">0.16 0.23 0.52 0.75</cell></row><row><cell>FL-BAS-3-DP</cell><cell>0.05</cell><cell cols="2">0.11 0.16 0.33 0.44</cell></row><row><cell>FL-BAS-4-DP</cell><cell>0.06</cell><cell cols="2">0.15 0.21 0.51 0.74</cell></row><row><cell>FL-CS-DP</cell><cell>0.21</cell><cell cols="2">0.26 0.32 0.57 0.79</cell></row><row><cell cols="2">FL-TOP-BIS-DP 1.25</cell><cell cols="2">1.59 1.79 2.18 2.34</cell></row><row><cell>FL-TOP-DP</cell><cell>0.50</cell><cell>0.61 0.64 0.87</cell><cell>1.0</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>Sensitivity S used for each scheme and for different compression ratio r on Fashion-MNIST. For FL-STD-DP, S is set to 2.40.</figDesc><table><row><cell>Algorithms</cell><cell cols="4">Compression ratio (r) 0.01% 0.05% 0.1% 0.5% 1%</cell><cell>5% 10%</cell></row><row><cell>FL-BASIC-DP</cell><cell>0.01</cell><cell>0.03</cell><cell>0.05</cell><cell cols="2">0.11 0.16 0.34 0.46</cell></row><row><cell>FL-BAS-2-DP</cell><cell>0.01</cell><cell>0.03</cell><cell>0.04</cell><cell cols="2">0.09 0.14 0.31 0.44</cell></row><row><cell>FL-BAS-3-DP</cell><cell>0.01</cell><cell>0.04</cell><cell>0.06</cell><cell cols="2">0.12 0.18 0.35 0.49</cell></row><row><cell>FL-BAS-4-DP</cell><cell>0.02</cell><cell>0.03</cell><cell>0.05</cell><cell cols="2">0.12 0.15 0.31 0.44</cell></row><row><cell>FL-CS-DP</cell><cell>0.002</cell><cell cols="4">0.005 0.006 0.01 0.02 0.04 0.06</cell></row><row><cell>FL-TOP-BIS-DP</cell><cell>0.60</cell><cell>0.73</cell><cell>0.81</cell><cell cols="2">1.03 1.13 1.31 1.32</cell></row><row><cell>FL-TOP-DP</cell><cell>0.23</cell><cell>0.46</cell><cell>0.59</cell><cell cols="2">1.03 1.18 1.31 1.32</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7 :</head><label>7</label><figDesc>Sensitivity S used for each scheme and for different compression ratio r on the medical dataset. For FL-STD-DP, S is set to 1.40.</figDesc><table><row><cell>Compression ratio (r)</cell><cell>Algorithms</cell><cell cols="4">Performance Accuracy Round Downstream Cost (Kilobyte) Upstream Cost (Kilobyte)</cell><cell>ε</cell></row><row><cell></cell><cell>FL-BASIC</cell><cell>0.14</cell><cell>111</cell><cell>12308.94</cell><cell>12.31</cell><cell>N/A</cell></row><row><cell></cell><cell>FL-BAS-2</cell><cell>0.16</cell><cell>185</cell><cell>20514.9</cell><cell>20.51</cell><cell>N/A</cell></row><row><cell></cell><cell>FL-BAS-3</cell><cell>0.27</cell><cell>200</cell><cell>22.17</cell><cell>22.17</cell><cell>N/A</cell></row><row><cell></cell><cell>FL-BAS-4</cell><cell>0.17</cell><cell>200</cell><cell>22.17</cell><cell>22.17</cell><cell>N/A</cell></row><row><cell></cell><cell>FL-CS</cell><cell>0.37</cell><cell>200</cell><cell>22178.27</cell><cell>22.17</cell><cell>N/A</cell></row><row><cell></cell><cell>FL-TOPK-BIS</cell><cell>0.59</cell><cell>198</cell><cell>21.95</cell><cell>21.95</cell><cell>N/A</cell></row><row><cell>0.1%</cell><cell>FL-TOP FL-BASIC-DP</cell><cell>0.78 0.14</cell><cell>199 167</cell><cell>22.06 18518.85</cell><cell>22.06 18.51</cell><cell>N/A 0.95</cell></row><row><cell></cell><cell>FL-BAS-2-DP</cell><cell>0.14</cell><cell>124</cell><cell>13750.53</cell><cell>13.75</cell><cell>0.88</cell></row><row><cell></cell><cell>FL-BAS-3-DP</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>FL-BAS-4-DP</cell><cell>0.15</cell><cell>137</cell><cell>15.19</cell><cell>15.19</cell><cell>0.90</cell></row><row><cell></cell><cell>FL-CS-DP</cell><cell>0.36</cell><cell>197</cell><cell>21845.59</cell><cell>21.84</cell><cell>1</cell></row><row><cell></cell><cell>FL-TOPK-BIS-DP</cell><cell>0.59</cell><cell>196</cell><cell>21.73</cell><cell>21.73</cell><cell>0.99</cell></row><row><cell></cell><cell>FL-TOP-DP</cell><cell>0.76</cell><cell>199</cell><cell>22.06</cell><cell>22.06</cell><cell>1</cell></row><row><cell></cell><cell>FL-BASIC</cell><cell>0.65</cell><cell>193</cell><cell>21402.03</cell><cell>107</cell><cell>N/A</cell></row><row><cell></cell><cell>FL-BAS-2</cell><cell>0.46</cell><cell>196</cell><cell>21734.70</cell><cell>108.66</cell><cell>N/A</cell></row><row><cell></cell><cell>FL-BAS-3</cell><cell>0.73</cell><cell>200</cell><cell>110.88</cell><cell>110.88</cell><cell>N/A</cell></row><row><cell></cell><cell>FL-BAS-4</cell><cell>0.41</cell><cell>197</cell><cell>109.22</cell><cell>109.22</cell><cell>N/A</cell></row><row><cell></cell><cell>FL-CS</cell><cell>0.57</cell><cell>185</cell><cell>20514.9</cell><cell>102.56</cell><cell>N/A</cell></row><row><cell></cell><cell>FL-TOPK-BIS</cell><cell>0.76</cell><cell>200</cell><cell>110.88</cell><cell>110.88</cell><cell>N/A</cell></row><row><cell>0.5%</cell><cell>FL-TOP FL-BASIC-DP</cell><cell>0.82 0.59</cell><cell>200 200</cell><cell>110.88 22178.27</cell><cell>110.88 110.88</cell><cell>N/A 1</cell></row><row><cell></cell><cell>FL-BAS-2-DP</cell><cell>0.38</cell><cell>200</cell><cell>22178.27</cell><cell>110.88</cell><cell>1</cell></row><row><cell></cell><cell>FL-BAS-3-DP</cell><cell>0.56</cell><cell>200</cell><cell>110.88</cell><cell>110.88</cell><cell>1</cell></row><row><cell></cell><cell>FL-BAS-4-DP</cell><cell>0.33</cell><cell>200</cell><cell>110.88</cell><cell>110.88</cell><cell>1</cell></row><row><cell></cell><cell>FL-CS-DP</cell><cell>0.53</cell><cell>200</cell><cell>22178.27</cell><cell>110.88</cell><cell>1</cell></row><row><cell></cell><cell>FL-TOPK-BIS-DP</cell><cell>0.68</cell><cell>184</cell><cell>102.01</cell><cell>102.01</cell><cell>0.97</cell></row><row><cell></cell><cell>FL-TOP-DP</cell><cell>0.81</cell><cell>200</cell><cell>110.88</cell><cell>110.88</cell><cell>1</cell></row><row><cell></cell><cell>FL-BASIC</cell><cell>0.71</cell><cell>194</cell><cell>21512.92</cell><cell>215.12</cell><cell>N/A</cell></row><row><cell></cell><cell>FL-BAS-2</cell><cell>0.59</cell><cell>200</cell><cell>22178.27</cell><cell>221.77</cell><cell>N/A</cell></row><row><cell></cell><cell>FL-BAS-3</cell><cell>0.76</cell><cell>200</cell><cell>221.77</cell><cell>221.77</cell><cell>N/A</cell></row><row><cell></cell><cell>FL-BAS-4</cell><cell>0.56</cell><cell>195</cell><cell>216.23</cell><cell>216.23</cell><cell>N/A</cell></row><row><cell></cell><cell>FL-CS</cell><cell>0.69</cell><cell>200</cell><cell>22178.27</cell><cell>221.77</cell><cell>N/A</cell></row><row><cell></cell><cell>FL-TOPK-BIS</cell><cell>0.79</cell><cell>197</cell><cell>218.45</cell><cell>218.45</cell><cell>N/A</cell></row><row><cell>1%</cell><cell>FL-TOP FL-BASIC-DP</cell><cell>0.83 0.65</cell><cell>200 197</cell><cell>221.77 21845.59</cell><cell>221.77 218.45</cell><cell>N/A 1</cell></row><row><cell></cell><cell>FL-BAS-2-DP</cell><cell>0.62</cell><cell>198</cell><cell>21956.48</cell><cell>219.56</cell><cell>1</cell></row><row><cell></cell><cell>FL-BAS-3-DP</cell><cell>0.66</cell><cell>198</cell><cell>219.56</cell><cell>219.56</cell><cell>1</cell></row><row><cell></cell><cell>FL-BAS-4-DP</cell><cell>0.52</cell><cell>198</cell><cell>219.56</cell><cell>219.56</cell><cell>1</cell></row><row><cell></cell><cell>FL-CS-DP</cell><cell>0.66</cell><cell>189</cell><cell>20958.46</cell><cell>209.58</cell><cell>0.98</cell></row><row><cell></cell><cell>FL-TOPK-BIS-DP</cell><cell>0.70</cell><cell>174</cell><cell>192.94</cell><cell>192.94</cell><cell>0.96</cell></row><row><cell></cell><cell>FL-TOP-DP</cell><cell>0.81</cell><cell>183</cell><cell>202.92</cell><cell>202.92</cell><cell>0.97</cell></row><row><cell></cell><cell>FL-BASIC</cell><cell>0.78</cell><cell>196</cell><cell>21734.70</cell><cell>1086.73</cell><cell>N/A</cell></row><row><cell></cell><cell>FL-BAS-2</cell><cell>0.72</cell><cell>199</cell><cell>22067.38</cell><cell>1103.36</cell><cell>N/A</cell></row><row><cell></cell><cell>FL-BAS-3</cell><cell>0.81</cell><cell>199</cell><cell>1103.36</cell><cell>1103.36</cell><cell>N/A</cell></row><row><cell></cell><cell>FL-BAS-4</cell><cell>0.76</cell><cell>196</cell><cell>1086.73</cell><cell>1086.73</cell><cell>N/A</cell></row><row><cell></cell><cell>FL-CS</cell><cell>0.82</cell><cell>200</cell><cell>22178.27</cell><cell>1108.91</cell><cell>N/A</cell></row><row><cell></cell><cell>FL-TOPK-BIS</cell><cell>0.83</cell><cell>196</cell><cell>1086.73</cell><cell>1086.73</cell><cell>N/A</cell></row><row><cell>5%</cell><cell>FL-TOP FL-BASIC-DP</cell><cell>0.84 0.76</cell><cell>200 195</cell><cell>1108.91 21623.81</cell><cell>1108.91 1081.18</cell><cell>N/A 0.99</cell></row><row><cell></cell><cell>FL-BAS-2-DP</cell><cell>0.72</cell><cell>195</cell><cell>21623.81</cell><cell>1081.18</cell><cell>0.99</cell></row><row><cell></cell><cell>FL-BAS-3-DP</cell><cell>0.76</cell><cell>199</cell><cell>1103.36</cell><cell>1103.36</cell><cell>1</cell></row><row><cell></cell><cell>FL-BAS-4-DP</cell><cell>0.75</cell><cell>191</cell><cell>1059.01</cell><cell>1059.01</cell><cell>0.99</cell></row><row><cell></cell><cell>FL-CS-DP</cell><cell>0.78</cell><cell>160</cell><cell>17742.61</cell><cell>887.13</cell><cell>0.94</cell></row><row><cell></cell><cell>FL-TOPK-BIS-DP</cell><cell>0.71</cell><cell>152</cell><cell>842.77</cell><cell>842.77</cell><cell>0.92</cell></row><row><cell></cell><cell>FL-TOP-DP</cell><cell>0.81</cell><cell>152</cell><cell>842.77</cell><cell>842.77</cell><cell>0.92</cell></row><row><cell></cell><cell>FL-BASIC</cell><cell>0.81</cell><cell>196</cell><cell>21734.70</cell><cell>2173.47</cell><cell>N/A</cell></row><row><cell></cell><cell>FL-BAS-2</cell><cell>0.78</cell><cell>199</cell><cell>22067.38</cell><cell>2206.74</cell><cell>N/A</cell></row><row><cell></cell><cell>FL-BAS-3</cell><cell>0.82</cell><cell>195</cell><cell>2162.38</cell><cell>2162.38</cell><cell>N/A</cell></row><row><cell></cell><cell>FL-BAS-4</cell><cell>0.79</cell><cell>200</cell><cell>2217.83</cell><cell>2217.83</cell><cell>N/A</cell></row><row><cell></cell><cell>FL-CS</cell><cell>0.85</cell><cell>182</cell><cell>20182.22</cell><cell>2018.22</cell><cell>N/A</cell></row><row><cell></cell><cell>FL-TOPK-BIS</cell><cell>0.84</cell><cell>196</cell><cell>2173.47</cell><cell>2173.47</cell><cell>N/A</cell></row><row><cell>10%</cell><cell>FL-TOP FL-BASIC-DP</cell><cell>0.85 0.79</cell><cell>199 189</cell><cell>2206.74 20958.46</cell><cell>2206.74 2095.85</cell><cell>N/A 0.98</cell></row><row><cell></cell><cell>FL-BAS-2-DP</cell><cell>0.77</cell><cell>189</cell><cell>20958.46</cell><cell>2095.85</cell><cell>0.98</cell></row><row><cell></cell><cell>FL-BAS-3-DP</cell><cell>0.79</cell><cell>183</cell><cell>2029.31</cell><cell>2029.31</cell><cell>0.97</cell></row><row><cell></cell><cell>FL-BAS-4-DP</cell><cell>0.78</cell><cell>195</cell><cell>2162.38</cell><cell>2162.38</cell><cell>0.99</cell></row><row><cell></cell><cell>FL-CS-DP</cell><cell>0.72</cell><cell>167</cell><cell>18518.85</cell><cell>1851.89</cell><cell>0.95</cell></row><row><cell></cell><cell>FL-TOPK-BIS-DP</cell><cell>0.69</cell><cell>138</cell><cell>1530.30</cell><cell>1530.30</cell><cell>0.90</cell></row><row><cell></cell><cell>FL-TOP-DP</cell><cell>0.80</cell><cell>157</cell><cell>1740.99</cell><cell>1740.99</cell><cell>0.93</cell></row><row><cell>100%</cell><cell>FL-STD FL-STD-DP</cell><cell>0.86 0.56</cell><cell>200 60</cell><cell>22178.27 6653.48</cell><cell>22178.27 6653.48</cell><cell>N/A 0.76</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 8 :</head><label>8</label><figDesc>Summary of results on Fashion-MNIST dataset.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>weight values for downstream and update/gradients for upstream traffic</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>More precisely, ∑ i G (ν i , ξ i ) = G (∑ i ν i , ∑ i ξ 2 i )</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>More baselines are considered but due to the lack of space, we have decided to present only those which return the best results. All other results can be found in the appendix( Section D).</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_3"><p>See Appendix A.4 for more details.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_4"><p>https://www.premierinc.com/newsroom/education/premierhealthcare-database-whitepaper 100 patients' records, hence, the number of features was reduced to 7,280 features.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_5"><p>https://www.resdac.org/cms-data/ variables/claim-inpatient-admission-typecode-ffs</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_6"><p>9 https://scikit-learn.org/ stable/modules/generated/ sklearn.preprocessing.MinMaxScaler.html</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">TensorFlow: Large-scale machine learning on heterogeneous systems</title>
		<author>
			<persName><forename type="first">Martín</forename><surname>Abadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Software available from tensorflow</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Deep learning with differential privacy</title>
		<author>
			<persName><forename type="first">Martin</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">Brendan</forename><surname>Mcmahan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Mironov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kunal</forename><surname>Talwar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM CCS</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">I have a dream! (differentially private smart metering)</title>
		<author>
			<persName><forename type="first">Gergely</forename><surname>Ács</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Claude</forename><surname>Castelluccia</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
			<publisher>IH</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Predictive accuracy: a misleading performance measure for highly imbalanced data</title>
		<author>
			<persName><forename type="first">Josephine</forename><surname>Akosa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the SAS Global Forum</title>
		<meeting>the SAS Global Forum</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">QSGD: randomized quantization for communicationoptimal stochastic gradient descent</title>
		<author>
			<persName><forename type="first">Dan</forename><surname>Alistarh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jerry</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryota</forename><surname>Tomioka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Milan</forename><surname>Vojnovic</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Machine learning at the wireless edge: Distributed stochastic gradient descent over-the-air</title>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Mohammadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amiri</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Deniz</forename><surname>Gündüz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Federated learning over wireless fading channels</title>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Mohammadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amiri</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Deniz</forename><surname>Gündüz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Improving palliative care with deep learning</title>
		<author>
			<persName><forename type="first">Anand</forename><surname>Avati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenneth</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephanie</forename><surname>Harman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lance</forename><surname>Downing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Nigam</surname></persName>
		</author>
		<author>
			<persName><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC Medical Informatics and Decision Making</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Evaluation measures for models assessment over imbalanced data sets</title>
		<author>
			<persName><forename type="first">Mohamed</forename><surname>Bekkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hassiba</forename><surname>Djema</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">A</forename><surname>Alitouche</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Information Engineering and Applications</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="27" to="38" />
			<date type="published" when="2013-01">01 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">signsgd: compressed optimisation for non-convex problems</title>
		<author>
			<persName><forename type="first">Jeremy</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu-Xiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kamyar</forename><surname>Azizzadenesheli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anima</forename><surname>Anandkumar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Practical secure aggregation for federated learning on user-held data</title>
		<author>
			<persName><forename type="first">Keith</forename><surname>Bonawitz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The balanced accuracy and its posterior distribution</title>
		<author>
			<persName><forename type="first">Kay</forename><surname>Henning Brodersen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soon</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Klaas</forename><surname>Ong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joachim</forename><forename type="middle">M</forename><surname>Enno Stephan</surname></persName>
		</author>
		<author>
			<persName><surname>Buhmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">20th International Conference on Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Finding frequent items in data streams</title>
		<author>
			<persName><forename type="first">Moses</forename><surname>Charikar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Farach-Colton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Colloquium on Automata, Languages, and Programming</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="693" to="703" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">François</forename><surname>Chollet</surname></persName>
		</author>
		<ptr target="https://keras.io" />
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><forename type="first">François</forename><surname>Chollet</surname></persName>
		</author>
		<ptr target="https://keras.io/datasets/" />
		<title level="m">Keras datasets</title>
		<imprint>
			<date type="published" when="2015">2015b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Binarized neural networks: Training deep neural networks with weights and activations constrained to +1 or -1</title>
		<author>
			<persName><forename type="first">Matthieu</forename><surname>Courbariaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Itay</forename><surname>Hubara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Soudry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ran</forename><surname>El-Yaniv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName><forename type="first">Terron</forename><surname>Marta</surname></persName>
		</author>
		<author>
			<persName><surname>Cuadrado</surname></persName>
		</author>
		<ptr target="https://ec.europa.eu/cefdigital/wiki/display/EHSEMANTIC/ICD-9-CM%3A+International+Classification+of+Diseases%2C+Ninth+Revision%2C+Clinical+Modification" />
		<title level="m">Icd-9-cm: International classification of diseases, ninth revision, clinical modification</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">The Algorithmic Foundations of Differential Privacy</title>
		<author>
			<persName><forename type="first">Cynthia</forename><surname>Dwork</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foundations and Trends in Theoretical Computer Science</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">3-4</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">RAPPOR: randomized aggregatable privacy-preserving ordinal response</title>
		<author>
			<persName><forename type="first">Úlfar</forename><surname>Erlingsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vasyl</forename><surname>Pihur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksandra</forename><surname>Korolova</surname></persName>
		</author>
		<idno type="DOI">10.1145/2660267.2660348</idno>
		<ptr target="https://doi.org/10.1145/2660267.2660348" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 ACM SIGSAC Conference on Computer and Communications Security</title>
		<editor>
			<persName><forename type="first">Gail-Joon</forename><surname>Ahn</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Moti</forename><surname>Yung</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Ninghui</forename><surname>Li</surname></persName>
		</editor>
		<meeting>the 2014 ACM SIGSAC Conference on Computer and Communications Security<address><addrLine>Scottsdale, AZ, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014">November 3-7, 2014. 2014</date>
			<biblScope unit="page" from="1054" to="1067" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Scalable and interpretable predictive models for electronic health records</title>
		<author>
			<persName><forename type="first">A</forename><surname>Fejza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Genevès</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Layaïda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bosson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">DSAA</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">The lottery ticket hypothesis: Training pruned neural networks</title>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Frankle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Carbin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Inverting gradients -how easy is it to break privacy in federated learning?</title>
		<author>
			<persName><forename type="first">Jonas</forename><surname>Geiping</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hartmut</forename><surname>Bauermeister</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannah</forename><surname>Dröge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Moeller</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Differentially private federated learning: A client level perspective</title>
		<author>
			<persName><forename type="first">Robin</forename><forename type="middle">C</forename><surname>Geyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tassilo</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moin</forename><surname>Nabi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Dsd: Dense-sparse-dense training for deep neural networks</title>
		<author>
			<persName><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Pool</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huizi</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Enhao</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shijian</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erich</forename><surname>Elsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Vajda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Catanzaro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">J</forename><surname>Dally</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning from imbalanced data</title>
		<author>
			<persName><forename type="first">Haibo</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edwardo</forename><forename type="middle">A</forename><surname>Garcia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on knowledge and data engineering</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Communication-efficient distributed sgd with sketching</title>
		<author>
			<persName><forename type="first">Nikita</forename><surname>Ivkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Rothchild</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Enayat</forename><surname>Ullah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ion</forename><surname>Stoica</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raman</forename><surname>Arora</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="13144" to="13154" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Advances and open problems in federated learning</title>
		<author>
			<persName><forename type="first">Peter</forename><surname>Kairouz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Compression boosts differentially private federated learning</title>
		<author>
			<persName><forename type="first">Raouf</forename><surname>Kerkouche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gergely</forename><surname>Ács</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Claude</forename><surname>Castelluccia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Genevès</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note>To appear in EuroS&amp;P 2021</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Federated learning: Strategies for improving communication efficiency</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">Brendan</forename><surname>Jakub Konecný</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felix</forename><forename type="middle">X</forename><surname>Mcmahan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ananda</forename><surname>Richtárik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dave</forename><surname>Theertha Suresh</surname></persName>
		</author>
		<author>
			<persName><surname>Bacon</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">MNIST handwritten digit database</title>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Corinna</forename><surname>Cortes</surname></persName>
		</author>
		<ptr target="http://yann.lecun.com/exdb/mnist/" />
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Lotteryfl: Personalized and communication-efficient federated learning with lottery ticket hypothesis on non-iid datasets</title>
		<author>
			<persName><forename type="first">Ang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingwei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Binghui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lin</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sicheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiran</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hai</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Deep gradient compression: Reducing the communication bandwidth for distributed training</title>
		<author>
			<persName><forename type="first">Yujun</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huizi</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bill</forename><surname>Dally</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Fedsel: Federated sgd under local differential privacy with top-k dimension selection</title>
		<author>
			<persName><forename type="first">Ruixuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Masatoshi</forename><surname>Yoshikawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hong</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Lecture Notes in Computer Science</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Transforming the premier perspective® hospital database into the observational medical outcomes partnership (omop) common data model</title>
		<author>
			<persName><forename type="first">Rupa</forename><surname>Makadia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><forename type="middle">B</forename><surname>Ryan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EGEMS</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Automating the medication regimen complexity index</title>
		<author>
			<persName><forename type="first">Margaret</forename><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sridevi</forename><surname>Sridharan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Janice</forename><surname>Foust</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Polina</forename><surname>Kogan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liliana</forename><surname>Pezzin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Penny</forename><surname>Feldman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Medical Informatics Association</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<publisher>JAMIA</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Communicationefficient learning of deep networks from decentralized data</title>
		<author>
			<persName><forename type="first">H</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Brendan</forename><surname>Mcmahan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eider</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Ramage</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seth</forename><surname>Hampson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Blaise</forename><surname>Agüera Y Arcas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AISTATS</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Learning differentially private recurrent language models</title>
		<author>
			<persName><forename type="first">H</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Brendan</forename><surname>Mcmahan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Ramage</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kunal</forename><surname>Talwar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Inference attacks against collaborative learning</title>
		<author>
			<persName><forename type="first">Luca</forename><surname>Melis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Congzheng</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emiliano</forename><surname>De Cristofaro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vitaly</forename><surname>Shmatikov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Rényi differential privacy of the sampled gaussian mechanism</title>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Mironov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kunal</forename><surname>Talwar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Survey of resampling techniques for improving classification performance in unbalanced datasets</title>
		<author>
			<persName><forename type="first">Ajinkya</forename><surname>More</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Comprehensive privacy analysis of deep learning: Passive and active white-box inference attacks against centralized and federated learning</title>
		<author>
			<persName><forename type="first">Milad</forename><surname>Nasr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Reza</forename><surname>Shokri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amir</forename><surname>Houmansadr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Symposium on Security and Privacy</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<author>
			<persName><forename type="first">Travis</forename><forename type="middle">E</forename><surname>Oliphant</surname></persName>
		</author>
		<title level="m">A guide to NumPy</title>
		<imprint>
			<publisher>Trelgol Publishing USA</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Scalable and accurate deep learning with electronic health records</title>
		<author>
			<persName><forename type="first">Alvin</forename><surname>Rajkomar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">npj Digital Medicine</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Fetchsgd: Communication-efficient federated learning with sketching</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Rothchild</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashwinee</forename><surname>Panda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Enayat</forename><surname>Ullah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikita</forename><surname>Ivkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ion</forename><surname>Stoica</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vladimir</forename><surname>Braverman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raman</forename><surname>Arora</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">1-bit stochastic gradient descent and its application to data-parallel distributed training of speech dnns</title>
		<author>
			<persName><forename type="first">Frank</forename><surname>Seide</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jasha</forename><surname>Droppo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dong</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INTERSPEECH</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Privacy-preserving deep learning</title>
		<author>
			<persName><forename type="first">Reza</forename><surname>Shokri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vitaly</forename><surname>Shmatikov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CCS</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Atomo: Communication-efficient learning via atomic sparsification</title>
		<author>
			<persName><forename type="first">Hongyi</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Terngrad: Ternary gradients to reduce communication in distributed deep learning</title>
		<author>
			<persName><forename type="first">Wei</forename><surname>Wen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Fashionmnist: a novel image dataset for benchmarking machine learning algorithms</title>
		<author>
			<persName><forename type="first">Han</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kashif</forename><surname>Rasul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roland</forename><surname>Vollgraf</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">idlg: Improved deep leakage from gradients</title>
		<author>
			<persName><forename type="first">Bo</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Konda</forename><surname>Reddy Mopuri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hakan</forename><surname>Bilen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Deep leakage from gradients</title>
		<author>
			<persName><forename type="first">Ligeng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhijian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
		<editor>Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence d&apos;Alché-Buc, Emily B. Fox, and Roman Garnett</editor>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
			<pubPlace>NeurIPS</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Voting-based approaches for differentially private federated learning</title>
		<author>
			<persName><forename type="first">Yuqing</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi-Hsuan</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francesco</forename><surname>Pittaluga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Masoud</forename><surname>Faraki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu-Xiang</forename><surname>Manmohan</surname></persName>
		</author>
		<author>
			<persName><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
