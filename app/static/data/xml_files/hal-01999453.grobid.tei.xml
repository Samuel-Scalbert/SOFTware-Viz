<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Dirichlet Process Mixture Models made Scalable and Effective by means of Massive Distribution</title>
				<funder ref="#_xDtXsu4">
					<orgName type="full">European Union</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Khadidja</forename><surname>Meguelati</surname></persName>
							<email>khadidja.meguelati@inria.fr</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Inria</orgName>
								<orgName type="institution" key="instit2">LIRMM Montpellier</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Benedicte</forename><surname>Fontez</surname></persName>
							<email>benedicte.fontez@supagro.fr</email>
							<affiliation key="aff1">
								<orgName type="department">Montpellier SupAgro</orgName>
								<orgName type="laboratory">MISTEA</orgName>
								<orgName type="institution">Univ.Montpellier Montpellier</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Nadine</forename><surname>Hilgert</surname></persName>
							<email>nadine.hilgert@inra.fr</email>
							<affiliation key="aff2">
								<orgName type="laboratory">MISTEA</orgName>
								<orgName type="institution" key="instit1">INRA</orgName>
								<orgName type="institution" key="instit2">Univ.Montpellier Montpellier</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Florent</forename><surname>Masseglia</surname></persName>
							<email>florent.masseglia@inria.fr</email>
							<affiliation key="aff3">
								<orgName type="institution" key="instit1">Inria</orgName>
								<orgName type="institution" key="instit2">LIRMM Montpellier</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Dirichlet Process Mixture Models made Scalable and Effective by means of Massive Distribution</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">A9EC4688E4F5D5EF5C1F78E6FEC12C6E</idno>
					<idno type="DOI">10.1145/3297280.3297327</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-04-12T14:53+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Dirichlet Process Mixture Model</term>
					<term>Clustering</term>
					<term>Parallelism ACM Reference Format:</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Clustering with accurate results have become a topic of high interest. Dirichlet Process Mixture (DPM) is a model used for clustering with the advantage of discovering the number of clusters automatically and offering nice properties like, e.g., its potential convergence to the actual clusters in the data. These advantages come at the price of prohibitive response times, which impairs its adoption and makes centralized DPM approaches inefficient. We propose DC-DPM, a parallel clustering solution that gracefully scales to millions of data points while remaining DPM compliant, which is the challenge of distributing this process. Our experiments, on both synthetic and real world data, illustrate the high performance of our approach on millions of data points. The centralized algorithm does not scale and has its limit on 100K data points, where it needs more than 7 hours. In this case, our approach needs less than 30 seconds.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this case, data to be considered include data on plants and crop images, like the one illustrated by Figure <ref type="figure" target="#fig_0">1</ref>, showing a view of a Durum crop. Automatic identification, from such images, of leaves, soil, and distinguishing plants from foreground, are of high value for experts since they provide the fundamental information used for popular supervised methods in the domain <ref type="bibr" target="#b16">[17]</ref>. One of the main difficulties, for clustering, is the fact that we don't know, in advance, the number of clusters to be discovered. To help performing cluster analysis, despite the unknown tackled number of clusters, statistics advocate for:</p><p>(1) Setting a number of clustering runs, with varying value of K, and selecting the one that minimizes a goodness of fit criteria. It may be a quadratic risk or the Residual Mean Squared Error of Prediction (RMSEP) <ref type="bibr" target="#b13">[14]</ref>. This approach needs the implementation of a cross-validation algorithm <ref type="bibr" target="#b13">[14]</ref>. The clustering approach in this case, may be a mixture model with an Expectation-Maximization (EM) algorithm <ref type="bibr" target="#b5">[6]</ref>, or K-means <ref type="bibr" target="#b13">[14]</ref>, for instance. (2) Making a hierarchical clustering and then cut off the tree at a given depth, usually decided by the end-user. Different approaches for pruning with advantages and drawbacks exist, see <ref type="bibr" target="#b13">[14]</ref>. (3) Using a Dirichlet Process Mixture (DPM) which automatically detects the number of clusters <ref type="bibr" target="#b7">[8]</ref>. In this work, we focus on the DPM approach since it allows estimating the number of clusters and assigning observations to clusters, in the same process. Furthermore, its implementation is quite straightforward in a Bayesian framework. Such properties of DPM make it a very appealing solution for many use-cases. Unfortunately, DPM is highly time consuming. Consequently, several attempts have been done to make it distributed <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b25">26]</ref>. However, while being effectively distributed, these approaches usually suffer from convergence issues (imbalanced data distribution on computing nodes) <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b25">26]</ref> or do not fully benefit from DPM properties <ref type="bibr" target="#b24">[25]</ref> (see our discussion in Section 3). Furthermore, making DPM parallel is not straightforward since it must compare each record to the set of existing clusters, a highly repeated number of times. That impairs the global performances of the approach in parallel, since comparing all the records to all the clusters would call for a high number of communications and make the process impracticable. Our goal is to propose a parallel DPM approach that fully exploits parallel architectures for better performances and offers meaningful results. Our main contribution is to keep consistency of clusters among worker nodes, and between the worker and the master nodes with regards to DPM properties. Our motivating example comes from the biology use-case described above, where the processing time on one image may go up to several days in a centralized environment. These performances in response time are the main reason why DPM is not used in the domain. The results are very informative for experts, but the processing times are prohibitive. In this case, parallelization is an appealing solution but it has to guarantee that results remain as informative as the ones targeted by a centralized run. We propose DC-DPM (Distributed Clustering by Dirichlet Process Mixture), a distributed DPM algorithm that allows each node to have a view on the local results of all the other nodes, while avoiding exhaustive data exchanges. The main novelty of our work is to propose a model and its estimation at the master level by exploiting the sufficient statistics from the workers, in a DPM compliant approach. Our solution takes advantage of the computing power of distributed systems by using parallel frameworks such as MapReduce or Spark <ref type="bibr" target="#b26">[27]</ref>. Our DC-DPM solution distributes the Dirichlet Process by identifying local clusters on the workers and synchronizing these clusters on the master. These clusters are then communicated as a basis among workers for local clustering consistency. We modified the Dirichlet Process to consider this basis in each worker. By iterating this process we seek global consistency of DPM in a distributed environment. Our experiments, using real and synthetic datasets, illustrate both the high efficiency and linear scalability of our approach. We report significant gains in response time, compared to centralized DPM approaches, with processing times of a few minutes, compared to several days in the centralized case. The paper is organized as follows. In Section 2 we state the problem and give the necessary background on Dirichlet Process Mixture. In Section 3 we discuss related work and in Section 4, we describe the details of our distributed solution for clutering by means of Dirichlet Process Mixture. Section 5 reports the results of our experimental evaluation to verify the efficiency and effectiveness of our approach, and Section 6 concludes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">PROBLEM DEFINITION AND BACKGROUND 2.1 Dirichlet Process Mixture Models</head><p>A Dirichlet Process (DP) is a probability distribution over distributions. In our use-case, a distribution over the image pixels could be "plant" with probability p 1 , and "not plant" with probability p 2 , with the property that p 1 +p 2 = 1. A DP generates a probability distribution G. We observe a sample θ 1 ,...,θ N from G. In our use-case, each θ i is the vector of possible pixel color values.</p><formula xml:id="formula_0">θ n | G iid ∼ G,n = 1,...,N G ∼ DP(α,G 0 ) G θ n</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>N</head><p>Where G is by construction a discrete probability distribution <ref type="bibr" target="#b21">[22]</ref> .</p><formula xml:id="formula_1">G = ∞ i=1 π i (v)δ ϕ i</formula><p>Therefore, observed variables θ n have a non null probability of having the same value ϕ i and this allows for clustering. In our use-case, "plant" pixels will have the same color vector ϕ expressing the green value. Clustering is very sensitive to the DP parameters given by the end user. G 0 is a continuous probability distribution from which the (ϕ i ) i ∈N are initially drawn. In our use-case, G 0 gives the color probability of all possible clusters in the image.</p><p>ϕ 1 ,...,ϕ i ,... ∼G 0 α is a scale parameter (α &gt; 0) which tunes the probability weights</p><formula xml:id="formula_2">π i (v). V 1 ,...,V i ,... ∼ Beta(1,α) π i (v) =v i i-1 j=1 (1-v j )</formula><p>α tunes indirectly the probability mass function for k N , the number of unique values (namely ϕ i ) in a sample of size N <ref type="bibr" target="#b2">[3]</ref>.</p><formula xml:id="formula_3">p(k N ) = S N ,k N N !α k N Γ(α) Γ(α +N )<label>(1)</label></formula><p>where S N ,k N is the unsigned Stirling number of the first kind.</p><p>With a Dirichlet Process Mixture we observe the sample y 1 ,...,y N from a mixture of distributions F (θ n ). In our use-case, we assume that colors are observed with a noise distributed according to F . The mixture is controlled by a DP on the parameters θ n .</p><formula xml:id="formula_4">y n ∼ F (θ n ),n = 1,...,N θ n ∼ G G ∼ DP(α 0 ,G 0 )</formula><p>In a Bayesian framework, the estimation of θ n is done on the posterior: P(θ 1 ,...,θ N |y 1 ,...,y N ). Instead of this representation, another parameterization is used to speed up computation of the posterior:</p><formula xml:id="formula_5">P(ϕ c 1 ,...,ϕ c N |y 1 ,...,y N )</formula><p>Where θ n = ϕ c n , c n is the cluster label of observation n, and ϕ c n is the unique value of the θ n belonging to the same cluster. The Gibbs algorithm <ref type="bibr" target="#b10">[11]</ref> samples the cluster labels c 1 ,...,c N and next the cluster parameters (here ϕ c , for all c ∈ {1,...,K } where K designs the number of cluster label values) instead of θ 1 ,...,θ N . Several versions of a Gibbs sampling are proposed by Neal in <ref type="bibr" target="#b18">[19]</ref> to simulate values from the posterior. The principle is to repeat the following loops at least until convergence to the posterior:</p><formula xml:id="formula_6">α π c n G 0 ϕ k y n N K N Dirichlet Process</formula><p>(1) Cluster assignment, for n = 1,...,N</p><p>• Remove observation n from its cluster. Check if the cluster is empty, if yes then remove the cluster and ϕ c n from the list {ϕ} of all possible values. • Draw c n from</p><formula xml:id="formula_7">P (c n =c | {c j } j n ,y n ,{ϕ}) ∝ #(c) N -1+α F (y n | ϕ c ) existing cluster α N -1+α ∫ F (y n | ϕ)dG 0 (ϕ) new cluster</formula><p>Where #(c) designs the number of observations affected to cluster c (after removing observation n from the sample).</p><formula xml:id="formula_8">• If c designs a new cluster, need to draw ϕ c from P(ϕ |y n ) ∝ F (y n | ϕ)G 0 (ϕ) • End loops (2) Update of {ϕ},</formula><p>• draw ϕ c from the posterior distribution of cluster c, P(ϕ | {y} c ) (which is proportional to the product of the prior G 0 and the likelihood of all observations affected to cluster c).</p><p>When distribution F and G 0 are conjugates, ϕ can be integrated out from the Gibbs sampling which becomes time-efficient (no need to update {ϕ}). Then</p><formula xml:id="formula_9">P (c n =c | {c j } j n ,y n ,{ϕ}) ∝ #(c) N -1+α ∫ F (y n | ϕ)dP(ϕ | {y} c ) existing cluster α N -1+α ∫ F (y n | ϕ)dG 0 (ϕ) new cluster</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Massive Distribution and Spark</head><p>Clustering via Dirichlet Process Mixture based on Gibbs Sampling is unable to scale to large datasets due to its high computational costs associated with Bayesian inference. For this reason, we aim to implement a parallel algorithm for DPM clustering in a massively distributed environment called Spark which is a parallel programming framework aiming to efficiently process large datasets. This programming model can perform analytics with in-memory techniques to overcome disk bottlenecks. Similar to MapReduce <ref type="bibr" target="#b3">[4]</ref>, Spark can be deployed on the Hadoop Distributed File System (HDFS) <ref type="bibr" target="#b22">[23]</ref>.</p><p>Unlike traditional in-memory systems, the main feature of Spark is its distributed memory abstraction, called resilient distributed datasets (RDD), that is an efficient and fault-tolerant abstraction for distributing data in a cluster. With RDD, the data can be easily persisted in main memory as well as on the hard drive. Spark is designed to support the execution of iterative algorithms.</p><p>To execute a Spark job, we need a master node to coordinate job execution, and some worker nodes to execute a parallel operation. These parallel operations are summarized to two types: (i) Transformations: to create a new RDD from an existing one (e.g., Map, MapToPair, MapPartition, FlatMap); and (ii) Actions: to return a final value to the user (e.g., Reduce, Aggregate or Count).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Problem Definition</head><p>The problem we address is as follows. Given a (potentially big) dataset of records find, by means of process performed in parallel, a partition of the dataset into disjoint subsets called clusters, such that:</p><p>• Similar records are assigned to the same cluster.</p><p>• Dissimilar records are assigned to different clusters.</p><p>• The union of the clusters is the original dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">RELATED WORK</head><p>We set our work in the context of parallel clustering. Previous works for distributed algorithms of unsupervised clustering already exist. Ene et al. <ref type="bibr" target="#b6">[7]</ref> gave a MapReduce algorithm for the k-center and k-median problems. Both algorithms use Iterative-Sample as a subprocedure to get a substantially smaller subset of points that represents all of the points well. To achieve this, they perform an iterative-Sample. However these algorithms require the number of clusters k to be specified in advance, which is considered as one of the most dificult problems to solve in data clustering. Debatty et al. <ref type="bibr" target="#b4">[5]</ref> proposed a MapReduce implementation of G-means which is an iterative algorithm that uses Anderson Darling test to verify if a subset of data follows a Gaussian distribution. However this algorithm overestimates the number of clusters, and then requires a post-processing step to merge clusters.</p><p>In our work, we focused on algorithms inspired by the DPM. Lovell et al. <ref type="bibr" target="#b15">[16]</ref> and Williamson et al. <ref type="bibr" target="#b25">[26]</ref> has suggested an alternative parametrisation for the Dirichlet process in order to derive non-approximate parallel MCMC inference for it, these approaches are criticized by Gal and Ghahramani in <ref type="bibr" target="#b9">[10]</ref>, these latter showed that the approaches suggested are impractical due to an extremely imbalanced distribution of the data, and gave directions for future research like the development of better approximate parallel inference. The main idea when data is distributed is to perform a DPM in each worker. The issues are then to share information between workers, and to synchronize and update clusters arising from workers at the master level. For synchronization, the main challenge is a problem of identification and of label switching of clusters. In this context we can use a relabelling algorithm like for example the one proposed by Stephens <ref type="bibr" target="#b14">[15]</ref> for mixture models. For parallel Latent Dirichlet Allocation (LDA) and Hierarchical Dirichlet Process (HDP), Newman et al. <ref type="bibr" target="#b19">[20]</ref> suggested to measure distance between clusters and then proposed a greedy matching. Wang and Lin <ref type="bibr" target="#b24">[25]</ref> gave a detailed review of literature and recent advanced in this topic before giving a new proposal. They proposed to use a stepwise hierarchical classification at the master level with half chance for split or merge at each step. They began with a full model considering all clusters from all workers as different components of the model. Their algorithm uses the standard Bayes Factor to compare nested models and choose the best split or merge. In conclusion, at the master level, the proposed algorithms diverge from a DPM-classifier and are not a scalable estimations of a DPM. Moreover, Wang and Lin <ref type="bibr" target="#b24">[25]</ref> used a fixed value for the scale parameter (α) in their implementation of the DPM at the workers level. The number of final clusters is related to this value (see equation 1). Authors like Miller and Harrison <ref type="bibr" target="#b17">[18]</ref> have demonstrated the inconsistency for the number of components of a DPM model with fixed α value. If the number of components identified at the worker level is underestimated, then the number of clusters at the master level might be underestimated. The reverse will increase considerably the running time at the master level.</p><p>In our approach, we suggest to keep to a DPM algorithm as much as possible, even at the master level, to be close to the good properties of a DPM-classifier, despite the fact that data is distributed. We also suggest a modification of the DPM model to share information among workers. In this way we expect to improve our clustering (better estimation) and suppress label switching. Finally, we do not fix a value to α but allow a different estimation in each worker to add flexibility to our model.</p><p>Furthermore <ref type="bibr" target="#b24">[25]</ref> is restricted to specific cases where noise in the observations follows the conjugate distribution of the cluster centers distribution. For example, a Gaussian noise imposes a Gaussian distribution of the centers. Therefore, this method is not suited for centers having positive values only.</p><p>Our goal is to work on any data, even with exclusively positive centers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">DC-DPM: DISTRIBUTED CLUSTERING VIA DPM</head><p>In this section, we present a novel parallel clustering approach called DC-DPM, adapted for independent data. Parallelization calls for particular attention to two main issues. The first one is the load balance between computing nodes. In our approach we distribute data evenly across the different nodes, and there is no data exchange between nodes during the processing. The second issue is the cost of communications. In order to be efficient, nodes send and receive as few information as possible by performing many iterations of Gibbs Sampling independently in each worker before synchronizing the global state at the master level and only communicating sufficient statistics between workers and master. The challenge of using sufficient statistics, in a distributed environment, is to remain in the DPM approach at all steps, including the synchronization between the worker and master nodes. The novelty of our approach is to approximate the DPM model even at the master level when local data is replaced by sufficient statistics between iterations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Architecture and Distributed Algorithm Data Distribution</head><p>Data is evenly distributed on the computing nodes when the process starts. This is a mere, sequential, distribution, that splits the dataset into equal sized partitions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Worker</head><p>This level handles the innovation parts of DPM (detection of new clusters) and the individual cluster assignment in each worker. The updates of the cluster labels in worker j depend on sample size proportions of the distributed data:</p><formula xml:id="formula_10">P(c n,j =c | c n,j ,y n,j ,{ϕ}) ∝ #(c) j N j -1+α F (y n,j ,ϕ c ),c = 1,...,K α N j -1+α ∫ F (y n,j ,ϕ)dG 0 (ϕ) new</formula><p>As the clusters are not known at the beginning, we cannot ensure that the sample size proportions of each cluster will be respected in each worker. If the data were uniformly distributed, each cluster would have only, in average, the same weight/proportion in all workers. Therefore we added a modification of the update, as can be seen in Algorithm 1.</p><p>Algorithm 1 DPM at worker level for each data y n do Draw c n,j from P(c n,j =c | {c l ,j } l n ,y n,j ,{ϕ},{w },α j ) ∝ #(c)+α j w c N j -1+α j F (y n,j ,ϕ c ),c = 1,...,K</p><formula xml:id="formula_11">α j w u N j -1+α j ∫ F (y n,j ,ϕ)dG 0 (ϕ) new Update of α j Draw ϕ c for new clusters</formula><p>where the weight w c is the proportion of observations from cluster c evaluated on the whole dataset and w u the proportion of non affected observations (awaiting the creation, innovation, discover of their real clusters). Therefore, these parameters are updated at the master level during the synchronization. Now, the scale parameter α j can be viewed as a tuning parameter between local (worker) and global (master) proportions. Following <ref type="bibr" target="#b8">[9]</ref> we use an inverse gamma prior to infer this parameter.</p><p>This modification of the update implies a slightly modified DPM in each worker j :</p><formula xml:id="formula_12">y n,j ∼ F (θ n,j ) θ n,j ∼ G j G j ∼ DP(α j ,G) α j ∼ IG(a,b) G = K c=1 w c δ ϕ c +w u G 0 , with w u + K c=1 w c = 1</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Master</head><p>This level handles the final individual assignment in the master node and therefore the final common number of clusters K. The master gets from each worker the following input: sample size of cluster k in worker j (n j,k ), cluster parameter values-sufficient statistics, individual predictive value (traditionally/usually the cluster mean value in the worker: ŷn,j = ȳj,c n, j =k ). At the master level, the observations are assigned by clusters. A cluster corresponds to a set of individuals belonging to the same cluster of the same worker. Each cluster has a representative or individual predictive value which is used to perform the end of the Gibbs sampling at the master level:</p><formula xml:id="formula_13">P(c n,j =c | c n,j ) ∝ #(c) N -1+γ F ( ŷn,j ,ϕ c ),c = 1,...,K γ N -1+γ ∫ F ( ŷn,j ,ϕ)dG 0 (ϕ) new</formula><p>Working at an individual level implies a slow Gibbs sampling with poor mixing <ref type="bibr" target="#b11">[12]</ref>. So, we suggest an update by clusters. In this view, we denote z j,k the master label of the cluster k in worker j. To take into account the worker information ({ϕ </p><formula xml:id="formula_14">P(z j,k =c | z j,k ) ∝ #(c) N -1+γ F ( ȳj,k ,ϕ c ),c = 1,...,K γ N -1+γ ∫ F ( ȳj,k ,ϕ)dG(ϕ | ϕ workerj k )</formula><p>The labels {c n,j } of all the observations in the cluster k of worker j are then assigned to the master label z j,k . Next, the cluster parameters ({ϕ c } c=1,...,K ) are updated from the posterior computed on the whole dataset. We assume that we don't need all the data but only sufficient statistics from all clusters from all workers to compute the posterior. This assumption is straightforward for many distributions, as the exponential family <ref type="bibr" target="#b0">[1]</ref>. Last, the synchronization of the workers is done through the definition of G using the updated parameters ({ϕ c } c=1,...,K ) and with weights drawn from a Dirichlet distribution Dir (n 1 ,...,n K ,γ ). The end user parameters of this Dirichlet distribution are updated at the master level from the whole dataset. The size n k is the sum of all observations having label k at the end of the master Gibbs sampling.</p><p>(w 1 ,...,w K ,w u ) ∼ Dir (n 1 ,...,n K ,γ ) γ ∼ IG(c,d)</p><p>By doing so, we do not have to consider label switching. Clusters are explicitly defined at the master level and parameter values are not updated in the worker. At the worker level, only innovation (creation of new clusters) is implemented. This is summarized by Algorithm 2.</p><p>Algorithm 2 DPM at master level for each (j,k) do Draw z j,k from P(z j,k =c | {c} j,k , ȳj,k ,{ϕ},γ ) ∝</p><formula xml:id="formula_15">#(c) N -1+γ F ( ȳj,k ,ϕ c ),c = 1,...,K γ N -1+γ ∫ F ( ȳj,k ,ϕ)dG(ϕ | ϕ workerj k )</formula><p>Update of ϕ and (w 1 ,...,w K ,w u )</p><p>The workflow of our DC-DPM approach is illustrated by Figure <ref type="figure" target="#fig_2">2</ref>. It consists in 4 steps:</p><p>(1) Identify local new clusters in the workers (2) Compute and send sufficient statistics and cluster sizes from each worker to the master (3) Synchronize and estimate cluster labels from sufficient statistics (4) Send updated cluster parameters and cluster sizes from master to workers Our first proposition concerns the synchronization and estimation of the DPM. It is done with a Gibbs sampling conditionally on the sufficient statistics instead of the whole dataset/individual observations. Our second proposition is a construction of a shared prior distribution updated at the master level and send to the workers' DPM. This distribution reflects the information/results collected from all workers and synchronized at the master.</p><p>The likelihood for one observationy i from the Exponential family is:</p><formula xml:id="formula_16">F (y i | η) =h(y i )exp η T ψ (y i )-a(η)</formula><p>where : </p><formula xml:id="formula_17">F (y 1 ,...,y n | η) = N i=1 h(y i ) exp η T N i=1 ψ (y i ) -N a(η)</formula><p>Among all the distributions included in the Exponential Family, we implemented the Normal case for the experiments:</p><formula xml:id="formula_18">F (. | ϕ c ) = N (ϕ c ,Σ 1 )</formula><p>. This choice corresponds to the simple linear model y n =ϕ c +ε n and ε n is Normally distributed N (0,Σ 1 ). In this case, the sample mean of cluster c, namely ȳc is a sufficient statistic and the posterior distribution can be conditioned only on its value:</p><formula xml:id="formula_19">P(ϕ | {y n } c n =c ) =P(ϕ | ȳc ) ∝ F ( ȳc | ϕ)G 0 (ϕ)</formula><p>When G 0 is not a conjugate prior (e.g., a normal distribution), the posterior distribution is not a usual one but a value from this posterior can be simulated with a Metropolis Hasting (MH) within Gibbs algorithm.</p><p>When variances are known and G 0 is a conjugate prior (normal distribution N (m,Σ 1 )), there is no use of MH algorithm. The posterior is a normal distribution N (ϕ</p><formula xml:id="formula_20">post c = Σ(#(c)Σ -1 2 ȳc +Σ -1 1 m),Σ post c</formula><p>)</p><p>where</p><formula xml:id="formula_21">Σ post c = (#(c)Σ -1 2 +Σ -1 1 ) -1 . The predictive posterior is a nor- mal distribution N (ϕ post c , Σ 2 + Σ post c</formula><p>). In our context, Σ </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTS</head><p>The parallel experimental evaluation was conducted on a cluster of 32 machines, each operated by Linux, with 64 Gigabytes of main memory, Intel Xeon CPU with 8 cores and 250 Gigabytes hard disk. The centralized approach is an implementation DPM in Scala, and was executed on a single machine with the same characteristics.</p><p>The distributed algorithm we proposed is an approximation of a classic DPM, we will compare its properties to a centralized DPM implementation, on synthetic data and also in our use-case for digital agronomy. The first step of our process is a distributed K-means that sets the initial state (usually we set K to be one tenth of the dataset size).</p><p>Reproducibility : All our experiments are fully reproducible. We make our code and data available at https://github.com/khadidjaM/ DC-DPM.</p><p>In the rest of this section, we describe the datasets in Section 5.1 and our evaluation criteria in Section 5.2. Then, in Section 5.3, we measure the performances, in response time, of our approach compared to the centralized approach and also by reporting its scalability and speed-up. We evaluate the clusters obtained by DC-DPM in the case of real and synthetic dataset in Section 5.4 and Section 5.5 discusses the results and interest of our work in a real use-case of agronomy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Datasets</head><p>We carried out our experiments on a real world and a synthetic dataset.</p><p>Our synthetic data are generated using a two-steps principle. In the first step we generate cluster centers according to a multivariate normal distribution with the same variance σ 2 1 for all dimensions. In the second step, we generate the data corresponding to each center, by using a multivariate normal distribution parameterized on the center with the same variance σ 2 2 for all dimensions. We generated a first batch of 5 datasets having size 20K, 40, 60, 80K and 100K with σ 2 1 = 1000 and σ 2 2 = 1. They represent 10 clusters. We generated a second batch of 5 datasets having size 2M, 4M, 6M, 8M and 10M with σ 2 1 = 100000 and σ 2 2 = 10. They represent 100 clusters. This type of generator is widely used in statistics, where methods are evaluated first on synthetic data before being applied on real data.</p><p>Our real data correspond to the use-case described in Section 1. The image used to test our algorithm was in RGB format. After preprocessing it contains 1,081,200 data points, described by a vector of 3 values (red, green and blue) belonging to [0,1].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Clustering Evaluation Criteria</head><p>There are two cases for evaluating the results of a clustering algorithm. Either there is a ground truth available, or there is not. In the case of an available ground truth, there are measures allowing to compare the clustering results to the reference, such as ARI, described below, for instance. This is usually exploited for experiments when one wants to check performances in a controlled environment, on synthetic data or labelled real data. In the case where there is no ground-truth (which is the usual case, because we don't know what should be discovered in real world applications of a clustering algorithm) the results may be evaluated by means of relative measures, like RSS, described below, for instance. In our experiments, we chose the following three criteria. 1. The Adjusted Rand Index (ARI): it is the corrected-for-chance version of the Rand Index <ref type="bibr" target="#b23">[24]</ref>, which is a function that measures the similarity between two data clustering results, for example between the ground truth class assignments (if known) and the clustering algorithm assignments. ARI values are in the range [-1,1] with a best value of 1. 2. The residual sum of squares (RSS): it is a measure of how well the centroids (means) represent the members of their clusters. It is the squared distance of each data from its centroid summed over all vectors. In the univariate case, the RSS value divided by the number of observations gives the value of the Mean Squared Error (MSE), an estimator of the residual variance. In multivariate dataset with independent variables, the RSS value divided by the number of observations gives an estimator of the sum of the variable variances. This sum represents its lower bound and also the best value to be observed in the clustering of synthetic data. To simplify, we give in the following the result of the RSS value divided by the number of data N and the variance. Therefore the lower bound is known and should be equal to the number of variables (for example 2 for our synthetic data). 3. K, the number of discovered clusters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Response Time</head><p>In this section we measure the clustering time in DC-DPM and compare it to the centralized approach. Figure <ref type="figure" target="#fig_4">3</ref> reports the response times of DC-DPM and the centralized approach on our synthetic data, limited to 100K data points. Actually, the centralized approach does not scale and would take several days for larger datasets.The distributed approach is run on a cluster of 8 nodes. The results reported by Figure <ref type="figure" target="#fig_4">3</ref> are in logarithmic scale. The clustering time increases with the number of data points for all approaches. This time is much lower in the case of DC-DPM, than the centralized approach. On 8 machines (64 cores) and for a dataset of 100K data points, DC-DPM performs the clustering in 24 seconds, while the centralized approach needs more than 7 hours on a single machine.</p><p>Figure <ref type="figure" target="#fig_5">4</ref> reports an extended view on the clustering time, only for DC-DPM, and with a dataset having up to 10 million data points from the synthetic dataset. DC-DPM is run on a cluster of 16 machines. The running time increases with the number of data points. Let us note that the centralized approach does not scale and cannot execute on such dataset size. DC-DPM enjoys linear scalability with the dataset size.</p><p>Figures <ref type="figure" target="#fig_6">5</ref> and<ref type="figure" target="#fig_7">6</ref> illustrate the parallel speed-up of our approach on 2M data points from the synthetic dataset and on more than 1 million data points obtained after preprocessing the image of our use-case. The results show optimal or near optimal gain. In Figure <ref type="figure" target="#fig_7">6</ref> we observe that the response time for 2 nodes is more than twice the response time for 4 nodes. That is unexpected when measuring a speed-up. However, the response times of our approach are very fast (a few minutes) and do not consider the time it takes for Spark to load-up, before running DC-DPM. The slight difference between an optimal speedup and the results reported in Figure <ref type="figure" target="#fig_7">6</ref> are due to that loading time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Clustering Evaluation</head><p>In the following experiments, we evaluate the clustering performance of DC-DPM and compare it to the centralized approach.</p><p>Table <ref type="table" target="#tab_1">1</ref> reports 1) the ARI value computed between the clustering obtained and the ground truth, 2) the RSS value divided by the number of data N and variance (σ 2  2 ), and 3) the number of clusters, obtained with DC-DPM and the centralized approach on our synthetic data while increasing the dataset size. The DC-DPM is run on a cluster of 8 nodes. DC-DPM performs as well as the centralized approach, there is a small gap in RSS values which is negligible compared to the gained time.</p><p>Table <ref type="table" target="#tab_2">2</ref> reports an extended view on the ARI value, and the RSS value divided by the number of data N and by the variance (σ 2  2 ), and number of clusters obtained with DC-DPM, while increasing dataset size (up to 10 million data points). DC-DPM is run on a cluster of 16 machines. The performance keeps showing the maximum possible accuracy, even with a large number of data points.       Figure <ref type="figure" target="#fig_8">7</ref> gives a visual representation of our 4M data points synthetic dataset. Each cluster is assigned to a color. Our goal is to retrieve these clusters. Figure <ref type="figure" target="#fig_9">8</ref> represents the results obtained by our approach on the data of Figure <ref type="figure" target="#fig_8">7</ref>, with 16 nodes. Each cluster is assigned to a different color. shows the performance of our approach with almost perfect results where the discovered clusters are the same as the actual ones from the data. This is confirmed by Table <ref type="table" target="#tab_2">2</ref>, line 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Use-case</head><p>Phenotyping and precision agriculture use more and more information from sensors and drones, like aerial images, leading to the emerging domain of digital agriculture (see for example http://ec.europa. eu/research/participants/portal/desktop/en/opportunities/h2020/topics/ dt-rur-12-2018.html). An important challenge, in this context, is to be able to distinguish clusters of plants: status (normal, hydric stress, disease,...) or species for example. Clustering, applied to images, is a key in this domain.</p><p>We want to discover clusters in the image presented in Section 1 and transformed as described in Section 5.1. We set σ 2 1 = 1 because our data is in the range of [0,1]. For both parameter values of σ 2 2 = 0.01 and σ 2 2 = 0.0025, the clusters are extracted in approximately 3 minutes with DC-DPM running in parallel on 16 computing nodes. This is confirmed by Figure <ref type="figure" target="#fig_7">6</ref>. The centralized approach does not scale on this data and we could not obtain results.</p><p>The number of clusters depends on the value of the variance error. A value of σ 2 2 = 0.01 gave a rough clustering with only K = 3 clusters. Those clusters identified the brightness in the RGB image (see figure <ref type="figure" target="#fig_10">9</ref>). A lower value of σ 2 2 = 0.0025 gave a clustering with k = 12 clusters, which is enough to reconstruct the image (see figure <ref type="figure" target="#fig_11">10</ref>). Depending on the aim of the clustering, different types of wavelength or data must be used for identification. The accuracy of   the clustering (number of clusters) relies on the variance value (σ 2 2 ). Clustering is used to detect structures in the data (genetic, population, status) before processing. This group detection allows reducing data dimension and bias in further prediction analysis.</p><p>DC-DPM was compared to the centralized DPM on a part of the the Durum image in the experimental field. RGB Image with DC-DPM (top, 12 clusters) and centralized DPM (bottom, 17 clusters), σ 2 2 = 0.0025. The results were quite similar as shown in figure <ref type="figure" target="#fig_12">11</ref>. The impact of σ 2 on the number of clusters varies for centralized and distributed approaches and may be adjusted by the end-user.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>We proposed DC-DPM, a novel and efficient parallel solution to perform clustering via DPM on millions of data points. We evaluated the performance of our solution over real world and synthetic datasets. The experimental results illustrate the excellent performance of DC-DPM (e.g., a clustering time of less than 30 seconds for 100K data points, while the centralized algorithm needs several hours). The results also illustrate the high performance of our approach with results that are comparable to the ones of the centralized version. Overall, the experimental results show that by using our parallel techniques, the clustering of very large volumes of data can now be done in small execution times, which are impossible to achieve using the centralized DPM approach. A nice perspective of our approach is to open fundamental research tracks such as DPM clustering on complex data like, e.g. time series.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Durum in an experimental field. RGB image.</figDesc><graphic coords="2,345.96,246.33,184.24,127.55" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>workerj k }), we replace the Dirichlet Process Mixture Models made Scalable and Effective by means of Massive DistributionSAC '19, April 8-12, 2019, Limassol, Cyprus prior predictive distribution ( ∫ F (y n,j ,ϕ)dG 0 (ϕ)) by a posterior predictive distribution. Eventually, we use the cluster mean value ( ȳj,k ) as an individual predictive value:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Diagram/workflow of the DC -DPM</figDesc><graphic coords="6,337.28,83.69,201.59,119.33" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>the mean value ϕ post c was replaced by an individual drawn from the posterior.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Logarithmic scale. Response time of DC-DPM and centralized DPM as a function of the dataset size.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Response time (minutes) of DC-DPM as a function of the dataset size.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Clustering time as a function of the number of computing nodes on the synthetic data.</figDesc><graphic coords="8,406.88,239.81,162.00,121.50" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Clustering time as a function of the number of computing nodes on the image of our use-case.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Visual representation of the synthetic dataset.</figDesc><graphic coords="8,230.34,239.81,162.00,121.50" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Visual representation of the results obtained by DC-DPM</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Clustering of the Durum image by DC-DPM with σ 2 = 0.01.</figDesc><graphic coords="9,53.80,83.68,162.00,121.64" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: Clustering of the Durum image by DC-DPM with σ 2 2 = 0.0025.</figDesc><graphic coords="9,230.34,83.68,162.00,121.64" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 11 :</head><label>11</label><figDesc>Figure 11: Clustering of a part of the Durum image by DC-DPM (top) and Centralized DPM (bottom).</figDesc><graphic coords="9,406.88,83.70,149.41,121.62" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Mixture Models made Scalable and Effective by means of Massive DistributionSAC '19, April 8-12, 2019, Limassol, Cyprus</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Clustering evaluation criteria obtained with the centralized DPM and with DC-DPM.</figDesc><table><row><cell cols="3">Centralized DPM</cell><cell></cell><cell cols="2">DC-DPM</cell></row><row><cell>ARI</cell><cell>RSS N ×σ 2 2</cell><cell cols="2">Clusters ARI</cell><cell>RSS N ×σ 2 2</cell><cell>Clusters</cell></row><row><cell>20K 1.00</cell><cell>2.01</cell><cell>10</cell><cell>1.00</cell><cell>2.04</cell><cell>10</cell></row><row><cell>40K 1.00</cell><cell>2.00</cell><cell>10</cell><cell>1.00</cell><cell>2.03</cell><cell>10</cell></row><row><cell>60K 1.00</cell><cell>2.00</cell><cell>10</cell><cell>1.00</cell><cell>2.02</cell><cell>10</cell></row><row><cell>80K 1.00</cell><cell>2.00</cell><cell>10</cell><cell>1.00</cell><cell>2.01</cell><cell>10</cell></row><row><cell>100K 1.00</cell><cell>2.00</cell><cell>10</cell><cell>1.00</cell><cell>2.02</cell><cell>10</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Clustering evaluation criteria obtained by DC-DPM .</figDesc><table><row><cell cols="3">ARI RSS/(N*σ 2 2 ) Clusters</cell></row><row><cell>2M 1.00</cell><cell>2.00</cell><cell>102</cell></row><row><cell>4M 1.00</cell><cell>2.00</cell><cell>100</cell></row><row><cell>6M 1.00</cell><cell>2.00</cell><cell>100</cell></row><row><cell>8M 1.00</cell><cell>2.02</cell><cell>99</cell></row><row><cell>10M 1.00</cell><cell>2.10</cell><cell>101</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="7">ACKNOWLEDGEMENTS</head><p>The research leading to these results has received funds from the <rs type="funder">European Union</rs>'s <rs type="programName">Horizon 2020 Framework Programme for Research and Innovation</rs>, under grant agreement No. <rs type="grantNumber">732051</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_xDtXsu4">
					<idno type="grant-number">732051</idno>
					<orgName type="program" subtype="full">Horizon 2020 Framework Programme for Research and Innovation</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Exponential Family</title>
		<idno type="DOI">10.1002/9780470627242.ch18</idno>
		<ptr target="https://onlinelibrary.wiley.com/doi/pdf/10.1002/9780470627242.ch18" />
		<imprint>
			<date type="published" when="2010">2010</date>
			<publisher>Wiley-Blackwell</publisher>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="93" to="97" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Monte Carlo simulation and clustering for customer segmentation in business organization</title>
		<author>
			<persName><forename type="first">A</forename><surname>Alamsyah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Nurriz</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICSTC.2017.8011861</idno>
		<ptr target="https://doi.org/10.1109/ICSTC.2017.8011861" />
	</analytic>
	<monogr>
		<title level="m">2017 3rd International Conference on Science and Technology -Computer (ICST)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="104" to="109" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Mixtures of Dirichlet Processes with Applications to Bayesian Nonparametric Problems</title>
		<author>
			<persName><forename type="first">Charles</forename><forename type="middle">E</forename><surname>Antoniak</surname></persName>
		</author>
		<idno type="DOI">10.1214/aos/1176342871</idno>
		<ptr target="https://doi.org/10.1214/aos/1176342871" />
	</analytic>
	<monogr>
		<title level="j">Ann. Statist</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1152" to="1174" />
			<date type="published" when="1974">1974. 1974</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">MapReduce: Simplified Data Processing on Large Clusters</title>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ghemawat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="107" to="113" />
			<date type="published" when="2008">2008. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Determining the k in k-means with MapReduce</title>
		<author>
			<persName><forename type="first">Thibault</forename><surname>Debatty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Michiardi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wim</forename><surname>Mees</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olivier</forename><surname>Thonnard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EDBT/ICDT Workshops</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="19" to="28" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Maximum likelihood from incomplete data via the EM algorithm</title>
		<author>
			<persName><forename type="first">Nan</forename><forename type="middle">M</forename><surname>Arthur P Dempster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donald</forename><forename type="middle">B</forename><surname>Laird</surname></persName>
		</author>
		<author>
			<persName><surname>Rubin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the royal statistical society. Series B</title>
		<imprint>
			<biblScope unit="page" from="1" to="38" />
			<date type="published" when="1977">1977. 1977</date>
		</imprint>
	</monogr>
	<note>methodological</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Fast clustering using MapReduce</title>
		<author>
			<persName><forename type="first">Alina</forename><surname>Ene</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sungjin</forename><surname>Im</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Moseley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 17th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="681" to="689" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Estimating normal means with a Dirichlet process prior</title>
		<author>
			<persName><forename type="first">D</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><surname>Escobar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Amer. Statist. Assoc</title>
		<imprint>
			<biblScope unit="volume">89</biblScope>
			<biblScope unit="page" from="268" to="277" />
			<date type="published" when="1994">1994. 1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Bayesian density estimation and inference using mixtures</title>
		<author>
			<persName><forename type="first">D</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Escobar</surname></persName>
		</author>
		<author>
			<persName><surname>West</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the american statistical association</title>
		<imprint>
			<biblScope unit="volume">90</biblScope>
			<biblScope unit="page" from="577" to="588" />
			<date type="published" when="1995">1995. 1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Pitfalls in the use of parallel inference for the Dirichlet process</title>
		<author>
			<persName><forename type="first">Yarin</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zoubin</forename><surname>Ghahramani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on Machine Learning (ICML-14)</title>
		<meeting>the 31st International Conference on Machine Learning (ICML-14)</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="208" to="216" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Gelman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">B</forename><surname>Carlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hal</forename><forename type="middle">S</forename><surname>Stern</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donald</forename><forename type="middle">B</forename><surname>Rubin</surname></persName>
		</author>
		<title level="m">Bayesian Data Analysis</title>
		<imprint>
			<publisher>Chapman and Hall/CRC</publisher>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
	<note>2nd ed. ed.</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Parallel gibbs sampling: From colored fields to thin junction trees</title>
		<author>
			<persName><forename type="first">Joseph</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yucheng</forename><surname>Low</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Gretton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlos</forename><surname>Guestrin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics</title>
		<meeting>the Fourteenth International Conference on Artificial Intelligence and Statistics</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="324" to="332" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A Survey of Outlier Detection Methodologies</title>
		<author>
			<persName><forename type="first">Victoria</forename><surname>Hodge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jim</forename><surname>Austin</surname></persName>
		</author>
		<idno type="DOI">10.1023/B:AIRE.0000045502.10941.a9</idno>
		<ptr target="https://doi.org/10.1023/B:AIRE.0000045502.10941.a9" />
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence Review</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="85" to="126" />
			<date type="published" when="2004-10-01">2004. 01 Oct 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">An introduction to statistical learning</title>
		<author>
			<persName><forename type="first">Gareth</forename><surname>James</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniela</forename><surname>Witten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Hastie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Tibshirani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
			<publisher>Springer</publisher>
			<biblScope unit="volume">112</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Markov chain Monte Carlo methods and the label switching problem in Bayesian mixture modeling</title>
		<author>
			<persName><forename type="first">Ajay</forename><surname>Jasra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><forename type="middle">C</forename><surname>Holmes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">A</forename><surname>Stephens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statist. Sci</title>
		<imprint>
			<biblScope unit="page" from="50" to="67" />
			<date type="published" when="2005">2005. 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Parallel markov chain monte carlo for dirichlet process mixtures</title>
		<author>
			<persName><forename type="first">Dan</forename><surname>Lovell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><forename type="middle">P</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName><surname>Mansingka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Big Learning</title>
		<imprint>
			<publisher>NIPS</publisher>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Machine learning for Big Data analytics in plants</title>
		<author>
			<persName><forename type="first">Chuang</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Helen</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangfeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trends in Plant Science</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">1 12</biblScope>
			<biblScope unit="page" from="798" to="808" />
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Inconsistency of Pitman-Yor process mixtures for the number of components</title>
		<author>
			<persName><forename type="first">W</forename><surname>Jeffrey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><forename type="middle">T</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><surname>Harrison</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3333" to="3370" />
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Markov chain sampling methods for Dirichlet process mixture models</title>
		<author>
			<persName><forename type="first">M</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><surname>Neal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of computational and graphical statistics</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="249" to="265" />
			<date type="published" when="2000">2000. 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Distributed algorithms for topic models</title>
		<author>
			<persName><forename type="first">David</forename><surname>Newman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Asuncion</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Padhraic</forename><surname>Smyth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="1801" to="1828" />
			<date type="published" when="2009-08">2009. Aug (2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A fast version of the k-means classification algorithm for astronomical applications</title>
		<author>
			<persName><forename type="first">I</forename><surname>Ordovás-Pascual</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sánchez Almeida</surname></persName>
		</author>
		<idno type="DOI">10.1051/0004-6361/201423806</idno>
		<ptr target="https://doi.org/10.1051/0004-6361/201423806" />
	</analytic>
	<monogr>
		<title level="j">Astronomy &amp; Astrophysics</title>
		<imprint>
			<biblScope unit="volume">565</biblScope>
			<biblScope unit="page">53</biblScope>
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A constructive definition of Dirichlet priors</title>
		<author>
			<persName><forename type="first">Jayaram</forename><surname>Sethuraman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statistica sinica</title>
		<imprint>
			<biblScope unit="page" from="639" to="650" />
			<date type="published" when="1994">1994. 1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">The Hadoop distributed filesystem: Balancing portability and performance</title>
		<author>
			<persName><forename type="first">J</forename><surname>Shafer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Rixner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Cox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. ISPASS</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Information Theoretic Measures for Clusterings Comparison: Variants, Properties, Normalization and Correction for Chance</title>
		<author>
			<persName><forename type="first">Xuan</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Vinh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Epps</surname></persName>
		</author>
		<author>
			<persName><surname>Bailey</surname></persName>
		</author>
		<ptr target="http://dl.acm.org/citation.cfm?id=1756006.1953024" />
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="2837" to="2854" />
			<date type="published" when="2010-12">2010. Dec. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Scalable Estimation of Dirichlet Process Mixture Models on Distributed Data</title>
		<author>
			<persName><forename type="first">Ruohui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
		<ptr target="http://dl.acm.org/citation.cfm?id=3171837.3171935" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th International Joint Conference on Artificial Intelligence (IJCAI&apos;17)</title>
		<meeting>the 26th International Joint Conference on Artificial Intelligence (IJCAI&apos;17)</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="4632" to="4639" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Parallel Markov chain Monte Carlo for nonparametric mixture models</title>
		<author>
			<persName><forename type="first">Sinead</forename><surname>Williamson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Avinava</forename><surname>Dubey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="98" to="106" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Spark: Cluster Computing with Working Sets</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zaharia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chowdhury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Franklin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shenker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Stoica</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HotCloud</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
