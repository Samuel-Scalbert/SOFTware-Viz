<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Relative Positional Encoding for Transformers with Linear Complexity</title>
				<funder ref="#_Jr5tsuq">
					<orgName type="full">Agence Nationale de la Recherche</orgName>
					<orgName type="abbreviated">ANR</orgName>
				</funder>
				<funder ref="#_hjzmjHS">
					<orgName type="full">European Union</orgName>
				</funder>
				<funder ref="#_kH5rnD2">
					<orgName type="full">French government</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Antoine</forename><surname>Liutkus</surname></persName>
							<email>antoine&lt;antoine.liutkus@inria.fr&gt;.</email>
							<affiliation key="aff0">
								<orgName type="laboratory" key="lab1">Inria</orgName>
								<orgName type="laboratory" key="lab2">Zenith Team</orgName>
								<orgName type="laboratory" key="lab3">UMR LIRMM</orgName>
								<orgName type="institution">Univ. Montpellier</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ondřej</forename><surname>Cífka</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">LTCI</orgName>
								<orgName type="institution" key="instit1">Télécom Paris</orgName>
								<orgName type="institution" key="instit2">Institut Polytechnique de Paris</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Shih-Lun</forename><surname>Wu</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Research Center for IT Innovation</orgName>
								<orgName type="institution">Academia Sinica</orgName>
								<address>
									<country key="TW">Taiwan</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">National Taiwan University</orgName>
								<address>
									<country key="TW">Taiwan</country>
								</address>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="institution">Taiwan AI Labs</orgName>
								<address>
									<country key="TW">Taiwan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Umut</forename><surname>Şimşekli</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Yi-Hsuan</forename><surname>Yang</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Research Center for IT Innovation</orgName>
								<orgName type="institution">Academia Sinica</orgName>
								<address>
									<country key="TW">Taiwan</country>
								</address>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="institution">Taiwan AI Labs</orgName>
								<address>
									<country key="TW">Taiwan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Gael</forename><surname>Richard</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Umut</forename><forename type="middle">S</forename><surname>¸ims ¸ekli</surname></persName>
							<affiliation key="aff5">
								<orgName type="department">INRIA -Département d&apos;Informatique de l&apos;</orgName>
								<orgName type="institution" key="instit1">Normale Supérieure</orgName>
								<orgName type="institution" key="instit2">PSL Research University</orgName>
								<address>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Gaël</forename><surname>Richard</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">LTCI</orgName>
								<orgName type="institution" key="instit1">Télécom Paris</orgName>
								<orgName type="institution" key="instit2">Institut Polytechnique de Paris</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Relative Positional Encoding for Transformers with Linear Complexity</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">6F93EBCCFE48982EFFCF0F945E582577</idno>
					<note type="submission">Submitted on 10 Jun 2021</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-04-12T14:50+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>published or not. The documents may come from teaching and research institutions in France or abroad, or from public or private research centers. L'archive ouverte pluridisciplinaire</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction 1.Linear Complexity Transformers</head><p>The Transformer model <ref type="bibr" target="#b55">(Vaswani et al., 2017)</ref> is a new kind of neural network that quickly became state-of-theart in many application domains, including the processing of natural language <ref type="bibr" target="#b19">(He et al., 2020)</ref>, images <ref type="bibr" target="#b14">(Dosovitskiy et al., 2020)</ref>, audio <ref type="bibr" target="#b23">(Huang et al., 2018;</ref><ref type="bibr" target="#b43">Pham et al., 2020)</ref> or bioinformatics <ref type="bibr" target="#b0">(AlQuraishi, 2019)</ref>   sion. Following classical non-parametric regression principles <ref type="bibr" target="#b39">(Nadaraya, 1964;</ref><ref type="bibr" target="#b62">Watson, 1964)</ref>, it consists in a simple weighted sum:</p><formula xml:id="formula_0">y m = n a mn v n n a mn ,<label>(1)</label></formula><p>where each attention coefficient a mn ∈ R + -gathered in the M × N matrix A -indicates how important the value v n is in the computation of the output y m .</p><p>One of the main contributions of the Transformer is an original method to compute these coefficients. D-dimensional feature vectors k n and q m are attached to all items of the input and output sequences and are called keys and queries, respectively. Gathering them in the N × D and M × D matrices K and Q, we get softmax dot-product attention as:</p><formula xml:id="formula_1">A = exp QK √ D ≡ [a mn = K(q m , k n )] mn ,<label>(2)</label></formula><p>where the function exp is applied element-wise. The righthand side in ( <ref type="formula" target="#formula_1">2</ref>) is a generalization introduced by <ref type="bibr" target="#b54">Tsai et al. (2019)</ref> and <ref type="bibr" target="#b7">Choromanski et al. (2020)</ref>, where K is a kernel function. Parameters pertain to how keys k n , values v n and queries q m are obtained from the raw sequences, usually by time-distributed fully connected layers.</p><p>The original Transformer architecture <ref type="bibr" target="#b55">(Vaswani et al., 2017)</ref> explicitly computes the attention matrix A, leading to a O(M N ) complexity that prevents it from scaling to very long sequence lengths. Although this is not necessarily a problem when sequence lengths are barely on the order of a few hundreds, as in some language processing tasks, it is prohibitive for very large signals like high-resolution images or audio.</p><p>Focusing on this scalability issue, several approaches have been recently investigated to allow for long sequences:</p><p>• Attention clustering schemes group items among which dependencies are computed through regular attention. This is either done by using simple proximity rules within the sequences, leading to chunking strategies <ref type="bibr" target="#b10">(Dai et al., 2019)</ref>, or by clustering the keys and values <ref type="bibr" target="#b47">(Roy et al., 2020)</ref>. Inter-cluster dependencies are either ignored or summarized via fixed-length context vectors that are coined in as memory <ref type="bibr" target="#b65">(Wu et al., 2020)</ref>. • Assuming the attention matrix to be sparse. In this case, only a few a mn are nonzero <ref type="bibr" target="#b6">(Child et al., 2019</ref>). • Assuming A has a particular (low-rank) structure and can be decomposed as the product of two smaller matrices.</p><p>A prototypical example is the Linformer <ref type="bibr">(Wang et al., 2020b)</ref>, which is limited to fixed-length inputs. Another very recent line of research in this same vein takes:</p><formula xml:id="formula_2">A ≈ φ(Q)φ(K) ,<label>(3)</label></formula><p>where φ : R D → R R is a non-linear feature map applied to each key k n and query q m , and R min(M, N ) <ref type="bibr" target="#b49">(Shen et al., 2020;</ref><ref type="bibr" target="#b27">Katharopoulos et al., 2020)</ref>.</p><p>• When K in (2) is a positive (semi)definite kernel, the Performer <ref type="bibr" target="#b7">(Choromanski et al., 2020)</ref> leverages reproducing kernel Hilbert spaces to show that a random φ may be used to exploit this convenient decomposition (3) on average, even when A is not low rank:</p><formula xml:id="formula_3">K 0 ⇔ A = E φ φ(Q)φ(K) ,<label>(4)</label></formula><p>where φ is drawn from a distribution that depends on K.</p><p>A simple example is φ W (k n ) = max(0, Wk n ), with a random W ∈ R R×D for some R ∈ N.</p><p>Whenever an efficient scheme like (3) or ( <ref type="formula" target="#formula_3">4</ref>) is used, the outputs can be obtained without computing the attention coefficients a mn , as in (10).<ref type="foot" target="#foot_0">1</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2.">Positional Encoding</head><p>In Transformer networks, the outputs y m are computed as linear combinations of all input values v n , weighted by attention coefficients a mn . In sequence modeling, it is reasonable to assume that the actual positions m and n should play a role in the computation, in addition to the content at these locations; otherwise, any permutation of the sequence would lead to the same output. Two core approaches were undertaken to incorporate position information:</p><p>• The original Transformer <ref type="bibr" target="#b55">(Vaswani et al., 2017)</ref> adds this information to the inputs of the network, i.e. before the first attention layer. This can be equivalently understood as augmenting the keys, values and queries:</p><formula xml:id="formula_4">k n ← k n + k n , v n ← v n + v n , q m ← q m + q m ,<label>(5)</label></formula><p>where we write k n ∈ R D for the keys positional encoding (PE; <ref type="bibr">Sukhbaatar et al., 2015)</ref> at position n ∈ N and analogously for values and queries. <ref type="bibr">Vaswani et al.</ref> propose a deterministic scheme based on trigonometric functions, which is shown to work as well as trainable embeddings. • As an example of positional encoding in the attention domain, a relative positional encoding (RPE) was proposed by <ref type="bibr" target="#b48">Shaw et al. (2018)</ref>, building on the idea that time lags m -n are more important than absolute positional encoding (APE) for prediction. It is written as:</p><formula xml:id="formula_5">A = exp QK + Ω √ D , with:<label>(6)</label></formula><formula xml:id="formula_6">Ω ≡ ω mn = D d=1 q md P d (m -n) mn .<label>(7)</label></formula><p>The terms P d now act as D different encodings for time lags selected based on the queries. This change is advocated as bringing important performance gains in many application areas and has enjoyed a widespread use ever since.</p><p>Although writing down the positional encoding in the attention domain is beneficial for performance <ref type="bibr" target="#b48">(Shaw et al., 2018;</ref><ref type="bibr" target="#b10">Dai et al., 2019;</ref><ref type="bibr" target="#b54">Tsai et al., 2019)</ref>, we are only aware of implementations that either require the computation of A, or clustered attention schemes, which in fine decompose A into smaller attention matrices, and compute them. This is in sharp contrast to (3) and ( <ref type="formula" target="#formula_3">4</ref>), which never compute the attention matrix.</p><p>Our contributions can be summarized as follows:</p><p>• We propose Stochastic Positional Encoding (SPE) as a general PE scheme in the keys domain, that enforces a particular attention pattern devised in the attention domain. This enables RPE without explicit computation of attention. To our knowledge, it is the first RPE strategy that is compatible with O(N ) Transformers like <ref type="bibr" target="#b7">Choromanski et al. (2020)</ref> and <ref type="bibr" target="#b27">Katharopoulos et al. (2020)</ref>. • We study the impact of SPE on performance on the Long-Range Arena benchmark <ref type="bibr" target="#b53">(Tay et al., 2021)</ref> and two music generation tasks. Since RPE was so far limited to short sequences, we believe this is the first study of its advantages on long-range predictions. Our results demonstrate better validation losses and extrapolation ability. • We provide additional resources on our companion website,<ref type="foot" target="#foot_2">2</ref> including Python implementations of SPE for Py-Torch and JAX/Flax.</p><p>Algorithm 1 Stochastic Positional Encoding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input</head><p>• position kernel P(m, n), number of replicas R.</p><p>• initial M × D and N × D queries Q and keys K.</p><p>Positional encoding:</p><formula xml:id="formula_7">• Draw the D independent couples {Q d , K d } d of M × R</formula><p>and N × R matrices as in section 2.1 • Set Q and K as in ( <ref type="formula" target="#formula_17">16</ref>) and ( <ref type="formula" target="#formula_18">17</ref>) Inference compute outputs Y with the O(N ) Transformer:</p><formula xml:id="formula_8">Y ← diag(d) -1 φ Q φ K V (10) with d = φ( Q) φ K 1 N and φ discussed in (3)/(4).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Stochastic Positional Encoding</head><p>Index set and notation. We assume that the input/output sequences are indexed by n, m ∈ T, where T is the index set. For regularly sampled sequences, we have T = N, but more settings are possible, like irregularly sampled time series (T = R) or images (T = N 2 ). In any case, the particular lists of input / output locations under consideration are written: N and M, with respective sizes N and M (the case N = M is called self-attention). The corresponding keys and values are hence indexed as {k n } n∈N and {v n } n∈N , while queries are {q m } m∈M . For convenience, we write a mn for the entries of the M × N attention matrix A.</p><p>We use bold uppercase for matrices, bold lowercase for vectors and a NumPy-like notation: if X k is a I × J matrix, x k,i and x k,:,j stand for its i th row and j th column, respectively.</p><p>Assumptions. In the remainder of this paper, we will seek an attention matrix A given by:</p><formula xml:id="formula_9">A = exp D d=1 q md P d (m, n)k nd mn √ D ,<label>(8)</label></formula><p>where {P d } D d=1 are position kernels. Defining P d ≡ [P d (m, n)] mn , this can be written in matrix form as:</p><formula xml:id="formula_10">A = exp D d=1 diag q :,d P d diag(k :,d ) √ D ,<label>(9)</label></formula><p>which is understood as having D positional attention templates P d jointly activated by the queries q :,d and keys k :,d .</p><p>Original RPE (7) can be seen as a special case, where some entries are kept constant.</p><p>Positional attention as covariance. The key idea for SPE is to see the attention kernel P d (m, n) as a covariance:</p><formula xml:id="formula_11">(∀M, N ) (∀m, n) P d (m, n) = E Q d (m)K d (n) ,<label>(11)</label></formula><p>where Q d (m) and K d (n) are two real and zero-mean ran-dom variables, which will be chosen with the single condition that their covariance function matches P d . Semantically, they should be understood as (randomly) encoding position m for queries and position n for keys, respectively. When multiplied together as in dot-product attention, they yield the desired attention template P d (m, n) on average. The central intuition is that the actual positional encodings do not matter as much as their dot-product.</p><p>In what follows, we will impose specific structures on the cross-covariance P d (m, n), which will in turn allow us to design random processes </p><formula xml:id="formula_12">Q d = {Q d (m)} m∈M and K d = {K d (n)} n∈N such that (11) holds.</formula><formula xml:id="formula_13">Q d ≡ [q d,m,r ∼ Q d (m)] mr , K d ≡ [k d,n,r ∼ K d (n)] nr . (<label>12</label></formula><formula xml:id="formula_14">)</formula><p>For large R, by the law of large numbers, we obtain:</p><formula xml:id="formula_15">P d ≈ Q d K d /R.<label>(13)</label></formula><p>This leads A in (9) to be given by:</p><formula xml:id="formula_16">A ≈ exp D d=1 diag q :,d Q d K d R diag(k :,d )/ √ D (14) ≈ exp D d=1 diag q :,d Q d D d=1 diag(k :,d )K d R √ D .<label>(15)</label></formula><p>Here, a crucial observation is that for large R, the crossterms Q d K d =d are negligible due to independence, provided that the means of the processes are selected to be zero. Finally, picking queries and keys as:</p><formula xml:id="formula_17">Q ← D d=1 diag q :,d Q d / 4 √ DR ,<label>(16)</label></formula><formula xml:id="formula_18">K ← D d=1 diag(k :,d )K d / 4 √ DR ,<label>(17)</label></formula><p>we see from (15-17) that we get back to the usual multiplicative scheme (2) with A = exp( Q K / √ R), where the queries/keys now have dimension R and can be used in (10) to directly get outputs without computing A.</p><p>The procedure is summarized in Algorithm 1: we provide a way (16-17) to achieve PE in the keys domain, such that the desired model ( <ref type="formula" target="#formula_9">8</ref>) is enforced in the attention domain, pa-rameterized by the attention kernels P d . Interestingly, this is done without ever computing attention matrices, complying with O(N ) Transformers. The remaining challenge, which we discuss next, is to generate Q d and K d enforcing (13).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Drawing Stochastic Positional Encodings</head><p>Inspecting (11), we notice that our objective is to draw samples from D pairs of centered random processes Q d , K d d , with a prescribed cross-covariance structure P d . It is reasonable to use Gaussian processes for this purpose <ref type="bibr" target="#b63">(Williams &amp; Rasmussen, 2006)</ref>, which have the maximum entropy for known mean and covariance. Such distributions are frequently encountered in geophysics in the co-kriging literature <ref type="bibr" target="#b37">(Matheron, 1963;</ref><ref type="bibr" target="#b17">Genton &amp; Kleiber, 2015)</ref>, where scientists routinely handle correlated random fields. The particular twists of our setup are: we have a generative problem, e.g. as in <ref type="bibr" target="#b57">Vořechovský (2008)</ref>; however, as opposed to their setting, we are not directly interested in the marginal covariance function of each output, provided that the desired cross-covariance structure holds.</p><p>The most straightforward application of SPE arises when we pick P d (m, n) = P d (m -n), i.e. a stationary position kernel, which was coined in as choosing relative attention in <ref type="bibr" target="#b48">Shaw et al. (2018)</ref> and boils down to enforcing a Toeplitz structure for the cross-covariance matrix</p><formula xml:id="formula_19">P d ≡ [P d (m -n)] m,n between Q d and K d .</formula><p>We propose two variants of SPE to handle this important special case, illustrated in Figure <ref type="figure" target="#fig_2">2</ref>. The first variant yields periodic covariance functions. It can be beneficial whenever attention should not vanish with large lags, as in traffic prediction <ref type="bibr" target="#b68">(Xue &amp; Salim, 2020)</ref> or, as we show, in music generation. The second variant generates vanishing covariance functions; a concept which has recently been shown useful <ref type="bibr" target="#b59">(Wang et al., 2021)</ref>, and notably yields smaller validation losses in some of our experiments.</p><p>Variant I. Relative and periodic attention (sineSPE). In our first approach, we consider the case where P d is periodic, which gets a convenient treatment. We assume:</p><formula xml:id="formula_20">P d (m, n) = K k=1 λ 2 kd cos(2πf kd (m -n) + θ kd ) , (<label>18</label></formula><formula xml:id="formula_21">)</formula><p>where K ∈ N is the number of sinusoidal components and</p><formula xml:id="formula_22">f d ∈ [0 1] K , θ d ∈ [-π π] K and λ d ∈ R K gather their K</formula><p>frequencies, phases, and weights, respectively. By using the matrix notation, we can rewrite (18) as:</p><formula xml:id="formula_23">P d = Ω(M, f d , θ d ) diag λd 2 Ω(N , f d , 0) , (<label>19</label></formula><formula xml:id="formula_24">)</formula><p>where v ≡ v p/2 p ∈ R 2K denotes a twice upsampled version of a vector v ∈ R K , • denotes the floor operation, and for an index set I, Ω(I, a, b) is a matrix of size |I| × 2K, with entries (0-based indexing):</p><formula xml:id="formula_25">[Ω (I, a, b)] nl = cos(2πa k n + b k ) if l = 2k sin(2πa k n + b k ) if l = 2k + 1</formula><p>It can be shown that if θ d = 0 and M = N , we get back to the (unique) Vandermonde decomposition for positive definite Toeplitz matrices<ref type="foot" target="#foot_3">3</ref>  <ref type="bibr" target="#b69">(Yang et al., 2016)</ref>, which boils down in our context to assuming that ∀τ, P d (0) ≥ P d (τ ). Since this is not always desirable, we keep the more general (19).</p><p>At this point, we can easily build Q d and K d . We draw a 2K × R matrix Z d with independent and identically distributed (i.i.d.) Gaussian entries of unit variance, and define:</p><formula xml:id="formula_26">Q d ← Ω(M, f d , θ d ) diag λd Z d / √ 2K ,<label>(20)</label></formula><formula xml:id="formula_27">K d ← Ω(N , f d , 0) diag λd Z d / √ 2K . (<label>21</label></formula><formula xml:id="formula_28">)</formula><p>It is easy to check that such a construction leads to (13). Its parameters are {f d , θ d , Λ d } d , which can be trained through stochastic gradient descent (SGD) as usual.</p><p>Variant II. Relative (vanishing) attention with regular sampling (convSPE). Due to their periodic structure, the covariance functions generated by Variant I are nonvanishing. Yet, our framework is flexible enough to allow for vanishing covariance structures, which may be more desirable depending on the application <ref type="bibr" target="#b59">(Wang et al., 2021)</ref>.</p><p>As opposed to Variant I, where we imposed a specific structure on P d , we will now follow an indirect approach, where P d will be implicitly defined based on our algorithmic construction. In this case, we assume that the signals are regularly sampled (typical in e.g. text, images, audio), and we will exploit the structure of Gaussian random matrices and basic properties of the convolution operation.</p><p>For ease of notation, we assume self attention, i.e. M = N . Let {Φ Q d , Φ K d } d denote a collection of filters, which will ultimately be learned from training data. The size and the dimension of these filters can be chosen according to the input data (i.e. can be vectors, matrices, tensors). We then propose the following procedure, which leads to a Toeplitz P d by means of convolutions: </p><formula xml:id="formula_29">Q d = Z d * Φ Q d , K d = Z d * Φ K d ,<label>(22)</label></formula><p>where * denotes convolution with appropriate dimension (e.g. 1D, 2D or 3D). Using convolutions with finite filters ensures vanishing covariance, as proven in the appendix.</p><p>Due to the independence of the entries of Z d , for large R, the product Z d Z d /R will tend to the identity matrix. Given the fact the convolution operations in ( <ref type="formula" target="#formula_29">22</ref>) can be equivalently expressed as a multiplication by triangular Toeplitz matrices constructed from the respective filters, it can be shown that, as R → ∞, 1 R Q d K d tends to the product of two triangular Toeplitz matrices. Hence, by using the properties of triangular Toeplitz matrices (cf. <ref type="bibr" target="#b31">Kucerovsky et al. 2016</ref>, Theorem 2.4), we conclude that, as R → ∞, our construction yields a Toeplitz matrix P d as desired. This approach is parameterized by the filters{Φ Q d , Φ K d } d , which will be learned from training data through SGD.</p><p>The variety of attention patterns P(m -n) that can be obtained directly depends on the kernel sizes, which is a classical result from signal processing <ref type="bibr" target="#b56">(Vetterli et al., 2014)</ref>. Cascading several convolutions as in the VGGNet <ref type="bibr" target="#b51">(Simonyan &amp; Zisserman, 2014</ref>) may be a convenient way to augment the expressive power of this convolutional SPE variant.</p><p>From a more general perspective, the two operations in ( <ref type="formula" target="#formula_29">22</ref>) can be understood as producing PE through filtering white noise, which is the core idea we introduce for PE. Other classical signal processing techniques may be used like using infinite impulse response filters. Such considerations are close to the ideas proposed in <ref type="bibr" target="#b15">(Engel et al., 2020)</ref>.</p><p>To summarize, the core difference between the two proposed constructions (20-21) and ( <ref type="formula" target="#formula_29">22</ref>) lies in the behaviour of RPE beyond a maximum lag, implicitly defined through the frequencies f d for (20-21) and through the sizes of the filters for ( <ref type="formula" target="#formula_29">22</ref>). While the sinusoidal construction leads to a periodic RPE, the filtering construction leads to a vanishing RPE, which is called monotonic in <ref type="bibr" target="#b59">(Wang et al., 2021)</ref>. Both may be the desired option depending on the application.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Gated SPE</head><p>Although RPE and the generalization (9) we propose are novel and efficient strategies to handle position information, it may be beneficial to also allow for attention coefficients that are computed without positional considerations, simply through q m , k n . As a general gating mechanism, we propose to weight between positional and non-positional attention through a gate parameter δ d ∈ [0 1]:</p><formula xml:id="formula_30">P d ≡ [δ d + (1 -δ d )P d (m, n)] m,n .<label>(23)</label></formula><p>This gating scheme can be implemented simply by augmenting Q d and K d generated as above through:</p><formula xml:id="formula_31">q d,m ← 1 -δ d q d,m + δ d d ,<label>(24)</label></formula><formula xml:id="formula_32">k d,m ← 1 -δ d k d,m + δ d d ,<label>(25)</label></formula><p>where d ∈ R R in ( <ref type="formula" target="#formula_31">24</ref>) and ( <ref type="formula" target="#formula_32">25</ref>) is the same and has i.i.d. standard Gaussian entries.</p><p>In practice, we can share some SPE parameters across the network, notably across layers, to strongly reduce computing time and memory usage. In our implementation, sharing means generating a single instance of Q and K for each head, on which a layer-wise gating is applied, before achieving PE through <ref type="bibr">(16)</ref><ref type="bibr">(17)</ref>. This is illustrated in Figure <ref type="figure" target="#fig_2">2</ref>. Experimental setup. We evaluate the proposed method in the Long-Range Arena (LRA; <ref type="bibr" target="#b53">Tay et al., 2021)</ref>, a benchmark for efficient Transformers, consisting of sequence classification tasks with a focus on long-range dependencies. We use the following tasks from this benchmark:</p><p>• ListOps: parsing and evaluation of hierarchical expressions. a longer variant of <ref type="bibr" target="#b40">(Nangia &amp; Bowman, 2018</ref>); • Text: movie review sentiment analysis on the IMDB corpus <ref type="bibr" target="#b36">(Maas et al., 2011</ref>); • Retrieval: article similarity classification on the All About NLP (AAN) corpus <ref type="bibr" target="#b44">(Radev et al., 2013</ref>); • Image: object recognition on the CIFAR10 dataset <ref type="bibr" target="#b30">(Krizhevsky, 2009)</ref> represented as pixel sequences. The tasks are challenging due to the large sequence lengths, deliberately increased by choosing a character-/pixel-level representation. An overview of the tasks can be found in the appendix. We do not include Pathfinder (a synthetic image classification task) as we were unable to reproduce the results of Tay et al. on this task, even through correspondence with the authors.</p><p>We evaluate SPE (the gated variant) on two efficient Transformer models: the (softmax) Performer <ref type="bibr" target="#b7">(Choromanski et al., 2020)</ref>, and a Linear Transformer <ref type="bibr" target="#b27">(Katharopoulos et al., 2020)</ref> with a ReLU feature map, i.e. choosing φ(•) = max(0, •) element-wise in (3). <ref type="foot" target="#foot_4">4</ref> It should be noted that the ReLU feature map does not approximate the softmax kernel, which SPE is designed for (see assumption 8). Nevertheless, it is possible to use SPE with any feature map in practice, allowing us to include Linear Transformer-ReLU as an interesting test of generalization to alternative kernels.</p><p>We adopt the configuration of Tay et al., only changing the PE and the batch sizes/learning rates to allow training on limited hardware with similar results. All other hyperparameters are kept identical to the original LRA. It is worth noting that the Image models are different from the rest in that they employ a single-layer network and only use the first position for prediction, dramatically limiting their ability to benefit from relative positional information.</p><p>Since we observe some variation between different runs, we train and evaluate each model 3 times (except for Performer with convolutional SPE, which is computationally more costly) and report the mean and standard deviation of the results.</p><p>The results of the benchmark are given in Table <ref type="table" target="#tab_2">1</ref>. The accuracies achieved by the baseline Linear Transformer-ReLU (APE) are similar to or surpass those reported by Tay et al., which is a clear validation of our experimental setup.</p><p>Discussion. Results on ListOps are poor overall, with accuracies around 17 %. This complies with <ref type="bibr" target="#b53">Tay et al. (2021)</ref>, who reasoned that "kernel-based models [e.g. Performer, Linear Transformers] are possibly not as effective on hierarchically structured data," leaving room for improvement. We also hypothesize this is largely due to some known issues with the training data for this task, which unfortunately have not been fixed at the time of this writing. <ref type="foot" target="#foot_5">5</ref>Regarding performance of SPE, we first notice that the sineSPE variant yields the best results on three tasks, which is a strong achievement and validates our approach, especially considering the difficulty of this evaluation benchmark. While it is only marginally better than APE for ListOps and Text, it is worth mentioning that sineSPE combined with the Linear Transformer-ReLU yields an accuracy improvement of ∼3 % on Retrieval compared to the best result obtained by <ref type="bibr" target="#b53">Tay et al. (2021)</ref>.</p><p>Regarding convSPE, its performance in the LRA is not as remarkable as it is for the music generation experiment reported later in section 3.2. This mitigated result appears somewhat in contradiction with the discussion found in <ref type="bibr" target="#b59">Wang et al. (2021)</ref>, which presents vanishing attention as a desirable property of PE. On the contrary, we empirically observe that our non-vanishing sinusoidal version sineSPE does behave better in these particular tasks.</p><p>Finally, the superior results of APE on Image are not unexpected, given the limited ability of these models to exploit relative positions. On the contrary, the relatively good performance of SPE on this task is in fact remarkable, especially considering that the baseline systems for this task use learnable APE.</p><p>As we will see later in our music generation experiments, there are tasks where our proposed SPE clearly yields remarkable improvements. Here in the LRA, we notice that it does not result in an obvious and systematic boost in performance. This raises interesting considerations:</p><p>(i) The variance of the Monte Carlo estimator might be problematic. We are enthusiastic about the elegant formulation of stochastic feature maps as in the Performer, which was a strong inspiration. Still, we must acknowledge that their computation relies on a Monte Carlo estimator (15). We suspect that the variance of the estimator might play a role in the final performance in large dimensions, which opens up the direction of exploring variance-reduced estimation methods, rather than plain Monte Carlo.</p><p>(ii) LRA tasks might not benefit from strong (R)PE schemes. The LRA was designed to compare Transformer architectures, filling a gap in this domain and standing as the de facto standard, justifying our choice. Still, although PE is known to be important in many cases, it is not known whether it is so in the LRA tasks. We feel that there is room for such a specialized comparison, which is scheduled in our future work, possibly leading to new long-range tasks where PE is critical.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Pop Piano Music Generation</head><p>In our music generation experiments (this subsection and section 3.3), music is represented as sequences of symbols (tokens) and a Performer <ref type="bibr" target="#b7">(Choromanski et al., 2020)</ref> is used as an autoregressive language model, which predicts a probability distribution over the next token given the past context. At test time, a new sequence is generated by iteratively sampling the next token, as commonly done in text generation.</p><p>Experimental setup. We train Performers for music generation, with 24 layers and 8 heads per layer on a dataset composed of 1 747 pop piano tracks, encoded using the recently proposed Revamped MIDI-derived format (REMI; <ref type="bibr" target="#b24">Huang &amp; Yang, 2020)</ref>. The sequences are composed of metrical tokens: bar, subbeat, and tempo, which represent musical timing; and note tokens: chord, pitch, duration, and volume, which describe the musical content (see the appendix for more details). We hold out 5% of the songs as the validation set.</p><p>We train the models with sequence length N = 2 048, corresponding to ∼1 minute of music. The only difference between our models is the PE strategy. We consider baseline APE, as well as SPE: sinusoidal or convolutional, with or without gating, resulting in 5 different models.</p><p>Results and discussion. For qualitative assessment, we first display in Figure <ref type="figure" target="#fig_1">1</ref> one attention pattern for each PE model: APE and (gated) sineSPE/convSPE, obtained as an average over 20 from-scratch generations for a chosen (layer, head). More similar plots can be found in appendix. Interestingly, we notice that for early layers, APE attention does not go much beyond training sequence length. This behaviour is not found in SPE variants, which consistently attend to all positions. Another remarkable feature of the proposed model (only displayed in the appendix) is that gating as described in section 2.2 visually disables PE altogether for some layers/heads, in which case attention is global.</p><p>Since the literature suggests that RPE improves generalization performance <ref type="bibr" target="#b48">(Shaw et al., 2018;</ref><ref type="bibr" target="#b70">Zhou et al., 2019;</ref><ref type="bibr" target="#b46">Rosendahl et al., 2019)</ref>, we display validation cross-entropy computed with teacher forcing <ref type="bibr" target="#b64">(Williams &amp; Zipser, 1989)</ref> in Figure <ref type="figure" target="#fig_3">3</ref>, as a function of the target token position. The values would indicate how well the models predict the token at a certain position given the preceding tokens, for tracks in the validation set. We notice that all SPE variants, especially convSPE, behave much better than APE for token positions beyond 2 048. This suggests that SPE inherits this celebrated advantage of RPE <ref type="bibr" target="#b23">(Huang et al., 2018)</ref> while being applicable to much longer sequences.</p><p>Recently, <ref type="bibr" target="#b59">Wang et al. (2021)</ref> defined metrics for the evaluation of PE, suggesting that translation invariance and monotonicity are desirable properties. The former states that the distances of two arbitrary τ -offset position embeddings should be identical, while the latter states that neighboring  positions should be assigned with position embeddings that are closer than faraway ones. Following their identical word probing methodology, we report these metrics in Figure <ref type="figure" target="#fig_4">4</ref>. As expected, SPE variants greatly outperform APE in terms of translation invariance. However, monotonicity does not seem a very relevant criterion in our music application, as can be seen when comparing scores in Figures <ref type="figure" target="#fig_3">3</ref> and<ref type="figure" target="#fig_4">4</ref>. It seems that music modeling can benefit from non-vanishing attention patterns. In any case, SPE scores are remarkably stable across positions, contrarily to APE, which rapidly degrades beyond the training length.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Groove Continuation</head><p>In this experiment, we evaluate Performers on a groove continuation task. After training on a dataset where each example has a uniform style ('groove'), we prime the model with a short prompt (2-bar musical fragment) and let it generate a continuation. We then observe whether the generated continuation matches the style of the prompt. We use two musically motivated style similarity metricstime-pitch and onset-duration proposed by <ref type="bibr" target="#b8">Cífka et al. (2019;</ref><ref type="bibr" target="#b2">2020)</ref> -to quantify the similarity of the generated continuation to the prompt. When listening to the generated music, we perceptually notice a drift in quality along time. For this reason, we divide each generated sample into four successive chunks of identical duration and evaluate them independently. The results are displayed in Figure <ref type="figure" target="#fig_5">5</ref>.</p><p>Discussion. We clearly see that SPE substantially outperforms APE in both metrics. Although APE visibly does manage to generate close to the desired style at the beginning of the sequence, this similarity strongly degrades over time. Both sineSPE and convSPE are much more stable in this regard, confirming the result from section 3.2 that SPE extrapolates better beyond the training sequence length. This matches our informal perceptual evaluation. <ref type="foot" target="#foot_6">6</ref>This experiment suggests that exploiting a local neighborhood is a robust way to process long sequences. This could appear as contradicting the use of long-range Transformers, but we highlight that gating is used here, enabling some heads to exploit long term-attention independently from position. Further comparisons with local attention schemes (e.g. <ref type="bibr" target="#b10">Dai et al., 2019;</ref><ref type="bibr" target="#b20">Hofstätter et al., 2020)</ref> could be interesting, although they were not included here due to <ref type="bibr" target="#b53">Tay et al. (2021)</ref> suggesting that they are clearly inferior, at least in the LRA setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Related Work</head><p>This paper is concerned with PE <ref type="bibr">(Sukhbaatar et al., 2015)</ref>, as a way to embed the position of each token as part of its features. This idea is a core ingredient for many subsequent groundbreaking studies <ref type="bibr" target="#b16">(Gehring et al., 2017;</ref><ref type="bibr" target="#b55">Vaswani et al., 2017)</ref>, and has been the actual topic of many investigations.</p><p>Absolute Positional Encoding (APE) based on sinusoids from <ref type="bibr" target="#b55">Vaswani et al. (2017)</ref> is the most widely used for Transformer-like architectures. However, PE q(n) and k(n) in ( <ref type="formula" target="#formula_4">5</ref>) can also be trained as in BERT <ref type="bibr" target="#b12">(Devlin et al., 2019;</ref><ref type="bibr" target="#b34">Liu et al., 2019)</ref>. Although the original Transformer only includes PE at the input layer, it may be included at all layers <ref type="bibr" target="#b11">(Dehghani et al., 2019;</ref><ref type="bibr" target="#b32">Lan et al., 2020)</ref>.</p><p>Relative positional encoding (RPE; <ref type="bibr" target="#b48">Shaw et al., 2018)</ref> is a way to leverage relative positions. It came with a O(N 2 D) space complexity, which was reduced to O(N 2 ) in <ref type="bibr" target="#b23">Huang et al. (2018)</ref>; <ref type="bibr" target="#b19">He et al. (2020)</ref>. Considering log-distances was proposed in <ref type="bibr" target="#b45">Raffel et al. (2020)</ref>. Several variants for RPE were introduced <ref type="bibr">(Huang et al., 2020;</ref><ref type="bibr" target="#b59">Wang et al., 2021)</ref>. They all apply learned RPE in the attention domain. Using fixed embedding functions was also considered for RPE <ref type="bibr" target="#b43">(Pham et al., 2020)</ref>, and masking RPE is used in <ref type="bibr">Kim et al. (2020)</ref> to promote local attention.</p><p>Keys-domain vs attention domain. Doing PE in the keys domain introduces position-content cross terms that are advocated as noisy and not beneficial in <ref type="bibr" target="#b28">Ke et al. (2020)</ref> and replaced by Untied attention, i.e. PE in the attention domain. This is also called disantangled attention in <ref type="bibr" target="#b19">He et al. (2020)</ref> and already proposed in <ref type="bibr" target="#b54">Tsai et al. (2019)</ref> through separable content-position attention kernels. All of these studies require the explicit computation and storage of A.</p><p>Non-integer positions were considered for structured inputs. Tree-based PE was proposed both for APE <ref type="bibr" target="#b50">(Shiv &amp; Quirk, 2019;</ref><ref type="bibr" target="#b66">Xiao et al., 2019;</ref><ref type="bibr" target="#b35">Ma et al., 2019)</ref> and RPE <ref type="bibr" target="#b42">(Omote et al., 2019)</ref>. Positional encoding of robots within arbitrary polygons is found in <ref type="bibr" target="#b4">Bose et al. (2019)</ref>.</p><p>Dynamical models for PE. Attention for machine translation was introduced in <ref type="bibr" target="#b1">Bahdanau et al. (2016)</ref>, which was retrospectively understood in <ref type="bibr" target="#b28">Ke et al. (2020)</ref> as using recurrent neural nets (RNN) for PE. In <ref type="bibr" target="#b5">Chen et al. (2018)</ref>, the hidden states of encoder RNNs are said to contain enough position information to skip explicit PE. <ref type="bibr" target="#b41">Neishi &amp; Yoshinaga (2019)</ref> builds on this view, but explicitly describes the idea for the first time. Their contribution is to replace the additive PE in (5) by an RNN. In the same vein, <ref type="bibr" target="#b33">Liu et al. (2020)</ref> generates PE using (neural) ordinary differential equations.</p><p>Convolutional contexts. Our convSPE variant involves convolving random noise. First, this can be related to <ref type="bibr" target="#b38">Mohamed et al. (2019)</ref>, who use convolutional neural networks for queries and keys computation. Second, the connections between convolutions and stationary processes have recently been highlighted by <ref type="bibr" target="#b67">Xu et al. (2020)</ref> as enforcing PE.</p><p>Multiplicative PE. Various levels of content-position interactions are formalized in <ref type="bibr" target="#b54">(Tsai et al., 2019)</ref>. Multiplicative strategies were proposed for both RPE <ref type="bibr">(Huang et al., 2020)</ref> and APE <ref type="bibr" target="#b10">(Dai et al., 2019)</ref>. The latter was generalized in <ref type="bibr" target="#b54">Tsai et al. (2019)</ref>. All these require the explicit computa-tion of the attention matrix. <ref type="bibr">Wang et al. (2020a)</ref> presents a scheme that is close to our sinusoidal variant, but without the stochastic part that is the key to go from ( <ref type="formula">14</ref>) to (15).</p><p>The limits of APE and RPE were highlighted by some authors. In <ref type="bibr">Wang &amp; Chen (2020)</ref>, the best performing models exploit both absolute and relative positions. In <ref type="bibr" target="#b26">Irie et al. (2019)</ref> and <ref type="bibr" target="#b54">Tsai et al. (2019)</ref>, it is found that removing APE altogether in the causal decoder part of Transformer-based architectures leads to comparable/better performance. It is also not clear which one is best between incorporating PE in the raw input signal (and hence propagating it through the value entries) or using it anew on the queries and keys only, as we do. Our choice is backed by <ref type="bibr" target="#b54">Tsai et al. (2019)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We propose a new Stochastic Positional Encoding (SPE), based on filtering random noise. As we show, the procedure generalizes relative PE and is a principled means to enforce any prescribed (but trained) cross-covariance structure, which we demonstrated should be the central concern in dot-product attention. In our experiments, we show that SPE brings an interesting gain in performance for large-scale transformer models <ref type="bibr" target="#b7">(Choromanski et al., 2020;</ref><ref type="bibr" target="#b27">Katharopoulos et al., 2020)</ref>, as compared to classical (sinusoidal) PE. This was expected, because RPE <ref type="bibr" target="#b48">(Shaw et al., 2018)</ref> is often advocated as beneficial. However, no way to incorporate it for long sequences was available so far and this is the core contribution of this paper. The natural future directions for our study are (i) Signal-dependent PE that incorporates the input sequence as an additional input for SPE, (ii) Nonstationary PE that utilizes both relative and absolute positions, (iii) Extending our approach to arbitrary attention kernels, e.g. defined implicitly through their (random) mappings as in (4). Indeed, SPE as it is presented here holds theoretically for dot-product attention kernels only, but our results given in Table <ref type="table" target="#tab_2">1</ref> suggest that this generalizes, asking an interesting research question.</p><p>when operating the convolutions in the frequency domain, which is advantageous for long filters. In higher dimensions, say 2D, this becomes O(DRN 1 N 2 P 1 P 2 ) in the original domain and O(DRN 1 N 2 log N 1 log N 2 ) in the frequency domain, where (N 1 , N 2 ) and (P 1 , P 2 ) are the shapes of noise and filters, respectively. • The bottleneck of gating is the generation of random noise d , which has complexity O(DR).</p><p>Note that this complexities of course must be multiplied by the number of heads considered, up to 8 in our experiments.</p><p>As can be seen, the complexities of the sinusoidal and convolutional variants are similar, depending on the length P of the filters and the number K of sinusoids.</p><p>Still, other aspects come into the play. First, the convolutional version requires generating noise whose size scales as N , while the sinusoidal version requires much smaller 2K-large noise matrices. Second, only a very small number of sinusoids was required in our experiments, whereas the convolutional version required longer contexts, so that we often had 2K P in practice. Finally, although this may change in the near future, deep learning frameworks like Py-Torch do not easily integrate convolutions in the frequency domain.</p><p>Sample-wise noise sharing. In practice, SPEs do not need to be drawn anew for each example. The most straightforward trick to reduce memory and computational footprint of the method is to share Q and K among all examples in each mini-batch, as we do in all our experiments. This can bring significant memory savings when SPE is used as a drop-in addition to networks trained with large batch sizes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Experimental Setup: Long-Range Arena</head><p>An overview of the Long-Range Arena <ref type="bibr" target="#b53">(Tay et al., 2021)</ref> tasks is given in table 2. We do not include Pathfinder (a synthetic image classification task) or its harder variant Pathfinder-X in this paper as we were unable to reproduce the results of <ref type="bibr">Tay et al. on</ref>  In all LRA experiments, we employ gated SPE with R ∈ {32, 64}. We consistently use K = 10 for sinusoidal (periodic) SPE and filters of length 128 for convolutional SPE. For convolutional SPE, we share Q and K across all layers (but not across attention heads); for sinusoidal SPE, Q and K are unique to each layer and head; in both cases, layerspecific gating is employed. Baseline experiments employ the same absolute positional encodings as Tay et al. (learn-7 https://github.com/google-research/ long-range-arena able APE for Image and sinusoidal APE for the remaining tasks). In models employing SPE, APE is removed.</p><p>The numbers of parameters of the models presented in the main document are shown in Table <ref type="table" target="#tab_5">3</ref>. We can see that SPE-based models have at most 3.1 % more parameters than the baselines. In the Image column, the numbers for SPEbased models are about 50 % lower due to the fact that the baselines on this task employ learnable APE.</p><p>We use code from the official LRA repository, including the authors' Transformer implementation, modified as necessary to incorporate SPE. We keep the same training configuration as provided by the LRA authors, but decrease the batch sizes (from 256 to 96 for Image and from 32 to 8 for the rest) and learning rates so as to fit within 16 GB of GPU memory. Our modified code and configuration files are available in our source code repository.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1. Resource usage</head><p>The typical training times of the LRA models are displayed in Table <ref type="table" target="#tab_6">4</ref>. Note that the times may not be comparable across models or tasks due to evaluation (which may be time-consuming) being done more frequently in some runs than others.</p><p>The total training time was 1 405 h (189 runs in total), out of which 273 h (61 runs) were spent on attempts to reproduce the results of <ref type="bibr" target="#b53">Tay et al. (2021)</ref> using Performer-softmax, Linear Transformer-ReLU and vanilla Transformer. Some of these preliminary experiments were distributed over 1-3 Tesla V100 GPUs with 32 GB of memory each. The final models were all trained on a single Tesla V100 or P100 GPU with 16 GB of memory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Experimental Setup: Music Generation</head><p>Our music Performers are implemented using the pytorch-fast-transformers package,<ref type="foot" target="#foot_7">8</ref> modified as necessary to incorporate SPE. The modified code and configuration files are available in our code repository.</p><p>All models have 24 layers with model dimension 512, 8 attention heads and 2 048 feed-forward units, which amount to ∼80 million trainable parameters. In models that use SPE, Q and K are shared across all layers (but not across attention heads); layer-specific gating is employed for models trained with gated SPE.</p><p>The models are trained with the Adam optimizer. We schedule the learning rate with linear warmup, followed by cosine decay. Full details of hyperparameters can be found in the provided configuration files.  <ref type="formula">2021</ref>), opensourced on GitHub. 9 It consists of 1,747 pure piano performances of various Japanese, Korean, and Western pop songs, amounting to a total duration of ∼100 hours. All the songs are in 4/4 time signature, namely four beats per bar (measure). We leave 5% (87 songs) as the validation set.</p><p>According to <ref type="bibr" target="#b22">Hsiao et al. (2021)</ref>, the piano performances are originally collected from the Internet in the MP3 (audio) format. Hsiao et al. further employed Onsets and Frames piano transcription <ref type="bibr" target="#b18">(Hawthorne et al., 2018)</ref>, madmom beat tracking tool <ref type="bibr" target="#b3">(Böck et al., 2016)</ref>, and chorder rule-based chord detection 10 to transcribe the audio into MIDI format with tempo, beat, and chord information.</p><p>Data representation. The representation adopted here is largely identical to the Revamped MIDI-derived (REMI) encoding by <ref type="bibr" target="#b24">Huang &amp; Yang (2020)</ref>, except that an extended set of chord tokens (described below) is used. REMI encodes a piano piece into a sequence composed of two types, metrical and note, of tokens. The metrical tokens are:</p><p>• bar: Marks the start of a musical bar.</p><p>• subbeat: Marks the musical timing within a bar. A bar is divided into 16 subbeats, which is equivalent to 4 beats. This symbolic timing provides an explicit time grid for sequence models to model music. • tempo: Determines the pace (in beats per minute, or bpm) at which the piece is played, varied per bar. The range of tempo tokens is <ref type="bibr">[32,</ref><ref type="bibr">224]</ref> bpm, in steps of 3 bpm for quantization.</p><p>9 https://github.com/YatingMusic/ compound-word-transformer 10 https://github.com/joshuachang2311/ chorder</p><p>The note tokens are:</p><p>• pitch: Marks a note played. The 88 pitch-es correspond to each key on the piano. • duration: Denotes the length of a played note, ranging from 1/2 to 16 subbeats, in steps of 1/2 subbeat. • volume (or, velocity): Denotes how loud a note is played.</p><p>A total of 24 volume levels are considered. • chord: Marks a change on the accompanying chord.</p><p>Each chord is described by its root note and quality, e.g., C-Maj7, E-min. A total of 133 distinct chord tokens are found in the dataset.</p><p>Please note that a single note played is represented by a cooccurring triple of (pitch, duration, volume). The aforementioned tokens constitute a vocabulary of size ∼340 for our REMI encoding. On average, we need a sequence with 5 300 tokens to represent a song.</p><p>Training and inference. In each training epoch, we randomly crop a segment of length 2 048 from each sample, and shift the pitches of the entire segment by -6 to 6 semitones randomly (this is called transposition in music) as data augmentation. We use batch size = 4, and set the learning rate to 0.0001 for APE and 0.0002 for all SPE models. For sineSPE, we choose the number of sines K = 5; for convSPE, the convolutional filter size is set to be 128, 512 for the gated and ungated variants respectively.</p><p>Detailed resource usage of each model is shown in Table <ref type="table" target="#tab_7">5</ref>.</p><p>During inference, we employ nucleus sampling <ref type="bibr" target="#b21">(Holtzman et al., 2019)</ref> with p = 0.9 and softmax temperature t = 1.2.</p><p>No post-processing on enforcing the grammatical correctness of the generated sequence is done.</p><p>Validation loss of the models trained on this task is listed in Table <ref type="table" target="#tab_8">6</ref>. On this metric, our convSPE variant performs the  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2. Groove Continuation</head><p>Training data. The Groove2Groove MIDI dataset 11 consists of accompaniments generated by the Band-in-a-Box software (BIAB). 12 We only use the training section of the Groove2Groove MIDI dataset and perform a custom training/validation/test split such that each section contains a unique set of BIAB styles (2 761 for training and 50 each for validation and testing). The code necessary to download, pre-process and split the dataset is included in the repository.</p><p>We convert each accompaniment to a trio consisting of bass, 11 http://doi.org/10.5281/zenodo.3958000 12 https://www.pgmusic.com/ drums and another randomly selected accompaniment track (e.g. piano, guitar). We then perform random data augmentation by skipping measures at the beginning, dropping some of the instruments, and transposition (pitch-shifting by -5 to +5 semitones). All randomization is done anew in each epoch.</p><p>Data representation. We use a representation similar to the one proposed by <ref type="bibr" target="#b9">Cífka et al. (2020)</ref>, but adapted to a multi-track (multi-instrument) setting. Specifically, we encode a piece of music as a sequence of the following types of event tokens, each with two integer arguments:</p><p>• note on(track, pitch): Begins a new note at the given pitch (0-127). • note off(track, pitch): Ends the note at the given pitch (0-127). • time shift(beats, offset): Advances current time by a given number of beats and then sets the offset within the beat, given as the number of ticks from its beginning (0-11). Maximum possible shift is (2, 0).</p><p>The track numbers range from 1 to 3, where 1 is always bass and 2 is always drums. The vocabulary of the model then consists of 794 tokens (3×128 note-ons, 3×128 note-offs, 24 time shifts, and 2 beginning-/end-of-sequence markers).</p><p>The main differences to the representation described in Section C.1 are a more compact encoding of timing, no representation of musical dynamics (for simplicity), and support for multiple tracks (not originally proposed by <ref type="bibr" target="#b9">Cífka et al., 2020</ref> but introduced here inspired by <ref type="bibr" target="#b13">Donahue et al., 2019)</ref>.</p><p>Training and inference. During training, each example is pre-processed and encoded as described above and the resulting token sequence is truncated to a length of 512. We train each model for a total of 24 epochs.</p><p>At test time, we sample with a softmax temperature of 0.6. We disallow sampling tokens that would result in invalid sequences (i.e. spurious note-offs, backward time shifts) in order to ensure that the generated sequence can be correctly decoded.</p><p>Various training details. Hyperparameter tuning was mostly performed in preliminary experiments (∼100 runs); these were mostly done on other variants of the dataset and with different sequence lengths (ranging from 256 to 20 k); this includes experiments discarded due to bugs discovered during or after training. Learning rates between 0.0001 and 0.0008 and batch sizes between 1 and 24 were considered.</p><p>For SPE, we considered both the gated and ungated variants with as many realizations as fit in memory (between 16 and 64). Model selection was based on validation loss and informal perceptual evaluation. Only a minimal attempt at further learning rate tuning was made for the final set of models with length 512, which did not appear to be particularly sensitive to it, and we chose to keep the initial learning rate 0.0004, which was found to perform well in all cases.</p><p>The models included in the main document -APE, sineSPE and convSPE -all use a batch size of 10 and finished training in about 3 h, 5 h and 6 h, respectively, using 9.7 GB, 14.4 GB and 14.8 GB of GPU memory. The total training time including all preliminary experiments was 852 hours.</p><p>Evaluation metrics. We use the objective metrics proposed by <ref type="bibr" target="#b8">Cífka et al. (2019;</ref><ref type="bibr" target="#b2">2020)</ref> to measure the style similarity between the generated continuation and the file from which the prompt was extracted. Given two pieces of music, each metric gathers musical event statistics of the two pieces in histograms called style profiles, and then computes the cosine similarity between them.</p><p>The two metrics used here, onset-duration and time-pitch, differ in what kind of events they use to construct the style profile:</p><p>• The onset-duration profile is defined as a 2D histogram relating note onset positions to note durations. More precisely, for all notes a in a piece of music, it records a tuple of the form where a, b is a pair of notes and pitch(•) represents the pitch of a note as its MIDI note number (the number of semitones from C -1 ). The histogram has 24 × 41 bins (24 for time lags between 0 and 4 beats and 41 bins for intervals between -20 and 20 semitones).</p><p>In both cases, the 2D histograms are flattened to vectors before computing cosine similarities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Additional Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.1. Attention Visualization: Music Generation</head><p>In this section, we display attention patterns produced by our pop piano music generation models.</p><p>Learned positional templates. We share the SPE modules across all layers of the Performer, but not across the attention heads, resulting in 512 learned positional kernels P d ) (number of heads × key dimensions per head. In Figure <ref type="figure">7</ref>, we display 16 randomly picked resulting templates P d for both sineSPE and convSPE, trained with gating. Details of the two variants are:</p><p>• sineSPE: We set the number of sines K = 5.</p><p>• convSPE: We use filters of size 128.</p><p>In accordance with the definition, all of the visualizations are plotted with the equation P d = Q d K d , which we never need to explicitly compute for linear transformers. From Figure <ref type="figure">7</ref>, we can observe that sineSPE learns to exploit a wide range of frequencies, and that convSPE is effective within small query-key offsets corresponding to the filter size, as expected.</p><p>Full Attention. Although the full attention matrix A is not computed in linear transformers, we can still obtain it offline by multiplying queries and keys through either A = exp(QK / √ D) (in the case of APE, where D is the key dimensions per head), or A = exp( Q K / √ R) (in the case of SPEs); then apply row-wise softmax operation on A as normalization.</p><p>Here, we present the (softmax-ed) attention matrices in the 1st, 3rd, 12th, 20th, and 24th (last) layers of all the five models trained on pop piano music generation in Figures 8-12. These are computed from one of each model's random from-scratch music generations. To examine the models' extrapolation ability, we let them generate a sequence of length 3 072, while the training sequence length is only 2 048. The attention matrices are lower-triangular due to causal masking. For better visualization, the color of each pixel is adjusted through min{1, a mn 0.4 /0.02 0.4 } in the plots, where a mn ∈ [0, 1] is the softmax-ed attention score. tokens beyond position 2 048 (the training sequence length) seems to concentrate around 2 048 in earlier layers, rather than paying global or local attention. Such behavior is not seen in any of our SPE models. This potentially explains APE's poor generalization to long sequences suggested by the stark increase in validation loss after position 2 048 (see Figure <ref type="figure" target="#fig_3">3</ref> in the main paper, and Table <ref type="table" target="#tab_8">6</ref> here).</p><p>Next, comparing Figures 9 and 10, it is obvious that gated SPE gives the model the freedom to switch off PE in some heads to achieve global attention (see Figure <ref type="figure">9</ref>), whereas the attention of ungated sineSPE (Figure <ref type="figure" target="#fig_1">10</ref>) largely stays periodic, which might not be always desirable. The same can be said for convSPE <ref type="bibr">(Figures 11 and 12)</ref>. The gated convSPE is able to look much further back in the middle layers than its ungated counterpart.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2. Attention Visualization: CIFAR10</head><p>Figure <ref type="figure" target="#fig_3">13</ref> displays attention maps extracted from models trained on the LRA CIFAR10 task. Note that these are onelayer networks, and classification is done by prepending a special CLS token to the sequence of pixel values and using the output at this first position as input to a feedforward classifier. Consequently, only the attention map at this single position (which is the one we display here) matters. (The model is therefore de facto not using selfattention, but rather attention with a single query and many keys. This removes the distinction between relative and absolute positions, which might explain why trainable APE performs better than SPE on this task.)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.3. Evaluation of Desired PE Properties</head><p>We employ identical word probing and the associated metrics introduced in <ref type="bibr" target="#b59">Wang et al. (2021)</ref> to compare the translation invariance and monotonicity properties of APE and our SPEs. The other properties mentioned in that work, namely symmetry and direction balance, are not evaluated here since the attention is uni-directional in our case. The models are also trained on pop piano music generation.</p><p>The metrics are calculated from attention matrices of each head in the 1st layer, averaged over all possible identicaltoken sequences (i.e., a sequence composed of repeated, same tokens; there are ∼340 of them for our REMI vocabulary). To eliminate the impact of applying row-wise softmax with causal masking on the translation invariance property, we compute the metrics on the unnormalized attention matrices, i.e., A = exp(QK / √ D) for APE, and</p><formula xml:id="formula_33">A = exp( Q K / √ R)</formula><p>for SPEs. Various combinations of query positions and query-key offsets are considered to examine whether the PE properties stay consistent when we extrapolate to longer sequences, as well as to look into their behavior in local and long-range attention spans.</p><p>We report the scores of the best-performing (i.e., lowestscoring) head of each model in Table <ref type="table">7</ref>. From the table, we can notice that the PE properties of APE often deteriorate drastically in cases of extrapolation. On the contrary, the scores of ungated SPE models, i.e., models in which we enforce the incorporation of positional information in every layer, remain remarkably consistent throughout the positions. The evaluation here provides additional evidence for the extrapolation capability of SPEs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.4. Impact of the Number R of Realizations</head><p>In the main document, we discussed how SPE asymptotically leads to the desired cross-covariance structure as R grows to infinity. In this section, we empirically study how performance is affected by that parameter in practice. A first thing to highlight is that each training batch yields a new set of realizations for the noise Z d , so that the network sees the right attention pattern on average. However, we may wonder whether how the number of realizations R impacts training and test performance. One can indeed notice that R may totally be set differently during training and inference, since it has no impact on the shape of the actual parameters/structure of the model. For this reason, we performed an ablation study where we use different values for R train at training time, resulting in a trained model, and then evaluate its performance using a possibly different value R test . The results are displayed in Figure <ref type="figure" target="#fig_4">14</ref>.</p><p>We can notice that the result achieved with R test = R train (highlighted in bold) is consistently close to the best result for the same R train , and conversely, choosing R test = R train often leads to a poor result. In other words, training and testing with the same R appears to be favorable for consistently good performance.</p><p>Another remarkable fact is that a higher R does not seem to imply better performance, even when R test = R train . On the contrary, convSPE achieved by far the highest accuracy with R = 4. This unexpected result seems contradictory to the fact that it means noisier attention patterns. Further investigation is required to explain this phenomenon, but we conjecture that this additional noise in the attention patterns leads to increased robustness of the trained model, helping generalization.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>to mention just a few. The core, novel component of the Transformer is the attention layer. It computes M output values y m from N input values v n , all being vectors of an arbitrary dimen-Proceedings of the 38 th International Conference on Machine Learning, PMLR 139, 2021. Copyright 2021 by the author(s).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Examples of attention patterns observed in the Performers trained for pop piano music generation (section 3.2) at inference time, for sequence length M = N = 3 072 while training sequences have length 2 048. (left) Absolute PE. (middle) Sinusoidal SPE. (right) Convolutional SPE. Note that SPE never requires computing these full attention patterns.</figDesc><graphic coords="2,308.38,171.64,75.06,75.06" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. (left) Generation of Q and K in SPE, which approximate the templates P d when multiplied together. (right) Q and K can be shared across layers. At each layer l, different gating is (optionally) used, before applying (16-17) to generate new queries Q and keys K.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Validation cross-entropy vs. token position on pop piano music generation task. (lower is better; the black vertical line indicates the maximum position to which the models are trained.)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>Figure4. PE evaluation metrics<ref type="bibr" target="#b59">(Wang et al., 2021)</ref> for the pop piano music generation task in the 1st layer (lower is better), w.r.t. query positions. Training sequence length is 2 048. Only querykey offsets &lt;128 are considered here. See appendix for details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 .</head><label>5</label><figDesc>Figure5. Musical style similarity for groove continuation (higher is better) between output and initial prompt through two musicallymotivated metrics, as a function of time in the output. Each data point corresponds to a single musical style.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>Experimental setup. The models (24-layer Performers with 8 attention heads) are trained on an accompaniment dataset comprising 5 522 samples in 2 761 different musical styles, encoded in a token-based format adopted from Cífka et al. (2020) and detailed in the appendix. All SPE-based models use gating in this experiment. Unlike the previous experiment, which leverages long training sequences, we consider training sequences of length N = 512, corresponding to 2-10 bars. At test time, the model is prompted with 2 bars in a style not seen during training and new tokens are sampled to complete the sequence to a length of 1 024, i.e. twice the training length.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>(</head><label></label><figDesc>start(a) mod 4, end(a)start(a)) ∈ [0, 4) × [0, 2), where start(a) and end(a) refer to the onset and offset time of a in beats. The expression start(a) mod 4 then represents the position of the note onset relative to the current bar, since all examples in the dataset are in a 4beat meter. These tuples are gathered in 24×12 histogram bins (24 for onset time and 12 for duration). • The time-pitch profile is also obtained as a 2D histogram, this time capturing time differences and pitch differences (intervals) between notes. The tuples it considers have the form (start(b)start(a), pitch(b)pitch(a)) ∈ [0, 4) × {-20, -19, . . . , 20}, a = b,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 8 Figure 7 .Figure 8 .Figure 9 .Figure 10 .Figure 11 .Figure 12 .</head><label>8789101112</label><figDesc>Figure 8 reveals a major drawback of APE: the attention of</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>The core advantage of this construction is to allow for P d to be factorized. Let us for now assume that we construct the distributions of {Q d (m), K d (n)} d in such a way that we can sample from them (we will see how in section 2.1) and consider R inde-</figDesc><table /><note><p>pendent realizations of them for given M and N , gathered in the M × R and N × R matrices Q d and K d :</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 .</head><label>1</label><figDesc>Long-Range Arena results (higher scores are better). Mean and standard deviation of accuracy over three runs is reported, except for Performer with convolutional SPE, where only a single run was completed. For comparison, the best result reported by<ref type="bibr" target="#b53">Tay et al. (2021)</ref>, along with the name of the best-performing model (in parentheses), is included.</figDesc><table><row><cell>ListOps</cell><cell>Text</cell><cell>Retrieval</cell><cell>Image</cell></row></table><note><p>3.1. Long-Range Arena</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>this task. All the datasets are described in detail in Tay et al. and available from the official LRA repository.7   </figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 .</head><label>2</label><figDesc>Long-Range Arena classification tasks used in this paper.</figDesc><table><row><cell>Name</cell><cell>Dataset</cell><cell>Input</cell><cell>Length</cell><cell cols="2">Goal # classes</cell></row><row><cell>ListOps</cell><cell>ListOps</cell><cell>expression with operations on lists of digits</cell><cell>2 k</cell><cell>evaluate expression</cell><cell>10</cell></row><row><cell>Text</cell><cell>IMDB</cell><cell>movie review as byte string</cell><cell>8 k</cell><cell>classify sentiment</cell><cell>2</cell></row><row><cell cols="2">Retrieval AAN</cell><cell>pair of articles as byte strings</cell><cell>2 × 4 k</cell><cell>detect citation link</cell><cell>2</cell></row><row><cell>Image</cell><cell cols="3">CIFAR10 8-bit gray-scale 32 × 32 image as byte string 1 k</cell><cell>recognize object</cell><cell>10</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 .</head><label>3</label><figDesc>Numbers of parameters of LRA models, identical for both Performer-softmax and Linear Transformer-ReLU. The pop piano MIDI dataset we use is derived from the one provided inHsiao et al. (</figDesc><table><row><cell></cell><cell>ListOps</cell><cell>Text Retrieval</cell><cell>Image</cell></row><row><cell cols="4">Baseline (APE) 19 982 858 3 486 722 1 087 618 248 458</cell></row><row><cell>+ sineSPE</cell><cell cols="3">20 078 090 3 518 466 1 103 490 119 242</cell></row><row><cell>+ convSPE</cell><cell cols="3">20 117 002 3 553 282 1 120 898 133 706</cell></row><row><cell>C.1. Pop Piano Music Generation</cell><cell></cell><cell></cell></row><row><cell>Training data.</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 .</head><label>4</label><figDesc>Training times for LRA models (hours). Numbers in parentheses are from Tesla P100 GPUs, the rest from Tesla V100 GPUs.</figDesc><table><row><cell>ListOps Text Retrieval Image</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 .</head><label>5</label><figDesc>Resource usage of models trained on pop piano music generation, on a Tesla V100 GPU with 32GB of memory. # of epochs and time to the checkpoint with the lowest validation loss are displayed. (ug: trained without SPE gating.)</figDesc><table><row><cell></cell><cell># epochs</cell><cell>Time</cell><cell>Memory</cell></row><row><cell>APE</cell><cell>72</cell><cell>9.74 h</cell><cell>14.34 GB</cell></row><row><cell>sineSPE</cell><cell>78</cell><cell>17.92 h</cell><cell>29.80 GB</cell></row><row><cell>sineSPE (ug)</cell><cell>78</cell><cell>16.31 h</cell><cell>18.29 GB</cell></row><row><cell>convSPE</cell><cell>80</cell><cell>28.02 h</cell><cell>30.01 GB</cell></row><row><cell>convSPE (ug)</cell><cell>68</cell><cell>24.76 h</cell><cell>18.99 GB</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 .</head><label>6</label><figDesc>Validation cross-entropy for models trained for pop piano music generation (mean and standard deviation) over all sequences.</figDesc><table><row><cell>Positions</cell><cell>Trained</cell><cell>Extrapolation</cell></row><row><cell>APE</cell><cell>1.721 ± 0.148</cell><cell>3.215 ± 0.200</cell></row><row><cell>sineSPE</cell><cell>1.694 ± 0.148</cell><cell>2.396 ± 0.359</cell></row><row><cell>sineSPE (ug)</cell><cell>1.754 ± 0.146</cell><cell>1.965 ± 0.170</cell></row><row><cell>convSPE</cell><cell>1.685 ± 0.151</cell><cell>1.932 ± 0.225</cell></row><row><cell>convSPE (ug)</cell><cell>1.733 ± 0.145</cell><cell>1.805 ± 0.163</cell></row><row><cell cols="3">best both within the trained positions and on extrapolation.</cell></row></table><note><p>(ug: trained without SPE gating). Trained: pos ≤ 2 048, Extrapolation: 2 048 &lt; pos ≤ 3 072.</p></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>A somewhat related strategy is used by the recent LambdaNetworks(Bello</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_1"><p>, 2020), which encapsulate the key-value information as a so-called lambda function to be applied query-wise, hence also avoiding the computation of a full attention matrix.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_2"><p>https://cifkao.github.io/spe/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_3"><p>If P d 0 and K ≥ N , (19) still holds but is not unique.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_4"><p>A model named 'Performer' is reported byTay et al., but communication  with the authors revealed it to be in fact equivalent to our Linear Transformer-ReLU, as it does not use random features. To avoid confusion, we refer to this model as such herein.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_5"><p>Currently, the official data loader for ListOps inadvertently strips some characters from the input sequences.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_6"><p>Examples: https://cifkao.github.io/spe/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_7"><p>https://github.com/idiap/ fast-transformers</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>This work was supported by the <rs type="funder">European Union</rs>'s <rs type="programName">Horizon 2020 research and innovation programme</rs> under the <rs type="grantName">Marie Skłodowska-Curie</rs> grant agreement No. <rs type="grantNumber">765068</rs> (MIP<rs type="grantNumber">-Frontiers</rs>) and in part by the <rs type="funder">French government</rs> under management of <rs type="funder">Agence Nationale de la Recherche</rs> as part of the "<rs type="programName">Investissements d'avenir" program</rs>, reference <rs type="grantNumber">ANR-19-P3IA-0001</rs> (<rs type="projectName">PRAIRIE 3IA Institute</rs>).</p><p>We would like to thank <rs type="person">Yi Tay</rs>, <rs type="person">Mostafa Dehghani</rs> and <rs type="person">Philip Pham</rs> for their help with troubleshooting the Long-Range Arena, and <rs type="person">Krzysztof Choromanski</rs> for clarifications about the Performer.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_hjzmjHS">
					<idno type="grant-number">765068</idno>
					<orgName type="grant-name">Marie Skłodowska-Curie</orgName>
					<orgName type="program" subtype="full">Horizon 2020 research and innovation programme</orgName>
				</org>
				<org type="funding" xml:id="_kH5rnD2">
					<idno type="grant-number">-Frontiers</idno>
				</org>
				<org type="funded-project" xml:id="_Jr5tsuq">
					<idno type="grant-number">ANR-19-P3IA-0001</idno>
					<orgName type="project" subtype="full">PRAIRIE 3IA Institute</orgName>
					<orgName type="program" subtype="full">Investissements d&apos;avenir&quot; program</orgName>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Material Introduction</head><p>This document comprises additional information that could not be included in the paper due to space constraints. It is structured as follows. In appendix A, we provide some further theoretical developments. In appendix B, we detail the experimental setup on the Long Range Arena. In appendix C, we detail our music generation experiments. Finally, we provide additional results in appendix D.</p><p>Our source code is available at:  In the main document, we claim that the convolutional variant leads to vanishing attention. We shortly prove this claim here. For ease of notation, the proof is given in the 1D case, but extends trivially to higher dimensions. The core idea is illustrated in Figure <ref type="figure">6</ref>. Convolutional SPE yields:</p><p>where Z d is a white Gaussian noise process, i.e.</p><p>Omitting the dependency on r for notational convenience (all realizations are independent), we can compute the positional attention as:    </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">AlphaFold at CASP13</title>
		<author>
			<persName><forename type="first">M</forename><surname>Alquraishi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">22</biblScope>
			<biblScope unit="page" from="4862" to="4865" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0473</idno>
		<idno>arXiv:</idno>
		<ptr target="1409.0473" />
		<title level="m">Neural machine translation by jointly learning to align and translate</title>
		<imprint>
			<date type="published" when="2016-05">May 2016</date>
		</imprint>
	</monogr>
	<note>cs, stat</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">LambdaNetworks: Modeling long-range interactions without attention</title>
		<author>
			<persName><forename type="first">I</forename><surname>Bello</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Learning Representations</title>
		<meeting>Int. Conf. Learning Representations</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Madmom: A new Python audio and music signal processing library</title>
		<author>
			<persName><forename type="first">S</forename><surname>Böck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Korzeniowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schlüter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Krebs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Widmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM International Multimedia Conf</title>
		<meeting>ACM International Multimedia Conf</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1174" to="1178" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">K</forename><surname>Bose</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Adhikary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Sau</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.09786[cs</idno>
		<idno>arXiv:</idno>
		<ptr target="1905.09786" />
		<title level="m">Positional encoding by robots with non-rigid movements</title>
		<imprint>
			<date type="published" when="2019-05">May 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">The best of both worlds: Combining recent advances in neural machine translation</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bapna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hughes</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.09849[cs</idno>
		<idno>arXiv: 1804.09849</idno>
		<ptr target="http://arxiv.org/abs/1804.09849" />
		<imprint>
			<date type="published" when="2018-04">April 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Generating long sequences with sparse Transformers</title>
		<author>
			<persName><forename type="first">R</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.10509</idno>
		<idno>arXiv: 1904.10509</idno>
		<ptr target="http://arxiv.org/abs/1904.10509" />
		<imprint>
			<date type="published" when="2019-04">April 2019</date>
		</imprint>
	</monogr>
	<note>cs, stat</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Rethinking attention with Performers</title>
		<author>
			<persName><forename type="first">K</forename><surname>Choromanski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Likhosherstov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Dohan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Sarlos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Hawkins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mohiuddin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Belanger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Colwell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Weller</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.14794</idno>
		<idno>arXiv:</idno>
		<ptr target="2009.14794" />
		<imprint>
			<date type="published" when="2020-09">September 2020</date>
		</imprint>
	</monogr>
	<note>cs, stat</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Supervised symbolic music style translation using synthetic data</title>
		<author>
			<persName><forename type="first">O</forename><surname>Cífka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename></persName>
		</author>
		<idno type="DOI">10.5281/zenodo.3527878</idno>
		<ptr target="https://doi.org/10.5281/zenodo.3527878" />
	</analytic>
	<monogr>
		<title level="m">Proc. International Society for Music Information Retrieval Conf</title>
		<meeting>International Society for Music Information Retrieval Conf</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="588" to="595" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Groove2Groove: One-shot music style transfer with supervision from synthetic data</title>
		<author>
			<persName><forename type="first">O</forename><surname>Cífka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename></persName>
		</author>
		<idno type="DOI">10.1109/TASLP.2020.3019642</idno>
		<ptr target="https://hal.archives-ouvertes.fr/hal-02923548" />
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Audio, Speech and Language Processing</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="2638" to="2650" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.02860</idno>
		<title level="m">Transformer-XL: Attentive language models beyond a fixed-length context</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Universal Transformers</title>
		<author>
			<persName><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gouws</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03819</idno>
		<idno>arXiv: 1807.03819</idno>
		<ptr target="http://arxiv.org/abs/1807.03819" />
		<imprint>
			<date type="published" when="2019-03">March 2019</date>
		</imprint>
	</monogr>
	<note>cs, stat</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Pre-training of deep bidirectional Transformers for language understanding</title>
		<author>
			<persName><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName><surname>Bert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805[cs</idno>
		<idno>arXiv:</idno>
		<ptr target="1810.04805" />
		<imprint>
			<date type="published" when="2019-05">May 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Improving multi-instrumental music generation with cross-domain pre-training</title>
		<author>
			<persName><forename type="first">C</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">H</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">E</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">W</forename><surname>Cottrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mcauley</surname></persName>
		</author>
		<author>
			<persName><surname>Lakhnes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISMIR</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Engel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Hantrakul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><surname>Ddsp</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.04643</idno>
		<title level="m">Differentiable digital signal processing</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Convolutional sequence to sequence learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Gehring</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yarats</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.03122[cs</idno>
		<idno>arXiv: 1705.03122</idno>
		<ptr target="http://arxiv.org/abs/1705.03122" />
		<imprint>
			<date type="published" when="2017-07">July 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Cross-covariance functions for multivariate geostatistics</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">G</forename><surname>Genton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Kleiber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statistical Science</title>
		<imprint>
			<biblScope unit="page" from="147" to="163" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Onsets and Frames: Dual-objective piano transcription</title>
		<author>
			<persName><forename type="first">C</forename><surname>Hawthorne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Elsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Engel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Oore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Eck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Society for Music Information Retrieval Conf</title>
		<meeting>Int. Society for Music Information Retrieval Conf</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Decoding-enhanced BERT with disentangled attention</title>
		<author>
			<persName><forename type="first">P</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><surname>Deberta</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.03654[cs</idno>
		<idno>arXiv:</idno>
		<ptr target="2006.03654" />
		<imprint>
			<date type="published" when="2020-06">June 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Local self-attention over long text for efficient document retrieval</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hofstätter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zamani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Craswell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hanbury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="2021" to="2024" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">The curious case of neural text degeneration</title>
		<author>
			<persName><forename type="first">A</forename><surname>Holtzman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Buys</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Forbes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. International Conference on Learning Representations</title>
		<meeting>International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Compound Word Transformer: Learning to compose full-song music over dynamic directed hypergraphs</title>
		<author>
			<persName><forename type="first">W.-Y</forename><surname>Hsiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-C</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AAAI Conf. Artificial Intelligence</title>
		<meeting>AAAI Conf. Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">C.-Z</forename><forename type="middle">A</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hawthorne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dinculescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">Music</forename><surname>Eck</surname></persName>
		</author>
		<author>
			<persName><surname>Transformer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.04281</idno>
		<idno>arXiv: 1809.04281</idno>
		<ptr target="http://arxiv.org/abs/1809.04281" />
		<imprint>
			<date type="published" when="2018-12">December 2018</date>
		</imprint>
	</monogr>
	<note>cs, eess, stat</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Pop Music Transformer: Generating music with rhythm and harmony</title>
		<author>
			<persName><forename type="first">Y.-S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM International Multimedia Conf</title>
		<meeting>ACM International Multimedia Conf</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Xiang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.13658</idno>
		<title level="m">Improve Transformer models with better relative position embeddings</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Language modeling with deep Transformers</title>
		<author>
			<persName><forename type="first">K</forename><surname>Irie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zeyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Schlüter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ney</surname></persName>
		</author>
		<idno type="DOI">10.21437/Interspeech.2019-2225</idno>
		<idno>arXiv:</idno>
		<ptr target="1905.04226" />
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3905" to="3909" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Transformers are RNNs: Fast autoregressive Transformers with linear attention</title>
		<author>
			<persName><forename type="first">A</forename><surname>Katharopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vyas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Pappas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Fleuret</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Machine Learning</title>
		<meeting>Int. Conf. Machine Learning</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="5156" to="5165" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Rethinking positional encoding in language pre-training</title>
		<author>
			<persName><forename type="first">G</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-Y</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.15595[cs</idno>
		<ptr target="http://arxiv.org/abs/2006.15595" />
		<imprint>
			<date type="published" when="2020-07">July 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Transformer with Gaussian-weighted self-attention for speech enhancement</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>El-Khamy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><surname>T-Gsa</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.06762</idno>
		<idno>arXiv:</idno>
		<ptr target="1910.06762" />
		<imprint/>
	</monogr>
	<note>cs, eess. February 2020</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
		<respStmt>
			<orgName>University of Toronto</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">On some properties of toeplitz matrices</title>
		<author>
			<persName><forename type="first">D</forename><surname>Kucerovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Mousavand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sarraf</surname></persName>
		</author>
		<idno type="DOI">10.1080/23311835.2016.1154705</idno>
		<ptr target="http://doi.org/10.1080/23311835.2016.1154705" />
	</analytic>
	<monogr>
		<title level="j">Cogent Mathematics</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">A lite BERT for self-supervised learning of language representations</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Soricut</surname></persName>
		</author>
		<author>
			<persName><surname>Albert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.11942[cs</idno>
		<idno>arXiv:</idno>
		<ptr target="1909.11942" />
		<imprint>
			<date type="published" when="2020-02">February 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Dhillon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-J</forename><surname>Hsieh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.09229</idno>
		<idno>arXiv:</idno>
		<ptr target="2003.09229" />
		<title level="m">Learning to encode position for Transformer with continuous dynamical model</title>
		<imprint>
			<date type="published" when="2020-03">March 2020</date>
		</imprint>
	</monogr>
	<note>cs, stat</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName><surname>Roberta</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692[cs</idno>
		<idno>arXiv:</idno>
		<ptr target="1907.11692" />
		<title level="m">A robustly optimized BERT pretraining approach</title>
		<imprint>
			<date type="published" when="2019-07">July 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Improving neural machine translation with neural syntactic distance</title>
		<author>
			<persName><forename type="first">C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tamura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Utiyama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Sumita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhao</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1205</idno>
		<ptr target="http://aclweb.org/anthology/N19-1205" />
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. North American Chapter of the Association for Computational Linguistics</title>
		<meeting>Conf. North American Chapter of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2032" to="2037" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Learning word vectors for sentiment analysis</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Maas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Daly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Potts</surname></persName>
		</author>
		<ptr target="https://www.aclweb.org/anthology/P11-1015" />
	</analytic>
	<monogr>
		<title level="m">Proc. Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="142" to="150" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Principles of geostatistics</title>
		<author>
			<persName><forename type="first">G</forename><surname>Matheron</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Economic geology</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1246" to="1266" />
			<date type="published" when="1963">1963</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Transformers with convolutional context for asr</title>
		<author>
			<persName><forename type="first">A</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Okhonko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.11660</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">On estimating regression</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">A</forename><surname>Nadaraya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Theory of Probability &amp; Its Applications</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="141" to="142" />
			<date type="published" when="1964">1964</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">ListOps: A diagnostic dataset for latent tree learning</title>
		<author>
			<persName><forename type="first">N</forename><surname>Nangia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bowman</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N18-4013</idno>
		<ptr target="https://www.aclweb.org/anthology/N18-4013" />
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. North American Chapter of the Association for Computational Linguistics: Student Research Workshop</title>
		<meeting>Conf. North American Chapter of the Association for Computational Linguistics: Student Research Workshop</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="92" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">On the relation between position information and sentence length in neural machine translation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Neishi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Yoshinaga</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/K19-1031</idno>
		<ptr target="https://www.aclweb.org/anthology/K19-1031" />
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Computational Natural Language Learning</title>
		<meeting>Conf. Computational Natural Language Learning</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="328" to="338" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Dependencybased relative positional encoding for Transformer NMT</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Omote</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tamura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ninomiya</surname></persName>
		</author>
		<idno type="DOI">10.26615/978-954-452-056-4099</idno>
		<ptr target="https://acl-bg.org/proceedings/2019/RANLP2019/pdf/RANLP099.pdf" />
	</analytic>
	<monogr>
		<title level="m">Proc. Natural Language Processing in a Deep Learning World</title>
		<meeting>Natural Language essing in a Deep Learning World</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="854" to="861" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Relative positional encoding for speech recognition and direct translation</title>
		<author>
			<persName><forename type="first">N.-Q</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-L</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-N</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-S</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Salesky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Stueker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Niehues</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Waibel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.09940</idno>
		<ptr target="http://arxiv.org/abs/2005.09940" />
		<imprint>
			<date type="published" when="2020-05">May 2020</date>
		</imprint>
	</monogr>
	<note>cs, eess</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">The ACL anthology network corpus</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Radev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Muthukrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Qazvinian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Abu-Jbara</surname></persName>
		</author>
		<idno type="DOI">10.1007/s10579-012-9211-2</idno>
		<ptr target="https://doi.org/10.1007/s10579-012-9211-2" />
	</analytic>
	<monogr>
		<title level="j">Language Resources and Evaluation</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="919" to="944" />
			<date type="published" when="2013-01">January 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.10683</idno>
		<idno>arXiv:</idno>
		<ptr target="1910.10683" />
		<title level="m">Exploring the limits of transfer learning with a unified text-to-text Transformer</title>
		<imprint>
			<date type="published" when="2020-07">July 2020</date>
		</imprint>
	</monogr>
	<note>cs, stat</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Analysis of positional encodings for neural machine translation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Rosendahl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">A K</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IWSLT</title>
		<meeting>IWSLT</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Efficient content-based sparse attention with routing Transformers</title>
		<author>
			<persName><forename type="first">A</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Saffar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Grangier</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.05997</idno>
		<idno>arXiv:</idno>
		<ptr target="2003.05997" />
		<imprint>
			<date type="published" when="2020-10">October 2020</date>
		</imprint>
	</monogr>
	<note>cs, eess, stat</note>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Shaw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.02155[cs</idno>
		<idno>arXiv:</idno>
		<ptr target="1803.02155" />
		<title level="m">Self-attention with relative position representations</title>
		<imprint>
			<date type="published" when="2018-04">April 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<author>
			<persName><forename type="first">Z</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.01243[cs</idno>
		<idno>arXiv: 1812.01243</idno>
		<ptr target="http://arxiv.org/abs/1812.01243" />
		<title level="m">Efficient attention: Attention with linear complexities</title>
		<imprint>
			<date type="published" when="2020-11">November 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Novel positional encodings to enable tree-based Transformers</title>
		<author>
			<persName><forename type="first">V</forename><surname>Shiv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Quirk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Advances in neural information processing systems</title>
		<meeting>Advances in neural information processing systems</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">End-toend memory networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.08895</idno>
		<idno>arXiv: 1503.08895</idno>
		<ptr target="http://arxiv.org/abs/1503.08895" />
		<imprint/>
	</monogr>
	<note>November 2015</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Long Range Arena: A benchmark for efficient Transformers</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Abnar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bahri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ruder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Metzler</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=qVyeW-grC2k" />
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Learning Representations</title>
		<meeting>Int. Conf. Learning Representations</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Transformer dissection: An unified understanding for Transformer&apos;s attention via the lens of kernel</title>
		<author>
			<persName><forename type="first">Y.-H</forename><forename type="middle">H</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yamada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-P</forename><surname>Morency</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Empirical Methods in Natural Language Processing and Int. Joint Conf. Natural Language Processing</title>
		<meeting>Conf. Empirical Methods in Natural Language essing and Int. Joint Conf. Natural Language essing</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4335" to="4344" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ł</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Advances in neural information processing systems</title>
		<meeting>Advances in neural information processing systems</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Foundations of signal processing</title>
		<author>
			<persName><forename type="first">M</forename><surname>Vetterli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kovačević</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">K</forename><surname>Goyal</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Simulation of simply cross correlated random fields by series expansion methods</title>
		<author>
			<persName><forename type="first">M</forename><surname>Vořechovský</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Structural safety</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="337" to="363" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<author>
			<persName><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lioma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">G</forename><surname>Simonsen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.12333[cs</idno>
		<idno>arXiv: 1912.12333</idno>
		<ptr target="http://arxiv.org/abs/1912.12333" />
		<title level="m">Encoding word order in complex embeddings</title>
		<imprint>
			<date type="published" when="2020-06">June 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">On position embeddings in BERT</title>
		<author>
			<persName><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lioma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">G</forename><surname>Simonsen</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=onxoVA9FxMw" />
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Learning Representations</title>
		<meeting>Int. Conf. Learning Representations</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Khabsa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.04768</idno>
		<idno>arXiv:</idno>
		<ptr target="2006.04768" />
		<title level="m">Linformer: Self-attention with linear complexity</title>
		<imprint>
			<date type="published" when="2020-06">June 2020</date>
		</imprint>
	</monogr>
	<note>cs, stat</note>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">What do position embeddings learn? An empirical study of pre-trained language model positional encoding</title>
		<author>
			<persName><forename type="first">Y.-A</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-N</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Empirical Methods in Natural Language Processing</title>
		<meeting>Conf. Empirical Methods in Natural Language essing</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Smooth regression analysis</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">S</forename><surname>Watson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sankhyā: The Indian Journal of Statistics, Series A</title>
		<imprint>
			<biblScope unit="page" from="359" to="372" />
			<date type="published" when="1964">1964</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">Gaussian processes for machine learning</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">K</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Rasmussen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
			<publisher>MIT press</publisher>
			<biblScope unit="volume">2</biblScope>
			<pubPlace>Cambridge, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">A learning algorithm for continually running fully recurrent neural networks</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zipser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="270" to="280" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">Memformer: The memory-augmented Transformer</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.06891[cs</idno>
		<idno>arXiv:</idno>
		<ptr target="2010.06891" />
		<imprint>
			<date type="published" when="2020-10">October 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title level="m" type="main">Lattice-Based Transformer encoder for neural machine translation</title>
		<author>
			<persName><forename type="first">F</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.01282[cs</idno>
		<idno>arXiv:</idno>
		<ptr target="1906.01282" />
		<imprint>
			<date type="published" when="2019-06">June 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.05217</idno>
		<title level="m">Positional encoding as spatial inductive bias in gans</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<title level="m" type="main">Transformer-based timewise long term relation modeling for citywide traffic flow prediction</title>
		<author>
			<persName><forename type="first">H</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">D</forename><surname>Salim</surname></persName>
		</author>
		<author>
			<persName><surname>Trailer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.05554</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Vandermonde decomposition of multilevel Toeplitz matrices with application to multidimensional super-resolution</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Stoica</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="3685" to="3701" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
		<title level="m" type="main">Improving generalization of transformer for speech recognition with parallel schedule sampling and relative positional embedding</title>
		<author>
			<persName><forename type="first">P</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.00203</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
