<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Driver Model for Take-Over-Request in Autonomous Vehicles</title>
				<funder ref="#_vuh3cuP #_8Tcwx2m">
					<orgName type="full">IDEX Academy of Université Côte d&apos;Azur</orgName>
				</funder>
				<funder ref="#_DUjay5b #_tbQF69s">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Ankica</forename><surname>Barisic</surname></persName>
							<email>ankica.barisic@univ-cotedazur.fr</email>
						</author>
						<author>
							<persName><forename type="first">Pierre</forename><surname>Sigrist</surname></persName>
							<email>pierre.sigrist@epicnpoc.com</email>
						</author>
						<author>
							<persName><forename type="first">Sylvain</forename><surname>Oliver</surname></persName>
							<email>sylvain.oliver@avisto.com</email>
						</author>
						<author>
							<persName><forename type="first">Aurélien</forename><surname>Sciarra</surname></persName>
							<email>aurelien.sciarra@avisto.com</email>
						</author>
						<author>
							<persName><forename type="first">Marco</forename><surname>Winckler</surname></persName>
							<email>marco.winckler@univ-cotedazur.fr</email>
						</author>
						<author>
							<persName><forename type="first">Ankica</forename><surname>Barišić</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Université Côte d&apos;Azur</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
								<orgName type="institution" key="instit3">INRIA</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">I3S</orgName>
								<address>
									<settlement>Sophia Antipolis</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Epicenpoc Sophia Antipolis</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<address>
									<settlement>Avisto Vallauris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<address>
									<settlement>Avisto Vallauris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="institution" key="instit1">Université Côte d&apos;Azur</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
								<orgName type="institution" key="instit3">INRIA</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff6">
								<orgName type="institution">I3S</orgName>
								<address>
									<settlement>Sophia Antipolis</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Driver Model for Take-Over-Request in Autonomous Vehicles</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">83CE478A7A8CDA56958938A0C6A34C9C</idno>
					<idno type="DOI">10.1145/3563359.3596994</idno>
					<note type="submission">Submitted on 12 Sep 2023</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-04-12T14:50+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Human-centered computing → User models</term>
					<term>User centered design</term>
					<term>• Software and its engineering → Empirical software validation</term>
					<term>• Computing methodologies → Modeling methodologies</term>
					<term>• Applied computing → Transportation User Profiling, Human Factors, Overlay User Model, Autonomous Vehicles, Control transfer, Takeover Request, Reaction Time</term>
				</keywords>
			</textClass>
			<abstract xml:lang="fr">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>établissements d'enseignement et de recherche français ou étrangers, des laboratoires publics ou privés.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>The field of autonomous vehicles (AVs) has grown rapidly with the aim of reducing human error and decreasing fatalities on the road. With the ultimate goal of achieving a Level 5 fully self-driving vehicle that can perform the Dynamic Driving Task (DDT) without any human intervention, addressing human factors remains a challenge. In Level 3 AVs, which are expected to perform the complete Dynamic Driving Task (DDT) within its Operational Design Domain (ODD), a safe transition process from the Automated Driving System (ADS) mode to manual driving is crucial <ref type="bibr" target="#b0">[1]</ref>. This transition requires AVs to issue an appropriate Take Over Request (TOR), and the driver's state plays a crucial role in this process, as a low level of attention can increase the driver's reaction time to take over control of the vehicle <ref type="bibr" target="#b9">[10]</ref>.</p><p>As automation increases, the driver's workload decreases, resulting in a lower level of attention paid to the system while it is operating <ref type="bibr" target="#b3">[4]</ref>. This can lead to boredom and states such as drowsiness or sleepiness <ref type="bibr" target="#b14">[15]</ref>. Furthermore, more effective communication methods for conveying TOR messages to the driver could reduce the overall time required to safely take over control of the vehicle <ref type="bibr" target="#b5">[6]</ref>.</p><p>To address these challenges, we profiled the driver's characteristics relevant to TOR and studied how to measure the reaction times of drivers continuously during autonomous operation, taking into account different mental states and communication methods. We illustrate our findings, based on state-of-the-art emphasis to have highly adaptive models that gather information about individual drivers, in the case of ADAVEC system. The ADAVEC system is in charge to operate the authority change following a protocol and a series of rules that take into account the driver's state and mastering of the situation when such a change is being operated.</p><p>The paper is structured as follows: Section 2 highlights the need to capture the individual driver's state for TOR, Section 3 gives an overview of existing studies to capture driver's mental state and response time in AV, Section 4 emphasizes multimodal interaction design for the warning system, Section 5 introduces the ADAVEC system environment, proposes the driver's model, and presents an illustrative scenario for monitoring driver reaction time. Finally, Section 6 gives pointers for future work and concludes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">HIGHLY ADAPTIVE DRIVER MODEL</head><p>From SAE Level 3 (Conditional Automation), the vehicle expects that the Fallback Ready User (FRU) is responsive to Autonomous Driving System (ADS) requests to intervene, called Take Over Request (TOR), as well as being able to properly detect and respond to DDT-related system failures <ref type="bibr" target="#b0">[1]</ref>. The TOR is a concept being studied in the context of the ADAVEC project, falling between SAE Level 3 and Level 4 (High automation), by executing automatically the driving function while keeping the driver informed and ready to take control at any time. Idea is to ensure a safe transition from automatic to manual driving mode while considering the AV and driver's state.</p><p>When a TOR is issued, drivers must process information from the environment and react in a reasonable time by activating the vehicle actuators to perform the dynamic driving task based on their understanding of the present situation. Before triggering a TOR, internal and external information about the driver's state and driving situation should be gathered for a safe outcome, especially in emergency situations <ref type="bibr" target="#b11">[12]</ref>.</p><p>A Fallback Ready State (FRS) can be defined as the minimum engagement/attention/awareness necessary from the driver in order to safely detect and react to TORs and system failures <ref type="bibr" target="#b0">[1]</ref>. It can be described in terms of Workload, which is defined as task demand for the human operator to accomplish mission requirements <ref type="bibr" target="#b6">[7]</ref>. Dunn et al. classified workload in AV systems in terms of Environment, Task, Equipment, and Operator <ref type="bibr" target="#b5">[6]</ref>.</p><p>The required FRS for a given situation may not be the same for two different individuals. Moreover, the reaction before a given stimulus (e.g. sound alert) may also be different. For example, in the case of workload, even if a task with the same objective workload is presented with two different users, each one may require a different level of awareness to act correctly, as the task may be more difficult for one person than another <ref type="bibr" target="#b1">[2]</ref>. Variance can also happen within the same person. If the same task is presented to a user at different points in time, the response may change depending on previous and current user states (e.g. level of distraction, emotions, increased knowledge, etc.).</p><p>Therefore, it is necessary to have highly adaptive FRU models which try to represent one particular driver and therefore allow a very high adaptivity of the AV. In contrast to stereotype-based user models, the idea is not to rely on demographic statistics but to find a specific solution for each user. Although drivers can take great benefit from this high adaptivity, this kind of model needs to gather a lot of information first <ref type="bibr" target="#b15">[16]</ref>. In general, there are two principal ways for an adaptive system to obtain user modelling information: to ask the user directly, and to derive it based on the user's activity with the system. We need to use a combination of these approaches, as the natural flow of the 'user-system' communication, to have an enabling infrastructure to study user experience for TOR.</p><p>The benefit of having this type of FRU model called the overlay model, is it's precision and flexibility. Fine-grained concept-based modelling allows systems to adjust their actions on a very detailed level. An overlay model is capable of dynamically and precisely reflecting the evolution of driver characteristics <ref type="bibr" target="#b15">[16]</ref>, which is especially important for AV. Among the drawbacks of this approach is the necessity of developing an accurate and formal domain model, which is a hard task for some domains, however being a common approach when working with AVs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">HUMAN FACTORS FOR TOR</head><p>The focus of this study is to examine the response time of drivers to TOR while accounting for the variability in different mental states that can impact a driver's situational awareness. Situational awareness refers to the driver's knowledge about navigation, environment and interaction, spatial orientation, and vehicle status. The objective is to assist the decision-making process of the system during authority transfer, and to enable the validation of the driver's reaction to multi-sensorial warnings. Training the FRU model for each individual is necessary when measuring physiological data, which can be challenging as it may pose a safety risk if the model is not accurately trained. Therefore, it is essential to identify measurements that are as objective and user-independent as possible <ref type="bibr" target="#b13">[14]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Driver mental state</head><p>A driver's mental load, stress, drowsiness, sleepiness, and fatigue level, among others, can reduce driver situational awareness, leading to longer response times or even a complete inability to respond to TOR. It is essential to track the driver's mental state nonintrusively and communicate effectively to retain their attention when needed.</p><p>Using biosignals, such as facial expressions, to determine the driver's state is a promising approach that eliminates the need for multiple sensors in the vehicle, which can be distracting <ref type="bibr" target="#b8">[9]</ref>. In-vehicle multi-modal data stream predictors through facial expressions, like Face2Multi-modal, provide a more user-friendly approach to capturing a driver's mental state. The collected data could serve as the building block for personalized Human-Machine Interaction (HMI) designs.</p><p>Recent studies have shown that subject-dependent classification and imbalance distributions play a crucial role in driver sleepiness detection in realistic conditions <ref type="bibr" target="#b13">[14]</ref>. A field-driving study resulted in a database that showed a significant decrease in subjectindependent classification, highlighting the importance of capturing the driver's mental state accurately in subject-dependant menner. Therefore, using cameras to capture the driver's mental state is a promising solution for improving AV safety.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Driver response time</head><p>Driver response time is a critical factor in TOR. When a transition of control from automated to manual driving is required, there is an interval for the driver to take control of the vehicle safely. During this interval, the driver needs to adapt from a state of low situational awareness to a higher one <ref type="bibr" target="#b4">[5]</ref>. Shared control between the vehicle and the driver should be guaranteed during this interval. One of the biggest challenges is to create a system that conveys the message in a clear, explicit way, while at the same time allowing for the possibility of continued automated control of the vehicle in the event that the driver cannot take over <ref type="bibr" target="#b10">[11]</ref>.</p><p>In <ref type="bibr" target="#b9">[10]</ref>, the authors introduce three different measures for driver response time: Take Over Time (TOT), Take Over Reaction Time (TOrt), and Lead time from a TOR to a critical event (TORlt). TOT is the time interval from when the transition of control is initiated until the driver has successfully taken control of the vehicle and resumed the driving task. TOrt is the time needed to return control of the vehicle to the human driver. TORlt determines the time it takes drivers to resume control from conditional automation in noncritical scenarios. The impact of different TORlt on drivers was studied, and the authors concluded that drivers require approximately 7 seconds to regain control of the vehicle. While some studies suggested that TOrt should be between 2 and 3.5 seconds, other studies showed that participants needed less than 1 second to take the vehicle.</p><p>It is essential to give drivers enough time to adapt to the situation, but not too long as it could confuse them due to the lack of an imminent emergency. On the other hand, some drivers might check the mirrors or adjust their seating position before taking control of the vehicle. An overview of the different reaction times measured in various scenarios can be found in <ref type="bibr" target="#b9">[10]</ref>. Response time from various alerts needs to be assessed from the time the alert began to the first indication of a response from the driver <ref type="bibr" target="#b14">[15]</ref>.</p><p>To model and predict the return time of the driver, one of the known approaches is to use a Bayesian point process, such as the log Gaussian Cox process <ref type="bibr" target="#b16">[17]</ref>. This approach enables encoding the prior domain knowledge and non-parametric estimation of latent intensity functions that capture user reaction. It allows for capturing the similarities among users in their return time by using a multi-task learning approach. Another model to predict driver performance, but related more to studying driver behavior, can be found in <ref type="bibr" target="#b2">[3]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">MULTIMODAL INTERACTION DESIGN FOR TOR WARNING SYSTEM</head><p>The way the AV behaves and communicates with the driver also requires personalization. The communication methods and amount of information necessary to keep the "human in the loop" may also depend on the preferences of each user. An individual may prefer a higher amount of information and communication in a mostly visual manner, while others may prefer a more simplified interface and voice communication <ref type="bibr" target="#b12">[13]</ref>.</p><p>The design of warning systems to capture drivers' attention in safety-critical situations has been a topic of interest for researchers. Research studies have shown that using a multisensory approach, also known as multimodal interaction design, increases the effectiveness of Human-Machine Interaction (HMI) <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b9">10]</ref>. The most common methods used in this approach are visual, sound, and haptic modalities. Redundancy, or the use of multiple modalities for presenting the same information, can significantly increase alertness and response to warning notifications.</p><p>Researchers have also investigated the effectiveness of different HMI modalities in conveying TOR-related information. In <ref type="bibr" target="#b7">[8]</ref> authors experiment with 7 multimodal signals (i.e., visual, auditory, tactile, visual-auditory, visual-tactile, auditory-tactile, and visual-auditory-tactile) while driving under SAE Level 3 automation. Findings indicate that trimodal combinations result in the shortest response time. Also, response times were longer and the perceived workload was higher when participants were engaged in a secondary task. In <ref type="bibr" target="#b5">[6]</ref> researchers found that it is important for AV drivers to be able to differentiate between a TOR and continuous feedback about the system's state. The second point is the importance of using any modality other than visual communication. They justified this by saying that in real life, as well as in the experiment, drivers are visually "over-whelmed" by other tasks. Concerning the displayed colors, drivers report that it is hard for them to notice the difference between the various shades. For them, it was either red or yellow or green. The perceived workload of the driver is least in multi-modal communication, and their performance is improved by a multi-modal approach.</p><p>Visual images can transmit a great amount of information in a single display, but TOR information can be missed by distracted drivers. Ambient alerts are easily detected by distracted drivers and found to be joyful, but it is hard to understand if they convey a particular message. Auditory and tactile alerts do not require drivers to take their eyes off the road, but acoustic alerts risk the message being unclear or not intuitive, while informative signals require a longer time to transmit urgent information. Vibrotactile alerts can enhance driver auditory or visual perception, but they transmit only a limited amount of information and can be obtrusive <ref type="bibr" target="#b9">[10]</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">ADAVEC SYSTEM</head><p>ADAVEC<ref type="foot" target="#foot_0">1</ref> is a software product designed for car manufacturers to create and validate the onboard user experience and to provide a simulation environment for autonomous vehicles while having a human in cockpit. It is meant to be a design and experiment studio for creating and testing user experiences in customizable conditions for authority transfer phases between the driver and the car. ADAVEC focuses on creating and testing drivable systems that provide personalized behaviour depending on the driver's characteristics.</p><p>The key ADAVEC components include Persona Manager, Driving Conditions Simulation, Cabin Monitoring System, Driving Monitoring System, Cabin Experience Manager, and Data Logging System. The Persona Manager allows customers to define and manage the various properties to create their own personas, based on defined characters, ADAVEC will adapt its behaviour regarding driver and cabin monitoring functions and trigger personalized answers/actions accordingly. The Driving Conditions Simulation is based on the CARLA 2 driving simulator server and the Autoware Foundation 3 autonomous driving module. It provides a large range of driving condition scenarios that can be extended, modified, and re-used at will. The Cabin Monitoring System developed by CanControls 4 is based on RGB and IR cameras. It provides a full range of information indicating the driver and passengers' activity such as gaze direction, distraction, smartphone usage, drowsiness, or sudden sickness. The Driving Monitoring System analyzes the driving scheme of the user and detects if the current way of driving is hazardous or not and triggers appropriate actions or advice. The Cabin Experience Manager, based on Epicnpoc's BOWL® Automotive starter kit, provides a large panel of possible interactions with the driver inside the cabin such as lighting, voice, graphical user interfaces, seat morphing, and haptics. Lastly, the Data Logging System provides an integrated, automated, and timestamped data logging system for each of its features, ensuring the origin, quality, and anonymization of the data system or human has created during a drive test. It has a connector to export the various data to the Elastic Suite, supporting algorithms to analyze, learn and enhance the user experience of the designed system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">FRU MODEL</head><p>To adapt the AV behavior based on the driver performance from the previous session, we need to explicitly model the driver. Personalization needs to be studied on different levels, focusing on high-level features (like preferences and background), long-term changes (change in experience level) and real-time evidence (behavior, response time).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">High-level features</head><p>High-level features that represent individual differences can be assumed to be constant over time. Birthdate is used to collect information regarding the driver's age, which together with Gender serves to fit the driver in the stereotype persona group. Additionally, driving category is used to indicate the driver's driving skills, which is usually important when the system needs to make a decision on whether the driver is capable of performing the expected manoeuvre. However, there are no studies that explicitly correlate these human factors to the TOR or to driving capabilities in general. During the initialization session, it can only be assumed that older drivers might have more trouble accepting and trusting the ADS, and drivers with higher categorization or longer driving experience might be more reliable in performing the TOR request. These are general assumptions, and concrete indicators can be established during the training/learning sessions. Moreover, Spoken language is important for setting up the language preferences for communication, while Vision and Auditory problems help the system configure appropriate settings for transmitting visual and auditory alerts. For instance, if a driver has vision problems like color blindness or difficulty seeing near or far objects, the system can adjust the visual alerts accordingly. If the driver has auditory problems, such as being deaf or having hearing difficulties, the system can adjust the auditory signals and frequency of alerts to ensure they are effective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Long-term FRU properties</head><p>Long-term changes in a driver's experience level are considered a crucial factor in characterizing the user profile in the Adavec system. To operationalize this concept, the authors introduce three experience levels: novice, intermediate, and expert.</p><p>During the novice phase, the driver is in the process of learning how to use the ADS system and is introduced to the communication methods provided by the vehicle. The driver is encouraged to test these methods during a simulation ride, and feedback is collected after each session. The user profile is initialized during this phase. Trust in the system may be lacking during this phase as the driver has little to no experience with the automation features.</p><p>In the intermediate phase, the driver has a basic knowledge of how the ADS system works, and behavioral adaptation of the user profile begins to occur. The system tries to gather driver feedback to assess their level of trust in the system, and an intermediate explanation of the system's operations is provided. Overreliance and over-trust in the automation features may develop in this phase resulting increase in safety-critical events associated with risky behaviors, such as distracted driving.</p><p>In the expert phase, the driver has hands-on experience in the transition of control authority, and sufficient data is collected to personalize the driver's response time. The driver is provided with a basic explanation of the system's actions by default. Overreliance and work underload may manifest during this phase in the form of drowsiness or inattention. By considering these experience levels, the Adavec system can better personalize the communication methods and alerts for each driver. Long-term changes are related to the driver's history, such as  <ref type="table" target="#tab_0">1</ref>, are seen to be static during the driving sessions. The driver can access these properties and change them manually. The change in experience level should be justified with sufficient data collected from the user through the feedback and the driving sessions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Real-time FRU evidence</head><p>We focused on gathering real-time evidence to assess the situational awareness of the driver during the driving session. To achieve this, we captured various dynamic properties related to the driver's mental state, behaviour, and position inside of cockpit using thirdparty tools. Properties that are relevant to decision-making for authority transfer, along with their variable values and descriptions, are presented in Table <ref type="table" target="#tab_1">2</ref>.</p><p>Properties are recorded as triple, with a timestamp at intervals of 1 second, and each value is assigned a confidence level of low, medium, or high. The properties include distraction indicating whether the driver is occupied with a non-driving-related task (NRDT), and occupation, indicating whether the driver is occupied with other driving tasks. Other properties include sleep, sickness, and drowsiness, indicating that the driver has entered a sleep state, is experiencing sudden sickness or faintness, or is feeling tired or sleepy. The Adavec system also tracks whether the driver has their feet on the pedals, their hands on the wheel, or their hands on the screen. Additionally, the system records where the driver is looking through the tracking_zone, and it detects faces with three different properties: front, rear, and total_occupation, which indicate the number of detected humans in front of the vehicle, behind the vehicle, and in total.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">ILLUSTRATIVE SCENARIO: MONITORING DRIVER REACTION TIME FOR TOR</head><p>To individualize the driving experience, the ADAVEC system saves a variety of data in its long-term memory. This encompasses biometric information such as facial and vocal recognition, as well as semantic data such as age and experience level. Additionally, the system records response times from previous driving sessions to inform future decision-making. By analyzing this data, the AV can propose customized communication methods based on the driver's previous experiences. For instance, if a particular driver's FRS results in a quicker response time for a specific type of alert, the AV may recommend using that same alert in similar situations in the future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Warrning system configurations</head><p>The current system has five different types of alerts:</p><p>• Images (Visual) -&gt; Visual images are displayed on the righthand touch screen to signal TOR, and there are various combinations of icons that can be used to draw the driver's attention, with a warning signal commonly coloured in red. • Ambient lights (Visual) -&gt; Ambient lights are situated in the cockpit, and the most common sequence includes a "green" ambient colour when everything is fine, "orange" when it is anticipated that TOR will be launched soon, and "red" when authority transfer from the driver is expected. The lighting can be continuous or blinking. • Informative (Acoustic) -&gt; For informative messages, we use text-to-speech, and like images, there are numerous combinations to transmit an alert. However, the message should be as clear and concise as possible, in a language that matches the driver's persona profile. • Beeps (Acoustic) -&gt; Beep sequences should be unique for TOR alerts, and it is preferable to use a single beep to transmit the warning quickly, followed by timely repetitions as the ODD exit point approaches. The beep should not overlap with informative acoustic warnings. • Seat Vibration (Heptic) -&gt; Seat vibrations are used in a similar manner to beeps, starting with light shaking that increases in frequency and strength as the ODD exit point approaches.</p><p>There are many combinations of alerts that we can utilize, using them sequentially or in parallel. The Bowl environment allows us to define different configurations, and designers can use it for experimentation. Moreover, each configuration has a different time duration. We define Talert as the time required to send the TOR warning to the driver. It takes some time for the driver to receive the message, particularly when we use visual images. We call this time Tinterpert.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Driver persona example</head><p>We give an illustrative example of FRU driver persona in Table <ref type="table" target="#tab_2">3</ref>, randomly generated based on the driver model presented in Section 6. This persona is of a 33-year-old female named Maria, with an intermediate experience level with ADS. She has a Class B driving license since 2010 and speaks both Spanish and English. Maria occasionally gets distracted while driving, particularly when receiving messages, but is generally focused. She experiences occasional sleepiness, especially during long drives, and drowsiness after lunch. Maria rarely gets sick. Her vision is reduced near vision, but she has no hearing or colour perception problems. Her total takeover reaction time is on average 6.6 seconds in normal situations, ranging to 8.6 when showing drowsiness and 9.2 when being sleepy. There is not enough information to estimate the reaction time for cases of sickens. She reacts fastest when multi-modal alert is transmitted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">TOR scenario</head><p>In Figure <ref type="figure" target="#fig_0">1</ref>, we illustrate the common TOR scenario for a noncritical event. The scenario begins with the vehicle in autonomous driving mode, and the driving condition simulator monitors the time to the ODD exit point. Based on preconfigured values of Tto-TOR (Time to TOR), the TOR request is sent. The cabin experience system launches the TOR alert, and the total time to transmit the message is estimated as the sum of Talert and Tinterpert. The cabin monitoring system then checks, in a given time T, if the following constraints are true at that moment: FeetOnPeddle, HandsOnWheel, EyesOnRoad, and ReadyMental. For each of these constraints, the system calculates the individual reaction time after the alert is submitted. Therefore, Treact is the reaction time of the driver and is calculated as maxT(T_FeetOnPeddle, T_HandsOnWheel, T_EyesOnRoad, T_ReadyMental). If these conditions are met, the authority is transferred to the driver, which also takes some time to have the vehicle in manual mode. We save this time as Ttransfer. We can see that in this scenario, TOT = Talert + Tinterpert + Treact + Ttransfer. We need to note that TtoTOR is always greater than TOT.</p><p>In cases where the conditions are not met at a given timestamp (ts) = T, it is necessary to determine whether there is enough time to try again and get the driver's attention to the TOR, based on the remaining time. If the system predicts that there is not enough time, it will perform an emergency manoeuvre and park the vehicle. However, if there is still time, it will repeat the previously described sequence of events. Note that in each attempt to get the driver's attention, the system may have a new configuration of alerts to use. Each individual reacts differently to the situation and the multimodal combination of the communication signals. Therefore, it is important to capture the exact response time of the driver (or that there was no response), and associate it with the communication signals used to find the most optimal combination.</p><p>The above scenario is simplistic and does not take into consideration the properties that indicate lower situation awareness, represented as the ReadyMental condition. The system continuously monitors these properties for the driver during the ride and is able to detect if any of them are true at any moment, providing the possibility to design different scenarios to react to any reduction in the driver's mental state. For instance, if the system detects that the driver has signs of sickness, it will perform an emergency manoeuvre and call for help if the driver does not recover when the vehicle is stopped. However, if the driver signals drowsiness or a sleeping state, the vehicle is aware that it might take longer for the driver to get into FRS when TOR takes place. In the case of a predicted ODD exit point, it is necessary to indicate to the driver TOR sooner. On the other hand, when unpredictable situations happen, such as sudden weather changes or traffic conditions, the vehicle needs to decide whether there is time to try to get the driver to FRS or to proceed immediately with an emergency manoeuvre.</p><p>Furthermore, in the case of the above-detected states (sickness, drowsiness or sleeping), the vehicle needs to confirm with the driver that they are really in the captured state, especially when the confidence level (c) is not 'high, ' by direct interaction over available communication channels. If the driver shows that they are not in a given state, the situation should be registered and post-analysis of the sensing data should be performed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">CONCLUSIONS AND FEATURE WORK</head><p>This research delves into the analysis of human factors that are pertinent to driver reaction time when authority transfer occurs. The study focuses on the software product ADAVEC, which is specifically designed for car manufacturers to create and validate the onboard user experience of AVs while having a human in the cockpit. Personalization of AV behaviour based on the driver's performance from the previous session requires the explicit modelling of the driver. Hence, the study investigates personalization on various levels, such as high-level features, long-term changes, and real-time evidence. The system follows different phases, namely, novice, intermediate, and expert, and initializes the driver profile during the novice phase. We emphasize the importance of accurately capturing the driver's response time and associating it with the communication signals used to determine the most optimal combination of warning alerts. This approach is expected to improve the prediction of when to use different types of warnings, leading to a more efficient and effective warning system.</p><p>As a future research direction, we aim to develop a simulation module for unrealistic personas based on existing driving data to validate the infrastructure and study the conditions and communication for different ADS levels. Furthermore, controlled experiments with real drivers are required to validate the system. Finally, the framework is planned to be extended to investigate the impact of different driving behaviour patterns for critical situations, enabling the system to make decisions in the case of timely critical manoeuvres.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: TOR scenario</figDesc><graphic coords="6,56.31,83.69,499.38,284.78" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Static properties</figDesc><table><row><cell>N</cell><cell>variable</cell><cell>value</cell></row><row><cell>1</cell><cell>birthdate</cell><cell>date</cell></row><row><cell>1</cell><cell>age</cell><cell>int</cell></row><row><cell>1</cell><cell>gander</cell><cell>m-male/f-female/u-unknown</cell></row><row><cell cols="3">1-* driving license string + date</cell></row><row><cell cols="2">1-* language</cell><cell>string</cell></row><row><cell>1</cell><cell>vision</cell><cell>o -no problem, f -far, n -near, nf -far and near</cell></row><row><cell>1</cell><cell>color</cell><cell>boolean</cell></row><row><cell>1</cell><cell>hearing</cell><cell>o -no problem, d -deaf, p -hearing problem</cell></row><row><cell>1</cell><cell>ADS level</cell><cell>n -novice, i -intermediate, e -expert</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Dynamic properties</figDesc><table><row><cell>variable</cell><cell>value</cell><cell>description</cell></row><row><cell>distraction</cell><cell>boolean</cell><cell>indicate if the driver is occu-</cell></row><row><cell></cell><cell></cell><cell>pied with non-driving task</cell></row><row><cell>occupation</cell><cell>boolean</cell><cell>indicate if the driver is occu-</cell></row><row><cell></cell><cell></cell><cell>pied with other driving task</cell></row><row><cell>sleep</cell><cell>boolean</cell><cell>indicates if driver is in sleep</cell></row><row><cell></cell><cell></cell><cell>state</cell></row><row><cell>sickness</cell><cell>boolean</cell><cell>indicates if driver is experienc-</cell></row><row><cell></cell><cell></cell><cell>ing sudden sickness or faint-</cell></row><row><cell></cell><cell></cell><cell>ness</cell></row><row><cell>drowsiness</cell><cell>boolean</cell><cell>indicates if driver is tired or</cell></row><row><cell></cell><cell></cell><cell>sleepy</cell></row><row><cell>feet_on_pedalle</cell><cell>boolean</cell><cell>Indicates if driver's feet are on</cell></row><row><cell></cell><cell></cell><cell>the pedals</cell></row><row><cell></cell><cell>hand_none</cell><cell></cell></row><row><cell>hands_on _wheel</cell><cell>hand_left hand_right hand_both</cell><cell>Indicates which hand(s) the dri-ver has on the wheel</cell></row><row><cell>hand_to_screen</cell><cell>boolean</cell><cell>indicates if driver's hand is at</cell></row><row><cell></cell><cell></cell><cell>the touch screen</cell></row><row><cell>tracking_zone</cell><cell>string</cell><cell>Indicates where the driver is</cell></row><row><cell></cell><cell></cell><cell>looking</cell></row><row><cell></cell><cell>.front: int</cell><cell>N of detected humans in front</cell></row><row><cell>faces_detection</cell><cell>.rear: int</cell><cell>N of detected humans in rear</cell></row><row><cell></cell><cell>.occupation: int</cell><cell>N of seat occupied</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Persona example</figDesc><table><row><cell>Name Maria</cell><cell cols="5">TOR Reaction time (s) average min max</cell></row><row><cell>Birthdate 01/01/1990 Age 33 Gender Female Driving License Class B (2010)</cell><cell>Feet on Pedal</cell><cell>Nominal Sickness Drowsiness Sleep</cell><cell>2.3 n/a 3.2 3.8</cell><cell>1.5 n/a 1.7 2.4</cell><cell>8 n/a 12 14.2</cell></row><row><cell>Language Spanish, English Vision Reduced near vision Color O (no problem) Hearing O (no problem)</cell><cell>Hands on Wheel</cell><cell>Nominal Sickness Drowsiness Sleep</cell><cell>1.9 n/a 3.1 3.2</cell><cell>1.2 n/a 1.6 2.1</cell><cell>7.7 n/a 11.2 12.1</cell></row><row><cell>ADS Level Intermediate Distraction Occasional (when receiving messages) Occupation Focused Sleepeness Occasional, especially during long drives</cell><cell>Eyes On Road</cell><cell>Nominal Sickness Drowsiness Sleep</cell><cell>2.1 n/a 3.3 3.4</cell><cell>0.7 n/a 1.8 2.2</cell><cell>15.7 n/a 12.7 13.7</cell></row><row><cell>Sickness Rarely gets sick</cell><cell></cell><cell>Nominal</cell><cell>2.3</cell><cell>1.7</cell><cell>14.9</cell></row><row><cell>Drowsiness Occasional, especially after lunch</cell><cell>Ready</cell><cell>Sickness</cell><cell>n/a</cell><cell>n/a</cell><cell>n/a</cell></row><row><cell>Alerts Warning Configuration</cell><cell>Mental</cell><cell>Drowsiness</cell><cell>3.4</cell><cell>2</cell><cell>13.4</cell></row><row><cell>Nominal v1-b1-al2-h1</cell><cell></cell><cell>Sleep</cell><cell>3.5</cell><cell>2.2</cell><cell>17</cell></row><row><cell>Sickness tv2-b3-al3-h2</cell><cell></cell><cell>Nominal</cell><cell>6.6</cell><cell>5.7</cell><cell>21.6</cell></row><row><cell>Drowsiness v3-b1-al2-h1 Sleep v3-b3-al1-h3</cell><cell>TOT</cell><cell>Sickness Drowsiness</cell><cell>n/a 8.6</cell><cell>n/a 6.3</cell><cell>n/a 27.5</cell></row><row><cell></cell><cell></cell><cell>Sleep</cell><cell>9.2</cell><cell>7.3</cell><cell>37.4</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>www.adavec.fr</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>www.carla.org</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>www.autoware.org</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"><p>www.cancontrols.com</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>ACKNOWLEDGMENTS</head><p>We want to thank for financial support to French national project '<rs type="programName">Adaptation automatique du Degré d'Autonomie du Véhicule à son Environnement et au Conducteur'</rs> (ADAVEC) (www.adavec.fr) with reference <rs type="grantNumber">DOS0110362/00</rs> and to <rs type="funder">IDEX Academy of Université Côte d'Azur</rs> for funding CC: <rs type="grantNumber">C460ZE08-EOTP</rs>:<rs type="projectName">BARISIC</rs><rs type="grantNumber">-DF</rs>:<rs type="grantNumber">D103</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_vuh3cuP">
					<idno type="grant-number">DOS0110362/00</idno>
					<orgName type="program" subtype="full">Adaptation automatique du Degré d&apos;Autonomie du Véhicule à son Environnement et au Conducteur&apos;</orgName>
				</org>
				<org type="funded-project" xml:id="_8Tcwx2m">
					<idno type="grant-number">C460ZE08-EOTP</idno>
					<orgName type="project" subtype="full">BARISIC</orgName>
				</org>
				<org type="funding" xml:id="_DUjay5b">
					<idno type="grant-number">-DF</idno>
				</org>
				<org type="funding" xml:id="_tbQF69s">
					<idno type="grant-number">D103</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<idno>J3016</idno>
		<ptr target="https://www.sae.org/standards/content/j3016_202104/" />
		<title level="m">Taxonomy and Definitions for Terms Related to Driving Automation Systems for On-Road Motor Vehicles -SAE International</title>
		<imprint>
			<date>_202104</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Stress , fatigue and inattention amongst city bus drivers -an explorative study on real roads within the ADAS &amp; ME project</title>
		<author>
			<persName><forename type="first">A</forename><surname>Christer Ahlström</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erik</forename><surname>Anund</surname></persName>
		</author>
		<author>
			<persName><surname>Kjellman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A Novel Driver Performance Model Based on Machine Learning</title>
		<author>
			<persName><forename type="first">Andrei</forename><surname>Aksjonov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pavel</forename><surname>Nedoma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Valery</forename><surname>Vodovozov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eduard</forename><surname>Petlenkov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Herrmann</surname></persName>
		</author>
		<idno type="DOI">10.1016/J.IFACOL.2018.07.044</idno>
		<ptr target="https://doi.org/10.1016/J.IFACOL.2018.07.044" />
	</analytic>
	<monogr>
		<title level="j">IFAC-PapersOnLine</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="267" to="272" />
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Ironies of automation</title>
		<author>
			<persName><forename type="first">Lisanne</forename><surname>Bainbridge</surname></persName>
		</author>
		<idno type="DOI">10.1016/0005-1098(83)90046-8</idno>
		<ptr target="https://doi.org/10.1016/0005-1098(83)90046-8" />
	</analytic>
	<monogr>
		<title level="j">Automatica</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="775" to="779" />
			<date type="published" when="1983">1983. 1983</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Towards an HRI Tutoring Framework for Long-term Personalization and Real-time Adaptation</title>
		<author>
			<persName><forename type="first">Giulia</forename><surname>Belgiovine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonas</forename><surname>Gonzalez-Billandon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Giulio</forename><surname>Sandini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francesco</forename><surname>Rea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alessandra</forename><surname>Sciutti</surname></persName>
		</author>
		<idno type="DOI">10.1145/3511047</idno>
		<ptr target="https://doi.org/10.1145/3511047" />
	</analytic>
	<monogr>
		<title level="m">Adjunct Proceedings of the 30th ACM Conference on User Modeling, Adaptation and Personalization</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Design guidelines for reliability communication in autonomous vehicles</title>
		<author>
			<persName><forename type="first">Sarah</forename><surname>Faltaous</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Baumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Schneegass</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lewis</forename><forename type="middle">L</forename><surname>Chuang</surname></persName>
		</author>
		<idno type="DOI">10.1145/3239060.3239072</idno>
		<ptr target="https://doi.org/10.1145/3239060.3239072" />
	</analytic>
	<monogr>
		<title level="m">Proceedings -10th International ACM Conference on Automotive User Interfaces and Interactive Vehicular Applications</title>
		<meeting>-10th International ACM Conference on Automotive User Interfaces and Interactive Vehicular Applications<address><addrLine>AutomotiveUI</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-09">2018. 2018. 9 2018</date>
			<biblScope unit="page" from="258" to="267" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Development of NASA-TLX (Task Load Index): Results of Empirical and Theoretical Research</title>
		<author>
			<persName><forename type="first">Sandra</forename><forename type="middle">G</forename><surname>Hart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lowell</forename><forename type="middle">E</forename><surname>Staveland</surname></persName>
		</author>
		<idno type="DOI">10.1016/S0166-4115(08)62386-9</idno>
		<ptr target="https://doi.org/10.1016/S0166-4115(08)62386-9" />
	</analytic>
	<monogr>
		<title level="j">Advances in Psychology</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="139" to="183" />
			<date type="published" when="1988">1988. 1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Multimodal Cue Combinations: A Possible Approach to Designing In-Vehicle Takeover Requests for Semi-autonomous Driving</title>
		<author>
			<persName><forename type="first">Gaojian</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clayton</forename><surname>Steele</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinrui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brandon</forename><forename type="middle">J</forename><surname>Pitts</surname></persName>
		</author>
		<idno type="DOI">10.1177/1071181319631053</idno>
		<ptr target="https://doi.org/10.1177/1071181319631053" />
		<imprint>
			<date type="published" when="2019-11">2019. 11 2019</date>
			<biblScope unit="page" from="1739" to="1743" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Face2multi-modal: In-vehicle multi-modal predictors via facial expressions</title>
		<author>
			<persName><forename type="first">Zhentao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rongze</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wangkai</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zilin</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangjun</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Sun</surname></persName>
		</author>
		<idno type="DOI">10.1145/3409251.3411716</idno>
		<ptr target="https://doi.org/10.1145/3409251.3411716" />
	</analytic>
	<monogr>
		<title level="m">Adjunct Proceedings -12th International ACM Conference on Automotive User Interfaces and Interactive Vehicular Applications</title>
		<meeting><address><addrLine>AutomotiveUI</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-09">2020. 2020. 9 2020</date>
			<biblScope unit="page" from="30" to="33" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Automated Driving: A Literature Review of the Take over Request in Conditional Automation</title>
		<author>
			<persName><forename type="first">Walter</forename><surname>Morales-Alvarez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oscar</forename><surname>Sipele</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Régis</forename><surname>Léberon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hadj</forename><surname>Hamma Tadjine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cristina</forename><surname>Olaverri-Monreal</surname></persName>
		</author>
		<idno type="DOI">10.3390/ELECTRONICS9122087</idno>
		<ptr target="https://doi.org/10.3390/ELECTRONICS9122087" />
	</analytic>
	<monogr>
		<title level="j">Electronics</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">2087 9</biblScope>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2020">2020. 2020. 12 2020. 2087</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Dynamic modeling of user preferences for stable recommendations</title>
		<author>
			<persName><forename type="first">Oluwafemi</forename><surname>Olaleke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Oseledets</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Evgeny</forename><surname>Frolov</surname></persName>
		</author>
		<idno type="DOI">10.1145/3450613.3456830</idno>
		<ptr target="https://doi.org/10.1145/3450613.3456830" />
	</analytic>
	<monogr>
		<title level="m">UMAP 2021 -Proceedings of the 29th ACM Conference on User Modeling, Adaptation and Personalization</title>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
			<biblScope unit="page" from="262" to="266" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Should Conditional Self-Driving Cars Consider the State of the Human Inside the Vehicle?</title>
		<author>
			<persName><forename type="first">David</forename><surname>Puertas-Ramirez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ana</forename><surname>Serrano-Mamolar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">Martin</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jesus</forename><forename type="middle">G</forename><surname>Boticario</surname></persName>
		</author>
		<idno type="DOI">10.1145/3450614.3462243</idno>
		<ptr target="https://doi.org/10.1145/3450614.3462243" />
	</analytic>
	<monogr>
		<title level="m">UMAP 2021 -Adjunct Publication of the 29th ACM Conference on User Modeling, Adaptation and Personalization</title>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
			<biblScope unit="page" from="137" to="141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Improving learning and reducing time: A constrained action-based reinforcement learning approach</title>
		<author>
			<persName><forename type="first">Shitian</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Markel</forename><surname>Sanz Ausin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Behrooz</forename><surname>Mostafavi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min</forename><surname>Chi</surname></persName>
		</author>
		<idno type="DOI">10.1145/3209219.3209232</idno>
		<ptr target="https://doi.org/10.1145/3209219.3209232" />
	</analytic>
	<monogr>
		<title level="m">UMAP 2018 -Proceedings of the 26th Conference on User Modeling, Adaptation and Personalization</title>
		<imprint>
			<date type="published" when="2018-07">2018. 7 2018</date>
			<biblScope unit="page" from="43" to="51" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Importance of subject-dependent classification and imbalanced distributions in driver sleepiness detection in realistic conditions</title>
		<author>
			<persName><forename type="first">Sofia</forename><surname>Cláudia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaime</forename><forename type="middle">S</forename><surname>Silveira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">André</forename><forename type="middle">L</forename><surname>Cardoso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christer</forename><surname>Lourenço</surname></persName>
		</author>
		<author>
			<persName><surname>Ahlström</surname></persName>
		</author>
		<idno type="DOI">10.1049/IET-ITS.2018.5284</idno>
		<ptr target="https://doi.org/10.1049/IET-ITS.2018.5284" />
	</analytic>
	<monogr>
		<title level="j">IET Intelligent Transport Systems</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="347" to="355" />
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Understanding the Impact of Technology: Do Advanced Driver Assistance and Semi-Automated Vehicle Systems Lead to Improper Driving Behavior? AAA Foundation for Traffic Safety</title>
		<author>
			<persName><forename type="first">Susan</forename><surname>Soccolich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naomi</forename><surname>Dunn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">A</forename><surname>Dingus</surname></persName>
		</author>
		<ptr target="https://aaafoundation.org/understanding-the-impact-of-technology-do-advanced-driver-assistance-and-semi-automated-vehicle-systems-lead-to-improper-driving-behavior/" />
		<imprint>
			<date type="published" when="2019-12">2019. 12 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Ontological technologies for user modelling</title>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Sosnovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Darina</forename><surname>Dicheva</surname></persName>
		</author>
		<idno type="DOI">10.1504/IJMSO.2010.032649</idno>
		<ptr target="https://doi.org/10.1504/IJMSO.2010.032649" />
	</analytic>
	<monogr>
		<title level="j">International Journal of Metadata, Semantics and Ontologies</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="32" to="71" />
			<date type="published" when="2010">2010. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A Bayesian point process model for user return time prediction in recommendation systems</title>
		<author>
			<persName><forename type="first">Sherin</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">K</forename><surname>Srijith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michal</forename><surname>Lukasik</surname></persName>
		</author>
		<idno type="DOI">10.1145/3209219.3209261</idno>
		<ptr target="https://doi.org/10.1145/3209219.3209261" />
	</analytic>
	<monogr>
		<title level="m">UMAP 2018 -Proceedings of the 26th Conference on User Modeling, Adaptation and Personalization</title>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
			<biblScope unit="page" from="363" to="364" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
