<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Improving Domain Adaptation By Source Selection</title>
				<funder>
					<orgName type="full">FUI MIVAO</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Kevin</forename><surname>Bascol</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Rémi</forename><surname>Emonet</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Elisa</forename><surname>Fromont</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Élisa</forename><surname>Fromont</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="laboratory">Laboratoire Hubert Curien UMR 5516</orgName>
								<orgName type="institution">Univ Lyon</orgName>
								<address>
									<addrLine>UJM-Saint-Etienne F-42023</addrLine>
									<settlement>Saint-Etienne</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">IRISA/INRIA rba</orgName>
								<orgName type="institution">Uni Rennes</orgName>
								<address>
									<postCode>35042</postCode>
									<settlement>Rennes cedex</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Bluecime inc</orgName>
								<address>
									<postCode>38330</postCode>
									<settlement>Montbonnot Saint-Martin</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Improving Domain Adaptation By Source Selection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">9E451F49F42F32F2DD7502E82C34CE4D</idno>
					<idno type="DOI">10.1109/ICIP.2019.8803325</idno>
					<note type="submission">Submitted on 22 May 2019</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-04-12T14:51+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Domain Adaptation</term>
					<term>Negative Transfer</term>
					<term>Deep Learning</term>
					<term>Image Classification</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>published or not. The documents may come from teaching and research institutions in France or abroad, or from public or private research centers. L'archive ouverte pluridisciplinaire</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION AND RELATED WORK</head><p>Domain adaptation <ref type="bibr" target="#b0">[1]</ref> consists in learning from a (labeled) source data distribution, a model that will be used on a different (but related and often unlabeled) target data distribution. Many real word tasks require the use of domain adaptation simply because of a lack of (target) labeled data or because of some shift between the source and the target data distribution that prevents from successfully using the learned model on the target data. When using deep learning, the most common domain adaptation algorithmic setting is to construct a common representation space for the two domains while keeping good performance on the source labeling task. This can be achieved through the use of adversarial techniques where feature representations from samples in different domains are encouraged to be indistinguishable <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>. Whatever the technique, the domain adaptation procedure is usually unsuccessful if the source domain is too different from the target one. In <ref type="bibr" target="#b3">[4]</ref> for example, the authors have empirically identified positive and negative transfer situations. We study domain adaptation for image classification with deep learning in the context of multiple available source domains and when no label are available on the target domain.</p><p>Notations We consider having D source domains. Data of the ith domain are noted Z i and are composed of examples X i and labels Y i . The target domain is given the index j.</p><p>A number of related works propose to select or weight (elements of) the source domains in order to improve the test accuracy on the target domain but none of these works explicitly evaluate and propose solutions to overcome the effect of negative transfer during the adaptation process. For example, the work from <ref type="bibr" target="#b4">[5]</ref> considers transfer learning from only one source domain and when the target task is a sub-task of the source task (as for us, no target label is available). They also extend the work of <ref type="bibr" target="#b1">[2]</ref> but they decompose the domain classifier according to each source classes. During the adaptation phase, each target example is weighted according to the class-domain classifier loss. The works from <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref> and <ref type="bibr" target="#b8">[9]</ref> tackle the problem of multi-source domain adaptation but their selection scheme makes use of a few target labels and is used to select one single source domain. The unpublished work from <ref type="bibr" target="#b9">[10]</ref> is the closest to ours. The authors propose to select multiple domains according to four possible distances (the χ 2 -divergence, the Maximum Mean Discrepancy, the Wassertein distance and the Kullback-Liebler divergence) and according to the classification performance on each single source domain. Both the distance and the performance features are weighted by a parameter β computed as:</p><formula xml:id="formula_0">β = arg min β≥0 D i=1 D k=1;k =i |ξ(Z i , Z k ) -βf (Z i , Z k )| (1)</formula><p>with f the set of considered features and ξ(Z i , Z k ) the performance of the classifier trained on Z i and tested on Z k . The authors show that on a homogeneous dataset, their method is better than randomly selecting the domains but not better than when using all of them. However, on a heterogeneous dataset, selecting the sources with their proposed distance is better than both selecting all the domains and selecting them randomly. Note that to optimize β, D classifiers should be trained which can be costly in practice (especially for deep neural networks). Besides, the authors do not provide any criterion to set the number of selected sources.</p><p>In Section 2, we show our strategy to automatically select the best sources to avoid negative transfer during do-main adaptation. Section 3 shows extensive experiments and promising results on both classical benchmarks and a new real world application related to ski-resort chairlift security. We conclude in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">DOMAIN SELECTION AND WEIGHTING</head><p>Considering a target domain j and D source domains, we propose an approach that automatically computes a weight vector p j ∈ ∆ D-1 ⊂ R D (probability simplex) and uses p j to reweight the domains (when sampling minibatches) during "domain-adversarial training <ref type="bibr" target="#b1">[2]</ref>". This training phase is usually done to fine-tune a pre-trained network. The proposed approach is modular as we decompose the computation of the domains weight vector p j in three configurable steps :</p><formula xml:id="formula_1">1. the distance vector d j = d j i D i=1</formula><p>is computed (distance of each source domain, i, to the target one, j), 2. it is mapped to a score vector s j = score(d j ), 3. it is normalized to a probability vector p j = s j / i s j i .</p><p>Computing pairwise dataset distances We focus here on a distance based on optimal-transport but other distances could be considered. For instance, could use the average minimal Euclidean distance (d j i = average x∈Xi min y∈Xj x -y ) or a distance based on auto-encoder where an autoencoder AE j is trained with the points of domain j and d j i = average x∈Xi x -AE j (x)</p><p>2 (average squared reconstruction error).</p><p>The optimal transport problem aims at finding the minimal cost for transforming a data distribution into another one <ref type="bibr" target="#b10">[11]</ref>. This minimal cost is defined as a sum of the (probability) mass to displace multiplied by the given displacement price (for instance the euclidean distance). The minimal cost is called the Wasserstein distance and constitutes a distance between distribution. In our discrete case (samples of points), the distance can be expressed as:</p><formula xml:id="formula_2">d j i = x∈Xj y∈Xi γ * x,y C x,y<label>(2)</label></formula><p>where C is a distance matrix between all pair of elements of the two domains. The optimal transport plan γ * is obtain by solving the optimal transport problem:</p><formula xml:id="formula_3">γ * = arg min γ∈Π(μj ,μi) γ, C F (<label>3</label></formula><formula xml:id="formula_4">)</formula><p>where F is the Frobenius inner product and the constraint set</p><formula xml:id="formula_5">Π(μ j , μi ) = γ ∈ R |Xj |×|Xi| + | γ1 = μS , γ T 1 = μT</formula><p>ensures that the transport plan γ does not create or remove mass (by ensuring that the marginal distributions μj and μi are preserved).</p><p>Two set of points, even if drawn from the same distribution, will exhibit a variable non-zero Wasserstein distance. To compensate for this sampling-induced bias and variance, we normalize the Wasserstein distance d j i by subtracting the mean and dividing by the variance, of d j j , obtained by sampling subsets of the source domain and computing their Wasserstein distance.</p><p>Transforming distances into scores Possible score functions include the inverse distances ( 1 d ) or the inverse squared distances ( 1 d 2 ). Here, we focus on the negative exponential scoring function:</p><formula xml:id="formula_6">s j i = e -λd j i<label>(4)</label></formula><p>The parameter λ allows a smooth interpolation between putting all the weight on the closest domain and having a uniform distribution of all domains. Thanks to this parameter, we will be able to control the variety of the subset of domains we are considering (see below).</p><p>Ensuring training set variety In case of many source domains and when some of them have a very small number of training examples, it becomes important to avoid selecting too few domains in the process (e.g., a single one). For generalization (i.e. avoiding overfitting) and transfer purpose, the training set should exhibit enough variety. For domain sizes n ∈ N D , a probability vector p j and a draw of N training samples (with replacement), we define the training set variety as the expected number of distinct samples we will use for training. The variety can be approximated as:</p><formula xml:id="formula_7">variety(p j , n, N ) ≈ i n i •   1 -1 - p j i n i N  <label>(5)</label></formula><p>Our probability vector p j depends on the λ parameter. As such, by varying λ from ∞ to 0, we can move from a minimal variety (sampling from the closest domain i, getting a diversity of approximately n i , the number of examples in this closest domain) to a maximal one (using a uniform p j , getting a variety of N -N N -1 N N ≈ 0.63N in case of a balanced n). In the experiments, we consider one "epoch" (with replacement) of N = i n i samples. We use p j and n to find the highest value of λ for which the variety is below a target variety.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">EXPERIMENTS</head><p>Our backbone classifier is a residual network (ResNet50) <ref type="bibr" target="#b11">[12]</ref> pre-trained on the ImageNet dataset <ref type="bibr" target="#b12">[13]</ref>. We report the average test-accuracy on the target domain using a 5-fold cross validation procedure. We use the following names to report the model performance: -Target (only): models trained on the labeled target dataset. Note that this is an ideal but unrealistic situation since, in our actual applications, there is no target label. This setting requires the target domain to be split into training/test sets.</p><p>-Only near.: models trained using only the nearest domain (according to our distance measure) to the target one; -Only far.: models trained using only the farthest domain (according to our distance measure) to the target one; -LODO: models trained using all domains but the target one, using domain adaptation on the remaining target domain, without using our domain selection method; -w/o near.: models trained using all domains but two: the target and the closest domain to the target one (according to our distance measure) are not used.</p><p>w/o far.: models trained using all domains but two: the target and the farthest domain to the target one (according to our distance measure) are not used.</p><p>-OURS: models trained by weighting the source domains with our method, using the variety criterion (with a target variety of half its maximum value).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Datasets</head><p>Office-Caltech (O-C) <ref type="bibr" target="#b13">[14]</ref> is a classical domain adaptation benchmark with four domains: Amazon (A), DSLR (D), Webcam (W) and Caltech (C). It is composed of the 10 classes (Backpack, Bike, Calculator, Headphones, Keyboard, Laptop, Monitor, Mouse, Mug, and Video-projector) common between Office-31 <ref type="bibr" target="#b14">[15]</ref> and Caltech256 <ref type="bibr" target="#b15">[16]</ref>.</p><p>ImageNet-Caltech (I-C). To control the discrepancy between each domain and validate our chosen domain similarity measure, we have designed a dataset using images from Caltech101 <ref type="bibr" target="#b16">[17]</ref> (C1), Caltech256 <ref type="bibr" target="#b15">[16]</ref> (C2) and from Ima-geNet <ref type="bibr" target="#b12">[13]</ref> (IN) with mixed labels (bird, car, chair, dog, and person, following, among others, <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19]</ref>) described in Table 1. This dataset is composed of three different types of domain. The "Good" (G ) domains are created with the true original classes, the "Bad" (B ) domains are created with different but similar original classes, and the "Random" (R ) domains are created with randomly chosen classes in the corresponding datasets. With this design, the "Good" domains are expected to be closer to each other since the original labels are the same. The "Bad" domains should be farther away from the "Good" ones and the "Random" datasets should be the farthest (and are expected to be far from each others too).</p><p>Bluecime. In <ref type="bibr" target="#b19">[20]</ref>, we introduced an image dataset for the classification of risky situations on chairlifts. The task is to detect if a chairlift vehicle is empty, with passengers in a safe situation, or with passengers in an unsafe situation (security railing not put down, children alone, ...). The images come from 21 different chairlifts: we consider that each chairlift represents a different domain. This dataset is currently not publicly available, so the actual sky resort names are replaced by letters in the corresponding performance table.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Results</head><p>In Figure <ref type="figure" target="#fig_2">1</ref>, we show the probabilities we obtain using the selection process described in Section 2 on the 3 datasets. Table <ref type="table">1</ref>: Classes used in each dataset to create the different domains (G = "good"; B = "bad"; R = "random", C1: from Caltech101; C2: from Caltech256; IN: from ImageNet). In parentheses: number of classes composing the superclass that has been used (e.g. 56 classes of birds).     In most datasets, only up to three domains have a selection probably greater than 0.1 and more than half the source domains are totally unused. For instance, during training with "Bad" ImageNet as the target domain, 67.2% of the training images come from "Bad" Caltech256, 21.6% from "Good" ImageNet, 6.7% from "Good" Caltech101, and only 4.6% from the three other source domains.    <ref type="bibr" target="#b19">[20]</ref>) and on all 21 domains.</p><formula xml:id="formula_8">G _C 1 G _I N B _C 1 B _C 2 B _I N R _C 1 R _I N G_C1 G_IN B_C1 B_C2 B_IN R_C1 R_IN</formula><p>In Table <ref type="table" target="#tab_4">2</ref>, we show the results on the Office-Caltech (O-C, first five columns) and ImageNet-Caltech (I-C, last nine columns) datasets. On both datasets, using only the nearest source domains is beneficial compared to using the farthest one (+5.49 accuracy points on O-C and +44.21 pts on I-C) which suggests that our distance is meaningful. On Office-Caltech, using the target domain as source (Target line) gives worst performance than using the nearest source (-2.23 pts), which can be explained by the lack of training data which induces overfitting phenomena. Since ImageNet-Caltech contains more data, the Target setting is much more suited, as expected, than the Only nearest one (+36.78 pts).</p><p>Using the LODO setting (all source domains are used during training), we observe better performance than with the Only nearest and Only farthest ones thanks to a more diverse training set (respectively, on O-C, +0.46 pts and +5.95 pts, on I-C, +6.73 pts and +50.94 pts). This means that there is a real trade-off between the domain selection and the number of remaining training data. However, if we remove the nearest source domain, the performance becomes worst than for the LODO setting (-5.12 pts on O-C and -6.41pts on I-C), on Office-Caltech we even get worst performance than using the Only nearest setting (-4.66pts). If we remove the farthest source domain, we do obtain better performance than with the LODO setting (+0.15 pts on O-C and +1.95 pts on I-C). We can conclude that using many data (LODO setting) is important and always better than choosing a single domain (even the most similar one) but selecting a good number of sources can be beneficial.</p><p>Our distance-based weighting approach provides better performance than removing the farthest source domain on Office-Caltech (+0.19pts). On ImageNet-Caltech, we get worst results than when removing the farthest domain (-0.31pts), but, still notably better than with the LODO setting (+1.64 pts). However, if we ignore the results on the "Random" domains (which are close to random by design), on average, the LODO setting gives 91.67% of accuracy, w/o farthest 92.58%, and our approach allows us to get the best accuracy performance of 93.14% which confirms the relevance of our approach.</p><p>In Table <ref type="table" target="#tab_5">3</ref>, we show the results on the Bluecime dataset. Due to the page limit, we only detail the results on 4 domains (the same ones as in <ref type="bibr" target="#b19">[20]</ref>) and give the averaged results over the 21 domains. As with the other datasets, we obtain better results by selecting the source domains (+1.12 pts). This shows that the proposed method works well even when there are much more source domains to select from.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">CONCLUSION</head><p>We have shown that unsupervised domain adaptation can be improved by selecting and weighting a good subset of the sources that are the most similar to the target domain. Our approach weights the sources according to the Wasserstein distance between unlabeled domain distributions and according to the variety of the data in the selected sources. Extensive experiments showed the relevance of our proposed weighting scheme. Future work involves exploring and reporting the behavior of our approach with different settings (e.g. combination of distances, scoring functions, variety criterion), that were left out due to the page limit.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 1 :</head><label>1</label><figDesc>Fig. 1: Probabilities used to reweight the different domains: ImageNet-Caltech (top-left), Office-Caltech (bottomleft), Bluecime (right).</figDesc><graphic coords="4,425.16,460.15,125.79,124.67" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>Accuracy averaged over 5 experiments on the Office-Caltech datasets (first 5 columns) and the datasets created from ImageNet and Caltech (last 9 columns). We use a ResNet50 pretrained on ImageNet, and train it for 50 epochs (batch-size 64, learning rate 10 -5 ). The last column is grayed-out as it gives the average including the "Random" domains.</figDesc><table><row><cell>Setting</cell><cell>A</cell><cell>B</cell><cell>C</cell><cell>D</cell><cell>All 21</cell></row><row><cell cols="6">LODO 95.70 98.26 98.28 98.69 95.94</cell></row><row><cell cols="6">OURS 95.63 98.38 98.07 98.94 97.06</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Accuracy on the Bluecime dataset, averaged on 5 experiments, on a selection of 4 domains (the same A/B/C/D as in</figDesc><table /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>This work is supported by the <rs type="funder">FUI MIVAO</rs> project.</p></div>
			</div>
			<listOrg type="funding">
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A theory of learning from different domains</title>
		<author>
			<persName><forename type="first">Shai</forename><surname>Ben-David</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Koby</forename><surname>Crammer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Kulesza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jennifer</forename><forename type="middle">Wortman</forename><surname>Vaughan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">79</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="151" to="175" />
			<date type="published" when="2010-05">May 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Domainadversarial training of neural networks</title>
		<author>
			<persName><forename type="first">Yaroslav</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Evgeniya</forename><surname>Ustinova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hana</forename><surname>Ajakan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pascal</forename><surname>Germain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Franois</forename><surname>Laviolette</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mario</forename><surname>Marchand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Adversarial discriminative domain adaptation</title>
		<author>
			<persName><forename type="first">Eric</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>CVPR</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2962" to="2971" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">To transfer or not to transfer</title>
		<author>
			<persName><forename type="first">Zvika</forename><surname>Michael T Rosenstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leslie</forename><surname>Marx</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">G</forename><surname>Pack Kaelbling</surname></persName>
		</author>
		<author>
			<persName><surname>Dietterich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS 2005 workshop on transfer learning</title>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="volume">898</biblScope>
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Partial transfer learning with selective adversarial networks</title>
		<author>
			<persName><forename type="first">Zhangjie</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2724" to="2732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Multisource domain adaptation and its application to early detection of fatigue</title>
		<author>
			<persName><forename type="first">Rita</forename><surname>Chattopadhyay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qian</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Davidson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sethuraman</forename><surname>Panchanathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jieping</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Knowledge Discovery from Data (TKDD)</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">18</biblScope>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Exploiting web images for event recognition in consumer videos: A multiple source domain adaptation approach</title>
		<author>
			<persName><forename type="first">Lixin</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shih-Fu</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012">2012. 2012</date>
			<biblScope unit="page" from="1338" to="1345" />
		</imprint>
	</monogr>
	<note>IEEE Conference on</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">On handling negative transfer and imbalanced distributions in multiple source transfer learning</title>
		<author>
			<persName><forename type="first">Jing</forename><surname>Liang Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hung</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kang</forename><surname>Ngo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statistical Analysis and Data Mining: The ASA Data Science Journal</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="254" to="271" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">On automated source selection for transfer learning in convolutional neural networks</title>
		<author>
			<persName><forename type="first">Muhammad</forename><surname>Jamal Afridi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arun</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erik</forename><forename type="middle">M</forename><surname>Shapiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">73</biblScope>
			<biblScope unit="page" from="65" to="75" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Distance based source domain selection for sentiment classification</title>
		<author>
			<persName><forename type="first">Lex Razoux</forename><surname>Schultz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Loog</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peyman</forename><surname>Mohajerin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Esfahani</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1808.09271</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">Cédric</forename><surname>Villani</surname></persName>
		</author>
		<title level="m">Optimal transport: old and new</title>
		<imprint>
			<publisher>Springer Science &amp; Business Media</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="volume">338</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2009">2009. 2009</date>
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
	<note>CVPR 2009. IEEE Conference on</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Geodesic flow kernel for unsupervised domain adaptation</title>
		<author>
			<persName><forename type="first">Boqing</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Sha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristen</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012">2012. 2012</date>
			<biblScope unit="page" from="2066" to="2073" />
		</imprint>
	</monogr>
	<note>IEEE Conference on</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Adapting visual category models to new domains</title>
		<author>
			<persName><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Kulis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mario</forename><surname>Fritz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="213" to="226" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Caltech-256 object category dataset</title>
		<author>
			<persName><forename type="first">Gregory</forename><surname>Griffin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Holub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning generative visual models from few training examples: An incremental bayesian approach tested on 101 object categories</title>
		<author>
			<persName><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer vision and Image understanding</title>
		<imprint>
			<biblScope unit="volume">106</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="59" to="70" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Undoing the damage of dataset bias</title>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tinghui</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomasz</forename><surname>Malisiewicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="158" to="171" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Unbiased metric learning: On the utilization of multiple datasets and web images for softening bias</title>
		<author>
			<persName><forename type="first">Chen</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ye</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><surname>Daniel N Rockmore</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1657" to="1664" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Improving chairlift security with deep learning</title>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Bascol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rémi</forename><surname>Emonet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elisa</forename><surname>Fromont</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raluca</forename><surname>Debusschere</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Intelligent Data Analysis</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1" to="13" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
