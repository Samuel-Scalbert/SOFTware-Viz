<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Schema First! Learn Versatile Knowledge Graph Embeddings by Capturing Semantics with MASCHInE</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2023-10-19">19 Oct 2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Davy</forename><surname>Monticolo</surname></persName>
							<email>davy.monticolo@univ-lorraine.fr</email>
							<affiliation key="aff0">
								<orgName type="institution">of</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Schema First! Learn Versatile Knowledge Graph Embeddings by Capturing Semantics with MASCHInE</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2023-10-19">19 Oct 2023</date>
						</imprint>
					</monogr>
					<idno type="MD5">08CCE691AEEBBC057A440F2AA5263457</idno>
					<idno type="arXiv">arXiv:2306.03659v2[cs.AI]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-04-12T14:51+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>CCS CONCEPTS</term>
					<term>Computing methodologies ‚Üí Artificial intelligence</term>
					<term>Knowledge representation and reasoning</term>
					<term>‚Ä¢ Information systems ‚Üí World Wide Web Knowledge Graph Embeddings, Schema-based Representation Learning, Link Prediction, Entity Clustering, Node Classification 1 Resource Description Framework. See</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Knowledge graph embedding models (KGEMs) have gained considerable traction in recent years. These models learn a vector representation of knowledge graph entities and relations, a.k.a. knowledge graph embeddings (KGEs). Learning versatile KGEs is desirable as it makes them useful for a broad range of tasks. However, KGEMs are usually trained for a specific task, which makes their embeddings task-dependent. In parallel, the widespread assumption that KGEMs actually create a semantic representation of the underlying entities and relations (e.g., project similar entities closer than dissimilar ones) has been challenged. In this work, we design heuristics for generating protographs -small, modified versions of a KG that leverage RDF/S information. The learnt protograph-based embeddings are meant to encapsulate the semantics of a KG, and can be leveraged in learning KGEs that, in turn, also better capture semantics. Extensive experiments on various evaluation benchmarks demonstrate the soundness of this approach, which we call Modular and Agnostic SCHema-based Integration of protograph Embeddings (MASCHInE). In particular, MASCHInE helps produce more versatile KGEs that yield substantially better performance for entity clustering and node classification tasks. For link prediction, using MASCHinE substantially increases the number of semantically valid predictions with equivalent rank-based performance.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Knowledge Graphs (KGs) such as DBpedia and YAGO represent facts as triples (‚Ñé, ùëü, ùë°) formed by a head ‚Ñé and a tail ùë° linked by a semantic relation ùëü which qualifies the nature of their relationship. Typical learning problems with KGs are Link Prediction (LP), Entity Clustering (EC), and Node Classification (NC).</p><p>In most cases, these problems are addressed using Knowledge graph embedding models (KGEMs), which generate vector representations for entities and relations of a KG, a.k.a. Knowledge Graph Embeddings (KGEs). While generating dense and numerical vectors for entities and relations, KGEMs are expected to produce KGEs that retain the underlying semantics of the KG <ref type="bibr" target="#b20">[21]</ref>. This widespread assumption that KGEMs create a semantic representation of the underlying entities and relations (i.e., project similar entities closer than dissimilar ones) has been challenged recently <ref type="bibr" target="#b13">[14]</ref>. Their semantic capabilities presumably inherited from the rich information contained in the KG is neither fully satisfying nor consistent for tasks such as concept clustering <ref type="bibr" target="#b13">[14]</ref>. In this work, we claim that this is because schemas (e.g. RDF/S and OWL) are often overlooked as an interesting source of information for improving embeddings.</p><p>Injecting schema-based information into the training phase could help enhancing the semantic awareness of KGEMs <ref type="bibr" target="#b12">[13]</ref>. This way, better performance can be reached using available ontological information, without introducing major overhead in models' complexity. A few works incorporate such schemas when training KGEM, whether it is in the loss function <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b12">13]</ref>, in the negative sampling procedure <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16]</ref>, or in the model representations <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b22">23]</ref>. All of those methods are usually bound to just one KGEM and therefore not universally applicable.</p><p>Another line of research that is less explored focuses on preprocessing graphs prior to encoding their components <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b17">18]</ref>, rather than focusing on the representation capability of the KGEM itself. This idea of summarizing <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b8">9]</ref> or coarsening <ref type="bibr" target="#b17">[18]</ref> RDF 1 graphs proves useful in many circumstances. The intended goals and expected benefits are multiple: scaling up training <ref type="bibr" target="#b9">[10]</ref>, initializing embeddings with more robust vector representations learnt on the coarsened graph <ref type="bibr" target="#b17">[18]</ref>, and improving predictive performance in KG-based downstream tasks <ref type="bibr" target="#b21">[22]</ref>. It is worth a reminder that several heuristics exist for coarsening or summarizing graphs <ref type="bibr" target="#b3">[4]</ref>. All these approaches, however, use statistics over the graphs, instead of considering schema information. In contrast, our work focuses on using RDF/S information to generate an abstraction of the KG that encapsulates general knowledge about how entities and relations interact based on entity types, and relation domains and ranges. We call such an abstraction a protograph (see left part of Figure <ref type="figure" target="#fig_1">2</ref>).</p><p>If considering schema-based information is usually done with the goal of improving results w.r.t. a given task such as LP <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b12">13]</ref>, it would be worth investigating whether the learnt embeddings actually have stronger semantic capabilities <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14]</ref>. If so, they are expected to provide better performance with regards to other tasks such as entity classification and clustering. In this work, we propose an approach named MASCHInE that leverages RDF/S information in a model-agnostic way, making it applicable to arbitrary KGEMs. Specifically, we create and embed protographs from a KG schema. The protograph embeddings are then used to initialize the actual KGEs, and training starts on these pre-trained KGEs. Finally, we assess the versatility of these KGEs by comparing results with a vanilla, traditional learning approach.</p><p>The main contributions of our work are summarized as follows:</p><p>‚Ä¢ We devise two heuristics for building protographs that aim at encapsulating concise and meaningful information derived from the schema that underpins a KG. ‚Ä¢ To the best of our knowledge, we propose the first work that studies the potential of pre-training embeddings with a protograph-assisted approach based on a schema, and subsequently use these embeddings for multiple tasks, e.g. link prediction, entity clustering and node classification. ‚Ä¢ Through extensive experiments on several benchmarks, we demonstrate the value of using schema-based protographs as a means to generate versatile, multi-purpose embeddings.</p><p>The remainder of the paper is structured as follows. Related work is presented in Section 2. In Section 3, we detail our approach for building protographs and how they fit into our approach MAS-CHInE. In Section 4, the potential usefulness of embeddings learnt with MASCHInE is discussed, and experiments w.r.t. several benchmarks are carried out. Lastly, Section 5 sums up the main findings and outlines promising future research directions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Knowledge graph embeddings. KGEMs have gained significant attention in recent years due to their ability to represent structured knowledge in a continuous vector space. The seminal translational model TransE <ref type="bibr" target="#b2">[3]</ref> represents entities and relations as lowdimensional vectors and defines the relationship between a head, a relation, and a tail using a translation operation in the embedding space. Most of the KGEMs subsequently proposed aim at addressing its limitations and improve the representation expressiveness of KGEs <ref type="bibr" target="#b20">[21]</ref>, such as DistMult <ref type="bibr" target="#b23">[24]</ref>, ComplEx <ref type="bibr" target="#b19">[20]</ref>, ConvE <ref type="bibr" target="#b7">[8]</ref>, and TuckER <ref type="bibr" target="#b0">[1]</ref>. Embeddings learnt by these KGEMs demonstrated potential applicability in tasks such as LP <ref type="bibr" target="#b20">[21]</ref>.</p><p>Schema-based representation learning. Many knowledge graphs are backed by a more or less expressive schema defining which classes of entities exist and by which relations they are allowed to interact, among others. Recent approaches in KG-based learning leverage the schema as a supplementary source of information to learn higher-quality embeddings for the task at hand. A common approach is to alter the negative sampling procedure in order to produce more realistic negative triples. For instance, type-constrained negative sampling (TCNS) <ref type="bibr" target="#b15">[16]</ref> replaces the head or the tail with a random entity belonging to the same type as the ground-truth entity. The model itself can be adapted to take schema-based information into account. In TaRP <ref type="bibr" target="#b5">[6]</ref>, type and entity-level information are simultaneously considered and encoded as prior probabilities and likelihoods of relations, respectively. Altering the loss function with schema-based information is another line of research. In <ref type="bibr" target="#b6">[7]</ref>, background ontological information is injected in the pairwise hinge loss function as constraints. Hubert et al. <ref type="bibr" target="#b12">[13]</ref> leverage domains and ranges of relations, and propose semantic-driven loss functions that extend the most frequently used loss functions in LP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">PROPOSED APPROACH</head><p>In this section, we detail the two heuristics we propose for building protographs using schema-based information. Next, we provide some perspective on the overall proposed approach MASCHInE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Protographs</head><p>Most KGs are underpinned by a schema, e.g., DBpedia, Wikidata, and YAGO. Generating a protograph using schema-based information to improve KGEM performance with respect to a given task is an under-explored avenue.</p><p>The protographs we build leverage RDF/S information and contain an entity ùëù ùê∂ for each class ùê∂, and the same set of relations as the KG. The first design principle consists in adding an edge for each relation with domain and range restrictions, i.e., given the two axioms ùëëùëúùëöùëéùëñùëõ(ùëü, ùê∂ ùëñ ), ùëüùëéùëõùëîùëí (ùëü, ùê∂ ùëó ), we add a triple (ùëù ùê∂ ùëñ , ùëü, ùëù ùê∂ ùëó ) to the protograph<ref type="foot" target="#foot_0">2</ref> . We refer to this strategy as P1.</p><p>The protograph P1 contains as many triples as there are relations with both a domain and range<ref type="foot" target="#foot_1">3</ref> in the KG. This may lead to relatively small-sized protographs from which there is little data for the KGEM to learn from. In addition, relation domains and ranges can refer to generic classes. If entities are typed in a fine-grained way -i.e. with more specific classes -such classes will be absent from the protograph. As such, we propose a second design principle. Assuming we observe the following axioms ùëëùëúùëöùëéùëñùëõ(ùëü, ùê∂ ùëñ ), ùëüùëéùëõùëîùëí (ùëü, ùê∂ ùëó ), ùë†ùë¢ùëèùëêùëôùëéùë†ùë†ùëÇ ùëì (ùê∂ ‚Ä≤ ùëñ , ùê∂ ùëñ ), ùë†ùë¢ùëèùëêùëôùëéùë†ùë†ùëÇ ùëì (ùê∂ ‚Ä≤ ùëó , ùê∂ ùëó ), we add the following triples to the protograph: (ùëù ùê∂ ùëñ , ùëü, ùëù ùê∂ ùëó ), (ùëù ùê∂ ‚Ä≤ ùëñ , ùëü, ùëù ùê∂ ùëó ), (ùëù ùê∂ ùëñ , ùëü, ùëù ùê∂ ‚Ä≤ ùëó ). In other words: for each subclass of the class appearing in the domain or range of a relation, an additional triple is created.</p><p>We refer to this protograph as P2. Figure <ref type="figure" target="#fig_0">1</ref> illustrates this process for a triple extracted from DBpedia.</p><p>P2 usually contains more triples than P1, at the possible expense of containing dubious ones. To illustrate, the DBpedia ontology contains the triples: ùëëùëúùëöùëéùëñùëõ(ùëêùë¢ùëüùëüùëíùëõùë°ùëä ùëúùëüùëôùëëùê∂‚Ñéùëéùëöùëùùëñùëúùëõ, ùëÜùëùùëúùëüùë°), ùëüùëéùëõùëîùëí (ùëêùë¢ùëüùëüùëíùëõùë°ùëä ùëúùëüùëôùëëùê∂‚Ñéùëéùëöùëùùëñùëúùëõ, ùê¥ùëîùëíùëõùë°), and ùë†ùë¢ùëèùëêùëôùëéùë†ùë†ùëÇ ùëì (ùêπùëéùëöùëñùëôùë¶, ùê¥ùëîùëíùëõùë°). They lead to the triple (ùëÉ ùëÜùëùùëúùëüùë° , ùëêùë¢ùëüùëüùëíùëõùë°ùëä ùëúùëüùëôùëëùê∂‚Ñéùëéùëöùëùùëñùëúùëõ, ùëÉ ùêπùëéùëöùëñùëô ùë¶ ) for which the relevancy and correctness are questionable. Conceptually, with P2, we accept some noise to be introduced in the protograph for the benefit of more triples being generated. However, the generation of dubious triples is limited by two factors: firstly, when adding triples that derive from an original triple (i.e. containing the domain and range of the relation), we only consider direct subclasses -in contrast to transitive subclasses. Secondly, we fix one side of the triple and generate as many new triples as there are direct subclasses of the class of the other side, i.e., we avoid generating triples (ùëù ùê∂ ‚Ä≤ ùëñ , ùëü, ùëù ùê∂ ‚Ä≤ ùëó ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">MASCHInE</head><p>The proposed MASCHInE approach allows for generating pretrained embeddings based on a protograph, to be further fine-tuned on the initial KG. MASCHInE is intended to produce more versatile KGEs that can be successfully used in various tasks. Conceptually, MASCHInE is made of four different modules that are depicted in Figure <ref type="figure" target="#fig_1">2</ref> and further detailed below.</p><p>(a) Protograph building is the preliminary step that generates a second source of data to learn from. In particular, schema-based information is leveraged to create a protograph that encapsulates more general knowledge about how entities and relations interact based on ontological constraints such as entity types, relation domains and ranges, etc. A mapping dictionary is created to pair entities between the KG and the protograph (the set of relations in the KG and the protograph are identical). In this work, each KG entity maps to its (potentially multiple) most specific classes -represented as entities in the protograph. In Section 3.1, we presented two heuristics for building protographs with such constraints. Other possibilities exist and we leave them for future work.</p><p>(b) Protograph KGE learning is the task of learning KGEs on the protograph itself. This can be achieved regardless of the KGEM at hand and is therefore model-agnostic <ref type="foot" target="#foot_2">4</ref> .</p><p>(c) Instance KGE initialization consists in transferring protograph embeddings to the corresponding entity and relation embeddings of the KG. To do so, we assign each entity in the KG the embedding vector of its corresponding protograph entity using the dictionary created in step (a) (i.e., its most specific class). When an entity maps to multiple protograph entities (i.e. when there exist mutiple most specific classes for a given entity), the protograph embeddings of its most specific classes are averaged and the resulting embedding is transferred to the entity.</p><p>(d) Fine-tuning is performed on the entity KGEs. At this stage, embeddings learnt on the protograph could still intervene in the fine-tuning procedure. In this work, however, we stick to plain entity KGE initialization, i.e. protograph embeddings are frozen after being transferred, and do not interact with entity KGEs anymore.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head><p>In our experiments, we apply the MASCHInE approach presented in Section 3. We experiment with five popular KGEMs. We first train on protographs, and then use the protograph embeddings to initialize entity and relation embeddings in the KG. Training starts on the KG and the fine-tuned KGEs are ultimately assessed in three tasks, namely LP, EC, and NC. For each task, we compare the vanilla setting (V), i.e., directly training the respective KGEM on the KG, with the MASCHInE approach based on P1 (resp. P2). Datasets, optimal hyperparameters, scripts, and pre-trained embeddings are publicly released <ref type="foot" target="#foot_3">5</ref> . In the following, protographs, KG entities and relations are encoded using five mainstream KGEMs: TransE <ref type="bibr" target="#b2">[3]</ref>, DistMult <ref type="bibr" target="#b23">[24]</ref>, ComplEx <ref type="bibr" target="#b19">[20]</ref>, ConvE <ref type="bibr" target="#b7">[8]</ref>, and TuckER <ref type="bibr" target="#b0">[1]</ref>. Datasets. Due to the need for schema-defined KGs, the following three benchmark datasets are considered: YAGO14k, FB15k187, and DBpedia77k <ref type="bibr" target="#b12">[13]</ref>. In particular, all entities are typed and relations have both a defined domain and range.  Table <ref type="table">2</ref>: Link prediction results on YAGO14k, FB15k187, and DBpedia77k using KGEMs trained without protograph (V), with P1, and P2. For a given dataset and KGEM, comparisons are made between the three settings, and best results are in bold fonts. Hits@K and Sem@K are abbreviated as H@K and S@K, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Link Prediction</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>YAGO14k FB15k187 DBpedia77k</head><p>MRR H@3 H@10 S@3 S@10 MRR H@3 H@10 S@3 S@10 MRR H@3 H@10 S@3 S@10 Evaluation metrics. This section is motivated by evaluating the fine-tuned KGEs for LP. Performance is assessed w.r.t. Mean Reciprocal Rank (MRR), Hits@ùêæ, and Sem@ùêæ <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12]</ref>. The latter accounts for the proportion of triples whose predicted head (resp. tail) belongs to the domain (resp. range) of the relation.</p><p>Implementation details. KGEMs were implemented in PyTorch.</p><p>In accordance with <ref type="bibr" target="#b12">[13]</ref>, KGEMs are trained during 400 epochs. For TransE, DistMult, and ComplEx, uniform random negative sampling <ref type="bibr" target="#b2">[3]</ref> was used to pair each train triple with one negative counterpart. ConvE and TuckER are trained using 1-N scoring <ref type="bibr" target="#b7">[8]</ref>.</p><p>In  <ref type="table" target="#tab_0">1</ref>) and entities share many classes together. In this situation, the protograph may not contain discriminative enough information for pre-training embeddings, which is reflected in the similar results achieved regardless of the training setting. In addition, we observe that some KGEMs seem to benefit more from the MASCHInE approach. In particular, ConvE and TuckER tend to perform better w.r.t. both rank-based and semantic-oriented metrics. Overall, we notice that including protographs in the training phase helps making semantically valid predictions, as evidenced by the better Sem@ùêæ under P1 and P2. Protographs may help organize the embedding space in such a way that geometric distances between embeddings better reflect their semantic similarities. Regarding rank-based metrics, we notice equivalent performance, which could be explained since LP relies less on semantics and more on relational patterns <ref type="bibr" target="#b1">[2]</ref> compared to other tasks such as entity clustering (see below).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Entity Clustering</head><p>Jain et al. <ref type="bibr" target="#b13">[14]</ref> demonstrate that embeddings do not consistently capture the semantics of the KG (in particular, they only considered schema-agnostic approaches). In this section, we investigate whether embeddings learnt for the LP task under P1 and P2 can actually prove useful in EC.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4.2.1</head><p>Experiments on the original datasets. We investigate whether the embeddings learnt for the LP task (see Section 4.1) under P1 and P2 allow a better class separability for the task of EC.</p><p>Datasets. We reuse YAGO14k, FB15k187, and DBpedia77k as presented in Section 4.1. The ground-truth labels are the most-generic classes an entity belongs to. It should be noted that entities belonging to several most-generic classes are filtered out as they would be assigned multiple ground-truth labels.</p><p>Implementation details. Entity embeddings are retrieved at the best epoch on the validation sets of YAGO14k, FB15k187, and DB-pedia77k, for all the KGEMs and under the three settings V, P1, and P2. Then, following the evaluation protocol in Jain et al. <ref type="bibr" target="#b13">[14]</ref>, the k-means clustering algorithm is run using default parameters of scikit-learn <ref type="foot" target="#foot_4">6</ref> .</p><p>Evaluation metrics. Clustering results are evaluated using the Adjusted Rand Index (ARI) -which measures the similarity between the cluster assignments by making pairwise comparisons -and Normalized Mutual Information (NMI) -which measures the agreement between the cluster assignments. It should be noted that these metrics are applicable as we have the ground-truth labels for each entity. In particular, we compare clusters output by k-means with ground-truth labels that are the classes entities belong to. ARI and NMI results are reported in Figure <ref type="figure" target="#fig_2">3</ref>. Lowest and highest scores are 0 (-1 for ARI) and 1, respectively.</p><p>Experimental results. Except for TuckER on FB15k187, both settings P1 and P2 provide substantial improvements. It seems that the entity embeddings learnt by TuckER are not able to cluster entities based on their classes, which diminishes the relevancy of using this model for drawing comparisons. That being said, the benefit of P1 and P2 is consistent across datasets. This is even more striking for YAGO14k, as evidenced in Figure <ref type="figure" target="#fig_3">4</ref>: entities sharing the same ground-truth class tend to cluster more under P2. Jain et al. <ref type="bibr" target="#b13">[14]</ref> pointed out the limitations of using KGEs for semantic representation of the underlying entities and relations. The relatively low performance of embeddings for EC under V (Figure <ref type="figure" target="#fig_2">3</ref> and Figure <ref type="figure" target="#fig_3">4</ref>) indeed suggests that the semantic similarity between entities is not reflected through geometric similarities in their embeddings. However, P1 and P2 generate entity latent representations whose geometric similarities are more representative of their classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Experiments on GEval clustering problems.</head><p>In this follow-up entity clustering experiment, the GEval evaluation framework <ref type="bibr" target="#b16">[17]</ref> is used. GEval provides gold standard datasets based on DBpedia and suggest clustering models, configurations and evaluation metrics <ref type="foot" target="#foot_5">7</ref> .</p><p>Datasets. GEval proposes 4 datasets based on DBpedia. The first dataset contain entities of classes dbo:AmericanFootballTeam and dbo:BasketballTeam. We do not use this dataset as DBpedia77k does not contain any of these entities. Instead, we use the other three datasets for which DBpedia77k has a biggest coverage. For clarity, we name these datasets as CC, CCB, and CAMAP. The first two ones contain different numbers of DBpedia entities of classes dbo:Cities and dbo:Countries, while the third dataset contains five types of entities. Therefore, there are two ground-truth clusters in the first two datasets and five clusters in the third one.</p><p>Implementation details. We rely on GEval guidelines and perform the preliminary filtering step described as follows. First, we select entity embeddings learnt on DBpedia77k for the LP task (see Section 4.1). Embeddings of entities that do not appear in the GEval benchmark datasets are filtered out. Clustering with all the algorithms suggested in the GEval framework is performed on the remaining entities and evaluated w.r.t. several metrics.</p><p>Evaluation metrics. As in the previous experiment, we use ARI.</p><p>Following the GEval guidelines, we additionally use Adjusted Mutual Information Score (AMI), V-Measure (VM), Fowlkes-Mallow Score (FM), Homogeneity (H), and Completeness (C). V-Measure is the harmonic mean of homogeneity and completeness measure, while the Fowlkes-Mallow Score is the geometric mean of pairwise precision and recall. All these metrics measure the correctness of the cluster assignments based on ground-truth labels. For each of these datasets, and under each setting, results achieved with the   best performing clustering method are shown in Table <ref type="table" target="#tab_5">3</ref>. Lowest and highest scores are 0 (-1 for ARI) and 1, respectively.</p><p>Experimental results. First, it is worth noting that results on CAMAP are significantly higher than on the other two datasets. Recall that CAMAP includes entities belonging to five different classes, whereas CC and CCB only contain entities from dbo:Cities and dbo:Countries. In this experiment, the embeddings learnt by the KGEMs are most useful for performing EC with five ground-truth clusters than with two, which might seem counterintuitive. However, in <ref type="bibr" target="#b13">[14]</ref>, it is demonstrated that in a KG, only a small subset of its entities actually have an embedding with a meaningful semantic representation. This is the reason why for the clustering task, results are not consistent across subsets of entities <ref type="bibr" target="#b13">[14]</ref>. This is in line with our experimental findings that lead to the same conclusion. <ref type="bibr" target="#b24">[25]</ref> is also concerned with the ability of KGEMs to embed entities from the same class to the same region of the embedding space. Their results suggest that entities belonging to rare classes have embeddings that are less separable w.r.t. to entity embeddings of other classes compared with entities belonging to frequent classes. This assumption needs to be qualified in light of our experimental findings: lots of entities in DBpedia77k are dbo:City and dbo:Country. Following <ref type="bibr" target="#b24">[25]</ref>, we should expect better results for CC and CCB in Table <ref type="table" target="#tab_5">3</ref>. However, it should be noted that both dbo:City and dbo:Country are subclasses of dbo:Place (equivalent to dbo:Location), which is over-represented in DBpedia77k <ref type="foot" target="#foot_6">8</ref> . It is arguably harder to separate entity embeddings for CC and CCB, as all of them actually belong to the same parent class. In contrast, CAMAP contain albums, cities, companies, movies, and universities. In this case, dbo:City is a child class of dbo:Place, dbo:Company and dbo:University are child classes of dbo:Agent while dbo:Album and dbo:Movie are child classes of dbo:Work.  With three different generic classes, the semantic similarities between entities in CAMAP is more easily reflected into the geometric similarities of their embeddings. Interestingly, the discrepancies in results, i.e., better results on CAMAP compared with CC and CCB, are setting-dependent (V, P1, P2). Indeed, they are alleviated when using either P1 or P2. Under V, TransE, ComplEx, and ConvE provide substantially better results on CAMAP compared to CC. Once using P2, we cannot see any significant difference in results on CAMAP compared to CC for these models. This means that training with a protograph-assisted approach led the KGEMs to generate higher-quality embeddings in terms of class separability, especially for entities in CC and CCB that are harder to distinguish. With the sole exception of TuckER whose performance may be due to non-optimal hyperparameter tuning, strong improvements are achieved w.r.t. the EC task on all datasets. Interestingly, not only V is outperformed by either P1 or P2 in each scenario, but actually by both of them at the same time. As a result, our schema-based, protograph-assisted learning approach MASCHInE proves effective for EC, regardless of the protograph design principle.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Node Classification</head><p>Following on from the previous clustering experiment that clearly showed the benefit of incorporating pre-trained protograph embeddings, in this section we are interested in performing node classification with such embeddings. In this experiment, entities are labelled on the basis of whether they comply with some description logic (DL) constructors.</p><p>Datasets. In this experiment, we use the synthetic DLCC node classification benchmark <ref type="bibr" target="#b18">[19]</ref>. The DLCC benchmark helps analyze the quality of KGEs by evaluating them w.r.t. which kinds of DL constructors they can represent. We use the synthetic gold standard datasets <ref type="foot" target="#foot_7">9</ref> . These are 12 synthetic datasets featuring labelled entities to classify and underpinned by a schema defining entity types, relation domains and ranges. The synthetic part of DLCC has been chosen because the authors argue that those pose classification problems which are not influenced by any side effects and correlations in the knowledge graph.</p><p>Implementation details. For these 12 test cases, their respective protographs are built. Then, the KGEMs are trained w.r.t. the LP task under the three different settings V, P1, and P2. In line with <ref type="bibr" target="#b18">[19]</ref>, multiple classifiers were trained for each test case: decision trees (DT), naive bayes (NB), k-nearest neighbors (k-NN), support vector machine (SVM), and multilayer perceptron (MLP). Based on the best entity embeddings learnt during LP, these models are used to classify entities using the default scikit-learn parameters.</p><p>Evaluation metrics. NC performance is measured using F-score, which is the harmonic mean of precision and recall. Lowest and highest scores are 0 and 1, respectively. For each model and test case, results achieved with the best classifier are reported in Table <ref type="table" target="#tab_6">4</ref>.</p><p>Experimental results. First, not all of the gold standard test cases are equally hard for the task of NC. For instance, entities in tc01, tc05, and tc06 are obviously easier to label than in the other datasets. Particularly, the DLCC synthetic gold standard datasets are built on the basis of different DL constructors in order to analyze which KG constructs are more easily recognized by KGEMs.</p><p>For the less trivial cases, the gains that can be achieved by using protographs are more significant. Especially in cases which are fairly badly solved by vanilla embeddings (e.g., tc07, tc12), there are gains of more than 10 percentage points in F1. It is further remarkable that on this problem, compared to link prediction and entity clustering, TuckER can also benefit from the protographs.</p><p>When looking at which protograph construction strategy is more suitable for which kind of problem, we see that for those test cases which use a concrete class in their definition (particularly tc07, tc08, tc11, and tc12), P2 is clearly superior to P1. This may be due to the fact that P1 does not create class embeddings for all classes used in the respective constructors. Therefore, the resulting entity embeddings are less discriminating.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION AND FUTURE WORK</head><p>In this paper, we present MASCHInE -a novel model-agnostic and protograph-assisted approach for learning KGEs. MASCHInE first trains embeddings on protographs and then transfers these embeddings to learn the final KGEs. We have shown that this approach produces more versatile KGEs, which can yield significant improvements in entity clustering and node classification. This is particularly noticeable for entity clustering, as protographs encode information about classes, on which entity clustering heavily relies on. We also demonstrated the advantage of using MASCHInE for link prediction: the use of protographs in the training procedure leads KGEMs to make more semantically valid predictions, which clearly highlights that geometric distance in the embedding space can be influenced by the semantics of the underlying KG.</p><p>In future work, we will extend MASCHInE by adding new protograph design principles. While in the current form, the transfer from the RDFS-based protograph embeddings to the KG entity embeddings is done once, we will study more expressive ontological constructs (e.g., OWL) and alternatives for KG and protograph iterative co-training. Additionally, it would be worth exploring whether pre-training over protographs provides benefits compared to passing the relevant RDFS triples to the KGEM as part of the whole graph. Comparing our approach with KGEMs that take advantage of additional information in the KG -e.g. attributes -also deserves further exploration.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Excerpt from DBpedia class hierarchy (left) and the generated prototriples (right) according to P2. The full URI names have been shortened due to space limitations. The prototriple highlighted in green (right) comes from the schema, whereas the following triples are generated by fixing the domain (resp. range) of dbo:headquarter and replacing the class in the range (resp. domain) with each of its direct subclasses.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Overview of MASCHInE. The overall training approach is based on several modules that each features different possibilities. First, the protograph is built following predefined design principles (a). Protograph embeddings are learnt (b) before being transferred to the corresponding entities and relations of the KG (c). Next, training starts on the KG (d). During this step, embeddings learnt on the protograph can still interact in the fine-tuning procedure (purple dotted arrow). Finally, the fine-tuned KGEs can be used in several tasks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Entity clustering results on YAGO14k, FB15k187, and DBpedia77k.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: PCA visualizations of YAGO14k entity embeddings using the first two principal components. Each point represents an entity, whose color denotes its ground-truth class.</figDesc><graphic coords="7,42.94,409.85,131.89,87.93" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>KG and protograph characteristics. Column headers from left to right: number of entities, relations, and triples.</figDesc><table><row><cell>Dataset</cell><cell></cell><cell>| E |</cell><cell>| R |</cell><cell>| T |</cell></row><row><cell></cell><cell cols="2">KG 14, 178</cell><cell>37</cell><cell>19, 183</cell></row><row><cell>YAGO14k</cell><cell>P1</cell><cell>22</cell><cell>37</cell><cell>37</cell></row><row><cell></cell><cell>P2</cell><cell>590</cell><cell>37</cell><cell>4, 959</cell></row><row><cell></cell><cell cols="4">KG 14, 305 187 278, 436</cell></row><row><cell>FB15k187</cell><cell>P1</cell><cell cols="2">138 187</cell><cell>187</cell></row><row><cell></cell><cell>P2</cell><cell cols="2">138 187</cell><cell>187</cell></row><row><cell></cell><cell cols="4">KG 76, 651 150 190, 028</cell></row><row><cell>DBpedia77k</cell><cell>P1</cell><cell cols="2">55 150</cell><cell>150</cell></row><row><cell></cell><cell>P2</cell><cell cols="2">186 150</cell><cell>3, 210</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1</head><label>1</label><figDesc></figDesc><table /><note><p>provides statistics for</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>this work, we stick with the loss function originally used for each KGEM. Evaluation is performed every 10 epochs and the best model w.r.t. MRR on the validation set is saved for performance assessment on the test set. The aforementioned procedure prevails for the vanilla training mode. For P1 and P2, we use the same procedure as for training on the KG, except that 200 epochs of training are previously performed on the protograph, for a total of 600 training epochs.</figDesc><table><row><cell>Experimental results. LP results are reported in Table 2. On</cell></row><row><cell>YAGO14k and DBpedia77k, learning embeddings with P1 or P2 pro-</cell></row><row><cell>vides better Sem@ùêæ values compared to V. No significant change is</cell></row><row><cell>noticed on FB15k187. This is likely to be an artefact of the Freebase</cell></row><row><cell>class hierarchy and how entities are typed in FB15k187: the class hi-</cell></row><row><cell>erarchy only has 2 levels, all relation domains and ranges are level-2</cell></row><row><cell>classes (therefore P1 and P2 are the same, see Table</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Entity clustering results on the GEval datasets. Bold font indicates which setting performs the best for a given model and dataset, while underlined results correspond to the second best setting.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>CC</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>CCB</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">CAMAP</cell></row><row><cell></cell><cell></cell><cell>ARI</cell><cell>AMI VM</cell><cell>FM</cell><cell>H</cell><cell>C</cell><cell>ARI</cell><cell>AMI VM</cell><cell>FM</cell><cell>H</cell><cell>C</cell><cell>ARI</cell><cell>AMI VM</cell><cell>FM</cell><cell>H</cell><cell>C</cell></row><row><cell></cell><cell>V</cell><cell cols="5">0.179 0.140 0.141 0.596 0.143 0.139</cell><cell cols="5">0.183 0.147 0.147 0.649 0.164 0.133</cell><cell cols="4">0.564 0.783 0.785 0.736 0.951 0.668</cell></row><row><cell>TransE</cell><cell>P1</cell><cell cols="5">0.621 0.514 0.515 0.823 0.505 0.526</cell><cell cols="5">0.288 0.311 0.311 0.692 0.349 0.280</cell><cell cols="4">0.593 0.795 0.796 0.756 0.956 0.682</cell></row><row><cell></cell><cell>P2</cell><cell cols="5">0.680 0.570 0.571 0.849 0.563 0.579</cell><cell cols="5">0.299 0.314 0.314 0.698 0.353 0.284</cell><cell cols="4">0.646 0.815 0.816 0.791 0.964 0.708</cell></row><row><cell></cell><cell>V</cell><cell cols="14">0.018 0.024 0.027 0.709 0.015 0.095 -0.024 0.009 0.010 0.770 0.006 0.028 -0.054 0.099 0.106 0.478 0.087 0.136</cell></row><row><cell>DistMult</cell><cell>P1</cell><cell cols="5">0.891 0.817 0.817 0.948 0.813 0.821</cell><cell cols="5">0.762 0.673 0.673 0.907 0.708 0.642</cell><cell cols="4">0.655 0.705 0.707 0.798 0.782 0.646</cell></row><row><cell></cell><cell>P2</cell><cell cols="5">0.891 0.824 0.824 0.948 0.818 0.830</cell><cell cols="5">0.756 0.626 0.627 0.907 0.646 0.608</cell><cell cols="4">0.933 0.872 0.872 0.963 0.886 0.859</cell></row><row><cell></cell><cell>V</cell><cell cols="10">-0.011 0.009 0.011 0.694 0.007 0.033 -0.024 0.009 0.010 0.771 0.006 0.028</cell><cell cols="4">0.139 0.345 0.350 0.520 0.341 0.360</cell></row><row><cell>ComplEx</cell><cell>P1</cell><cell cols="5">0.897 0.827 0.828 0.951 0.823 0.832</cell><cell cols="5">0.619 0.560 0.560 0.845 0.606 0.521</cell><cell cols="4">0.969 0.916 0.917 0.983 0.939 0.896</cell></row><row><cell></cell><cell>P2</cell><cell cols="5">0.319 0.264 0.265 0.665 0.269 0.262</cell><cell cols="5">0.493 0.460 0.460 0.788 0.507 0.422</cell><cell cols="4">0.961 0.914 0.914 0.978 0.938 0.892</cell></row><row><cell></cell><cell>V</cell><cell cols="5">0.311 0.234 0.235 0.691 0.223 0.249</cell><cell cols="5">0.271 0.242 0.243 0.687 0.271 0.220</cell><cell cols="4">0.591 0.608 0.611 0.76 0.672 0.559</cell></row><row><cell>ConvE</cell><cell>P1</cell><cell cols="5">0.878 0.805 0.805 0.942 0.799 0.811</cell><cell cols="5">0.719 0.628 0.628 0.889 0.665 0.596</cell><cell cols="4">0.934 0.880 0.881 0.964 0.928 0.839</cell></row><row><cell></cell><cell>P2</cell><cell cols="5">0.916 0.848 0.848 0.960 0.847 0.849</cell><cell cols="5">0.790 0.697 0.697 0.919 0.727 0.669</cell><cell cols="4">0.954 0.915 0.915 0.975 0.968 0.868</cell></row><row><cell></cell><cell>V</cell><cell cols="5">0.179 0.138 0.139 0.596 0.141 0.137</cell><cell cols="5">0.158 0.120 0.121 0.639 0.135 0.109</cell><cell cols="4">0.396 0.526 0.529 0.618 0.636 0.453</cell></row><row><cell>TuckER</cell><cell>P1</cell><cell cols="5">0.069 0.051 0.052 0.543 0.053 0.051</cell><cell cols="5">0.137 0.094 0.094 0.631 0.105 0.086</cell><cell cols="4">0.376 0.635 0.637 0.599 0.795 0.531</cell></row><row><cell></cell><cell>P2</cell><cell cols="5">0.047 0.042 0.043 0.533 0.043 0.042</cell><cell cols="5">0.033 0.051 0.052 0.578 0.058 0.046</cell><cell cols="4">0.276 0.487 0.490 0.520 0.621 0.405</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Node classification results on the 12 DLCC test cases. Listed are the results of the best classifier for each combination of model and test case. Bold fonts indicate which setting performs the best for a given model and test case. Underlined results show which combination of model and setting performs the best for each test case.</figDesc><table><row><cell cols="14">Test Case Setting TransE DistMult ComplEx ConvE TuckER Test Case Setting TransE DistMult ComplEx ConvE TuckER</cell></row><row><cell></cell><cell>V</cell><cell>0.702</cell><cell>0.810</cell><cell>0.820</cell><cell>0.885</cell><cell>0.832</cell><cell></cell><cell>V</cell><cell>0.654</cell><cell>0.564</cell><cell>0.566</cell><cell>0.732</cell><cell>0.669</cell></row><row><cell>tc01</cell><cell>P1</cell><cell>0.647</cell><cell>0.754</cell><cell>0.784</cell><cell>0.875</cell><cell>0.857</cell><cell>tc02</cell><cell>P1</cell><cell>0.566</cell><cell>0.554</cell><cell>0.554</cell><cell>0.764</cell><cell>0.637</cell></row><row><cell></cell><cell>P2</cell><cell>0.637</cell><cell>0.794</cell><cell>0.769</cell><cell>0.852</cell><cell>0.792</cell><cell></cell><cell>P2</cell><cell>0.571</cell><cell>0.627</cell><cell>0.627</cell><cell>0.687</cell><cell>0.679</cell></row><row><cell></cell><cell>V</cell><cell>0.584</cell><cell>0.571</cell><cell>0.554</cell><cell>0.629</cell><cell>0.669</cell><cell></cell><cell>V</cell><cell>0.638</cell><cell>0.540</cell><cell>0.512</cell><cell>0.818</cell><cell>0.570</cell></row><row><cell>tc03</cell><cell>P1</cell><cell>0.589</cell><cell>0.541</cell><cell>0.519</cell><cell>0.699</cell><cell>0.664</cell><cell>tc04</cell><cell>P1</cell><cell>0.760</cell><cell>0.812</cell><cell>0.832</cell><cell>0.870</cell><cell>0.552</cell></row><row><cell></cell><cell>P2</cell><cell>0.521</cell><cell>0.544</cell><cell>0.526</cell><cell>0.622</cell><cell>0.556</cell><cell></cell><cell>P2</cell><cell>0.805</cell><cell>0.802</cell><cell>0.818</cell><cell>0.818</cell><cell>0.680</cell></row><row><cell></cell><cell>V</cell><cell>0.704</cell><cell>0.683</cell><cell>0.678</cell><cell>0.822</cell><cell>0.814</cell><cell></cell><cell>V</cell><cell>0.645</cell><cell>0.940</cell><cell>0.938</cell><cell>0.940</cell><cell>0.950</cell></row><row><cell>tc05</cell><cell>P1</cell><cell>0.887</cell><cell>0.937</cell><cell>0.940</cell><cell>0.940</cell><cell>0.814</cell><cell>tc06</cell><cell>P1</cell><cell>0.718</cell><cell>0.875</cell><cell>0.898</cell><cell>0.935</cell><cell>0.950</cell></row><row><cell></cell><cell>P2</cell><cell>0.937</cell><cell>0.940</cell><cell>0.940</cell><cell>0.940</cell><cell>0.902</cell><cell></cell><cell>P2</cell><cell>0.740</cell><cell>0.828</cell><cell>0.788</cell><cell>0.895</cell><cell>0.942</cell></row><row><cell></cell><cell>V</cell><cell>0.578</cell><cell>0.515</cell><cell>0.552</cell><cell>0.560</cell><cell>0.522</cell><cell></cell><cell>V</cell><cell>0.552</cell><cell>0.552</cell><cell>0.520</cell><cell>0.598</cell><cell>0.575</cell></row><row><cell>tc07</cell><cell>P1</cell><cell>0.505</cell><cell>0.540</cell><cell>0.560</cell><cell>0.560</cell><cell>0.552</cell><cell>tc08</cell><cell>P1</cell><cell>0.585</cell><cell>0.560</cell><cell>0.558</cell><cell>0.575</cell><cell>0.580</cell></row><row><cell></cell><cell>P2</cell><cell>0.582</cell><cell>0.605</cell><cell>0.655</cell><cell>0.675</cell><cell>0.648</cell><cell></cell><cell>P2</cell><cell>0.595</cell><cell>0.588</cell><cell>0.588</cell><cell>0.578</cell><cell>0.672</cell></row><row><cell></cell><cell>V</cell><cell>0.552</cell><cell>0.522</cell><cell>0.525</cell><cell>0.730</cell><cell>0.745</cell><cell></cell><cell>V</cell><cell>0.578</cell><cell>0.572</cell><cell>0.562</cell><cell>0.640</cell><cell>0.690</cell></row><row><cell>tc09</cell><cell>P1</cell><cell>0.560</cell><cell>0.538</cell><cell>0.552</cell><cell>0.745</cell><cell>0.780</cell><cell>tc10</cell><cell>P1</cell><cell>0.598</cell><cell>0.550</cell><cell>0.538</cell><cell>0.625</cell><cell>0.692</cell></row><row><cell></cell><cell>P2</cell><cell>0.550</cell><cell>0.615</cell><cell>0.598</cell><cell>0.672</cell><cell>0.742</cell><cell></cell><cell>P2</cell><cell>0.538</cell><cell>0.510</cell><cell>0.535</cell><cell>0.590</cell><cell>0.588</cell></row><row><cell></cell><cell>V</cell><cell>0.632</cell><cell>0.518</cell><cell>0.535</cell><cell>0.688</cell><cell>0.700</cell><cell></cell><cell>V</cell><cell>0.650</cell><cell>0.538</cell><cell>0.552</cell><cell>0.702</cell><cell>0.708</cell></row><row><cell>tc11</cell><cell>P1</cell><cell>0.628</cell><cell>0.542</cell><cell>0.522</cell><cell>0.618</cell><cell>0.685</cell><cell>tc12</cell><cell>P1</cell><cell>0.572</cell><cell>0.530</cell><cell>0.565</cell><cell>0.708</cell><cell>0.700</cell></row><row><cell></cell><cell>P2</cell><cell>0.630</cell><cell>0.552</cell><cell>0.558</cell><cell>0.638</cell><cell>0.678</cell><cell></cell><cell>P2</cell><cell>0.625</cell><cell>0.802</cell><cell>0.738</cell><cell>0.802</cell><cell>0.752</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>Note that ùê∂ ùëñ and ùê∂ ùëó might be the same, so there can be cycles in the protograph.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1"><p>In the KGs used in this paper, all relations have an explicit domain and range.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2"><p>In our experiments, the same KGEM is used for both pre-training over the protograph and fine-tuning embeddings on the KG.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_3"><p>https://github.com/nicolas-hbt/versatile-embeddings</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_4"><p>https://scikit-learn.org/stable/index.html</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_5"><p>https://github.com/mariaangelapellegrino/Evaluation-Framework</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_6"><p>In DBpedia77k, 11, 586 entities belong to the dbo:Place class.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_7"><p>https://github.com/janothan/DL-TC-Generator</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">TuckER: Tensor Factorization for Knowledge Graph Completion</title>
		<author>
			<persName><forename type="first">Ivana</forename><surname>Balazevic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carl</forename><surname>Allen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 2019 Conf. on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019</title>
		<meeting>of the 2019 Conf. on Empirical Methods in Natural Language essing and the 9th International Joint Conference on Natural Language essing, EMNLP-IJCNLP 2019<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-11-03">2019. November 3-7, 2019</date>
			<biblScope unit="page" from="5184" to="5193" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Poisoning Knowledge Graph Embeddings via Relation Inference Patterns</title>
		<author>
			<persName><forename type="first">Peru</forename><surname>Bhardwaj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">D</forename><surname>Kelleher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luca</forename><surname>Costabello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Declan O'</forename><surname>Sullivan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021-08-01">2021. August 1-6, 2021</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1875" to="1888" />
		</imprint>
	</monogr>
	<note>Virtual Event</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Translating Embeddings for Modeling Multi-relational Data</title>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alberto</forename><surname>Garc√≠a-Dur√°n</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oksana</forename><surname>Yakhnenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conf. on Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="2787" to="2795" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Summarizing semantic graphs: a survey</title>
		<author>
			<persName><forename type="first">Sejla</forename><surname>Cebiric</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fran√ßois</forename><surname>Goasdou√©</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haridimos</forename><surname>Kondylakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dimitris</forename><surname>Kotzinos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ioana</forename><surname>Manolescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georgia</forename><surname>Troullinou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mussab</forename><surname>Zneika</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">VLDB J</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="295" to="327" />
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">HARP: Hierarchical Representation Learning for Networks</title>
		<author>
			<persName><forename type="first">Haochen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yifan</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Skiena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI-18), the 30th innovative Applications of Artificial Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational Advances in Artificial Intelligence (EAAI-18)</title>
		<meeting><address><addrLine>New Orleans, Louisiana, USA</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2018-02-02">2018. February 2-7, 2018</date>
			<biblScope unit="page" from="2127" to="2134" />
		</imprint>
	</monogr>
	<note>Proc. of the 32nd AAAI Conf. on Artificial Intelligence</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Type-augmented Relation Prediction in Knowledge Graphs</title>
		<author>
			<persName><forename type="first">Zijun</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pavan</forename><surname>Kapanipathi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kartik</forename><surname>Talamadupula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tian</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">35th Conf. on Artificial Intelligence, AAAI 2021, 33rd Conf. on Innovative Applications of Artificial Intelligence, IAAI 2021, The 11th Symposium on Educational Advances in Artificial Intelligence</title>
		<meeting><address><addrLine>EAAI</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2021-02-02">2021. 2021. February 2-9, 2021</date>
			<biblScope unit="page" from="7151" to="7159" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Injecting Background Knowledge into Embedding Models for Predictive Tasks on Knowledge Graphs</title>
		<author>
			<persName><forename type="first">Nicola</forename><forename type="middle">Flavio</forename><surname>Claudia D'amato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicola</forename><surname>Quatraro</surname></persName>
		</author>
		<author>
			<persName><surname>Fanizzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Semantic Web -18th International Conf., ESWC 2021</title>
		<title level="s">Proc. (Lecture Notes in Computer Science</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021-06-06">2021. June 6-10</date>
			<biblScope unit="volume">12731</biblScope>
			<biblScope unit="page" from="441" to="457" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Convolutional 2D Knowledge Graph Embeddings</title>
		<author>
			<persName><forename type="first">Tim</forename><surname>Dettmers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pasquale</forename><surname>Minervini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pontus</forename><surname>Stenetorp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI-18), the 30th innovative Applications of Artificial Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational Advances in Artificial Intelligence (EAAI-18)</title>
		<meeting><address><addrLine>New Orleans, Louisiana, USA</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2018-02-02">2018. February 2-7, 2018</date>
			<biblScope unit="page" from="1811" to="1818" />
		</imprint>
	</monogr>
	<note>Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">RDF graph summarization for first-sight structure discovery</title>
		<author>
			<persName><forename type="first">Fran√ßois</forename><surname>Goasdou√©</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pawel</forename><surname>Guzewicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ioana</forename><surname>Manolescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">VLDB J</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1191" to="1218" />
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Scaling Up Graph Neural Networks Via Graph Coarsening</title>
		<author>
			<persName><forename type="first">Zengfeng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shengzhong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chong</forename><surname>Xi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD &apos;21: The 27th ACM SIGKDD Conference on Knowledge Discovery and Data Mining</title>
		<meeting><address><addrLine>Virtual Event, Singapore</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2021-08-14">2021. August 14-18, 2021</date>
			<biblScope unit="page" from="675" to="684" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Knowledge Graph Embeddings for Link Prediction: Beware of Semantics!</title>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Hubert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Monnin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Armelle</forename><surname>Brun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Davy</forename><surname>Monticolo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Deep Learning for Knowledge Graphs (DL4KG 2022) co-located with the 21th International Semantic Web Conference (ISWC 2022)</title>
		<meeting>the Workshop on Deep Learning for Knowledge Graphs (DL4KG 2022) co-located with the 21th International Semantic Web Conference (ISWC 2022)</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Sem@ùêæ : Is my knowledge graph embedding model semantic-aware?</title>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Hubert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Monnin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Armelle</forename><surname>Brun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Davy</forename><surname>Monticolo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Treat Different Negatives Differently: Enriching Loss Functions with Domain and Range Constraints for Link Prediction</title>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Hubert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Monnin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Armelle</forename><surname>Brun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Davy</forename><surname>Monticolo</surname></persName>
		</author>
		<idno>CoRR abs/2303.00286</idno>
		<imprint>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Do Embeddings Actually Capture Knowledge Graph Semantics?</title>
		<author>
			<persName><forename type="first">Nitisha</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan-Christoph</forename><surname>Kalo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wolf-Tilo</forename><surname>Balke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ralf</forename><surname>Krestel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Semantic Web -18th International Conference, ESWC 2021, Virtual Event</title>
		<title level="s">Proceedings (Lecture Notes in Computer Science</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021-06-06">2021. June 6-10, 2021</date>
			<biblScope unit="volume">12731</biblScope>
			<biblScope unit="page" from="143" to="159" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Improving Knowledge Graph Embeddings with Ontological Reasoning</title>
		<author>
			<persName><forename type="first">Nitisha</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trung-Kien</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohamed</forename><forename type="middle">H</forename><surname>Gad-Elrab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daria</forename><surname>Stepanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Semantic Web -International Semantic Web Conf. ISWC</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">12922</biblScope>
			<biblScope unit="page" from="410" to="426" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Type-Constrained Representation Learning in Knowledge Graphs</title>
		<author>
			<persName><forename type="first">Denis</forename><surname>Krompa√ü</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephan</forename><surname>Baier</surname></persName>
		</author>
		<author>
			<persName><surname>Volker Tresp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Semantic Web -14th International Semantic Web Conf. (ISWC)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015">2015. 9366</date>
			<biblScope unit="page" from="640" to="655" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">GEval: A Modular and Extensible Evaluation Framework for Graph Embedding Techniques</title>
		<author>
			<persName><forename type="first">Angela</forename><surname>Maria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abdulrahman</forename><surname>Pellegrino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martina</forename><surname>Altabba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Petar</forename><surname>Garofalo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Ristoski</surname></persName>
		</author>
		<author>
			<persName><surname>Cochez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Semantic Web -17th International Conference, ESWC 2020</title>
		<title level="s">Proceedings (Lecture Notes in Computer Science</title>
		<meeting><address><addrLine>Heraklion, Crete, Greece</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020-05-31">2020. May 31-June 4, 2020</date>
			<biblScope unit="volume">12123</biblScope>
			<biblScope unit="page" from="565" to="582" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Probabilistic Coarsening for Knowledge Graph Embeddings</title>
		<author>
			<persName><forename type="first">Marcin</forename><surname>Pietrasik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Marek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Axioms</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">275</biblScope>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
	<note>Reformat</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">The DLCC Node Classification Benchmark for Analyzing Knowledge Graph Embeddings</title>
		<author>
			<persName><forename type="first">Jan</forename><surname>Portisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heiko</forename><surname>Paulheim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Semantic Web -ISWC 2022 -21st International Semantic Web Conference, Virtual Event</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022-10-23">2022. October 23-27, 2022</date>
			<biblScope unit="volume">13489</biblScope>
			<biblScope unit="page" from="592" to="609" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Complex Embeddings for Simple Link Prediction</title>
		<author>
			<persName><forename type="first">Th√©o</forename><surname>Trouillon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Welbl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">√âric</forename><surname>Gaussier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Bouchard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 33rd International Conf. on Machine Learning, ICML</title>
		<meeting>of the 33rd International Conf. on Machine Learning, ICML</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="2071" to="2080" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A Survey on Knowledge Graph Embeddings for Link Prediction</title>
		<author>
			<persName><forename type="first">Meihong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linling</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoli</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Symmetry</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">485</biblScope>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Exploiting Global Semantic Similarities in Knowledge Graphs by Relational Prototype Entities</title>
		<author>
			<persName><forename type="first">Xueliang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiajun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2206.08021</idno>
		<imprint>
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Representation Learning of Knowledge Graphs with Hierarchical Types</title>
		<author>
			<persName><forename type="first">Ruobing</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Fifth International Joint Conference on Artificial Intelligence, IJCAI 2016</title>
		<meeting>the Twenty-Fifth International Joint Conference on Artificial Intelligence, IJCAI 2016<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IJCAI/AAAI Press</publisher>
			<date type="published" when="2016-07">2016. July 2016. 2965-2971</date>
			<biblScope unit="page" from="9" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Embedding Entities and Relations for Learning and Inference in Knowledge Bases</title>
		<author>
			<persName><forename type="first">Bishan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3rd International Conference on Learning Representations</title>
		<imprint>
			<publisher>ICLR</publisher>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">What is the schema of your knowledge graph?: leveraging knowledge graph embeddings and clustering for expressive taxonomy learning</title>
		<author>
			<persName><forename type="first">Amal</forename><surname>Zouaq</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F√©lix</forename><surname>Martel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The International Workshop on Semantic Big Data, SBD@SIGMOD 2020</title>
		<meeting>The International Workshop on Semantic Big Data, SBD@SIGMOD 2020<address><addrLine>Portland, Oregon, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2020-06-19">2020. June 19, 2020</date>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
