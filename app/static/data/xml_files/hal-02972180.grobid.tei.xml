<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Dataset Independent Baselines for Relation Prediction in Argument Mining</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Oana</forename><surname>Cocarascu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Imperial College London</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">King&apos;s College London</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Elena</forename><surname>Cabrio</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">Université Côte d&apos;Azur</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
								<address>
									<settlement>Inria</settlement>
									<region>I3S</region>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Serena</forename><surname>Villata</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">Université Côte d&apos;Azur</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
								<address>
									<settlement>Inria</settlement>
									<region>I3S</region>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Francesca</forename><surname>Toni</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Imperial College London</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Dataset Independent Baselines for Relation Prediction in Argument Mining</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">6090B496C2675AEE64831843A698FB14</idno>
					<idno type="DOI">10.3233/FAIA200490</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-04-12T14:52+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Argument Mining</term>
					<term>Relation Prediction</term>
					<term>Machine Learning Methods Computational Models of Argument H. Prakken</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Argument(ation) Mining (AM) is the research area which aims at extracting argument components and predicting argumentative relations (i.e., support and attack) from text. In particular, numerous approaches have been proposed in the literature to predict the relations holding between arguments, and applicationspecific annotated resources were built for this purpose. Despite the fact that these resources were created to experiment on the same task, the definition of a single relation prediction method to be successfully applied to a significant portion of these datasets is an open research problem in AM. This means that none of the methods proposed in the literature can be easily ported from one resource to another. In this paper, we address this problem by proposing a set of dataset independent strong neural baselines which obtain homogeneous results on all the datasets proposed in the literature for the argumentative relation prediction task in AM. Thus, our baselines can be employed by the AM community to compare more effectively how well a method performs on the argumentative relation prediction task.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Argument(ation) Mining (AM) is "the general task of analyzing discourse on the pragmatics level and applying a certain argumentation theory to model and automatically analyze the data at hand" <ref type="bibr" target="#b15">[16]</ref>. Two tasks are crucial in AM <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b19">20]</ref>: 1) argument component detection within the input natural language text, aiming at the identification of the textual boundaries of the arguments and their classification (claim, premise); and 2) relation prediction, aiming at identifying (support, attack) relations between argumentative components, possibly identified in the first stage. In this paper we focus on the second task. Despite the high volume of approaches tackling the relation prediction task with satisfying results (see <ref type="bibr" target="#b5">[6]</ref> for an extensive list), a problem arises: these solutions heavily rely on the peculiar features of the dataset taken into account for the experimental setting and are hardly portable from one application domain to another. On the one side, this issue can be explained by the huge number of heterogeneous application domains where argumentative text may be analysed (e.g., online reviews, blogs, political debates, legal cases). On the other side, it represents a drawback for the comparison of the different approaches proposed in the literature, which are often presented as solutions addressing the relation prediction task from a dataset independent point of view. A side drawback for the AM community is therefore a lack of large annotated resources for this task, as most available resources cannot be successfully reused, being highly context-based. Even the employment of pretrained language models (e.g., BERT <ref type="bibr" target="#b11">[12]</ref>) does not address this issue.</p><p>In this paper, we tackle this issue by proposing a set of strong cross-dataset baselines based on different neural architectures. Our baselines are shown to perform homogeneously over all the datasets proposed in the literature for the relation prediction task in AM, differently from individual methods proposed in the literature. Our contribution is to bestow the AM community with a set of strong cross-dataset baselines to compare with in order to demonstrate how well a relation prediction method for AM performs.</p><p>We focus on two types of argumentative relations: attack and support, given that the majority of datasets target only these two types of relations. We define neural baselines to address the corresponding binary classification problem, analysing, to the best of our knowledge, all available datasets for this task, ranging from persuasive essays to user-generated content, to political speeches. Given two arguments, we are interested in determining the argumentative relation between the first, called child argument, and the second, called parent argument, using a neural model. For example, the child argument People know video game violence is fake may attack the parent argument Youth playing violent games exhibit more aggression. In our baselines, each of the two arguments is represented using embeddings as well as other features. We propose three neural network architectures for the classification task, two concerned with the way child and parent are passed through the network (concat model and mix model), and an attention-based model. We also explore BERT as an alternative to our baselines: although this is used successfully to boost performances for other tasks in Natural Language Processing, it is generally not competitive for relation prediction with the datasets we consider.</p><p>We conduct experiments with a number of datasets, chosen either because they were specially created for relation prediction in AM or because they can be easily transformed to be used for this task. These are: Essays (essay) <ref type="bibr" target="#b32">[33]</ref>, Microtexts (micro) <ref type="bibr" target="#b28">[29]</ref>, Nixon-Kennedy (nk) <ref type="bibr" target="#b22">[23]</ref>, Debatepedia (db) <ref type="bibr" target="#b4">[5]</ref>, IBM (ibm) <ref type="bibr" target="#b0">[1]</ref>, ComArg (com) <ref type="bibr" target="#b2">[3]</ref>, Web-content (web) <ref type="bibr" target="#b6">[7]</ref>, CDCP (cdcp) <ref type="bibr" target="#b27">[28]</ref>, UKP (ukp) <ref type="bibr" target="#b33">[34]</ref>, AIFdb (aif) <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b30">31]</ref>. Datasets' statistics can be found in Table <ref type="table" target="#tab_0">1</ref> <ref type="foot" target="#foot_0">1</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Neural baselines for relation prediction</head><p>We use four types of features: word embeddings, sentiment features, syntactic features, computed for both child and parent, and textual entailment from child to parent. We refer to the last three types of features as standard features. Word embeddings are distributed representations of texts in an n-dimensional space. Textual entailment represents the class (amongst entailment, contradiction, or neutral) obtained using AllenNLP<ref type="foot" target="#foot_1">2</ref> , a textual entailment model based on decomposable attention <ref type="bibr" target="#b26">[27]</ref>. The features related to sentiment are based on manipulation of SentiWordNet <ref type="bibr" target="#b14">[15]</ref> and the sentiment of the entire (child and parent) texts analysed using the VADER sentiment analyser <ref type="bibr" target="#b16">[17]</ref>. Every WordNet synset <ref type="bibr" target="#b23">[24]</ref> can be associated to three scores describing how objective, positive, and negative it is. For every word in the (child and parent) texts, we select the first synset and compute its positive score and its negative score. In summary, the features related to sentiment for a text t that consists of n words, W i = 1 . . .w n , are the following: (i) sentiment score (∑ w i pos score(w i )neg score(w i )), (ii) number of positive/negative/neutral words in t, (iii) sentiment polarity class and score of t. Syntactic features consist of text statistics (e.g., number of words) and word statistics with respect to part-of-speech tags (i.e., number of words, nouns, verbs, first person singular, etc.) and lexical diversity (i.e., number of unique words divided by the total number of words in text t).</p><p>We describe the three neural architectures we propose for determining the argumentative relation (of attack or support) holding between child and parent. For all, we report only configurations of the architectures and number/size of the hidden layers which performed the best<ref type="foot" target="#foot_2">3</ref> . For our models, we use GRUs <ref type="bibr" target="#b10">[11]</ref> as they take less time to train and are more efficient.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Concat model (C).</head><p>In this model, each of the child and parent embeddings is passed through a GRU. We concatenate the standard features of the child and of the parent. The merged standard vector is then concatenated with the outputs of the GRUs. The resulting vector is passed through 2 dense layers (of 256 neurons and 64 neurons, with sigmoid as activation function), and then to softmax to determine the argumentative relation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mix model (M).</head><p>In this model, we first concatenate the child and parent embeddings and then pass them through a GRU, differently from the concat model where we pass each embedding vector through a GRU first. We concatenate the standard features that we obtain for the child and for the parent. The merged standard vector is then concatenated with the output of the GRU. From this stage, the network resembles the concat model: the resulting vector is passed through 2 dense layers (of 256 neurons and 64 neurons, with sigmoid as activation function), to be then finally passed to softmax. Attention model (A). Inspired by the demonstrated effectiveness of attention-based models <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b34">35]</ref>, we combine the GRU-based model with attention mechanisms. Each of the child and parent embeddings is passed through a GRU and we compute attention in two directions. We concatenate the standard features of the child and of the parent. The merged standard vector is then concatenated with the outputs of the GRUs. The resulting vector is passed through a single dense layers (128 neurons, with sigmoid as activation function), that is then passed to softmax.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Experimental results</head><p>Non-neural baselines. For training we have used the larger datasets, aif, essay, ibm and web. We resampled the minority class from the essay dataset and used our models on the oversampled dataset. We did not use for training the ukp dataset as the parent is a topic instead of an argument. The models were then tested on the remaining datasets, with the average being computed on testing datasets. We report the F 1 performance of the attack class (A) and the support class (S) for the non-neural baselines in Table <ref type="table" target="#tab_1">2</ref>. We used Random Forests (RF) <ref type="bibr" target="#b3">[4]</ref> with 15 trees in the forest and gini impurity criterion and SVM with linear kernel using LIBSVM <ref type="bibr" target="#b8">[9]</ref>, obtained as a result of performing a grid search, as it is the most commonly used algorithm in the works that experiment on the datasets we considered <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b24">25]</ref>. On top of the standard features used for our neural models, for the baselines we added the following features: TF-IDF, number of common nouns, verbs and adjectives between the two texts as in <ref type="bibr" target="#b22">[23]</ref>, a different sentiment score nr pos-nr neg nr pos+nr neg+1 as in <ref type="bibr" target="#b0">[1]</ref>, with all features being normalized.</p><p>Neural baselines with non-contextualised word embeddings. Table <ref type="table" target="#tab_2">3</ref> shows the best baselines for relation prediction in AM. We experimented with GloVe (300-dimensional) embeddings <ref type="bibr" target="#b29">[30]</ref>, using pre-trained word representations in all our models. We used 100 as the sequence size as we noticed that there are few instances with more than 100 words. We used a batch size of 32 and trained for 10 epochs (as a higher number of epochs led to overfitting). We report the results using embeddings and syntactic features and the results with all the features presented in Section 2. We also conducted a feature ablation experiment (with embeddings being always used) and observed that syntactic features contribute the most to performance, with the other types of features bringing small improvements when used together only with embeddings. In addition, we have run experiments using two datasets for training to test whether combining two datasets improves performance. During training, we used one of the large datasets (aif, essay, ibm, web) and one of the remaining datasets (represented as blanks in the table ). Amongst the proposed architectures, the attention model generally performs better. Using only a single dataset for training, the model that performs the best is the mix model using all features and trained on the essay dataset. The best results are obtained when using another dataset along one of the larger datasets for training. This is because combining data from two domains we are able to learn better the types of argumentative relations. When using syntactic features, adding micro, cdcp, and ukp does not improve the results compared to using a single dataset for training. Indeed, cdcp has only one type of relation (i.e. support) resulting in an imbalanced dataset, and in ukp, the parent argument is a topic, which does not improve the prediction task. When using all features, micro, com, ukp, and nk do not contribute to an increase in performance. The best performing model is the attention mechanism trained on the web and essay datasets using syntactic features (0.544 macro average F 1 ).  <ref type="table">4</ref>. Experimental results with F 1 for Attack and for Support relations. XB stands for the number of BERT layers used (X=3,4) and YD stands for the number of dense layers (Y=1,2) used before the final layer that predicts the class. The blanks indicate the training datasets. The Average (Avg) and the Macro (Macr) Avg do not include the results for the training datasets.</p><p>beddings <ref type="bibr" target="#b11">[12]</ref> analyse the entire sentence before assigning embeddings to individual words. We employ BERT embeddings to test whether they bring any improvements to the classification task. While for GloVe vectors we do not need the original, trained model in order to use the embeddings, for the BERT embeddings we require the pre-trained language models that we can then fine tune using the datasets of the downstream task. We try different combinations: using 3 or 4 BERT layers and using 1 dense layer (of 64 neurons) or 2 dense layers (of 128 and 32 neurons, respectively) before the final layer that determines the class. Table <ref type="table">4</ref> shows the results with BERT embeddings instead of GloVe, using feature ablation (syntactic vs all features) and two datasets for training to test whether this can improve performance. The best results are obtained using 4 BERT layers and 2 dense layers (0.537 macro average F 1 ). However, this best BERT baseline does not outperform the best results with the attention model and GloVe embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Related work</head><p>In terms of results reported on the datasets we have conducted our experiments on, most works perform a cross-validation evaluation or, in the case of datasets consisting of several topics, the models proposed are trained on some of the topics and tested on the remaining topics. For essay, an Integer Linear Programming model was used to achieve 0.947 F 1 for support and 0.413 F 1 for attack on the testing dataset using cross-validation to select the model <ref type="bibr" target="#b32">[33]</ref>. Using SVM, 0.946 F 1 for support and 0.456 F 1 for attack were obtained <ref type="bibr" target="#b32">[33]</ref>. Using a modification of the Integer Linear Programming model to accommodate the lack of some features used for essay but not present in micro, 0.855 F 1 was obtained for support and 0.628 F 1 for attack. On micro, an evidence graph model was used to achieve 0.71 F 1 using cross-validation <ref type="bibr" target="#b28">[29]</ref>. On nk, 0.77 F 1 for attack and 0.75 F 1 for support were obtained using SVM and cross-validation <ref type="bibr" target="#b22">[23]</ref>. SVM accuracy results on the testing dataset using coverage (i.e. number of claims identified over the number of total claims) were reported in <ref type="bibr" target="#b0">[1]</ref> as follows: 0.849 accuracy for 10% coverage, 0.740 accuracy for 60% coverage, 0.632 accuracy for 100% coverage. RF were evaluated on web and aif using cross-validation, achieving 0.717 F 1 and 0.831 F 1 , respectively <ref type="bibr" target="#b7">[8]</ref>. Structured SVMs were evaluated in a cross-validation setting on cdcp and ukp using various types of factor graphs, full and strict <ref type="bibr" target="#b24">[25]</ref>. On cdcp, F 1 was 0.493 on the full graph and 0.50 on the strict graph, whereas on ukp, F 1 was 0.689 on the full graph and 0.671 on the strict graph. No results on the two-class datasets were reported for db, com, and ukp. The results on ukp treat either supporting and attacking arguments as a single or consider three types of relations: support, attack, neither. The latter type of reporting results on three classes is also given on the com. Some other works have started investigating the dataset independence in AM. <ref type="bibr" target="#b25">[26]</ref> showed how models may overlook textual content when provided with the context surrounding the span by relying on contextual markers for predicting relations and tested their method on the essay dataset. <ref type="bibr" target="#b20">[21]</ref> integrated (claim and other domain) lexicon information into neural networks with attention tested on ukp. <ref type="bibr" target="#b18">[19]</ref> experimented with span representations, originally developed for other tasks, on the essay dataset. Other works have used contextualised word embeddings for relation prediction in AM <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b31">32]</ref>. More recently, <ref type="bibr" target="#b13">[14]</ref> proposed and tested on ukp an argument retrieval system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Discussion and Conclusion</head><p>Dataset independence is one of the biggest in AM. AM model for relation prediction on every individual dataset we considered in this paper would perform better than any general baseline on that dataset. We believe an AM model would require leveraging a diverse corpus to be of use in a real-world system. Most works have previously focused on a moderate-sized corpus distributed across a small set of topics <ref type="bibr" target="#b13">[14]</ref>. This paper is a step towards the applicability of AM techniques across datasets. Our baselines perform homogeneously in terms of average over all existing datasets for relation prediction in AM while using generic features. We propose as baseline the model that performed the best, with the baseline using attention mechanism with GloVe embeddings and syntactic features trained on the web and essay datasets (0.544 macro average F 1 ). The results for the attack class are generally worse than those for support as the datasets that are used in training (e.g. essay, ibm) have fewer instances for the attack class than for support (see Table <ref type="table" target="#tab_0">1</ref>). The datasets differ at granularity: some consist of pairs of sentences (e.g., ibm) whereas others include pair of multiple-sentence arguments (e.g., nk). Additionally, the argumentative on relations can be domain-specific and their semantic nature may vary between corpora (e.g., com). We considered the unified task of determining support or attack between any two texts.</p><p>Embeddings represent the differentiating feature for the models we experimented with. Whilst word embeddings are often used as the first data processing layer in a deep learning model, we employed TF-IDF features for the non-neural models that we considered as baselines. Other works that address the task of relation prediction make use of features specific to the single dataset of interest, making it difficult to test those models on other datasets. For instance, for the essay dataset, <ref type="bibr" target="#b32">[33]</ref> use structural features such as number of preceding and following tokens in the covering sentence, number of components in paragraph, number of preceding and following components in paragraph, relative position of the argument component in paragraph. For the other datasets, <ref type="bibr" target="#b33">[34]</ref> use topic similarity features (as the parent argument is a topic), <ref type="bibr" target="#b22">[23]</ref> use the position of the topic and similarity with other related/unrelated pair from the dataset, keyword embeddings of topics from the dataset. We have used only general purpose features that are meaningful for all datasets addressing the relational AM task. Surprisingly, BERT embeddings (achieving state-of-the-art performances in many tasks <ref type="bibr" target="#b11">[12]</ref>) do not bring improvements here, compared to non-contextualised word embeddings.</p><p>To conclude, several resources have been built recently for the task of argumentative relation prediction, covering different topics like political speeches, Wikipedia articles, persuasive essays. Given the heterogeneity of these different kinds of text, it is hard to compare cross-dataset the different proposed approaches. We addressed this nonportability issue by making a broad comparison of different deep learning methods using both non-contextualised and contextualised word embeddings for a large set of datasets for the argumentative relation prediction task, an important and still widely open problem. We proposed a set of strong dataset-independent baselines based on several neural architectures and have shown that our models perform homogeneously over all existing datasets for relation prediction in AM.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Datasets' statistics.</figDesc><table><row><cell></cell><cell cols="2">essays micro</cell><cell>nk</cell><cell>db</cell><cell>ibm</cell><cell>com</cell><cell>web</cell><cell>cdcp</cell><cell>ukp</cell><cell>aif</cell></row><row><cell># attacks</cell><cell>497</cell><cell>108</cell><cell>378</cell><cell>141</cell><cell>1069</cell><cell>296</cell><cell>1301</cell><cell>0</cell><cell cols="2">5935 9854</cell></row><row><cell># supports</cell><cell>4841</cell><cell>263</cell><cell>353</cell><cell>179</cell><cell>1325</cell><cell>462</cell><cell cols="4">1329 1220 4759 7543</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Experimental results for non-neural baselines with F 1 for Attack and for Support. The blanks indicate the training dataset. The Average (Avg) and the Macro (Macr) Avg exclude the results for the training datasets.</figDesc><table><row><cell></cell><cell></cell><cell>essay micro db ibm com web cdcp ukp nk</cell><cell>aif</cell><cell>Avg Macr Avg</cell></row><row><cell>non-neural baselines</cell><cell>RF RF SVM SVM</cell><cell cols="3">F 1 A 0.32 0.24 0.40 F 1 S 0.57 0.74 0.64 F 1 A 0.57 0.40 0.45 0.53 0.43 0.33 0.38 0.67 0.59 0.85 0.53 0.59 0.54 0.636 -0.39 0.55 0.44 0.381 -0.60 0.52 0.57 0.509 F 1 S 0.44 0.47 0.52 0.41 0.57 0.51 0.45 0.50 0.38 0.472 F 1 A 0.34 0.36 0.33 0.29 0.38 -0.42 0.42 0.40 0.368 F 1 S 0.71 0.67 0.65 0.67 0.59 0.84 0.57 0.56 0.49 0.639 F 1 A 0.49 0.35 0.39 0.39 0.38 -0.56 0.57 0.520 0.456 F 1 S 0.50 0.54 0.52 0.59 0.60 0.67 0.46 0.47 0.500 0.539</cell><cell>0.508 0.490 0.503 0.498</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Neural baselines with contextualised word embeddings. Contextualised word embeddings such as the Bidirectional Encoder Representations from Transformers (BERT) em-Experimental results with F 1 for Attack and for Support for the Concat, Mix, and Attention architectures, with GloVE embeddings. The blanks indicate the training datasets. The Average (Avg) and the Macro (Macr) Avg do not include the results for the training datasets.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>essay micro db ibm com web cdcp ukp nk</cell><cell>aif</cell><cell>Avg Macr Avg</cell></row><row><cell>embed. + syntactic</cell><cell>C G A G A G</cell><cell>F 1 A F 1 S F 1 A F 1 S F 1 A F 1 S</cell><cell cols="3">0.35 0.43 0.48 0.31 0.45 0.71 0.68 0.58 0.70 0.54 0.77 0.47 -0.58 0.37 0.58 0.53 0.53 -0.61 0.59 0.55 0.537 0.43 0.433 0.50 0.619 0.61 0.60 0.42 0.50 0.72 0.43 0.38 0.47 0.516 0.36 0.48 0.43 0.39 -0.52 0.45 0.51 0.449 0.75 0.66 0.62 0.68 0.79 0.52 0.56 0.54 0.640</cell><cell>0.526 0.526 0.544</cell></row><row><cell>all features</cell><cell>M G A G A G</cell><cell>F 1 A F 1 S F 1 A F 1 S F 1 A F 1 S</cell><cell cols="3">0.37 0.43 0.43 0.40 0.46 0.71 0.64 0.61 0.70 0.55 0.78 0.11 0.78 0.51 0.599 -0.71 -0.46 0.466 0.36 0.54 0.50 0.51 -0.59 0.59 0.55 0.520 0.67 0.63 0.49 0.51 0.74 0.47 0.41 0.49 0.551 0.43 0.54 0.49 0.46 -0.59 0.63 0.63 0.539 0.68 0.55 0.57 0.56 0.65 0.46 0.38 0.47 0.540</cell><cell>0.532 0.535 0.539</cell></row><row><cell></cell><cell></cell><cell></cell><cell>essay micro db ibm com web cdcp ukp nk</cell><cell>aif</cell><cell>Avg Macr Avg</cell></row><row><cell>BERT + syntactic</cell><cell cols="5">4B F 1 A 2D F 1 S 4B F 1 A 1D F 1 S 4B F 1 A 0.50 0.36 0.46 0.55 0.47 0.53 0.50 0.61 0.57 0.59 0.49 0.69 0.47 0.48 0.46 0.545 -0.56 0.48 0.45 0.506 0.36 0.48 0.40 0.45 0.42 -0.53 0.37 0.430 0.69 0.67 0.61 0.62 0.57 0.79 0.50 0.50 0.619 0.50 -0.52 0.47 0.50 0.473 2D F 1 S 0.61 0.62 0.59 0.61 0.74 0.52 0.50 0.61 0.600</cell><cell>0.526 0.525 0.537</cell></row><row><cell>all features</cell><cell cols="5">4B F 1 A 1D F 1 S 3B F 1 A 0.48 0.34 0.48 0.53 0.50 0.54 0.51 0.59 0.56 0.55 0.47 0.67 0.45 0.48 0.49 0.533 -0.59 0.51 0.49 0.524 0.45 -0.45 0.50 0.54 0.463 2D F 1 S 0.57 0.65 0.60 0.64 0.73 0.55 0.52 0.54 0.600</cell><cell>0.529 0.532</cell></row><row><cell cols="2">Table</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>For more details about the individual datasets, we refer the reader to the relevant publications.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>https://allennlp.org</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>We also experimented with 1 and 2 hidden layers, and hidden layer sizes of 32, 64, 128, and 256, trying all possible combinations towards best configurations. We did not consider a higher number of hidden layers due to the small size of the data.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_3"><p>O. Cocarascu et al. / Dataset Independent Baselines for Relation Prediction in Argument Mining</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Stance classification of contextdependent claims</title>
		<author>
			<persName><forename type="first">R</forename><surname>Bar-Haim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Bhattacharya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Dinuzzo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Saha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Slonim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EACL</title>
		<meeting>EACL</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="251" to="261" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">On logical specifications of the argument interchange format</title>
		<author>
			<persName><forename type="first">F</forename><surname>Bex</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Modgil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Prakken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Reed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Logic and Computation</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="951" to="989" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Back up your stance: Recognizing arguments in online discussions</title>
		<author>
			<persName><forename type="first">F</forename><surname>Boltužić</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Šnajder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st Workshop on Argumentation Mining</title>
		<meeting>the 1st Workshop on Argumentation Mining</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="49" to="58" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Random forests</title>
		<author>
			<persName><forename type="first">L</forename><surname>Breiman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="5" to="32" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Node: A benchmark of natural language arguments</title>
		<author>
			<persName><forename type="first">E</forename><surname>Cabrio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Villata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COMMA</title>
		<meeting>COMMA</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="449" to="450" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Five years of argument mining: a data-driven analysis</title>
		<author>
			<persName><forename type="first">E</forename><surname>Cabrio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Villata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IJCAI</title>
		<meeting>IJCAI</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="5427" to="5433" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Towards relation based argumentation mining</title>
		<author>
			<persName><forename type="first">L</forename><surname>Carstens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Toni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd Workshop on Argumentation Mining</title>
		<meeting>the 2nd Workshop on Argumentation Mining</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="29" to="34" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Using argumentation to improve classification in natural language problems</title>
		<author>
			<persName><forename type="first">L</forename><surname>Carstens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Toni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Internet Technology</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1" to="30" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">LIBSVM: A library for support vector machines</title>
		<author>
			<persName><forename type="first">C</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Intelligent Systems and Technology TIST</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1" to="27" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Towards an argument interchange format</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">I</forename><surname>Chesñevar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mcginnis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Modgil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Rahwan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">R</forename><surname>Simari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>South</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Vreeswijk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Willmott</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowledge Eng Review</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="293" to="316" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning phrase representations using RNN Encoder-Decoder for statistical machine translation</title>
		<author>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Van Merriënboer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">¸</forename><surname>Gülc ¸ehre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1724" to="1734" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">BERT: pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL-HLT</title>
		<meeting>NAACL-HLT</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Determining relative argument specificity and stance for complex argumentative structures</title>
		<author>
			<persName><forename type="first">E</forename><surname>Durmus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Ladhak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Cardie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4630" to="4641" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Corpus wide argument mining -A working solution</title>
		<author>
			<persName><forename type="first">L</forename><surname>Ein-Dor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shnarch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Dankin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Halfon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Sznajder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Alzate</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gleize</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Choshen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bilu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Aharonov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Slonim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI</title>
		<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="7683" to="7691" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Sentiwordnet: A publicly available lexical resource for opinion mining</title>
		<author>
			<persName><forename type="first">A</forename><surname>Esuli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Sebastiani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of LREC</title>
		<meeting>LREC</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="417" to="422" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Argumentation mining in user-generated web discourse</title>
		<author>
			<persName><forename type="first">I</forename><surname>Habernal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Gurevych</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="125" to="179" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">VADER: A parsimonious rule-based model for sentiment analysis of social media text</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Hutto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Gilbert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICWSM</title>
		<meeting>ICWSM</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">The argument interchange format</title>
		<author>
			<persName><forename type="first">R</forename><surname>Iyad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Reed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Argumentation in Artificial Intelligence</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="383" to="402" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">An empirical study of span representations in argumentation structure parsing</title>
		<author>
			<persName><forename type="first">T</forename><surname>Kuribayashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ouchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Inoue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Reisert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Miyoshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Suzuki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Inui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4691" to="4698" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Argument mining: A survey</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Reed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="765" to="818" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Lexicon guided attentive neural network model for argument mining</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th Workshop on Argument Mining</title>
		<meeting>the 6th Workshop on Argument Mining</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="67" to="73" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Argumentation mining: State of the art and emerging trends</title>
		<author>
			<persName><forename type="first">M</forename><surname>Lippi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Torroni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Internet Technology</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">10</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Never retreat, never retract: Argumentation analysis for political speeches</title>
		<author>
			<persName><forename type="first">S</forename><surname>Menini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Cabrio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tonelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Villata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI</title>
		<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4889" to="4896" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Wordnet: A lexical database for english</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">A</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="39" to="41" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Argument mining with structured SVMs and RNNs</title>
		<author>
			<persName><forename type="first">V</forename><surname>Niculae</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Cardie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="985" to="995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Dissecting content and context in argumentative relation analysis</title>
		<author>
			<persName><forename type="first">J</forename><surname>Opitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Frank</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th Workshop on Argument Mining</title>
		<meeting>the 6th Workshop on Argument Mining</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="25" to="34" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A decomposable attention model for natural language inference</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Täckström</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2249" to="2255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A corpus of eRulemaking user comments for measuring evaluability of arguments</title>
		<author>
			<persName><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Cardie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of LREC</title>
		<meeting>LREC</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Joint prediction in MST-style discourse parsing for argumentation mining</title>
		<author>
			<persName><forename type="first">A</forename><surname>Peldszus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Stede</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="938" to="948" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">GloVe: Global vectors for word representation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">AIF+: dialogue in the argument interchange format</title>
		<author>
			<persName><forename type="first">C</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wells</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Devereux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Rowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COMMA</title>
		<meeting>COMMA</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="311" to="323" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Classification and clustering of arguments with contextualized word embeddings</title>
		<author>
			<persName><forename type="first">N</forename><surname>Reimers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schiller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Beck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Daxenberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Stab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Gurevych</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="567" to="578" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Parsing argumentation structures in persuasive essays</title>
		<author>
			<persName><forename type="first">C</forename><surname>Stab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Gurevych</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="619" to="659" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Cross-topic argument mining from heterogeneous sources</title>
		<author>
			<persName><forename type="first">C</forename><surname>Stab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schiller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Rai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Gurevych</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NeurIPS</title>
		<meeting>NeurIPS</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="6000" to="6010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Hierarchical attention networks for document classification</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">H</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL HLT</title>
		<meeting>NAACL HLT</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1480" to="1489" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
