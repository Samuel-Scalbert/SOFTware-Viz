<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Improving Anchor-based Explanations</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Julien</forename><surname>Delaunay</surname></persName>
							<email>julien.delaunay@irisa.fr</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Inria</orgName>
								<orgName type="institution" key="instit2">IRISA</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Luis</forename><surname>Galárraga</surname></persName>
							<email>luis.galarraga@inria.fr</email>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">Inria</orgName>
								<orgName type="institution" key="instit2">IRISA</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Christine</forename><surname>Largouët</surname></persName>
							<email>christine.largouet@irisa.fr</email>
							<affiliation key="aff2">
								<orgName type="institution" key="instit1">Agrocampus Ouest</orgName>
								<orgName type="institution" key="instit2">IRISA</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Improving Anchor-based Explanations</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">8D9DDAD27C2A5045A80AB714B9B390C3</idno>
					<idno type="DOI">10.1145/1122445.1122456</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-04-12T14:45+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Explainable AI</term>
					<term>Machine Learning</term>
					<term>Rule Mining</term>
					<term>Discretization</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Rule-based explanations are a popular method to understand the rationale behind the answers of complex machine learning (ML) classifiers. Recent approaches, such as Anchors, focus on local explanations based on if-then rules that are applicable in the vicinity of a target instance. This has proved effective at producing faithful explanations, yet anchor-based explanations are not free of limitations. These include long overly specific rules as well as explanations of low fidelity. This work presents two simple methods that can mitigate such issues on tabular and textual data. The first approach proposes a careful selection of the discretization method for numerical attributes in tabular datasets. The second one applies the notion of pertinent negatives to explanations on textual data. Our experimental evaluation shows the positive impact of our contributions on the quality of anchor-based explanations.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Explanations based on logical rules are a popular strategy to explain the logic of complex black-box machine learning (ML) classifiers. However, approximating a complex model with human-readable rules incurs an inevitable trade-off: Fidelity can only be achieved at the expense of complexity, and complex explanations miss the whole point of explainable ML. For this reason recent approaches, such as Anchors <ref type="bibr" target="#b3">[Ribeiro et al. 2018]</ref>, focus on explanations of local scope. These are if-then rules -also called anchors -that mimic the black box in the vicinity of a target instance. This strategy relies on the assumption that the black-box classifier is simpler to approximate when we focus on a particular region of the space.</p><p>While local rule-based explanations yield and simple and locally faithful explanations, their quality can still be very sensitive to Authors' addresses: Julien Delaunay, Inria/IRISA, France, julien.delaunay@irisa.fr; Luis Galárraga, Inria/IRISA, France, luis.galarraga@inria.fr; Christine Largouët, Agrocampus Ouest/IRISA, France, christine.largouet@irisa.fr.</p><p>Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. © 2018 Association for Computing Machinery. 0730-0301/2018/8-ART111 $15.00 https://doi.org/10.1145/1122445.1122456 some design factors. One of such factors is the discretization of the numerical attributes for tabular data. Figure <ref type="figure" target="#fig_0">1</ref> illustrates the anchors obtained for the same dataset with two discretization methods. When running Anchors with a suitable discretization method on the left-hand side of the figure, we obtain the anchor 𝑥 &gt; -5.78 ⇒ Red that matches the black box's behavior more faithfully than the anchor obtained by another discretization method on the righthand side. Another factor that can impact the quality of an anchor is the training set used to learn the explanation. Anchors <ref type="bibr" target="#b3">[Ribeiro et al. 2018]</ref> generates training samples by perturbing the instance of interest according to a neighborhood generation strategy. Figure <ref type="figure" target="#fig_1">2</ref> shows the average anchor length (number of conditions on the rule's antecedent) and precision across 10 instances of three explanations learned with different neighborhood generation methods. The strategy in dark blue (pertinent negatives explained later) provides the explanation with the best trade-off between rule length and precision.</p><p>In this work we study the impact of discretization and neighborhood generation on the different metrics that define the quality of anchor-based explanations. Our contributions focus on the tabular and text variants of Anchors and include (i) the application of MDLP <ref type="bibr" target="#b1">[Fayyad and Irani 1993]</ref> to discretize the numerical attributes on tabular data, and (ii) the definition of pertinent negatives on text classifiers. Before elaborating on our contributions, we provide a proper introduction to Anchors in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">ANCHORS</head><p>Consider a black-box classifier 𝑓 : X 𝑑 → Y that maps instances 𝑥 ∈ X 𝑑 to a set Y of classes. Each instance 𝑥 is a vector of 𝑑 attributes that can be either categorical or numerical, and 𝑥 [ 𝑗] denotes the value of 𝑥 for the 𝑗-th attribute. <ref type="bibr" target="#b3">[Ribeiro et al. 2018]</ref> defines an anchor as a logical rule 𝑅 that explains a black-box 𝑓 . The generated rule has the form:</p><formula xml:id="formula_0">𝑅 : B ⇒ 𝑓 (𝜂 -1 (𝑧)) = 𝑓 (𝑥) with B = 𝑗 ∈𝐹 ⊆ {1,...,𝑑 ′ } 𝑧 [ 𝑗]</formula><p>The left-hand side (or antecedent) of the rule is a conjunction of conditions that predicts 𝑓 (𝑥), i.e., the class of the target instance 𝑥 according to the black box. An example is the rule age ∈ [28, 37]∧ workclass="private" ⇒ "well-paid" (for simplicity we write only the predicted value on the right-hand side). For tabular data, the interpretable space can be obtained by discretizing the numerical variables -to turn them categorical -and then binarizing the resulting conditions. For text classifiers, the surrogate space is usually defined on the presence or absence of words.</p><p>The method proposed by <ref type="bibr" target="#b3">[Ribeiro et al. 2018</ref>], learns rule-based explanations from the answers of the black box 𝑓 on a randomly generated neighborhood Z ⊆ {0, 1} 𝑑 ′ constructed around 𝑧 = 𝜂 (𝑥) ∈ Z. Anchors applies principles of depth-first search and multi-armed bandit theory to output the shortest anchor with the largest coverage that satisfies the precision guarantee prec(𝑅) = 𝑃 (𝑓 (𝜂 -<ref type="foot" target="#foot_0">1</ref> (𝑧)) = 𝑓 (𝑥) | B ∧ 𝑧 ∈ Z) ≥ 𝜏 for a user-defined precision threshold 𝜏. The coverage of an anchor is the ratio of instances in Z that match the anchor's antecedent, i.e., cov(𝑅) = 𝑃 (B | 𝑧 ∈ Z).</p><p>The next two sections present our contributions that highlight some of the limitations of Anchors and propose improvements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">IMPACT OF DISCRETIZATION ON TABULAR DATA</head><p>The variant of Anchors for tabular data assumes we have access to the black box's training dataset D ⊆ X 𝑑 . Tabular Anchors uses D to discretize the numerical attributes properly according to the data distribution. It supports three discretization methods. Two of them, decile and quartile, are based on classical quantile discretization. In contrast, the entropy discretization method splits the domain of an attribute 𝑗 in a dataset D (denoted by D [ 𝑗]) so that the information entropy of 𝑓 (𝑥) -for 𝑥 ∈ D and black box 𝑓 -is minimized. Anchors' entropy-based discretization outperforms quantile-based discretization in terms of coverage, precision, and anchor length. However, as we show later in this section, it can still lead to relatively long anchors. On these grounds we investigate the performance of two new discretization methods. Its key heuristic lies in the selection of the "best" cut points.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">New Discretization Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Experimental Evaluation</head><p>We evaluate the quality of Anchors when used with five discretization methods. This includes the three methods already supported, i.e., quartile (Q), decile (D), entropy (E), and the methods proposed in this work, i.e., MDLP (M) and k-means (K). The precision threshold 𝜏 (Section 2) is set to the default value, that is, 𝜏 = 0.95.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Metrics.</head><p>The quality of an anchor is defined by its coverage, precision, and length. Shorter anchors with high coverage and precision are preferred. We highlight that the coverage of an anchor is almost analogous to the recall: It defines a trade-off with the precision. The only difference with respect to recall is that coverage is not necessarily bounded by 1. To account for this issue, we define the normalized coverage ncov(R) of an anchor 𝑅 as follows:</p><formula xml:id="formula_1">ncov(𝑅) = cov(𝑅) 𝑃 (𝑓 (𝜂 -1 (𝑧)) = 𝑓 (𝑥) | 𝑧 ∈ Z)</formula><p>That is, the standard coverage is now normalized by the maximal attainable coverage of an anchor that explains the class given by 𝑓 (𝑥). With this formulation we can define the F1 measure of an anchor as the harmonic mean of the precision and the normalized coverage. This score provides a trade-off between coverage and precision.</p><p>3.2.2 Datasets. We use three synthetic and two real datasets for our evaluation. The synthetic datasets were generated by drawing 10k instances with the functions make_blobs, make_moons, and make_circles available in scikit-learn 1 . The real datasets comprise (i) Titanic<ref type="foot" target="#foot_1">2</ref> , where the goal is to predict if a passenger of the Titanic survived based on her age, sex, class, etc., and (ii) Adult<ref type="foot" target="#foot_2">3</ref> where we aim at predicting if a person earns more than 50k USD also based on personal characteristics.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Support Vector Machines Logistic Regression Multilayer Perceptron</head><formula xml:id="formula_2">Random Forest K M D Q E K M D Q E K M D Q E K M D Q E Blobs 0.</formula><formula xml:id="formula_3">M D Q E K M D Q E K M D Q E K M D Q E Blobs 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Results</head><p>Table <ref type="table" target="#tab_0">1</ref> summarizes the F1 performance of Anchors for a set of instances 4 of each dataset for all the studied black-box models and discretization methods. The labels K, M, D, Q, and E denote k-means, MDLP, decile, quartile, and entropy respectively. We observe that MDLP achieves overall the best F1, followed by quartile and entropy.</p><p>In particular, MDLP and quartile split the domain of attributes into fewer intervals, leading to less specific conditions with potentially higher coverage. The use of the black-box labels for binning gives MDLP a significant advantage over a simple quartile discretization.</p><p>Besides, the focus on compression minimality makes MDLP output fewer intervals than the entropy strategy. Table <ref type="table" target="#tab_1">2</ref> confirms our intuitions as we observe that MDLP yields on average the shortest anchors. All discretization methods obtain a good performance on the highly structured dataset Blobs (depicted in Figure <ref type="figure" target="#fig_0">1</ref>). An example of an anchor using MDLP in the Adult dataset is age ≤ 22 ∧ relationship = "own-child" ⇒ &lt; 50kUSD.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">IMPROVING THE QUALITY OF ANCHORS ON TEXT</head><p>In some of our experiments with Anchors on text data, it was impossible to attain the default precision threshold 𝜏 = 0.95. This phenomenon makes Anchors output rules with a precision smaller than 𝜏. We argue that the maximal attainable precision of Anchors depends on (i) the distribution of the training neighborhood and the expressiveness of the rule language. In this section we study the performance of Anchors for two different neighborhood generation strategies and propose an extension of the rule language by considering negated conditions, known as pertinent negatives in the explainable AI literature.</p><p>4 10k for the synthetic datasets, 100 for Titanic and Adult.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Neighborhood Generation Strategies</head><p>The variant of Anchors for text classification converts a textual instance into a surrogate binary vector where each entry defines the absence or presence of a word of the target phrase. Consider, for example, a black-box classifier 𝑓 for sentiment analysis and the target instance "This is a good book". Anchors will convert this instance into a five-component vector, i.e., 11111, and generate neighbors by randomly toggling off bits of this binary representation. Examples are the instances 10101 or 11101. An anchor is induced from that set of neighbors and their class labels according to the black box 𝑓 . However, 𝑓 operates in a different space than Anchors. Hence, the inverse conversion function 𝜂 -1 must map the generated neighbors to actual text instances. The strategy called mask words does so by replacing the words of each zero component with a neutral wildcard unseen before by the black box. In our example the neighbor 11101 becomes "This is a W book" for wildcard W. The strategy called replace words, on the other hand, replaces toggled-off words with random words that have the same syntactic role, i.e., they would be assigned the same part-of-speech tag. For instance the neighbor 11101 could become "This is a great book".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Pertinent Negatives</head><p>We highlight that anchors are defined on conjunctions of nonnegated conditions. For text data this entails conditions on the presence of words in phrases. This design decision guarantees simpler rules while keeping the search space under control. On the downside, it imposes limits on the expressiveness of explanations.</p><p>Inspired by the work presented by <ref type="bibr" target="#b0">[Dhurandhar et al. 2018]</ref>, we propose to change the language of Anchors and provide explanations on the absence of words. Those words are known as pertinent negatives. We can also see pertinent negatives as counterfactual explanations or words whose presence would change the answer of the black box. Considering the absence of all possible words in the corpus makes the search space for anchors prohibitively large. Hence we apply two mechanisms to alleviate this fact. First, we focus on a limited set of words. This set consists of the top 𝑘 most frequent words that co-occur next to the words of the target instance, stopwords excluded. For our example phrase "This is a good book", our algorithm would consider words such as scientific, interesting, or very as they may often appear with "book" and "good". Second, we set an upper bound 𝑛 in the number of pertinent negatives allowed in explanations.</p><p>It follows that a neighborhood generation method purely based on pertinent negatives represents a phrase as a vector of 𝑚 + 𝑝 components where 𝑚 is the number of words in the target phrase and 𝑝 is the number of pertinent negatives. The target instance is mapped to a vector where the first 𝑚 elements are set to 1 and the remaining are set to 0. Neighbors are then generated by randomly toggling on the zero entries of the pertinent negatives, which instructs Anchors to add the word to the phrase. Our goal is to show the potential and viability of pertinent negatives in Anchors, thus we leave as future work the implementation of a hybrid approach that combines pertinent negatives with one of the classical strategies for neighborhood generation based on present words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Experimental Evaluation</head><p>We evaluate the discussed neighborhood generation strategies using the F1 measure and the anchor length as quality criteria. For pertinent negatives we use 𝑛 = 20. 4.3.1 Datasets. Our experimental datasets comprise (i) Polarity<ref type="foot" target="#foot_3">5</ref> , a set of movie reviews for sentiment analysis, and (ii) Tweets<ref type="foot" target="#foot_4">6</ref> , a set of tweets, where the goal is to predict the occurrence of emojis. 4.3.2 Black-box models. We use the same black-box models as in Section 3.2.3. Those models were trained on a vector representation of the phrases based on word counts and provided by the class CountVectorizer of scikit-learn.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Results</head><p>We summarize the aggregated results for the F1 measure and the anchor size among 10 randomly selected instances in Tables <ref type="table" target="#tab_2">3</ref> and<ref type="table" target="#tab_3">4</ref>. We first observe that the replace words strategy lies far behind the others in terms of F1 (except for logistic regression on Tweets). While it usually produces anchors of high precision, the coverage of those anchors is very low, in other words, it generates overly specific explanations. This intuition is confirmed by Figure <ref type="figure">3</ref>, where we can observe that replace words yields, on average, longer anchors than the other strategies. These overly specific anchors are a consequence of the neighborhood generation strategy. By replacing toggled-off words with other words of the same syntactic role, the neighbor instances become very unstable: the addition of a single word can change the meaning of the phrase as well as the black box's answer. We observe this phenomenon to a lesser extent when using the strategy mask words. This happens because replacing a word with the wildcard forces the black box to make a decision on the basis of fewer words. We observe that pertinent negatives achieves the best F1 score and leads to shorter and more stable anchors. This happens because the training set is based on the variation of a small and carefully selected set of features (words) that exhibit a high correlation with the features present in the target phrase. An example of an anchor with pertinent negatives is ¬caring ∧ downpur ⇒ for the tweet "Totally worth getting caught in this evening's downpour. #jacquelineonassisreservoir". The addition of the word "caring" would have made the classifier predict a different emoji.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>In this work we have studied the impact of two elements in the design of anchor-based explanations, namely the discretization of numerical features for tabular data, and the neighborhood generation strategy for text data. We have shown that by careful adjusting those elements, we can obtain a significant increase in the precision, coverage, and length of anchor-based explanations. As future work, we envision to explore post-hoc discretization methods, i.e., methods that rediscretize the variables of an anchor in order to improve its coverage. We will also consider the combination of the pertinent negatives and mask words strategies in order to provide more expressive and accurate anchors defined both on the presence or absence of words. The code, data, and experimental results are available at https://github.com/juliendelaunay35000/anchors.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Two anchors (depicted as green lines) learned with different discretizations of the numerical features. The target instance is marked as a violet star</figDesc><graphic coords="2,444.06,267.46,126.10,89.90" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Trade-off between F1 score and anchor length for three neighborhood generation methods</figDesc><graphic coords="3,65.81,83.69,216.23,121.52" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>3.1.1 K-means. We propose a baseline discretization method based on the k-means clustering algorithm [Jain and Dubes 1988]. This method splits the domain D [ 𝑗] of an attribute into 𝑘 clusters that minimize intra-cluster distance while maximizing inter-cluster distance. Distance is based on the absolute difference of the values in D [ 𝑗]. Therefore, and unlike the entropy-based method, our adaptation of k-means does not make use of the labels provided by the black box 𝑓 . The parameter 𝑘 is chosen using the Elbow method [Thorndike 1953]. 3.1.2 MDLP Discretization. [Fayyad and Irani 1993] proposes a method for discretization of continuous-valued attributes into multiple intervals based on the Minimum Description Length Principle (MDLP). Intuitively, MDLP returns the minimal number of "pure" intervals needed to separate instances from distinct classes. Compared to a traditional entropy-based method, MDLP focuses on compression minimality, hence it outputs as few intervals as possible.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>F1 score for Anchors using different discretization methods. MR denotes the mean rank of the method.</figDesc><table><row><cell></cell><cell>89</cell><cell>1</cell><cell>0.84</cell><cell>0.85</cell><cell>1</cell><cell>0.89</cell><cell>1</cell><cell>0.84</cell><cell>0.85</cell><cell>1</cell><cell>0.89</cell><cell>1</cell><cell>0.84</cell><cell>0.85</cell><cell>1</cell><cell>0.89</cell><cell>1</cell><cell>0.84</cell><cell>0.85</cell><cell>1</cell></row><row><cell>Circles</cell><cell cols="3">0.45 0.87 0.43</cell><cell>0.46</cell><cell>0.77</cell><cell cols="3">0.45 0.87 0.43</cell><cell>0.46</cell><cell>0.77</cell><cell cols="3">0.45 0.87 0.43</cell><cell>0.46</cell><cell>0.77</cell><cell cols="3">0.45 0.87 0.43</cell><cell>0.46</cell><cell>0.77</cell></row><row><cell>Moons</cell><cell>0.6</cell><cell>0.7</cell><cell cols="3">0.66 0.72 0.68</cell><cell>0.6</cell><cell>0.7</cell><cell cols="3">0.66 0.72 0.68</cell><cell>0.6</cell><cell>0.7</cell><cell cols="3">0.66 0.72 0.68</cell><cell>0.6</cell><cell>0.7</cell><cell cols="3">0.66 0.72 0.68</cell></row><row><cell>Adult</cell><cell>0.66</cell><cell>0.66</cell><cell cols="3">0.66 0.98 0.66</cell><cell>0.66</cell><cell>0.96</cell><cell cols="3">0.66 0.98 0.66</cell><cell>0.66</cell><cell>0.96</cell><cell cols="3">0.66 0.98 0.66</cell><cell>0.66</cell><cell>0.65</cell><cell cols="3">0.66 0.98 0.66</cell></row><row><cell cols="4">Titanic 0.42 0.93 0.33</cell><cell>0.42</cell><cell>0.42</cell><cell cols="3">0.42 0.93 0.33</cell><cell>0.42</cell><cell>0.42</cell><cell cols="3">0.42 0.93 0.33</cell><cell>0.42</cell><cell>0.42</cell><cell cols="3">0.42 0.93 0.33</cell><cell>0.42</cell><cell>0.42</cell></row><row><cell>MR</cell><cell>3</cell><cell>1.4</cell><cell>3.6</cell><cell>2</cell><cell>2</cell><cell>3.2</cell><cell>1.4</cell><cell>3.8</cell><cell>2</cell><cell>2.2</cell><cell>3.2</cell><cell>1.4</cell><cell>3.8</cell><cell>2</cell><cell>2.2</cell><cell>3</cell><cell>1.6</cell><cell>3.6</cell><cell>2</cell><cell>2</cell></row><row><cell></cell><cell></cell><cell cols="3">Support Vector Machines</cell><cell></cell><cell></cell><cell cols="3">Logistic Regression</cell><cell></cell><cell></cell><cell cols="3">Multilayer Perceptron</cell><cell></cell><cell></cell><cell cols="3">Random Forest</cell><cell></cell></row><row><cell></cell><cell>K</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Anchor length using different discretization methods. MR denotes the mean rank of the method.</figDesc><table><row><cell>1</cell><cell>1</cell><cell>1</cell></row></table><note><p>3.2.3 Black-box models. We tested our contributions on a variety of black-box classifiers, namely logistic regression, support vector machines, multi-layer perceptron, and random forests.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Length of textual Anchors for different neighborhood generation strategies.</figDesc><table><row><cell></cell><cell cols="3">Support Vector Machines</cell><cell cols="3">Logistic Regression</cell><cell cols="3">Multlayer Perceptron</cell><cell cols="3">Random Forest</cell></row><row><cell></cell><cell cols="2">RW MW</cell><cell>PN</cell><cell cols="9">RW MW PN RW MW PN RW MW PN</cell></row><row><cell>Tweets</cell><cell>5.1</cell><cell>4.3</cell><cell>8.1</cell><cell>4.6</cell><cell>6.4</cell><cell>9.1</cell><cell>2.1</cell><cell>4.6</cell><cell>7.2</cell><cell>3.7</cell><cell>5.4</cell><cell>4.2</cell></row><row><cell cols="2">Polarity 8.3</cell><cell>7.7</cell><cell>4.9</cell><cell>6.7</cell><cell cols="2">3.6 4.5</cell><cell>6.9</cell><cell cols="2">6.6 2.3</cell><cell>2</cell><cell>2</cell><cell>2</cell></row><row><cell></cell><cell cols="3">Support Vector Machines</cell><cell cols="3">Logistic Regression</cell><cell cols="3">Multilayer Perceptron</cell><cell cols="3">Random Forest</cell></row><row><cell></cell><cell cols="2">RW MW</cell><cell>PN</cell><cell cols="3">RW MW PN</cell><cell cols="3">RW MW PN</cell><cell cols="3">RW MW PN</cell></row><row><cell cols="3">Tweets 0.63 0.41</cell><cell>0.82</cell><cell cols="9">0.56 0.31 0.91 0.85 0.44 0.95 0.57 0.27 0.95</cell></row><row><cell cols="2">Polarity 0.35</cell><cell>1</cell><cell>0.87</cell><cell cols="3">0.35 0.98 0.86</cell><cell>0.52</cell><cell>1</cell><cell>0.98</cell><cell>0.47</cell><cell>1</cell><cell>0.79</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>F1 score of textual Anchors for different neighborhood generation strategies.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>https://scikit-learn.org</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>https://www.kaggle.com/c/titanic/data</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>https://archive.ics.uci.edu/ml/datasets/adult</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_3"><p>http://www.cs.cornell.edu/people/pabo/movie-review-data/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_4"><p>https://competitions.codalab.org/competitions/17344#learn_the_details-data</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Explanations based on the Missing: Towards Contrastive Explanations with Pertinent Negatives</title>
		<author>
			<persName><forename type="first">Amit</forename><surname>Dhurandhar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pin-Yu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ronny</forename><surname>Luss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chun-Chen</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pai-Shun</forename><surname>Ting</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karthikeyan</forename><surname>Shanmugam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Payel</forename><surname>Das</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Conference on Neural Information Processing Systems (NIPS</title>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
			<biblScope unit="page" from="590" to="601" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Multi-Interval Discretization of Continuous-Valued Attributes for Classification Learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Usama</surname></persName>
		</author>
		<author>
			<persName><surname>Fayyad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Keki</surname></persName>
		</author>
		<author>
			<persName><surname>Irani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th International Joint Conference on Artificial Intelligence (IJCAI-1993)</title>
		<meeting>the 13th International Joint Conference on Artificial Intelligence (IJCAI-1993)</meeting>
		<imprint>
			<date type="published" when="1993">1993</date>
			<biblScope unit="page" from="1022" to="1029" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Algorithms for Clustering Data</title>
		<author>
			<persName><forename type="first">K</forename><surname>Anil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><forename type="middle">C</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><surname>Dubes</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1988">1988</date>
			<publisher>Prentice-Hall</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Anchors: High-Precision Model-Agnostic Explanations</title>
		<author>
			<persName><forename type="first">Marco</forename><surname>Túlio Ribeiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sameer</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlos</forename><surname>Guestrin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence</title>
		<meeting>the Thirty-Second AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1527" to="1535" />
		</imprint>
	</monogr>
	<note>AAAI-18</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Who belongs in the family?</title>
		<author>
			<persName><forename type="first">Robert</forename><forename type="middle">L</forename><surname>Thorndike</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychometrika</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="267" to="276" />
			<date type="published" when="1953-12">1953. Dec 1953</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
