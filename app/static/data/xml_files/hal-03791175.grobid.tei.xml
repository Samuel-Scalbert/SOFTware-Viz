<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Fact-checking Multidimensional Statistic Claims in French</title>
				<funder ref="#_7DrduEg">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Oana</forename><surname>Balalau</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Inria</orgName>
								<orgName type="institution" key="instit2">Institut Polytechnique de Paris</orgName>
								<address>
									<addrLine>1 rue Estienne Honoré d&apos;Orves</addrLine>
									<postCode>91120</postCode>
									<settlement>Palaiseau</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Simon</forename><surname>Ebel</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Inria</orgName>
								<orgName type="institution" key="instit2">Institut Polytechnique de Paris</orgName>
								<address>
									<addrLine>1 rue Estienne Honoré d&apos;Orves</addrLine>
									<postCode>91120</postCode>
									<settlement>Palaiseau</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Théo</forename><surname>Galizzi</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Inria</orgName>
								<orgName type="institution" key="instit2">Institut Polytechnique de Paris</orgName>
								<address>
									<addrLine>1 rue Estienne Honoré d&apos;Orves</addrLine>
									<postCode>91120</postCode>
									<settlement>Palaiseau</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ioana</forename><surname>Manolescu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Inria</orgName>
								<orgName type="institution" key="instit2">Institut Polytechnique de Paris</orgName>
								<address>
									<addrLine>1 rue Estienne Honoré d&apos;Orves</addrLine>
									<postCode>91120</postCode>
									<settlement>Palaiseau</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Quentin</forename><surname>Massonnat</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Inria</orgName>
								<orgName type="institution" key="instit2">Institut Polytechnique de Paris</orgName>
								<address>
									<addrLine>1 rue Estienne Honoré d&apos;Orves</addrLine>
									<postCode>91120</postCode>
									<settlement>Palaiseau</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Antoine</forename><surname>Deiana</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">FranceInfo</orgName>
								<address>
									<addrLine>116 Av. du Président Kennedy</addrLine>
									<postCode>75016</postCode>
									<settlement>Paris</settlement>
									<country>Radio France, France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Emilie</forename><surname>Gautreau</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">FranceInfo</orgName>
								<address>
									<addrLine>116 Av. du Président Kennedy</addrLine>
									<postCode>75016</postCode>
									<settlement>Paris</settlement>
									<country>Radio France, France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Antoine</forename><surname>Krempf</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">FranceInfo</orgName>
								<address>
									<addrLine>116 Av. du Président Kennedy</addrLine>
									<postCode>75016</postCode>
									<settlement>Paris</settlement>
									<country>Radio France, France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Thomas</forename><surname>Pontillon</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">FranceInfo</orgName>
								<address>
									<addrLine>116 Av. du Président Kennedy</addrLine>
									<postCode>75016</postCode>
									<settlement>Paris</settlement>
									<country>Radio France, France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Gérald</forename><surname>Roux</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">FranceInfo</orgName>
								<address>
									<addrLine>116 Av. du Président Kennedy</addrLine>
									<postCode>75016</postCode>
									<settlement>Paris</settlement>
									<country>Radio France, France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Joanna</forename><surname>Yakin</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">FranceInfo</orgName>
								<address>
									<addrLine>116 Av. du Président Kennedy</addrLine>
									<postCode>75016</postCode>
									<settlement>Paris</settlement>
									<country>Radio France, France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Fact-checking Multidimensional Statistic Claims in French</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">4145F0F3CA52F9DC602BC17E34FC09CA</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-04-12T14:51+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>To strengthen public trust and counter disinformation, computational fact-checking, leveraging digital data sources, attracts interest from the journalists and the computer science community. A particular class of interesting data sources comprises statistics, that is, numerical data compiled mostly by governments, administrations, and international organizations. Statistics are often multidimensional datasets, where multiple dimensions characterize one value, and the dimensions may be organized in hierarchies. This paper describes STATCHECK, a statistic fact-checking system jointly developed by the authors, which are either computer science researchers or fact-checking journalists working for a French-language media with a daily audience of more than 15 millions (aud,  2022). The technical novelty of STATCHECK is twofold: (i) we focus on multidimensional, complex-structure statistics, which have received little attention so far, despite their practical importance; and (ii) novel statistical claim extraction modules for French, an area where few resources exist. We validate the efficiency and quality of our system on large statistic datasets (hundreds of millions of facts), including the complete INSEE (French) and Eurostat (European Union) datasets, as well as French presidential election debates.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Professional journalism work has always involved verifying information with the help of trusted sources. In recent years, the proliferation of media in which public figures make statements, in particular online, has led to an explosion in the amount of content that may need to be verified to distinguish accurate from inaccurate, and even potentially dangerous, information.</p><p>To help journalists deal with the deluge of information, computational fact-checking <ref type="bibr" target="#b6">(Cazalens et al., 2018;</ref><ref type="bibr" target="#b18">Nakov et al., 2021)</ref> emerges as a growing, multidisciplinary field. The main tasks of a fact-checking system are: identifying the claims made in an input document, finding the relevant evidence from a reference corpus, and (optionally) producing an automated verdict (is the claim true or false?). A reference corpus can be a knowledge graph <ref type="bibr" target="#b8">(Ciampaglia et al., 2015)</ref>, Web sources such as Wikipedia <ref type="bibr" target="#b19">(Nie et al., 2019;</ref><ref type="bibr" target="#b26">Yoneda et al., 2018)</ref>, or relational tables <ref type="bibr" target="#b7">(Chen et al., 2020;</ref><ref type="bibr" target="#b12">Herzig et al., 2020;</ref><ref type="bibr" target="#b15">Jo et al., 2019;</ref><ref type="bibr" target="#b17">Karagiannis et al., 2020)</ref>.</p><p>For fact-checks to be convincing, professional journalists prefer reference sources of high quality, carefully built by specialists. These include statistics produced and shared by governmental and international organizations, such as INSEE, the French national statistics institute<ref type="foot" target="#foot_0">1</ref> and Eurostat, the equivalent European Union office<ref type="foot" target="#foot_1">2</ref> . Technically speaking, such statistics are multidimensional tables, where a fact is a number, characterized by one or more a dimensions, such as a geographical unit, time interval, and other categories such as "Education level", etc. Unfortunately, such data sources are significantly more complex than relational tables, making their usage challenging. Consequently, despite the interest in such sources, only a few works have used them for automatic fact-checking <ref type="bibr" target="#b5">(Cao et al., 2018;</ref><ref type="bibr" target="#b11">Duc Cao et al., 2019)</ref>.</p><p>In our collaboration between computer scientists and fact-checking journalists, we have developed, deployed, and continue to be extending STATCHECK, a fact-checking system specialized in the French media arena. STATCHECK builds upon the open-source code base of <ref type="bibr" target="#b5">(Cao et al., 2018;</ref><ref type="bibr" target="#b11">Duc Cao et al., 2019)</ref>. We significantly improved its data ingestion speed and more than doubled its statistic corpus by adding Eurostat data. Different from <ref type="bibr" target="#b7">(Chen et al., 2020;</ref><ref type="bibr" target="#b12">Herzig et al., 2020;</ref><ref type="bibr" target="#b15">Jo et al., 2019;</ref><ref type="bibr" target="#b17">Karagiannis et al., 2020;</ref><ref type="bibr" target="#b8">Ciampaglia et al., 2015;</ref><ref type="bibr" target="#b19">Nie et al., 2019;</ref><ref type="bibr" target="#b26">Yoneda et al., 2018;</ref><ref type="bibr" target="#b2">Aly et al., 2021)</ref>, STATCHECK also includes a claim detection step, which saves journalists' time by focusing their attention on the claims worth checking; our claim detection module significantly outperforms the only one we know of for French <ref type="bibr" target="#b11">(Duc Cao et al., 2019)</ref>.</p><p>Outline. Below, we start by presenting a set of functional requirements derived from the journalist authors' experience in Section 2. Next, we describe the actual organization of statistic databases, and the STATCHECK architecture, in Section 3. Then, we explain how this architecture is instantiated over two different sources, INSEE and Eurostat, whose size and organization significantly vary, in Section 4; we ingest and index all the data to support efficient search over it (Section 5). Finally, our claim detection modules are described in Section 6, then we conclude.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Fact-Checking Work Routine and Requirements</head><p>The journalist authors are part of the same team, specializing in fact-finding and fact-checking in a French-speaking national media. The material they author is disseminated through both the native and online media channels of their news organization. Their work is split among the two main classes identified in <ref type="bibr" target="#b16">(Juneja and Mitra, 2022</ref>): short-term claim centric, focusing on the veracity of statements made continuously by public figures, which need to be checked relatively quickly; and long-term issue-centric, whereas individual journalists maintain and increase their knowledge of application topics, such as "law enforcement", "education and research", "defense", etc. The short-term, claim-centric work raises several requirements. First, journalists know whose claims might interest their audience. Thus, they need an interesting subset (selection) of social media content to be made available through a Web platform. Journalists specify a set of social media account handles (currently Twitter and Facebook), and need the ability to modify this set themselves, as people gravitate in and out of the public's atten-tion. Second, whenever claims about statistic entities are made in this social media content sphere, bringing these claims to their attention, isolating them from the mass of social communication of the figures they follow, saves journalists time and effort. Third, as previously noted in <ref type="bibr" target="#b6">(Cazalens et al., 2018;</ref><ref type="bibr" target="#b22">Saeed and Papotti, 2021)</ref>, data sources relevant to a given claim must be quickly identified and as precisely as possible. This again saves journalists time to search statistic data sources that may be very large, i.e., Eurostat publishes thousands of datasets, some with millions of rows.</p><p>The long-term, issue-centric work also benefits from these functionalities, yet it is more open; journalists may peruse claims for which they have not identified relevant sources yet, but still appreciate a recommendation of most likely checkworthy claims. User-friendly means to filter messages considered check-worthy (Should messages about the future, such as electoral promises, be considered, or not? Is a number required in a statistical claim, or not?) are also appreciated.</p><p>Common to both kinds of work, the newsroom involved in this project has the core tenet that any verdict or judgment must be vetted by journalists, since publishing it engages their professional responsibility. This has a set of consequences. (i) Journalists need to analyze the facts relevant to a claim and interpret them in a nuanced way for their audience. For instance, a difference of 5% between a number stated in a claim, and the value in a reference source, may be negligible or, on the contrary, a serious attempt to mislead, depending on the context. Thus, unlike prior systems <ref type="bibr" target="#b7">(Chen et al., 2020;</ref><ref type="bibr" target="#b12">Herzig et al., 2020;</ref><ref type="bibr" target="#b15">Jo et al., 2019;</ref><ref type="bibr" target="#b17">Karagiannis et al., 2020)</ref>, STATCHECK does not compute a "true/false" verdict, leaving this tasks to journalists. (ii) For transparency and trust, links to any fact on its original publishing site must be provided together in the fact-check.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Fact-checking Based on Multidimensional Statistics</head><p>A multidimensional dataset consists of a set of facts, each having one value along a set of dimensions. For instance, Figure <ref type="figure" target="#fig_0">1</ref> (top) represents a three-dimensional dataset: French departments are on the horizontal axis, education levels on the vertical axis, while years are on the third (depth) axis. In each cell, the dataset stores the number of students in the respective department, level of study, and year. In practice, actual Open Data statistics published by the government or international organizations are typically much more complex, as shown at the bottom of Figure <ref type="figure" target="#fig_0">1</ref>. First, to save space, dimension values may be encoded into short codes, e.g., "HI" for "High school", "MI" for "Middle school", etc.; a decoding dictionary, associating a human-understandable term to each code, is published with, or close to the data cells. Although not shown in the figure, dimension names are similarly encoded. Second, header cells, shown in yellow and green in the figure, may be mixed with data cells; this requires effort to interpret them correctly. Note also that there can be a hierarchy of headers, e.g., a dataset at the granularity of departments may also include region names, e.g., " Île-de-France" and "Grand Est", placed in the data files above, or close to, the region header cells. Third, datasets may also contain partially aggregated results, illustrated by the orange box holding the sum of all facts for one region (Grand Est), one education level (elementary), and the three years. Fourth, for each dataset, there may exist a separate, textual description, which contains a title, e.g., "French student population", and other comments. Data representation in files. In practice, a multidimensional statistic dataset is published as a file, which can be CSV, a spreadsheet etc. For that, it is laid out in a bidimensional format, with some facts on each line, and as many lines as needed. If the data has more than two dimensions, which is often the case, this leads to row header cells encod-ing several dimensions and their values, such as "HI 2019", "MI 2019" etc. in the figure. The file may start with the column headers (yellow), then the encoded multidimensional row header cell "EL 2019" followed by the four cells corresponding to it, then a similar line for "MI 2019", a line for "HI 2019", followed by similar lines for 2020, then 2021 etc. Partially aggregated results are interspersed between such lines. Challenges. To exploit such datasets for factchecking, a set of challenges must be addressed. The useful information, e.g., "How many elementary school students were in Île-de-France region in 2019?", is a number in a cell. To find such information, we must identify and store its relationships with human-understandable descriptions of its dimensions, such as "Education level: Elementary school". In this example, the question is asked at a granularity (region) that is more coarse than the granularity of the data. To find the answer, we must exploit the fact that Paris and Essonne are departments in the Île-de-France region. Further, statistic claims may use similar but different language, e.g., a claim may be made about "pupils in Île-de-France". Linguistic knowledge must be leveraged to connect the claim terminology with that of the dataset. As mentioned in our requirements (Section 2), fine-granularity answers are preferred, that is: if the answer consists of one or a few cells only, those should be extracted from the dataset and returned, to avoid journalists' efforts to search in potentially large files. Finally, speed at scale is important, to enable journalists to work efficiently. Architecture. To address these challenges, based on the requirements described in Section 2, we have devised an architecture shown in Figure <ref type="figure" target="#fig_1">2</ref>.</p><p>The modules in the lower row acquire and process reference datasets (Section 4), e.g., statistics about education in France. Those in the upper row acquire content to be fact-checked, e.g., a tweet stating: "More teachers are needed to educate 200K pupils in Île-de-France!", extract claims (Section 6), in this case "200K pupils in Îlede-France", and identify the most relevant facts for checking these claims, by searching the appropriately indexed reference datasets (Section 5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Statistic Fact Database and Storage</head><p>By crawling, we acquired the complete INSEE and Eurostat statistics, and store them as follows. INSEE publishes each statistic report as an HTML page containing a description (title and comments on the data), and statistic tables in Excel or in HTML. As of May 2022, there are 60,002 Excel files (each of which may contain several tables) and 58,849 HTML tables. The table organization varies significantly across the datasets; nested headers are frequent. The largest table has 50.885 lines. Following <ref type="bibr" target="#b4">(Cao et al., 2017)</ref>, to capture all the elements of an INSEE dataset, we turn it in an RDF graph (www-rdf), where each data cell, header cell, and partial aggregate becomes an RDF node (URI). Each data cell or partial aggregate node is connected, through an RDF triple, to the cells corresponding to its closest header cells. Thus, the number of elementary school students in Paris in 2019 is connected to header cells labeled "Paris", respectively, "Elementary school 2019" (where "EL 2019" was decoded using the dictionary). Finally, each header cell is connected through an RDF triple to its parent header cell. This allows us to easily find out that the elementary school students in Paris in 2019 are also to be counted as being in the Île-de-France region. We also create an RDF node per dataset, which is connected to all its header cells and to the textual title and comments (each modeled as an RDF literal). The INSEE corpus lead to 7,362,538,629 RDF triples, including 22,366,376 header cells. We store them in the Jena Fuseki server with the TDB2 persistent back-end (www-tdb2).</p><p>Eurostat publishes 6,803 statistic tables, ranging from 2 lines to 37 million lines, and 580 dictionaries that, together, decode 243,083 statistical concepts codes into natural-language descriptions, all of which we acquired in STATCHECK' database. Together, the Eurostat data files total 414.908.786 lines. In Eurostat, dimension hierarchies are described in the dictionaries; we store these in memory. The statistic tables are simplestructure TSV files, thus, storing each of them as a table in a relational database was an option. However, their number is relatively high, and storing a file in a database inevitably increases its storage footprint. Therefore, to keep the data more compact, in view also of future extensions of our platform with more statistics from the World Health Organization, World Development Index etc., we store them as plain files, complemented by specialized indexes, as we explain below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Statistic Search</head><p>Given a keyword query Q = {k 1 , k 2 , . . . , k n }, such as "middle school pupils in Île-de-France in 2020", the task we consider here is to find:</p><p>• the most relevant facts from our complete IN-SEE and Eurostat corpus;</p><p>• or, if a concrete fact is not found, but some datasets as a whole appear related to the query, return those datasets.</p><p>There may be several fact-(or cell-) level as well as dataset-level answers; we return a ranked list based on their relevance.</p><p>We call metadata of a statistic dataset all the natural-language elements that are part of or associated with the dataset: its title, comments, and human-understandable versions of all its header values. We use L = {T, C, H} to denote the set of the locations in which a term can appear in metadata, respectively: the dataset title, a comment, or a header. The locations are important since a term appearing in a title is more significant than one appearing in a header, and we exploit this when retrieving the datasets most relevant for a query (Section 5.1). Also, locations help determine whether a dataset matches some keywords headers of different dimensions -in which case the cell(s) at the intersection of those dimensions likely have a very relevant result (Section 5.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Dataset Indexing and Search</head><p>We split the metadata of each dataset d into a set of tokens T = {t 1 , . . . , t N }, and remove stop words. For each token t, we identify based on a Word2Vec model the 50 tokens t closest semantically to t. Next, for each appearance of a token t in a location l within d, our term-location index I T L stores: (i) the index entry (t, d, l) corresponding to the token actually found in d; and (ii) 50 entries of the form (t , d, l, dist), for the 50 tokens closest to t. These extra entries enable answering queries on terms close to (but disjoint from) the dataset content. For instance, when t is "school", t could be "teacher", "pupil", "student", etc. For fast access, I T L is hosted in the Redis in-memory key-value store (www-redis). To find the datasets relevant for the query Q, we look up the query keywords in I T L , and consider relevant any dataset associated with at least one keyword.</p><p>The above indexing mechanism leverages word distances. Separately, we used geographic resources, in particular (Eurostat, 2022) for EU locations, to make our system aware of the relationships between geographic units (cities, departments, regions) across Europe. This ensures that a dataset is considered relevant if it mentions a geographic unit that includes or is included in the query. It is important to identify geographic names in the metadata. We have adopted the FlashText algorithm <ref type="bibr" target="#b23">(Singh, 2017)</ref>, capable of finding, in a dataset metadata of size N , one of M fixed keywords in O(N ) time complexity. This is much faster than the O(N M ) cost of regular expression pattern matching used in the previous system <ref type="bibr" target="#b5">(Cao et al., 2018)</ref> and significantly sped up indexing of the INSEE corpus 3 .</p><p>Coarser-grain indexing of Eurostat statistics The large size of this corpus prevents cell-or rowlevel metadata indexing, as the index might outgrow the memory. Instead, we index occurrences of statistical concept codes in datasets, as follows. Let c be a Eurostat concept, e.g., "EL", appearing in dataset d at a location l ∈ L, and d c be the decoding of c, e.g., "Elementary school" for "EL". Let T dc = {t 1 , t 2 , . . . , t N } be the tokens in d c , and for 1 ≤ i ≤ N , let t j i , for 1 ≤ j ≤ 50, be the tokens closest to t i . For each t i ∈ T dc , we insert in the term-dataset index I T , also stored in Redis:</p><p>3 Together with other optimizations related to batching calls to the Spacy tokenizer and pipelining the indexing with the data acquisition process, this brought the total INSEE indexing time from 29 hours to 4 minutes.</p><p>• a (t i , d, l) entry;</p><p>• for every t j i similar to t i , an entry (t j i , d, l, dist, t i ), where dist is the distance between t i and t j i .</p><p>Indexing the complete Eurostat data in this way took around 4 minutes. Given the query Q = {k 1 , . . . , k n }, we search I CL and I T for entries of the form (k i , d, l) or (k i , d, l, dist, k i ). Any dataset having an entry for at least one k i is potentially interesting; we retain the 20 highest-score ones.</p><p>Dataset ranking We rank datasets based on the relevance score introduced in <ref type="bibr" target="#b5">(Cao et al., 2018)</ref>. It is a weighted combination of the word distances between the query keywords and the datasets' metadata; the weights reflect the locations where relevant terms appear in each dataset. We have also experimented with the classic BM25 (Robertson and Zaragoza, 2009) computed over all the datasets' metadata, but the results were less good, in particular, because BM25 does not handle synonyms well. We also considered embedding the query and the metadata using Sentence-Bert <ref type="bibr" target="#b20">(Reimers and Gurevych, 2019)</ref> and comparing these with the query embedding, but opted not to use it because, for our purposes, the term location in the metadata is important, and treating the metadata as a single text loses this information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Data Cell Indexing and Search</head><p>Our next task is to extract results at the finest granularity level possible. Let d be one of the most interesting datasets, and I(d) be the set of all index entries for the query Q and d. For our sample query Q and dataset in Figure <ref type="figure" target="#fig_0">1</ref>, I(d) contains:</p><p>• For "middle school", header (H) entries for "Middle school" (exact), as well as for "High school" and "Elementary school" (similar); a title (T ) entry for "student" (similar); and a comment (C) entry for "school" (similar);</p><p>• For "pupils", H, T , and C entries for the similar words above;</p><p>• For " Île-de-France", an exact H entry, and two similar H entries for "Paris" and "Essonne";</p><p>• For "2020", exact H entries.</p><p>If I(d) only features title (T ) or comment (C) locations, then d is pertinent as a whole, and we do not search for cell-level answers.</p><p>On the contrary, if I(d) has several header entries (having l = H), matching two or more distinct query keywords (or close terms), this means that d holds some fine-granularity results for the query. If I(d) holds an entry along each dataset dimension d, these entries, together, designate exactly one cell, which we should return. Otherwise, the result is a collection of all the cells from d characterized by the dimension values designated by the entries in I(d).</p><p>In our example, we should return the cells for "MI 2019", "2020", and locations "Paris" and "Essonne", which belong to Île-de-France. For that:</p><p>1. If d is an INSEE dataset, I(d) contains the headers of the respective row and column headers. Then, the cell is identified by asking a SPARQL (W3C, 2013) query, evaluated by Fuseki, as in <ref type="bibr" target="#b5">(Cao et al., 2018)</ref>. The query requests "all the data cells from dataset d whose closest header cells are those from I(d)".</p><p>2. If d is an Eurostat dataset, I(d) only specifies that "some row (column) headers match", and more effort is needed to identify the relevant cells. A Eurostat file has at most a few dozen columns, but it may have tens of millions of rows.</p><p>• To find the column referred to by an I(d) entry whose key is k, we search for k in the first (header) line of d. • To identify the relevant rows efficiently, we created another index I R on the Eurostat data files, inspired by the Adaptive Positional Map of <ref type="bibr" target="#b1">(Alagiannis et al., 2015)</ref>. I R stores the positions, in the data file of d, of the rows containing a certain keyword k in their header. We store I R directly as a binary file on disk. • Knowing the rows and column indexes, we read those row(s) from d, and extract from them the relevant data cell(s).</p><p>Using Fuseki, cell extraction takes 35ms up to 2.86s. On Eurostat, using I R , we record 4.76µs up to 2.66s. The lower bound is higher for INSEE because we have to pass SPARQL queries across a connection to the Fuseki server.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Claim Detection</head><p>A claim is a statement to be validated, that is, we aim to establish if it is true or false. The validation is achieved by finding related statements, called evidence, which back up or disprove the claim. In our work, the claims are detected in an input text, while the evidence is retrieved from a set of trusted sources, our reference datasets. Our platform detects claims from text stored in .txt, .odt, .docx or .pdf files, and from the Twitter and Facebook posts of public figures. Our platform regularly retrieves the most recent updates of a predefined group of users for posts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Statistical Claim Detection</head><p>Previous work addresses statistical claim detection in a supervised manner by predicting statistical entity-value pair from text patterns <ref type="bibr" target="#b25">(Vlachos and Riedel, 2015)</ref>. In <ref type="bibr" target="#b11">(Duc Cao et al., 2019)</ref>, the authors introduced a statistical claim detection method that given an input set of statistical entities, e.g. chômage, coefficient budgétaire) and a sentence, it retrieves all the statistical statements of the form statistical entity, numerical value, and unit, date present in the sentence. The statistical statement, if present, represents the statistical claim to be verified. The statistical entities and units are retrieved using exact string matching, while the date is extracted using HeidelTime <ref type="bibr" target="#b24">(Strötgen and Gertz, 2010)</ref>, a time expression parser. If the parser finds no date, the posting timestamp is used. More context about the claim to be verified is found using a Named Entity Recognition (NER) model, which returns organizations and locations. We note, however, that the organization and location are optional, while a statistical statement is not complete without one of its three elements. The initial statistical entity list is constructed from the reference datasets by taking groups of tokens from the headers of tables, we refer to <ref type="bibr" target="#b11">(Duc Cao et al., 2019)</ref> for more details.</p><p>We improved this method to optimize both its speed and the quality of extractions. We refer to the two methods as OriginalStatClaim <ref type="bibr" target="#b11">(Duc Cao et al., 2019)</ref> and StatClaim. We first performed a more careful match between the tokens of a sentence and our input statistical entities. Using the syntactic tree of the sentence and a lemmatizer, statistical entities are matched using their lemma and are extended to contain the entire nominal group of the matched token. Numerical val-ues are associated with units using both lemmas matching from our set of units and syntactic analysis. Units can be a noun following a numerical value or a nominal group containing one or more units. (e.g. "millions d'euros"). As in the original approach, if we retrieve a statistical statement of the form statistical entity, numerical value, and unit, date , we have found a claim to verify. In the default setting of our algorithm, a claim should contain all three elements. In addition, we filter out claims from sentences whose verb is in the future tense or the first person since these are promises about the future and not verifiable. Journalists found, however, that these may also be interesting for their long-term, issuecentric work <ref type="bibr" target="#b16">(Juneja and Mitra, 2022)</ref>. Thus, STATCHECK allows them to turn the future and first-person filters on and off.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Check-worthy Claim Detection</head><p>To complement the statistical claim detection model, we developed a model that is not conditioned on a set of initial statistical entities. The model classifies a sentence as check-worthy or not, where check-worthiness is defined as sentences containing factual claims that the general public will be interested in learning about their veracity <ref type="bibr" target="#b3">(Arslan et al., 2020)</ref>. We leveraged the ClaimBuster dataset (Arslan et al., 2020), containing check-worthy claims in English from the U.S. Presidential debates, to train a crosslingual language model, XLM-R <ref type="bibr" target="#b9">(Conneau et al., 2019)</ref>, which can perform zero-shot classification on French sentences after training on English data.</p><p>The ClaimBuster dataset ClaimBuster is a crowd-sourced dataset where the sentences from the 15 U.S. presidential elections debates from 1960 to 2016 have been annotated. The labels are Non-Factual Sentences (NFS), Unimportant Factual Sentences (UFS) or Check-Worthy Factual Sentences (CFS). The dataset contains 23K sentences, and the authors produced a subset of higher quality of 11K sentences for training models on classification tasks. In this smaller dataset, the NFS and UFS labels are grouped as negative labels, and the CFS labels are considered positive. We chose this higher-quality dataset to fine-tune the XLM-R model. 2.5TB of Common Crawl data. It achieves stateof-the-art results on multilingual tasks such as the XNLI benchmark <ref type="bibr" target="#b10">(Conneau et al., 2018)</ref>, while remaining competitive on monolingual tasks. We used a pretrained model with a vocabulary size of 250K, 12 hidden layers of size 768 and 12 attention heads. We used a weighted cross-entropy loss to account for the unbalanced ratio of labels. The dataset was split into train, dev and test datasets with a ratio of 80%/%10%/10%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fine-tuning the XLM-R model</head><p>Evaluation To optimize the performance, we trained the model with different hyperparameters. The best results were obtained with a learning rate of 5 • 10 -5 , a batch size of 64, and using the AdamW optimizer. To evaluate the performance of the different models on French data, we annotated 200 randomly sampled French tweets and labeled them as check-worthy or not following the definition in <ref type="bibr" target="#b3">(Arslan et al., 2020)</ref>. Two annotators labeled each tweet; in the golden standard, a tweet is deemed check-worthy if both annotators agree on it, and not check-worthy otherwise. The Cohen Kappa score for inter-annotator agreement is 0.6, which is considered moderate to substantial agreement. The results can be found in Table 1. The performance on this test set is encouraging, however lower than on the original English dataset. This is expected given the zero-shot setting, as the tweets' format and vocabulary might differ from the ones in the training dataset. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Integration and Evaluation of the Claim Detection Models</head><p>We evaluate the claim detection models, (Origi-nalStatClaim <ref type="bibr" target="#b11">(Duc Cao et al., 2019)</ref>, StatClaim and CheckWorthyClaim), on a set of 1595 tweets. Each tweet was labeled with two classes: "Verifiable numerical claim" (True if the tweet contains at least one numerical and verifiable claim") and "INSEE statistical claim" (True if the tweet contains at least one numerical, statistical claim verifiable against the INSEE dataset"). We chose these two labels as the first one gives us an indication of the tweets that can be verified if we had unlimited access to resources, while the second class identifies the tweets verifiable in the setting in which we have access to only one resource. We gathered 1595 random tweets from our scraped dataset to construct our set. Then, we automatically detected if a tweet contained a numerical value, if not, the tweet was labeled as negative for both classes. After that first step, we manually labeled the remaining 101 tweets. Two annotators labeled each tweet, and the gold standard was chosen as True if both annotators agreed. For the class "verifiable numerical claim", we obtained a Kappa inter-Annotator Agreement score of 0.917 (almost perfect agreement), and 59 tweets were labeled as positive. For the class "INSEE statistical claim" we obtained an inter-annotator Agreement score of 0.807 (substantial agreement) and 16 tweets were labeled as positive.</p><p>Evaluation procedure For StatClaim and Orig-inalStatClaim, a tweet is considered positive if models return at least one extracted statistical statement. Our StatClaim was used in its default configuration: extractions with numerical values and without verbs conjugated in the future or in the first person. For CheckWorthyClaim, a tweet is considered positive if the model returns a checkworthy score &gt; 0.9. We report the results in Table 2 and Table <ref type="table" target="#tab_1">3</ref>. StatClaim performs better than the original at detecting INSEE verifiable claims, and CheckWorthyClaim vastly outperforms both models on the detection of numerical claims, as they are a subset of check-worthy sentences that the model was trained to detect. Finally, we evaluate the performance of our model directly against the journalist authors' prior manual work. For example, during the 2022 French presidential debate, the journalist team highlighted 29 of the 1954 uttered sentences and fact-checked them. The XLM-R model, on the other hand, classifies 443 of these sentences as check-worthy, and 27 of the 29 sentences chosen by the journalists are correctly classified. In other words, our model reduces by 77% the number of sentences to consider while retaining 93% of the sentences the journalists actually want to factcheck, saving the journalists considerable time without them missing too many important claims.</p><p>Default claim detection strategy. By default, STATCHECK uses StatClaim for statistical claim detection. However, given the good performance of CheckWorthyClaim on numerical claims, we allow users to switch to it, even if we might not be able to verify them against the reference datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion and Perspectives</head><p>Fact-checking journalists need automated tools to help scale up their daily work. We developed the STATCHECK tool, which allows the journalist authors to focus their attention directly on check-worthy statements falling into one of two overlapping classes: those that can be checked based on statistics from two major institutions; and those that human users find interesting, even if the data to back up the checks is not present in the database. STATCHECK is in daily use in the factchecking team; Figure <ref type="figure" target="#fig_2">3</ref> illustrates its GUI.</p><p>Quantitative question answering based on open data is gaining interest <ref type="bibr" target="#b13">(Ho et al., 2020</ref><ref type="bibr" target="#b14">(Ho et al., , 2022))</ref>. In our continuing collaboration, we will work to extend STATCHECK with more multidimensional statistic datasets from national governments and international organizations.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Multidimensional Datasets Example</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Multidimensional statistic data: conceptual view (top), structure of actual published dataset (bottom).</figDesc><graphic coords="4,87.01,150.26,185.53,111.82" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: STATCHECK architecture overview.</figDesc><graphic coords="5,138.67,62.81,317.47,108.46" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Screen captures of STATCHECK' GUI. Top: statistic search interface with sample query result (data cell with row header in blue and column header in red); bottom: tweet analysis interface.</figDesc><graphic coords="9,93.32,150.79,408.19,57.03" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Example of Insee dataset</figDesc><graphic coords="12,72.00,210.52,218.25,214.84" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Example of Eurostat dataset</figDesc><graphic coords="12,307.28,447.36,218.27,203.32" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Evaluation of the fine-tuned XLM-R model.</figDesc><table><row><cell>The XLM-R</cell></row><row><cell>model is a Transformer-based masked language</cell></row><row><cell>model trained on one hundred languages with</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 3 :</head><label>3</label><figDesc>Model evaluation on INSEE statistical claims.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>https://www.insee.fr</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>https://https://ec.europa.eu/eurostat</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgements This work is partially funded by <rs type="projectName">AI Chair SourcesSay</rs> project (<rs type="grantNumber">ANR-20-CHIA-0015-01</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_7DrduEg">
					<idno type="grant-number">ANR-20-CHIA-0015-01</idno>
					<orgName type="project" subtype="full">AI Chair SourcesSay</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m">Media audience survey</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note>anonymized for double-blind reviewing</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Nodb: efficient query execution on raw data files</title>
		<author>
			<persName><forename type="first">Ioannis</forename><surname>Alagiannis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Renata</forename><surname>Borovica-Gajic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miguel</forename><surname>Branco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stratos</forename><surname>Idreos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anastasia</forename><surname>Ailamaki</surname></persName>
		</author>
		<idno type="DOI">10.1145/2830508</idno>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="112" to="121" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Feverous: Fact extraction and verification over unstructured and structured information</title>
		<author>
			<persName><forename type="first">Rami</forename><surname>Aly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhijiang</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Schlichtkrull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Thorne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Vlachos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christos</forename><surname>Christodoulopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oana</forename><surname>Cocarascu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arpit</forename><surname>Mittal</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.2106.05707</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A Benchmark Dataset of Check-worthy Factual Claims</title>
		<author>
			<persName><forename type="first">Naeemul</forename><surname>Fatma Arslan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengkai</forename><surname>Hassan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><surname>Tremayne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">14th International AAAI Conference on Web and Social Media</title>
		<imprint>
			<publisher>AAAI</publisher>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Extracting linked data from statistic spreadsheets</title>
		<author>
			<persName><forename type="first">Tien</forename><surname>Duc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cao</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Ioana</forename><surname>Manolescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Tannier</surname></persName>
		</author>
		<idno type="DOI">10.1145/3066911.3066914</idno>
	</analytic>
	<monogr>
		<title level="m">International Workshop on Semantic Big Data, International Workshop on Semantic Big Data</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Searching for Truth in a Database of Statistics</title>
		<author>
			<persName><forename type="first">Tien-Duc</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ioana</forename><surname>Manolescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Tannier</surname></persName>
		</author>
		<ptr target="https://gitlab.inria.fr/cedar/excel-search" />
	</analytic>
	<monogr>
		<title level="m">WebDB</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A content management perspective on fact-checking</title>
		<author>
			<persName><forename type="first">Sylvie</forename><surname>Cazalens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philippe</forename><surname>Lamarre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Leblay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ioana</forename><surname>Manolescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Tannier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">WWW (Companion</title>
		<imprint>
			<biblScope unit="page" from="565" to="574" />
			<date type="published" when="2018">2018</date>
			<publisher>ACM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Tabfact : A large-scale dataset for table-based fact verification</title>
		<author>
			<persName><forename type="first">Wenhu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongmin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianshu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunkai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shiyang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiyou</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wang</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Computational fact checking from knowledge networks</title>
		<author>
			<persName><forename type="first">Gianluca</forename><surname>Ciampaglia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prashant</forename><surname>Shiralkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luis</forename><forename type="middle">M</forename><surname>Rocha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johan</forename><surname>Bollen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Filippo</forename><surname>Menczer Menczer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alessandro</forename><surname>Flammini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS ONE</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Unsupervised cross-lingual representation learning at scale</title>
		<author>
			<persName><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kartikay</forename><surname>Khandelwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vishrav</forename><surname>Chaudhary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Wenzek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francisco</forename><surname>Guzmán</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.02116</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Xnli: Evaluating crosslingual sentence representations</title>
		<author>
			<persName><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruty</forename><surname>Rinott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adina</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><forename type="middle">R</forename><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Extracting statistical mentions from textual claims to provide trusted content</title>
		<author>
			<persName><forename type="first">Tien</forename><surname>Duc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cao</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Ioana</forename><surname>Manolescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Tannier</surname></persName>
		</author>
		<ptr target="https://ec.europa.eu/eurostat/estat-navtree-portlet-prod/BulkDownloadListing?sort=1&amp;file=dic%2Ffr%2Fgeo.dic" />
	</analytic>
	<monogr>
		<title level="m">European geographic location</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note>NLDB</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">TaPas: Weakly supervised table parsing via pre-training</title>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Herzig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Krzysztof</forename><surname>Pawel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Nowak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francesco</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Piccinno</surname></persName>
		</author>
		<author>
			<persName><surname>Eisenschlos</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.398</idno>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="4320" to="4333" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Entities with quantities: Extraction, search, and ranking</title>
		<author>
			<persName><forename type="first">Thinh</forename><surname>Vinh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Koninika</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niko</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Klaus</forename><surname>Kleer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gerhard</forename><surname>Berberich</surname></persName>
		</author>
		<author>
			<persName><surname>Weikum</surname></persName>
		</author>
		<idno type="DOI">10.1145/3336191.3371860</idno>
	</analytic>
	<monogr>
		<title level="m">WSDM &apos;20: The Thirteenth ACM International Conference on Web Search and Data Mining</title>
		<meeting><address><addrLine>Houston, TX, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2020-02-03">2020. February 3-7, 2020</date>
			<biblScope unit="page" from="833" to="836" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Enhancing knowledge bases with quantity facts</title>
		<author>
			<persName><forename type="first">Thinh</forename><surname>Vinh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daria</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dragan</forename><surname>Stepanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jannik</forename><surname>Milchevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gerhard</forename><surname>Strötgen</surname></persName>
		</author>
		<author>
			<persName><surname>Weikum</surname></persName>
		</author>
		<idno type="DOI">10.1145/3485447.3511932</idno>
	</analytic>
	<monogr>
		<title level="m">WWW &apos;22: The ACM Web Conference 2022, Virtual Event</title>
		<meeting><address><addrLine>Lyon, France</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2022-04-25">2022. April 25 -29, 2022</date>
			<biblScope unit="page" from="893" to="901" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Verifying text summaries of relational data sets</title>
		<author>
			<persName><forename type="first">Saehan</forename><surname>Jo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Immanuel</forename><surname>Trummer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weicheng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuezhi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niyati</forename><surname>Mehta</surname></persName>
		</author>
		<idno type="DOI">10.1145/3299869.3300074</idno>
	</analytic>
	<monogr>
		<title level="m">SIGMOD, SIGMOD &apos;19</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="299" to="316" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Human and technological infrastructures of fact-checking</title>
		<author>
			<persName><forename type="first">Prerna</forename><surname>Juneja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tanushree</forename><surname>Mitra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2022 Conference On Computer Supported Cooperative Work And Social Computing (CSCW &apos;22)</title>
		<meeting>the 2022 Conference On Computer Supported Cooperative Work And Social Computing (CSCW &apos;22)</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note>Proceedings of the ACM on Human-Computer Interaction</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Scrutinizer: Fact checking statistical claims</title>
		<author>
			<persName><forename type="first">Georgios</forename><surname>Karagiannis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammed</forename><surname>Saeed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paolo</forename><surname>Papotti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Immanuel</forename><surname>Trummer</surname></persName>
		</author>
		<idno type="DOI">10.14778/3415478.3415520</idno>
	</analytic>
	<monogr>
		<title level="j">Proc. VLDB Endow</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2965" to="2968" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Automated fact-checking for assisting human fact-checkers</title>
		<author>
			<persName><forename type="first">Preslav</forename><surname>Nakov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Corney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maram</forename><surname>Hasanain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Firoj</forename><surname>Alam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tamer</forename><surname>Elsayed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alberto</forename><surname>Barrón-Cedeño</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paolo</forename><surname>Papotti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaden</forename><surname>Shaar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Giovanni</forename><surname>Da</surname></persName>
		</author>
		<author>
			<persName><forename type="first">San</forename><surname>Martino</surname></persName>
		</author>
		<idno type="DOI">10.24963/ijcai.2021/619</idno>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<publisher>Survey Track</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="4551" to="4558" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Combining fact extraction and verification with neural semantic matching networks</title>
		<author>
			<persName><forename type="first">Yixin</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haonan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<idno type="DOI">10.1609/aaai.v33i01.33016859</idno>
	</analytic>
	<monogr>
		<title level="m">The Thirty-Third AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6859" to="6866" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Sentence-BERT: Sentence embeddings using siamese bertnetworks</title>
		<author>
			<persName><forename type="first">Nils</forename><surname>Reimers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">The probabilistic relevance framework: BM25 and beyond</title>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Zaragoza</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>Now Publishers Inc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Factchecking statistical claims with tables</title>
		<author>
			<persName><forename type="first">Mohammed</forename><surname>Saeed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paolo</forename><surname>Papotti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Data Engineering Bulletin</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Replace or retrieve keywords in documents at scale</title>
		<author>
			<persName><forename type="first">Vikash</forename><surname>Singh</surname></persName>
		</author>
		<idno>CoRR, abs/1711.00046</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Heideltime: High quality rule-based extraction and normalization of temporal expressions</title>
		<author>
			<persName><forename type="first">Jannik</forename><surname>Strötgen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Gertz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int&apos;l. Workshop on Semantic Evaluation</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="321" to="324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Identification and verification of simple claims about statistical properties</title>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Vlachos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D15-1312</idno>
		<ptr target="https://jena.apache.org/documentation/tdb2/" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<title level="s">Association for Computational Linguistics</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Lisbon, Portugal; W3C</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013">2015. 2013</date>
			<biblScope unit="page" from="2596" to="2601" />
		</imprint>
	</monogr>
	<note>SPARQL 1.1 query language</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">UCL machine reading group: Four factor framework for fact finding (HexaF)</title>
		<author>
			<persName><forename type="first">Takuma</forename><surname>Yoneda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Welbl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pontus</forename><surname>Stenetorp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W18-5515</idno>
		<editor>FEVER</editor>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>Association for Computational Linguistics</publisher>
			<biblScope unit="page" from="97" to="102" />
			<pubPlace>Brussels, Belgium</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
