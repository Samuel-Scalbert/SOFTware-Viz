<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Integrating knowledge graph embeddings to improve mention representation for bridging anaphora resolution</title>
				<funder ref="#_dKSW2hz">
					<orgName type="full">Agence Nationale de la Recherche</orgName>
					<orgName type="abbreviated">ANR</orgName>
				</funder>
				<funder ref="#_NvngehM">
					<orgName type="full">CPER Nord-Pas de Calais/FEDER</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Onkar</forename><surname>Pandit</surname></persName>
							<email>onkar.pandit@inria.fr</email>
							<affiliation key="aff0">
								<orgName type="department">MAGNET</orgName>
								<orgName type="institution">Inria Lille -Nord Europe</orgName>
								<address>
									<settlement>Villeneuve d&apos;Ascq</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Pascal</forename><surname>Denis</surname></persName>
							<email>pascal.denis@inria.fr</email>
							<affiliation key="aff0">
								<orgName type="department">MAGNET</orgName>
								<orgName type="institution">Inria Lille -Nord Europe</orgName>
								<address>
									<settlement>Villeneuve d&apos;Ascq</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Liva</forename><surname>Ralaivola</surname></persName>
							<email>l.ralaivola@criteo.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Criteo AI Lab</orgName>
								<address>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Integrating knowledge graph embeddings to improve mention representation for bridging anaphora resolution</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">37960297D45EEB379607F57839629E2F</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-04-12T14:54+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Lexical semantics and world knowledge are crucial for interpreting bridging anaphora. Yet, existing computational methods for acquiring and injecting this type of information into bridging resolution systems suffer important limitations. Based on explicit querying of external knowledge bases, earlier approaches are computationally expensive (hence, hardly scalable) and they map the data to be processed into high-dimensional spaces (careful handling of the curse of dimensionality and overfitting has to be in order). In this work, we take a different and principled approach which naturally addresses these issues. Specifically, we convert the external knowledge source (in this case, WordNet) into a graph, and learn embeddings of the graph nodes of low dimension to capture the crucial features of the graph topology and, at the same time, rich semantic information. Once properly identified from the mention text spans, these low dimensional graph node embeddings are combined with distributional text-based embeddings to provide enhanced mention representations. We illustrate the effectiveness of our approach by evaluating it on commonly used datasets, namely ISNotes <ref type="bibr" target="#b22">(Markert et al., 2012)</ref> and BASHI (RÃ¶siger, 2018). Our enhanced mention representations yield significant accuracy improvements on both datasets when compared to different standalone text-based mention representations.</p><p>1 WordNet is a lexical database and not a knowledge graph in the stricter sense. But, the graph is constructed over it, to be subsequently used as the knowledge graph.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>An anaphor is an expression whose interpretation depends upon a previous expression in the discourse, an antecedent. A Bridging anaphor is a special type of anaphor where there is non-identical or associative relation with its antecedent <ref type="bibr" target="#b1">(Clark, 1975)</ref>, as in the following example:</p><p>"Starbucks has a new take on the unicorn frappuccino. One employee accidentally leaked a picture of the secret new drink."</p><p>In this case, the anaphor One employee depends on the antecedent Starbucks for the complete interpretation and holds non-identical relationship with the antecedent, hence, a bridging anaphor.</p><p>We here address the problem of learning from a set of anaphor-antecedent pairs a predictor capable of accurately identify such pairs in unseen texts. More precisely, if bridging resolution comprises two main tasks, bridging anaphora recognition and bridging anaphora resolution, we solely focus on the task of bridging anaphora resolution and assume that bridging anaphor recognition has already been performed.</p><p>Semantic information on anaphor-antecedent pairs plays a crucial role in resolving bridging anaphora. Consider again the previous example: if the resolution system has the knowledge that Starbucks is a company and companies have employees, then it is easy to establish the link between them. Standard text-based features either hand-crafted or automatically extracted from word embeddings <ref type="bibr">(Mikolov et al., 2013a</ref><ref type="bibr" target="#b27">, Pennington et al., 2014)</ref>, are not sufficient for bridging resolution <ref type="bibr">(Hou, 2018b)</ref>. Earlier systems <ref type="bibr">(Poesio et al., 2004, Lassalle and</ref><ref type="bibr" target="#b17">Denis, 2011)</ref> have proposed to extract this information from knowledge bases, the web, or raw text through queries of the form "X of Y". The estimated number of occurrences in these sources gives the probability of relations between X and Y. These types of queries were generalized by <ref type="bibr" target="#b7">(Hou et al., 2013)</ref> where all queries of the type "X preposition Y", i.e. beyond the mere "of" preposition, were considered. However, these approaches extract only shallow features, capturing relations between pair of nodes instead of taking advantage of broader information that is present in knowledge graphs. Therefore, attempting to extend these strategies to take into account a larger amount of information on mentions may translate into learning problems where the input space is of high dimension, which might be a hurdle when dealing with moderate size datasets -for instance, the datasets that we consider here, i.e ISNotes <ref type="bibr" target="#b22">(Markert et al., 2012)</ref> and ARRAU <ref type="bibr" target="#b43">(Uryupina et al., 2019)</ref> respectively contain 663 training pairs and 5512 training pairs.</p><p>Recently proposed approaches tried to remedy these shortcomings <ref type="bibr">(Hou, 2018b</ref><ref type="bibr">, Hou, 2018a)</ref>. Hou learned embeddings on the pairs of nouns present in the text which are connected by prepositional or possessive structure (e.g. "X of Y"). She creates "pseudo knowledge" by generating these noun-pairs and learn embeddings on these pairs. Her approach is better at capturing fine-grained semantics than vanilla word embeddings such as Word2Vec <ref type="bibr">(Mikolov et al., 2013a)</ref>, Glove <ref type="bibr" target="#b27">(Pennington et al., 2014)</ref>, etc. however, it still depends on the presence of the required noun-pairs in the corpus. The use of knowledge graphs, either manually or automatically constructed, can alleviate this problem as they contain general semantic and world-knowledge. We empirically demonstrate that embeddings constructed on these graphs indeed provide additional information and complement these text-based embeddings.</p><p>In the present work, we propose to use low-dimensional graph node embeddings on knowledge graphs to capture semantic information. We use WordNet 1 <ref type="bibr" target="#b4">(Fellbaum, 1998)</ref> as a knowledge graph in the experiments, though our approach can be extended to any knowledge graph. We hypothesize that the lowdimensional vectors learned on the nodes of WordNet graph capture lexical semantics such as hypernymy, hyponymy, meronymy, etc. as well as general relatedness between nodes. This way we eliminate the cumbersome task of manually designing features as well as the burden of querying. Moreover, as we shall see, the low dimensionality of the embedding space does not go against its use with small datasets. But obtaining node embeddings for a mention is non-trivial a task, as it requires mapping a potentially ambiguous multi-token expression onto a specific node in the graph (synset in case of WordNet). This entails several key steps, such as: (i) mention normalization where the mention is mapped to a standardized form which might be present in the graph, (ii) handling the absent knowledge case where the referred entity is unavailable in the knowledge graph and possibly (iii) sense disambiguation in the presence of multiple senses for the mention. We propose simple yet effective heuristics to address these issues, as detailed in the coming sections. These knowledge graph embeddings are combined with distributional text-based embeddings to produce improved mention representations.</p><p>We address the problem of bridging resolution as a ranking problem, where the trained model assigns a score to anaphor-candidate antecedent pairs, preferring this ranking approach over a classification perspective for it to be less sensitive to class-imbalance, and making it focused on learning relative scores. Specifically, we train a ranking SVM model to predict scores for anaphor-candidate antecedent pairs, an approach that has been successfully applied to the related task of coreference resolution <ref type="bibr" target="#b35">(Rahman and Ng, 2009)</ref>. We observe that integrating node embeddings with text-based embeddings produces increased accuracy, substantiating the ability of graph node embeddings in capturing the semantic information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Bridging anaphora resolution. Earlier approaches <ref type="bibr" target="#b31">(Poesio et al., 1997</ref><ref type="bibr" target="#b30">, Poesio and Vieira, 1998</ref><ref type="bibr" target="#b32">, Poesio et al., 2004</ref><ref type="bibr" target="#b17">, Lassalle and Denis, 2011)</ref> put restrictions on the resolution task either by constraining the types of noun-phrases (NP) to be considered as bridging anaphor or by restricting relations between bridging anaphors and antecedent where most of the approaches tackle specific type of anaphor like definite noun-phrases. A pairwise model combining lexical semantic features as well as salience features to perform bridging resolution limited to mereological relations only is studied by <ref type="bibr" target="#b32">(Poesio et al., 2004)</ref> on the GNOME corpus. Lexical distance is used as one feature in their approach. WordNet <ref type="bibr" target="#b4">(Fellbaum, 1998)</ref> is used to acquire the distance. For a noun head X of an anaphor x and Y of potential antecedent y, the query of the form "X of Y " is provided to WordNet. But recall in WordNet is low, so as an alternative, Google API is used to get the distance between anaphor-antecedent. The API yields number of hits, from which lexical distance is calculated. Based on this method (Lassalle and Denis, 2011) developed a system that resolves mereological bridging anaphors in French.</p><p>Improving on the previous approach, <ref type="bibr" target="#b7">(Hou et al., 2013)</ref> proposed a generic query with all possible prepositions. Their query is formulated as "X preposition Y " instead of limiting to of preposition. They propose a global model as opposed to previous approaches that relied only on local features. In this model, they infer links globally instead of choosing from candidate set of the specific anaphor as they argue that the probability of noun phrase (NP) being antecedent increases if it is already antecedent to another anaphor. Their assumption is opposite to the local salience hypothesis of <ref type="bibr" target="#b41">(Sidner, 1979)</ref> as the local models indirectly assume that the most salient candidate among the nearest context is the best suitable for antecedent. Rule-based full bridging resolution system is proposed in <ref type="bibr" target="#b8">(Hou et al., 2014)</ref> where they devised rules for linking anaphors to antecedents. Some of the rules as well as the corresponding features are acquired by querying the knowledge sources, albeit different queries such as a query to get a list of nouns which denote a part of buildingwall, window, or list of personal relationshusband, sister, etc. They also propose a learning-based system by converting the rules into features but observe slight gain.</p><p>The work <ref type="bibr">(Hou, 2018b</ref>) created word embeddings for bridging (embeddings PP) by exploring the syntactic structure of noun phrases (NPs) to derive contexts for nouns in the GloVe model. She generalizes previous approaches of querying as her PP context model uses all prepositions for all nouns in big corpora. The deterministic approach proposed in <ref type="bibr">(Hou, 2018a)</ref> is the extension to the work done in <ref type="bibr">(Hou, 2018b)</ref> which creates new embeddings (embedding bridging) by combining embeddings PP and GloVe. Her approach is efficient and solves the scalability and curse of dimensionality issues. But her approach depends on the presence of the NP having a specific syntactic structure so that the algorithm can identify it as "X preposition Y ". This algorithm misses those anaphor-antecedent pairs which do not possess this structure. The work <ref type="bibr" target="#b36">(Roesiger et al., 2018)</ref> uses neural networks trained on the relation classification tasks to get the semantic information between anaphor and antecedent. This information is integrated into the state-of-the-art systems for coreference and bridging resolution. The system fails at capturing broader semantic relations as only six semantic relations are predicted with neural networks, due to this they observe marginal improvement in the bridging resolution.</p><p>All the previous works assume that the mentions are detected, i.e., noun phrases are presented and the task is to choose the correct NP as an antecedent. This is discarded in the latest system, BARQA <ref type="bibr" target="#b11">(Hou, 2020)</ref>. She casts bridging anaphora resolution as a question answering problem where answer produces antecedent for an anaphor. She also pointed out that most of the previous approaches relied only on the features of the antecedent-anaphor ignoring the context around them. However, she ignores any semantic information and relies on BERT <ref type="bibr" target="#b3">(Devlin et al., 2018)</ref> architecture to capture both contextual information as well as required common sense knowledge.</p><p>Knowledge Graph Embeddings Graph embeddings represent graph (whole or sub-graph) or nodes with the lower dimensional vector. The work <ref type="bibr" target="#b6">(Hamilton et al., 2017)</ref> details a generic framework of the commonly used graph embedding algorithms. In recent times, embedding algorithms specifically for knowledge graphs have been proposed -RESCAL <ref type="bibr" target="#b25">(Nickel et al., 2011)</ref>, DistMult <ref type="bibr" target="#b46">(Yang et al., 2014)</ref>, ComplEx <ref type="bibr" target="#b42">(Trouillon et al., 2016)</ref>, HolE <ref type="bibr" target="#b26">(Nickel et al., 2016)</ref> learn embeddings for knowledge graph completion, <ref type="bibr" target="#b0">(Bansal et al., 2019)</ref> propose A2N neighborhood attention-based technique, <ref type="bibr" target="#b45">(Xu and Li, 2019)</ref> embed relations with dihedral groups whereas <ref type="bibr" target="#b24">(Nathani et al., 2019)</ref> employ graph attention network to acquire embeddings. In this work, we used WordNet as a knowledge graph so we are interested in the graph node embeddings learned particularly on WordNet <ref type="bibr" target="#b5">(Goikoetxea et al., 2015</ref><ref type="bibr" target="#b38">, Saedi et al., 2018</ref><ref type="bibr" target="#b16">, Kutuzov et al., 2019)</ref>. Though, <ref type="bibr" target="#b5">(Goikoetxea et al., 2015</ref><ref type="bibr" target="#b38">, Saedi et al., 2018)</ref> do not produce embeddings for senses present in WordNet as they encode corresponding words. However, path2vec <ref type="bibr" target="#b16">(Kutuzov et al., 2019)</ref> produces embeddings for each sense present in WordNet by optimizing graph-based similarity metric. The use of knowledge graph embeddings to infuse common sense knowledge into NLP systems is becoming popular, and our work falls into this category. Language model <ref type="bibr" target="#b28">(Peters et al., 2019)</ref>, domain-specific natural language inference (NLI) <ref type="bibr" target="#b40">(Sharma et al., 2019)</ref>, entity disambiguation <ref type="bibr" target="#b39">(Sevgili et al., 2019)</ref> have been some of the tasks where graph embeddings have been used. To the best of our knowledge, this is the first work where graph embeddings are used for bridging anaphora resolution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Knowledge-aware mention representation</head><p>In this paper, we propose a new, knowledge-aware mention representations for bridging resolution. These representations combine two components: (i) distributional embeddings learned from raw text data, and (ii) graph node embeddings learned from relational data obtained from a knowledge graph. Specifically, the final representation v m for a mention m is obtained by concatenating the text-based contextual embeddings g m and the knowledge graph node embeddings</p><formula xml:id="formula_0">h m : v m = [g m , h m ].</formula><p>For the distributional embeddings g m , we use off-the-shelf word embeddings such as word2vec <ref type="bibr">(Mikolov et al., 2013a)</ref>, glove <ref type="bibr" target="#b27">(Pennington et al., 2014)</ref>, BERT <ref type="bibr" target="#b3">(Devlin et al., 2018)</ref>, or embeddings pp <ref type="bibr">(Hou, 2018b)</ref>. Except for BERT, we average over embeddings of the mention's head word and common nouns appearing in the mention before the head, as mentioned in <ref type="bibr">(Hou, 2018a)</ref>. With BERT, mention embeddings are obtained by averaging over embeddings of all the words of the mention.</p><p>However, obtaining knowledge graph-based embeddings h m for the mention is a much more challenging task, comprising different steps. Before detailing those steps, we first briefly describe the knowledge graph -WordNet and how we compute node embeddings in the following paragraphs.</p><p>Knowledge Graph is a graph with nodes being entities or abstract concepts and edges denoting the relation between them. A node in the knowledge graph can be a real-world entity such as a person, a place, etc. or can be an abstract concept such as a word, a sense, etc. A knowledge graph can be domain-specific (WordNet <ref type="bibr" target="#b4">(Fellbaum, 1998)</ref> captures the semantic relation between words and meanings) or open domain (DBpedia <ref type="bibr" target="#b20">(Lehmann et al., 2015)</ref> for general-purpose knowledge). The central purpose of knowledge graphs is to store common sense knowledge in a structured format so that machines can easily access it. In this work, we have used WordNet as a knowledge repository but our approach is generic and can be applied with any other knowledge graph.</p><p>WordNet <ref type="bibr" target="#b4">(Fellbaum, 1998)</ref> primarily consists of synsets, i.e., a set of synonyms of words. The synsets which refer to the same concept are grouped together giving it a thesaurus-like structure. Each synset consists of its definition and small example showing its use in a sentence. The synsets are connected with different relations such as synonymy, antonymy, hypernymy, hyponymy, meronymy, etc. In addition to the semantic knowledge, it also includes a bit of common sense knowledge such as real world entities like cities, countries and famous people. However, WordNet stores this knowledge as a database in its basic form, so a graph is constructed based on WordNet for further use. Subsequently, the node embeddings learned on this graph will automatically capture the semantic information associated with the senses.</p><p>We briefly discuss different WordNet node embedding algorithms used in our study. We use random walk and neural language model based embeddings <ref type="bibr" target="#b5">(Goikoetxea et al., 2015)</ref>, matrix factorization based WordNet embeddings <ref type="bibr" target="#b38">(Saedi et al., 2018)</ref> and graph-similarity based path2vec <ref type="bibr" target="#b16">(Kutuzov et al., 2019)</ref> embeddings. The important distinction between these methods is that the first two algorithms <ref type="bibr" target="#b5">(Goikoetxea et al., 2015</ref><ref type="bibr" target="#b38">, Saedi et al., 2018)</ref> produce word embeddings and path2vec produces embeddings corresponding to each sense present in WordNet. The path2vec algorithm naturally encodes WordNet nodes as it actually produces embeddings for senses as opposed to <ref type="bibr" target="#b5">(Goikoetxea et al., 2015</ref><ref type="bibr" target="#b38">, Saedi et al., 2018)</ref> algorithms as they conflate all the senses to produce word embeddings instead of generating embeddings for each sense while losing some finer semantic information in the process.</p><p>The approach proposed by <ref type="bibr" target="#b5">(Goikoetxea et al., 2015)</ref> is based on the well-known neural language model Continuous Bag of Words and Skip-gram <ref type="bibr">(Mikolov et al., 2013b)</ref>. The main idea is to produce artificial sentences from WordNet and to apply the language models on these sentences to produce word embeddings. For this, they perform random walk starting at any arbitrary vertex in WordNet, then map each WordNet sense to the corresponding word to produce an artificial sentence. Each random walk produces a sentence, repeating this process several times gives a collection of sentences. Finally, this collection of sentences is considered as the corpus for learning word embeddings.</p><p>A different approach based on matrix factorization is taken in <ref type="bibr" target="#b38">(Saedi et al., 2018)</ref> to produce embeddings. The procedure starts by creating the adjacency matrix M from WordNet graph. The element M ij in the matrix M is set to 1 if there exists any relation between words w i and w j . 2 Furthermore, words which are not connected directly but via other nodes should also have an entry in the matrix, albeit with lower weights than 1. Accordingly, matrix M G is constructed to get the overall affinity strength between words. In the analytical formulation, M G can be constructed from the adjacency matrix M as M G = (I -Î±M ) -1 where I is the identity matrix and 0 &lt; Î± &lt; 1 decay factor to control the effect of longer paths over shorter ones. Following that, matrix M G is normalized to reduce the bias towards words which have more number of senses and finally a Principal Component Analysis is applied on it to produce dense word vectors.</p><p>The path2vec <ref type="bibr" target="#b16">(Kutuzov et al., 2019)</ref> learns embeddings based on a pairwise similarity between nodes. The fundamental concept is that pairwise similarity between nodes of the graph should remain the same after their projection in the vector space. The model is flexible enough to consider any user-defined similarity measure while encoding. The objective function is designed to produce such embeddings for nodes which reduce the difference between actual graph-based pairwise similarity and vector similarity. It also preserves the similarity between adjacent nodes. Formally, for the graph G = (V, E) where V, E denote a set of vertices and edges, respectively, the objective is -</p><formula xml:id="formula_1">(a,b)âV min va,v b (v T a v b -s(a, b)) 2 -Î±(v T a v n + v T b v m )</formula><p>where n, m are adjacent nodes of nodes a, b respectively, s(a, b) is the user-defined similarity measure between a, b and v a , v b , v n , v m denote the embeddings of a, b, n, m, respectively. To show the ability of their model in adapting to different pairwise similarity measures. Mention normalization. The first step for being able to align a mention with a particular node in the knowledge base and ultimately its graph embedding, is to convert the mention into a normalized form that can be easily matched. Consider mentions like the wall, one employee, beautiful lady or the famous scientist Einstein; none of these can be directly matched to a knowledge graph node (in this case WordNet synset<ref type="foot" target="#foot_1">3</ref> ). We propose to normalize them into a single word, respectively to wall, employee, lady and Einstein. We design simple rules to normalize mentions. For this, as a first step, we remove articles and commonly used quantifiers like the, a, an, one, all etc. from the mention. If we find an entry in the knowledge graph with this modified word then we get the corresponding embedding, otherwise, we go a step further and extract the head of the mention and try to obtain embeddings for it. Specifically, we use the parsed tree of the mention and Collins' head finder algorithm <ref type="bibr" target="#b2">(Collins, 2003)</ref> to get the head.</p><p>Absence of Knowledge. Even after mention normalization, it might still be possible that a mention cannot be aligned with a node in the knowledge graph, simply because some entities are not present therein. This leads to the unavailability of the corresponding node embeddings. We use zero vector of the same dimensions to resolve these cases where node embeddings are absent.</p><p>Sense disambiguation The knowledge graph may contain multiple concepts or senses for a given entity. This is the case in all the knowledge graphs. The reason is that the same word has many senses or refer to different real world entities. For example, the word bank can refer to a financial institution or the land alongside the river, the entity Michael Jordan can refer to the scientist or the basketball player. Due to this ambiguity, there are multiple node embeddings for the same mention as they capture entirely different concepts<ref type="foot" target="#foot_2">4</ref> . However, recognizing the correct sense is crucial to get accurate embedding. We explore two simple heuristics to tackle the issue of multiple senses of an entity -1. Lesk <ref type="bibr" target="#b21">(Lesk, 1986)</ref> algorithm to get the correct sense of the mention depending on the context, as in the discourse, the meaning of the mention depends on the context in which it is uttered. 2. Unweighted average over embeddings of all the senses of the mention.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Ranking Model</head><p>Let D be the given document containing M = {m 1 , m 2 , â¢ â¢ â¢ , m nm }, n m number of mentions. Let A = {a 1 , a 2 , â¢ â¢ â¢ , a na } denote the set of all anaphors and A â M. Let a be any anaphor in the set A and j be its position in the set M, then E a be the set of candidate antecedents for a which is defined as E a = {m i : m i â M, i &lt; j}. Let T a and F a be the set of true antecedents and false candidate antecedents of a such that T a âª F a = E a , T a â© F a = â. Let each anaphor a is represented with the feature vector v a and candidate antecedent e represented with v e where e â E a . Then the goal is to predict score s(v a , v e ) between anaphor a and candidate antecedent e. The score denotes the possibility of anaphor a having bridging relation with the candidate antecedent e, so a higher score denotes a higher chance of e being true antecedent.</p><p>The model is trained to reduce the ranking loss calculated based on the scores obtained between anaphor-candidate antecedents. The ranking strategy is fairly obviousfor an anaphor a high scoring candidate antecedent from E a is ranked higher than the low scoring one. Let this prediction ranking strategy be r and true ranking is given by r * . For a candidate antecedent, if predicted rank is not the same as true rank then it is called discordant candidate, otherwise concordant. The difference between true and predicted ranking strategy can be measured with Kendall's rank correlation coefficient -Ï . Formally, concordant C, discordant D candidates and Ï are calculated as -</p><formula xml:id="formula_2">C = (t,f )â(TaÃFa) I s(va,vt)&gt;s(va,v f ) , D = |T a Ã F a | -C and Ï (r * , r ) = C -D C + D</formula><p>where I is an indicator function which takes value 1 if s(v a , v t ) &gt; s(v a , v f ) else 0 and | â¢ | denotes cardinality of the set. The empirical ranking loss <ref type="bibr" target="#b13">(Joachims, 2002)</ref> captures the number of wrongly predicted ranks which is given as</p><formula xml:id="formula_3">- L = 1 n a na i=1 -Ï (r * i , r i )</formula><p>Inference We consider all the anaphors in the test document separately. For each anaphor, we consider all previously occurring mentions as candidate antecedents<ref type="foot" target="#foot_3">5</ref> and find out the compatibility score for each anaphor-candidate antecedent pair with the above ranking model. We apply best first strategy to choose the most appropriate antecedent from the list of candidate antecedents. In this strategy, the highest scoring pair is selected as anaphor-antecedent pair. Formally, let a be any anaphor and E a denote a set of candidate antecedents for a. Let s(a, e) be the score between a and e where e â E a . Let Ãªa be the predicted antecedent of a which is given by -Ãªa = argmax eâEa s(a, e)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experimental Setup</head><p>Data We used ISNotes <ref type="bibr" target="#b22">(Markert et al., 2012)</ref> and BASHI <ref type="bibr" target="#b37">(RÃ¶siger, 2018)</ref> datasets for experiments.</p><p>ISNotes and BASHI consist of 50 different OntoNotes documents, containing 663 and 459 anaphors, respectively. BASHI dataset annotates comparative anaphors as bridging anaphors which are 115 in numbers, remaining are referential anaphors. Following the setup from <ref type="bibr" target="#b11">(Hou, 2020)</ref>, we only consider 344 referential bridging anaphors in this work as well from the BASHI dataset. In the experiments, we implemented nested cross-validation to select the best hyperparameter combination. The setup isfirst we make 10 sets of train and test documents containing 45 and 5 documents respectively with 10-fold division. Then at each fold, 45 training documents are further divided into 5 sets of 36-9 actual training and development documents. Each hyperparameter combination is trained on these 5-sets and evaluated. The highest averaged accuracy over the 5-sets of development documents gives the best hyperparameter combination. Once the best hyperparameter setting is obtained the SVM model is re-trained over 45 documents (36+9). For each fold number of accurately linked anaphors is calculated. The accurately predicted number of anaphors over each fold is added to get the total number of accurately linked anaphors from the complete dataset. Thus, the system is evaluated by the accuracy of predicted pairs <ref type="bibr" target="#b11">(Hou, 2020)</ref>.</p><p>For the training data, we have positive samples where we know true anaphor-antecedent pairs but no negative samples. We generate these pairs by considering all the noun phrases (NPs) which occur In our experiments section, we present results for different text-based embeddings -word2vec (WV), glove (GV), BERT (BE), embeddings pp (EP), BERT + embeddings pp (BEP) and the last columnshows the absence of text-based embeddings. Also, in each row, WordNet node embeddings based on different algorithms, except the first row, are added -path2vec with Lesk (PL), path2vec with averaged senses (PA), random walk based (RW) and WordNet embeddings (WNV). The other section of the table -SOTA, shows results with previously proposed systems -Pairwise Model III (PMIII), MLN model II (MMII) <ref type="bibr" target="#b7">(Hou et al., 2013)</ref>, embeddings bridging (EB) <ref type="bibr">(Hou, 2018a)</ref>, the combination of embeddings bridging and MLN model (MMEB) and the latest system, BARQA <ref type="bibr" target="#b11">(Hou, 2020)</ref>. The results with * are statistically signficant in comparison to the results based only on text embeddings with p-value &lt; 10 -4 with McNemar's test and Wilcoxon signed-rank test. before the anaphor in the window of some fixed number of sentences. All the mention pairs which do not hold bridging relations are considered as negative samples for training. Similarly at the test time, for an anaphor, all the previous mentions in the fixed window size are considered as candidate antecedents.</p><p>Implementation We obtained pre-trained word2vec <ref type="bibr">(Mikolov et al., 2013a)</ref>, Glove <ref type="bibr" target="#b27">(Pennington et al., 2014)</ref>, BERT <ref type="bibr" target="#b3">(Devlin et al., 2018)</ref> and embeddings pp <ref type="bibr">(Hou, 2018b)</ref> embeddings. We used spanBERT <ref type="bibr" target="#b15">(Joshi et al., 2020)</ref> embeddings in our experiments as it gave better results in <ref type="bibr" target="#b11">(Hou, 2020)</ref>. Also, we used pre-trained WordNet embeddings provided by respective authors of <ref type="bibr" target="#b5">(Goikoetxea et al., 2015</ref><ref type="bibr" target="#b38">, Saedi et al., 2018</ref><ref type="bibr" target="#b16">, Kutuzov et al., 2019)</ref>. In the case of path2vec <ref type="bibr" target="#b16">(Kutuzov et al., 2019)</ref>, embeddings learned with different similarity measures such as -Leacock-Chodorow similarities <ref type="bibr" target="#b18">(Leacock and Chodorow, 1998)</ref>; Jiang-Conrath similarities <ref type="bibr" target="#b12">(Jiang and Conrath, 1997)</ref>; Wu-Palmer similarities <ref type="bibr" target="#b44">(Wu and Palmer, 1994)</ref>; and Shortest path similarities <ref type="bibr" target="#b19">(Lebichot et al., 2018)</ref>, are provided. We experimented with all the four similarity measures and found out that the shortest path based similarity measure produced better results most of the time, so we have used those embeddings in our experiments. We used python implementation of Lesk algorithm from nltk<ref type="foot" target="#foot_4">6</ref> library to select the best sense from multiple senses of the mention. Two sentences previous to mention and two sentences after the mention, including the sentence in which the mention occurs, are given to this algorithm as a context for a mention.</p><p>Both anaphor and candidate antecedent's embeddings are obtained as mentioned above, afterwards, element-wise product of these vectors is provided to the ranking SVM. We also did preliminary experiments with the concatenation of the vectors but element-wise product gave better results. We used SV M rank <ref type="bibr" target="#b14">(Joachims, 2006)</ref> implementation for our experiments. In the experiments with SVM, we did grid search over C = 0.001,0.01,0.1,1, 10,100 with the use of linear kernel. We also use random fourier features (rff) trick proposed by <ref type="bibr" target="#b33">(Rahimi and Recht, 2008)</ref> to approximate non-linear kernels. We found, use of non-linear kernels slightly improved results in comparison to linear kernels so reported only those results. We also varied different widow sizes of sentences -2,3,4 and all previous sentences, in addition to NPs from the first sentence (salience), to get candidate antecedents for an anaphor. Out of these settings, the window size of 2 and salience have yielded the best results which are reported here.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Results</head><p>Comparison between distributional and graph embeddings is shown in Table <ref type="table" target="#tab_0">1</ref> in our experiments section. The first row corresponding to ISNotes and BASHI dataset shows results with only text-based embeddings. We observe that on both the datasets the best performance is obtained with the use of BERT embeddings showing the efficacy of these embeddings when only one type of text-based embeddings is used. It shows that the context of the mention plays important role in resolving bridging anaphora. The second best scores are obtained with embeddings pp which are specially designed embeddings for the task. We also observe further improvement in the results when two best performing text-based embeddings -BERT and embeddings pp are combined (noted as BEP in the Table) <ref type="foot" target="#foot_5">7</ref> .</p><p>The following rows (2-4) of Table <ref type="table" target="#tab_0">1</ref> show the results obtained with the addition of WordNet information with different embeddings algorithms -path2vec <ref type="bibr" target="#b16">(Kutuzov et al., 2019)</ref>(PL and PA), random walk based embeddings <ref type="bibr" target="#b5">(Goikoetxea et al., 2015)</ref> (RW) and WordNet embeddings <ref type="bibr" target="#b38">(Saedi et al., 2018</ref>)(WNV). The results from these rows in comparison with the result from the first row prove the effectiveness of the external information and substantiates our claims<ref type="foot" target="#foot_6">8</ref> . Interestingly, it also shows that BERT though trained on a huge unlabelled corpus is not inherently efficient at capturing common sense knowledge required for bridging anaphora resolution. Though, it has been competitive at capturing relational knowledge required for other nlp tasks like question answering <ref type="bibr" target="#b29">(Petroni et al., 2019)</ref>. Moreover, external information seems to be complementing embeddings pp embeddings which are custom tailored for bridging tasks, further consolidating our claims. We compare results from path2vec Lesk (PL) with path2vec average (PA) to see which strategy of disambiguation is effective. But the observations are not conclusive, as in some cases performance with the use of averaging strategy is better than choosing the best sense with Lesk. The reason is that Lesk is a naive algorithm which considers overlapping words in the context to get the best sense. Further, in each row of the second last column of the table, results obtained by combining external information with BERT embeddings and embeddings pp show that even the best performing text-based embeddings can still benefit from the external information.</p><p>Comparison between different WordNet embeddings We first examine the effectiveness of external knowledge without any text-based embeddings. These scores are noted in the last column of our experiments section against each WordNet graph node embeddings. The lower scores in this column in comparison with text-based embeddings reveal that the features learned with WordNet embeddings are not sufficient and should be complemented with the contextual features. This observation further substantiates our observation of higher scores with BERT embeddings showing the importance of context (Table <ref type="table" target="#tab_0">1</ref>, the first row). Further, we consider results from averaged embedding over senses (PA) for comparing path2vec with the other two embeddings as it is the closest analogous setting to correlate. This comparison shows, there is no best algorithm amongst these WordNet embeddings as sometimes we get better results with path2vec and sometimes with random walk based embeddings. This result is surprising as even after losing some semantic information, RW produces competent results compared to path2vec. This might be happening because of errors in sense disambiguation with path2vec.</p><p>Comparison with previous studies The results of different state-of-the-art systems on both the datasets are presented in SOTA section of Table <ref type="table" target="#tab_0">1</ref>. These results are obtained from Hou's latest work <ref type="bibr" target="#b11">(Hou, 2020)</ref>. In BARQA <ref type="bibr" target="#b11">(Hou, 2020)</ref>, mentions are also detected in her model, so we considered results where gold mentions are considered for the equal comparison. We observe that, on ISNotes dataset, our model's performance is better than rule-based approaches from Pairwise Model III and MLN model II <ref type="bibr" target="#b7">(Hou et al., 2013)</ref>, embeddings bridging based deterministic approach from <ref type="bibr">(Hou, 2018a)</ref>  7 Error Analysis</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Mention normalization and sense disambiguation</head><p>ISNotes dataset contains 663 anaphors and combining those with candidate antecedents of each anaphor we get more than 9500 mentions out of which 10% of mentions can not be mapped to WordNet entries. The situation is similar in the case of BASHI dataset as around 8% of the 5933 mentions can not be mapped to WordNet entries. We analyze cases where normalized mention is failed to map to any sense in WordNet. There are broadly two reasons for not getting WordNet entry for the mention -1. Normalization error 2. Inherent limitations of WordNet. We note down some of the examples from each category in the Table <ref type="table" target="#tab_1">2</ref>. The first three mentions are wrongly normalized as Los Angeles to Angeles and Hong Kong to Kong, otherwise, both the cities are present in WordNet. The cases like U.S.S.R shows limitations of our simple normalization approach, the normalization should map U.S.S.R to Soviet Russia which is present in WordNet. The other three examples show the inherent limitations of WordNet as those entities are absent from WordNet.</p><p>WordNet contains multiple senses for a given word because of which we get on an average 7 senses for the given mention. We used a simple Lesk algorithm for disambiguation which takes into account the context of the normalized mention to determine the correct sense. We present some examples of disambiguation with Lesk in Table <ref type="table" target="#tab_1">2</ref>. It correctly disambiguates in the first three examples but fails for the following three. This is because of the count of overlapping words between sense's context and definition in WordNet. For example, the last example contains words like blood, breeder in the context because of which it selects sense as a group of organisms and not an organization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Anaphor-antecedent predictions</head><p>We analyze a few anaphor-antecedent pairs which were identified incorrectly with BERT-based mention representation but with the addition of WordNet information, we were able to correct it. The underlined and bold lettered phrases denote antecedent and anaphor, respectively.</p><p>(1). Staar Surgical Co.'s board said that it has removed Thomas R. Waggoner [...]. [..] that John R. Ford resigned as a director, and that Mr. Wolf was named a member of the board.</p><p>(2). So far this year, rising demand for OPEC oil and production restraint by some members have kept prices firm despite rampant cheating by others.</p><p>(3). One building was upgraded to red status while people were taking things out, and a resident who was not allowed to go back inside called up the stairs to his girlfriend, telling her to keep <ref type="bibr">[...]</ref>.</p><p>WordNet contains relations where company and director are related where director works at company. The OPEC oil is stored as a corporation which in turn is related to prices and stairs are part of building. This information is present in WordNet which has been used for resolving these pairs as opposed to relying only on the textual information in case of mention representation only with BERT.</p><p>Conversely, we also observed a few pairs where the addition of extra information has been detrimental. The italic faced phrase is the selected antecedent with WordNet based system but without WordNet correct antecedent (shown with underline) was selected for boldfaced anaphor.  Example 4, News Corporation is closer to founder than Partners as head word is Partners for the long phrase. Thus, the system assigns higher scores to wrong candidate antecedent. Similarly, in example 5, the dunes are closer to sand than treasure chest. In the example 6, WordNet contains Atalantis as legendary island and not as a space shuttle thus astronauts is closer to space probe than island, thus receiving a higher score than the correct antecedent. These mistakes can be attributed to the process of normalizing mentions as well as limitations of WordNet. Interestingly, these examples show the inadequacy of BERT in capturing the partOf relation but efficacy of capturing some form of relatedness of the terms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion</head><p>We presented a simple approach of incorporating external semantic knowledge for bridging anaphora resolution. We combined contextual embeddings learned only on the text with the knowledge graph node embeddings. We establish the potency of knowledge graph embeddings with the experiments with the use of different WordNet graph embeddings on the ISNotes and BASHI datasets. Though we apply a simplistic approach to solve mention normalization, absent knowledge resolution and sense disambiguation to obtain node embeddings, we achieve competitive results on both the datasets. Moreover, this study opens up further investigation into the design of sophisticated methods to incorporate knowledge graph embeddings for bridging anaphora resolution such as improved mention normalization and sense disambiguation, incorporating knowledge from multiple knowledge sources.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>(4).Within the same nine months, News Corp. [...]. Meanwhile, American Health Partners, publisher of American Health magazine, is deep in debt, and Owen Lipstein, founder[...]. (5)[...] the magnificent dunes where the Namib Desert meets the Atlantic Ocean [...]Since this treasure chest [...] up a diamond from the sand.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>( 6 )</head><label>6</label><figDesc>.The space shuttle Atlantis landed [...] that dispatched the Jupiter -bound Galileo space probe. The five astronauts returned [...].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Results of our experiments and state-of-the-art models over two datasets -ISNotes and BASHI.</figDesc><table><row><cell>Data</cell><cell></cell><cell></cell><cell cols="3">Our Experiments</cell><cell></cell><cell></cell><cell>SOTA</cell></row><row><cell></cell><cell></cell><cell>WV</cell><cell>GV</cell><cell>BE</cell><cell>EP</cell><cell>BEP</cell><cell>-</cell><cell>SYS</cell><cell>ACC</cell></row><row><cell></cell><cell>-</cell><cell cols="5">25.94 27.60 32.87 31.08 37.10</cell><cell>-</cell><cell>PMIII</cell><cell>36.35</cell></row><row><cell></cell><cell>+ PL</cell><cell cols="7">26.40 28.61 34.39 31.81 43.87  *  20.06 MMII</cell><cell>41.32</cell></row><row><cell>ISNotes</cell><cell>+ PA</cell><cell cols="7">24.74 30.92 33.18 33.24 39.82  *  19.53 EB</cell><cell>39.52</cell></row><row><cell></cell><cell>+ RW</cell><cell cols="7">27.75 27.6 34.12 33.24 46.30  *  22.06 MMEB</cell><cell>46.46</cell></row><row><cell></cell><cell cols="8">+ WNV 21.71 25.13 31.69 26.80 33.28 17.64 BARQA 50.08</cell></row><row><cell></cell><cell>-</cell><cell cols="5">22.92 17.48 31.23 28.51 33.52</cell><cell>-</cell><cell>PMIII</cell><cell>-</cell></row><row><cell></cell><cell>+ PL</cell><cell cols="7">30.95 21.49 35.53 29.26 36.68  *  16.44 MMII</cell><cell>-</cell></row><row><cell>BASHI</cell><cell>+ PA</cell><cell cols="7">24.07 19.2 35.24 29.48 38.94  *  17.62 EB</cell><cell>29.94</cell></row><row><cell></cell><cell>+ RW</cell><cell cols="7">26.64 18.91 34.38 28.91 38.83  *  15.75 MMEB</cell><cell>-</cell></row><row><cell></cell><cell cols="8">+ WNV 20.92 18.05 26.36 21.20 27.80 12.97 BARQA 38.66</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>and competitive Mention Mapping Error lists examples of mentions for which no entry is found in WordNet after normalization. The first three mentions are not found because of normalization error but the next three entities are not present in WordNet. Mention Sense Selection notes a few mentions and their senses selected by Lesk algorithm. For the first three mentions, Lesk disambiguates correctly but fails in the next three. The correct senses of the last three are Japanese people, racecourse and organization, respectively. in comparison with the combination of MLN model and embeddings bridging but lags to BARQA model. The reason might be that MLN model combines hand-crafted rules in addition to carefully crafted embeddings. On the other hand, BARQA system is trained on additional data obtained by forming quasi-bridging pairs. However, with BASHI dataset we observe best results, as the model achieves significant gains in comparison with embeddings bridging and moderate gains against BARQA.</figDesc><table><row><cell cols="2">Mention Mapping Error</cell><cell>Mention Sense Selection</cell><cell></cell></row><row><cell>Mention</cell><cell>Normalized Mention</cell><cell>Mention</cell><cell>Selected Sense</cell></row><row><cell cols="2">Los Angeles, Cali. Angeles</cell><cell>[...] future generations of memory chips</cell><cell>electronic equipment</cell></row><row><cell>Hong Kong</cell><cell>Kong</cell><cell>The move by the coalition of political parties [...]</cell><cell>organization</cell></row><row><cell>U.S.S.R</cell><cell>U.S.S.R</cell><cell>[...] when the rising Orange River threatened to swamp the course [...]</cell><cell>route</cell></row><row><cell>IBM</cell><cell>IBM</cell><cell>[...] U.S. industry to head off the Japanese, who now dominate [...]</cell><cell>language</cell></row><row><cell>politburo member Joachim Herrman</cell><cell>Herrman</cell><cell>[...] potential investors at race tracks [...]</cell><cell>magnetic paths</cell></row><row><cell>U.S. district judge Jack B. Weinstein</cell><cell>Weinstein</cell><cell>The Thoroughbred Owners and Breeders Association [...]</cell><cell>a group of organisms</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>They also experimented by weighting relations differently (e.g. 1 for hypernymy, hyponymy, antonymy and synonymy, 0.8 for meronymy and holonymy and 0.5 for others) but obtained the best results without weighting.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1"><p>In case of WordNet embeddings from<ref type="bibr" target="#b5">(Goikoetxea et al., 2015</ref><ref type="bibr" target="#b38">, Saedi et al., 2018)</ref>, normalized mention is mapped to words.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2"><p>This difficulty does not arise in the cases where embeddings are learned for words instead of senses<ref type="bibr" target="#b5">(Goikoetxea et al., 2015</ref><ref type="bibr" target="#b38">, Saedi et al., 2018)</ref>. But, the problem is prevalent for node embeddings learned for actual nodes of the graph.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_3"><p>In ISNotes dataset 71% of anaphors have antecedent either in the previous two sentences or the first sentence of the document. So, mentions only from the previous two sentences and the first sentence are considered as candidate antecedents. We apply the same strategy for BASHI dataset as well.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_4"><p>https://www.nltk.org/_modules/nltk/wsd.html</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_5"><p>We combine BERT and embeddings pp embeddings by concatenating both the vectors</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_6"><p>Except with the addition of WordNet embeddings (WNV) as results with WNV are mostly inferior in comparison with only text-based embeddings. Lower coverage for WNV, around 65% as opposed to</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_7"><p>90% for the other two embeddings as only 60,000 words were present in pre-trained WNV embeddings, might be the possible reason. Also, the vector dimension is significantly higher -850 in comparison to 300 for the other two.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>We thank the three anonymous reviewers for their comments and feedback. This work was supported by the <rs type="funder">French National Research Agency</rs> via grant no <rs type="grantNumber">ANR-16-CE33-0011-01</rs> as well as by <rs type="funder">CPER Nord-Pas de Calais/FEDER</rs> <rs type="programName">DATA Advanced data science</rs> and technologies 2015-2020.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_dKSW2hz">
					<idno type="grant-number">ANR-16-CE33-0011-01</idno>
				</org>
				<org type="funding" xml:id="_NvngehM">
					<orgName type="program" subtype="full">DATA Advanced data science</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A2N: Attending to neighbors for knowledge graph inference</title>
		<author>
			<persName><forename type="first">Trapit</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Da-Cheng</forename><surname>Juan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sujith</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-07">2019. July</date>
			<biblScope unit="page" from="4387" to="4392" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Bridging</title>
		<author>
			<persName><forename type="first">H</forename><surname>Herbert</surname></persName>
		</author>
		<author>
			<persName><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Theoretical Issues in Natural Language Processing</title>
		<imprint>
			<date type="published" when="1975">1975</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Head-driven statistical models for natural language parsing</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="589" to="637" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">WordNet: An Electronic Lexical Database</title>
		<author>
			<persName><forename type="first">Christiane</forename><surname>Fellbaum</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
			<publisher>Bradford Books</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Random walks and neural network language models on bases</title>
		<author>
			<persName><forename type="first">Josu</forename><surname>Goikoetxea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aitor</forename><surname>Soroa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Denver, Colorado</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015-05">2015. May-June</date>
			<biblScope unit="page" from="1434" to="1439" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Representation learning on graphs: Methods and applications</title>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rex</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<idno>arxiv:1709.05584</idno>
		<imprint>
			<date type="published" when="2017-09">2017. September 2017</date>
		</imprint>
	</monogr>
	<note>Published in the IEEE Data Engineering Bulletin. version with minor corrections</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Global inference for bridging anaphora resolution</title>
		<author>
			<persName><forename type="first">Yufang</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katja</forename><surname>Markert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Strube</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Atlanta, Georgia</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013-06">2013. June</date>
			<biblScope unit="page" from="907" to="917" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A rule-based system for unrestricted bridging resolution: Recognizing bridging anaphora and finding links to antecedents</title>
		<author>
			<persName><forename type="first">Yufang</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katja</forename><surname>Markert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Strube</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014-10">2014. October</date>
			<biblScope unit="page" from="2082" to="2093" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A deterministic algorithm for bridging anaphora resolution</title>
		<author>
			<persName><forename type="first">Yufang</forename><surname>Hou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018-10">2018. October-November</date>
			<biblScope unit="page" from="1938" to="1948" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Enhanced word representations for bridging anaphora resolution</title>
		<author>
			<persName><forename type="first">Yufang</forename><surname>Hou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>New Orleans, Louisiana</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018-06">2018. June</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
	<note>Short Papers</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Bridging anaphora resolution as question answering</title>
		<author>
			<persName><forename type="first">Yufang</forename><surname>Hou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020-07">2020. July</date>
			<biblScope unit="page" from="1428" to="1438" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Semantic similarity based on corpus statistics and lexical taxonomy</title>
		<author>
			<persName><forename type="first">Jay</forename><forename type="middle">J</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">W</forename><surname>Conrath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th Research on Computational Linguistics International Conference</title>
		<title level="s">The Association for Computational Linguistics and Chinese Language Processing</title>
		<meeting>the 10th Research on Computational Linguistics International Conference<address><addrLine>Taipei, Taiwan</addrLine></address></meeting>
		<imprint>
			<publisher>ACLCLP</publisher>
			<date type="published" when="1997-08">1997. August</date>
			<biblScope unit="page" from="19" to="33" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Optimizing search engines using clickthrough data</title>
		<author>
			<persName><forename type="first">Thorsten</forename><surname>Joachims</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD 02</title>
		<meeting>the Eighth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD 02<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page">133142</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Training linear svms in linear time</title>
		<author>
			<persName><forename type="first">Thorsten</forename><surname>Joachims</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD 06</title>
		<meeting>the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD 06<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page">217226</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Spanbert: Improving pre-training by representing and predicting spans</title>
		<author>
			<persName><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">S</forename><surname>Weld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="64" to="77" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning graph embeddings from WordNet-based similarity measures</title>
		<author>
			<persName><forename type="first">Andrey</forename><surname>Kutuzov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Dorgham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oleksiy</forename><surname>Oliynyk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Biemann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Panchenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighth Joint Conference on Lexical and Computational Semantics (*SEM 2019)</title>
		<meeting>the Eighth Joint Conference on Lexical and Computational Semantics (*SEM 2019)<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-06">2019. June</date>
			<biblScope unit="page" from="125" to="135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Leveraging different meronym discovery methods for bridging resolution in french</title>
		<author>
			<persName><forename type="first">Emmanuel</forename><surname>Lassalle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pascal</forename><surname>Denis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Anaphora Processing and Applications -8th Discourse Anaphora and Anaphor Resolution Colloquium</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">Iris</forename><surname>Hendrickx</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Lalitha</forename><surname>Sobha</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">AntÃ³nio</forename><surname>Devi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Ruslan</forename><surname>Horta Branco</surname></persName>
		</editor>
		<editor>
			<persName><surname>Mitkov</surname></persName>
		</editor>
		<meeting><address><addrLine>DAARC; Faro, Portugal</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2011-10-06">2011. 2011. October 6-7, 2011</date>
			<biblScope unit="volume">7099</biblScope>
			<biblScope unit="page" from="35" to="46" />
		</imprint>
	</monogr>
	<note>Revised Selected Papers</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Combining Local Context and WordNet Similarity for Word Sense Identification</title>
		<author>
			<persName><forename type="first">Claudia</forename><surname>Leacock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Chodorow</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="page">265</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">A constrained randomized shortest-paths framework for optimal exploration</title>
		<author>
			<persName><forename type="first">Bertrand</forename><surname>Lebichot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Guex</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilkka</forename><surname>KivimÃ¤ki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Saerens</surname></persName>
		</author>
		<idno>CoRR, abs/1807.04551</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Dbpedia -a large-scale, multilingual knowledge base extracted from wikipedia</title>
		<author>
			<persName><forename type="first">Jens</forename><surname>Lehmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Isele</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Jakob</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jentzsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kontokostas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pablo</forename><forename type="middle">N</forename><surname>Mendes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hellmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Morsey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Van Kleef</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Auer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Bizer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Semantic Web</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="167" to="195" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Automatic sense disambiguation using machine readable dictionaries: How to tell a pine cone from an ice cream cone</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Lesk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th Annual International Conference on Systems Documentation, SIGDOC 86</title>
		<meeting>the 5th Annual International Conference on Systems Documentation, SIGDOC 86<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="1986">1986</date>
			<biblScope unit="page">2426</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Collective classification for fine-grained information status</title>
		<author>
			<persName><forename type="first">Katja</forename><surname>Markert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yufang</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Strube</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 50th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Jeju Island, Korea</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012-07">2012. July</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="795" to="804" />
		</imprint>
	</monogr>
	<note>Long Papers</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Linguistic regularities in continuous space word representations</title>
		<author>
			<persName><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">; Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Zweig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Atlanta, Georgia</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013-06">2013. 2013. June</date>
			<biblScope unit="page" from="746" to="751" />
		</imprint>
	</monogr>
	<note>Efficient estimation of word representations in vector space</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning attention-based embeddings for relation prediction in knowledge graphs</title>
		<author>
			<persName><forename type="first">Deepak</forename><surname>Nathani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jatin</forename><surname>Chauhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charu</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manohar</forename><surname>Kaul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-07">2019. July</date>
			<biblScope unit="page" from="4710" to="4723" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A three-way model for collective learning on multi-relational data</title>
		<author>
			<persName><forename type="first">Maximilian</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hans-Peter</forename><surname>Volker Tresp</surname></persName>
		</author>
		<author>
			<persName><surname>Kriegel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on International Conference on Machine Learning, ICML11</title>
		<meeting>the 28th International Conference on International Conference on Machine Learning, ICML11<address><addrLine>Madison, WI, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page">809816</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Holographic embeddings of knowledge graphs</title>
		<author>
			<persName><forename type="first">Maximilian</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lorenzo</forename><surname>Rosasco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomaso</forename><surname>Poggio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence, AAAI16</title>
		<meeting>the Thirtieth AAAI Conference on Artificial Intelligence, AAAI16</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page">19551961</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014-10">2014. October</date>
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Knowledge enhanced contextual word representations</title>
		<author>
			<persName><forename type="first">Matthew</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><forename type="middle">L</forename><surname>Logan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">V</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Roy</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vidur</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sameer</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Language models as knowledge bases?</title>
		<author>
			<persName><forename type="first">Fabio</forename><surname>Petroni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>RocktÃ¤schel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anton</forename><surname>Bakhtin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-11">2019. November</date>
			<biblScope unit="page" from="2463" to="2473" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A corpus-based investigation of definite description use</title>
		<author>
			<persName><forename type="first">Massimo</forename><surname>Poesio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Renata</forename><surname>Vieira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="183" to="216" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Resolving bridging references in unrestricted text</title>
		<author>
			<persName><forename type="first">Massimo</forename><surname>Poesio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Renata</forename><surname>Vieira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simone</forename><surname>Teufel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Operational Factors in Practical, Robust Anaphora Resolution for Unrestricted Texts</title>
		<imprint>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Learning to resolve bridging references</title>
		<author>
			<persName><forename type="first">Massimo</forename><surname>Poesio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rahul</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Axel</forename><surname>Maroudas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Janet</forename><surname>Hitzeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics (ACL-04)</title>
		<meeting>the 42nd Annual Meeting of the Association for Computational Linguistics (ACL-04)<address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004-07">2004. July</date>
			<biblScope unit="page" from="143" to="150" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Random features for large-scale kernel machines</title>
		<author>
			<persName><forename type="first">Ali</forename><surname>Rahimi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Recht</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Platt</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Koller</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><surname>Singer</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><forename type="middle">T</forename><surname>Roweis</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="1177" to="1184" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Curran</forename><surname>Associates</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Inc</forename></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Supervised models for coreference resolution</title>
		<author>
			<persName><forename type="first">Altaf</forename><surname>Rahman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2009 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2009-08">2009. August</date>
			<biblScope unit="page" from="968" to="977" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Integrating predictions from neural-network relation classifiers into coreference and bridging resolution</title>
		<author>
			<persName><forename type="first">Ina</forename><surname>Roesiger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maximilian</forename><surname>KÃ¶per</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kim</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Anh</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sabine</forename><surname>Schulte Im Walde</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First Workshop on Computational Models of Reference, Anaphora and Coreference</title>
		<meeting>the First Workshop on Computational Models of Reference, Anaphora and Coreference<address><addrLine>New Orleans, Louisiana</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018-06">2018. June</date>
			<biblScope unit="page" from="44" to="49" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">BASHI: A corpus of wall street journal articles annotated with bridging links</title>
		<author>
			<persName><forename type="first">Ina</forename><surname>RÃ¶siger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018)</title>
		<meeting>the Eleventh International Conference on Language Resources and Evaluation (LREC 2018)<address><addrLine>Miyazaki, Japan</addrLine></address></meeting>
		<imprint>
			<publisher>European Language Resources Association (ELRA</publisher>
			<date type="published" when="2018-05">2018. May</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">WordNet embeddings</title>
		<author>
			<persName><forename type="first">Chakaveh</forename><surname>Saedi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">AntÃ³nio</forename><surname>Branco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">JoÃ£o AntÃ³nio</forename><surname>Rodrigues</surname></persName>
		</author>
		<author>
			<persName><forename type="first">JoÃ£o</forename><surname>Silva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The Third Workshop on Representation Learning for NLP</title>
		<meeting>The Third Workshop on Representation Learning for NLP<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018-07">2018. July</date>
			<biblScope unit="page" from="122" to="131" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Improving neural entity disambiguation with graph embeddings</title>
		<author>
			<persName><forename type="first">Ãzge</forename><surname>Sevgili</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Panchenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Biemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: Student Research Workshop</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics: Student Research Workshop<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-07">2019. July</date>
			<biblScope unit="page" from="315" to="322" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Incorporating domain knowledge into medical NLI using knowledge graphs</title>
		<author>
			<persName><forename type="first">Soumya</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bishal</forename><surname>Santra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhik</forename><surname>Jana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Santosh</forename><surname>Tokala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niloy</forename><surname>Ganguly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pawan</forename><surname>Goyal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-11">2019. November</date>
			<biblScope unit="page" from="6092" to="6097" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Towards a computational theory of definite anaphora comprehension in English discourse</title>
		<author>
			<persName><forename type="first">Candace</forename><forename type="middle">L</forename><surname>Sidner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1979">1979</date>
			<pubPlace>Cambridge, MA, USA</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Massachusetts Institute of Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Complex embeddings for simple link prediction</title>
		<author>
			<persName><forename type="first">Tho</forename><surname>Trouillon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Welbl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ric</forename><surname>Gaussier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Bouchard</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Annotating a broad range of anaphoric phenomena, in a variety of genres: The arrau corpus</title>
		<author>
			<persName><forename type="first">Olga</forename><surname>Uryupina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ron</forename><surname>Artstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antonella</forename><surname>Bristot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Federica</forename><surname>Cavicchio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francesca</forename><surname>Delogu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kepa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Massimo</forename><surname>Rodriguez</surname></persName>
		</author>
		<author>
			<persName><surname>Poesio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Natural Language Engineering</title>
		<imprint>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Verbs semantics and lexical selection</title>
		<author>
			<persName><forename type="first">Zhibiao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martha</forename><surname>Palmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd Annual Meeting on Association for Computational Linguistics, ACL 94</title>
		<meeting>the 32nd Annual Meeting on Association for Computational Linguistics, ACL 94</meeting>
		<imprint>
			<publisher>USA. Association for Computational Linguistics</publisher>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page">133138</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Relation embedding with dihedral group in knowledge graph</title>
		<author>
			<persName><forename type="first">Canran</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruijiang</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-07">2019. July</date>
			<biblScope unit="page" from="263" to="272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Embedding entities and relations for learning and inference in knowledge bases</title>
		<author>
			<persName><forename type="first">Bishan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen</forename><surname>Tau Yih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
