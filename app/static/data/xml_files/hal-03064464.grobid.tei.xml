<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Attention-Based Recurrent Neural Network for Plant Disease Classification</title>
				<funder ref="#_3xFSZJ8">
					<orgName type="full">Agence Nationale de la Recherche</orgName>
					<orgName type="abbreviated">ANR</orgName>
				</funder>
				<funder>
					<orgName type="full">Agropolis Fondation, Numev, Cemeb</orgName>
				</funder>
				<funder ref="#_7QhKwRQ">
					<orgName type="full">DigitAG</orgName>
				</funder>
				<funder ref="#_7UtQrA8">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2020-12-14">14 December 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">John</forename><surname>Louis</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Van</forename><surname>Hemert</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Michael</forename><surname>Gomez Selvaraj</surname></persName>
						</author>
						<author role="corresp">
							<persName><forename type="first">Pierre</forename><surname>Bonnet</surname></persName>
							<email>pierre.bonnet@cirad.fr</email>
							<affiliation key="aff4">
								<orgName type="laboratory" key="lab1">AMAP</orgName>
								<orgName type="laboratory" key="lab2">CIRAD</orgName>
								<orgName type="institution" key="instit1">Univ Montpellier</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
								<orgName type="institution" key="instit3">INRA</orgName>
							</affiliation>
							<affiliation key="aff5">
								<orgName type="institution">IRD</orgName>
								<address>
									<settlement>Montpellier</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff6">
								<orgName type="laboratory">CIRAD</orgName>
								<orgName type="institution">UMR AMAP</orgName>
								<address>
									<settlement>Montpellier</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hervé</forename><surname>Goëau</surname></persName>
							<affiliation key="aff4">
								<orgName type="laboratory" key="lab1">AMAP</orgName>
								<orgName type="laboratory" key="lab2">CIRAD</orgName>
								<orgName type="institution" key="instit1">Univ Montpellier</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
								<orgName type="institution" key="instit3">INRA</orgName>
							</affiliation>
							<affiliation key="aff5">
								<orgName type="institution">IRD</orgName>
								<address>
									<settlement>Montpellier</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff6">
								<orgName type="laboratory">CIRAD</orgName>
								<orgName type="institution">UMR AMAP</orgName>
								<address>
									<settlement>Montpellier</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Alexis</forename><surname>Joly</surname></persName>
							<affiliation key="aff7">
								<orgName type="laboratory">LIRMM -UMR 5506</orgName>
								<orgName type="institution">INRIA Sophia-Antipolis -ZENITH Team</orgName>
								<address>
									<settlement>Montpellier</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Sue</forename><forename type="middle">Han</forename><surname>Lee</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">Swinburne University of Technology Sarawak Campus</orgName>
								<address>
									<settlement>Kuching</settlement>
									<country key="MY">Malaysia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Agricultural University of Athens</orgName>
								<address>
									<country key="GR">Greece</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Corteva Agriscience TM</orgName>
								<address>
									<country key="US">United States</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="laboratory">Consultative Group on International Agricultural Research (CGIAR)</orgName>
								<address>
									<country key="US">United States</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Attention-Based Recurrent Neural Network for Plant Disease Classification</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2020-12-14">14 December 2020</date>
						</imprint>
					</monogr>
					<idno type="MD5">C863CCD96D0B755363506273BFBAD2FC</idno>
					<idno type="DOI">10.3389/fpls.2020.601250</idno>
					<note type="submission">This article was submitted to Technical Advances in Plant Science, a section of the journal Frontiers in Plant Science Received: 31 August 2020 Accepted: 09 November 2020</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-04-12T14:54+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>plant disease classification</term>
					<term>deep learning</term>
					<term>recurrent neural network</term>
					<term>automated visual crops analysis</term>
					<term>precision agriculture technologies</term>
					<term>crops monitoring</term>
					<term>pests analysis</term>
					<term>smart farming</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Plant diseases have a significant impact on global food security and the world's agricultural economy. Their early detection and classification increase the chances of setting up effective control measures, which is why the search for automatic systems that allow this is of major interest to our society. Several recent studies have reported promising results in the classification of plant diseases from RGB images on the basis of Convolutional Neural Networks (CNN). These studies have been successfully experimented on a large number of crops and symptoms, and they have shown significant advantages in the support of human expertise. However, the CNN models still have limitations. In particular, CNN models do not necessarily focus on the visible parts affected by a plant disease to allow their classification, and they can sometimes take into account irrelevant backgrounds or healthy plant parts. In this paper, we therefore develop a new technique based on a Recurrent Neural Network (RNN) to automatically locate infected regions and extract relevant features for disease classification. We show experimentally that our RNN-based approach is more robust and has a greater ability to generalize to unseen infected crop species as well as to different plant disease domain images compared to classical CNN approaches. We also analyze the focus of attention as learned by our RNN and show that our approach is capable of accurately locating infectious diseases in plants. Our approach, which has been tested on a large number of plant species, should thus contribute to the development of more effective means of detecting and classifying crop pathogens in the near future.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Plant diseases are a major threat to agricultural production, causing severe food recessions and affecting crop quality <ref type="bibr" target="#b2">(Bhange and Hingoliwala, 2015)</ref>. To detect plant diseases in crops, plant pathologists generally use molecular and serological methods or measurements of various parameters, such as morphological change, temperature change, change in transpiration rate, or volatile organic compound emission from infected plants <ref type="bibr" target="#b7">(Fang et al., 2015)</ref>. Although it is an effective means of controlling plant diseases, consulting experts is nonetheless a costly and time-consuming process, especially since it is not always easy to bring an expert in time before the disease spreads to the crops. In recent years, automated classification of plant diseases has been addressed by the computer vision community to compensate for the lack of human expertise. Researchers used deep learning techniques to automatically identify diseases in individual crops, such as banana <ref type="bibr" target="#b23">(Selvaraj et al., 2019)</ref>, coffee <ref type="bibr" target="#b13">(Kumar et al., 2020)</ref>, grape <ref type="bibr">(Liu et al., 2020)</ref>, cassava <ref type="bibr" target="#b20">(Ramcharan et al., 2017)</ref>, tomato <ref type="bibr" target="#b6">(Durmuş et al., 2017;</ref><ref type="bibr" target="#b9">Fuentes et al., 2017;</ref><ref type="bibr">Liu and Wang, 2020)</ref>, and apple <ref type="bibr" target="#b17">(Liu et al., 2017)</ref>, as well as in multi-crops <ref type="bibr" target="#b19">(Mohanty et al., 2016;</ref><ref type="bibr" target="#b8">Ferentinos, 2018;</ref><ref type="bibr" target="#b28">Too et al., 2018)</ref>. In most cases, researchers fine-tune off-the-shelf Convolutional Neural Networks (CNNs) <ref type="bibr" target="#b22">(Saleem et al., 2019)</ref>.</p><p>Although the evaluated CNN methods in these publications appear to be effective and seem to learn relevant feature representations of the diseases, they unfortunately also learn irrelevant disease characteristics such as background noise <ref type="bibr" target="#b19">(Mohanty et al., 2016;</ref><ref type="bibr" target="#b1">Atabay, 2017)</ref> or uninfected plant parts <ref type="bibr" target="#b8">(Ferentinos, 2018;</ref><ref type="bibr" target="#b27">Toda and Okura, 2019;</ref><ref type="bibr" target="#b15">Lee et al., 2020)</ref>. For example, <ref type="bibr" target="#b1">(Atabay, 2017)</ref> has shown that a CNN trained on tomato plant diseases has neuron activations that fall mostly in the background. Unfortunately, it has been shown that background suppression with image segmentation does not give better results than an ordinary colored background with CNN <ref type="bibr" target="#b19">(Mohanty et al., 2016)</ref>, confirming a dependence of background characteristics for disease identification. Even worse, <ref type="bibr" target="#b8">Ferentinos (2018)</ref> showed that a CNN tends to be confused between similar crops of different disease classes. It is thus indicated that a CNN model, which is supposed to learn the visual representation of plant diseases, tends to be biased toward irrelevant crop characteristics. Region-based deep neural networks can help to focus on contaminated parts <ref type="bibr" target="#b9">(Fuentes et al., 2017</ref><ref type="bibr" target="#b10">(Fuentes et al., , 2019))</ref>, but such a technique involves labor-intensive annotations of disease locations and also depends heavily on prior knowledge of plant diseases.</p><p>Henceforth, these observations have motivated us to go beyond existing practices by exploring a new technique for identifying plant diseases that allows us to automatically learn the regions of interest in the plant image, which correspond to the infected regions, and then to identify the diseases. Inspired by recent work on multi-organ plant identification that has shown the ability of an attention-based Recurrent Neural Network (RNN) to locate relevant regions of plant structures without any prior human annotation <ref type="bibr" target="#b14">(Lee et al., 2018)</ref>, we have adapted this approach to learn visual representations of plant diseases and show that discriminating infected regions of a plant can be successfully located and highlighted for disease identification, as illustrated in Figure <ref type="figure" target="#fig_0">1</ref>.</p><p>Our contribution in this paper is three-fold: firstly, to our knowledge, this is the first time that the RNN-based approach is being explored to learn representations of plant diseases at that scale and that a comparison of identification performance is made against the widely used CNN approaches in this field. Second, we show quantitatively that the RNN approach outperforms the CNN approaches. Finally, we also show qualitatively that the RNN approach is able to detect precisely the infected regions through the neuron activations. The source code of our computational implementation is provided on an open online repository to facilitate its long-term accessibility and use by the scientific community 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">MATERIALS AND METHODS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Attention-Based RNN Model</head><p>A recurrent neural network (RNN) is a class of neural networks where connections between nodes of a layer form a directed graph along a sequence of variables (e.g., a temporal sequence). The recurrent connections typically allow for modeling of the relationship between the current state of a variable and the previous states (similarly to a Markov chain). The RNN-based approach has received much attention because of its ability to handle sequential data to make predictions, such as in language translation <ref type="bibr" target="#b25">(Sutskever et al., 2014)</ref> or action recognition <ref type="bibr" target="#b5">(Du et al., 2018;</ref><ref type="bibr" target="#b24">Song et al., 2018)</ref>. Improved RNN models, such as Long Short-Term Memory networks (LSTMs) or Gated Recurrent Units (GRU), enable training on long sequences, overcoming problems like vanishing gradients. Recently, a few publications have shown the effectiveness of RNN approaches to sequentially process variable-length data of fixed sizes, such as a picture. For example, it has been shown that a RNN architecture based on GRU can efficiently model dependencies between different images of plant observations <ref type="bibr" target="#b14">Lee et al. (2018)</ref> or that LSTM can be used to capture discriminating regions of images for fine-grained classification <ref type="bibr" target="#b29">(Zhao et al., 2017)</ref>. Attention is a mechanism that can be combined in the RNN to allow it to focus on certain parts of the input when predicting a certain part of the output, thus enabling an easier learning and of higher quality. For instance, RNN with attention mechanism was used in <ref type="bibr" target="#b21">Ren and Zemel (2017)</ref> to capture the spatial structure in images and produce detailed instance segmentation.</p><p>Inspired by these previous works, we combine in a RNN an attention mechanism with Gated Recurrent Units to dynamically push salient plant disease characteristics to the forefront in order to strengthen the model in learning disease characteristics for identification. Figure <ref type="figure" target="#fig_2">2</ref> shows the framework of the proposed architecture. First, a CNN trained on a plant disease classification task is used as a visual features extractor: a plant image is thus encoded as CNN features (i.e., a tensor of feature maps at a given output of a chosen convolutional layer). These CNN features can be considered as a new smaller image of activations with as many channels as filters used in the convolution layer. This new image is then sliced into sub-parts of the same size to get local activations in many regions covering all of the image. These new local CNN features can then be used to build a sequence and feed an RNN based on GRUs, enabling an attention mechanism to locate important parts or components in the CNN features. It extends the effective pixel neighborhood in each sub-part and maximizes the information gain across several sub-parts of the CNN features. Finally, prediction error is minimized throughout the optimization process.</p><p>1 the web link to the open online repository on which the source code is hosted will be displayed here after acceptance of the manuscript.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.1.">Our Overall RNN Architecture</head><p>We denote a plant disease image as I and the corresponding feature maps extracted by the convolutional layers of the CNN as δ ∈ R H×W×C , where H, W and C are, respectively, the height, width, and number of channels in the feature maps. The CNN model is initially pre-trained and optimized purely based on plant disease target classes. A sequence of T regional feature maps {δ 1 , δ 2 , • • • , δ T } ∈ δ is then generated by slicing the global feature map δ following the sliding direction shown in Figure <ref type="figure" target="#fig_2">2</ref>. The resulting sequence of feature maps is then used as input of the RNN module displayed in Figure <ref type="figure" target="#fig_2">2A</ref> and detailed in Figure <ref type="figure" target="#fig_2">2B</ref>. Thanks to the RNN connections between the different feature maps, the network is able to iteratively learn the discriminant visual patterns and to model the spatial relationship between them. For instance, brown specks that spread from side to side on a part of the leaf can be distinguished from brown specks that appear randomly on the leaf.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.2.">Attention Module</head><p>The attention module is used to model the relative contribution of each pixel of the T regional feature maps. Specifically, it forces an explicit additional step in the reasoning process, identifying salient regions by assigning different importance to features from different image regions. The attention mechanism is introduced by the λ t terms (also called regional attention map) that control the contribution of the pixels of the t-th state and that are trained by the neural network. A larger λ t value indicates higher importance. More formally, the attention function g : δ t , h t-1 → ǫ t is defined as follows:</p><formula xml:id="formula_0">ζ t = {tanh(δ t W δ + h t-1 W h )}W a</formula><p>(1)</p><formula xml:id="formula_1">λ t = softmax(ζ t ) (2) ǫ t = g(δ t , h t-1 ) = i,j λ t,ij .δ t,ij<label>(3)</label></formula><p>where</p><formula xml:id="formula_2">W δ ∈ R C×C , W h ∈ R E×C , W a ∈ R C×1</formula><p>are the embedding matrices, E is the dimensionality of GRU cell, and δ t,ij denotes the value of the t-th regional feature map at position (i, j) ∈ H ′ × W ′ . Note that ǫ t is the output representation (feature vector) for the t-th regional feature map.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.3.">Bi-Directional Training</head><p>Inspired by previous experiments <ref type="bibr" target="#b14">(Lee et al., 2018)</ref> that show that bidirectional states modeling performs better compared to uni-state modeling in plant-view correlation learning, we built a bidirectional states modeling mechanism where the forward neuron activations -→ h T and the backward neuron activations ←h 0 model P(h t |δ t , h 0 , • • • , h T-1 ) and P(h t |δ t , h T , • • • , h 1 ), respectively. In order to correlate between both states, the final output activations of the forward and backward GRU are cascaded as follows:</p><formula xml:id="formula_3">h = [ -→ h T , ← - h 0 ]</formula><p>. We then multiply h with a class embedding matrix, W em , which is s(I) = W em h before normalizing it with a softmax function: P(r|I) = where M and r stand for the total number of classes and the target class, respectively. After performing the softmax operation, we find the maximum likelihood of the sample by applying the objective function, L = -logP(r|I).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.4.">Implementation Details</head><p>To extract the CNN features, we used an extension of a GoogleNet architecture <ref type="bibr" target="#b26">Szegedy et al. (2015)</ref> with modified  convolutional layers and additional batch normalization to increase accuracy and reduce computational complexity 2 . After training the CNN on the disease classification task, we extracted CNN features with a size of 14 × 14 × 576 from the convolutional layer Inception_4d and sliced them with a stride of 1 into regional patches with a size of H ′ = W ′ = 8 each to finally feed the RNN. The RNN is trained using the Tensorflow library <ref type="bibr" target="#b0">(Abadi et al., 2016)</ref>. We use the ADAM optimizer <ref type="bibr" target="#b12">(Kingma and Ba, 2014)</ref> with the parameters α = 1e -08, β1 = 0.9, and β2 = 0.999. We applied the weight decay L 2 with the penalty multiplier set to 1 ×10 -4 and dropout ratio set to 0.5, respectively. We set the learning rate to 1 ×10 -4 , and the mini batch size was set to 30.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.5.">Features Visualization Method</head><p>We describe here the methodology used to visualize the visual features captured by the CNN and the RNN model (see Figures <ref type="figure" target="#fig_3">3</ref><ref type="figure" target="#fig_4">4</ref><ref type="figure" target="#fig_5">5</ref>of the results section). For the CNN model (GoogleNet), we first tracked the position of the highest activation across all the feature maps extracted from the last convolutional layer. From this, we accumulated the first 30 dominant activations and assessed them according to the original image. For the RNN model, we simply displayed a subset of the regional attention maps λ t .</p><p>2 https://github.com/AdelineMomo/CNN-plant-disease (Caffe).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Experimental Dataset</head><p>Plant Village (PV) <ref type="bibr" target="#b11">(Hughes and Salathé, 2015)</ref> is a popular dataset dedicated to the evaluation of automated identification of plant diseases under controlled environments. It has 38 crop-disease pairs, with 26 crop-disease categories concerning 14 crop plants. The dataset was provided with predefined training and test sets, and a configuration with a percentage ratio of 80 and 20% is used, giving a total of 10,495 training images and 4,310 test images. Since plant diseases, named by vernacular names, share the same visual characteristics for different species, we categorized leaf samples from the PV dataset into 21 classes (20 diseases and one healthy class) and trained a classifier based on these classes. Note that, in practice, it is impossible to collect all disease samples from different crops under different environments to train a deep model. In fact, what we intended to achieve is a model that is general enough to represent knowledge in a way that can be transferred between different plant disease tasks. We therefore explored the generalization of models to identify disease of unseen crops and also plants that are captured under different contexts (typically in the field).</p><p>To evaluate the ability of the model to generalize to unseen crops, we excluded one crop from the training set and use it only for testing. More specifically, we removed all the leaf samples from the pepper crop, i.e., the ones from the Pepper_bell Bacterial_spot class and the ones from the Pepper_bell healthy, so as to treat the pepper class as an unseen crop in this experiment  while the Bacterial_spot can be learnt through other crops.</p><p>Besides the PV dataset, we also assessed the robustness of our models using pictures related to the same disease categorization and from reliable online sources that are not restricted to a controlled environment. We used 119 and 64 images from IPM and Bing, respectively, collected by <ref type="bibr" target="#b19">Mohanty et al. (2016)</ref> (121 number of Bing images were expected, but half of them are no longer available).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">RESULTS AND DISCUSSION</head><p>The experimental results are presented in the Table <ref type="table" target="#tab_0">1</ref>. As there are only few images of pepper crops in IPM and Bing (two images in IPM and one in Bing), we considered the number of collections insufficient to infer the performance of the models in recognizing the disease of an unseen crop. We therefore tested the model using only the seen crop images in IPM and Bing. We compared the performance of the models with the Inception-V3 model employed by <ref type="bibr" target="#b3">Brahimi et al. (2018)</ref> since it was reported to be the best approach for disease identification on the PV dataset.</p><p>The model is trained on the basis of the aforementioned 21 target classes with the pepper crop excluded from the training set. First, by examining the accuracy of the PV-SC test set (seen crops from the PlantVillage dataset), we can see that both CNN models and our new attention-based RNN achieved very high accuracy values. This is mainly due to the fact that the images in this test set were acquired under exactly the same conditions as the training set so that any method can exhibit a high performance. Secondly, we can observe that both CNN models as well as our new attention-based RNN achieved lower accuracy values for the IPM-SC and Bing-SC test sets as well as the PV-UCB (unseen crop from the PlantVillage dataset) compared to the PV-SC. We believe that this is due to a change in the data distribution between the PV data and the IPM-SC and BING-SC test sets, as the two data are collected under different conditions, and the disease training data are not sufficiently diverse to cover the ranges of visual appearance of the disease found in the unseen crop. To cope with such difficult datasets, CNN models achieved a much lower performance, which means that the models formed have some difficulty in generalizing to the unseen crop of PlantVillage (PV-USC) or to images acquired in a different domain (IPM-SC and Bing-SC). In comparison, our attention-based RNN model was much more accurate over the three series of tests. Although the accuracy is far from perfect, it is still quite reasonable considering the difficulty of the problem.</p><p>We further compared the generalization ability of our new model and that of a classical CNN thanks to visualization experiments presented in Figures <ref type="figure" target="#fig_3">3,</ref><ref type="figure" target="#fig_4">4</ref>. As shown in Figure <ref type="figure" target="#fig_3">3</ref>, the regions of the image leading to a strong activation of the CNN neurons do not really seem to correspond to the visual patterns characteristic of the disease. Indeed, they largely correspond to healthy leaf features, specifically the venation. On the contrary, Figure <ref type="figure" target="#fig_4">4</ref> shows that the activated regions of the attention maps of our new RNN model do match the disease spots much more precisely and accurately. Figure <ref type="figure" target="#fig_5">5</ref> shows a similar visualization on two leaves of the Pepper_bell crop affected by Bacterial_spot.</p><p>Here again, we can observe that most of the activated regions do correspond to altered parts of the leaf, whereas the Pepper_bell was not even present in the training set.</p><p>In line with our results, we deduced that the transferability of Seq-RNN knowledge is more useful than that of CNN in differentiating data, especially those taken in the field. This could be due to the fact that CNN is learned on the basis that its spatial information collapses in the final convolutional layer, resulting in the relativity of local features that are important for representing diseases not retained. This might be also the reason the CNN searches for global characteristics that are not relevant for the infected area but are relevant for leaf characteristics, such as shape and venation.</p><p>On the other hand, Seq-RNN takes the convolutional map, where substantial spatial information is retained as input and is formulated in such a way as to learn the relationship between the neighboring regions of an image. Because of this formulation, the Seq-RNN's attention mechanism will be forced to detect salient regions, where, in this case, there are the obvious infected regions that appear in the local areas of an image. This disease-focused knowledge could be effectively transferred to field images where the plant structure may not be easily visible (due to organ deformation or clutter) but the visual aspect of the disease is evident. Thus, in field images where the plant structure is not visible or may be difficult to distinguish, the disease characteristics become more determinant, allowing Seq-RNN, which has a greater knowledge of the disease, to better differentiate the data than the CNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">CONCLUSIONS AND FUTURE DIRECTIONS</head><p>We presented in this study a new efficient computational architecture that opens up new perspectives for the automated classification of plant diseases. We showed that our RNN approach has a higher generalization ability than the classical CNN approach, especially in distinguishing disease samples that are different from the training set. This is a major critical point in plant pathology, as it is highly difficult (due to required expertise level and time consumed) to produce a complete and diversified visual dataset of all the symptoms of any crop disease at a global scale. Our approach could thus overcome the problems related to the lack of training data usually necessary for the development of performing recognition deep learning models.</p><p>In this study we also analyzed the attention maps learned by our RNN and showed that it meets our expectation of localization of infectious diseases in plants images. We believe that our RNNbased approach, which captures the context of the relationship between local features, could provide a better insight into where the machine actually detects relevant information. It is important to note that by assessing the machine's perspective in disease differentiation, humans will certainly benefit, as we all know that human capabilities are limited when it comes to identifying the thousands of diseases for all the plants in the world.</p><p>In this paper, we have shown that the integration of our RNN approach in the design of the deep learning architecture for learning disease representation can not only provide a better classification performance but also contribute to the knowledge of plant diseases, which could potentially be useful to address doubts that have not yet been resolved. Future research directions should focus on the integration of CNN and RNN based models to simultaneously address the learning of rich global and local visual representations within a deep end-to-end network. Furthermore, alternative slicing patterns on convolutional maps before transmitting the extracted patches to RNN should be studied for a better modeling of the local characteristics of the plant disease. In addition, complementary statistical analyses should be performed on the evaluation of the RNN approach for field images that are of main interest to farmers and all field agricultural actors. We believe that these findings could encourage future research to rethink the current de facto paradigm of purely relying on the CNN in plant disease identification.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>FIGURE 1 |</head><label>1</label><figDesc>FIGURE 1 | The learned attention maps by our proposed approach on two diseases that have contaminated leaves. The visualizations highlight the infected regions. (A) Powdery mildew disease, on cherry plant. (B) Cedar apple rust disease, on apple.</figDesc><graphic coords="3,56.64,430.75,481.92,234.96" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>FIGURE 2 |</head><label>2</label><figDesc>FIGURE 2 | The proposed architecture: from an image of a contaminated plant, feature maps are first extracted from a given convolutional layer of a pre-trained CNN. They are then sliced into several patches following a "snaking" sliding direction. The patches then feed into Gated Recurrent Units that share, combine, and retain relevant information in a bidirectional way to update an internal representation of plant disease. Soft attention mechanism is used to infer discriminating local features. (A) Overall architecture. (B) Soft attention mechanism.</figDesc><graphic coords="5,56.64,69.47,481.92,529.20" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>FIGURE 3 |</head><label>3</label><figDesc>FIGURE 3 | Visualization of activations of the learned features with the CNN model (GoogleNet). Best viewed in color.</figDesc><graphic coords="6,156.14,69.80,283.44,146.88" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>FIGURE 4 |</head><label>4</label><figDesc>FIGURE 4 | Visualization of attention maps learned using our attention-based RNN on a leaf infected by the Leaf mold disease (to be compared with Figure3). Best view in color.</figDesc><graphic coords="6,56.64,264.87,481.92,95.52" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>FIGURE 5 |</head><label>5</label><figDesc>FIGURE 5 | Visualization of attention maps learned using our new attention-based RNN model on two correctly classified images of the Unseen Crop of pepper_bell Bacterial_spot.</figDesc><graphic coords="7,56.64,69.43,481.92,204.24" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE 1 |</head><label>1</label><figDesc>Top-1 accuracy (%) comparison on Plant Village (PV), IPM, and Bing.</figDesc><table><row><cell>2018)</cell><cell>98.05 29.63</cell><cell>29.06</cell><cell>28.57</cell></row><row><cell>Our CNN (GoogleNet)</cell><cell>99.17 18.98</cell><cell>37.61</cell><cell>36.51</cell></row><row><cell>Our new model Seq-RNN</cell><cell>98.17 58.80</cell><cell>40.17</cell><cell>39.68</cell></row></table><note><p><p><p><p><p>Method</p>PV-SC PV-UCB IPM-SC Bing-SC</p>CNN InceptionV3 of Brahimi et al. (</p>Note that the performance of IPM and Bing are based only on Seen Crop (SC) images, whereas the performance on PV is measured for both Seen Crop (PV-SC) and Unseen Crop (PV-UCB), i.e., for the pepper_bell Bacterial_spot.</p>The best accuracy for each dataset is shown in bold.</p></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>Frontiers in Plant Science | www.frontiersin.org</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_1"><p>December 2020 | Volume 11 | Article 601250</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_2"><p>Frontiers in Plant Science | www.frontiersin.org 4 December 2020 | Volume 11 | Article 601250</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_3"><p>Frontiers in Plant Science | www.frontiersin.org 5 December 2020 | Volume 11 | Article 601250</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_4"><p>Frontiers in Plant Science | www.frontiersin.org 8 December 2020 | Volume 11 | Article 601250</p></note>
		</body>
		<back>

			<div type="funding">
<div><head>FUNDING</head><p>This project was supported by <rs type="funder">Agropolis Fondation, Numev, Cemeb</rs>, #<rs type="funder">DigitAG</rs>, under the reference <rs type="grantNumber">ID 1604-019</rs> through the <rs type="programName">Investissements d'avenir programme</rs> (<rs type="projectName">Labex Agro</rs>: <rs type="grantNumber">ANR-10-LABX-0001-01</rs>). This work was also supported by the <rs type="funder">French National Research Agency</rs> under the <rs type="programName">Investments for the Future Program</rs>, referred as <rs type="grantNumber">ANR-16-CONV-0004</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_7QhKwRQ">
					<idno type="grant-number">ID 1604-019</idno>
					<orgName type="project" subtype="full">Labex Agro</orgName>
					<orgName type="program" subtype="full">Investissements d&apos;avenir programme</orgName>
				</org>
				<org type="funding" xml:id="_3xFSZJ8">
					<idno type="grant-number">ANR-10-LABX-0001-01</idno>
					<orgName type="program" subtype="full">Investments for the Future Program</orgName>
				</org>
				<org type="funding" xml:id="_7UtQrA8">
					<idno type="grant-number">ANR-16-CONV-0004</idno>
				</org>
			</listOrg>

			<div type="availability">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DATA AVAILABILITY STATEMENT</head><p>Publicly available datasets were analyzed in this study. This data can be found here: https://doi.org/10.3389/fpls.2016.01419 <ref type="bibr" target="#b19">(Mohanty et al., 2016)</ref> and https://arxiv.org/abs/1511.08060 <ref type="bibr" target="#b11">(Hughes and Salathé, 2015)</ref>.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>AUTHOR CONTRIBUTIONS</head><p>SL and HG conducted experiments and data analysis. SL wrote the first draft of the manuscript. HG, PB, and AJ revised the manuscript. All authors designed the research, contributed to the article and approved the submitted version.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conflict of Interest:</head><p>The authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Tensorflow: a system for large-scale machine learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">OSDI</title>
		<meeting><address><addrLine>Savannah, GA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="265" to="283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Deep residual learning for tomato plant leaf disease identification</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">A</forename><surname>Atabay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Theoret. Appl. Inform. Technol</title>
		<imprint>
			<biblScope unit="volume">95</biblScope>
			<biblScope unit="page" from="6800" to="6808" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Smart farming: pomegranate disease detection using image processing</title>
		<author>
			<persName><forename type="first">M</forename><surname>Bhange</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hingoliwala</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.procs.2015.08.022</idno>
	</analytic>
	<monogr>
		<title level="j">Proc. Comput. Sci</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="page" from="280" to="288" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deep learning for plant diseases: detection and saliency map visualisation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Brahimi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Arsenovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Laraba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sladojevic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Boukhalfa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Moussaoui</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-90403-0_6</idno>
	</analytic>
	<monogr>
		<title level="m">Human and Machine Learning</title>
		<editor>
			<persName><forename type="first">J</forename><surname>Zhou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>Chen</surname></persName>
		</editor>
		<meeting><address><addrLine>Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="93" to="117" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title/>
		<idno type="DOI">10.1007/978-3-319-90403-0_6</idno>
		<imprint>
			<biblScope unit="page" from="0" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Recurrent spatial-temporal attention network for action recognition in videos</title>
		<author>
			<persName><forename type="first">W</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<idno type="DOI">10.1109/TIP.2017.2778563</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="1347" to="1360" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Disease detection on the leaves of the tomato plants by using deep 1learning</title>
		<author>
			<persName><forename type="first">H</forename><surname>Durmuş</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">O</forename><surname>Güneş</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kırcı</surname></persName>
		</author>
		<idno type="DOI">10.1109/Agro-Geoinformatics.2017.8047016</idno>
	</analytic>
	<monogr>
		<title level="m">2017 6th International Conference on Agro-Geoinformatics</title>
		<meeting><address><addrLine>Fairfax, VA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
	<note>: IEEE)</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Current and prospective methods for plant disease detection</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">P</forename><surname>Ramasamy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">C</forename><surname>Rains</surname></persName>
		</author>
		<idno type="DOI">10.3390/bios5030537</idno>
	</analytic>
	<monogr>
		<title level="j">Biosensors</title>
		<editor>
			<persName><forename type="first">G</forename><forename type="middle">C</forename><surname>Rains</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="537" to="561" />
			<date type="published" when="2015">2015</date>
			<publisher>MDPI</publisher>
			<pubPlace>Basel</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep learning models for plant disease detection and diagnosis</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">P</forename><surname>Ferentinos</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.compag.2018.01.009</idno>
	</analytic>
	<monogr>
		<title level="j">Comput. Electron. Agric</title>
		<imprint>
			<biblScope unit="volume">145</biblScope>
			<biblScope unit="page" from="311" to="318" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A robust deep-learningbased detector for real-time tomato plant diseases and pests recognition</title>
		<author>
			<persName><forename type="first">A</forename><surname>Fuentes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Park</surname></persName>
		</author>
		<idno type="DOI">10.3390/s17092022</idno>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<date type="published" when="2017">2017. 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep learning-based phenotyping system with glocal description of plant anomalies and symptoms</title>
		<author>
			<persName><forename type="first">A</forename><surname>Fuentes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Park</surname></persName>
		</author>
		<idno type="DOI">10.3389/fpls.2019.01321</idno>
	</analytic>
	<monogr>
		<title level="j">Front. Plant Sci</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">1321</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Hughes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Salathé</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.08060</idno>
		<ptr target="https://arxiv.org/abs/1511.08060" />
		<title level="m">An open access repository of images on plant health to enable the development of mobile disease diagnostics</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
	<note>preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<ptr target="https://arxiv.org/abs/1412.6980" />
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
	<note>preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Disease detection in coffee plants using convolutional neural network</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Madhav</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICCES48766.2020.9138000</idno>
	</analytic>
	<monogr>
		<title level="m">2020 5th International Conference on Communication and Electronics Systems (ICCES) (Coimbatore: IEEE)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="755" to="760" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Multi-organ plant classification based on convolutional and recurrent neural networks</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">S</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Remagnino</surname></persName>
		</author>
		<idno type="DOI">10.1109/TIP.2018.2836321</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="4287" to="4301" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">New perspectives on plant disease characterization based on deep learning</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Goëau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.compag.2020.105220</idno>
	</analytic>
	<monogr>
		<title level="j">Comput. Electron. Agric</title>
		<imprint>
			<biblScope unit="volume">170</biblScope>
			<biblScope unit="page">105220</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Grape leaf disease identification using improved deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.3389/fpls.2020.01082</idno>
	</analytic>
	<monogr>
		<title level="j">Front. Plant Sci</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">1082</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Identification of apple leaf diseases based on deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.3390/sym10010011</idno>
	</analytic>
	<monogr>
		<title level="j">Symmetry</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Tomato diseases and pests detection based on improved Yolo V3 convolutional neural network</title>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.3389/fpls.2020.00898</idno>
	</analytic>
	<monogr>
		<title level="j">Front. Plant Sci</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">898</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Using deep learning for image-based plant disease detection</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">P</forename><surname>Mohanty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Hughes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Salathé</surname></persName>
		</author>
		<idno type="DOI">10.3389/fpls.2016.01419</idno>
	</analytic>
	<monogr>
		<title level="j">Front. Plant Sci</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">1419</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deep learning for image-based cassava disease detection</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ramcharan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Baranowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mccloskey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Legg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Hughes</surname></persName>
		</author>
		<idno type="DOI">10.3389/fpls.2017.01852</idno>
	</analytic>
	<monogr>
		<title level="j">Front. Plant Sci</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">1852</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">End-to-end instance segmentation with recurrent attention</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2017.39</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)<address><addrLine>Honolulu, HI</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="21" to="26" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Plant disease detection and classification by deep learning</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Saleem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Potgieter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Arif</surname></persName>
		</author>
		<idno type="DOI">10.3390/plants8110468</idno>
	</analytic>
	<monogr>
		<title level="j">Plants</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">468</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Ai-powered banana diseases and pest detection</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">G</forename><surname>Selvaraj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vergara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ruiz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Safari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Elayabalan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Ocimati</surname></persName>
		</author>
		<idno type="DOI">10.1186/s13007-019-0475-z</idno>
	</analytic>
	<monogr>
		<title level="j">Plant Methods</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page">92</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Spatio-temporal attentionbased LSTM networks for 3d action recognition and detection</title>
		<author>
			<persName><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.1109/TIP.2018.2818328</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="3459" to="3471" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<meeting><address><addrLine>Montreal, QC</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2015.7298594</idno>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting><address><addrLine>Boston, MA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">How convolutional neural networks diagnose plant disease</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Toda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Okura</surname></persName>
		</author>
		<idno type="DOI">10.1155/2019/9237136</idno>
	</analytic>
	<monogr>
		<title level="j">Plant Phenomics</title>
		<imprint>
			<biblScope unit="page">9237136</biblScope>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A comparative study of fine-tuning deep learning models for plant disease identification</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">C</forename><surname>Too</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yujian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Njuki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yingchun</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.compag.2018.03.032</idno>
	</analytic>
	<monogr>
		<title level="j">Comput. Electron. Agric</title>
		<imprint>
			<biblScope unit="volume">161</biblScope>
			<biblScope unit="page" from="272" to="279" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Diversified visual attention networks for fine-grained object classification</title>
		<author>
			<persName><forename type="first">B</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<idno type="DOI">10.1109/TMM.2017.2648498</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Multimedia</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="1245" to="1256" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
