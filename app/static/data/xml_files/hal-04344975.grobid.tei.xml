<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SEM@K: IS MY KNOWLEDGE GRAPH EMBEDDING MODEL SEMANTIC-AWARE?</title>
				<funder ref="#_gcHcb25">
					<orgName type="full">AILES</orgName>
				</funder>
				<funder>
					<orgName type="full">CNRS</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2023-12-07">7 Dec 2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Nicolas</forename><surname>Hubert</surname></persName>
							<email>nicolas.hubert@univ-lorraine.fr</email>
							<idno type="ORCID">0000-0002-4682-422X</idno>
						</author>
						<author>
							<persName><forename type="first">Pierre</forename><surname>Monnin</surname></persName>
							<email>pierre.monnin@inria.fr</email>
						</author>
						<author>
							<persName><forename type="first">Armelle</forename><surname>Brun</surname></persName>
							<email>armelle.brun@loria.fr</email>
						</author>
						<author>
							<persName><forename type="first">Davy</forename><surname>Monticolo</surname></persName>
							<email>davy.monticolo@univ-lorraine.fr</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="laboratory">LORIA</orgName>
								<orgName type="institution" key="instit1">Université de Lorraine</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Université de Lorraine</orgName>
								<address>
									<settlement>ERPI</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution" key="instit1">Université Côte d&apos;Azur</orgName>
								<orgName type="institution" key="instit2">Inria</orgName>
								<orgName type="institution" key="instit3">CNRS</orgName>
								<address>
									<region>I3S</region>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution" key="instit1">Université de Lorraine</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
								<orgName type="institution" key="instit3">LORIA</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="institution">Université de Lorraine</orgName>
								<address>
									<settlement>ERPI</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">SEM@K: IS MY KNOWLEDGE GRAPH EMBEDDING MODEL SEMANTIC-AWARE?</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2023-12-07">7 Dec 2023</date>
						</imprint>
					</monogr>
					<idno type="MD5">3394A20E3E4A4050BB572498100ACFF6</idno>
					<idno type="DOI">10.3233/SW-233508</idno>
					<idno type="arXiv">arXiv:2301.05601v2[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-04-12T14:51+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Knowledge Graph Embeddings</term>
					<term>Link Prediction</term>
					<term>Model Evaluation</term>
					<term>Semantic-Oriented Metrics</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Using knowledge graph embedding models (KGEMs) is a popular approach for predicting links in knowledge graphs (KGs). Traditionally, the performance of KGEMs for link prediction is assessed using rank-based metrics, which evaluate their ability to give high scores to ground-truth entities. However, the literature claims that the KGEM evaluation procedure would benefit from adding supplementary dimensions to assess. That is why, in this paper, we extend our previously introduced metric Sem@K that measures the capability of models to predict valid entities w.r.t. domain and range constraints. In particular, we consider a broad range of KGs and take their respective characteristics into account to propose different versions of Sem@K. We also perform an extensive study to qualify the abilities of KGEMs as measured by our metric. Our experiments show that Sem@K provides a new perspective on KGEM quality. Its joint analysis with rank-based metrics offers different conclusions on the predictive power of models. Regarding Sem@K, some KGEMs are inherently better than others, but this semantic superiority is not indicative of their performance w.r.t. rank-based metrics. In this work, we generalize conclusions about the relative performance of KGEMs w.r.t. rank-based and semantic-oriented metrics at the level of families of models. The joint analysis of the aforementioned metrics gives more insight into the peculiarities of each model. This work paves the way for a more comprehensive evaluation of KGEM adequacy for specific downstream tasks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>A knowledge graph (KG) is commonly seen as a directed multi-relational graph in which two nodes can be linked through potentially several semantic relationships. More formally, a knowledge graph KG = (E, R, T ) where E, R and T ⊆ E × R × E are a set of entities (nodes), relations (edge labels) and triples, respectively. A KG is represented as a collection of such triples -a.k.a. facts -denoted as (h, r, t) ∈ T where h ∈ E and t ∈ E are two entities of the graph and are respectively named the head and tail of the triple, while r ∈ R is a predicate that qualifies the nature of the relationship holding between these entities. For instance, in the sample KG depicted in Fig. <ref type="figure" target="#fig_0">1</ref>: E = {BarackObama,MichelleObama,EmmanuelMacron,USA,France} and R = {spouseOf,presidentOf,supports,livesIn}.</p><p>KGs are inherently incomplete, incorrect, or overlapping and thus major refinement tasks include entity matching, question answering, and link prediction <ref type="bibr" target="#b0">Paulheim [2017]</ref>, <ref type="bibr" target="#b1">Wang et al. [2017]</ref>. The latter is the focus of this paper. Link prediction (LP) aims at completing KGs by leveraging existing facts to infer missing ones. In the LP task, one is provided with a set of incomplete triples, where the missing head (resp. tail) needs to be predicted. This amounts to holding a set of triples T ′ where, for each triple, either the head h or the tail t is missing. This task can be subdivided into a head prediction phase -which consists in predicting the most plausible head h for each (?, r, t) -and a tail prediction phase -which consists in predicting the most plausible tail t for each (h, r, ?). In the sample KG depicted in Fig. <ref type="figure" target="#fig_0">1</ref>, an example of triple to be predicted during the tail prediction phase would be (EmmanuelMacron,livesIn,?), where the expected tail to be inferred is France. Training a Knowledge Graph Embedding Model (KGEM) firstly requires corrupting existing triples by replacing either their head h or their tail t with another entity to generate negative counterparts. This procedure is called negative sampling <ref type="bibr" target="#b2">Bordes et al. [2013]</ref>, <ref type="bibr" target="#b3">Krompaß et al. [2015]</ref>, <ref type="bibr">Jain et al. [2021a]</ref>.</p><p>Secondly, the KGEM iteratively learns to assign higher scores to true triples than to their negative counterparts.</p><p>The performance of KGEM for LP is evaluated using rank-based metrics such as Hits@K, Mean Rank (MR), and Mean Reciprocal Rank (MRR) that assess whether ground-truth entities are indeed given higher scores <ref type="bibr" target="#b1">Wang et al. [2017]</ref>, <ref type="bibr" target="#b5">Rossi et al. [2021]</ref>. However, various works recently raised some caveats about such metrics <ref type="bibr" target="#b6">Berrendorf et al. [2020]</ref>, <ref type="bibr" target="#b7">Hoyt et al. [2022]</ref>, <ref type="bibr" target="#b8">Tiwari et al. [2021]</ref>. For instance, they are not well-suited for drawing comparisons across datasets <ref type="bibr" target="#b6">Berrendorf et al. [2020]</ref>. More importantly, they only provide a partial picture of KGEM performance <ref type="bibr" target="#b6">Berrendorf et al. [2020]</ref>. Indeed, LP can lead to nonsensical triples, such as (BarackObama,isFatherOf,USA), being predicted as highly plausible facts, although they violate constraints on the domain and range of relations <ref type="bibr">Jain et al. [2021a]</ref>, <ref type="bibr" target="#b9">Wang et al. [2019]</ref>. KGEMs with such issues may nevertheless reach a satisfying performance in terms of rank-based metrics.</p><p>Few works propose to go beyond the mere traditional quantitative performance of KGEMs and address their ability to capture the semantics of the original KG, e.g., domain and range constraints, hierarchy of classes <ref type="bibr" target="#b10">Paulheim [2018]</ref>, <ref type="bibr">Jain et al. [2021b]</ref>, <ref type="bibr" target="#b12">Monnin et al. [2022]</ref>. According to <ref type="bibr" target="#b6">Berrendorf et al. Berrendorf et al. [2020]</ref>, this would give a more complete picture of the performance of a KGEM. This is why we advocate for additional qualitative and semantic-oriented metrics to supplement traditional rank-based metrics and propose Sem@K to address this need. The relevance of using semantic-oriented metrics -more specifically Sem@K -is clearly visible in Fig. <ref type="figure" target="#fig_1">2</ref>: Sem@K provides a supplementary dimension to the evaluation procedure and allows to confidently choose between two models. They are equally good in terms of rank-based metrics but Model A predicts entities that are semantically valid w.r.t. the range constraint.</p><p>More specifically, in this work, our goal is to assess the ability of popular KGEMs to capture the semantic profile of relations in a LP task, i.e., whether KGEMs predict entities that respects domain and range of relations. Henceforth, we refer to this aspect as the semantic awareness of KGEMs. To do so, we build on Sem@K, a semantic-oriented metric that we previously introduced <ref type="bibr">Hubert et al. [2022a,b]</ref> In <ref type="bibr">Hubert et al. [2022a]</ref>, Sem@K was specifically defined for the recommendation task which was seen as predicting tails for a unique target relation. Sem@K was then extended in <ref type="bibr" target="#b14">Hubert et al. [2022b]</ref> to the more generic LP task, where not only tails but also heads are corrupted and all relations are considered. In the present work, we deepen the study of the semantic awareness of KGEMs by proposing different versions of Sem@K that take into account the different characteristics of KGs (e.g., hierarchy of types). Moreover, the semantic awareness of a wider range of KGEMs is analyzed -especially convolutional models. Likewise, a broader array of KGs is used, in order to benchmark the semantic capabilities of KGEMs on mainstream LP datasets. Thus, the following research questions are addressed:</p><p>• RQ1: how semantic-aware agnostic KGEMs are?</p><p>• RQ2: how the evaluation of KGEM semantic awareness should adapt to the typology of KGs?</p><p>• RQ3: does the evaluation of KGEM semantic awareness offer different conclusions on the relative superiority of some KGEMs compared to rank-based metrics?</p><p>Accordingly, the main contributions of this work are:</p><p>• to evaluate KGEM semantic awareness on any kind of KG, we extend a previously defined semantic-oriented metric and tailor it to support a broader range of KGs. • we perform an extensive study of the semantic awareness of state-of-the-art KGEMs on mainstream KGs. We show that most of the observed trends apply at the level of families of models. • we perform a dynamic study of the evolution of KGEM semantic awareness vs. their performance in terms of rank-based metrics along training epochs. We show that a trade-off may exist for most KGEMs. • our study supports the view that agnostic KGEMs are quickly able to infer the semantics of KG entities and relations.</p><p>The remainder of the paper is structured as follows. Related work is presented in Section 2. Section 3 outlines the main motivations for assessing the semantic awareness of KGEMs. Section 4 subsequently presents Sem@K, the semantic-oriented metric that fulfills this purpose. Sem@K comes in different flavors based on the typology of the datasets at hand and the intended use cases. In Section 5, we detail the datasets and KGEMs used in this work, before presenting the experimental findings in Section 6. A thorough discussion is provided in Section 7. Lastly, Section 8 outlines future research directions.</p><p>2 Related work 2.1 Link prediction using knowledge graph embedding models Several LP approaches have been proposed to complete KGs. Symbolic approaches relying on rule-based <ref type="bibr" target="#b15">Galárraga et al. [2013</ref><ref type="bibr" target="#b16">Galárraga et al. [ , 2015]]</ref>, <ref type="bibr" target="#b17">Lajus et al. [2020]</ref>, <ref type="bibr" target="#b18">Meilicke et al. [2019]</ref>, <ref type="bibr" target="#b19">Ott et al. [2021]</ref> or path-based reasoning <ref type="bibr" target="#b20">Lao et al. [2011]</ref>, <ref type="bibr" target="#b21">Xiong et al. [2017]</ref>, <ref type="bibr" target="#b22">Das et al. [2018]</ref> are somewhat popular but are not considered in the present work. Instead, KGEMs are the focus of this paper.</p><p>In particular, this work is concerned with assessing the semantic awareness of agnostic KGEMs. The semantic awareness of such models is defined as their ability to score higher entities whose types belong to the domain and range of relations. Agnostic KGEMs are defined as KGEMs that rely solely on the structure of the KG to learn entity and relation representations. In this respect, these models differ according to several criteria, such as the nature of the embedding space or the type of scoring function <ref type="bibr" target="#b1">Wang et al. [2017</ref><ref type="bibr">Wang et al. [ , 2021a]]</ref>, <ref type="bibr" target="#b24">Ji et al. [2022]</ref>, <ref type="bibr" target="#b5">Rossi et al. [2021]</ref>. In this work, KGEMs are considered with respect to three main families of models that are traditionally distinguished in the literature.</p><p>Geometric models are additive models that consider relations as geometric operations in the latent embedding space. A head entity h is spatially transformed using an operation τ that depends on the relation embedding r. Then the distance between the resulting vector and the tail entity t is used as a measure for assessing the plausibility of a fact (h, r, t).</p><p>A distance-based function δ is used to define the scoring function f of such KGEMs: f (h, r, t) = δ(τ (h, r), t). A large array of geometric models is purely translational and is based on TransE <ref type="bibr" target="#b2">Bordes et al. [2013]</ref> and its extensions. TransE was the earliest introduced geometric model for link prediction. It enforces the sum of the head and relation embeddings to lie in a close neighborhood of the tail embedding. The distance-based function δ is usually the L1 or L2 norm. TransE does not properly handle 1-to-N, N-to-1, nor N-to-N relations <ref type="bibr" target="#b1">Wang et al. [2017]</ref> and yet has been found to be very efficient in multi-relational settings <ref type="bibr" target="#b25">Chowdhury et al. [2019]</ref>. A myriad of translational models have been proposed since then, such as TransH <ref type="bibr" target="#b26">Wang et al. [2014]</ref>, TransR <ref type="bibr" target="#b27">Lin et al. [2015]</ref>, TransD <ref type="bibr" target="#b28">Ji et al. [2015]</ref>, TransA <ref type="bibr" target="#b29">Jia et al. [2016] and</ref><ref type="bibr">TransG Xiao et al. [2016]</ref>. In addition to these purely translational models, some recent geometric models either replace or combine the translation operation with rotation-wise transformations to make their models even more expressive and deal with difficult relational patterns such as symmetric or anti-symmetric relations. A case in point is RotatE <ref type="bibr" target="#b31">Sun et al. [2019]</ref>, which considers relations as rotations in a complex latent space. h, r and t are all embedded in C d where d denotes the embeddings' dimension. The use of the rotation operation allows RotatE to properly address many relational patterns. In particular, RotatE is able to take account for symmetry -which is not modeled correctly by TransE. In the wake of RotatE, a handful of other roto-translational models have subsequently been proposed in the literature, e.g. QuatE <ref type="bibr" target="#b32">Zhang et al. [2019]</ref>, DualE <ref type="bibr" target="#b33">Cao et al. [2021]</ref> and <ref type="bibr">HAKE Zhang et al. [2020a]</ref>.</p><p>Semantic matching models are named this way as they usually use a similarity-based function to define their own scoring function. Semantic matching models are also referred to as matrix factorization models or tensor decomposition models, in the sense that a KG can be seen as a 3D adjacency matrix, in other words a three-way tensor. This tensor can be decomposed into a set of low-dimensional vectors that actually represent the entity and relation embeddings.</p><p>Semantic matching models are subdivided into bilinear and non-bilinear models. Bilinear models share the characteristic of capturing interactions between two entity vectors using multiplicative terms. <ref type="bibr">RESCAL Nickel et al. [2011]</ref> is the earliest introduced bilinear model for LP. It models entities as vectors and relations as matrices. The components w i,j of the relation matrix W r ∈ R d×d account for the interaction intensity between the i-th embedding component of e h ∈ R d and the j-th embedding component of e t ∈ R d under the relation r. As pointed out by the original author of RESCAL <ref type="bibr">Nickel et al. [2016a]</ref>, the parameter complexity of this model can drastically increase when the embedding dimension is large. DistMult <ref type="bibr" target="#b37">Yang et al. [2015]</ref> alleviates this scalability issue by enforcing all relation-matrices W r to be diagonal. Doing so, DistMult trades computational advantages for less expresiveness. Indeed, DistMult is not able to model anti-symmetric relations. However, DistMult still achieves state-of-the-art performance in most cases <ref type="bibr" target="#b38">Kadlec et al. [2017]</ref>. <ref type="bibr">ComplEx Trouillon et al. [2016]</ref> relies on complex-valued representations for both entities and relations.</p><p>Because the Hadamard product is used in the scoring function, ComplEx is not commutative in the complex space and can properly model anti-symmetric relations. Other bilinear models with distinctive features were later proposed, such as Analogy <ref type="bibr" target="#b40">Liu et al. [2017]</ref>, SimplE Kazemi and Poole <ref type="bibr">[2018]</ref> and DihEdral <ref type="bibr" target="#b43">Xu and Li [2019]</ref> that are supposedly more expressive models. Another category of semantic matching models leverage interactions between entities and relations using non-bilinear operations. For instance, HolE <ref type="bibr">Nickel et al. [2016b]</ref> uses the circular correlation operation, while TuckER <ref type="bibr" target="#b45">Balazevic et al. [2019]</ref> relies on the Tucker decomposition.</p><p>Neural network-based models rely on neural networks to perform LP. In neural networks, the parameters (e.g. weights and biases) are organized into different layers, with usually non-linear activation functions between each of these layers.</p><p>The first introduced neural-network based model for LP is Neural Tensor Network (NTN) <ref type="bibr" target="#b46">Socher et al. [2013]</ref>. It can be seen as a combination of multi-layer perceptrons (MLPs) and bilinear models <ref type="bibr" target="#b24">Ji et al. [2022]</ref>. NTN defines a distinct neural network for each relation. This choice of parameterization makes NTN similar to RESCAL in the sense that both models achieve great expressiveness at the expense of computational concerns. The most recent neural network-based models rely on more sophisticated layers to perform a broader set of operations. Convolutional models are by far the most representative family of such models. They use convolutional layers to learn deep and expressive features from the input data, which pass through such layers to undergo convolution with low-dimensional filters ω. The resulting feature maps subsequently go through dense layers to obtain a final plausibility score. Compared with fully connected neural networks, convolution-based models are able to capture complex relationships with fewer parameters by learning non-linear features. While ConvE <ref type="bibr" target="#b47">Dettmers et al. [2018]</ref> reshapes head entity and relation embeddings before concatenating them into a unique input matrix to pass through convolutional layers, ConvKB Nguyen et al.</p><p>[2018] does not perform any reshaping and also puts the tail embedding into the concatenated input matrix. Other models were later proposed, such as ConvR <ref type="bibr" target="#b49">Jiang et al. [2019]</ref> and <ref type="bibr">InteractE Vashishth et al. [2020a]</ref> that both process triples independently. Another branch of convolutional models also considers the local neighborhood around each central entity. These are based on Graph Neural Networks (GNNs). The most representative GNN-based model for LP is R-GCN Schlichtkrull et al. <ref type="bibr">[2018]</ref>. The key idea is to accumulate messages from the local neighborhood of the central node over multi-hop relations. By doing so, R-GCN is better able to model a long range of relational dependencies. However, R-GCN does not outperform baselines for the LP task <ref type="bibr" target="#b52">Ferrari et al. [2022]</ref>. Subsequent models have claimed superiority over R-GCN: <ref type="bibr">SACN Shang et al. [2019]</ref> introduces a weighted GCN to adjust the amount of aggregated information from the local neighborhood and <ref type="bibr">KBGAT Nathani et al. [2019]</ref> relies on attention mechanism to generate more accurate embeddings. More recently, <ref type="bibr">CompGCN Vashishth et al. [2020b]</ref> and DisenKGAT <ref type="bibr" target="#b56">Wu et al. [2021]</ref> showcased impressive performance with regard to the LP task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Combining embeddings and semantics</head><p>The possibility of using additional semantic information has been extensively studied in recent works <ref type="bibr">Jain et al. [2021a]</ref>, <ref type="bibr" target="#b3">Krompaß et al. [2015]</ref>, <ref type="bibr" target="#b57">Niu et al. [2020]</ref>, <ref type="bibr" target="#b58">Cui et al. [2021]</ref>, <ref type="bibr" target="#b59">Xie et al. [2016]</ref>, <ref type="bibr" target="#b60">Lv et al. [2018]</ref>, <ref type="bibr">Wang et al. [2021b]</ref>.</p><p>In general, the semantic information stems directly from an ontology, originally defined by Gruber as an "explicit specification of a conceptualization" Gruber <ref type="bibr">[1993]</ref>. Ontologies formally describe a specific application domain of interest (e.g. education, pharmacology, etc.) in which several classes (or concepts) and relations are identified and formally specified. Ontologies support KG construction by providing a schema that specifies the nature of entities, the semantic profile of relations, and other constraints that give the KG a semantic coherence.</p><p>A significant part of the literature incorporates such semantic information to constrain the negative sampling procedure and generate meaningful negative triples <ref type="bibr">Jain et al. [2021a]</ref>, <ref type="bibr" target="#b3">Krompaß et al. [2015]</ref>. For instance, type-constrained negative sampling (TCNS) <ref type="bibr" target="#b3">Krompaß et al. [2015]</ref> replaces the head or the tail of a triple with a random entity belonging to the same type (rdf:type) as the ground-truth entity. Jain et al. <ref type="bibr">Jain et al. [2021a]</ref> go a step further and use ontological reasoning to iteratively improve KGEM performance by retraining the model on inconsistent predictions.</p><p>Semantic information can also be embedded in the model itself. In fact, some KGEMs leverage ontological information such as entity types and hierarchy.</p><p>Embedding models project entities and relations of a KG into a vector space. Thus, the semantics of the original KG may not be fully preserved <ref type="bibr">Jain et al. [2021a]</ref>, <ref type="bibr" target="#b10">Paulheim [2018]</ref>. As stated by <ref type="bibr" target="#b10">Paulheim Paulheim [2018]</ref>, because embeddings are not meant to preserve the semantics of the KG, they are not interpretable and this can severely hinder explainability in domains such as recommender systems. Consequently, <ref type="bibr" target="#b10">Paulheim Paulheim [2018]</ref> advocates for semantic embeddings. Similarly, Jain et al. <ref type="bibr">Jain et al. [2021b]</ref> perform a thorough evaluation of popular KGEMs to better assess whether embeddings can express similarities between entities of the same type. A key finding is that because of overlapping relations among entities of different types, fine-grained semantics cannot be properly reflected by embeddings For instance, the task of finding semantically similar entities does not always provide satisfying results when working with entity embeddings <ref type="bibr">Jain et al. [2021b]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Evaluating KGEM performance for link prediction</head><p>KGEM performance is evaluated in two stages. During the validation phase, KGEM performance is evaluated on the validation set T valid after regular -often uniform -intervals of epochs. This way, the best epoch is identified. During the test phase, KGEM performance is ultimately evaluated on the test set T test after retrieving the optimal model parameters achieved on the best epoch of validation. Whether during the validation or test phase, KGEM performance is evaluated the same way: sifting through every triple of the test (resp. validation) set, both head and tail predictions are performed. In the case of head prediction, this amounts to taking a triple (h, r, t) from the test (resp. validation) set, hiding the ground-truth head entity -resulting in (?, r, t) -and letting the KGEM assign a score to every possible entity as a candidate for the head position. These scores are finally ordered and reflect the plausibility of such facts. Tail prediction is performed analogously on (h, r, ?).</p><p>In both cases, the rank of the ground-truth entity from the test (resp. validation) set is used to compute aggregated rank-based metrics based on the top-K scored entities. The rank of the ground-truth entity can be determined in two different ways that depend on how observed facts -i.e. facts that already exist in the KG -are considered. In the raw setting, observed facts outranking the ground-truth are not filtered out, while this is the case in the filtered setting. For instance, assuming head prediction is performed on the given ground-truth triple (BarackObama, livesIn, USA), a KGEM may assign a lower score to this triple than to the following triple: (MichelleObama, livesIn, USA). The latter triple actually represents an observed fact. In the raw setting, this triple would not be filtered out from top-K scored triples. This can cause the evaluation procedure to not properly assess the KGEM performance. This is why in practice, the filtered setting is commonly preferred. In the present work, the filtered setting is also used.</p><p>KGEM performance is almost exclusively assessed using the following rank-based metrics: Hits@K, Mean Rank (MR), and Mean Reciprocal Rank (MRR) <ref type="bibr" target="#b7">Hoyt et al. [2022]</ref>. Disagreements exist as to how and when these metrics can be used and compared properly. In the following, we recall their definitions and discuss their limits.</p><p>Hits@K (Eq. ( <ref type="formula">1</ref>)) accounts for the proportion of ground-truth triples appearing in the first K top-scored triples:</p><formula xml:id="formula_0">Hits@K = 1 |B| q∈B 1 [rank(q) ≤ K] (1)</formula><p>where B is the batch of ground-truth triples, rank(q) is the position of the ground-truth triple q in the sorted list of triples, and 1 [rank(q) ≤ K] yields 1 if q is ranked between 1 and K, 0 otherwise. This metric is bounded in the [0, 1] range and its values increase with K, where the higher the better.</p><p>Mean Rank (MR) (Eq. ( <ref type="formula">2</ref>)) corresponds to the arithmetic mean over ranks of ground-truth triples:</p><formula xml:id="formula_1">MR = 1 |B| q∈B rank(q) (2)</formula><p>This metric is bounded in the [0, |E|] interval, where |E| stands for the number of entities in the KG, where the lower the better.</p><p>Mean Reciprocal Rank (MRR) (Eq. ( <ref type="formula">3</ref>)) corresponds to the arithmetic mean over reciprocals of ranks of ground-truth triples:</p><formula xml:id="formula_2">MRR = 1 |B| q∈B 1 rank(q)</formula><p>(3) Contrary to MR, MRR is a metric bounded in the [0, 1] interval. Higher results indicate better performance. Because this metric does not use any threshold K compared to Hits@K, it is less sensitive to outliers. In addition, it is often used for performing early stopping and for tracking the best epoch during training <ref type="bibr" target="#b6">Berrendorf et al. [2020]</ref>, <ref type="bibr" target="#b7">Hoyt et al. [2022]</ref>.</p><p>As mentioned in Section 1, these metrics present some caveats. LP is often used to complete knowledge graphs, where the Open World Assumption (OWA) prevails. KGs are incomplete and, due to the OWA, an unobserved triple used as a negative one can still be positive. It follows that traditional evaluation methods based on rank-based metrics may systematically underestimate the true performance of a KGEM <ref type="bibr" target="#b9">Wang et al. [2019]</ref>.</p><p>In addition, the aforementioned rank-based metrics have intrinsic and theoretical flaws, as pointed out in several works <ref type="bibr" target="#b6">Berrendorf et al. [2020]</ref>, <ref type="bibr" target="#b7">Hoyt et al. [2022]</ref>, <ref type="bibr" target="#b8">Tiwari et al. [2021]</ref>. For example, Hits@K does not take into account triples whose rank is larger than K. As such, a model scoring the ground-truth in position K + 1 would be considered equally good as another model scoring the ground-truth in position K + d with d ≫ 1. It follows that Hits@K is not a suitable metric for drawing comparisons between models <ref type="bibr" target="#b7">Hoyt et al. [2022]</ref>. MR alleviates this concern as it does not consider any threshold K. In Section 2.2, several approaches incorporating the semantics of entities and relations into the embeddings were mentioned. However, in such cases, the use of semantic information such as entity types and the hierarchy of classes is intended to improve KGEM performance in terms of the aforementioned rank-based metrics only. The underlying semantics of KGs is considered as an additional source of information during training but the ability of KGEMs to generate predictions in accordance with these semantic constraints is never directly addressed. This encourages further assessment of the semantic capabilities of KGEMs, as firstly suggested in <ref type="bibr" target="#b14">Hubert et al. [2022b]</ref>. In our work, we directly address this issue by assessing to what extent KGEMs are able to give high scores to triples whose head (resp. tail) belongs to the domain (resp. range) of the relation. When such information is not available -i.e. the KG does not rely on a schema containing rdfs:domain and rdfs:range properties -extensional constraints can still be used to evaluate the semantic capabilities of KGEMs, as detailed in Section 4.</p><p>3 Motivations and problem formulation</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Motivating example</head><p>To motivate the use of Sem@K to evaluate KGEM quality, this section builds upon a minimalist example which is representative of the issue encountered while benchmarking the performance of several KGEM on the same dataset. As depicted in Fig. <ref type="figure" target="#fig_1">2</ref>, two KGEMs that have been trained on the whole training set are tested on a batch of test triples. These KGEMs are referred to as Model A and Model B. Without loss of generality, it is assumed that the test set only comprises the three triples shown in Fig. <ref type="figure" target="#fig_1">2</ref>. For the sake of clarity, only the tail prediction pass and the top-5 ranked candidate entities are depicted in Fig. <ref type="figure" target="#fig_1">2</ref>. It should be noted that the performance of both models are strictly equal in terms of MR, MRR and Hits@K with K ≤ 5: MR=8/3, Hits@1=1/3, Hits@3=2/3 and Hits@5=1. MRR can only be computed knowing the total number of entities in the KG but two models having the same MR on the same dataset have de facto the same MRR as well. Distinguishing between these two models by relying solely on traditional rank-based metrics is not possible. One might even draw the misleading conclusion that these models are equally good. But they are not: Model A gives high scores to semantically valid entities with regard to the range of relations, while Model B semantic awareness is very low. In other words, Model B does not capture the semantic profile of relations well. This case in point illustrates the need for using semantic-oriented metrics alongside common rank-based metrics, so as to better assess the overall quality of KGEMs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Problem formulation</head><p>The traditional evaluation of KGEMs solely based on rank-based metrics can be flawed for several reasons. First, KGEMs benchmarked on the same test sets can exhibit very similar results. Using only rank-based metrics, the final choice only depends on the best achieved MRR and/or Hits@K. This raises the questions whether the chosen  Model A and Model B output scores for each possible entity and only the top-five ranked tail candidates are depicted here. Model A and Model B have the same Hits@1, Hits@3 and Hits@5 values. But Model A has better semantic capabilities. Green, blue and white cells respectively denote the ground-truth entity, entities other than the ground-truth and semantically valid, and entities other than the ground-truth and semantically invalid model is actually the best one, or whether its slight superiority over other KGEMs can be due to other factors such as better hyperparameter tuning or better modeling of a relational pattern highly present in the test set. Moreover, using only rank-based metrics does not provide the full picture of KGEM quality for the downstream LP task, as some dimensions of KGEMs are left unassessed (see Section 2). In this work, contrary to the mainstream approach consisting in comparing KGEM performance exclusively in terms of rank-based metrics, the trained KGEMs are also evaluated in terms of Sem@K which measures the ability of KGEMs to predict semantically valid triples with respect to the domain and range of relations.</p><p>4 Measuring KGEMs semantic awareness with Sem@K</p><p>The standard LP evaluation protocol consists in reporting aggregated results, considering the rank-based metrics presented in Section 2.3. As mentioned above, these metrics only provide a partial picture of KGEM performance <ref type="bibr" target="#b6">Berrendorf et al. [2020]</ref>. To give a more comprehensive assessment of KGEMs, we aim at assessing their semantic awareness using our proposed metric called Sem@K <ref type="bibr">Hubert et al. [2022a,b]</ref>. In <ref type="bibr">Hubert et al. [2022a]</ref>, Sem@K was specifically defined for the recommendation task which was seen as predicting tails for a unique target relation. Sem@K was then extended in <ref type="bibr" target="#b14">Hubert et al. [2022b]</ref> to the more generic LP task, where not only tails but also heads are corrupted and all relations are considered. In this work, this original formalization for LP is presented in Section 4.1, and enriched to take into account schemaless KGs (Section 4.3), or KGs with a class hierarchy (Section 4.4). As a consequence, Sem@K comes in 3 different versions (respectively denoted Sem@K[base], Sem@K[ext], and Sem@K[wup]) so as to adapt to KG typology. These distinct versions and their adequacy regarding the KG at hand are summarized in Table <ref type="table">1</ref> and further detailed below. In the following, when no suffix is provided, it is assumed that we are concerned with Sem@K in general, regardless of the actual version.</p><p>Table <ref type="table">1</ref>: Typology of KGs and their respective adequacy for the presented Sem@K versions KG type Sem</p><formula xml:id="formula_3">@K[ext] Sem@K[base] Sem@K[wup] Schemaless × Schema-defined, w/o class hierarchy × × Schema-defined, w/ class hierarchy × × × 4.1 Definition of Sem@K[base]</formula><p>This version of Sem@K (Eq. ( <ref type="formula">4</ref>)) accounts for the proportion of triples that are semantically valid in the first K top-scored triples:</p><formula xml:id="formula_4">Sem@K = 1 |B| q∈B 1 K q ′ ∈S K q compatibility(q, q ′ ) (4)</formula><p>where, given a ground-truth triple q = (h, r, t), S K q is the list of the top-K candidate triples scored by a given KGEM (i.e. by predicting the tail for (h, r, ?) or the head for (?, r, t)). The function compatibility(q, q ′ ) (Eq. ( <ref type="formula" target="#formula_5">5</ref>)) assesses whether the candidate triple q ′ is semantically compatible with its ground-truth counterpart q. In this work, by semantic compatibility we refer to the fact that the predicted head (resp. tail) belongs to the domain (resp. range) of the relation:</p><formula xml:id="formula_5">compatibility(q, q ′ ) = 1, if types(q ′ h ) ∩ domain(q r ) ̸ = ∅ ∧ types(q ′ t ) ∩ range(q r ) ̸ = ∅ 0, otherwise<label>(5)</label></formula><p>where types(e) returns the types of entity e and domain(r) (resp. range(r)) is the domain (resp. range) of the relation r. q r , q ′ h , and q ′ t denote the ground-truth relation, the head and the tail of the ranked triple q ′ , respectively. It is noteworthy that with this formula, we allow entities to instantiate multiple types, and domains and ranges to be defined with multiple types. Sem@K is bounded in the [0, 1] interval. Compared to Hits@K (Eq. ( <ref type="formula">1</ref>)), Sem@K is non-monotonic: increasing K can lead to either lower or higher Sem@K values.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">A note on Sem@K, untyped entities, and the Open World Assumption</head><p>Traditional KGEM evaluation can be performed in all situations, regardless of whether entities are typed or whether the KG comes with a proper schema. When measuring the semantic capability of KGEMs -e.g. with Sem@K -some concerns arise. For instance, a fair question to ask is the following: how should untyped entities be considered? Indeed, in some KGs, some entities are left untyped. For example, in DBpedia the entity dbr:1._FC_Union_Berlin_players does not belong to any other class than owl:Thing. In some other cases as in DB15K and DB100K d'Amato et al.</p><p>[2021], entities have incomplete typing.</p><p>Under the OWA, an untyped entity should not count in the calculation of Sem@K since it is not possible to determine whether this entity has no types or no known types. Although this seems to be a fair option, it raises a major issue: it makes possible to score different sets of entities with rank-based metrics and Sem@K, which is not desirable. If there are M untyped entities in an ordered list of ranked entities, Hits@K, MR and MRR are calculated regardless of this fact, i.e. still taking into account the M untyped entities. However, Sem@K cannot be calculated for these M untyped entities. As such, MR, MRR and Hits@K would be calculated on the original entity set, whereas Sem@K would be computed on a different set of entities. This issue would be even more acute in the case of Sem@1 when the first ranked entity is untyped: Hits@1 and Sem@1 would be calculated on two different entities, which is not acceptable. Consequently, one strategy consists in removing untyped entities from the evaluation protocol, both regarding rank-based and semantic-based metrics. By doing so, consistency is ensured in the ranked list of entities across rank-based and semantic metrics evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Sem@K[ext] for schemaless KGs</head><p>Not all KGs come with a proper schema, e.g. relations do not appear in any rdfs:domain or rdfs:range clauses. In that particular situation, it can still be desirable to assess the semantic awareness of KGEMs. One approach is to maintain a list of all entities that have been observed as heads (resp. tails) of each relation r : domain(r) = {e : ∃(e, r, t) ∈ T } (resp. range(r) = {e : ∃(h, r, e) ∈ T }). Hence, in this case, Sem@K[ext] is defined as in Eq. ( <ref type="formula">4</ref>) and (5) but using these definitions of domain and range. Therefore, an entity will be considered as a semantically valid head (resp. tail) with respect to the relation if this entity appears as a head (resp. tail) in any other triple observed in the KG with the same relation. Sem@K as previously defined equally penalizes all entities that are not of the expected type. However, KGs may be equipped with a class hierarchy that, in turn, can support a more fine-grained penalty for entities depending on the distance or similarity between their type and the expected domain (resp. range) in this hierarchy. To illustrate, consider Figure <ref type="figure" target="#fig_2">3</ref> that depicts a subset of the DBpedia ontology dbo class hierarchy. Using the hierarchy-free version of Sem@K for the test triple (dbr:The_Social_Network, dbo:director, dbr:David_Fincher), predicting dbr:Friends or dbr:Central_Park as head would be penalized the same way in the compatibility function. However, it is clear that an entity of class dbo:TelevisionShow is semantically closer to dbo:Film -the domain of the relation dbo:director -than an entity of class dbo:Park, and thus should be less penalized.</p><p>To leverage such a semantic relatedness between concepts in Sem@K, the compatibility function can be adapted accordingly:</p><formula xml:id="formula_6">compatibility(q, q ′ ) = min    max c∈type(q ′ h ) c ′ ∈domain(qr) σ(c, c ′ ), max c∈type(q ′ t ) c ′ ∈range(qr) σ(c, c ′ )    (6)</formula><p>where σ(c, c ′ ) measure the semantic similarity between the two classes c and c ′ based on the class hierarchy. It should be noted that in this formula type(e) only returns the most specific classes instantiated by e.</p><p>Several similarity measures σ have been proposed in the literature <ref type="bibr" target="#b65">Rada et al. [1989]</ref>, <ref type="bibr" target="#b66">Wu and Palmer [1994]</ref>, <ref type="bibr">Leacock and Chodorow [1998]</ref>, <ref type="bibr" target="#b68">Resnik [1999]</ref>, <ref type="bibr" target="#b69">Li et al. [2003]</ref>. In this work, the Wu-Palmer similarity <ref type="bibr" target="#b66">Wu and Palmer [1994]</ref> (Eq. 7) is used:</p><formula xml:id="formula_7">σ (c, c ′ ) = 2 × δ (c ∧ c ′ , ρ) δ (c, c ∧ c ′ ) + δ (c ′ , c ∧ c ′ ) + 2 × δ (c ∧ c ′ , ρ) (7)</formula><p>where ρ is the root of the hierarchy (e.g. owl:Thing), δ (c, c ′ ) is the number of edges linking c to c ′ , and c ∧ c ′ represents the least common subsumer of c and c ′ . The Wu-Palmer similarity is well suited to a class hierarchy and provides a good indication of the semantic relatedness between the domain (resp. range) class and the classes of the chosen entity. This gives rise to the Sem@K[wup] version.</p><p>Considering the example in Fig. <ref type="figure" target="#fig_2">3</ref>, using the Wu-Palmer score in the calculation of Sem@K, a head prediction of dbr:Friends and dbr:Central_Park for the ground-truth triple (dbr:The_Social_Network, dbo:director, dbr:David_Fincher) are now differently penalized. The instance dbr:Friends is of type dbo:TelevisionShow, so we have: σ (dbo:TelevisionShow, dbo:Film) = 1/2. The instance dbr:Central_Park is of type dbo:Park, so we have: σ (dbo:Park, dbo:Film) = 0. This illustrates that incorporating the Wu-Palmer score into Sem@K calculation leads to more precise semantic comparisons that take into account the available class hierarchy. It should be noted that comparing the classes of a candidate entity with the expected class can result in the same penalization as the vanilla version of Sem@K. However, in most cases, two classes do not lie in a completely disjoint part of the hierarchy of classes. As such, the Wu-Palmer score between the classes of a candidate entity and the expected class is rarely 0.</p><p>5 Experimental setting</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Datasets</head><p>In order to draw reliable and general conclusions, a broad range of KGs are used in our experiments. They have been chosen due to their mainstream adoption in recent research works around KGEMs for LP and the fact that they have different characteristics (e.g. entities, relations, classes, presence of a class hierarchy). In this section, the schemadefined and schemaless KGs used in the experiments are detailed. Note that in our experiments, all the schema-defined KGs come with a class hierarchy inherited from either Freebase <ref type="bibr" target="#b70">Bollacker et al. [2008]</ref>, DBpedia <ref type="bibr" target="#b71">Auer et al. [2007]</ref>, YAGO <ref type="bibr" target="#b72">Suchanek et al. [2007]</ref>, or schema.org. To meet requirements for evaluating KGEMs w.r.t. Sem@K (i.e. classes instantiated by entities, domain and range for relations), when necessary, subsets of schema-defined KGs are used, as explained in Section 5.1.1. Among the 4 schema-defined KGs presented hereafter, FB15K237-ET, DB93K and YAGO3-37K are derived from already existing KGs, while YAGO4-19K was specifically created and is made available on Zenodo<ref type="foot" target="#foot_0">2</ref> and GitHub<ref type="foot" target="#foot_1">3</ref> . The other KGs are made available on GitHub<ref type="foot" target="#foot_2">4</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.1">Schema-defined KGs</head><p>The statistics of the datasets FB15K237-ET, DB93K, YAGO3-37K and YAGO4-19K used in our experiments are provided in Table <ref type="table" target="#tab_2">2</ref>. As discussed in Section 4.2, to create an experimental evaluation setting as unbiased and flawless as possible, the schema-defined KGs used in the experiments are filtered to keep typed entities only. This way, Sem@K is calculated under the CWA.</p><p>FB15K237-ET derives from FB15K <ref type="bibr" target="#b2">Bordes et al. [2013]</ref> -a dataset extracted from the cross-domain Freebase <ref type="bibr">KG Bollacker et al. [2008]</ref>. In these experiments, we do not use FB15K, as it has been noted that this dataset suffers from test leakage, i.e., a large number of test triples can be obtained by simply inverting the position of the head and tail in the train triples <ref type="bibr" target="#b73">Toutanova and Chen [2015]</ref>. The later introduced FB15K237 dataset Toutanova and Chen [2015] is a subset of the original FB15K without these inverse relations. To the best of our knowledge, there is no schema-defined version of FB15K237. However, a schema-defined version of FB15K is provided in <ref type="bibr" target="#b59">Xie et al. [2016]</ref>. Consequently, we based ourselves on this version of FB15K and mapped the extracted entity types, relation domains and ranges to the entities and relations found in FB15K237. The resulting schema-defined version of FB15K237 is named FB15K237-ET and includes only typed entities. Besides, validation and test sets contain triples whose relation have a well-defined domain (resp. range), as well as more than 10 possible candidates as head (resp. tail). This ensures Sem@K is not unduly penalized and can be calculated on the same set of entities as Hits@K and MRR, at least until K = 10.</p><p>DB93K is a subset of DB100K, which was first introduced in Ding et al. <ref type="bibr">[2018]</ref>. A slightly modified version of DB100K has been proposed in d'Amato et al. <ref type="bibr">[2021]</ref>. Contrary to the initial version of DB100K, the latter version is schema-defined: entities are properly typed and most relations have a domain and/or a range. This second version is considered in the following experiments. However, some inconsistencies were found in the dataset. Some DBpedia entities only instantiate Wikidata<ref type="foot" target="#foot_3">5</ref> or schema.org<ref type="foot" target="#foot_4">6</ref> classes, while instantiation of classes from the DBpedia ontology actually exist. Moreover, some entities are only partially typed. It must also be noted that domains and ranges of relations have been extracted from DBpedia more than two years ago. DBpedia is a communautary and open-source project: DBpedia classification system relies on human curation, which sometimes implies a lack of coverage for some resources and updates for others. Consequently we associated all relations of DB100K to their most up-to-date domains and ranges<ref type="foot" target="#foot_5">7</ref> . Similarly for entities, we associated all entities to their most up-to-date classes. Finally, we removed all untyped entities as well as validation and test relations having less than 10 observed entities, so as not to unfairly penalize Sem@K results in the validation and test phases.</p><p>YAGO3-37K derives from the schema-defined YAGO39K dataset <ref type="bibr" target="#b60">Lv et al. [2018]</ref> extracted from the cross-domain YAGO3 <ref type="bibr">KG Suchanek et al. [2007]</ref>. Compared to the original YAGO39K, in our experiments only typed entities are kept. In addition, relations having less than 10 observed heads or tails in the training set are discarded from the validation and test splits, for the same reason that keeping them would not reflect the actual Sem@K values. It should be noted that in the YAGO3 ontology, most relations have very generic domains and ranges which are very close to the root of the ontology hierarchy. To produce a more challenging evaluation setting of the models' semantic awareness, a We built YAGO4-19K with several other subsets of the YAGO4 knowledge graph Tanon et al. <ref type="bibr">[2020]</ref>. Similarly to other datasets, we focused on relations with a defined domain and range, and more than 10 triples to constitute the validation and test sets. We purposedly favored difficult relations to feature in the validation and test sets. To enrich the training set, additional relations were added based on a manual selection. Selected relations in validation and test sets as well as additional relations in the training set can be found on the GitHub repository of the dataset. It should be noted that the class hierarchy associated with entities in YAGO 4 is schema.org.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.2">Extensional KGs</head><p>Another range of datasets used in these experiments do not come with an ontological schema. In particular, this means relations do not have a clearly-defined domain or range. Although Codex-S and Codex-M are based on the Wikidata schema which does possess property constraints linking subject types to value type constraints, we limit ourselves to KGs that represent this information with rdfs:domain and rdfs:range predicates. Consequently, in this work, we only report Sem@K[ext] results for Codex-S and Codex-M. The statistics of the datasets Codex-S, Codex-M and WN18RR used in our experiments are provided in Table <ref type="table" target="#tab_3">3</ref>.</p><p>WN18RR <ref type="bibr" target="#b47">Dettmers et al. [2018]</ref> originates from WN18, which is a subset of the WordNet <ref type="bibr">KG Miller [1995]</ref>. As for FB15K, Toutanova et al. In our experiments, we do not directly use these provided negative triples. Instead, we use the same Uniform Random Negative Sampling schema as for other datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Baseline models</head><p>In this work, the semantic awareness of the most popular semantically agnostic KGEMs is analyzed. More specifically, the translational models TransE <ref type="bibr" target="#b2">Bordes et al. [2013]</ref> and TransH <ref type="bibr" target="#b26">Wang et al. [2014]</ref>, the semantic matching models DistMult <ref type="bibr" target="#b37">Yang et al. [2015]</ref>, ComplEx <ref type="bibr" target="#b39">Trouillon et al. [2016]</ref> and SimplE Kazemi and Poole <ref type="bibr">[2018]</ref>, and the convolu- TransH <ref type="bibr" target="#b26">Wang et al. [2014]</ref> is an extension of TransE. It allows entities to have distinct representations when involved in different relations. Specifically, e h and e t are projected into relation-specific hyperplanes with projection matrices w r . If (h, r, t) holds, the projected entities e h ⊥ = e h -w T r e h w r and e t ⊥ = e t -w T r e t w r are expected to be linked by the relation-specific translation vector d r . Thus, the scoring function is</p><formula xml:id="formula_8">f (h, r, t) = d(e h ⊥ + d r -e t ⊥ )<label>(9)</label></formula><p>TransH often showcases better performance than TransE with only slightly more parameters <ref type="bibr" target="#b26">Wang et al. [2014]</ref>.</p><p>DistMult <ref type="bibr" target="#b37">Yang et al. [2015]</ref> is a semantic matching model. It is characterized as such because it uses a similaritybased scoring function and matches the latent semantics of entities and relations by leveraging their vector space representations. More specifically, DistMult is a bilinear diagonal model that uses a trilinear dot product as its scoring function: f (h, r, t) = ⟨e h , W r , e t ⟩ (10) It is similar to <ref type="bibr">RESCAL Nickel et al. [2011]</ref> -the very first semantic matching model -but restricts relation matrices W r ∈ R d×d to be diagonal.</p><p>ComplEx <ref type="bibr" target="#b39">Trouillon et al. [2016]</ref> is also a semantic matching model. It extends DistMult by using complex-valued vectors to represent entities and relations: e h , e r , e t ∈ C d . As a result, ComplEx is better able to model antisymmetric relations than DistMult <ref type="bibr" target="#b31">Sun et al. [2019]</ref>. Its scoring function uses the Hadamard product:</p><formula xml:id="formula_9">f (h, r, t) = Re (e h ⊙ e r ⊙ e t )<label>(11)</label></formula><p>where e t denotes the conjugate of e t .</p><p>SimplE Kazemi and Poole [2018] models each fact in both a direct and an inverse form. To do so, an entity e is simultaneously represented by two vectors e h , e t ∈ R d . Depending on whether e appears as head or tail in a given triple, either e h or e t is used. Consequently, the two entity representations e h and e t are learned independently. Likewise, each relation r comes with a direct and an inverse vectors e r and e r -1 . The scoring function reflects the interaction between all the aforementioned entity and relation embeddings: embeddings and layers of nonlinear features. The output is ultimately scored against the tail embedding t using the dot product. More precisely, the following scoring function is used: f (h, r, t) = g (vec (g (concat ( e h , e r ) * ω)) W) • e t (13) where g denotes a non-linear function, vec is the vectorization operation reshaping a tensor into a vector, concat is the concatenation operator, * and . denote a convolution and a dot product, respectively, e denotes a 2D reshaping of e and ω is the set of convolutional filters.</p><formula xml:id="formula_10">f (h, r, t) = 1 2 ( e h h ,</formula><p>ConvKB Nguyen et al. <ref type="bibr">[2018]</ref> also represents entities and relations as same-sized vectors. However, ConvKB does not reshape the embeddings of entities and relations. Plus, ConvKB also considers the tail embedding in the concatenation operation, thus obtaining the 2D matrix [h; r; t] after concatenation. Convolution by a set ω of T filters of shape 1 * 3 is applied on this input. The resulting T * 3 feature map then passes through a dense layer with one neuron and a weight matrix W . Finally, the following scoring function assesses the plausibility of a given triple (h, r, t):</p><p>f (h, r, t) = concat (g ([e h , e r , e t ] * ω)) • w (14) It has been claimed that the concatenation of a set of feature maps generated by convolution should increase the learning ability of latent features compared to ConvE <ref type="bibr" target="#b24">Ji et al. [2022]</ref>. However, recent works point out the evaluation procedure used in the original implementation of ConvKB <ref type="bibr" target="#b53">Shang et al. [2019]</ref>, <ref type="bibr" target="#b78">Sun et al. [2020]</ref>, which may result in overly optimistic results on the benchmark datasets FB15K237 and WN18RR. Nonetheless, due to the popularity of this model, we choose to include it in our experiments. <ref type="bibr" target="#b51">Schlichtkrull et al. [2018]</ref> extends the idea of applying graph convolutional networks (GCNs) to multi-relational data. R-GCN operates on local graph neighborhoods and applies a convolution operation to the neighbors of each entity. By aggregating the messages coming from all the neighbors of an entity, the embedding of the latter is updated in accordance. Each entity thus has a hidden representation which directly depends on the hidden representations of its neighbors. This process of accumulating messages (i.e. the hidden representations of neighboring entities) and aggregating them so as to update the hidden representation of the central node is performed for each layer of the R-GCN model. More formally, the hidden representation of the entity i in the layer (l + 1) is defined as:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>R-GCN</head><formula xml:id="formula_11">h (l+1) i = σ   W (l) 0 h (l) i + r∈R j∈N r i 1 c i,r W (l) r h (l) j  <label>(15)</label></formula><p>where σ(.) can be any element-wise activation function, N r i denotes the set of neighboring entities of entity i considering the relation r, W (l) r is the relation-specific weight matrix for layer l and c i,r is a normalization constant. To update entity hidden representation taking into account its previous state, a self-connection is also incorporated into the activation function σ(.). Such layers can be stacked multiple times in order to better learn interactions and dependencies across several relational steps. However, as applying Eq.( <ref type="formula" target="#formula_11">15</ref>) directly would dramatically increase the number of parameters for KGs having lots of different relation types, basis-decomposition and block-diagonal-decomposition are proposed in <ref type="bibr" target="#b51">Schlichtkrull et al. [2018]</ref> to reduce model parameter size and prevent overfitting. In the case of basis decomposition which is used in the original paper, the relation-specific weight matrix W </p><formula xml:id="formula_12">h (l+1) r = W (l) rel h (l)</formula><p>r (17) where W rel is a learnable transformation matrix which projects all the relations into the same embedding space as entities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Implementation details and hyperparameters</head><p>For the sake of comparisons, MRR, Hits@K and Sem@K all need to rely on the same code implementation. More specifically, for R-GCN 8 and CompGCN 9 , existing implementations were reused and Sem@K values were calculated Table <ref type="table">5</ref>: Results on YAGO3-37K. Bold fonts and gray cells denote the best achieved results and the worst achieved results among the models reported in the table, respectively. Full results are available in Appendix B, Table <ref type="table" target="#tab_15">8</ref> Rank-based Sem@K[base] Sem@K[wup] Sem@K[ext] Model Family Model MRR H@1 H@3 H@10 S@1 S@3 S@10 S@1 S@3 S@10 S@1 S@3 S@10 Geometric on the trained models. Other KGEMs were implemented in PyTorch. To avoid time-consuming hyperparameter tuning, we took inspiration from the hyperparameters provided by LibKGE<ref type="foot" target="#foot_8">10</ref> for Codex-S, Codex-M, WN18RR and FB15K237. However, LibKGE does not benchmark all the datasets and models used in our experiments. For such models, we stick to the hyperparameters provided by the original authors, when available. For the models with no reported best hyperparameters, as well as for the remaining datasets used in the experiments -i.e. DB93K, YAGO3-37K and YAGO4-19K -different combinations of hyperparameters were manually tried. We first trained our KGEMs for 1, 000 epochs, then noticed the best achieved results were found around epoch 400 or below. Consequently, we stick to a maximum of 400 epochs of training as in LibKGE (except R-GCN which is trained during 4, 000 epochs due to lower convergence to the best achieved results). For each positive triple in the training set, one corresponding negative triple is generated. To ensure fair comparisons between our models, we stick with Uniform Random Negative Sampling <ref type="bibr" target="#b2">Bordes et al. [2013]</ref>. The chosen hyperparameters leading to the best performance on the validation dataset are provided in Appendix (see Appendix A, Table <ref type="table" target="#tab_12">7</ref>). Recall that the objective of this work is to perform a fair and insightful assessment of the semantic awareness of KGEMs. As such, the intended purpose was not to reach optimal performance in terms of rank-based metrics. Instead, the objective is to identify a set of hyperparameters that provides satisfying and stable performance, and then study how Sem@K behaves, both for a fixed epoch -e.g. for the best epoch in terms of MRRand dynamically w.r.t. the number of epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Results</head><p>In the following, we perform an extensive analysis of the results obtained using the aforedescribed KGEMs and KGs.</p><p>For the sake of clarity, the complete range of tables and plots are placed in Appendix B and C, respectively. When necessary to support our claim, some of them are duplicated in the body text.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Semantic awareness of KGEMs</head><p>This section draws on the Sem@K values (see Tables <ref type="table" target="#tab_15">8</ref> and<ref type="table">9</ref>) achieved at the best epoch in terms of MRR to provide an analysis of the semantic awareness of state-of-the-art KGEMs. In other words, for such models we only consider a snapshot of their best epochs in terms of rank-based metrics.</p><p>A major finding is that models performing well with respect to rank-based metrics are not necessarily the most competitive when it comes to their semantic capabilities. On YAGO3-37K (see Table <ref type="table">5</ref>) for instance, ConvE showcases impressive MRR and Hits@K values. However, it is far from being the best KGEM in terms of Sem@K, as it is outperformed by CompGCN, R-GCN and all translational models.</p><p>From a coarse-grained viewpoint, conclusions about the relative superiority of models with the distinct consideration of rank-based metrics and semantic awareness can be generalized at the level of models families. For example, semantic matching models (DistMult, ComplEx, SimplE) globally achieve better MRR and Hits@K values while their semantic capabilities are in most cases lower than the ones of translational models (TransE, TransH) -see Table <ref type="table" target="#tab_15">8</ref> and Table <ref type="table">9</ref> for detailed results w.r.t. rank-based and semantic-oriented metrics. A condensed view of the comparison between MRR and Sem@10 is also reported in Fig. <ref type="figure">4</ref> and Fig. <ref type="figure">5</ref>. The respective hierarchies of such models for the benchmarked schema-defined and schemaless KGs are depicted in Fig. <ref type="figure">6</ref> and Fig. <ref type="figure">7</ref>, respectively. It is clearly visible that KGEMs are grouped by family. In particular, GNNs and translational models showcase very promising semantic capabilities. GNNs are almost always the best regarding Sem@K[ext] values -not only for schemaless KGs (Fig. <ref type="figure">7</ref>), but also for schema-defined KGs (see Table <ref type="table" target="#tab_15">8</ref> for full results, and Fig. <ref type="figure">4</ref> for a quick glimpse). This means GNNs are more capable of predicting entities that have been observed as head (resp. tail) of a given relation. Translational models are very</p><formula xml:id="formula_13">7 U D Q V ( 7 U D Q V + ' LV W P X OW &amp; R P S OH [ 6 LP S O( &amp; R Q Y ( &amp; R Q Y . % 5 * &amp; 1 &amp; R P S * &amp; 1 055 6HP#.</formula><p>(a) FB15K237-ET</p><formula xml:id="formula_14">7 U D Q V ( 7 U D Q V + ' LV W P X OW &amp; R P S OH [ 6 LP S O( &amp; R Q Y ( &amp; R Q Y . % 5 * &amp; 1 &amp; R P S * &amp; 1 055 6HP#. (b) DB93K 7 U D Q V ( 7 U D Q V + ' LV W P X OW &amp; R P S O( [ 6 LP S O( &amp; R Q Y ( &amp; R Q Y . % 5 * &amp; 1 &amp; R P S * &amp; 1 055 6HP#. (c) YAGO3-37K 7 U D Q V ( 7 U D Q V + ' LV W P X OW &amp; R P S O( [ 6 LP S O( &amp; R Q Y ( &amp; R Q Y . % 5 * &amp; 1 &amp; R P S * &amp; 1 055 6HP#. (d) YAGO4-19K</formula><p>Figure <ref type="figure">4</ref>: MRR and Sem@10 results achieved at the best epoch in terms of MRR for each model and on each schemadefined dataset</p><formula xml:id="formula_15">7 UD Q V ( 7 UD Q V + ' LV WP X OW &amp; R P S OH [ 6 LP S O( &amp; R Q Y ( &amp; R Q Y . % 5 * &amp; 1 &amp; R P S * &amp; 1 055 6HP#. (a) Codex-S 7 UD Q V ( 7 UD Q V + ' LV WP X OW &amp; R P S O( [ 6 LP S O( &amp; R Q Y ( &amp; R Q Y . % 5 * &amp; 1 &amp; R P S * &amp; 1 055 6HP#. (b) Codex-M 7 UD Q V ( 7 UD Q V + ' LV WP X OW &amp; R P S O( [ 6 LP S O( &amp; R Q Y ( &amp; R Q Y . % 5 * &amp; 1 &amp; R P S * &amp; 1 055 6HP#.</formula><p>(c) WN18RR</p><p>Figure <ref type="figure">5</ref>: MRR and Sem@10 results achieved at the best epoch in terms of MRR for each model and on each schemaless dataset</p><formula xml:id="formula_16">Lowest Sem@K TransH Highest Sem@K CompGCN TransE RGCN DistMult SimplE ComplEx ConvKB ConvE (a) FB15K237-ET Lowest Sem@K TransH Highest Sem@K CompGCN TransE RGCN SimplE DistMult ComplEx ConvKB ConvE (b) DB93K Lowest Sem@K TransH Highest Sem@K CompGCN TransE RGCN DistMult SimplE ComplEx ConvKB ConvE (c) YAGO3-37K Lowest Sem@K TransE TransH Highest Sem@K CompGCN ConvE RGCN DistMult SimplE ComplEx ConvKB (d) YAGO4-19K</formula><p>Figure <ref type="figure">6</ref>: Sem@K[base] comparisons between KGEMs on the 4 benchmarked schema-defined KGs. Colors indicate the family of models: blue, purple, green, and yellow cells denote GNNs, translational, convolutional, and semantic matching models, respectively. Regarding Sem@K, the relative hierarchy of models is consistent across KGs and we can clearly see that KGEMs are grouped by families of models competitive in terms of Sem@K[base]. In Fig. <ref type="figure">6</ref>, we clearly see that they consistently rank among the best performing models regarding Sem@K[base]. Interestingly, the semantic matching models DistMult, ComplEx and SimplE perform relatively poorly. This observation holds regardless of the nature of the KG, as they systematically rank among the worst performing models for schema-defined (Fig. <ref type="figure">6</ref>) and schemaless (Fig. <ref type="figure">7</ref>) KGs.</p><p>Therefore, it seems that translational models are better able at recovering the semantics of entities and relations to properly predict entities that are in the domain (resp. range) of a given relation, while semantic matching models might sometimes be better at predicting entities already observed in the domain (resp. range) of a given relation (e.g. DistMult reaches very high Sem@K[ext] values on Codex-S, as evidenced in Fig. <ref type="figure">7a</ref>). In cases when translational and semantic matching models provide similar results in terms of rank-based metrics, the nature of the dataset at hand -whether it is schema-defined or schemaless -might thus strongly influence the final choice of a KGEM.</p><p>Interestingly, CompGCN which is by far the most recent and sophisticated model used in our experiments, outperforms all the other models in terms of semantic awareness as, with very limited exceptions, it is the best in terms of Sem@K[base], Sem@K[ext] and Sem@K[wup]. In addition, it should be noted that R-GCN provides satisfying results as well. Except in a very few cases (e.g. outperformed by ComplEx on Codex-S and WN18RR in terms of Sem@1), R-GCN showcases better semantic awareness than semantic matching models. Most of the time, R-GCN also provides comparable or even higher semantic capabilities than translational models. In particular, Sem@K values achieved with R-GCN are similar to the ones achieved with TransE and TransH (e.g. on FB15K237-ET and YAGO3- Figure <ref type="figure">7</ref>: Sem@K[ext] comparisons between KGEMs on the 3 benchmarked schemaless KGs. Colors indicate the family of models: blue, purple, green, and yellow cells denote GNNs, translational, convolutional, and semantic matching models, respectively. Regarding Sem@K, the relative hierarchy of models is consistent across KGs and we can clearly see that KGEMs are grouped by families of models. 37K, see Tables <ref type="table" target="#tab_15">8a</ref> and<ref type="table" target="#tab_15">8c</ref>) while the latter models are actually outperformed by R-GCN in terms of Sem@K[ext]. It appears clearly on Codex-M and WN18RR (Tables <ref type="table">9b</ref> and<ref type="table">9c</ref>), although the conclusion holds for all datasets.</p><p>Our experimental results suggest that the structure of GNNs seems to be able to encode the latent semantics of the entities and relations of the graph. This ability may be attributed to the fact that, contrary to translational models which only model the local neighborhood of each triple, GNNs update entity embeddings (and relation embeddings in the case of CompGCN) based on the information found in the extended, h-hop neighborhood of the central node. While translational and semantic matching models treat each triple independently, GNNs model interactions between entities on a large range of semantic relations. It is likely that this extended neighborhood comprises signals or patterns that help the model infer the classes of entities, thus providing very promising semantic capabilities in all experimental conditions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Dynamic appraisal of KGEM semantic awareness</head><p>For certain models, rank-based metrics performance and semantic capabilities improve jointly. For others, the enhancement of their performance in terms of rank-based metrics comes at the expense of their semantic awareness. Interestingly, trends emerge relatively to families of models. First, we observe that a trade-off exists for semantic matching models. Results are particularly striking on FB15K237-ET (see Fig. <ref type="figure">8</ref>), where it is obvious that after reaching the best Sem@K values after a few epochs, Sem@K values of DistMult and ComplEx quickly drop while MRR continues rising. Conversely, translational models are more robust to Sem@K degradation throughout the epochs. Even though the best achieved Sem@K values are also reported in the very first epochs, once these values are reached they remain stable for the remaining epochs of training. This might be due to the geometric nature of such KGEMs, which will organize the representation space so as to h + r falls in a region of the space where neighboring entities of the ground-truth tail t all are entities of the expected type. This is highly related to the block structure property, which is a common statistical pattern found in KGs <ref type="bibr">Nickel et al. [2016a]</ref>. It refers to the fact that entities can naturally belong to different groups (blocks), such that all the entities of a given group are linked to entities of another group through the same relationship. In this case, each group comprises entities of the same class. Translational models will naturally group entities of the same class in the same region of the representation space, as this is determined by the translation vector in that space. Figure <ref type="figure">8</ref>: Evolution of MRR ( ), Sem@1 ( ), Sem@3 ( ), and Sem@10 ( ) for translational <ref type="bibr">(TransE, TransH)</ref> and semantic matching models (DistMult, ComplEx) on FB15K237-ET Plots of the joint evolution of MRR and Sem@K values show that most of the KGEMs reaches their best Sem@K values after a few number of epochs. This means that predictions get semantically valid in the early stages of training. As previously mentioned, Sem@K then usually start to decrease, as it has been noted for semantic matching models in particular. To this respect, an excerpt of the head and tail predictions of DistMult on YAGO3-37K is depicted in Fig. <ref type="figure">9</ref>. Even though the ground-truth entity does not show up in neither the head nor the tail top-K list, we clearly see that after only 30 epochs of training, predictions made by DistMult are more meaningful than after 400 epochs of training. This relative trade-off between making semantically valid predictions and predictions that comprise the ground-truth entity higher in the top-K list calls for finding a compromise in terms of training. The LP task is usually addressed in terms of rank-based metrics only, hence the choice of performing more and more training epochs so as to find the optimal KGEM in terms of MRR and Hits@K. However, as discussed in the present work, adding training steps may improve KGEM performance at the expense of its semantic awareness. In many cases, rank-based metrics values only slightly increase, whereas Sem@K values drastically drop. For instance, comparing MRR vs. Sem@K evolution of DistMult on FB15K237-ET (Fig. <ref type="figure" target="#fig_0">10c</ref>), we clearly see that after a moderate number of epochs, any additional epoch of training only provides a very slight improvement in terms of MRR, while it is very detrimental to Sem@K values. Depending on the use case, such a decline in the semantic capabilities of the model is not desirable, and a compromise is to be found between training more to increase KGEM predictive performance and stopping training early enough so as not to deteriorate its semantic awareness too much.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">On the use of Sem@K for different kinds of KGs</head><p>As reported in Table <ref type="table">1</ref>, KGs based upon a schema and a class hierarchy are candidates for the computation of all the versions of Sem@K. For the schema-defined KGs used in our experiments, we choose to report values regarding all these metrics so as to enable multi-view comparisons across models. From Table <ref type="table" target="#tab_15">8</ref> it can be clearly seen that the relative superiority of models is consistent throughout the different Sem@K definitions. From a higher perspective, this means that even for schema-defined KGs with a hierarchy class, Sem@K[ext] is already a good proxy. This may be a good option to only rely on the Sem@K[ext] whenever the computation of Sem@K[base] is too expensive, due to the entity type checking part. This is even more true for Sem@K[wup], which requires an additional step of semantic relatedness computation.</p><p>&lt;Mozambique&gt; &lt;hasCapital&gt; &lt;Maputo&gt; </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Head Prediction</head><p>Figure <ref type="figure">9</ref>: Top-ten ranked entities for head and tail predictions at epochs 30 and 400 for a sample triple from YAGO3-37K. Green, blue and white cells respectively denote the ground-truth entity, entities other than the ground-truth and semantically valid, and entities other than the ground-truth and semantically invalid. In this case, semantic validity is based on the domain and range of the relation &lt;hasCapital&gt;</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Discussion</head><p>Three major research questions have been formulated in Section 1. Based on the analysis presented in Section 6, we discuss each research question individually. We ultimately discuss the potential for further considerations of semantics into KGEMs.</p><p>7.1 RQ1: how semantic-aware agnostic KGEMs are?</p><p>From a coarse-grained viewpoint, we noted that KGEMs trained in an agnostic way prove capable of giving higher scores to semantically valid triples. However, disparities exist between models. Interestingly, these disparities seem to derive from the family of such models. Globally, translational models and GNNs -represented by R-GCN and CompGCN in this work -provide promising results. It appears that the two aforementioned families of KGEMs are better able than semantic matching models (DistMult, ComplEx, SimplE) at recovering the semantics of entities and relations to give higher score to semantically valid triples. In fact, semantic matching models are almost systematically the worst performing models in terms of semantic awareness. From a dynamic standpoint, it is worth noting the high semantic capabilities of KGEMs reached during the first epochs of training. In most cases, this is even during the first epochs that the optimal semantic awareness is attained.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">RQ2: how KGEM semantic awareness' evaluation should adapt to the typology of KGs?</head><p>Drawing on the initial version of Sem@K as presented in <ref type="bibr" target="#b14">Hubert et al. [2022b]</ref> -referred to herein as Sem@K[base] -an issue is quickly encountered when it comes to schemaless KGs, which do not contain any rdfs:domain (resp. rdfs:range) clause to indicate the class that candidate heads (resp. tails) should belong to. Our work introduces Sem@K[ext] -a new version of Sem@K that overcomes the aforementioned limitation. In addition, even with schema-defined KGs, Sem@K[base] is not necessarily sufficient in itself. This metric can be further enriched whenever a KG comes with a class hierarchy. In Section 4.4, we integrate class hierarchy into Sem@K by means of a similarity measure between concepts. We subsequently provide an example using the Wu-Palmer similarity score. The resulting Sem@K[wup] is used in the experiments in Section 6 and provide a finer-grained measure of KGEMs semantic awareness.</p><p>7.3 RQ3: does the evaluation of KGEM semantic awareness offers different conclusions on the relative superiority of some KGEMs?</p><p>A major finding is that models performing well with respect to rank-based metrics are not necessarily the most competitive regarding their semantic capabilities. We previously noted that translational models globally showcase better Sem@K values compared to semantic matching models. Considering MRR and Hits@K, the opposite conclusion is often drawn. Hence, the performance of KGEMs in terms of rank-based metrics is not indicative of their semantic capabilities. The only exception that might exist is for GNNs that perform well both in terms of rank-based metrics and semantic-oriented measures.</p><p>The answers provided to the research questions also lead to consider new matters. As evidenced in Table <ref type="table" target="#tab_15">8</ref> and Table <ref type="table">9</ref>, some KGs are more challenging with regard to Sem@K results. Due to its tailored extraction strategy that purposedly favored difficult relations to feature in the validation and test sets, YAGO4-19K is the schema-defined KG with the lowest achieved Sem@K. This observation raises a deeper question: what characteristics of a KG make it inherently challenging for KGEMs to recover the semantics of entities and relations? An extensive study of the influence of KGs characteristics on the semantic capabilities of KGEMs would require to benchmark them on a broad set of KGs with varying dimensions, so as to determine those that are the most prevalent. Such characteristics can be the total number of relations, the average number of instances per class, or a combination of different factors. We leave this experimental study for future work.</p><p>Recall that this work is motivated by the possibility of going beyond a mere assessment of KGEM performance regarding rank-based metrics. We showed that these metrics only evaluate one aspect of such models, somehow providing a partial view on the quality of KGEMs. Our proposal for further assessing KGEM semantic capabilities aims at diving deeper into their predictive expressiveness and measuring to what extent their predictions are semantically valid. However, this second evaluation component does not shed full light on the respective KGEM peculiarities. Other evaluation components may be added, such as the storage and computational requirements of KGEMs Wang et al.</p><p>[2021c], <ref type="bibr" target="#b80">Portisch et al. [2020]</ref> and the environmental impact of their training and testing procedure <ref type="bibr" target="#b61">Peng et al. [2021]</ref>. Furthermore, the explainability of KGEMs is another dimension that deserves great attention <ref type="bibr">Zhang et al. [2020b]</ref>, <ref type="bibr" target="#b83">Rossi et al. [2022]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.4">Towards further considerations of semantics in knowledge graph embeddings models</head><p>The Sem@K metric presented in this work allows for a more comprehensive evaluation of KGEMs. Based on domains and ranges of relations, Sem@K assesses to what extent the predictions of a model are semantically valid. The present work constitutes one of several stepping stones toward the further consideration of ontological and semantic information in KGEM design and evaluation.</p><p>It should be noted that due to the only consideration of domains and ranges, Sem@K cannot indicate whether predictions are logically consistent with other constraints posed by the ontology. This is in contrast with Inc@K presented in <ref type="bibr">Jain et al. [2021a]</ref> that takes a broader set of ontological axioms into account. However, Inc@K and Sem@K intrinsically assess distinct dimensions of predictions. While the former is concerned with the logical consistency of predictions, the latter focuses on whether these predictions are semantically valid. For instance, an ontology can specify that City is the range of livesIn, that Seattle is a City but not a Capital, and that entities of type President should be linked to a Capital through the relation livesIn. Hence, it would still be meaningful and semantically correct to predict that (BarackObama,livesIn,Seattle). However, this prediction is not logically consistent w.r.t. ontology specifications. Inc@K and SemK thus consider triples at different levels: while a given triple can be meaningful and semantically correct on its own, its combination with other triples may not be semantically valid or consistent with the ontology. In future work, we will consider more expressive ontologies and see how the broad collection of axioms that constitute them can be incorporated into Sem@K.</p><p>KGEMs evaluated in this work are all agnostic to ontological information in their design. However, some models that consider or ensure specific ontological or logical properties exist. For example, HAKE <ref type="bibr">Zhang et al. [2020a]</ref> is constructed with the purpose of preserving hierarchies, Logic Tensor Networks <ref type="bibr" target="#b84">Badreddine et al. [2022]</ref> are designed to ensure logical reasoning, and the training of <ref type="bibr">TransOWL and TransROWL d'Amato et al. [2021]</ref> is enriched with additional triples deduced from, e.g., inverse predicates or equivalent classes. Because of the integration of semantic information in their design or training, one could wonder if they present improved semantic awareness compared to agnostic models. Additionally, KGEMs can also be used to predict triples that represent class instantiations. A possible extension of the present work thus consists in studying whether predicted links and class instantiations are consistent and lead to increased Sem@K values. This would further qualify and highlight the semantic awareness and the consistency of predictions of KGEMs. We leave these questions for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion</head><p>In this work, we consider the link prediction task and extend our previously introduced Sem@K metric to measure the ability of KGEMs to assign higher scores to triples that are semantically valid. In particular, to adapt to different types of KGs (e.g., schemaless, class hierarchy), we introduce Sem@K[base], Sem@K[ext], or Sem@K[wup]. Compared with the traditional evaluation approach that solely relies on rank-based metrics, we show that the evaluation procedure is enhanced with the addition of semantic-oriented metrics that bring an additional perspective on KGEM quality. Our experiments with different types of KGs highlight that there is no clear correlation between the performance of KGEMs in terms of traditional rank-based metrics versus their performance regarding semantic-oriented ones. In some cases, however, a trade-off does exist. Consequently, this calls for monitoring KGEM training under more scrutiny. Our experiments also point out that most of the conclusions that have been drawn actually hold at the level of families of models.</p><p>In future work, we will conduct experiments considering a broader array of KGEM families (e.g., KGEMs that include semantics) and propose evaluation metrics that consider additional and more expressive ontological constraints.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Hyperparameters</head><p>For datasets with no reported optimal hyperparameters, grid-search based on curated hyperpamaters were performed.</p><p>The full hyperparameter space is provided in Table <ref type="table" target="#tab_11">6</ref>. Chosen hyperparameters for each pair of KGEM and dataset are provided in Table <ref type="table" target="#tab_12">7</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Results achieved with the best reported hyperparameters</head><p>Results achieved with the best reported hyperparameters are presented in Tables <ref type="table" target="#tab_15">8</ref> and<ref type="table">9</ref>.</p><p>C Evolution of MRR and Sem@K values with respect to the number of epochs</p><p>The evolution of MRR and Sem@K values with respect to the number of epochs is presented in <ref type="bibr">Fig. 10,</ref><ref type="bibr">11 ,</ref><ref type="bibr">12,</ref><ref type="bibr">13,</ref><ref type="bibr">14,</ref><ref type="bibr">15,</ref><ref type="bibr">and 16</ref>. For equity and clarity sakes, we choose to present 2 KGEMs for each family of model (translational models, semantic matching models, CNNs, and GNNs). Regarding semantic matching models, DistMult and ComplEx are chosen, as the evolution of their MRR and Sem@K values is less erratic than SimplE. The evolution of MRR and Sem@K values for SimplE are made available on the GitHub repository of the datasets<ref type="foot" target="#foot_9">11</ref> .    Model MRR H@1 H@3 H@10 S@1 S@3 S@10 S@1 S@3 S@10 S@1 S@3 S@10 Model MRR H@1 H@3 H@10 S@1 S@3 S@10 S@1 S@3 S@10 S@1 S@3 S@10 Model MRR H@1 H@3 H@10 S@1 S@3 S@10 S@1 S@3 S@10 S@1 S@3 S@10 </p><formula xml:id="formula_17">-3 5e -3 1e -3 1e -3 1e -3 1e -3 1e -3 λ 1e -2 1e -1 1e -5 1e -2 1e -2 1e -2 1e -</formula><formula xml:id="formula_18">-3 1e -3 5e -3 1e -3 1e -3 1e -3 1e -3 λ 1e -5 1e -5 1e -5 1e -5 1e -5 1e -5 1e -5 R-GCN d 500 500 500 500 500 500 500 lr 1e -2 1e -2 1e -2 1e -2 1e -2 1e -2 1e -2 λ 1e -2 1e -2 1e -2 1e -2 1e -2 1e -2 1e -</formula><formula xml:id="formula_19">-3 1e -3 1e -3 1e -3 1e -3 1e -3 1e -3 λ 0 0 0 0 0 0 0</formula><formula xml:id="formula_20">d) YAGO4-19K Rank-based Sem@K[base] Sem@K[wup] Sem@K[ext] Model Family<label>(</label></formula><p>Model MRR H@1 H@3 H@10 S@1 S@3 S@10 S@1 S@3 S@10 S@1 S@3 S@10  Figure <ref type="figure" target="#fig_0">16</ref>: Evolution of MRR ( ), Sem@1 ( ), Sem@3 ( ), and Sem@10 ( ) on WN18RR</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Excerpt of a KG containing some influential political figures and relations holding between them</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Motivating example. Tail prediction is performed for the three test triples contained in the upper insert.Model A and Model B output scores for each possible entity and only the top-five ranked tail candidates are depicted here. Model A and Model B have the same Hits@1, Hits@3 and Hits@5 values. But Model A has better semantic capabilities. Green, blue and white cells respectively denote the ground-truth entity, entities other than the ground-truth and semantically valid, and entities other than the ground-truth and semantically invalid</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Excerpt from the DBpedia class hierarchy</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>CompGCN</head><label></label><figDesc>Vashishth et al. [2020b]  improves over R-GCN by not only learning entity representations but also relation representations. Concretely, CompGCN performs a composition operation ϕ(.) over each edge in the neighborhood of a central node. The composed embeddings are subsequently convolved with direction-specific weight matrices W O and W I -for original and inverse relations, respectively. In addition to the entity representation update, relation representations are also updated individually:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 10 :Figure 11 :Figure 12 :Figure 13 :Figure 14 :Figure 15 :</head><label>101112131415</label><figDesc>Figure10: Evolution of MRR ( ), Sem@1 ( ), Sem@3 ( ), and Sem@10 ( ) on FB15K237-ET</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Therefore, MR allows to compare KGEM performance on the same dataset. Nonetheless, MR is sensitive to the number of KG entities (see Eq. (2))<ref type="bibr" target="#b6">Berrendorf et al. [2020]</ref>: a MR of 10 indicates very good performance if the set of entities is in the thousands, but it would indicate poor performance if the set of entities is much more restricted. Therefore, MR does not allow comparisons across datasets.Recent works recommend using adjusted version of the aforementioned metrics. The Adjusted Mean Rank (AMR) proposed in<ref type="bibr" target="#b63">Ali et al. [2022]</ref> compares the mean rank against the expected mean rank under a model with random scores. In<ref type="bibr" target="#b6">Berrendorf et al. [2020]</ref>, Berrendorf et al. transform the AMR to define the Adjusted Mean Rank Index (AMRI) bounded in the [0, 1] interval. This way, a value of 1 indicates an optimal performance of the model. A value of 0 indicates a model performance similar to a model assigning random scores. A negative value indicates that the model performs worse than random. However, all these attempts at producing a better evaluation framework still focus on the quantitative assessment of KGEMs, i.e. the improvement of already existing rank-based metrics.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Statistics of the schema-defined, hierarchical KGs used in the experiments Dataset |E| |R| |C| |T train | |T valid | |T test |</figDesc><table><row><cell cols="3">FB15K237-ET 14,541 237</cell><cell cols="4">532 271,575 15,337 17,928</cell></row><row><cell>DB93K</cell><cell cols="2">92,574 277</cell><cell cols="4">311 237,062 18,059 36,424</cell></row><row><cell>YAGO3-37K</cell><cell>37,335</cell><cell>33</cell><cell cols="2">132 351,599</cell><cell>4,220</cell><cell>4,016</cell></row><row><cell>YAGO4-19K</cell><cell>18,960</cell><cell cols="2">74 1,232</cell><cell>27,447</cell><cell>485</cell><cell>463</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Statistics of the schemaless KGs used in the experiments Dataset |E| |R| |T train | |T valid | |T test | relations was identified and only validation and test triples whose relation belongs to this subset are kept. The resulting dataset is named YAGO3-37K.</figDesc><table><row><cell>Codex-S</cell><cell>2,034</cell><cell>42</cell><cell>32,888</cell><cell>1,827</cell><cell>1,828</cell></row><row><cell cols="2">Codex-M 17,050</cell><cell cols="4">51 185,584 10,310 10,311</cell></row><row><cell cols="2">WN18RR 40,943</cell><cell>11</cell><cell>86,834</cell><cell>3,034</cell><cell>3,134</cell></row><row><cell>subset of hard</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc><ref type="bibr" target="#b73">Toutanova and Chen [2015]</ref> reported a huge test leakage in the original WN18 dataset. More specifically, 94% of the train triples have inverse relations that are linked to test triples.<ref type="bibr" target="#b47">Dettmers et al. Dettmers et al. [2018]</ref> remove all inverse relations to propose WN18RR. In WN18RR, entities are nouns, verbs, and adjectives. Relations such as hypernym and derivationally related from hold between observed entities. Such relations are not linked to any rdfs:domain or rdfs:range predicates. Besides, relations such as derivationally related from can contain nouns, verbs or adjectives as both head or tail. It follows that it is not possible to infer any expected entity type for relations -in this case the word qualifier. This is why WN18RR is used in the extensional setting.<ref type="bibr" target="#b77">Safavi and Koutra [2020]</ref> and Codex-M Safavi and Koutra [2020] are datasets extracted from Wikidata and Wikipedia. They cover a wider scope and purposely contain harder facts than most KGs<ref type="bibr" target="#b77">Safavi and Koutra [2020]</ref>. Consequently, these datasets prove to be more challenging for the link prediction task. Compared to WN18RR, Codex-S and Codex-M contain entity types, relation descriptions and Wikipedia page extracts. Nonetheless, Wikidata does not contain rdfs:domain or rdfs:range predicates. The property constraints present in Wikidata are harder to manipulate than the rdfs:domain and rdfs:range clauses found in DBpedia. As such, in our experiments Codex-S and Codex-M are used in the extensional setting. Codex-S and Codex-M initially come with already generated hard negatives.</figDesc><table><row><cell>Codex-S</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Summary of the KGEMs used in the experiments + e r -e t ∥ p Pairwise Hinge TransH ∥e h ⊥ + d r -e t ⊥ ∥ p ConvE Dettmers et al. [2018], ConvKB Nguyen et al. [2018], R-GCN Schlichtkrull et al. [2018] and CompGCNVashishth et al. [2020b]  are considered. Note that in the analysis of the results in Section 6, a distinction will be made between pure convolutional KGEMs (ConvE, ConvKB) and GNNs (R-GCN, CompGCN). Although the latter have convolutional layers, they are able to capture long-range interactions between entities due to their ability to consider k-hop neighborhoods. The characteristics of the models used in our experiments are mentioned hereinafter and summarized in</figDesc><table><row><cell>Model Family</cell><cell>Model</cell><cell></cell><cell>Scoring Function</cell><cell>Loss Function</cell></row><row><cell>Geometric</cell><cell>TransE</cell><cell></cell><cell cols="2">∥e h Pairwise Hinge</cell></row><row><cell></cell><cell>DistMult</cell><cell></cell><cell>⟨e h , W r , e t ⟩</cell><cell>Pairwise Hinge</cell></row><row><cell cols="2">Semantic Matching ComplEx</cell><cell></cell><cell>Re (e h ⊙ e r ⊙ e t )</cell><cell>Pointwise Logistic</cell></row><row><cell></cell><cell>SimplE</cell><cell>1 2</cell><cell>e h h , e r , e t t + e t h , e -1 r , e h t</cell><cell>Pointwise Logistic</cell></row><row><cell></cell><cell>ConvE</cell><cell cols="3">g (vec (g (concat ( e h , e r )  *  ω)) W) • e t Binary Cross-Entropy</cell></row><row><cell>Convolutional</cell><cell>ConvKB R-GCN</cell><cell cols="2">concat (g ([e h , e r , e t ]  *  ω)) • w DistMult decoder</cell><cell>Pointwise Logistic Binary Cross-Entropy</cell></row><row><cell></cell><cell>CompGCN</cell><cell></cell><cell>ConvE decoder</cell><cell>Binary Cross-Entropy</cell></row><row><cell>tional models</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 .</head><label>4</label><figDesc>TransE<ref type="bibr" target="#b2">Bordes et al. [2013]</ref> is the earliest translational model. It learns representations of entities and relations such that for a triple (h, r, t), e h + e r ≈ e t , where e h , e r and e t are the head, relation and tail embeddings, respectively.</figDesc><table /><note><p><p><p><p>The scoring function is</p>f (h, r, t) = d(e h + e r -e t )</p>(8</p>) where d is a distance function, usually the L1 or L2 norm.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 6 :</head><label>6</label><figDesc>Hyperparameter search space</figDesc><table><row><cell>Hyperparameters</cell><cell>Range</cell></row><row><cell>Batch Size</cell><cell>{128, 256, 512, 1024, 2048}</cell></row><row><cell cols="2">Embedding Dimension {50, 100, 150, 200}</cell></row><row><cell>Regularizer Type</cell><cell>{None, L1, L2}</cell></row><row><cell cols="2">Regularizer Weight (λ) {1e -1 , 1e -2 , 1e -3 , 1e -4 , 1e -5 }</cell></row><row><cell>Learning Rate (lr)</cell><cell>{0.005, 0.003, 0.001, 0.0005, 0.0003, 0.0001}</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 7 :</head><label>7</label><figDesc>Chosen hyperparameters for schema-defined and schemaless KGs used in the experiments. |B|, d, lr, and λ denote the batch size, embedding dimension, learning rate, and regularization weight, respectively. We experimentally found that L2 regularizer systematically worked the best. We therefore decide not to refer to it in the table. For ConvKB, the number of filters T was set to 128. Parameters specific to R-GCN and CompGCN were left as defined in the original implementations.</figDesc><table><row><cell></cell><cell cols="8">Hyperparameter FB15K237-ET DB93K YAGO3-37K YAGO4-19K Codex-S Codex-M WN18RR</cell></row><row><cell></cell><cell>|B|</cell><cell>512</cell><cell>256</cell><cell>256</cell><cell>512</cell><cell>128</cell><cell>128</cell><cell>512</cell></row><row><cell>TransE</cell><cell>d lr</cell><cell>100 1e -3</cell><cell>200 5e -3</cell><cell>150 5e -3</cell><cell>100 5e -2</cell><cell>100 1e -3</cell><cell>100 1e -3</cell><cell>100 1e -3</cell></row><row><cell></cell><cell>λ</cell><cell>1e -5</cell><cell>1e -5</cell><cell>1e -5</cell><cell>1e -5</cell><cell>1e -5</cell><cell>1e -5</cell><cell>1e -5</cell></row><row><cell></cell><cell>|B|</cell><cell>512</cell><cell>256</cell><cell>256</cell><cell>512</cell><cell>128</cell><cell>128</cell><cell>512</cell></row><row><cell>TransH</cell><cell>d lr</cell><cell>100 1e -3</cell><cell>200 5e -3</cell><cell>150 5e -3</cell><cell>100 5e -2</cell><cell>100 1e -3</cell><cell>100 1e -3</cell><cell>100 1e -3</cell></row><row><cell></cell><cell>λ</cell><cell>1e -5</cell><cell>1e -5</cell><cell>1e -5</cell><cell>1e -5</cell><cell>1e -5</cell><cell>1e -5</cell><cell>1e -5</cell></row><row><cell></cell><cell>|B|</cell><cell>1024</cell><cell>1024</cell><cell>1024</cell><cell>1024</cell><cell>1024</cell><cell>1024</cell><cell>1024</cell></row><row><cell>DistMult</cell><cell>d lr</cell><cell>100 1e -3</cell><cell>200 1e -3</cell><cell>150 5e -3</cell><cell>100 5e -2</cell><cell>100 1e -3</cell><cell>100 1e -3</cell><cell>100 1e -3</cell></row><row><cell></cell><cell>λ</cell><cell>1e -5</cell><cell>1e -5</cell><cell>1e -5</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell></row><row><cell></cell><cell>|B|</cell><cell>1024</cell><cell>1024</cell><cell>1024</cell><cell>1024</cell><cell>1024</cell><cell>1024</cell><cell>1024</cell></row><row><cell>ComplEx</cell><cell>d lr</cell><cell>100 1e</cell><cell>200</cell><cell>150</cell><cell>100</cell><cell>100</cell><cell>100</cell><cell>100</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head></head><label></label><figDesc>2   </figDesc><table><row><cell></cell><cell>|B|</cell><cell>1024</cell><cell>1024</cell><cell>1024</cell><cell>1024</cell><cell>1024</cell><cell>1024</cell><cell>1024</cell></row><row><cell>SimplE</cell><cell>d lr</cell><cell>100 1e -3</cell><cell>200 5e -3</cell><cell>150 1e -3</cell><cell>100 1e -3</cell><cell>100 1e -3</cell><cell>100 1e -3</cell><cell>100 1e -3</cell></row><row><cell></cell><cell>λ</cell><cell>1e -2</cell><cell>1e -1</cell><cell>1e -5</cell><cell>0</cell><cell>1e -2</cell><cell>1e -2</cell><cell>0</cell></row><row><cell></cell><cell>|B|</cell><cell>512</cell><cell>256</cell><cell>256</cell><cell>512</cell><cell>512</cell><cell>512</cell><cell>512</cell></row><row><cell>ConvE</cell><cell>d lr</cell><cell>200 1e -3</cell><cell>200 5e -3</cell><cell>200 5e -3</cell><cell>200 1e -3</cell><cell>200 1e -3</cell><cell>200 1e -3</cell><cell>200 1e -3</cell></row><row><cell></cell><cell>λ</cell><cell>1e -5</cell><cell>1e -5</cell><cell>1e -5</cell><cell>1e -5</cell><cell>1e -5</cell><cell>1e -5</cell><cell>1e -5</cell></row><row><cell></cell><cell>|B|</cell><cell>512</cell><cell>256</cell><cell>256</cell><cell>512</cell><cell>512</cell><cell>512</cell><cell>512</cell></row><row><cell>ConvKB</cell><cell>d lr</cell><cell>100 1e</cell><cell>100</cell><cell>100</cell><cell>100</cell><cell>100</cell><cell>100</cell><cell>100</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head></head><label></label><figDesc>2   </figDesc><table><row><cell></cell><cell>|B|</cell><cell>1024</cell><cell>1024</cell><cell>1024</cell><cell>1024</cell><cell>1024</cell><cell>1024</cell><cell>1024</cell></row><row><cell>CompGCN</cell><cell>d lr</cell><cell>200 1e</cell><cell>200</cell><cell>200</cell><cell>200</cell><cell>200</cell><cell>200</cell><cell>200</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 8 :</head><label>8</label><figDesc>Rank-based and semantic-based results on the schema-defined knowledge graphs. Bold fonts indicate which model performs best with respect to a given metric.</figDesc><table><row><cell cols="2">(a) FB15K237-ET</cell><cell></cell><cell></cell></row><row><cell>Rank-based</cell><cell>Sem@K[base]</cell><cell>Sem@K[wup]</cell><cell>Sem@K[ext]</cell></row><row><cell>Model Family</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>https://doi.org/10.5281/zenodo.7526244</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1"><p>https://github.com/pmonnin/YAGO4-LP</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2"><p>https://github.com/nicolas-hbt/benchmark-sematk</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_3"><p>https://www.wikidata.org/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_4"><p>https://schema.org/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_5"><p>SPARQL queries were fired against DBpedia as of November 9, 2022</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_6"><p>https://github.com/toooooodo/RGCN-LinkPrediction</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_7"><p>https://github.com/malllabiisc/CompGCN</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10" xml:id="foot_8"><p>https://github.com/uma-pi1/kge</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="11" xml:id="foot_9"><p>https://github.com/nicolas-hbt/benchmark-sematk</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>This work is supported by the <rs type="funder">AILES</rs> <rs type="projectName">PIA3</rs> project (see https://www.projetailes.com/). Experiments presented in this paper were carried out using the Grid'5000 testbed, supported by a scientific interest group hosted by <rs type="institution">Inria</rs> and including <rs type="funder">CNRS</rs>, RENATER and several Universities as well as other organizations (see https://www.grid5000.fr).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_gcHcb25">
					<orgName type="project" subtype="full">PIA3</orgName>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Model MRR H@1 H@3 H@10 S@1 S@3 S@10 Geometric </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Rank-based Sem@K[ext] Model Family</head><p>Model MRR H@1 H@3 H@10 S@1 S@3 S@10 </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Knowledge graph refinement: A survey of approaches and evaluation methods</title>
		<author>
			<persName><forename type="first">Heiko</forename><surname>Paulheim</surname></persName>
		</author>
		<idno type="DOI">10.3233/SW-160218</idno>
	</analytic>
	<monogr>
		<title level="j">Semantic Web</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="489" to="508" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Knowledge graph embedding: A survey of approaches and applications</title>
		<author>
			<persName><forename type="first">Quan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhendong</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Knowl. Data Eng</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2724" to="2743" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Translating embeddings for modeling multi-relational data</title>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alberto</forename><surname>García-Durán</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oksana</forename><surname>Yakhnenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conf. on Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="2787" to="2795" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Type-constrained representation learning in knowledge graphs</title>
		<author>
			<persName><forename type="first">Denis</forename><surname>Krompaß</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephan</forename><surname>Baier</surname></persName>
		</author>
		<author>
			<persName><surname>Volker Tresp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Semantic Web -14th International Semantic Web Conf. (ISWC)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">9366</biblScope>
			<biblScope unit="page" from="640" to="655" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Improving knowledge graph embeddings with ontological reasoning</title>
		<author>
			<persName><forename type="first">Nitisha</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trung-Kien</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohamed</forename><forename type="middle">H</forename><surname>Gad-Elrab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daria</forename><surname>Stepanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Semantic Web -International Semantic Web Conf. ISWC</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">12922</biblScope>
			<biblScope unit="page" from="410" to="426" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Knowledge graph embedding for link prediction: A comparative analysis</title>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Rossi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denilson</forename><surname>Barbosa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donatella</forename><surname>Firmani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antonio</forename><surname>Matinata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paolo</forename><surname>Merialdo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Knowledge Discovery from Data</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1" to="14" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">On the ambiguity of rank-based evaluation of entity alignment or link prediction methods</title>
		<author>
			<persName><forename type="first">Max</forename><surname>Berrendorf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Evgeniy</forename><surname>Faerman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurent</forename><surname>Vermue</surname></persName>
		</author>
		<author>
			<persName><surname>Volker Tresp</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.06914</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">A unified framework for rank-based evaluation metrics for link prediction in knowledge graphs</title>
		<author>
			<persName><forename type="first">Charles Tapley</forename><surname>Hoyt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Berrendorf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mikhail</forename><surname>Gaklin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><forename type="middle">M</forename><surname>Volker Tresp</surname></persName>
		</author>
		<author>
			<persName><surname>Gyori</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.07544</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Revisiting the evaluation protocol of knowledge graph completion methods for link prediction</title>
		<author>
			<persName><forename type="first">Sudhanshu</forename><surname>Tiwari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iti</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlos</forename><forename type="middle">R</forename><surname>Rivero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW &apos;21: The Web Conf</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="809" to="820" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">On evaluating embedding models for knowledge base completion</title>
		<author>
			<persName><forename type="first">Yanjie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Ruffinelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rainer</forename><surname>Gemulla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><surname>Broscheit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Meilicke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 4th Workshop on Representation Learning for NLP, RepL4NLP@ACL</title>
		<meeting>of the 4th Workshop on Representation Learning for NLP, RepL4NLP@ACL</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="104" to="112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Make embeddings semantic again!</title>
		<author>
			<persName><forename type="first">Heiko</forename><surname>Paulheim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the ISWC Posters &amp; Demonstrations, Industry and Blue Sky Ideas Tracks</title>
		<title level="s">CEUR Workshop Proceedings</title>
		<meeting>of the ISWC Posters &amp; Demonstrations, Industry and Blue Sky Ideas Tracks</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">2180</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Do embeddings actually capture knowledge graph semantics?</title>
		<author>
			<persName><forename type="first">Nitisha</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan-Christoph</forename><surname>Kalo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wolf-Tilo</forename><surname>Balke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ralf</forename><surname>Krestel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Semantic Web -18th International Conf., ESWC</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">12731</biblScope>
			<biblScope unit="page" from="143" to="159" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Discovering alignment relations with graph convolutional networks: A biomedical case study</title>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Monnin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chedy</forename><surname>Raïssi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amedeo</forename><surname>Napoli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adrien</forename><surname>Coulet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Semantic Web</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="379" to="398" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">New strategies for learning knowledge graph embeddings: The recommendation case</title>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Hubert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Monnin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Armelle</forename><surname>Brun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Davy</forename><surname>Monticolo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EKAW -23rd International Conf. on Knowledge Engineering and Knowledge Management</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="66" to="80" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Knowledge Graph Embeddings for Link Prediction: Beware of Semantics! In DL4KG@ISWC 2022: Workshop on Deep Learning for Knowledge Graphs</title>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Hubert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Monnin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Armelle</forename><surname>Brun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Davy</forename><surname>Monticolo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">held as part of ISWC 2022: the 21st International Semantic Web Conference</title>
		<meeting><address><addrLine>Virtual, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022-10">October 2022b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">AMIE: association rule mining under incomplete evidence in ontological knowledge bases</title>
		<author>
			<persName><forename type="first">Luis</forename><surname>Antonio Galárraga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christina</forename><surname>Teflioudi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katja</forename><surname>Hose</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabian</forename><forename type="middle">M</forename><surname>Suchanek</surname></persName>
		</author>
		<idno type="DOI">10.1145/2488388.2488425</idno>
	</analytic>
	<monogr>
		<title level="m">22nd International World Wide Web Conference, WWW &apos;13</title>
		<meeting><address><addrLine>Rio de Janeiro, Brazil</addrLine></address></meeting>
		<imprint>
			<publisher>International World Wide Web Conferences Steering Committee / ACM</publisher>
			<date type="published" when="2013">May 13-17, 2013. 2013</date>
			<biblScope unit="page" from="413" to="422" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Fast rule mining in ontological knowledge bases with AMIE+</title>
		<author>
			<persName><forename type="first">Luis</forename><surname>Galárraga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christina</forename><surname>Teflioudi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katja</forename><surname>Hose</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabian</forename><forename type="middle">M</forename><surname>Suchanek</surname></persName>
		</author>
		<idno type="DOI">10.1007/s00778-015-0394-1</idno>
	</analytic>
	<monogr>
		<title level="j">VLDB J</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="707" to="730" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Fast and exact rule mining with AMIE 3</title>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Lajus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luis</forename><surname>Galárraga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabian</forename><forename type="middle">M</forename><surname>Suchanek</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-49461-2_3</idno>
	</analytic>
	<monogr>
		<title level="m">The Semantic Web -17th International Conference, ESWC 2020</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<meeting><address><addrLine>Heraklion, Crete, Greece</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020-06-04">May 31-June 4, 2020. 2020</date>
			<biblScope unit="volume">12123</biblScope>
			<biblScope unit="page" from="36" to="52" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Anytime bottom-up rule learning for knowledge graph completion</title>
		<author>
			<persName><forename type="first">Christian</forename><surname>Meilicke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melisachew</forename><forename type="middle">Wudage</forename><surname>Chekol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Ruffinelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heiner</forename><surname>Stuckenschmidt</surname></persName>
		</author>
		<idno type="DOI">10.24963/ijcai.2019/435</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence, IJCAI 2019</title>
		<meeting>the Twenty-Eighth International Joint Conference on Artificial Intelligence, IJCAI 2019<address><addrLine>Macao, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">August 10-16, 2019. 2019</date>
			<biblScope unit="page" from="3137" to="3143" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">SAFRAN: an interpretable, rule-based link prediction method outperforming embedding models</title>
		<author>
			<persName><forename type="first">Simon</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Meilicke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Samwald</surname></persName>
		</author>
		<idno type="DOI">10.24432/C5MK57</idno>
	</analytic>
	<monogr>
		<title level="m">3rd Conference on Automated Knowledge Base Construction, AKBC 2021</title>
		<meeting><address><addrLine>Virtual</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021">October 4-8, 2021, 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Random walk inference and learning in A large scale knowledge base</title>
		<author>
			<persName><forename type="first">Ni</forename><surname>Lao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><forename type="middle">M</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
		<ptr target="https://aclanthology.org/D11-1049/" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2011 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Edinburgh, UK</addrLine></address></meeting>
		<imprint>
			<publisher>ACL</publisher>
			<date type="published" when="2011-07-31">27-31 July 2011. 2011</date>
			<biblScope unit="page" from="529" to="539" />
		</imprint>
	</monogr>
	<note>, A meeting of SIGDAT, a Special Interest Group of the ACL</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deeppath: A reinforcement learning method for knowledge graph reasoning</title>
		<author>
			<persName><forename type="first">Wenhan</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thien</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wang</forename></persName>
		</author>
		<idno type="DOI">10.18653/v1/d17-1060</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, EMNLP 2017</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing, EMNLP 2017<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017">September 9-11, 2017. 2017</date>
			<biblScope unit="page" from="564" to="573" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Go for a walk and arrive at the answer: Reasoning over paths in knowledge bases using reinforcement learning</title>
		<author>
			<persName><forename type="first">Rajarshi</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shehzaad</forename><surname>Dhuliawala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manzil</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Vilnis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ishan</forename><surname>Durugkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Akshay</forename><surname>Krishnamurthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=Syg-YfWCW" />
	</analytic>
	<monogr>
		<title level="m">6th International Conference on Learning Representations, ICLR 2018</title>
		<title level="s">Conference Track Proceedings. OpenReview.net</title>
		<meeting><address><addrLine>Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-05-03">April 30 -May 3, 2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A survey on knowledge graph embeddings for link prediction</title>
		<author>
			<persName><forename type="first">Meihong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linling</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoli</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Symmetry</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">485</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A survey on knowledge graphs: Representation, acquisition, and applications</title>
		<author>
			<persName><forename type="first">Shaoxiong</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shirui</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erik</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pekka</forename><surname>Marttinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
		<idno type="DOI">10.1109/TNNLS.2021.3070843</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Networks Learn. Syst</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="494" to="514" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Neural factorization for offer recommendation using knowledge graph embeddings</title>
		<author>
			<persName><forename type="first">Gourab</forename><surname>Chowdhury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Madiraju</forename><surname>Srilakshmi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mainak</forename><surname>Chain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sudeshna</forename><surname>Sarkar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the SIGIR Workshop on eCommerce</title>
		<meeting>of the SIGIR Workshop on eCommerce</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">2410</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Knowledge graph embedding by translating on hyperplanes</title>
		<author>
			<persName><forename type="first">Zhen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianwen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianlin</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Twenty-Eighth AAAI Conf. on Artificial Intelligence</title>
		<meeting>of the Twenty-Eighth AAAI Conf. on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1112" to="1119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Learning entity and relation embeddings for knowledge graph completion</title>
		<author>
			<persName><forename type="first">Yankai</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuan</forename><surname>Zhu</surname></persName>
		</author>
		<ptr target="http://www.aaai.org/ocs/index.php/AAAI/AAAI15/paper/view/9571" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence</title>
		<meeting>the Twenty-Ninth AAAI Conference on Artificial Intelligence<address><addrLine>Austin, Texas, USA</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2015">January 25-30, 2015. 2015</date>
			<biblScope unit="page" from="2181" to="2187" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Knowledge graph embedding via dynamic mapping matrix</title>
		<author>
			<persName><forename type="first">Guoliang</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shizhu</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liheng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
		<idno type="DOI">10.3115/v1/p15-1067</idno>
		<ptr target="https://doi.org/10.3115/v1/p15-1067" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing, ACL 2015</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing, ACL 2015<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<publisher>The Association for Computer Linguistics</publisher>
			<date type="published" when="2015">July 26-31, 2015. 2015</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="687" to="696" />
		</imprint>
	</monogr>
	<note>Long Papers</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Locally adaptive translation for knowledge graph embedding</title>
		<author>
			<persName><forename type="first">Yantao</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanzhuo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hailun</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaolong</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xueqi</forename><surname>Cheng</surname></persName>
		</author>
		<ptr target="http://www.aaai.org/ocs/index.php/AAAI/AAAI16/paper/view/12018" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence</title>
		<meeting>the Thirtieth AAAI Conference on Artificial Intelligence<address><addrLine>Phoenix, Arizona, USA</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2016">February 12-17, 2016. 2016</date>
			<biblScope unit="page" from="992" to="998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Transg : A generative model for knowledge graph embedding</title>
		<author>
			<persName><forename type="first">Han</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minlie</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoyan</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/p16-1219</idno>
		<ptr target="https://doi.org/10.18653/v1/p16-1219" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>The Association for Computer Linguistics</publisher>
			<date type="published" when="2016">August 7-12, 2016. 2016</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note>Long Papers</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Rotate: Knowledge graph embedding by relational rotation in complex space</title>
		<author>
			<persName><forename type="first">Zhiqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhi-Hong</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian-Yun</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">7th International Conf. on Learning Representations</title>
		<imprint>
			<publisher>ICLR</publisher>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Quaternion knowledge graph embeddings</title>
		<author>
			<persName><forename type="first">Shuai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lina</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Liu</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2019/hash/d961e9f236177d65d21100592edb0769-Abstract.html" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019</title>
		<meeting><address><addrLine>Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">December 8-14, 2019. 2019</date>
			<biblScope unit="page" from="2731" to="2741" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Dual quaternion knowledge graph embeddings</title>
		<author>
			<persName><forename type="first">Zongsheng</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qianqian</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaochun</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qingming</forename><surname>Huang</surname></persName>
		</author>
		<ptr target="https://ojs.aaai.org/index.php/AAAI/article/view/16850" />
	</analytic>
	<monogr>
		<title level="m">Thirty-Fifth AAAI Conference on Artificial Intelligence, AAAI 2021, Thirty-Third Conference on Innovative Applications of Artificial Intelligence, IAAI 2021, The Eleventh Symposium on Educational Advances in Artificial Intelligence, EAAI 2021, Virtual Event</title>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2021">February 2-9, 2021. 2021</date>
			<biblScope unit="page" from="6894" to="6902" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Learning hierarchy-aware knowledge graph embeddings for link prediction</title>
		<author>
			<persName><forename type="first">Zhanqiu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianyu</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongdong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Wang</surname></persName>
		</author>
		<ptr target="https://ojs.aaai.org/index.php/AAAI/article/view/5701" />
	</analytic>
	<monogr>
		<title level="m">The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The Thirty-Second Innovative Applications of Artificial Intelligence Conference, IAAI 2020, The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2020</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2020">February 7-12, 2020. 2020</date>
			<biblScope unit="page" from="3065" to="3072" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A three-way model for collective learning on multi-relational data</title>
		<author>
			<persName><forename type="first">Maximilian</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hans-Peter</forename><surname>Volker Tresp</surname></persName>
		</author>
		<author>
			<persName><surname>Kriegel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 28th International Conf. on Machine Learning, ICML</title>
		<meeting>of the 28th International Conf. on Machine Learning, ICML</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="809" to="816" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">A review of relational machine learning for knowledge graphs</title>
		<author>
			<persName><forename type="first">Maximilian</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Evgeniy</forename><surname>Volker Tresp</surname></persName>
		</author>
		<author>
			<persName><surname>Gabrilovich</surname></persName>
		</author>
		<idno type="DOI">10.1109/JPROC.2015.2483592</idno>
	</analytic>
	<monogr>
		<title level="j">Proc. IEEE</title>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="11" to="33" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Embedding entities and relations for learning and inference in knowledge bases</title>
		<author>
			<persName><forename type="first">Bishan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3rd International Conf. on Learning Representations</title>
		<imprint>
			<publisher>ICLR</publisher>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Knowledge base completion: Baselines strike back</title>
		<author>
			<persName><forename type="first">Rudolf</forename><surname>Kadlec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ondrej</forename><surname>Bajgar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Kleindienst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 2nd Workshop on Representation Learning for NLP, Rep4NLP@ACL</title>
		<meeting>of the 2nd Workshop on Representation Learning for NLP, Rep4NLP@ACL</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="69" to="74" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Complex embeddings for simple link prediction</title>
		<author>
			<persName><forename type="first">Théo</forename><surname>Trouillon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Welbl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Éric</forename><surname>Gaussier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Bouchard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 33rd International Conf. on Machine Learning, ICML</title>
		<meeting>of the 33rd International Conf. on Machine Learning, ICML</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="2071" to="2080" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Analogical inference for multi-relational embeddings</title>
		<author>
			<persName><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuexin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning, ICML 2017</title>
		<title level="s">Proceedings of Machine Learning Research</title>
		<meeting>the 34th International Conference on Machine Learning, ICML 2017<address><addrLine>Sydney, NSW, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-08-11">6-11 August 2017</date>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="2168" to="2178" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title/>
		<author>
			<persName><surname>Pmlr</surname></persName>
		</author>
		<ptr target="http://proceedings.mlr.press/v70/liu17d.html" />
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Simple embedding for link prediction in knowledge graphs</title>
		<author>
			<persName><forename type="first">Seyed</forename><surname>Mehran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kazemi</forename></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Poole</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems 2018</title>
		<meeting><address><addrLine>NeurIPS; Montréal, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-12-03">2018. December 3-8, 2018. 2018</date>
			<biblScope unit="page" from="4289" to="4300" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Relation embedding with dihedral group in knowledge graph</title>
		<author>
			<persName><forename type="first">Canran</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruijiang</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/p19-1026</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019</title>
		<meeting>the 57th Conference of the Association for Computational Linguistics, ACL 2019<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-08-02">July 28-August 2, 2019. 2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="263" to="272" />
		</imprint>
	</monogr>
	<note>Long Papers</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Holographic embeddings of knowledge graphs</title>
		<author>
			<persName><forename type="first">Maximilian</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lorenzo</forename><surname>Rosasco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomaso</forename><forename type="middle">A</forename><surname>Poggio</surname></persName>
		</author>
		<ptr target="http://www.aaai.org/ocs/index.php/AAAI/AAAI16/paper/view/12484" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence</title>
		<meeting>the Thirtieth AAAI Conference on Artificial Intelligence<address><addrLine>Phoenix, Arizona, USA</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2016">February 12-17, 2016. 2016</date>
			<biblScope unit="page" from="1955" to="1961" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Tucker: Tensor factorization for knowledge graph completion</title>
		<author>
			<persName><forename type="first">Ivana</forename><surname>Balazevic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carl</forename><surname>Allen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1522</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">November 3-7, 2019. 2019</date>
			<biblScope unit="page" from="5184" to="5193" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Reasoning with neural tensor networks for knowledge base completion</title>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2013/hash/b337e84de8752b27eda3a12363109e80-Abstract.html" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 26: 27th Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>Lake Tahoe, Nevada, United States</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-12-05">2013. December 5-8, 2013. 2013</date>
			<biblScope unit="page" from="926" to="934" />
		</imprint>
	</monogr>
	<note>Proceedings of a meeting held</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Convolutional 2d knowledge graph embeddings</title>
		<author>
			<persName><forename type="first">Tim</forename><surname>Dettmers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pasquale</forename><surname>Minervini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pontus</forename><surname>Stenetorp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<ptr target="https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/17366" />
	</analytic>
	<monogr>
		<title level="m">AAAI-18), the 30th innovative Applications of Artificial Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational Advances in Artificial Intelligence (EAAI-18)</title>
		<meeting><address><addrLine>New Orleans, Louisiana, USA</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2018">February 2-7, 2018. 2018</date>
			<biblScope unit="page" from="1811" to="1818" />
		</imprint>
	</monogr>
	<note>Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">A novel embedding model for knowledge base completion based on convolutional neural network</title>
		<author>
			<persName><forename type="first">Tu</forename><surname>Dai Quoc Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dat</forename><surname>Dinh Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dinh</forename><forename type="middle">Q</forename><surname>Quoc Nguyen</surname></persName>
		</author>
		<author>
			<persName><surname>Phung</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/n18-2053</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT<address><addrLine>New Orleans, Louisiana, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">June 1-6, 2018. 2018</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="327" to="333" />
		</imprint>
	</monogr>
	<note>Short Papers</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Adaptive convolution for multi-relational learning</title>
		<author>
			<persName><forename type="first">Xiaotian</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/n19-1103</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019<address><addrLine>Minneapolis, MN, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">June 2-7, 2019. 2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="978" to="987" />
		</imprint>
	</monogr>
	<note>Long and Short Papers</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Interacte: Improving convolution-based knowledge graph embeddings by increasing feature interactions</title>
		<author>
			<persName><forename type="first">Shikhar</forename><surname>Vashishth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soumya</forename><surname>Sanyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nilesh</forename><surname>Vikram Nitin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Partha</forename><forename type="middle">P</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><surname>Talukdar</surname></persName>
		</author>
		<ptr target="https://ojs.aaai.org/index.php/AAAI/article/view/5694" />
	</analytic>
	<monogr>
		<title level="m">The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The Thirty-Second Innovative Applications of Artificial Intelligence Conference, IAAI 2020, The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2020</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2020">February 7-12, 2020. 2020</date>
			<biblScope unit="page" from="3009" to="3016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Modeling relational data with graph convolutional networks</title>
		<author>
			<persName><forename type="first">Sejr</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">N</forename><surname>Schlichtkrull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rianne</forename><surname>Bloem</surname></persName>
		</author>
		<author>
			<persName><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-93417-4_38</idno>
	</analytic>
	<monogr>
		<title level="m">The Semantic Web -15th International Conference, ESWC 2018</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<meeting><address><addrLine>Heraklion, Crete, Greece</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">June 3-7, 2018. 2018</date>
			<biblScope unit="volume">10843</biblScope>
			<biblScope unit="page" from="593" to="607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Comprehensive analysis of knowledge graph embedding techniques benchmarked on link prediction</title>
		<author>
			<persName><forename type="first">Ilaria</forename><surname>Ferrari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Giacomo</forename><surname>Frisoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paolo</forename><surname>Italiani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gianluca</forename><surname>Moro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Claudio</forename><surname>Sartori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Electronics</title>
		<idno type="ISSN">2079- 9292</idno>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">23</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">End-to-end structure-aware convolutional networks for knowledge base completion</title>
		<author>
			<persName><forename type="first">Chao</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yun</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinbo</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="DOI">10.1609/aaai.v33i01.33013060</idno>
	</analytic>
	<monogr>
		<title level="m">The Thirty-Third AAAI Conference on Artificial Intelligence, AAAI 2019, The Thirty-First Innovative Applications of Artificial Intelligence Conference, IAAI 2019, The Ninth AAAI Symposium on Educational Advances in Artificial Intelligence</title>
		<meeting><address><addrLine>EAAI; Honolulu, Hawaii, USA</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2019-01-27">2019. January 27 -February 1, 2019. 2019</date>
			<biblScope unit="page" from="3060" to="3067" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Learning attention-based embeddings for relation prediction in knowledge graphs</title>
		<author>
			<persName><forename type="first">Deepak</forename><surname>Nathani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jatin</forename><surname>Chauhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charu</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manohar</forename><surname>Kaul</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/p19-1466</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019</title>
		<meeting>the 57th Conference of the Association for Computational Linguistics, ACL 2019<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-08-02">July 28-August 2, 2019. 2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4710" to="4723" />
		</imprint>
	</monogr>
	<note>Long Papers</note>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Composition-based multi-relational graph convolutional networks</title>
		<author>
			<persName><forename type="first">Shikhar</forename><surname>Vashishth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soumya</forename><surname>Sanyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nitin</forename><surname>Vikram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Partha</forename><forename type="middle">P</forename><surname>Talukdar</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=BylA_C4tPr" />
	</analytic>
	<monogr>
		<title level="m">8th International Conference on Learning Representations, ICLR 2020</title>
		<meeting><address><addrLine>Addis Ababa, Ethiopia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">April 26-30, 2020. 2020</date>
		</imprint>
	</monogr>
	<note>OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Disenkgat: Knowledge graph embedding with disentangled graph attention network</title>
		<author>
			<persName><forename type="first">Junkang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wentao</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuezhi</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenqiang</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fuzheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<idno type="DOI">10.1145/3459637.3482424</idno>
	</analytic>
	<monogr>
		<title level="m">CIKM &apos;21: The 30th ACM International Conference on Information and Knowledge Management, Virtual Event</title>
		<meeting><address><addrLine>Queensland, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2021">November 1 -5, 2021. 2021</date>
			<biblScope unit="page" from="2140" to="2149" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Autoeter: Automated entity type representation with relation-aware attention for knowledge graph embedding</title>
		<author>
			<persName><forename type="first">Guanglin</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongfei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shiliang</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingyang</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.findings-emnlp.105</idno>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: EMNLP 2020, Online Event</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020-11-20">16-20 November 2020. 2020</date>
			<biblScope unit="page" from="1172" to="1181" />
		</imprint>
	</monogr>
	<note>EMNLP 2020 of Findings of ACL</note>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Type-augmented relation prediction in knowledge graphs</title>
		<author>
			<persName><forename type="first">Zijun</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pavan</forename><surname>Kapanipathi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kartik</forename><surname>Talamadupula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tian</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Ji</surname></persName>
		</author>
		<ptr target="https://ojs.aaai.org/index.php/AAAI/article/view/16879" />
	</analytic>
	<monogr>
		<title level="m">Thirty-Fifth AAAI Conference on Artificial Intelligence, AAAI 2021, Thirty-Third Conference on Innovative Applications of Artificial Intelligence, IAAI 2021, The Eleventh Symposium on Educational Advances in Artificial Intelligence, EAAI 2021, Virtual Event</title>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2021">February 2-9, 2021. 2021</date>
			<biblScope unit="page" from="7151" to="7159" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Representation learning of knowledge graphs with hierarchical types</title>
		<author>
			<persName><forename type="first">Ruobing</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<ptr target="http://www.ijcai.org/Abstract/16/421" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Fifth International Joint Conference on Artificial Intelligence, IJCAI 2016</title>
		<meeting>the Twenty-Fifth International Joint Conference on Artificial Intelligence, IJCAI 2016<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IJCAI/AAAI Press</publisher>
			<date type="published" when="2016-07-15">9-15 July 2016. 2016</date>
			<biblScope unit="page" from="2965" to="2971" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Differentiating concepts and instances for knowledge graph embedding</title>
		<author>
			<persName><forename type="first">Xin</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juanzi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/d18-1222</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018-11-04">October 31 -November 4, 2018. 2018</date>
			<biblScope unit="page" from="1971" to="1979" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Transet: Knowledge graph embedding with entity types</title>
		<author>
			<persName><forename type="first">Peng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuzhang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xingchen</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Electronics</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page">1407</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">A translation approach to portable ontology specifications</title>
		<author>
			<persName><forename type="first">R</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><surname>Gruber</surname></persName>
		</author>
		<idno type="DOI">10.1006/knac.1993.1008</idno>
	</analytic>
	<monogr>
		<title level="j">Knowl. Acquis</title>
		<idno type="ISSN">1042-8143</idno>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="199" to="220" />
			<date type="published" when="1993-06">jun 1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Bringing light into the dark: A large-scale evaluation of knowledge graph embedding models under a unified framework</title>
		<author>
			<persName><forename type="first">Mehdi</forename><surname>Ali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Berrendorf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><forename type="middle">Tapley</forename><surname>Hoyt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurent</forename><surname>Vermue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mikhail</forename><surname>Galkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sahand</forename><surname>Sharifzadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Asja</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jens</forename><surname>Volker Tresp</surname></persName>
		</author>
		<author>
			<persName><surname>Lehmann</surname></persName>
		</author>
		<idno type="DOI">10.1109/TPAMI.2021.3124805</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="8825" to="8845" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Injecting background knowledge into embedding models for predictive tasks on knowledge graphs</title>
		<author>
			<persName><forename type="first">Nicola</forename><forename type="middle">Flavio</forename><surname>Claudia D'amato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicola</forename><surname>Quatraro</surname></persName>
		</author>
		<author>
			<persName><surname>Fanizzi</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-77385-4_26</idno>
	</analytic>
	<monogr>
		<title level="m">The Semantic Web -18th International Conference, ESWC 2021, Virtual Event</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">June 6-10, 2021. 2021</date>
			<biblScope unit="volume">12731</biblScope>
			<biblScope unit="page" from="441" to="457" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Development and application of a metric on semantic nets</title>
		<author>
			<persName><forename type="first">Roy</forename><surname>Rada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hafedh</forename><surname>Mili</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ellen</forename><surname>Bicknell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maria</forename><surname>Blettner</surname></persName>
		</author>
		<idno type="DOI">10.1109/21.24528</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Syst. Man Cybern</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="17" to="30" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Verbs semantics and lexical selection</title>
		<author>
			<persName><forename type="first">Zhibiao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martha</forename><surname>Palmer</surname></persName>
		</author>
		<idno type="DOI">10.3115/981732.981751</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd Annual Meeting on Association for Computational Linguistics, ACL &apos;94</title>
		<title level="s">Association for Computational Linguistics</title>
		<meeting>the 32nd Annual Meeting on Association for Computational Linguistics, ACL &apos;94<address><addrLine>USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="133" to="138" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title level="m" type="main">Combining Local Context and WordNet Similarity for Word Sense Identification</title>
		<author>
			<persName><forename type="first">Claudia</forename><surname>Leacock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Chodorow</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="page" from="265" to="266" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Semantic similarity in a taxonomy: An information-based measure and its application to problems of ambiguity in natural language</title>
		<author>
			<persName><forename type="first">Philip</forename><surname>Resnik</surname></persName>
		</author>
		<idno type="DOI">10.1613/jair.514</idno>
	</analytic>
	<monogr>
		<title level="j">J. Artif. Intell. Res</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="95" to="130" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">An approach for measuring semantic similarity between words using multiple information sources</title>
		<author>
			<persName><forename type="first">Yuhua</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zuhair</forename><surname>Bandar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Mclean</surname></persName>
		</author>
		<idno type="DOI">10.1109/TKDE.2003.1209005</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Knowl. Data Eng</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="871" to="882" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Freebase: a collaboratively created graph database for structuring human knowledge</title>
		<author>
			<persName><forename type="first">Kurt</forename><forename type="middle">D</forename><surname>Bollacker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Colin</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Praveen</forename><forename type="middle">K</forename><surname>Paritosh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Sturge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jamie</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the ACM SIGMOD International Conf. on Management of Data</title>
		<meeting>of the ACM SIGMOD International Conf. on Management of Data</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1247" to="1250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Dbpedia: A nucleus for a web of open data</title>
		<author>
			<persName><forename type="first">Sören</forename><surname>Auer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Bizer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georgi</forename><surname>Kobilarov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jens</forename><surname>Lehmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Cyganiak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zachary</forename><forename type="middle">G</forename><surname>Ives</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Semantic Web, 6th International Semantic Web Conf., 2nd Asian Semantic Web Conf., ISWC + ASWC</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="volume">4825</biblScope>
			<biblScope unit="page" from="722" to="735" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Yago: a core of semantic knowledge</title>
		<author>
			<persName><forename type="first">Fabian</forename><forename type="middle">M</forename><surname>Suchanek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gjergji</forename><surname>Kasneci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gerhard</forename><surname>Weikum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 16th International Conf. on World Wide Web, WWW</title>
		<meeting>of the 16th International Conf. on World Wide Web, WWW</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="697" to="706" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Observed versus latent features for knowledge base and text inference</title>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 3rd Workshop on Continuous Vector Space Models and their Compositionality</title>
		<meeting>of the 3rd Workshop on Continuous Vector Space Models and their Compositionality</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="57" to="66" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Improving knowledge graph embedding using simple constraints</title>
		<author>
			<persName><forename type="first">Boyang</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Guo</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P18-1011</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, ACL 2018</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics, ACL 2018<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">July 15-20, 2018. 2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="110" to="121" />
		</imprint>
	</monogr>
	<note>Long Papers</note>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">YAGO 4: A reason-able knowledge base</title>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Pellissier Tanon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gerhard</forename><surname>Weikum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabian</forename><forename type="middle">M</forename><surname>Suchanek</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-49461-2_34</idno>
	</analytic>
	<monogr>
		<title level="m">The Semantic Web -17th International Conference, ESWC 2020</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<meeting><address><addrLine>Heraklion, Crete, Greece</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020-06-04">May 31-June 4, 2020. 2020</date>
			<biblScope unit="volume">12123</biblScope>
			<biblScope unit="page" from="583" to="596" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Wordnet: A lexical database for english</title>
		<author>
			<persName><forename type="first">George</forename><forename type="middle">A</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="39" to="41" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Codex: A comprehensive knowledge graph completion benchmark</title>
		<author>
			<persName><forename type="first">Tara</forename><surname>Safavi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danai</forename><surname>Koutra</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.669</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP 2020</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP 2020<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020">November 16-20, 2020. 2020</date>
			<biblScope unit="page" from="8328" to="8350" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">A re-evaluation of knowledge graph completion methods</title>
		<author>
			<persName><forename type="first">Zhiqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shikhar</forename><surname>Vashishth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soumya</forename><surname>Sanyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Partha</forename><forename type="middle">P</forename><surname>Talukdar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 58th Annual Meeting of the Association for Computational Linguistics ACL</title>
		<meeting>of the 58th Annual Meeting of the Association for Computational Linguistics ACL</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="5516" to="5522" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">A lightweight knowledge graph embedding framework for efficient inference and storage</title>
		<author>
			<persName><forename type="first">Haoyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaqing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Defu</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Gao</surname></persName>
		</author>
		<idno type="DOI">10.1145/3459637.3482224</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th ACM International Conference on Information and Knowledge Management, CIKM &apos;21</title>
		<meeting>the 30th ACM International Conference on Information and Knowledge Management, CIKM &apos;21<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1909" to="1918" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Rdf2vec light -A lightweight approachfor knowledge graph embeddings</title>
		<author>
			<persName><forename type="first">Jan</forename><surname>Portisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Hladik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heiko</forename><surname>Paulheim</surname></persName>
		</author>
		<ptr target="http://ceur-ws.org/Vol-2721/paper520.pdf" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ISWC 2020 Demos and Industry Tracks: From Novel Ideas to Industrial Practice co-located with 19th International Semantic Web Conference (ISWC 2020), Globally online</title>
		<meeting>the ISWC 2020 Demos and Industry Tracks: From Novel Ideas to Industrial Practice co-located with 19th International Semantic Web Conference (ISWC 2020), Globally online</meeting>
		<imprint>
			<date type="published" when="2020">November 1-6, 2020 (UTC. 2020</date>
			<biblScope unit="volume">2721</biblScope>
			<biblScope unit="page" from="79" to="84" />
		</imprint>
	</monogr>
	<note>CEUR Workshop Proceedings</note>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Highly efficient knowledge graph embedding learning with Orthogonal Procrustes Analysis</title>
		<author>
			<persName><forename type="first">Xutan</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guanyi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenghua</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Stevenson</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.naacl-main.187</idno>
		<ptr target="https://aclanthology.org/2021.naacl-main.187" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Association for Computational Linguistics</title>
		<meeting>the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2021-06">June 2021</date>
			<biblScope unit="page" from="2364" to="2375" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<monogr>
		<title level="m" type="main">XTransE: Explainable Knowledge Graph Embedding for Link Prediction with Lifestyles in e-Commerce</title>
		<author>
			<persName><forename type="first">Wen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shumin</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huajun</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-981-15-3412-6_8</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="78" to="87" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Explaining link prediction systems based on knowledge graph embeddings</title>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Rossi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donatella</forename><surname>Firmani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paolo</forename><surname>Merialdo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tommaso</forename><surname>Teofili</surname></persName>
		</author>
		<idno type="DOI">10.1145/3514221.3517887</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2022 International Conference on Management of Data, SIGMOD &apos;22</title>
		<meeting>the 2022 International Conference on Management of Data, SIGMOD &apos;22<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="2062" to="2075" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Logic tensor networks</title>
		<author>
			<persName><forename type="first">Samy</forename><surname>Badreddine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Artur</forename><forename type="middle">S</forename><surname>Garcez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luciano</forename><surname>Serafini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Spranger</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.artint.2021.103649</idno>
		<ptr target="https://doi.org/10.1016/j.artint.2021.103649" />
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">303</biblScope>
			<biblScope unit="page">103649</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
