<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Privacy and utility of x-vector based speaker anonymization</title>
				<funder ref="#_qrm4EgW #_UBn9Dfv">
					<orgName type="full">Agence Nationale de la Recherche</orgName>
					<orgName type="abbreviated">ANR</orgName>
				</funder>
				<funder>
					<orgName type="full">Japan Science and Technology Agency</orgName>
				</funder>
				<funder ref="#_xn4TdPy">
					<orgName type="full">French National Research Agency</orgName>
				</funder>
				<funder>
					<orgName type="full">Inria</orgName>
				</funder>
				<funder>
					<orgName type="full">CNRS</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Brij</forename><surname>Mohan</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">UMR 9189</orgName>
								<orgName type="institution" key="instit1">Univ. Lille</orgName>
								<orgName type="institution" key="instit2">Inria</orgName>
								<orgName type="institution" key="instit3">CNRS</orgName>
								<orgName type="institution" key="instit4">Centrale Lille</orgName>
								<address>
									<addrLine>-CRIStAL</addrLine>
									<postCode>F-59000</postCode>
									<settlement>Lille</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Lal</forename><surname>Srivastava</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">UMR 9189</orgName>
								<orgName type="institution" key="instit1">Univ. Lille</orgName>
								<orgName type="institution" key="instit2">Inria</orgName>
								<orgName type="institution" key="instit3">CNRS</orgName>
								<orgName type="institution" key="instit4">Centrale Lille</orgName>
								<address>
									<addrLine>-CRIStAL</addrLine>
									<postCode>F-59000</postCode>
									<settlement>Lille</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Mohamed</forename><surname>Maouche</surname></persName>
							<email>mohamed.maouche@inria.fr</email>
							<affiliation key="aff0">
								<orgName type="laboratory">UMR 9189</orgName>
								<orgName type="institution" key="instit1">Univ. Lille</orgName>
								<orgName type="institution" key="instit2">Inria</orgName>
								<orgName type="institution" key="instit3">CNRS</orgName>
								<orgName type="institution" key="instit4">Centrale Lille</orgName>
								<address>
									<addrLine>-CRIStAL</addrLine>
									<postCode>F-59000</postCode>
									<settlement>Lille</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">Université de Lorraine</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
								<orgName type="institution" key="instit3">Inria</orgName>
								<orgName type="institution" key="instit4">LORIA</orgName>
								<address>
									<postCode>F-54000</postCode>
									<settlement>Nancy</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Md</forename><surname>Sahidullah</surname></persName>
							<email>md.sahidullah@inria.fr</email>
							<affiliation key="aff0">
								<orgName type="laboratory">UMR 9189</orgName>
								<orgName type="institution" key="instit1">Univ. Lille</orgName>
								<orgName type="institution" key="instit2">Inria</orgName>
								<orgName type="institution" key="instit3">CNRS</orgName>
								<orgName type="institution" key="instit4">Centrale Lille</orgName>
								<address>
									<addrLine>-CRIStAL</addrLine>
									<postCode>F-59000</postCode>
									<settlement>Lille</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="laboratory">Laboratoire Informatique d&apos;Avignon (LIA)</orgName>
								<orgName type="institution">Avignon Université</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Emmanuel</forename><surname>Vincent</surname></persName>
							<email>emmanuel.vincent@inria.fr</email>
							<affiliation key="aff0">
								<orgName type="laboratory">UMR 9189</orgName>
								<orgName type="institution" key="instit1">Univ. Lille</orgName>
								<orgName type="institution" key="instit2">Inria</orgName>
								<orgName type="institution" key="instit3">CNRS</orgName>
								<orgName type="institution" key="instit4">Centrale Lille</orgName>
								<address>
									<addrLine>-CRIStAL</addrLine>
									<postCode>F-59000</postCode>
									<settlement>Lille</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="laboratory">Laboratoire Informatique d&apos;Avignon (LIA)</orgName>
								<orgName type="institution">Avignon Université</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Aurélien</forename><surname>Bellet</surname></persName>
							<email>aurelien.bellet@inria.fr</email>
							<affiliation key="aff0">
								<orgName type="laboratory">UMR 9189</orgName>
								<orgName type="institution" key="instit1">Univ. Lille</orgName>
								<orgName type="institution" key="instit2">Inria</orgName>
								<orgName type="institution" key="instit3">CNRS</orgName>
								<orgName type="institution" key="instit4">Centrale Lille</orgName>
								<address>
									<addrLine>-CRIStAL</addrLine>
									<postCode>F-59000</postCode>
									<settlement>Lille</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">Université de Lorraine</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
								<orgName type="institution" key="instit3">Inria</orgName>
								<orgName type="institution" key="instit4">LORIA</orgName>
								<address>
									<postCode>F-54000</postCode>
									<settlement>Nancy</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Marc</forename><surname>Tommasi</surname></persName>
							<email>marc.tommasi@univ-lille.fr</email>
							<affiliation key="aff0">
								<orgName type="laboratory">UMR 9189</orgName>
								<orgName type="institution" key="instit1">Univ. Lille</orgName>
								<orgName type="institution" key="instit2">Inria</orgName>
								<orgName type="institution" key="instit3">CNRS</orgName>
								<orgName type="institution" key="instit4">Centrale Lille</orgName>
								<address>
									<addrLine>-CRIStAL</addrLine>
									<postCode>F-59000</postCode>
									<settlement>Lille</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">Université de Lorraine</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
								<orgName type="institution" key="instit3">Inria</orgName>
								<orgName type="institution" key="instit4">LORIA</orgName>
								<address>
									<postCode>F-54000</postCode>
									<settlement>Nancy</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Natalia</forename><surname>Tomashenko</surname></persName>
							<email>natalia.tomashenko@univ-avignon.fr</email>
							<affiliation key="aff0">
								<orgName type="laboratory">UMR 9189</orgName>
								<orgName type="institution" key="instit1">Univ. Lille</orgName>
								<orgName type="institution" key="instit2">Inria</orgName>
								<orgName type="institution" key="instit3">CNRS</orgName>
								<orgName type="institution" key="instit4">Centrale Lille</orgName>
								<address>
									<addrLine>-CRIStAL</addrLine>
									<postCode>F-59000</postCode>
									<settlement>Lille</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xin</forename><surname>Wang</surname></persName>
							<email>wangxin@nii.ac.jp</email>
							<affiliation key="aff0">
								<orgName type="laboratory">UMR 9189</orgName>
								<orgName type="institution" key="instit1">Univ. Lille</orgName>
								<orgName type="institution" key="instit2">Inria</orgName>
								<orgName type="institution" key="instit3">CNRS</orgName>
								<orgName type="institution" key="instit4">Centrale Lille</orgName>
								<address>
									<addrLine>-CRIStAL</addrLine>
									<postCode>F-59000</postCode>
									<settlement>Lille</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Junichi</forename><surname>Yamagishi</surname></persName>
							<email>jyamagis@nii.ac.jp</email>
							<affiliation key="aff0">
								<orgName type="laboratory">UMR 9189</orgName>
								<orgName type="institution" key="instit1">Univ. Lille</orgName>
								<orgName type="institution" key="instit2">Inria</orgName>
								<orgName type="institution" key="instit3">CNRS</orgName>
								<orgName type="institution" key="instit4">Centrale Lille</orgName>
								<address>
									<addrLine>-CRIStAL</addrLine>
									<postCode>F-59000</postCode>
									<settlement>Lille</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">National Institute of Informatics</orgName>
								<address>
									<settlement>Tokyo</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">B</forename><forename type="middle">M L</forename><surname>Srivastava</surname></persName>
							<email>brij.srivastava@inria.fr</email>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">Université de Lorraine</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
								<orgName type="institution" key="instit3">Inria</orgName>
								<orgName type="institution" key="instit4">LORIA</orgName>
								<address>
									<postCode>F-54000</postCode>
									<settlement>Nancy</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Privacy and utility of x-vector based speaker anonymization</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">24ED59697601D2C2B8C6550C08E34A0A</idno>
					<idno type="DOI">10.1109/TASLP.2022.3190741</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-04-12T14:44+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>speaker anonymization</term>
					<term>privacy</term>
					<term>linkability</term>
					<term>voice conversion</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We study the scenario where individuals (speakers) contribute to the publication of an anonymized speech corpus. Data users leverage this public corpus for downstream tasks, e.g., training an automatic speech recognition (ASR) system, while attackers may attempt to de-anonymize it using auxiliary knowledge. Motivated by this scenario, speaker anonymization aims to conceal speaker identity while preserving the quality and usefulness of speech data. In this article, we study xvector based speaker anonymization, the leading approach in the VoicePrivacy Challenge, which converts the speaker's voice into that of a random pseudo-speaker. We show that the strength of anonymization varies significantly depending on how the pseudospeaker is chosen. We explore four design choices for this step: the distance metric between speakers, the region of speaker space where the pseudo-speaker is picked, its gender, and whether to assign it to one or all utterances of the original speaker. We assess the quality of anonymization from the perspective of the three actors involved in our threat model, namely the speaker, the user and the attacker. To measure privacy and utility, we use respectively the linkability score achieved by the attackers and the decoding word error rate achieved by an ASR model trained on the anonymized data. Experiments on LibriSpeech show that the best combination of design choices yields state-of-the-art performance in terms of both privacy and utility. Experiments on Mozilla Common Voice further show that it guarantees the same anonymization level against re-identification attacks among 50 speakers as original speech among 20,000 speakers.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>S PEECH is a rich source of personal information including sensitive attributes such as identity <ref type="bibr" target="#b0">[1]</ref>, accent <ref type="bibr" target="#b1">[2]</ref>, or pathological condition <ref type="bibr" target="#b2">[3]</ref>. When the speaker's goal is not biometric authentication but some other voice-based interaction, for example, exchanging with voice assistants or customer helplines, speaker anonymization is desirable. Indeed, the availability of efficient techniques to infer these attributes from speech as well as recent advances in voice cloning <ref type="bibr" target="#b3">[4]</ref> pose severe privacy and security risks <ref type="bibr" target="#b4">[5]</ref>.</p><p>Throughout this article, we consider the following threat model. Given a public dataset of anonymized speech contributed by several speakers, an attacker records/finds a sample of speech of a speaker and attempts to find which utterances in the anonymized dataset are spoken by this speaker, possibly leveraging some knowledge about the anonymization method. A good speaker anonymization method must defeat such linkage attacks by concealing speaker identity, while preserving the utility of speech for data users as measured for instance by the perceived speech naturalness and/or the performance of downstream tasks such as training an automatic speech recognition (ASR) system. <ref type="foot" target="#foot_0">1</ref> Figure <ref type="figure" target="#fig_0">1</ref> shows the three actors involved in this model, namely the speaker, the attacker and the user, along with their actions. The goals of the speaker and the user are intimately linked, while the attacker operates independently.</p><p>Speaker anonymization methods have been studied for just over a decade. They include noise addition <ref type="bibr" target="#b6">[7]</ref>, speech transformation <ref type="bibr" target="#b7">[8]</ref>, voice conversion <ref type="bibr" target="#b8">[9]</ref>- <ref type="bibr" target="#b10">[11]</ref>, speech synthesis <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>, or adversarial learning <ref type="bibr" target="#b13">[14]</ref>. In this study, we focus on x-vector based anonymization <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b14">[15]</ref>, which converts the original speaker's voice into that of a target (pseudo-)speaker, due to the naturalness of its output and its promising results so far. In order to implement and assess such an anonymization method, the following questions arise from the speaker's and user's perspectives: Q1: How to optimally choose and assign the target pseudo-speaker? Q2: How well is utility preserved? Q3: How much residual speaker information remains? Furthermore, the attacker must address the following questions: Q4: Can privacy protection be defeated using some knowledge of the anonymization method? Q5: How does the number of possible speakers affect the re-identification performance?</p><p>In this article, we extend the two target pseudo-speaker generation strategies in <ref type="bibr" target="#b12">[13]</ref> (fully random, or at a fixed distance from the original as measured by cosine distance between x-vectors) into a whole family of strategies based on four design choices: the distance metric between x-vectors, the region of x-vector space where the pseudo-speaker is picked, its gender, and whether to assign it to one or all utterances of the original speaker. Our experiments suggest an optimal combination of design choices to balance privacy and utility (answering Q1). We train and/or evaluate ASR models on anonymized speech to assess utility (answering Q2). We show that some speaker information remains in the pitch sequence and apply two different pitch transformation techniques to remove it (answering Q3). We conduct these experiments for three types of attackers <ref type="bibr" target="#b15">[16]</ref>, where stronger attackers have more knowledge about the anonymization method (answering Q4). Finally, we conduct additional experiments with more than 20,000 possible speakers (answering Q5). These contributions significantly extend our preliminary study <ref type="bibr" target="#b16">[17]</ref>, which provided less detail, did not include utterance-vs. speaker-level target assignment and pitch transformation, did not evaluate privacy against the strongest (Semi-Informed) attacker or with a large number of possible speakers, and did not evaluate utility for ASR training.</p><p>The structure of the article is as follows. In Section II, we introduce x-vector based anonymization and position it among other related anonymization methods. Section III presents the four considered design choices. Sections IV and V describe the main experimental setup and the corresponding results. Experiments with more speakers are conducted in Section VI. We conclude in Section VII.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. X-VECTOR BASED ANONYMIZATION</head><p>Speaker anonymization aims to conceal the speaker's identity from a speech signal such that it cannot be used to clone the speaker's voice or to re-identify the speaker through automatic speaker verification (ASV) or automatic speaker identification (ASI). Early methods based on voice conversion required both the original and the target speaker to be part of the training set for the voice conversion system. Jin et al. <ref type="bibr" target="#b8">[9]</ref> convert all speakers into a single target. Bahmaninezhad et al. <ref type="bibr" target="#b10">[11]</ref> convert a given speaker into the average of all speakers of the same gender. Pobar and Ipšić <ref type="bibr" target="#b9">[10]</ref> pre-train a set of speaker transformations and identify the speaker at test time to select one of the corresponding transformations. These methods are hardly applicable in practice since, in the context of anonymization, the amount of speech from the original speaker is often limited to one utterance. To relax this constraint, Magariños et al. <ref type="bibr" target="#b17">[18]</ref> find the closest source speaker in the training set and apply one of the corresponding transformations, while Justin et al. <ref type="bibr" target="#b11">[12]</ref> transcribe speech into a diphone sequence and re-synthesize it using a single target. Although they do not require the original speaker to belong to the training set anymore, these two methods suffer from three limitations. First, they still result in a limited set of target speakers or speaker transformations, which prevents the original speaker from choosing an arbitrary unseen speaker as the target. Second, using a real speaker's voice as the target raises ethical concerns. Third, the phonetic transcription step in <ref type="bibr" target="#b11">[12]</ref> is error-prone. This motivates the objective of converting the original speaker's voice into an arbitrary, imaginary pseudo-speaker's voice without relying on a transcription step. Speaker embeddings such as x-vectors <ref type="bibr" target="#b18">[19]</ref> (a low-dimensional representation extracted from an intermediate layer of an ASI model) provide the continuous representation needed to define and generate such pseudo-speakers.</p><p>Fang et al. <ref type="bibr" target="#b12">[13]</ref> address this objective using a speakerindependent speech synthesis system. They select x-vectors within an external pool of speakers and average them to obtain a target pseudo-speaker x-vector. This x-vector, along with a representation of the original linguistic and intonation contents, is provided as input to a neural source-filter (NSF) based speech synthesizer <ref type="bibr" target="#b19">[20]</ref> to produce anonymized speech.</p><p>In the following, we use the anonymization system shown in Fig. <ref type="figure" target="#fig_1">2</ref>, that is a variant of the one in <ref type="bibr" target="#b12">[13]</ref>. This system represents speaker identity, linguistic content and intonation using x-vectors v, 2 bottleneck (BN) features B <ref type="bibr" target="#b20">[21]</ref> (a lowdimensional representation extracted from an intermediate layer of an ASR model) and pitch sequences p, respectively. It comprises four steps: Step 1 (Feature extraction) extracts pitch and BN features and the x-vector from the input signal.</p><p>Step 2 (X-vector anonymization) generates a target x-vector v * by averaging N * candidate x-vectors from an external pool 2 Following <ref type="bibr" target="#b12">[13]</ref>, we use raw x-vectors to represent speaker identity instead of x-vectors compressed and rotated by linear discriminant analysis (LDA), as classically done in the context of ASV. Unless the projected dimension is carefully chosen after several experiments, the impact of the LDA transformation on speaker-specific information cannot be ascertained. Hence we defer experiments with LDA-transformed x-vectors to a future study. of speakers. <ref type="foot" target="#foot_1">3</ref> Step 3 (Pitch conversion) is an optional step which receives the target pitch statistics from the anonymization module and transforms the original pitch sequence into p * . Step 4 (Speech synthesis) synthesizes a speech waveform from the anonymized x-vector v * and the original B and p (or optionally p * ) features using an acoustic model (AM) and the NSF model. With the exception of Step 3 which is new (see Section V-D), this system is identical to the first anonymization baseline for the VoicePrivacy Challenge <ref type="bibr" target="#b5">[6]</ref>. We refer to <ref type="bibr" target="#b21">[22]</ref> for details on the feature dimensions and the architectures of the models in Steps 1 and 4 . We note that there have been other interesting attempts to generate a target pseudo-speaker x-vector for speaker anonymization in the systems submitted to the first VoicePrivacy Challenge. Mawalim et al. <ref type="bibr" target="#b22">[23]</ref> modified the significant elements of the source speaker x-vector that were determined using singular value decomposition and variant analysis to anonymize the identity. Perero-Codosero et al. <ref type="bibr" target="#b23">[24]</ref> transformed the original x-vector using an autoencoder with adversarial training to suppress speaker, gender and accent information. Turner et al. <ref type="bibr" target="#b24">[25]</ref> fitted a Gaussian mixture model based generative model over the external pool of speakers, and then proposed to sample target x-vectors from this model to preserve the distributional properties of x-vectors. Readers are referred to Tomashenko et al. <ref type="bibr" target="#b25">[26]</ref> for an in-depth analysis of the objective and subjective evaluation results achieved by the two challenge baselines and the 16 submissions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. ANONYMIZATION DESIGN CHOICES</head><p>Now given the ability to generate arbitrary external targets in Step 2 (yellow box in Fig. <ref type="figure" target="#fig_1">2</ref>), the question arises of which strategy the speaker shall employ to select the candidate xvectors and achieve a suitable privacy-utility tradeoff. Fang et al. <ref type="bibr" target="#b12">[13]</ref> select candidate x-vectors at random within the whole pool or within a fixed interval of distances from the original xvector. Han et al. <ref type="bibr" target="#b14">[15]</ref> select a single target x-vector at random within a maximum distance from the original x-vector. In the following, we expand these initial strategies into a broader range of strategies governed by the choice of the distance metric between x-vectors, the region of x-vector space where the candidates are selected, their gender, and the assignment of the resulting target x-vector to one or all utterances of the original speaker. These four design choices, which are illustrated in Fig. <ref type="figure">3</ref>, are detailed below. For the sake of focus, we do not explore other design choices such as the size or the diversity of the anonymization pool.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Distance metric: cosine vs. PLDA</head><p>To design advanced candidate selection strategies, the speaker must first choose a distance metric which dictates the properties of the x-vector space. We compare two such metrics.</p><p>The first one is the cosine distance, which was used by <ref type="bibr" target="#b12">[13]</ref>. For a pair of x-vectors ω i and ω j , it is defined as Fig. <ref type="figure">3</ref>. Zoomed-in view of the x-vector anonymization step in Fig. <ref type="figure" target="#fig_1">2</ref> showing the design choices for the generation of the target x-vector.</p><formula xml:id="formula_0">d cos (ω i , ω j ) = 1 - ω i • ω j ||ω i || 2 ||ω j || 2 .<label>(1)</label></formula><p>The second metric is based on probabilistic linear discriminant analysis (PLDA) <ref type="bibr" target="#b26">[27]</ref>, that is the log-likelihood ratio of the hypotheses that ω i and ω j belong to the same speaker (H s ) vs. different speakers (H d ). Previous studies <ref type="bibr" target="#b27">[28]</ref> have shown that PLDA yields state-of-the-art performance as the x-vector similarity metric in the context of ASV. This is attributed to its formulation which estimates the factorized within-speaker and between-speaker variability in speaker space, making it a superior metric even for short utterances <ref type="bibr" target="#b28">[29]</ref>. More specifically, PLDA models x-vectors ω as ω = m + V y + Dz, where m is the center of the x-vector space, the columns of V capture speaker variability (eigenvoices) with y depending only on the speaker, and the columns of D encode channel variability (eigenchannels) with z varying from one recording to another. The parameters m, V and D are trained on xvectors extracted using the x-vector extractor in Step 1 from the dataset used to train that extractor itself (see <ref type="bibr" target="#b21">[22]</ref> for details on this dataset). The log-likelihood ratio score</p><formula xml:id="formula_1">d PLDA (ω i , ω j ) = log p(ω i , ω j |H s ) p(ω i , ω j |H d )<label>(2)</label></formula><p>can be computed in closed form <ref type="bibr" target="#b29">[30]</ref>. We propose to use -d PLDA as the "distance" between a pair of x-vectors.</p><p>B. Proximity: random, far, near, dense, or sparse</p><p>We propose three alternative criteria resulting in five different "proximity" choices to restrict the region of x-vector space from which candidate x-vectors are selected.</p><p>1) Random: The simplest candidate x-vector selection strategy is to select N * x-vectors with a given gender uniformly at random from the pool. Note that this strategy does not allow us to choose particular regions of interest in the x-vector space.</p><p>2) Far/near: Alternatively, the chosen distance metric can be used to find candidate x-vectors which resemble most (near) or least (far) the original speaker v. In essence, we rank all the x-vectors in the pool in increasing order of their distance from v and select either the top N (near) or the bottom N (far). To introduce some randomness, N * &lt; N x-vectors are selected out of these N uniformly at random.</p><p>3) Dense/sparse: Another alternative is to identify clusters of x-vectors in the pool and rank them based on their cardinality. We construct these clusters using the Affinity Propagation <ref type="bibr" target="#b30">[31]</ref> algorithm (see detailed procedure in Section IV-B). We filter out the cluster which is closest to the original speaker, then randomly select one cluster among those with most (dense) or least (sparse) members. <ref type="foot" target="#foot_2">4</ref> We then randomly select half of the members of that cluster.</p><p>In all five cases, the selected candidate x-vectors are averaged to obtain the target (pseudo-speaker) x-vector v * .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Gender selection: same, opposite, or random</head><p>In practice, instead of applying one of these five proximity choices to the entire speaker pool, we apply it to a genderdependent pool which consists of either all males or all females of the original pool. We propose three possible gender selection choices: same where all speakers in the pool have the same gender as the original speaker; opposite where they all have the opposite gender; and random where either of the two gender-dependent pools is selected at random. This allows us to avoid averaging candidate x-vectors from both genders with each other, and to assess the impact of gender selection on privacy and utility.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Assignment: speaker-or utterance-level</head><p>The generation of the anonymized waveform is conditioned upon the x-vector sequence, whose length is equal to the number of frames in the original utterance. All the x-vectors in this sequence are identical to each other to indicate a single pseudo-speaker throughout the utterance. In theory, these xvectors should also be identical across all utterances spoken by this pseudo-speaker but, according to <ref type="bibr" target="#b31">[32]</ref>, x-vectors also contain channel, duration, and phonetic information, in addition to speaker and gender. Hence, the x-vectors computed for different utterances may exhibit some variations due to utterancespecific properties. To assess the effect of these variations on privacy and utility, we propose two assignment strategies for the target x-vector: speaker-level (perm) or utterance-level (rand). In the former case, we average the utterance-level xvectors of all utterances of the original speaker into a single speaker-level x-vector v, we generate a corresponding target x-vector v * , and we use it to anonymize all utterances of that speaker. In the latter case, we consider the utterance-level xvector v u for each utterance u of the original speaker, we generate a corresponding target x-vector v * u (using the same distance metric, proximity, and gender across all utterances), and we use it to anonymize that utterance only.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTAL SETUP</head><p>The VoicePrivacy Challenge <ref type="bibr" target="#b5">[6]</ref> assumed that the attacker does not have access to the anonymization system and that the user is unaware that speech has been anonymized. Privacy and utility were consequently assessed using an ASV system and an ASR system trained on original (non-anonymized) speech. This resulted in overestimated privacy and underestimated utility with respect to an attacker or a user who have access to the anonymization scheme <ref type="bibr" target="#b15">[16]</ref>. Also, the utility for ASR training was not evaluated. Following <ref type="bibr" target="#b15">[16]</ref>, we advocate for a complete study of the utility/privacy trade-off, which is key to the success of downstream tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Data</head><p>The experiments in Section V rely on the same datasets as the VoicePrivacy Challenge. Among the components of the anonymization system, the ASR AM is trained on the trainclean-100 and train-other-500 subsets of LibriSpeech <ref type="bibr" target="#b32">[33]</ref>, the x-vector extractor is trained on VoxCeleb1 <ref type="bibr" target="#b33">[34]</ref> and VoxCeleb2 <ref type="bibr" target="#b34">[35]</ref>, and the speech synthesis AM and NSF model are trained on the train-clean-100 subset of LibriTTS <ref type="bibr" target="#b35">[36]</ref>. The trainother-500 subset of LibriTTS is used as the external pool of speakers for x-vector anonymization. The development and test sets are built from the dev-clean and test-clean subsets of LibriSpeech, respectively. <ref type="foot" target="#foot_3">5</ref> Each of these two sets consists of trial utterances from 40 speakers and enrollment utterances from a subset of 29 speakers (see Section IV-C). Details about the number of male and female speakers and the number of utterances in each dataset can be found in <ref type="bibr" target="#b5">[6]</ref>.</p><p>In Section VI, we employ the same trained models and the same external pool of speakers but we build mutiple test sets from the Mozilla Common Voice <ref type="bibr" target="#b36">[37]</ref> English corpus in order to study the attacker's performance against a larger number of enrolled speakers. Following the approach in [38, Appendix A.1], we select 24,610 male speakers with a total speech duration greater than 10 s after removing silent frames using voice activity detection (VAD). All utterances with a signal-to-noise ratio (SNR) above 75 dB are used for enrollement, in the limit of a total duration of 2 min per speaker. <ref type="foot" target="#foot_4">6</ref> The remaining utterances from 20 speakers whose total duration is greater than 5 min are selected for trial. The resulting numbers of utterances and trials are given in Table <ref type="table">I</ref>.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Algorithm settings</head><p>The dense and sparse anonymization choices are implemented as follows. We use Affinity Propagation <ref type="bibr" target="#b30">[31]</ref> to cluster the speakers in the external pool. This non-parametric clustering method determines the number of clusters automatically via a message passing algorithm. Two parameters govern the number of clusters: the preference parameter assigns a higher weight to samples which are likely candidates for centroids, and the damping factor weights the so-called responsibility and availability messages. In our experiments, equal preference is assigned to all samples and the damping factor is set to 0.5. Out of 1,160 speakers in the pool, 80 clusters are found, including 46 male and 34 female. The number of speakers per cluster ranges from 6 to 36. Candidate x-vector selection is achieved by picking either the 10 clusters with least members (sparse) or the 10 clusters with most members (dense). The remaining clusters are ignored. During anonymization, one of the 10 clusters is selected at random and 50% of its members are averaged to produce the target x-vector.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Privacy evaluation</head><p>As explained in Section I, privacy protection can be seen as a contest between two entities: a speaker who publishes anonymized utterances, and an attacker who attempts to uncover the speaker's identity by comparing these utterances with utterances whose speaker is known. Following the classical ASV terminology adopted in the VoicePrivacy Challenge, these are called trial and enrollment utterances, respectively, and each such comparison is called a trial. The attacker has full control over the enrollment set and the speaker identities within it. Hence he/she may use some knowledge about the anonymization scheme to transform the enrollment data and reduce the mismatch with the trial data. To assess the strength of anonymization against attackers with increasing knowledge, we perform the evaluation in four scenarios:</p><p>• Baseline: The speaker does not perform any anonymization. The attacker uses original speech for enrollment and an ASV system trained on original speech. This offers the lowest possible privacy protection. • Ignorant: The speaker anonymizes his/her speech, unbeknownst to the attacker who still uses original speech for enrollment and an ASV system trained on original speech.</p><p>• Lazy-Informed: The speaker anonymizes his/her speech.</p><p>The attacker anonymizes the enrollment data using the same anonymization system and the same design choices. However, he/she is not aware of the random numbers drawn by the speaker to obtain the random target gender (Section III-C) or the candidate x-vectors (Section III-B). Hence, different pseudo-speakers are assigned to the trial and enrollment utterances of a given speaker.<ref type="foot" target="#foot_5">7</ref> • Semi-Informed The speaker anonymizes his/her speech.</p><p>The attacker anonymizes the enrollment data using the same system and design choices. In addition, he/she anonymizes the training dataset for the ASV system and re-trains it. This scenario is the one in which the speaker is most "vulnerable" despite anonymization, hence we consider it as the most trustworthy assessment of privacy. <ref type="foot" target="#foot_6">8</ref>In Section V, privacy is assessed in terms of the linkability <ref type="bibr" target="#b39">[40]</ref>, <ref type="bibr" target="#b40">[41]</ref>, denoted as D sys ↔ , achieved by an x-vector-PLDA ASV system trained on the train-clean-360 subset of Lib-riSpeech (anonymized in the Semi-Informed scenario, original otherwise). This metric computes the overlap between the distributions of PLDA scores of same-speaker and differentspeaker trials. It behaves similarly to the equal error rate (EER) and log-likelihood ratio cost function <ref type="bibr" target="#b41">[42]</ref> used in the VoicePrivacy Challenge, but it does not rely on any restrictive assumption (e.g., threshold-based decision) which makes it a more trustworthy metric <ref type="bibr" target="#b40">[41]</ref>. For the sake of reproducibility, we use the same set of trials as in <ref type="bibr" target="#b5">[6]</ref>. <ref type="foot" target="#foot_7">9</ref> Linkability varies from 0 to 1, where lower values indicate higher privacy. The 95% confidence interval on the linkability computed via the jackknife method <ref type="bibr" target="#b42">[43]</ref> varies from ±0.0001 to ±0.0002.</p><p>Formally, the local linkability metric D ↔ (θ) for two random utterances i and j with a score</p><formula xml:id="formula_2">θ = d cos (ω i , ω j ) or θ = -d PLDA (ω i , ω j ) is defined as p(H s | θ)-p(H d | θ).</formula><p>When the local metric is negative, an attacker can deduce with some confidence that the two utterances are from different speakers. The authors in <ref type="bibr" target="#b39">[40]</ref> argued that the local metric should estimate the strength of the link described by a score rather than measure how much a score describes different-speaker relationships. Therefore they proposed a clipped version of the difference:</p><formula xml:id="formula_3">D ↔ (θ) = max(0, p(H s | θ) -p(H d | θ)).<label>(3)</label></formula><p>The global linkability metric D sys ↔ is then defined as the mean value of D ↔ (θ) over all same-speaker scores:</p><formula xml:id="formula_4">D sys ↔ = p(θ | H s ) • D ↔ (θ) dθ.</formula><p>In practice, D ↔ (θ) is rewritten as In Section VI, we also evaluate the average rank of the true speaker and the top-k precision achieved for closed-set ASI. Instead of training speaker classification systems on subsets of Common Voice, which would overfit the speakers therein, we compute the PLDA scores between each trial utterance and all enrollment utterances (one per speaker, including the true speaker) using the same x-vector and PLDA models as in Section V and sort them in decreasing order. The higher the rank and the lower the top-k precision, the higher the privacy.</p><formula xml:id="formula_5">(2•α•lr(θ))/(1+α•lr(θ))-</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Utility evaluation</head><p>In Section V-A, we evaluate the utility for ASR decoding in terms of the word error rate (WER) achieved by an ASR system trained on the train-clean-360 subset of LibriSpeech and applied to the anonymized utterances. In Sections V-B and V-C, we evaluate the utility both for ASR decoding and training in terms of the WER achieved by an ASR system trained either on the original or the anonymized train-clean-360 dataset and used to decode either original or anonymized speech. For more details on the ASR system architecture, see <ref type="bibr" target="#b5">[6]</ref>. A lower WER indicates higher utility. The 95% confidence interval on the WER varies from ±0.2% for the lowest WER values to ±0.4% for the highest ones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. RESULTS AND DISCUSSION</head><p>The design choices introduced in Section III result in 54 possible combinations, among which 48 combinations correspond to 2 distances × 4 non-random proximities × 3 gender selections × 2 assignments, and 6 combinations correspond to random proximity with 3 gender selections × 2 assignments. To assess the impact of these choices, our experiments are organized according to the three actors in our threat model. First, the speaker finds the two most promising combinations of design choices on the development set in terms of privacy in the Ignorant and Lazy-Informed scenarios and utility for ASR decoding. This is motivated by the high computational cost of anonymizing the train-clean-360 subset of LibriSpeech and retraining ASV and ASR systems on it, which prevents the evaluation of privacy in the Semi-Informed scenario and utility for ASR training for all 54 combinations. Second, the user assesses the utility of these two combinations for both ASR training and decoding. Third, the attacker quantifies the resulting privacy in the Semi-Informed scenario, which leads us to identify the best combination among these two. Finally, we show how the proposed pitch transformation further improves privacy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Speaker's perspective</head><p>We first evaluate the design choices from the speaker's perspective in terms of privacy in the Ignorant and Lazy-Informed scenarios and utility for ASR decoding on the development set. The results are displayed in the form of swarm plots, i.e., scatter plots where each dot represents the privacy or utility value associated with one combination of design choices. In order to avoid overlapping dots with similar values, the dots are spread horizontally.</p><p>1) Distance: Figure <ref type="figure">4</ref> evaluates the effect of the chosen distance metric on privacy. We observe that both cosine distance and PLDA result in similarly low linkability in the Ignorant case but PLDA marginally outperforms cosine distance (i.e., it results in a lower linkability) in the Lazy-Informed case. Since both distance measures perform similarly in terms of utility (see Fig. <ref type="figure">9(a)</ref>), PLDA has an advantage. Therefore we consider only PLDA as the distance metric in the following experiments.</p><p>2) Proximity: Next, we assess the five choices of target proximity described in Section III-B, namely random, near, far, sparse and dense. The distance metric is fixed to PLDA and the values of N and N * are fixed to 200 and 100, respectively. <ref type="foot" target="#foot_8">10</ref>We observe in Fig. <ref type="figure">5</ref> that, although selecting candidate x-vectors far from the original speaker achieves the lowest linkability in the Ignorant case together with the random strategy, it is largely outperformed in the Lazy-Informed case by selection from sparse or dense clusters and by the random strategy. This shows that clustering based pseudo-speaker Compared to the sparse selection strategy, the dense strategy provides comparable privacy protection in the Lazy-Informed case, but much higher utility (see Fig. <ref type="figure">9(b)</ref>). This can be attributed to the fact that speakers in sparse clusters stand out more from the crowd than those in dense clusters, therefore they are more likely to suffer from poor ASR performance.</p><p>Finally, random target selection yields similar privacy protection in the Lazy-Informed case and slightly better utility as compared to dense. Hence we consider the random and dense strategies to be the best choices for proximity.</p><p>3) Gender selection: We now investigate the gender selection strategy described in Section III-C. The distance is fixed to PLDA and proximity to dense or random. As per the results shown in Fig. <ref type="figure" target="#fig_4">6</ref> it is hard to find the best choice for gender selection in terms of privacy since the linkability is not consistently lower for any specific choice.</p><p>In order to make a suitable choice, we introduce the additional requirement that the chosen anonymization scheme obfuscates the original speaker's gender. The different anonymization schemes can be visually compared in Fig. <ref type="figure" target="#fig_5">7</ref>. Same gender selection (Fig. <ref type="figure" target="#fig_5">7</ref>    Furthermore, we conduct gender identification experiments over the original and anonymized x-vectors shown in Fig. <ref type="figure" target="#fig_5">7</ref> to measure the degree of gender obfuscation caused by same vs. random gender selection. We employ the k-nearest neighbour algorithm with 5-fold cross-validation to predict the gender of speakers in the LibriSpeech train-clean-360 dataset which contains 921 speakers. The mean cross-validation accuracy for each dataset reported in Table II corroborates the visual observations in Fig. <ref type="figure" target="#fig_5">7</ref>.</p><p>4) Assignment: Finally the choice of pseudo-speaker assignment is examined from the speaker's perspective as described in Section III-D. The distance is fixed to PLDA, proximity to dense and gender selection to random. The results reported in Fig. <ref type="figure" target="#fig_6">8</ref> show that utterance-level assignment results in lower linkability than speaker-level assignment. However, the WER resulting from utterance-level assignment is higher than from speaker-level assignment (see Fig. <ref type="figure">9(d)</ref>). In the following, in order to conform with the requirements of the VoicePrivacy Challenge [22, Section 3.2], we choose speaker-level assignment. This ensures that "all trial utterances from a given speaker appear to be uttered by the same pseudospeaker".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Speaker</head><p>Based on these indications, the speaker may choose specific parameters according to their application needs. For the sake of further experimentation, we choose distance as PLDA, proximity as random or dense, gender selection as random and assignment as speaker-level to be the two best combinations of design choices based on our observations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. User's perspective</head><p>We now present complementary results from the user's perspective. Recall that in our threat model the user exploits the anonymized speech data for some downstream task. His/her primary concern is hence the utility of the data for that task. So far, we have only evaluated the utility for ASR decoding using an ASR system trained on the original train-clean-360 dataset (see Fig. <ref type="figure">9</ref>). We now evaluate the utility for ASR decoding using an ASR system trained on anonymized data, as well as the utility for ASR training. To do so, we anonymize the trainclean-360 dataset using either of the two best combinations of design choices, and we retrain the ASR system on it.</p><p>Figure <ref type="figure" target="#fig_0">10</ref>  We observe a WER degradation in the AO and OA scenarios, which indicates a mismatch between training and test data. The degradation is higher when original speech is decoded using the retrained model (OA) than when anonymized speech is decoded using the original model (AO). This asymmetry suggests a "loss of generalization" of the ASR model trained on anonymized speech, due to the unintentional exclusion of certain factors of variability of original speech.</p><p>Fortunately, the WERs on the test set in the AA scenario are almost as low as those in the OO scenario. This indicates that anonymization yields viable speech data for ASR training and ASR decoding with a WER similar to original speech, provided that training and decoding are both conducted on anonymized speech. No significant difference is observed depending on the proximity choice made by the speaker.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Attacker's perspective</head><p>Finally, we present complementary results from the attacker's perspective. The primary objective of the attacker is to find the original speaker's identity of anonymized speech utterances, i.e., to achieve high linkability. So far, we have only reported the linkability achieved by Ignorant and Lazy-Informed attackers on the development set. We now present the results achieved by these attackers and by a Semi-Informed attacker on both the development and test sets.</p><p>The results for the two best combinations of design choices are shown in Fig. <ref type="figure" target="#fig_8">11</ref>. We observe that the linkability increases as the strength of the attacker increases. It goes up to 0.44 with random proximity, but stays below 0.22 with dense proximity, even for the strongest (Semi-Informed) attacker. This indicates the robustness of dense over random proximity. Therefore, we ultimately recommend the following combination of choices to the speaker: PLDA distance, dense proximity, random gender selection, and speaker-level assignment. We recall that the latter choice is a requirement set by the VoicePrivacy challenge. Whenever speaker-level assignment is not required, we recommend utterance-level assignment for higher privacy. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>BL Ign</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Pitch conversion</head><p>Sticking with the best combination of design choices, we bring one last improvement: we explore three pitch conversion methods to further enhance privacy and increase the naturalness of anonymized speech. Indeed, the pitch sequence p might reveal some information about the speaker <ref type="bibr" target="#b43">[44]</ref> which is carried over to the synthesized speech. Also, keeping the pitch sequence p unchanged while possibly changing the gender of the x-vector results in inconsistent features which affect the naturalness of the synthesized speech.</p><p>The three conversion methods operate on nonzero pitch values only. Indeed, zero pitch values correspond to unvoiced or silent frames, and must remain equal to zero to preserve the phonetic content of the utterance. Conversely, nonzero pitch values must remain nonzero. In the rest of this section, the term "pitch sequence" and the notation p refer to a sequence stripped off its zero values.</p><p>The first method called logarithm Gaussian pitch conversion <ref type="bibr" target="#b44">[45]</ref> was recently employed in <ref type="bibr" target="#b45">[46]</ref> for voice anonymization. The target pitch sequence p * is obtained by linearly scaling the original pitch sequence p in the logarithmic domain as</p><formula xml:id="formula_6">log(p * ) = log(p) -µ src σ src σ tgt + µ tgt ,<label>(4)</label></formula><p>where µ src , σ src are the mean and standard deviation of p, and µ tgt , σ tgt are the mean and standard deviation of the target pseudo-speaker's pitch "sequence" p ps . The latter is obtained by concatenating the pitch sequences of all utterances of the N * candidate speakers composing the pseudo-speaker; it is stored by the x-vector anonymization module (Step 2 in Fig. <ref type="figure" target="#fig_1">2</ref>) and passed to the pitch conversion module (Step 3 ).</p><p>In addition, we propose two other methods, which we call percentile and minmax based pitch conversion. Percentile based pitch conversion maps each percentile of the original pitch distribution to the corresponding percentile of the target pitch distribution. To do so, the sequences p and p ps are sorted in ascending order, yielding p sorted and p sorted </p><p>where • denotes rounding down to the nearest integer. This mapping is an instance of one-dimensional optimal transport between the two distributions <ref type="bibr" target="#b46">[47]</ref>. To the best of our knowledge, this pitch conversion method is new. Minmax based pitch conversion linearly scales the range of pitch values, such that the minimum and maximum values in the original sequences are mapped to the minimum and maximum values of the target pseudo-speaker: </p><p>One benefit of percentile or minmax based conversion is that the converted pitch values belong to the range of pitch values for the N * candidate speakers composing the pseudo-speaker, while in case of Gaussian normalization some converted pitch values may be beyond that range or even beyond the range of valid pitch values for male or female speakers.</p><p>It is observed in Fig. <ref type="figure" target="#fig_12">12</ref>(a) that logarithm Gaussian pitch conversion and to a lesser extent minmax based conversion significantly increase the WER, while percentile based conversion maintains a WER close to the original. Figure <ref type="figure" target="#fig_12">12(b)</ref> shows that percentile and minmax based conversion substantially improve privacy, especially against the Semi-Informed attacker, while logarithm Gaussian conversion results in a more modest improvement or no improvement. We conclude that percentile based conversion is a suitable pitch conversion method which increases privacy with no significant loss of utility. According to informal listening results (not show in the figure), it also improves the naturalness of cross-gender voice conversion.</p><p>Overall, the experiments in Section V exhibited the benefits of the proposed improvements to x-vector based voice conversion in terms of privacy against the strongest (Semi-Informed) attacker. Specifically, x-vector based voice conversion with the best combination of design choices and with percentile based pitch conversion reduces the attacker's linkability by one order of magnitude with respect to original speech. This is to be contrasted with signal processing based methods such as <ref type="bibr" target="#b47">[48]</ref> which offer almost no protection against such a strong attacker.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. LARGE-SCALE SPEAKER STUDY</head><p>Similarly to other studies following the VoicePrivacy Challenge setup, all experiments above have relied on a small set of 29 enrolled speakers. In this section we analyze the attacker's performance against a larger number of enrolled speakers. This number reflects the attacker's knowledge: a smaller number means that the attacker was able to narrow down the list of speakers who may have uttered the trial utterances using contextual information. 11 Our goal is to study whether the speaker's identity gets hidden in the crowd or is still revealed to some extent within a large set of enrolled speakers.</p><p>Previous research has studied the impact of the number of speakers from a voice spoofing perspective where an attacker aims to be accepted through an ASV authentication system by finding the closest (trial) impostor to a given (enrolled) target speaker <ref type="bibr" target="#b48">[49]</ref>, <ref type="bibr" target="#b49">[50]</ref>. The attacker has access to a speech sample of the target speaker and to the ASV scoring mechanism. The authors showed that the chances of acceptance reach up to 50% as the number of impostors approaches 10 5 . A similar problem was posed by the Multi-Target Speaker Detection Challenge <ref type="bibr" target="#b50">[51]</ref> which aims to identify and assess the membership of a speaker to a set of blacklisted speakers. The authors showed that the performance for both tasks degrades as the number of speakers in the blacklist increases. In the following, we do not assess the worst-case performance like <ref type="bibr" target="#b49">[50]</ref>. Instead, we measure the overall speaker recognition performance as the number of different-speaker trials increases manifold.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Experimental setup</head><p>We use the Mozilla Common Voice English corpus because of its large number of speakers. To the best of our knowledge this is the first time this corpus is used for ASV and privacy related experiments.</p><p>As mentioned in Section IV-A, we consider a total population of 24,610 male speakers. Out of these, utterances from 20 speakers are selected as the public trial data subjected to reidentification attack. After computing PLDA scores between the trial and enrollment utterances, we get 4,696 same-speaker scores and 115,563,864 different-speaker scores (see Table <ref type="table">I</ref>).</p><p>We measure the attacker's performance against an increasing number of enrolled speakers. In the first step, we select for enrollement the same 20 speakers as for trial, and we use the corresponding same-and different-speaker scores. In the second step, we add 20 other speakers for enrollement and we include the corresponding different-speaker scores. In the following steps, we double the number of other speakers at each step, i.e., the total number of enrolled speakers increases from 20 to 20,500. The added speakers are randomly sampled 5 times from the entire speaker population to avoid any bias.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Open-set results</head><p>Figure <ref type="figure" target="#fig_14">13</ref> reports the performance of different attackers in terms of linkability. The linkability of original speech is equal 11 The attacker may obtain this contextual information by inspecting the metadata and/or the statistics of the public, anonymized dataset, or by simply listening to individual utterances. to 0.80 with 20 speakers and decreases to 0.70 with more than a few hundred speakers. The linkability of anonymized speech is much lower. For Ignorant and Lazy-Informed attackers, it starts from 0.18 and decreases to 0.06. The linkability curve for the Semi-Informed attacker is surprisingly below those of the two other attackers, but it also follows a decreasing trend.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Closed-set results</head><p>The above evaluation in terms of linkability, which assumes that the attackers rely on open-set ASV, hides the fact that the chance of finding the true speaker of a trial utterance decreases very quickly as the number of enrolled speakers increases. To highlight it, we perform closed-set ASI as explained in Section IV-C and report the rank of the true speaker. The higher the rank, the lower the ASI performance. Since increasing the number of speakers is expected to increase the rank, we also report the normalized rank, that is the rank divided by the number of speakers, and the chance-level rank, that is the expected rank when the attacker selects the true speaker at random (see Appendix A).</p><p>Figure <ref type="figure" target="#fig_16">14</ref>(a) shows that the rank of the true speaker increases almost linearly as a function of the number of speakers. On original speech, it remains much lower than chance-level even with thousands of speakers, which can be attributed to the distinct characteristics of speakers in the population. On anonymized speech, it converges close to the chance-level rank for all attackers. The normalized rank plot in Fig. <ref type="figure" target="#fig_16">14(b)</ref> shows that, beyond a few hundred speakers, the Ignorant and Lazy-Informed attackers perform more poorly than chancelevel, while the Semi-Informed attacker maintains a consistent performance that is slightly better than chance-level.</p><p>In addition, we study the top-k precision obtained by the attackers compared to the baseline performance for k ∈ {1, 20}. We observe in Fig. <ref type="figure" target="#fig_0">15</ref> that the precision drops much faster on anonymized data than original data, i.e., finding the true speaker of an anonymized utterance among a set of S speakers is equivalent to finding the true speaker of an original utterance among S speakers, where S increases at a much faster rate than S. The plot for k = 1 shows that without anonymization the true speaker can be uniquely identified with 40% accuracy among 20,500 speakers, while after anonymization the risk  of being uniquely identified becomes negligible beyond a few dozen speakers. For k = 20, our anonymization scheme provides the same level of protection against a Semi-Informed attacker among 52 speakers than raw speech among 20,500 speakers. Additional results for k ∈ {10, 50} (not shown here) follow a similar trend.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. CONCLUSION</head><p>Our work aimed to answer three questions from the speaker's and user's perspectives, and two questions from the attacker's perspective to realistically assess the privacy and utility of x-vector based speaker anonymization (see Section I).</p><p>To answer Q1, we introduced four design choices and studied their effect on the privacy of anonymized speech and its utility for ASR training and decoding. Based on our findings, we recommended the following optimal combination of choices: PLDA distance, dense proximity, random gender selection, and utterance-level assignment (unless otherwise required). To answer Q3, we then investigated three pitch conversion methods for removing the residual speaker information carried by the pitch sequence and to enhance the naturalness of the synthesized speech. While classical logarithm Gaussian conversion resulted in little or no improvement of privacy, the proposed percentile based conversion method significantly improved privacy with little loss of utility. Overall, x-vector based voice conversion with the best combination of design choices and with percentile based pitch conversion reduced the linkability against the strongest (Semi-Informed) attacker by one order of magnitude with respect to original speech (answering Q4), and it increased the WER on LibriSpeech test-clean from 4.1% to 4.8% only in the situation when ASR training and decoding are both conducted on anonymized speech (answering Q2).</p><p>To answer Q5, we further evaluated the proposed anonymization scheme as a function of the number of enrolled speakers, which reflects the attacker's ability to narrow down the list of speakers who may have uttered the trial utterances using contextual information. We conducted closedset ASI by incrementally adding thousands of speakers in the population and observed that the rank of the true speaker quickly increases and converges close to chance-level after anonymization. Another interesting observation can be made by looking at top-k precision curves: the loss of precision before anonymization that is seen after adding thousands of speakers in the enrollment set is equivalent to adding only a couple of speakers after anonymization. Specifically, the best combination of design choices offers the same level of protection against re-identification attacks among 52 speakers as original speech among 20,500 speakers.</p><p>In the future, we plan to study the worst-case performance of the proposed speaker anonymization scheme and characterize which speakers are easier to re-identify. We also plan to provide analytical lower bounds on privacy by using techniques</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Considered threat model. Speakers anonymize speech to conceal their identity before publication; attackers use biometric technology and knowledge of the anonymization method to re-identify it; users (e.g., speech technology companies) use the published data for downstream tasks such as ASR training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig.2. General architecture of the anonymization system.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .Fig. 5 .</head><label>45</label><figDesc>Fig. 4. Privacy against Ignorant and Lazy-Informed attackers depending on the distance choice. Each swarm plot shows the 24 linkability values on the development set resulting from all combinations of 4 proximity (excluding random), 3 gender selection, and 2 assignment choices.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>(b)) causes male and female clusters to move apart. A similar result is observed with opposite gender selection (not shown in the figure). On the</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Privacy against Ignorant and Lazy-Informed attackers depending on the gender selection choice. Distance is fixed to PLDA and proximity to dense or random. Each swarm plot shows the 4 linkability values on the development set resulting from all combinations of the 2 proximity and 2 assignment choices.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. t-SNE visualization of speaker-level x-vectors from the LibriSpeech train-clean-360 dataset anonymized using different proximity (random or dense) and gender selection (same or random, in parentheses) choices. Gaussian pitch normalization (see Section V-D) was used in all three cases.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. Privacy against Ignorant and Lazy-Informed attackers depending on the assignment choice. Distance is fixed to PLDA, proximity to dense or random, and gender selection to random. Each swarm plot shows the 2 linkability values on the development set resulting from the 2 proximity choices.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 9 .Fig. 10 .</head><label>910</label><figDesc>Fig. 9. Utility of anonymized speech for ASR decoding compared to original (Baseline) speech depending on the design choices made by the speaker. Each swarm plot shows the WER values obtained on the development set using an ASR system trained on the original train-clean-360 dataset. The design choices in subfigures a, b, c, and d are fixed or vary in the same way as in Figs. 4, 5, 6 and 8, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 11 .</head><label>11</label><figDesc>Fig. 11. Privacy against Ignorant (Ign), Lazy-Informed (Lazy-I) and Semi-Informed (Semi-I) attackers depending on the proximity choice (random or dense) made by the speaker, compared to original speech (BL). Distance is fixed to PLDA, gender selection to random, and assignment to speaker-level.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>ps.</head><label></label><figDesc>Each value p[i]in the original sequence is converted into a percentile [i]:[i] = rank of p[i] in p sorted length(p sorted ) × 100.(5)Then, the converted pitch value p * [i] corresponding to [i] is picked in p sorted ps as p * [i] = p sorted ps length(p sorted ps ) × [i] 100</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>p</head><label></label><figDesc>* [i] = (p[i] -min(p)) × max(p ps ) -min(p ps ) max(p) -min(p) + min(p ps ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. 12 .</head><label>12</label><figDesc>Fig.<ref type="bibr" target="#b11">12</ref>. Utility and privacy resulting from logarithm Gaussian (gauss), percentile or minmax based pitch conversion on top of x-vector anonymization, compared to original pitch (no-conv). Top: utility for ASR decoding with an ASR model trained on original (A-O) or anonymized (A-A) speech, compared to original speech (BL). Bottom: privacy against Ignorant (Ign), Lazy-Informed (Lazy-I) and Semi-Informed (Semi-I) attackers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Fig. 13 .</head><label>13</label><figDesc>Fig. 13. Open-set ASV performance of Ignorant, Lazy-Informed and Semi-Informed attackers as a function of the number of enrolled speakers, compared to original speech (Baseline).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Fig. 14 .</head><label>14</label><figDesc>Fig. 14. Closed-set ASI performance of Ignorant, Lazy-Informed and Semi-Informed attackers as a function of the number of speakers, compared to original speech (Baseline).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>= 20 Fig. 15 .</head><label>2015</label><figDesc>Fig.<ref type="bibr" target="#b14">15</ref>. Top-k ASI precision of Ignorant, Lazy-Informed and Semi-Informed attackers as a function of the number of speakers, compared to original speech (Baseline). The numbers of speakers needed to achieve an equivalent drop in precision before vs. after anonymization are highlighted.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE II GENDER</head><label>II</label><figDesc>IDENTIFICATION ACCURACY OVER THE ORIGINAL AND ANONYMIZED X-VECTORS IN FIG. 7.</figDesc><table><row><cell cols="2">Anonymization scheme Mean cross-validation accuracy (%)</cell></row><row><cell>Original</cell><cell>98.58</cell></row><row><cell>Random (same)</cell><cell>100.00</cell></row><row><cell>Random (random)</cell><cell>70.46</cell></row><row><cell>Dense (random)</cell><cell>53.31</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>Legally speaking, the term "anonymization" refers to a method that fully achieves this goal. Following<ref type="bibr" target="#b5">[6]</ref>, we use it in a broader sense to refer to a method that aims to achieve this goal, even when it has failed to do so.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1"><p>There is no guarantee that averaging produces a valid x-vector, but all our experiments show that the synthesized anonymized speech is of good quality.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2"><p>Note that the terms sparse and dense do not directly reflect the density of x-vectors, since they do not take the diameter of the clusters into account. However, we find that this relation holds in practice.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_3"><p>The VoicePrivacy Challenge involves development and evaluation sets built from LibriSpeech and VCTK. Due to space limitations, we focus on LibriSpeech.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_4"><p>The SNR was computed using the WADA-SNR<ref type="bibr" target="#b38">[39]</ref> algorithm available at https://gist.github.com/johnmeade/d8d2c67b87cda95cd253f55c21387e75.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_5"><p>The Ignorant and Lazy-Informed scenarios were called OA and AA in the VoicePrivacy Challenge. The Lazy-Informed scenario was also called Semi-Ignorant in<ref type="bibr" target="#b16">[17]</ref>.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_6"><p><ref type="bibr" target="#b7">8</ref> The Informed scenario in<ref type="bibr" target="#b15">[16]</ref> where the attacker is aware of the random numbers drawn by the speaker is not part of our study, since it falls into a security problem rather than just a privacy problem.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_7"><p>As classically assumed in the speaker verification literature, the two speakers in each trial have the same original gender. In practice though, the gender of the original speaker may be unknown to the attacker. Hence, the resulting linkability values can be seen as worst-case values from the speaker's point of view and best-case values from the attacker's point of view.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10" xml:id="foot_8"><p>We noticed a sharp decline in utility for smaller values of N * .</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>ACKNOWLEDGMENT This work was supported in part by the <rs type="funder">French National Research Agency</rs> under projects <rs type="projectName">HARPOCRATES</rs> (<rs type="grantNumber">ANR-19-DATA-0008</rs>) and <rs type="projectName">DEEP-PRIVACY</rs> (<rs type="grantNumber">ANR-18-CE23-0018</rs>), by the <rs type="funder">European Union</rs>'s <rs type="programName">Horizon 2020 Research and Innovation Program</rs> under Grant Agreement No. <rs type="grantNumber">825081</rs> COMPRISE (https://www.compriseh2020.eu/), and jointly by the <rs type="funder">French National Research Agency</rs> and the <rs type="funder">Japan Science and Technology Agency</rs> under project VoicePersonae. Experiments presented in this paper were partially carried out using the Grid'5000 testbed, supported by a scientific interest group hosted by <rs type="funder">Inria</rs> and including <rs type="funder">CNRS</rs>, RENATER and several Universities as well as other organizations (see https://www. grid5000.fr).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_qrm4EgW">
					<idno type="grant-number">ANR-19-DATA-0008</idno>
					<orgName type="project" subtype="full">HARPOCRATES</orgName>
				</org>
				<org type="funded-project" xml:id="_UBn9Dfv">
					<idno type="grant-number">ANR-18-CE23-0018</idno>
					<orgName type="project" subtype="full">DEEP-PRIVACY</orgName>
					<orgName type="program" subtype="full">Horizon 2020 Research and Innovation Program</orgName>
				</org>
				<org type="funding" xml:id="_xn4TdPy">
					<idno type="grant-number">825081</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>inspired from differential privacy <ref type="bibr" target="#b51">[52]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX A DERIVATION OF THE CHANCE-LEVEL RANK</head><p>Let R ∈ {1, . . . , N spk } be the set of all possible ranks for a given speaker that can be obtained with probability P (R). The expected rank is equal to:</p><p>To obtain the chance-level rank, we set P (R) = 1 N spk . Hence</p><p>(9) When the rank is normalized, we divide the chance-level rank by N spk to obtain the normalized chance-level rank</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Speaker identification and verification using Gaussian mixture speaker models</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Reynolds</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Speech Communication</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="91" to="108" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The accented English speech recognition challenge 2020: Open datasets, tracks, baselines, results and methods</title>
		<author>
			<persName><forename type="first">X</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="6918" to="6922" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Pathological speech processing: State-of-the-art, current challenges, and future directions</title>
		<author>
			<persName><forename type="first">R</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chaspari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Narayanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="6470" to="6474" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Voice mimicry attacks assisted by automatic speaker verification</title>
		<author>
			<persName><forename type="first">V</forename><surname>Vestman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kinnunen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">G</forename><surname>Hautamäki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sahidullah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Speech and Language</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="page" from="36" to="54" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The GDPR &amp; speech data: Reflections of legal and technology communities, first steps towards a common understanding</title>
		<author>
			<persName><forename type="first">A</forename><surname>Nautsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Jasserand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Kindt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Todisco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Trancoso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Evans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Interspeech</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3695" to="3699" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Introducing the VoicePrivacy initiative</title>
		<author>
			<persName><forename type="first">N</forename><surname>Tomashenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">M L</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Nautsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yamagishi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Patino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-F</forename><surname>Bonastre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-G</forename><surname>Noé</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Todisco</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Interspeech</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1693" to="1697" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Privacy-preserving sound to degrade automatic speaker verification performance</title>
		<author>
			<persName><forename type="first">K</forename><surname>Hashimoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yamagishi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Echizen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="5500" to="5504" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Voicemask: Anonymize and sanitize voice input on mobile devices</title>
		<author>
			<persName><forename type="first">J</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X.-Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Deng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.11460</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Speaker deidentification via voice transformation</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Toth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Schultz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">W</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE Workshop on Automatic Speech Recognition &amp; Understanding (ASRU)</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="529" to="533" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Online speaker de-identification using voice transformation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Pobar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Ipšić</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">37th International Convention on Information and Communication Technology, Electronics and Microelectronics (MIPRO)</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1264" to="1267" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Convolutional neural network based speaker de-identification</title>
		<author>
			<persName><forename type="first">F</forename><surname>Bahmaninezhad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Hansen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Odyssey</title>
		<imprint>
			<biblScope unit="page" from="255" to="260" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Speaker de-identification using diphone recognition and speech synthesis</title>
		<author>
			<persName><forename type="first">T</forename><surname>Justin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Štruc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Dobrišek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Vesnicer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Ipšić</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Mihelič</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 11th IEEE International Conference and Workshops on Automatic Face and Gesture Recognition (FG)</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Speaker anonymization using x-vector and neural waveform models</title>
		<author>
			<persName><forename type="first">F</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yamagishi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Echizen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Todisco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-F</forename><surname>Bonastre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 10th ISCA Speech Synthesis Workshop</title>
		<meeting>10th ISCA Speech Synthesis Workshop</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="155" to="160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Privacypreserving adversarial representation learning in ASR: Reality or illusion?</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">M L</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bellet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tommasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Vincent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Interspeech</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3700" to="3704" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Voiceindistinguishability: Protecting voiceprint in privacy-preserving speech data release</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yoshikawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE International Conference on Multimedia and Expo (ICME)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Evaluating voice conversion-based privacy protection against informed attackers</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">M L</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Vauquier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sahidullah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bellet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tommasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Vincent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="2802" to="2806" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Design choices for x-vector based speaker anonymization</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">M L</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Tomashenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yamagishi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Maouche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bellet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tommasi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Interspeech</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1713" to="1717" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Reversible speaker deidentification using pre-trained transformation functions</title>
		<author>
			<persName><forename type="first">C</forename><surname>Magariños</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lopez-Otero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Docio-Fernandez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Rodriguez-Banga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Erro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Garcia-Mateo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Speech and Language</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="page" from="36" to="52" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">X-vectors: Robust DNN embeddings for speaker recognition</title>
		<author>
			<persName><forename type="first">D</forename><surname>Snyder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Garcia-Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP)</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="5329" to="5333" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Neural source-filter-based waveform model for statistical parametric speech synthesis</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Takaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yamagishi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="5916" to="5920" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Improved bottleneck features using pretrained deep neural networks</title>
		<author>
			<persName><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Seltzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Interspeech</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="237" to="240" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<author>
			<persName><forename type="first">N</forename><surname>Tomashenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">M L</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Nautsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yamagishi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-F</forename><surname>Bonastre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-G</forename><surname>Noé</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Todisco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Patino</surname></persName>
		</author>
		<ptr target="https://www.voiceprivacychallenge.org/vp2020/docs/VoicePrivacy2020EvalPlanv13.pdf" />
		<title level="m">The VoicePrivacy 2020 Challenge evaluation plan</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">X-Vector Singular Value Modification and Statistical-Based Decomposition with Ensemble Regression Modeling for Speaker Anonymization System</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">O</forename><surname>Mawalim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Galajit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Karnjana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Unoki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Interspeech</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1703" to="1707" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">X-vector anonymization using autoencoders and adversarial training for preserving speech privacy</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Perero-Codosero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">M</forename><surname>Espinoza-Cuadros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Hernández-Gómez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Speech &amp; Language</title>
		<imprint>
			<biblScope unit="volume">74</biblScope>
			<biblScope unit="page">101351</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Speaker anonymization with distribution-preserving x-vector generation for the VoicePrivacy challenge</title>
		<author>
			<persName><forename type="first">H</forename><surname>Turner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Lovisotto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Martinovic</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.13457</idno>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">The voiceprivacy 2020 challenge: Results and findings</title>
		<author>
			<persName><forename type="first">N</forename><surname>Tomashenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Patino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">M L</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-G</forename><surname>Noé</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Nautsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yamagishi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>O'brien</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Chanclu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-F</forename><surname>Bonastre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Todisco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Maouche</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Speech &amp; Language</title>
		<imprint>
			<biblScope unit="volume">74</biblScope>
			<biblScope unit="page">101362</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Probabilistic linear discriminant analysis</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">9th European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="531" to="542" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Bayesian speaker verification with heavy-tailed priors</title>
		<author>
			<persName><forename type="first">P</forename><surname>Kenny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Odyssey</title>
		<imprint>
			<biblScope unit="page">14</biblScope>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">On the use of PLDA i-vector scoring for clustering short segments</title>
		<author>
			<persName><forename type="first">I</forename><surname>Salmun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Opher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Lapidot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Odyssey</title>
		<imprint>
			<biblScope unit="page" from="407" to="414" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Constrained discriminative PLDA training for speaker verification</title>
		<author>
			<persName><forename type="first">J</forename><surname>Rohdin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Biswas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Shinoda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1670" to="1674" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Affinity propagation: Clustering data by passing messages</title>
		<author>
			<persName><forename type="first">D</forename><surname>Dueck</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
		<respStmt>
			<orgName>University of Toronto</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. dissertation</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Probing the information encoded in x-vectors</title>
		<author>
			<persName><forename type="first">D</forename><surname>Raj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Snyder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="726" to="733" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">LibriSpeech: an ASR corpus based on public domain audio books</title>
		<author>
			<persName><forename type="first">V</forename><surname>Panayotov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP)</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="5206" to="5210" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">VoxCeleb: a large-scale speaker identification dataset</title>
		<author>
			<persName><forename type="first">A</forename><surname>Nagrani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Interspeech</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2616" to="2620" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">VoxCeleb2: Deep speaker recognition</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Nagrani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Interspeech</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1086" to="1090" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">LibriTTS: A corpus derived from LibriSpeech for text-tospeech</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Dang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Interspeech</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1526" to="1530" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Common voice: A massively-multilingual speech corpus</title>
		<author>
			<persName><forename type="first">R</forename><surname>Ardila</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Henretty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kohler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Meyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Morais</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Saunders</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">M</forename><surname>Tyers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Weber</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.06670</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">M L</forename><surname>Srivastava</surname></persName>
		</author>
		<title level="m">Speaker anonymization -representation, evaluation and formal guarantees</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
		<respStmt>
			<orgName>University of Lille</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. dissertation</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Robust signal-to-noise ratio estimation based on waveform amplitude distribution analysis</title>
		<author>
			<persName><forename type="first">C</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Stern</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Interspeech</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="2598" to="2601" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">General framework to evaluate unlinkability in biometric template protection systems</title>
		<author>
			<persName><forename type="first">M</forename><surname>Gomez-Barrero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Galbally</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Rathgeb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Busch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Forensics and Security</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1406" to="1420" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">A comparative study of speech anonymization metrics</title>
		<author>
			<persName><forename type="first">M</forename><surname>Maouche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">M L</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Vauquier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bellet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tommasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Vincent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Interspeech</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1708" to="1712" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Application-independent evaluation of speaker detection</title>
		<author>
			<persName><forename type="first">N</forename><surname>Brümmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Du</forename><surname>Preez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Speech and Language</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">2-3</biblScope>
			<biblScope unit="page" from="230" to="275" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Jackknife</title>
		<author>
			<persName><forename type="first">H</forename><surname>Abdi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Encyclopedia of Research Design</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">How vulnerable are prosodic features to professional imitators?</title>
		<author>
			<persName><forename type="first">M</forename><surname>Farrús</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wagner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Anguita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hernando</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
	<note>in Odyssey</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">High quality voice conversion through phoneme-based linear mapping functions with STRAIGHT for Mandarin</title>
		<author>
			<persName><forename type="first">K</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">4th International Conference on Fuzzy Systems and Knowledge Discovery (FSKD)</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="410" to="414" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">A study of F0 modification for x-vector based speech pseudonymization across gender</title>
		<author>
			<persName><forename type="first">P</forename><surname>Champion</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Jouvet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Larcher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2nd AAAI Workshop on Privacy-Preserving Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><surname>Villani</surname></persName>
		</author>
		<title level="m">Optimal Transport: Old and New</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Speaker anonymisation using the McAdams coefficient</title>
		<author>
			<persName><forename type="first">J</forename><surname>Patino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Tomashenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Todisco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Nautsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Evans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Interspeech</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1099" to="1103" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Voice biometrics security: Extrapolating false alarm rate via hierarchical bayesian modeling of speaker verification scores</title>
		<author>
			<persName><forename type="first">A</forename><surname>Sholokhov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kinnunen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vestman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">A</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Speech and Language</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="page">101024</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Extrapolating false alarm rates in automatic speaker verification</title>
	</analytic>
	<monogr>
		<title level="m">Interspeech</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="4218" to="4222" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">MCE 2018: The 1st multi-target speaker detection and identification challenge evaluation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Shon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Dehak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Reynolds</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Glass</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Interspeech</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="356" to="360" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">The algorithmic foundations of differential privacy</title>
		<author>
			<persName><forename type="first">C</forename><surname>Dwork</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foundations and Trends in Theoretical Computer Science</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">3-4</biblScope>
			<biblScope unit="page" from="211" to="407" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
