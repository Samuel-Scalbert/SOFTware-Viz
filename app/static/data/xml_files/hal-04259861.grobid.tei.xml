<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Find-2-Find: Multitask Learning for Anaphora Resolution and Object Localization</title>
				<funder>
					<orgName type="full">German Research Center for Artificial Intelligence (DFKI)</orgName>
				</funder>
				<funder>
					<orgName type="full">French National Institute for Research in Digital Science and Technology (Inria)</orgName>
				</funder>
				<funder ref="#_6SxczZX">
					<orgName type="full">joint IMPRESS</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Cennet</forename><surname>Oguz</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Pascal</forename><surname>Denis</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Emmanuel</forename><surname>Vincent</surname></persName>
							<email>emmanuel.vincent@inria.fr</email>
						</author>
						<author>
							<persName><forename type="first">Simon</forename><surname>Ostermann</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">German Research Center for Artificial Intelligence (DFKI)</orgName>
								<orgName type="institution">Saarland Informatics</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">UMR 9189 CRIStAL</orgName>
								<orgName type="institution" key="instit1">Univ. Lille</orgName>
								<orgName type="institution" key="instit2">Inria</orgName>
								<orgName type="institution" key="instit3">CNRS</orgName>
								<orgName type="institution" key="instit4">Centrale Lille</orgName>
								<address>
									<postCode>F-59000</postCode>
									<settlement>Lille</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Josef</forename><surname>Van Genabith</surname></persName>
							<email>josef.van_genabith@dfki.depascal.denis</email>
							<affiliation key="aff0">
								<orgName type="laboratory">German Research Center for Artificial Intelligence (DFKI)</orgName>
								<orgName type="institution">Saarland Informatics</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution" key="instit1">Universit√© de Lorraine</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
								<orgName type="institution" key="instit3">Inria</orgName>
								<orgName type="institution" key="instit4">LORIA</orgName>
								<address>
									<postCode>F-54000</postCode>
									<settlement>Nancy</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="laboratory">German Research Center for Artificial Intelligence (DFKI)</orgName>
								<orgName type="institution">Saarland Informatics</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Find-2-Find: Multitask Learning for Anaphora Resolution and Object Localization</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">A13263D8F41A6DCC3D293B6328685D67</idno>
					<note type="submission">Submitted on 26 Oct 2023</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-04-12T14:46+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>come from teaching and research institutions in France or abroad, or from public or private research centers. L'archive ouverte pluridisciplinaire</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Deep neural networks have achieved enormous success in various language and computer vision tasks, such as multimodal understanding using video-text and image-text data <ref type="bibr" target="#b25">(Malmaud et al., 2015;</ref><ref type="bibr" target="#b0">Alayrac et al., 2016;</ref><ref type="bibr">Zhou et al., 2018b;</ref><ref type="bibr" target="#b27">Miech et al., 2019;</ref><ref type="bibr" target="#b27">Zhukov et al., 2019)</ref>. However, many current systems require a large number of accurate annotations, including image-level labels, location-level labels (bounding boxes and key points), and pixellevel labels.</p><p>A specific type of video data with naturally occurring semi-aligned texts are narrated instructional videos. Such videos are available in large quantities (e.g. on YouTube) and often chosen for learning Figure <ref type="figure">1</ref>: Examples of visual and linguistic ambiguities. Figure <ref type="figure">1</ref>) represents the visual ambiguity related to which specific pan (in Figure <ref type="figure">1a</ref>) is referenced with the phrase the pan because many pans occur on the stove.</p><p>Figure <ref type="figure" target="#fig_4">2</ref>) shows the linguistic ambiguity with the use of the pronoun them (in Figure <ref type="figure" target="#fig_4">2b</ref>).</p><p>joint text-video embeddings in multimodal understanding <ref type="bibr">(Zhou et al., 2018b;</ref><ref type="bibr" target="#b27">Miech et al., 2019)</ref>. They often contain a narration explaining the visual content of the corresponding time frames in the video <ref type="bibr" target="#b25">(Malmaud et al., 2015;</ref><ref type="bibr" target="#b0">Alayrac et al., 2016;</ref><ref type="bibr">Zhou et al., 2018b;</ref><ref type="bibr" target="#b27">Miech et al., 2019;</ref><ref type="bibr" target="#b27">Zhukov et al., 2019)</ref>. Instructional videos often contain visual and linguistic ambiguities that can be easily resolved by humans but pose two unique key challenges for automatic text-image processing systems. The first challenge is visual ambiguity, occurring when it is necessary to ground a referring expression in an image with ambiguous visual referents. In Figure <ref type="figure">1</ref> (1b), it is not clear from the picture which pan is referred to with the noun phrase the pan without a correct bounding box annotation. The second key challenge is linguistic ambiguity, instantiated for example by the use of anaphoric pronouns <ref type="bibr">(Figure 1,</ref><ref type="bibr">(2b)</ref> or null pronouns (Figure <ref type="figure" target="#fig_4">2</ref>).</p><p>In this work, we focus on modeling cases where both ambiguities are intertwined, a phenomenon we refer to as visual-linguistic ambiguity.    1 provides a motivating example: To find which pan in (1b) is denoted by the textual span the pan, we need to find its antecedent in the preceding text. The visual object localization of the textual antecedent the hot pan in (1a) then includes supplementary visual information about the correct pan: The referent is the hot pan with beef in it. In Figure <ref type="figure">1</ref>, To find the correct location of the object referred to as them in (2b), we first need to find the textual antecedent to understand what them refers to. When identifying pierogi as antecedent, we can use the pierogi for the visual object localization of them.</p><p>An even more complex case is null pronouns (Figure <ref type="figure" target="#fig_4">2</ref>): To do object localization and anaphora resolution, the first requirement is the detection of the null pronoun, which then needs to be resolved and located in the image. A located visual object of a null pronoun then assists in finding the textual antecedent, and a resolved null pronoun helps to apply object localization.</p><p>Our guiding idea in this work is that anaphora resolution, the task of connecting linguistic expressions such as the anaphor (i.e., the repeated reference) and its antecedent (i.e., the previous mention in the document). <ref type="bibr" target="#b30">(Poesio et al., 2018;</ref><ref type="bibr" target="#b7">Fang et al., 2022;</ref><ref type="bibr" target="#b28">Oguz et al., 2022)</ref>, and object localization, the task of identifying the location of one or more objects in an image and drawing bounding boxes around their visual space <ref type="bibr" target="#b36">(Tompson et al., 2015;</ref><ref type="bibr">Zhou et al., 2016;</ref><ref type="bibr" target="#b2">Choe et al., 2020)</ref>, can jointly help to resolve visual-linguistic ambiguities. To test this we propose a multitask learning neural model for jointly resolving visual-linguistic ambiguity.</p><p>Our contributions are two-fold: First, we present a new dataset<ref type="foot" target="#foot_0">1</ref> (Section 4), Find2Find, for the joint evaluation of anaphora resolution and object localization based on an extension of Chop&amp;Change <ref type="bibr" target="#b28">(Oguz et al., 2022)</ref>. Our new data set contains 500 recipes with annotated anaphora and associated object localization. Together with the new data set, we propose the new task of multimodal resolution of Visual-Linguistic Ambiguities. The task provides a unique opportunity for models to fuse text and vision information for solving anaphora resolution and object localization at the same time. Second, we present a new multitask learning system 1 for modeling the two tasks of anaphora resolution and object localization jointly, using a fusion of visual and textual data. Our experiments show that information from each of the tasks mutually benefits performance on the other task. Our idea is based on the fact that in both tasks, the goal is to extract mentions from a given text: what connects object localization and anaphora resolution is that in visual object localization, a corresponding language expression needs to be found, and in anaphora resolution, accurate spans that resolve the anaphoric relations between the anaphor and the antecedents need to be identified.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Anaphora Resolution</head><p>Anaphora resolution <ref type="bibr" target="#b30">(Poesio et al., 2018;</ref><ref type="bibr" target="#b7">Fang et al., 2022;</ref><ref type="bibr" target="#b28">Oguz et al., 2022)</ref> is the process of resolving the relations between an anaphor (i.e., a reference expression) and its antecedent (i.e., the previous mention of the same entity). The relations between anaphor and antecedent mostly appear in two different forms: coreference and bridging. Coreference resolution <ref type="bibr">(Clark and Manning, 2016a;</ref><ref type="bibr" target="#b24">Lee et al., 2017)</ref> is the task of finding the linguistic expressions that refer to the same real-world entities in a document, whereas bridging resolution <ref type="bibr" target="#b39">(Yu and Poesio, 2020;</ref><ref type="bibr" target="#b22">Kobayashi et al., 2022)</ref> fo-cuses on entities with an associative relation that does not express the same entity but relates to it (e.g., a car and its engine). Most previous work and datasets <ref type="bibr" target="#b38">(Yu et al., 2022)</ref> tackle coreference resolution and bridging resolution separately. An exception is <ref type="bibr" target="#b8">Fang et al. (2021)</ref>, <ref type="bibr" target="#b7">Fang et al. (2022), and</ref><ref type="bibr" target="#b28">Oguz et al. (2022)</ref>, which focus on documents with rich anaphoric relations for anaphora annotation and resolution of coreference and bridging with end-to-end neural networks <ref type="bibr" target="#b24">(Lee et al., 2017)</ref>.</p><p>Anaphora resolution is composed of two subtasks: mention detection and antecedent selection. A typical neural-based method for anaphora resolution starts initially with neural-based mentionranking modeling <ref type="bibr">(Clark and Manning, 2016a)</ref>, using distributional features of entities <ref type="bibr">(Clark and Manning, 2016b</ref>) with predefined mentions. <ref type="bibr" target="#b24">Lee et al. (2017)</ref> combine mention detection and antecedent selection in an end-to-end neural learning system. <ref type="bibr" target="#b39">Yu and Poesio (2020)</ref> propose a multi-task learning system for coreference and bridging resolution with an end-to-end learning approach <ref type="bibr" target="#b24">(Lee et al., 2017)</ref>. Here, coreference and bridging resolution models learn the mention detection with the objective of coreference/bridging resolution: if the span is resolved then it is a mention.</p><p>Various feature sets are used for anaphora resolution along with contextualized language features <ref type="bibr" target="#b18">(Joshi et al., 2019</ref><ref type="bibr" target="#b17">(Joshi et al., , 2020))</ref>, e.g., the token length of spans <ref type="bibr">(Clark and Manning, 2016b)</ref>, contextdependent boundary representations with a headfinding attention mechanism over the span <ref type="bibr" target="#b24">(Lee et al., 2017)</ref>, distance features of the anaphor and the antecedent based on word distance <ref type="bibr">(Clark and Manning, 2016a)</ref> and sentence distance <ref type="bibr" target="#b28">(Oguz et al., 2022)</ref> where additionally visual features are used for anaphora resolution in recipes for cooking videos. Our work is similar in spirit: however, unlike <ref type="bibr" target="#b28">Oguz et al. (2022)</ref> we combine anaphora resolution with object localization tasks from computer vision in a joint multitask learning model to benefit both leveraging visual features of entities for anaphora resolution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Object Localization</head><p>Object localization is the task of identifying the location of one or more objects in an image, e.g. Figure <ref type="figure" target="#fig_5">3 (a,</ref><ref type="figure">b,</ref><ref type="figure">c</ref>). Object localization has been studied in computer vision for a long time with various learning methods <ref type="bibr" target="#b11">(Gokberk Cinbis et al., 2014;</ref><ref type="bibr">Zhou et al., 2018a;</ref><ref type="bibr" target="#b13">Huang et al., 2018)</ref>. Multiple Instance Learning (MIL). MIL <ref type="bibr" target="#b6">(Dietterich et al., 1997</ref>) is a learning strategy that adresses the essence of the incomplete annotation problem, where only coarse-grained labels are available for learning. MIL has been used effectively for weakly-supervised learning in several computer vision works including object tracking <ref type="bibr" target="#b1">(Babenko et al., 2010)</ref>, object localization <ref type="bibr" target="#b11">(Gokberk Cinbis et al., 2014)</ref>, image classification <ref type="bibr" target="#b37">(Wu et al., 2015)</ref>. Huang et al. ( <ref type="formula">2018</ref>) extend MIL reference awareness for visual grounding of instructional videos. We extend MIL for object localization with anaphoric information to avoid the issue of ambiguous language references such as it, them <ref type="bibr">(Zhou et al., 2018a;</ref><ref type="bibr" target="#b13">Huang et al., 2018)</ref>.</p><p>Weakly Supervised Object Localization The most common way to do object localization is supervised learning, which uses object-level categories with bounding boxes, e.g., Figure <ref type="figure" target="#fig_5">3</ref> a. However, for a data-greedy neural learning system, object-level bounding box annotation is timeconsuming and expensive. Weakly supervised object localization approaches avoid this problem and focus on learning object localization with imagelevel object labels (e.g., Figure <ref type="figure" target="#fig_5">3</ref> b) under the MIL paradigm <ref type="bibr" target="#b5">(Deselaers et al., 2012;</ref><ref type="bibr" target="#b31">Prest et al., 2012;</ref><ref type="bibr" target="#b11">Gokberk Cinbis et al., 2014;</ref><ref type="bibr" target="#b29">Oquab et al., 2015)</ref>.</p><p>Object localization with the MIL approach aims to match object labels with object bounding boxes (e.g., Figure <ref type="figure" target="#fig_5">3 b</ref>). Studies on object localization with image descriptions rather than object labels take weak supervision further <ref type="bibr" target="#b19">(Karpathy and Fei-Fei, 2015;</ref><ref type="bibr">Zhou et al., 2018a;</ref><ref type="bibr" target="#b13">Huang et al., 2018)</ref>, as in Figure <ref type="figure" target="#fig_5">3</ref>  To date, object localization studies have not explored learning from to learn of mention extraction from image descriptions, instead extracting the mentions by using parsing methods <ref type="bibr" target="#b20">(Kiddon et al., 2015;</ref><ref type="bibr" target="#b14">Huang et al., 2017</ref><ref type="bibr" target="#b13">Huang et al., , 2018) )</ref> or using the predefined mentions list (Zhou et al., 2018a) before the learning process. Thus, we claim object localization and anaphora resolution share a subtask of entity extraction like the mention detection process in anaphora resolution as explained in Section 2.1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Task</head><p>A recipe consists of instructions I where a cooking instruction I i (e.g. add chopped onions and carrots to the pan) consists of n nominal or null spans and one verbal predicate where n ‚â• 1. A span x i of I i might be an incorrect consecutive fragment add chopped or a gold span e, e.g., a noun phrase chopped onions, a pronoun it, or a null pronoun [œï] as in mix [œï]. Null pronouns are extremely common in recipe instructions <ref type="bibr" target="#b20">(Kiddon et al., 2015;</ref><ref type="bibr" target="#b14">Huang et al., 2017)</ref>. In our approach, we also have a video clip that contains the visual content of the instruction I i with the action and the entities included in the process. Following Zhou et al. (2018a); Huang et al. ( <ref type="formula">2018</ref>), we evenly divide each video clip into three equal parts and randomly sample one image (one frame) V i from each of the three sub-clips to capture the temporal changes of entities. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Anaphora Resolution</head><p>The task of anaphora resolution is to assign each gold span (anaphor) e i where e i ‚àà {x i,1 , . . . , x i,1 } of each instruction I i to one or more gold spans (antecedent) y i ‚àà {œµ, I 1 , . . . , I i-1 , e 1,1 , . . . , e i-1,n }, a dummy antecedent œµ, all preceding instructions I 1 , . . . , I i-1 and all preceding gold spans e 1,1 , . . . , e i-1,n from the previous instructions I 1 , . . . , I i-1 . For a nominal span in Figure <ref type="figure">1</ref> 1b, the anaphor span the pan refers to the antecedent a hot pan in a previous instruction. For a null pronouns example in Figure <ref type="figure" target="#fig_4">2</ref>, the null pronouns œï refers to two previous instructions as the antecedents because the null pronoun does not point to any entity and it is also not a new entity for the recipe, it is instead produced by the previous instructions.</p><p>The selection of dummy œµ as antecedent indicates that the anaphor is an incorrect sequence of consecutive words or a singular entity without an antecedent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Object Localization</head><p>An image V ‚àà R H√óW is represented as a bag of ten regions (e.g., the visualization of region proposals of the given positive frame in Figure <ref type="figure" target="#fig_6">4</ref>), v h√ów 1 , .., v h√ów 10 with suitable side lengths of window patches h and w where h &lt; H and w &lt; W . Given a span x i and ten region proposals v h√ów 1 , .., v h√ów 10 , the task of object localization is to identify whether or not the region proposal belongs to the object of interest, i.e., the text x i when x i is a gold span (e.g., null pronoun [œï] or nominal spans) of the corresponding instruction I i . Therefore, the task is to link the text span x i to a corresponding visual region proposal v i where x i is a gold span {e i,1 , . . . , e i,n } of the instruction. We use the dot-product attention between the span x i and the region v i for ranking the visual-semantic matching.   <ref type="formula">2022</ref>) use the YouCookII dataset to propose a multimodal anaphora resolution dataset, Chop&amp;Change, with a novel annotation schema to address the state change of entities in cooking instructions of recipe videos. While the Chop&amp;Change annotation schema captures three anaphoric relations (coreference, near-identity, and bridging), in our work here we concentrate on anaphora resolution ignoring relations and focusing on finding the antecedent. Table <ref type="table" target="#tab_1">1</ref> shows that the Chop&amp;Change dataset includes 264 training recipe documents and 89 test documents in total. For our work here, we increase the number of annotated recipes to 400 for train and 100 for test recipes by using the Chop&amp;Change annotation schema. All annotated recipes are associated with respective videos. The structure of annotation is explained with an example in Appendix A.</p><p>Object Localization Data. To construct our test set we examine Huang et al. ( <ref type="formula">2018</ref>) who present a study on reference-aware visual grounding and provide an object localization dataset, FindIt, of 62 YouCookII videos for the given entities in the pro-vided textual descriptions (i.e., instructions). After a deep examination of the FindIt dataset, we find that 30 videos of FindIt could be used with our annotated recipes. Since only 30 videos produce an insufficient test set for the evaluation of object localization, we annotated 70 more videos from the recipes we annotated for anaphora resolution. We extract the frames of the instruction clips by using the temporal boundaries. We obtain 3 video subsets of consecutive frames of the instruction to acquire the state changes of entities in time. We pick one frame with a clear visual content of each entity from the 3 subsets. Thus, each entity is represented in a maximum of three frames for state changes. For evaluation, we annotate the visual object with a bounding box on the selected frames for each entity. In total, we have 5,688 images for 100 recipes annotated for our object localization test set. Note that we do not annotate bounding boxes for the training data as we train our models in a weakly supervised setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Methodology</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Model</head><p>In this section, we explain the details of our anaphora resolution and object localization models. Additionally, we formulate the multitask learning approach of joint anaphora resolution and object localization (see Figure <ref type="figure" target="#fig_6">4</ref>). In order to analyze a given text, it is important to resolve referring expressions. Thus, text-based anaphora resolution is an important method of identifying the antecedent of anaphora. However, here we also have object localization, providing a bounding box for the visual content of language references. The tasks of object localization and anaphora resolution share information via the mention extraction and representation part of the model. Therefore, our mention representations are trained with anaphora resolution and object localization over the mention extraction and representation. The implementation details can be seen in Appendix B.</p><p>As implied in the explanation of both tasks, before resolving references in the previous context or on the image, referring expressions and language references in the text need to be identified, a task known as Mention Extraction. In the next Section, we first describe our approach to representing mentions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.1">Mention Extraction and Representation</head><p>We consider all continuous token sequences with up to L words as a potential mention span and compute the corresponding span score. We use SpanBERT <ref type="bibr" target="#b17">(Joshi et al., 2020)</ref> as a state-of-art representation for coreference resolution. We capture the linguistic dependencies between anaphor and antecedent in a recipe document by exploiting selfattention: we define SPANBERT(w 1 , . . . , w T ) to be the SpanBERT representation of a recipe, where w 1 is the first token and w T refers to the last token of the recipe. SpanBERT captures the long-range dependencies between the antecedent and anaphor in the recipes. A span x i consists of zero or more tokens of instruction I i . We use the verb as a pointer for null pronouns. For example, mix is the token of the null pronoun œï in the instruction mix [œï] (Figure <ref type="figure" target="#fig_4">2</ref>). The vector representation g i of a given span x i is obtained by concatenating the contextualized SpanBERT word vectors of its boundary tokens and its width feature:</p><formula xml:id="formula_0">g i = [x * START(i) , x * END(i) , œï(i)] œï(i) = WIDTH(END(i) -START(i)).</formula><p>START(i) and END(i) represent the starting and ending token indexes for g i , respectively. œï(i) is the width feature of the span where WIDTH(.) is the embedding function of the predefined bins of <ref type="bibr">[1,</ref><ref type="bibr">2,</ref><ref type="bibr">3,</ref><ref type="bibr">4,</ref><ref type="bibr">8,</ref><ref type="bibr">16]</ref> as defined by <ref type="bibr">Clark and Manning (2016b)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.2">Weakly Supervised Object Localization</head><p>Following prior work <ref type="bibr" target="#b19">(Karpathy and Fei-Fei, 2015;</ref><ref type="bibr" target="#b14">Huang et al., 2017</ref><ref type="bibr" target="#b13">Huang et al., , 2018))</ref>, we observe that sentence descriptions of pictures make frequent references to objects in the pictures and their attributes. However, the references are not always clearly defined. Particularly in a video, the use of pronouns and ellipses are extremely common <ref type="bibr" target="#b20">(Kiddon et al., 2015;</ref><ref type="bibr" target="#b14">Huang et al., 2017)</ref>. Our object localization model follows a Weakly Supervised Object Localization (WSOL) strategy <ref type="bibr" target="#b13">(Huang et al., 2018;</ref><ref type="bibr">Zhou et al., 2018a;</ref><ref type="bibr" target="#b2">Choe et al., 2020)</ref>: Only full image descriptions (Figure <ref type="figure" target="#fig_5">3 (c)</ref>) are used for localization instead of a specific object-level label (as in bounding box and specific label pairs for each object in a picture like in Figure <ref type="figure" target="#fig_5">3 (a)</ref>).</p><p>Following the object region and text ranking approach, we formulate the task of WSOL as mapping the frame region v i to the text span x i . We use positive and negative frames where positive frames come from the video clip V i of instruction I i whereas negative frames are drawn from other videos without shared entities. We define pos i to be a region vector set that includes the region proposals v h√ów 1 , .., v h√ów 10 from the positive image V i , and neg i to contain the region proposals v h√ów 1 , .., v h√ów 10 from the negative image, e.g., the negative and positive frames in Figure <ref type="figure" target="#fig_6">4</ref>.</p><p>The aim of WSOL is to produce a scoring function to maximize the joint probability of positive frame regions v i ‚àà pos i and minimize the joint probability of negative regions r i ‚àà neg i with the text span x i . We concatenate the mention representation vector g i of span x i and the visual object region vector r i of the region v i to obtain the WSOL input, [g i , r i ], effectively fusing textual and visual information in one input. We then prepare positive (FFNN(g i , r i ) = 1) and negative (FFNN(g i , r i ) = 0) samples to train a model with attention-based deep MIL <ref type="bibr" target="#b15">(Ilse et al., 2018)</ref>. Positive examples depict exactly the action in question, whereas negative examples correspond to one of four special cases as described below:</p><formula xml:id="formula_1">FFNN(gi, ri) = Ô£± Ô£¥ Ô£¥ Ô£¥ Ô£≤ Ô£¥ Ô£¥ Ô£¥ Ô£≥ 0 xi = œµ, ‚àÄri 0 xi / ‚àà {ei,1, . . . , ei,n}, ‚àÄri 0 xi ‚àà {ei,1, . . . , ei,n}, ri ‚àà neg i 1 xi ‚àà {ei,1, . . . , ei,n}, ri ‚àà pos i</formula><p>Our localization model uses the span representation g i that is extracted by the mention extraction and ten positive, i.e., pos i , and ten negatives, i.e., neg i , region representation vectors r i to learn the best region from pos i for the given span g i . Thus, our mention detection model learns the span vector g i also based on the object localization objective (see also Figure <ref type="figure" target="#fig_6">4</ref>). We define the label of FFNN(g i , r i ) as 1 when the span x i is a gold span {e i,1 , . . . , e i,n } and the region vector r i represent the positive regions pos i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.3">Anaphora Resolution</head><p>Following <ref type="bibr" target="#b24">Lee et al. (2017)</ref> and <ref type="bibr" target="#b28">Oguz et al. (2022)</ref>, we implement our anaphora resolution system as an end-to-end system with mention detection but now extended by object localization. For anaphora resolution, the representation of a span pair g ij is obtained by concatenating the two span embeddings [g i , g j ] and their element-wise multiplication, g i ‚Ä¢ g j , among others:</p><formula xml:id="formula_2">g ij = [g i , g j , g i ‚Ä¢ g j , œï dist (i, j)]</formula><p>where the feature vector œï dist (i, j) is the distance DISTANCE(START(j) -START(i)) between the index of the instruction span i and span j. DIS-TANCE(‚Ä¢) is an embedding function of the predefined bins of <ref type="bibr">[1, 2, 3.., 30]</ref> as in <ref type="bibr" target="#b28">Oguz et al. (2022)</ref>. We use softmax(FFNN(g ij )) to score the resolution for anaphor g i and antecedent g j pairs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Evaluation</head><p>Following <ref type="bibr" target="#b12">Hou et al. (2018)</ref> and <ref type="bibr" target="#b39">Yu and Poesio (2020)</ref>, we assess the performance of our end-toend anaphora resolution with the F1-score where precision is the result of dividing the number of correctly predicted pairs by the total number of predicted pairs and recall is computed by dividing the number of correctly predicted pairs by the total number of gold pairs.</p><p>To evaluate object localization, we follow prior work <ref type="bibr" target="#b9">(Fukui et al., 2016;</ref><ref type="bibr" target="#b35">Rohrbach et al., 2016;</ref><ref type="bibr" target="#b13">Huang et al., 2018)</ref> and compute accuracy as the ratio of phrases for which the predicted bounding box overlaps with the ground-truth by more than 0.5 Intersection-over-Union (IoU).</p><p>6 Experimental Setup 6.1 Input 6.1.1 Cooking Instructions.</p><p>To encode the recipes we use spanBERT <ref type="bibr" target="#b17">(Joshi et al., 2020)</ref>, a transformer model designed to better represent and predict spans of text. We use the concatenation of the boundary tokens to represent each span <ref type="bibr">(Clark and Manning, 2016a,b;</ref><ref type="bibr" target="#b24">Lee et al., 2017;</ref><ref type="bibr" target="#b22">Kobayashi et al., 2022)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.2">Frame Regions of Instructions</head><p>Image regions can be proposed by either off-theshelf object localizers, e.g., region proposal networks <ref type="bibr" target="#b10">(Girshick et al., 2014;</ref><ref type="bibr" target="#b34">Ren et al., 2015)</ref>, or dense sliding windows (e.g., random regions). Region proposal networks and dense sliding window methods neglect language semantics. Therefore, we leverage the state-of-art method RegionCLIP <ref type="bibr" target="#b40">(Zhong et al., 2022)</ref> which uses a CLIP model <ref type="bibr" target="#b32">(Radford et al., 2021)</ref> to match image regions with template captions to align these region-text pairs in the feature space. We select the first 20 region proposals of each entity with the highest objectness scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.1">Anaphora Resolution</head><p>Candidate and Gold Spans. Without any pruning, we consider all continuous token sequences <ref type="bibr">(Clark and Manning, 2016b;</ref><ref type="bibr" target="#b24">Lee et al., 2017)</ref> as potential spans for anaphor/antecedent candidates for the training and testing phases. Additionally, we consider gold spans for the training and testing phases in order to investigate the performance of anaphora resolution models without mention detection noise.</p><p>With and without Object Localization. To understand the effect of object localization on anaphora resolution we examine the anaphora resolution results with and without object localization. Here, we remove the object localization model and data from the training and testing phases of our anaphora resolution model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.2">Object Localization</head><p>With and without Anaphora Resolution. We investigate the object localization performance with and without anaphora resolution to understand the impact of anaphora resolution. We perform four different experiments for object localization. <ref type="bibr" target="#b13">(Huang et al., 2018)</ref>: Since we use the object region proposal with the highest objectness scores with the given ten region proposals, we first examine random selection with the highest objectness score to show the complexity of object localization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Random</head><p>Deep Visual-Semantic Alignment (DVSA) <ref type="bibr" target="#b19">(Karpathy and Fei-Fei, 2015)</ref>: This weakly supervised visual grounding method is based on image-based regions and given gold and candidate   mentions without anaphoric information, which uses multiple-instance learning. Thus, we consider DVSA as a baseline for weakly supervised object localization to our method without anaphora resolution. We examine the results of DVSA with gold and candidate mentions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>AR-Cand. Mentions</head><p>We train and test the object localization model with all continuous tokens <ref type="bibr">(Clark and Manning, 2016b;</ref><ref type="bibr" target="#b24">Lee et al., 2017)</ref> as potential spans for anaphor/antecedent candidates. Candidate spans also increase the noise in the object label set because we consider all possible spans as labels of the objects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>AR-Gold Mentions</head><p>We show the results of object localization when we use gold mentions and anaphor information for training and testing.</p><p>7 Results and Discussion</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Overview</head><p>We investigate the anaphora resolution and object localization results of gold and candidate spans comparing the F1-scores and Top-1 scores with multitask learning using both tasks and single tasks. Overall, our results in Table <ref type="table" target="#tab_5">3</ref> demonstrate that replacing single-task learning with our multitask joint learning approach improves anaphora reso-lution and object localization for both candidate and gold spans. The difference between the results of candidate and gold spans demonstrates that the mention extraction model propagates errors to anaphora resolution and object localization (the sequential structure of model see Figure <ref type="figure" target="#fig_6">4</ref>). For example, we have the candidate spans such as all n-grams words (bigrams words such as cook the, the bacon, bacon fat) of an instruction cook the bacon fat with the same visual features with bacon and fat in a pan. Thus, an incorrect mention of mention detection directly causes an error in object localization and anaphora resolution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Weakly Supervised Object Localization</head><p>Overall, we observe an improvement in the performance of object localization with multitask learning shown in Table <ref type="table" target="#tab_3">2</ref>. Table <ref type="table" target="#tab_3">2</ref> shows that our joint multitask learning method outperforms DVSA even with candidate mention. Note that DVSA is not the component that is responsible for extracting the mentions for object localization. Thus, we do not analyze object localization for null pronouns and candidate mentions because DVSA ranks the language expression and region pairs and does not apply mention extraction. However, we attached the DVSA method for ranking the gold nominal mentions with the given region proposals.</p><p>The results of object localization with null pronouns clearly demonstrate the benefits of anaphora resolution for object localization. For example, our model localizes the singular mention a jar in <ref type="bibr">Figure 5 (a)</ref>, and the anaphor another piece of bread in Figure <ref type="figure" target="#fig_8">5</ref> (b) better than other methods. Thanks to the mention detection and representation of our multitask learning approach, our object localization model is capable of localization of null pronouns, e.g., Figure <ref type="figure" target="#fig_8">5 (c,</ref><ref type="figure">d</ref>) as our model learns to represent the null pronouns in the mention detection process as a mention. Thus, the results of null pronouns  clearly evidence the contribution of anaphora resolution for object localization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">Anaphora Resolution</head><p>Multitask learning of anaphora resolution and object localization increases the performance of anaphora resolution. Table <ref type="table" target="#tab_5">3</ref> shows &gt; 2% improvements for anaphora resolution with gold mentions and more than 1% for candidate mentions with combined training and testing. When nominal and zero anaphora are investigated separately, the results of nominal anaphora demonstrate an improvement on the results of gold mentions when object localization is included. Additionally, the results of zero anaphora show significant improvements with multitask learning for gold and candidate mentions. Thus, we observe a big part of the improvement for the combined experiments with object localization comes from the zero anaphora resolution samples.</p><p>The benefit of object localization on anaphora resolution is also seen in candidate nominal mentions; it is however not as significant as in gold nominal mentions, as the errors of mention detection for candidate spans directly cause localization as well as anaphora resolution errors.</p><p>The most common error in anaphora resolution is to find the closest antecedent in the entity chain. For example, for tomato-‚Üí it -‚Üí tomato, the first tomato is the antecedent of it, and it is the antecedent of the second tomato. However, the model fails to select the first tomato as the antecedent for the second one.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion and Future Work</head><p>In this work, we study the problem of visuallinguistic ambiguity in multimodal data and propose the novel task of joint anaphora resolution and object localization. We create the Find2Find dataset for object localization and anaphora resolution with cooking recipes to improve the performance of both tasks by exploiting their commonalities. We implement a model for the joint learning of anaphora resolution and object localization, fusing visual and textual information, and show empirically that a multitask learning paradigm mutually improves both tasks. Especially, the results of zero anaphora resolution indicate that object localization helps to avoid linguistic ambiguity of null pronouns. Our joint multitask learning approach does not apply the temporal features of evolving visual entities even though the temporal features of textual recipes <ref type="bibr" target="#b28">(Oguz et al., 2022)</ref> are included due to instruction order. In future work, we claim the temporal encoding of visual objects can improve the results of joint anaphora resolution and object localization.  shape the mixture into small patty fry patties in a pan with some oil 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2.</head><p>3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4.</head><p>shred the potatoes and sweet potatoes to a bowl add garlic salt , black pepper and chilli powder 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>6.</head><p>7.</p><p>8.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>9.</head><p>10.</p><p>cook the onions in it </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Anaphora Annotation</head><p>Anaphora resolution is a challenging task for instructional languages because of temporally evolving entities <ref type="bibr" target="#b28">(Oguz et al., 2022)</ref>. For example, Figure <ref type="figure" target="#fig_10">6</ref> demonstrates how the onions of the sixth instruction step refers to the onions of the fourth instruction and the onions of the fourth instruction refers to the onions of the third instruction: the onions -‚Üí the onion -‚Üí the onions. Most of the errors of anaphora resolution for recipes occur when the model predicts the onions of the third instruction as the antecedent of the anaphor the onions of the sixth instruction. Additionally, Figure <ref type="figure" target="#fig_10">6</ref> shows the null pronouns in the eighth instruction and the antecedents such as the fifth, sixth, and seventh instructions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Implementation Details</head><p>We use Adam (Kingma and Ba, 2014) for optimization and a learning rate of 0.001. We clip gradients element-wise at 5 and use 0.3 dropouts for regularization. We use Negative log-likelihood as a loss function for the both task of anaphora resolution and object localization.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>add red wine to the pan add the beef into a hot pan boil pierogi in boiled water fry them with onion and butter</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>add butter and garlic to the pan ...</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: An example to display how visual-linguistic ambiguity occurs with a zero anaphor. The zero anaphor [œï] refers to two previous instructions as shown. The entities are aligned to the object with the arrows and the color codes.</figDesc><graphic coords="3,91.23,71.23,127.96,71.98" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Different examples of object localization. The dashed lines in (b) and (c) represent the occurrence of annotation for only the test data whereas the straight lines (a) indicate that annotation is present for train and test.</figDesc><graphic coords="4,341.49,51.58,148.27,222.41" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: The architecture of the multitask learning framework of anaphora resolution and object localization.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>. The YouCookII dataset(Zhou et al., 2018a,b)  includes manually provided descriptions (i.e., instructions) of actions with the corresponding temporal boundaries (i.e. start and end timestamps) in 2,000 cooking videos. The videos provide visual input of the corresponding objects to observe changes of the objects clearly.Oguz et al. (</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: The examples of object localization results of random with the green bounding box, DVSA with the blue bounding box, and our multitask learning method with the red bounding box. The task is the object localization of the gold mention in bold font in the instructions.</figDesc><graphic coords="9,71.21,85.03,110.59,62.20" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: An example of the annotation of anaphora resolution for cooking instructions. The grey boxes represent the singular mentions without any anaphoric relations. The beginning of the arrows shows the anaphora whereas the ends point to the antecedent.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>c. For example,<ref type="bibr" target="#b13">Huang et al. (2018)</ref> propose to extend object localization to localization based on context-dependent referring expressions, and<ref type="bibr" target="#b23">(Kuo et al., 2022)</ref> offers a general-purpose model for object localization by using a wide range of referring expressions, localization or detection queries for zero, one, or multiple objects. Another challenge for object localization with image descriptions is the automatic extraction of object labels from image descriptions. Zhou et al. (2018a) extract object labels manually for training and testing object localization models whereas Huang et al.</figDesc><table /><note><p><p><p><p>(2018)  </p>apply the pre-trained Stanford CoreNLP parser</p><ref type="bibr" target="#b26">(Manning et al., 2014)</ref> </p>for entity detection.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Annotated Data Statistics</figDesc><table><row><cell></cell><cell>Train</cell><cell>Test</cell></row><row><cell>Entity</cell><cell cols="2">9,316 2,842</cell></row><row><cell cols="2">Null Pronoun 1,002</cell><cell>282</cell></row><row><cell>Pronoun</cell><cell>314</cell><cell>129</cell></row><row><cell cols="3">Noun Phrases 8,000 2,431</cell></row><row><cell cols="3">Instruction 4,633 1,422</cell></row><row><cell>Recipe</cell><cell>400</cell><cell>100</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>The Top-1 results of object localization with gold and candidate mentions. The column group named Nominal shows the results of object localization for nominal phrases, Null depicts the results of null pronouns, and All refers to the full dataset.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Results of the anaphora resolution with and without object localization for gold and candidate We show the results for the full test datasets in Anaphora Res. columns, for only null pronouns in Zero Anaphora Res., and the resolution results of anaphoric mentions for all nominal phrases in Nominal Anaphora Res. part.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>on Computer Vision and Pattern Recognition, pages 16793-16803. Bolei Zhou, Aditya Khosla, Agata Lapedriza, Aude Oliva, and Antonio Torralba. 2016. Learning deep features for discriminative localization. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR). Luowei Zhou, Nathan Louis, and Jason J. Corso. 2018a. Weakly-supervised video object grounding from text by loss weighting and object interaction. In BMVC. Luowei Zhou, Chenliang Xu, and Jason J. Corso. 2018b. Towards automatic learning of procedures from web instructional videos. In AAAI. Dimitri Zhukov, Jean-Baptiste Alayrac, Ramazan Gokberk Cinbis, David Fouhey, Ivan Laptev, and Josef Sivic. 2019. Cross-task weakly supervised learning from instructional videos. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 3537-3545.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>https://github.com/OguzCennet/odar</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="9">Acknowledgements</head><p>We would like to thank <rs type="person">Gokul Srinivasagan</rs> and <rs type="person">David Meier</rs> for helping with the annotation of the object localization, <rs type="person">Alina Leippert</rs> for helping with the annotation of the anaphora resolution. This research was funded by the <rs type="funder">joint IMPRESS</rs> (<rs type="grantNumber">01|S20076</rs>) project between the <rs type="funder">French National Institute for Research in Digital Science and Technology (Inria)</rs> and the <rs type="funder">German Research Center for Artificial Intelligence (DFKI)</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_6SxczZX">
					<idno type="grant-number">01|S20076</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Limitations</head><p>The first and most important limitation is the pre-trained region proposal network (RegionCLIP <ref type="bibr" target="#b40">(Zhong et al., 2022)</ref>). Our object localization is highly dependent on the quality of the region proposals. Therefore, a better region proposal network delivers more improvements for our multitask learning.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Unsupervised learning from narrated instruction videos</title>
		<author>
			<persName><forename type="first">Jean-Baptiste</forename><surname>Alayrac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nishant</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Lacoste-Julien</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="4575" to="4583" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Robust object tracking with online multiple instance learning</title>
		<author>
			<persName><forename type="first">Boris</forename><surname>Babenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1619" to="1632" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Evaluating weakly supervised object localization methods right</title>
		<author>
			<persName><forename type="first">Junsuk</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seong</forename><surname>Joon Oh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seungho</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanghyuk</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeynep</forename><surname>Akata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyunjung</forename><surname>Shim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deep reinforcement learning for mention-ranking coreference models</title>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D16-1245</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Austin, Texas</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2256" to="2262" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">b. Improving coreference resolution by learning entitylevel distributed representations</title>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P16-1061</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="643" to="653" />
		</imprint>
	</monogr>
	<note>: Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Weakly supervised localization and learning with generic knowledge</title>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Deselaers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bogdan</forename><surname>Alexe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vittorio</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">100</biblScope>
			<biblScope unit="page" from="275" to="293" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Solving the multiple instance problem with axis-parallel rectangles</title>
		<author>
			<persName><forename type="first">Richard</forename><forename type="middle">H</forename><surname>Thomas G Dietterich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom√°s</forename><surname>Lathrop</surname></persName>
		</author>
		<author>
			<persName><surname>Lozano-P√©rez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial intelligence</title>
		<imprint>
			<biblScope unit="volume">89</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="31" to="71" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">What does it take to bake a cake? the RecipeRef corpus and anaphora resolution in procedural text</title>
		<author>
			<persName><forename type="first">Biaoyan</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><surname>Baldwin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karin</forename><surname>Verspoor</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2022.findings-acl.275</idno>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: ACL 2022</title>
		<meeting><address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="3481" to="3495" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">ChEMU-ref: A corpus for modeling anaphora resolution in the chemical domain</title>
		<author>
			<persName><forename type="first">Biaoyan</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Druckenbrodt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Saber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiayuan</forename><surname>Akhondi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karin</forename><surname>Baldwin</surname></persName>
		</author>
		<author>
			<persName><surname>Verspoor</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.eacl-main.116</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume</title>
		<meeting>the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1362" to="1375" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">Akira</forename><surname>Fukui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dong</forename><forename type="middle">Huk</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daylen</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anna</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.01847</idno>
		<title level="m">Multimodal compact bilinear pooling for visual question answering and visual grounding</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="580" to="587" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Multi-fold mil training for weakly supervised object localization</title>
		<author>
			<persName><forename type="first">Ramazan</forename><surname>Gokberk Cinbis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Verbeek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="2409" to="2416" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Unrestricted bridging resolution</title>
		<author>
			<persName><forename type="first">Yufang</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katja</forename><surname>Markert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Strube</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="237" to="284" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Finding &quot;it&quot;: Weakly-supervised, reference-aware visual grounding in instructional videos</title>
		<author>
			<persName><surname>De-An</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shyamal</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucio</forename><surname>Buch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Animesh</forename><surname>Dery</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juan</forename><forename type="middle">Carlos</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Unsupervised visual-linguistic reference resolution in instructional videos</title>
		<author>
			<persName><surname>De-An</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><forename type="middle">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juan</forename><forename type="middle">Carlos</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2183" to="2192" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Attention-based deep multiple instance learning</title>
		<author>
			<persName><forename type="first">Maximilian</forename><surname>Ilse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakub</forename><surname>Tomczak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2127" to="2136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title/>
		<author>
			<persName><surname>Pmlr</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Span-BERT: Improving pre-training by representing and predicting spans</title>
		<author>
			<persName><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">S</forename><surname>Weld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<idno type="DOI">10.1162/tacl_a_00300</idno>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="64" to="77" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">BERT for coreference resolution: Baselines and analysis</title>
		<author>
			<persName><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Weld</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1588</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="5803" to="5808" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep visualsemantic alignments for generating image descriptions</title>
		<author>
			<persName><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="3128" to="3137" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Mise en place: Unsupervised interpretation of instructional recipes</title>
		<author>
			<persName><forename type="first">Chlo√©</forename><surname>Kiddon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thandavam</forename><surname>Ganesa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Ponnuraj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="982" to="992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Constrained multi-task learning for bridging resolution</title>
		<author>
			<persName><forename type="first">Hideo</forename><surname>Kobayashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yufang</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Ng</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2022.acl-long.56</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 60th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="759" to="770" />
		</imprint>
	</monogr>
	<note>ume 1: Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Findit: Generalized localization with natural language queries</title>
		<author>
			<persName><forename type="first">Weicheng</forename><surname>Kuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fred</forename><surname>Bertsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Piergiovanni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anelia</forename><surname>Saffar</surname></persName>
		</author>
		<author>
			<persName><surname>Angelova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2022: 17th European Conference</title>
		<meeting><address><addrLine>Tel Aviv, Israel</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022-10-23">2022. October 23-27, 2022</date>
			<biblScope unit="volume">XXXVI</biblScope>
			<biblScope unit="page" from="502" to="520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">End-to-end neural coreference resolution</title>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D17-1018</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="188" to="197" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Malmaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vivek</forename><surname>Rathod</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Johnston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.01558</idno>
		<title level="m">What&apos;s cookin&apos;? interpreting cooking videos using text, speech and vision</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">The stanford corenlp natural language processing toolkit</title>
		<author>
			<persName><forename type="first">Mihai</forename><surname>Christopher D Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jenny</forename><forename type="middle">Rose</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Finkel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Bethard</surname></persName>
		</author>
		<author>
			<persName><surname>Mc-Closky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 52nd annual meeting of the association for computational linguistics: system demonstrations</title>
		<meeting>52nd annual meeting of the association for computational linguistics: system demonstrations</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="55" to="60" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Howto100m: Learning a text-video embedding by watching hundred million narrated video clips</title>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Miech</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dimitri</forename><surname>Zhukov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean-Baptiste</forename><surname>Alayrac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Makarand</forename><surname>Tapaswi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2630" to="2640" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Chop and change: Anaphora resolution in instructional cooking videos</title>
		<author>
			<persName><forename type="first">Cennet</forename><surname>Oguz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivana</forename><surname>Kruijff-Korbayova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emmanuel</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pascal</forename><surname>Denis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Josef</forename><surname>Van Genabith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: AACL-IJCNLP 2022</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="364" to="374" />
		</imprint>
	</monogr>
	<note>Online only</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Is object localization for free?-weaklysupervised learning with convolutional neural networks</title>
		<author>
			<persName><forename type="first">Maxime</forename><surname>Oquab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L√©on</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="685" to="694" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Anaphora resolution with the ARRAU corpus</title>
		<author>
			<persName><forename type="first">Massimo</forename><surname>Poesio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yulia</forename><surname>Grishina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Varada</forename><surname>Kolhatkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nafise</forename><surname>Moosavi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ina</forename><surname>Roesiger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Roussel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabian</forename><surname>Simonjetz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandra</forename><surname>Uma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olga</forename><surname>Uryupina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juntao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heike</forename><surname>Zinsmeister</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W18-0702</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First Workshop on Computational Models of Reference, Anaphora and Coreference</title>
		<meeting>the First Workshop on Computational Models of Reference, Anaphora and Coreference<address><addrLine>New Orleans, Louisiana</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="11" to="22" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Learning object class detectors from weakly annotated video</title>
		<author>
			<persName><forename type="first">Alessandro</forename><surname>Prest</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Leistner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Javier</forename><surname>Civera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vittorio</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2012 IEEE Conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="3282" to="3289" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Learning transferable visual models from natural language supervision</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jong</forename><forename type="middle">Wook</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Hallacy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pamela</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="8748" to="8763" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title/>
		<author>
			<persName><surname>Pmlr</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Grounding of textual phrases in images by reconstruction</title>
		<author>
			<persName><forename type="first">Anna</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ronghang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2016: 14th European Conference</title>
		<meeting><address><addrLine>Amsterdam, The Netherlands</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016-10-11">2016. October 11-14, 2016</date>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="817" to="834" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Efficient object localization using convolutional networks</title>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Goroshin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arjun</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christoph</forename><surname>Bregler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Deep multiple instance learning for image classification and auto-annotation</title>
		<author>
			<persName><forename type="first">Jiajun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinan</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="3460" to="3469" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">The CODI-CRAC 2022 shared task on anaphora, bridging, and discourse deixis in dialogue</title>
		<author>
			<persName><forename type="first">Juntao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sopan</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ramesh</forename><surname>Manuvinakurike</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lori</forename><surname>Levin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Massimo</forename><surname>Poesio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Strube</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carolyn</forename><surname>Ros√©</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the CODI-CRAC 2022 Shared Task on Anaphora, Bridging, and Discourse Deixis in Dialogue</title>
		<meeting>the CODI-CRAC 2022 Shared Task on Anaphora, Bridging, and Discourse Deixis in Dialogue<address><addrLine>Gyeongju, Republic of Korea</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Multitask learning-based neural bridging reference resolution</title>
		<author>
			<persName><forename type="first">Juntao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Massimo</forename><surname>Poesio</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.coling-main.315</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Computational Linguistics</title>
		<meeting>the 28th International Conference on Computational Linguistics<address><addrLine>Barcelona, Spain (Online</addrLine></address></meeting>
		<imprint>
			<publisher>International Committee on Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="3534" to="3546" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Regionclip: Region-based language-image pretraining</title>
		<author>
			<persName><forename type="first">Yiwu</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianwei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengchuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noel</forename><surname>Codella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liunian</forename><surname>Harold Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiyang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yin</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference</title>
		<meeting>the IEEE/CVF Conference</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
