<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Privacy-Preserving Adversarial Representation Learning in ASR: Reality or Illusion?</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Brij</forename><surname>Mohan</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Lal</forename><surname>Srivastava</surname></persName>
							<email>brij.srivastava@inria.fr</email>
							<affiliation key="aff0">
								<orgName type="institution">INRIA</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Aurélien</forename><surname>Bellet</surname></persName>
							<email>aurelien.bellet@inria.fr</email>
							<affiliation key="aff0">
								<orgName type="institution">INRIA</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Marc</forename><surname>Tommasi</surname></persName>
							<email>marc.tommasi@inria.fr</email>
							<affiliation key="aff1">
								<orgName type="institution">Université de Lille</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Emmanuel</forename><surname>Vincent</surname></persName>
							<email>emmanuel.vincent@inria.fr</email>
							<affiliation key="aff2">
								<orgName type="institution" key="instit1">Université de Lorraine</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
								<orgName type="institution" key="instit3">Inria</orgName>
								<address>
									<addrLine>Loria</addrLine>
									<postCode>F-54000</postCode>
									<settlement>Nancy</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Privacy-Preserving Adversarial Representation Learning in ASR: Reality or Illusion?</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">EF3A033AB602BC2778100C8156773EA4</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-04-12T14:53+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>speech recognition</term>
					<term>end-to-end system</term>
					<term>privacy</term>
					<term>adversarial training</term>
					<term>speaker recognition</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Automatic speech recognition (ASR) is a key technology in many services and applications. This typically requires user devices to send their speech data to the cloud for ASR decoding. As the speech signal carries a lot of information about the speaker, this raises serious privacy concerns. As a solution, an encoder may reside on each user device which performs local computations to anonymize the representation. In this paper, we focus on the protection of speaker identity and study the extent to which users can be recognized based on the encoded representation of their speech as obtained by a deep encoderdecoder architecture trained for ASR. Through speaker identification and verification experiments on the Librispeech corpus with open and closed sets of speakers, we show that the representations obtained from a standard architecture still carry a lot of information about speaker identity. We then propose to use adversarial training to learn representations that perform well in ASR while hiding speaker identity. Our results demonstrate that adversarial training dramatically reduces the closedset classification accuracy, but this does not translate into increased open-set verification error hence into increased protection of the speaker identity in practice. We suggest several possible reasons behind this negative result.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>With the emergence of pervasive voice assistants <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref> like Amazon Alexa, Apple's Siri and Google Home, voice has become one of the most widespread forms of human-machine interaction. In this context, the speech signal is sent from the user device to a cloud-based service, where automatic speech recognition (ASR) and natural language understanding are performed in order to address the user request. 1 While recent studies have identified security vulnerabilities in these devices <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref>, such studies tend to hide more important privacy risks that can have long-term impact. Indeed, state-of-the-art speech processing algorithms can infer not only the spoken contents from the speech signal, but also the speaker's identity <ref type="bibr" target="#b4">[5]</ref>, intention <ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref>, gender <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11]</ref>, emotional state <ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref>, pathological condition <ref type="bibr" target="#b14">[15]</ref><ref type="bibr" target="#b15">[16]</ref><ref type="bibr" target="#b16">[17]</ref>, personality <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19]</ref> and cultural <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b20">21]</ref> attributes to a great extent. These algorithms require just a few tens of hours of training data to achieve reasonable accuracy, which is easier than ever to collect via virtual assistants. The dissemination of voice signals in large data centers thereby poses severe privacy threats to the users in the long run.</p><p>These privacy issues have little been investigated so far. The most prominent studies use homomorphic encryption and bit 1 See e.g., https://cloud.google.com/speech-to-text/ string comparison <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b22">23]</ref>. While these methods provide strong cryptographic guarantees, they come at a large computational overhead and can hardly be applied to state-of-the-art end-toend deep neural network based systems.</p><p>An alternative software architecture is to pre-process voice data on the device to remove some personal information before sending it to web services. Although this does not rule out all possible risks, a change of representation of the voice signal can contribute to limiting unsolicited uses of data. In this paper, we investigate how much of a user's identity is encoded in speech representations built for ASR. To this end, we conduct closed-and open-set speaker recognition experiments. The closed-set experiment refers to a classification setting where all test speakers are known at training time. In contrast, the openset experiment (a.k.a. speaker verification) aims to measure the capability of an attacker to discriminate between speakers in a more realistic setting where the test speakers are not known beforehand. We implement the attacker with the state-of-the-art x-vector speaker recognition technique <ref type="bibr" target="#b23">[24]</ref>.</p><p>The representations of speech we consider in our work are given by the encoder output of end-to-end deep encoderdecoder architectures trained for ASR. Such architectures are natural in our privacy-aware context, as they correspond to encoding speech on the user device and decoding in the cloud. Our baseline network follows the ESPnet architecture <ref type="bibr" target="#b24">[25]</ref>, with one encoder and two decoders: one based on connectionist temporal classification (CTC) and the other on an attention mechanism. Inspired by <ref type="bibr" target="#b25">[26]</ref>, we further propose to extend the network with a speaker-adversarial branch so as to learn representations that perform well in ASR while hiding the speaker identity.</p><p>Several papers have recently proposed to use adversarial training for the goal of improving ASR performance by making the learned representations invariant to various conditions. While general form of acoustic variabilities have been studied <ref type="bibr" target="#b26">[27]</ref>, there is some work specifically on speaker invariance <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b28">29]</ref>. Interestingly, there is no general consensus on whether it is more appropriate to use speaker classification in an adversarial or a multi-task manner, despite the fact that these two strategies implement opposite means (i.e., encouraging representations to be speaker-invariant or speaker-specific). This question was studied in <ref type="bibr" target="#b29">[30]</ref>, in which the authors conclude that both approaches only provide minor improvements in terms of ASR performance. Their speaker classification experiments also show that the baseline system already tends to learn speaker-invariant features. However, they did not run speaker verification experiments and hence did not assess the suitability of these features for the goal of anonymization.</p><p>In contrast to these studies which aim to increase ASR performance, our goal is to assess the potential benefit of adversarial training for concealing speaker identity in the con- text of privacy-friendly ASR. Our contributions are the following. First, we combine CTC, attention and adversarial learning within an end-to-end ASR framework. Second, we design a rigorous protocol to quantify speaker identity in ASR representations through a series of closed-set classification and open-set verification experiments. Third, we run these experiments on the Librispeech corpus <ref type="bibr" target="#b30">[31]</ref> and show that this framework dramatically reduces speaker classification accuracy, but does not increase speaker verification error. We suggest several possible reasons behind this disparity. The structure of the rest of the paper is as follows. In Section 2, we describe the baseline ASR model and our proposed adversarial model. Section 3 explains the experimental setup and presents our results. Finally, we conclude and discuss future work in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Proposed model</head><p>We start by describing the ASR model we use as a baseline, before introducing our speaker-adversarial network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Baseline ASR model</head><p>We use the end-to-end ASR framework presented in <ref type="bibr" target="#b31">[32]</ref> as the baseline architecture. It is composed of three sub-networks: an encoder which transforms the input sequence of speech feature vectors into a new representation φ, and two decoders that predict the character sequence from φ. We assume that these networks have already been trained using data previously collected by the service provider (which may be public data, opt-in user data, etc). Then, in the deployment phase of the system that we envision, the encoder would run on the user device and the resulting representation φ would be sent to the cloud for decoding.</p><p>The first decoder is based on CTC and the second on an attention mechanism. As argued in <ref type="bibr" target="#b31">[32]</ref>, attention works well in most cases because it does not assume conditional independence between the output labels (unlike CTC). However, it is so flexible that it allows nonsequential alignments which are undesirable in the case of ASR. Hence, CTC acts as a regularizer to prune such misaligned hypotheses. We denote by θe the parameters of the encoder, and by θc and θa the parameters of the CTC and attention decoders respectively. The model is trained in an end-to-end fashion by minimizing an objective function Lasr which is a combination of the losses Lc and La from both decoder branches: min θe,θc,θa Lasr(θe, θc, θa) = λLc(θe, θc) + (1 -λ)La(θe, θa), with λ ∈ [0, 1] a trade-off parameter between the two decoders.</p><p>We now formally describe the form of the two losses Lc and La. We denote each sample in the dataset as Si = (Xi, Yi, zi), where Xi = {x1, ..., xT } is the sequence of T acoustic feature frames, Yi = {y1, ..., yM } is the sequence of M characters in the transcription, and zi is the speaker label. In the case of CTC, several intermediate label sequences of length T are created by repeating characters and inserting a speacial blank label to mark character boundaries. Let Ψ(Yi) be the set of all such intermediate label sequences. The CTC loss Lc(θe, θc) is computed as Lc = -ln P (Yi|Xi; θe, θc) where P (Yi|Xi; θe, θc) = ψ∈Ψ(Y i ) P (ψ|Xi; θe, θc). This sum is computed by assuming conditional independence over Xi, hence P (ψ|Xi; θe, θc) = T t=1 P (ψt|Xi; θe, θc) ≈ T t=1 P (ψt; θe, θc). The attention branch does not require an intermediate label representation and conditional independence is not assumed, hence the loss is simply computed as La(θe, θa) = -m∈M ln P (ym|Xi, y1:m-1; θe, θa).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Speaker-adversarial model</head><p>In order to encourage the network to learn representations that are not only good at ASR but also hide speaker identity, we propose to extend the above architecture with what we call a speaker-adversarial branch. This branch models an adversary which attempts to infer the speaker identity from the encoded representation φ. We denote by θs the parameters of the speaker-adversarial branch. Given the encoder parameters θe, the goal of the adversary is to find θs that minimizes the loss L spk (θe, θs) = -ln P (zi|Xi; θe, θs). Our new model is then trained in an end-to-end manner by optimizing the following min-max objective: min θe,θc,θa max θs Lasr(θe, θc, θa) -αL spk (θe, θs), where α ≥ 0 is a trade-off parameter between the ASR objective and the speaker-adversarial objective. The baseline network can be recovered by setting α = 0. Note that the max part of the objective corresponds to the adversary, which controls only the speaker-adversarial parameters θs. The goal of the speaker-adversarial branch is to act as a "good adversary" and produce useful gradients to remove the speaker identity information from the encoded representation φ. In practice, we use a gradient reversal layer <ref type="bibr" target="#b32">[33]</ref> between the encoder and the speaker-adversarial branch so that the whole network can be trained end-to-end via backpropagation. We refer to Fig. <ref type="figure" target="#fig_0">1</ref> for an illustration of the full architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Experimental evaluation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Datasets</head><p>We use the Librispeech corpus <ref type="bibr" target="#b30">[31]</ref> for all the experiments. We use different subsets for ASR training, adversarial training, and speaker verification. For the sake of clarity we refer to them as data-full, data-adv, and data-spkv, respectively (see Table <ref type="table" target="#tab_0">1</ref>).</p><p>The data-full set is almost the original Librispeech corpus, including train-960 for training, and dev-other for validation, and test-clean and test-other for test, except that utterances with more than 3,000 frames or more than 400 characters have been removed from train-960 for faster training.</p><p>The data-adv set is a 100 h subset of train-960, which is obtained by removing long utterances from the original Librispeech train-100 set similarly to above. It is split into three subsets in order to perform closed-set speaker identification experiments, since the speakers in the original train/dev/test splits are disjoint. There are 251 speakers in data-adv: we assign 2 utterances per speaker to each test-adv and dev-adv. The remaining utterances are used for training and referred to as train-adv.</p><p>For speaker verification with x-vectors <ref type="bibr" target="#b23">[24]</ref>, we use dataspkv, which is again derived from data-full. The train-960 subset was augmented using room impulse responses, isotropic and point-source noises <ref type="bibr" target="#b33">[34]</ref> as well as music and speech <ref type="bibr" target="#b34">[35]</ref> as per the standard sre16 recipe for training x-vectors <ref type="bibr" target="#b23">[24]</ref> from the Kaldi toolkit <ref type="bibr" target="#b35">[36]</ref>, which we adapted to Librispeech. This increased the amount of data by a factor of 4. A subset of the augmented data containing 373,985 utterances was used to train the x-vector representation and another subset containing 422,491 utterances to train the probabilistic linear discriminant analysis (PLDA) backend. These subsets are referred to as train-spkv and train-plda, respectively. For evaluation, we built an enrollment set (test-clean-enroll) and a trial set (test-clean-trial) from the test-clean data. Out of 40, 29 speakers were selected from test-clean based on sufficient data availability. For each speaker, we selected a 1 min subset after speech activity detection for enrollment and used the rest for trials. The details of the trials are given in Table <ref type="table" target="#tab_1">2</ref>. The ACC and the EER will be computed for different representations: the baseline filterbank features, the representations encoded by the network trained for ASR only (corresponding to φ0) as well as those obtained with the speaker-adversarial approach (corresponding to φα for some values of α &gt; 0).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Network architecture and training</head><p>For all experiments, we use the ESPnet <ref type="bibr" target="#b24">[25]</ref> toolkit which implements the hybrid CTC/attention architecture <ref type="bibr" target="#b31">[32]</ref>. The input features are 80-dimensional mel-scale filterbank coefficients with pitch and energy features, totalling 84 features per frame. The encoder is composed of a VGG-like convolutional neural network (CNN) layer followed by 5 bidirectional long shortterm memory (LSTM) layers with 1,024 units. The VGG layer contains 4 convolutional layers followed by max pooling. The feature maps used in the convolution layers are of dimensions (1×64), (64×64), (64×128) and (128×128). The attentionbased decoder consists of location-aware attention <ref type="bibr" target="#b36">[37]</ref> with 10 convolutional channels of size 100 each followed by 2 LSTM layers with 1,024 units. The CTC loss is computed over several possible label sequences using dynamic programming. In all experiments, the trade-off parameter λ between the two decoder losses is set to 0.5. We train a single-layer recurrent neural network language model (RNNLM) with 1,024 hidden units over the train-960 transcriptions and use it to rescore the ASR hypotheses. The resulting WER is very close to the state of the art <ref type="bibr" target="#b37">[38]</ref> when trained on train-960. Finally, we implemented the speaker-adversarial branch via a 3 bidirectional LSTM layers with 512 units followed by a softmax layer with 251 outputs corresponding to the 251 speakers in data-adv. The adversarial loss L spk is summed across all vectors in the sequence. The speaker label zi is duplicated to match the length of the sequence, which is smaller than T due to the subsampling performed within the encoder. Due to this subsampling as well as to the use of bidirectional LSTM layers within the encoder and the speaker-adversarial branch, the frame-level adversarial loss approximates well a utterance-level speaker loss that would be computed from a fixed-sized utterance-level representation, while being easier to train.</p><p>In all experiments, we start by pre-training the ASR branch for 10 epochs over data-full and then the speaker-adversarial branch for 15 epochs on data-adv in order to get a strong adversary on the pre-trained encoded representations. Then, due to time constraints, all networks are fine-tuned on data-adv: we run 15 epochs of adversarial training (which corresponds to simple ASR training when α = 0). Due to this, the WER is comparable to that typically achieved by end-to-end methods when trained on the train-100 subset of Librispeech rather than the full train-960 set. Finally, freezing the resulting encoder, we further fine-tune the speaker-adversarial branch only for 5 epochs to make sure that the reported ACC reflects the performance of a well-trained adversary.</p><p>The encoder network contains 133.5M parameters. To encode a 10s audio file, it perform 1.1e12 arithmetic operations which can be executed in-parallel on a 40 core CPU in 17.6s  and on a single Tesla P100 GPU in 149ms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Results and discussion</head><p>We train our speaker-adversarial network for α ∈ {0, 0.5, 2.0}, leading to three encoded representations φα(X). Recall that α = 0 corresponds to the baseline ASR system as it ignores the speaker-adversarial branch. Table <ref type="table" target="#tab_2">3</ref> summarizes the results. The first column presents the ACC and EER obtained with the input filerbank features, which are consistent with the numbers reported in the literature. As expected, speaker identification and verification can be addressed to very high accuracy on those features. Using the encoded representation φ0(X) trained for ASR only already provides a significant privacy gain: the ACC is divided by 2 and the EER is multiplied by 4, which suggests that a reasonable amount of speaker information is removed during ASR training. Nevertheless, φ0(X) still contains some speaker identity information.</p><p>More interestingly, our results clearly show that adversarial training drastically reduces the performance in speaker identification but not in verification. On the other hand, and counterintuitive to the speaker-invariance claims by several previous studies, we observe that the verification performance actually improves after adversarial training. This exhibits a possible limitation in the generalization of adversarial training to unseen speakers and hence establishes the need for further investigation. The reason for the disparity between classification and verification performance might be that the speaker-adversarial branch does not inherently perform verification and hence is not optimized for that task. It might also be attributed to the representation capacity of that branch, to the number of speakers presented during adversarial training, and/or to the exact range of α needed for generalizable anonymization. These factors of variation open several venues for future experiments. We also notice that the WER stays reasonably low and stabilizes to the value of 12.5% after increasing α from 0.5 to 2. In particular, for α = 2 the WER is just 1.6% absolute more than the baseline (α = 0). We evaluate whether utterances from the same speaker stay in the same neighborhood or are scattered in the representation space. We compute t-SNE embeddings on the x-vector representations of 20 utterances for 10 speakers (5 male, 5 female), shown in Figure <ref type="figure" target="#fig_2">2</ref>. When using filterbanks, we can observe well-clustered utterances. The clusters break down when training the x-vectors on φ0. For the x-vectors trained on φ0.5 and φ2.0, the clusters start to re-emerge. The silhouette scores for x-vectors extracted from filterbank, φ0, φ0.5 and φ2.0 representations are 0.14, -0.17, -0.05 and -0.09 respectively, are consistent with the observed EER values.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Conclusions and future work</head><p>We proposed to combine CTC and attention losses with a speaker-adversarial loss within an end-to-end framework with the goal of learning privacy-preserving representations for ASR. Such representations could be safely transmitted to cloudservices for decoding. We investigate the level of speaker identity anonymization achieved by adversarial training through closed-set speaker classification and open-set speaker verification measures. Adversarial training appears to dramatically reduce the closed-set classification accuracy, seemingly indicating a high-level of anonymization. However, this observation does not match with the open-set verification results, which correspond to the real scenario of an adversary trying to confirm the identity of a suspected speaker. Hence we conclude that the adversarial training does not immediately generalize to produce anonymous representations in speech. We hypothesize that this disparity might be attributed to the representation capacity of the adversarial branch, the size of the training set, the formulation of the adversarial loss, and/or the value of the trade-off parameter with the ASR loss. As a future work, we plan to modify the speaker adversarial branch to inherently optimize for verification instead of classification and ascertain the impact of these experimental choices over different datasets, including for languages not seen in training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Acknowledgements</head><p>This work was supported in part by the European Union's Horizon 2020 Research and Innovation Program under Grant Agreement No. 825081 COMPRISE (https://project. inria.fr/comprise/) and by the French National Research Agency under project DEEP-PRIVACY (ANR-18-CE23-0018). Experiments were carried out using the Grid'5000 testbed, supported by a scientific interest group hosted by Inria and including CNRS, RENATER and several Universities as well as other organizations. The authors would like to thank Md Sahidullah for providing the speaker verification data split.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Architecture of the proposed model. The speaker-adversarial branch is shown as a red box. The red arrow indicates gradient reversal. When the model is deployed, the encoder could reside at the client side, while the decoder can be hosted by cloud services.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Visualization of x-vector representations of 20 utterances of 10 speakers computed by t-SNE (perplexity equals to 30). Males are represented by circles and females by triangles.</figDesc><graphic coords="5,45.74,67.19,147.39,110.54" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Splits of Librispeech used in our experiments.</figDesc><table><row><cell>dataset</cell><cell>data split</cell><cell># utts</cell><cell>duration (h)</cell></row><row><cell></cell><cell>train-960</cell><cell>281,231</cell><cell>960.98</cell></row><row><cell></cell><cell>test-clean</cell><cell>2,620</cell><cell>5.40</cell></row><row><cell>data-full</cell><cell>dev-clean</cell><cell>2,703</cell><cell>5.39</cell></row><row><cell></cell><cell>test-other</cell><cell>2,939</cell><cell>5.34</cell></row><row><cell></cell><cell>dev-other</cell><cell>2,864</cell><cell>5.12</cell></row><row><cell></cell><cell>train-adv</cell><cell>27,535</cell><cell>97.05</cell></row><row><cell>data-adv</cell><cell>dev-adv</cell><cell>502</cell><cell>1.77</cell></row><row><cell></cell><cell>test-adv</cell><cell>502</cell><cell>1.77</cell></row><row><cell></cell><cell>train-spkv</cell><cell>373,985</cell><cell>1,388.79</cell></row><row><cell>data-spkv</cell><cell>train-plda test-clean-enroll</cell><cell>422,491 438</cell><cell>1,443.96 0.75</cell></row><row><cell></cell><cell>test-clean-trial</cell><cell>1496</cell><cell>3.60</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Detailed description of the trial set (test-clean-trial) for speaker verification experiments. known at training time. It is evaluated over the testadv set using the same classifier architecture as the speakeradversarial branch of the proposed model (see Section 2). As opposed to the ACC, the EER measures how well the representations hide the speaker identity for unknown speakers, in an open-set scenario. It reflects the process of confirming whether a person is actually who the attacker thinks it might be. It evaluated over the trial set (see Table2) using x-vector-PLDA.</figDesc><table><row><cell></cell><cell cols="2">Male Female</cell></row><row><cell># Speakers</cell><cell>13</cell><cell>16</cell></row><row><cell># Genuine trials</cell><cell>449</cell><cell>548</cell></row><row><cell cols="2"># Impostor trials 9,457</cell><cell>11,196</cell></row><row><cell>3.2. Evaluation metrics</cell><cell></cell><cell></cell></row><row><cell cols="3">For all tested systems, we measure ASR performance in terms</cell></row><row><cell cols="3">of the word error rate (WER) and we assess the amount of infor-</cell></row><row><cell cols="3">mation about speaker identity in the encoded speech represen-</cell></row></table><note><p>tation in terms of both speaker classification accuracy (ACC) and speaker verification equal error rate (EER). The WER is reported on the test-clean set. The ACC measures how well speakers can be discriminated in a closed-set setting, i.e., speak-ers are</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>ASR and speaker recognition results with different representations. WER (%) is reported on test-clean set, ACC (%) on test-adv set and EER (%) on test-clean-trial.</figDesc><table><row><cell></cell><cell>Filterbank</cell><cell>φ0</cell><cell>φ0.5</cell><cell>φ2.0</cell></row><row><cell>WER</cell><cell>-</cell><cell>10.9</cell><cell>12.5</cell><cell>12.5</cell></row><row><cell>ACC</cell><cell>93.1</cell><cell>46.3</cell><cell>6.4</cell><cell>2.5</cell></row><row><cell>EER Pooled</cell><cell>5.72</cell><cell cols="3">23.07 21.97 19.56</cell></row><row><cell>EER Male</cell><cell>3.34</cell><cell cols="3">19.38 18.26 16.26</cell></row><row><cell>EER Female</cell><cell>7.48</cell><cell cols="3">26.46 24.45 22.45</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Alexa vs. vs. Cortana vs. Google Assistant: a comparison of speech-based natural user interfaces</title>
		<author>
			<persName><forename type="first">G</forename><surname>López</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Quesada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Guerrero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Applied Human Factors and Ergonomics</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="241" to="250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Next-generation of virtual personal assistants (Microsoft Cortana, Apple Siri, Amazon Alexa and Google Home)</title>
		<author>
			<persName><forename type="first">V</forename><surname>Kepuska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Bohouta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CCWC</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="99" to="103" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">The insecurity of home digital voice assistants -Amazon Alexa as a case study</title>
		<author>
			<persName><forename type="first">X</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G.-H</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Xie</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.03327</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Alexa, can i trust you?</title>
		<author>
			<persName><forename type="first">H</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Iorga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Voas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="100" to="104" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Speaker identification and verification using Gaussian mixture speaker models</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Reynolds</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Speech Communication</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="91" to="108" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Speech intention classification with multimodal deep learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Marsic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Canadian Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="260" to="271" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Prosody conveys speaker&apos;s intentions: Acoustic cues for speech act perception</title>
		<author>
			<persName><forename type="first">N</forename><surname>Hellbernd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Sammler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Memory and Language</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="page" from="70" to="86" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Speech act classification: A study in the lexical analysis of English speech activity verbs</title>
		<author>
			<persName><forename type="first">T</forename><surname>Ballmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Brennstuhl</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
			<publisher>Springer Science &amp; Business Media</publisher>
			<biblScope unit="volume">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Dialog act modeling for conversational speech</title>
		<author>
			<persName><forename type="first">A</forename><surname>Stolcke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shriberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bates</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Coccaro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Meteer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ries</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Van Ess-Dykema</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Spring Symposium on Applying Machine Learning to Discourse Processing</title>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="98" to="105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Robust GMM based gender classification using pitch and RASTA-PLP parameters of speech</title>
		<author>
			<persName><forename type="first">Y.-M</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z.-Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Falk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-Y</forename><surname>Chan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning and Cybernetics</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="3376" to="3379" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Gender classification in two emotional speech databases</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kotti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kotropoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICPR</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Survey on speech emotion recognition: Features, classification schemes, and databases</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">El</forename><surname>Ayadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Kamel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Karray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="572" to="587" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Automatic speech classification to five emotional states based on gender information</title>
		<author>
			<persName><forename type="first">D</forename><surname>Ververidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kotropoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EU-SIPCO</title>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="341" to="344" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Emotion recognition by speech signals</title>
		<author>
			<persName><forename type="first">O.-W</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-W</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EuroSpeech</title>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Feature analysis for automatic detection of pathological speech</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Dibazar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">W</forename><surname>Berger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2nd Joint EMBS-BMES Conference</title>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="182" to="183" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Feature analysis of pathological speech signals using local discriminant bases technique</title>
		<author>
			<persName><forename type="first">K</forename><surname>Umapathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Krishnan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical and Biological Engineering and Computing</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="457" to="464" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">The INTERSPEECH 2013 computational paralinguistics challenge: social signals, conflict, emotion, autism</title>
		<author>
			<persName><forename type="first">B</forename><surname>Schuller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Steidl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Batliner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vinciarelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Scherer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Ringeval</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chetouani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Weninger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Eyben</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Marchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Interspeech</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="148" to="152" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Computational paralinguistics: emotion, affect and personality in speech and language processing</title>
		<author>
			<persName><forename type="first">B</forename><surname>Schuller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Batliner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
			<publisher>John Wiley &amp; Sons</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A survey on perceived speaker traits: Personality, likability, pathology, and the first challenge</title>
		<author>
			<persName><forename type="first">B</forename><surname>Schuller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Steidl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Batliner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Nöth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vinciarelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Burkhardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Van Son</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Weninger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Eyben</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Bocklet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Speech &amp; Language</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="100" to="131" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Cultural and linguistic factors in audiovisual speech processing: The McGurk effect in Chinese subjects</title>
		<author>
			<persName><forename type="first">K</forename><surname>Sekiyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Perception &amp; Psychophysics</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="73" to="80" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Social signal processing: Survey of an emerging domain</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vinciarelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bourlard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1743" to="1759" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Privacy-preserving machine learning for speech processing</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Pathak</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<publisher>Springer Science &amp; Business Media</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Privacy preserving encrypted phonetic search of speech data</title>
		<author>
			<persName><forename type="first">C</forename><surname>Glackin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Chollet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Dugan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Cannings</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tahir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">G</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rajarajan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE ICASSP</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="6414" to="6418" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">X-vectors: Robust DNN embeddings for speaker recognition</title>
		<author>
			<persName><forename type="first">D</forename><surname>Snyder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Garcia-Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE ICASSP</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="5329" to="5333" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Espnet: End-to-end speech processing toolkit</title>
		<author>
			<persName><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Karita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hayashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Nishitoba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Unno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">E Y</forename><surname>Soplin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Heymann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wiesner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Interspeech</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2207" to="2211" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Learning anonymized representations with adversarial neural networks</title>
		<author>
			<persName><forename type="first">C</forename><surname>Feutry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Piantanida</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Duhamel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.09386</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Invariant representations for noisy speech recognition</title>
		<author>
			<persName><forename type="first">D</forename><surname>Serdyuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Audhkhasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Brakel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ramabhadran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.01928</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Speaker invariant feature extraction for zero-resource languages with adversarial learning</title>
		<author>
			<persName><forename type="first">T</forename><surname>Tsuchiya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Tawara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ogawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kobayashi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE ICASSP</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2381" to="2385" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Speaker-invariant training via adversarial learning</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Mazalov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE ICASSP</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="5969" to="5973" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">To reverse the gradient or not: An empirical comparison of adversarial and multi-task learning in speech recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Adi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Zeghidour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Liptchinsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Synnaeve</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.03483</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Librispeech: an ASR corpus based on public domain audio books</title>
		<author>
			<persName><forename type="first">V</forename><surname>Panayotov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE ICASSP</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="5206" to="5210" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Hybrid CTC/attention architecture for end-to-end speech recognition</title>
		<author>
			<persName><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Hershey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hayashi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of Selected Topics in Signal Processing</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1240" to="1253" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Domainadversarial training of neural networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Ustinova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ajakan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Germain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Laviolette</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Marchand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="2096" to="2030" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A study on data augmentation of reverberant speech for robust speech recognition</title>
		<author>
			<persName><forename type="first">T</forename><surname>Ko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Peddinti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Seltzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE ICASSP</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5220" to="5224" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">MUSAN: A music, speech, and noise corpus</title>
		<author>
			<persName><forename type="first">D</forename><surname>Snyder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Povey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1510.08484v1</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">The Kaldi speech recognition toolkit</title>
		<author>
			<persName><forename type="first">D</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ghoshal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Boulianne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Burget</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Glembek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hannemann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Motlicek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Schwarz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Tech. Rep</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Attention-based models for speech recognition</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">K</forename><surname>Chorowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Serdyuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="577" to="585" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Fully convolutional speech recognition</title>
		<author>
			<persName><forename type="first">N</forename><surname>Zeghidour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Liptchinsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.06864</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
