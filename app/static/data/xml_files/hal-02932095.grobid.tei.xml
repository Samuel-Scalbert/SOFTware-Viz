<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="fr">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Hybrid Bi-LSTM-CRF Model for Sequence Labeling Applied to the Sourcing Domain</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Hasnaa</forename><surname>Daoud</surname></persName>
							<email>hasnaa.daoud@silex-france.com</email>
							<affiliation key="aff0">
								<address>
									<settlement>Silex</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Molka</forename><forename type="middle">Tounsi</forename><surname>Dhouib</surname></persName>
							<email>dhouib@i3s.unice.fr</email>
							<affiliation key="aff0">
								<address>
									<settlement>Silex</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">Université Côte d&apos;Azur</orgName>
								<orgName type="institution" key="instit2">Inria</orgName>
								<orgName type="institution" key="instit3">CNRS</orgName>
								<orgName type="institution" key="instit4">I3S</orgName>
								<address>
									<settlement>Sophia Antipolis</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jerôme</forename><surname>Rancati</surname></persName>
							<email>jerome.rancati@silex-france.com</email>
							<affiliation key="aff0">
								<address>
									<settlement>Silex</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Catherine</forename><surname>Faron</surname></persName>
							<email>faron@i3s.unice.fr</email>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">Université Côte d&apos;Azur</orgName>
								<orgName type="institution" key="instit2">Inria</orgName>
								<orgName type="institution" key="instit3">CNRS</orgName>
								<orgName type="institution" key="instit4">I3S</orgName>
								<address>
									<settlement>Sophia Antipolis</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Andrea</forename><forename type="middle">G B</forename><surname>Tettamanzi</surname></persName>
							<email>tettaman@i3s.unice.fr</email>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">Université Côte d&apos;Azur</orgName>
								<orgName type="institution" key="instit2">Inria</orgName>
								<orgName type="institution" key="instit3">CNRS</orgName>
								<orgName type="institution" key="instit4">I3S</orgName>
								<address>
									<settlement>Sophia Antipolis</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A Hybrid Bi-LSTM-CRF Model for Sequence Labeling Applied to the Sourcing Domain</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">1679FC51A902EDB81101E6394EA27E39</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-04-12T14:52+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Traitement du Langage Naturel</term>
					<term>Extraction d&apos;Information</term>
					<term>Etiquetage de Séquences</term>
					<term>Réseaux de neurones artificiels Natural Language Processing</term>
					<term>Information Extraction</term>
					<term>Sequence Labeling</term>
					<term>Artificial Neural Networks</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Dans un certain nombre de domaines, les entreprises sont souvent confrontées à la tâche de traiter au quotidien des quantités importantes de demandes textuelles. L'extraction automatique des informations clés à partir des demandes clients, peut aider à accélérer le processus de traitement. Silex France est aujourd'hui confrontée à ces enjeux dans le cadre du traitement des demandes de sourcings. Dans cet article, nous partageons nos résultats d'étiquetage de séquences en nous basant sur une méthode hybride BiLSTM-CRF, dans un contexte industriel. Le travail est intégré dans la plate-forme B2B Silex pour la recommandation des prestataires de services. Les expériences faites sur les données de la plateforme B2B Silex montrent qu'avec un bon choix de features à extraire et des hyperparamètres, la combinaison du modèle Bi-LSTM-CRF permet de réussir l'extraction d'infomation à partir des demandes textuelles, même dans un contexte de petites données (small data). En effet, le contenu textuel traité est sous forme de phrases complètes générées par des utilisateurs, et est ainsi exposé à des erreurs de frappe. Pour gérer ce type de données, nous combinons plusieurs types de features extraites décrivant le contenu textuel tels que : (i) la sémantique, (ii) la syntaxe, (iii) les caractères des mots, (iv) la position des mots.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="fr">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Natural Language Processing (NLP) is a branch of Artificial Intelligence (AI) that helps computers understand, interpret and manipulate human language. Information Extraction (IE) is a crucial task in the field of NLP and linguistics that is widely used for tasks such as Question Answering, Machine Translation, Entities Extraction, Event Extraction... In this paper, we report on an IE task we conducted in the context of Silex company which develops a SaaS esourcing platform for the identification of the best providers that meet expressed needs. It takes into consideration the requested service, costs, deadlines, innovation, and quality <ref type="bibr" target="#b3">[4]</ref>. There are two main user communities within Silex platform : (i) service providers, and (ii) service buyers. Silex aims to automatically analyze the textual descriptions of service requests and providers in order to better evaluate opportunities in a faster way with more targeted sourcings. We aim to extract key phrases from customers' requests. In the context of sourcing requests, a key phrases usually indicates a product, a service, an occupation (job title) or a skill. These are the main entities in Sourcing domain. These types of information can be considered as contextualized named entities. In fact, it turns out that in some requests, a customer may talk about his own services, but we aim to extract only the services he needs. We propose an IE approach based on a Bi-LSTM-CRF architecture, able to analyze textual descriptions (service providers and service requests) and extract the relevant parts of the text that summarize a provider's offer/ a customer need (such as services, products, occupations, skills). In this paper, we focus on information extraction from service requests. Processing of service requests is more challenging than that of service offers : (i) text requests are generally short (50 words on average in our case) ; (ii) the content of these descriptions is user-generated, and then is subject to typing errors ; (iii) finally, a user may describe his own products or services to contextualize his request, which would create confusion. This raises the issue of distinguishing between the real user's need and the general context of the sourcing request description. Therefore the task is not the extraction of any expression describing a service or a product in a sourcing request. Our main research question is : How to extract relevant information that summarizes a customer's need ? We focus on the following sub-questions :</p><p>-Which is the best approach to extract information from short texts ? -Which types of embeddings must we use to extract relevant information in our case ? -How to deal with the limited number of data ? Our approach is based on the Bi-LSTM-CRF framework <ref type="bibr" target="#b10">[11]</ref> ; a Bidirectional Long-term and Short-term Mermory (Bi-LSTM) encodes an input sequence words and a Conditional Random Fields model (CRF) labels every sequence word. In <ref type="bibr" target="#b10">[11]</ref>, the authors use two types of embeddings : word embeddings and character based embeddings (computed using a Bi-LTSM). The final embeddings, which constitutes inputs of the main Bi-LSTM-CRF are obtained by concatenating word embeddings and character based embeddings. Our contribution lies in the addition of two other types of embeddings computed with two Bi-LSTMs : (i) Syntactic embeddings and (ii) Position embeddings. Also, to answer Silex use case, since we have only 858 annotated service requests, we adapted the architecture hyper-parameters to our context of small data. This paper is organized as follows : Section 2 presents the related works. Section 3 describes our information extraction approach. Section 4 describes our data and our implementation. Section 5 reports and discusses the results of our experiments. Section 6 concludes with an outline of future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Works</head><p>There are three common techniques in the literature for the NER task <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b11">12]</ref> : (i) knowledge-based unsupervised systems, (ii) feature-engineered supervised systems and (iii) feature-inferring neural network systems. Our work is mostly related to feature-inferring neural network systems. <ref type="bibr" target="#b2">[3]</ref> proposes semi-supervised method where a deep neural network model learns internal representations on the basis of vast amounts of mostly unlabeled training data. This model presents two main limitations : (i) it doesn't take into account long-term dependencies between words because it is based on a simple feed-forward neural network and limits the use of context to a fixed-size window ; (ii) by using only word embedding, the model is unable to exploit other features such as letter's cases or complex aspects of user-generated content. These types of considerations could be useful, especially for rare words. To overcome some of the limitations of <ref type="bibr" target="#b2">[3]</ref>'s model, deep learning algorithms such as Recurrent Neural Networks (RNN) are successfully applied for sequence labeling task. Authors of <ref type="bibr" target="#b17">[18]</ref> present a detailed comparison between the Convolution Neural Networks (CNN), and RNN in the particular context of NER. The specificity of RNN is that it allows a neural network to exhibit temporal dynamic behavior to process input sequences with no limitation on the input size. However, RNN are not adapted when input sequences are getting too long ; in a RNN, gradients are back-propagated through all time steps as well. This means that the longer our sequence size is, the more gradients we will be taking the product of. This leads to the vanishing gradient problem. Long-term and Short-term Memory Networks (LSTMs) are a particular type of RNN that are designed to avoid this problem through the use of LSTM cells, which makes it easy to learn about long term dependencies <ref type="bibr" target="#b4">[5]</ref>. <ref type="bibr" target="#b10">[11]</ref> proposes a more powerful neural network model that incorporates a bi-directional LSTM (Bi-LSTM) and CRF. This model is based on robust learning with the dropout, which allows good recognition results of NER. The bidirectional LSTM model takes into account the whole context enables to effectively train a model with the flexible use of long range context <ref type="bibr" target="#b5">[6]</ref>. In addition to the architecture, the way we present input data to a neural network matters. For example, we can see a textual sentence in different ways : (i) sequence of words, (ii) sequence of letters, (iii) sequence of chunks... Enriching the input sequences with accessible additional data can also help to have better results. Character-based representations are very important in our use case. In fact, our data are user-generated. Then, it is important to capture morphological and orthographic patterns. <ref type="bibr" target="#b1">[2]</ref> presents a hybrid bidirectional LSTM and bidirectional CNN neural network architecture that helps to exploit explicit character level features such as prefixes and suffixes, which could be useful especially with rare words for which word embeddings are poorly (or are not) trained. <ref type="bibr" target="#b13">[14]</ref> introduces the neural character embedding in the NER task for English and achieves the state-of-the-art. <ref type="bibr" target="#b0">[1]</ref> explored ways to improve point-of-sale labeling using different types of auxiliary losses, and different representations of words. They built their model based on Bi-LSTM layers and showed that introducing word representations through their characters gives better results in terms of model speed and performance. <ref type="bibr" target="#b8">[9]</ref> proposes a simple yet effective dependencyguided LSTM-CRF model that takes the complete depen-dency trees and captures syntactic properties for the Named Entities Recognition task. Furthermore, <ref type="bibr" target="#b10">[11]</ref> incorporated character-level structure into word representation. Each input vector consists of two parts : (i) pre-trained word-level representation <ref type="bibr" target="#b12">[13]</ref> and (ii) task-related character-level representation. The authors of <ref type="bibr" target="#b10">[11]</ref> adopted a bidirectional LSTM to capture information in both forward and backward directions and concatenate the outputs of these two LSTMs. Most related works cited in this paper use only word embedding, or combine them with character based representations <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b0">1]</ref>, while we enrich input sequences with Part Of Speech tagging and word position information. The way we represent sequences improves the results in our use case.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodology</head><p>Bi-LSTM-CRF model is introduced by Huang and al <ref type="bibr" target="#b7">[8]</ref>. It has been compared to LSTM, Bi-LSTM and LSTM-CRF models. Best results in the paper are achieved with Bi-LTSM-CRF model in different sequence tagging tasks (Part-of-Speech Tagging, Chunking, and NER tasks). In the first part of this section, we present the state of the art Bi-LSTM-CRF model and how we use it in our architecture. In the second part, we detail the features we introduce to enrich word embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Architecture</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Bi-LSTM :</head><p>Bidirectional LSTM or Bi-LSTM model <ref type="bibr" target="#b6">[7]</ref> is composed of two LSTMs each one processes the sequence in a different direction. The main characteristics of LSTM are (i) the ability to operate on sequential data and (ii) the ability to capture long term dependencies thanks to a memory-cell. LSTM takes as input a sequence of vectors X = (x 1 , x 2 , ..., x n ) and returns another sequence of vectors, named hidden states vectors H = (h 1 , h 2 , ..., h n ) as output. Below, we detail mathematical equations in LSTM network we used in this work :</p><formula xml:id="formula_0">i t = σ(W xi x t + b xi + W hi h (t-1) + b hi ) f t = σ(W if x t + b if + W hf h (t-1) + b hf ) C t = tanh(W iC x t + b iC + W hC h (t-1) + b hC ) o t = σ(W io x t + b io + W ho h (t-1) + b ho ) c t = f t x c (t-1) + i t xC t 1 h t = o t x tanh(c t )</formula><p>Where x t is the input at time t, h t is the hidden state at time t, c t is the cell state at time t, h t is the hidden state of the layer at time t-1 or the initial hidden state at time 0, and i t , f t , C t , o t are the input, forget, cell, and output gates, respectively. σ is the sigmoid function. Figure <ref type="figure">1</ref> presents the LSTM architecture, where pink</p><formula xml:id="formula_1">1. x is Hadamard product FIGURE 1 -LSTM Architecture.</formula><p>circles represent arithmetic operators and rectangles represent LSTM gates. The drawback of the LSTM model is that it processes input from left to right, which involves that it can only encode dependencies on previous tokens. That is why a second LSTM is used to process input in the reverse direction (i.e., from right to left). This new layer makes it possible to detect dependencies on the right context of a token. In our model, we use Bi-LTSMs to :</p><p>-Extract character based embeddings, syntactic representations, and position representations. This part will be detailed in section 3.2. -Combine all of these representations to extract complete dependency information. The idea is to concatenate the above representations and pretrained word embeddings to have complete representations of words, then to pass the obtained representations by a Bi-LSTM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Linear Conditional Random Field</head><p>The way sequences are encoded is not the only important part in a sequence labeling problem. The chosen classifier also plays a crucial role. One simple approach is to classify each word independently. The problem with this approach is that it assumes that given the input, all of the entity labels are independent.</p><p>In order to relax this assumption, Generative Models named HMMs (Hidden Markov Models) make two assumptions : (i) Each state depends only on its immediate predecessor. (ii) Each observation variable depends only on the current state. This last statement makes it impossible to add additional knowledge in the model (e.g contextual information). Using conditional Random Fields (CRF) <ref type="bibr" target="#b9">[10]</ref> is a solution to overcome this issue. CRFs are undirected graphical models and are partially similar to HMMs. Nonetheless, they are not Generative but Discriminative Models trained to maximize the conditional probability of observation and state sequences <ref type="bibr" target="#b14">[15]</ref>. The primary advantage of CRFs over HMMs is their conditional nature, resulting in the relaxation of the independence assumptions required by HMMs in order to ensure tractable inference.</p><p>In our Bi-LSTM-CRF model, after extracting a sequence hidden vectors (h 1 , h 2 , h 3 , ..., h n ) with Bi-LSTMs (with n the sequence length), we compute scores associated with each label j at position t in the sequence with a linear layer. We consider P the resulted matrix of size n × k (with k the number of labels) :</p><formula xml:id="formula_2">P t = W hp h t + b hp ∀t ∈ [[1, n]]<label>(1)</label></formula><p>with :</p><p>h t = (h t l , h tr ) hidden vector at position t in the sequence (concatenation of two hidden vectors ; right context hidden vector and left context hidden vector) -W hp weight matrix of dimension (k, m) (with m the hidden vectors size) -b hp bias vector of dimension k As explained earlier, CRF model does not rely only on matrix P since we assume that predicted variables in a sequence depend on each other. Inference in linear CRF models is based on maximizing the following conditional probability :</p><formula xml:id="formula_3">P (y|H) = exp(score(H, y)) y ∈Y exp(score(H, y ))<label>(2)</label></formula><p>with :</p><p>-H the matrix of hidden vectors : </p><formula xml:id="formula_4">H = [h 1 , h 2 ,</formula><p>With A the transition scores matrix (A i,j transition score from state i to state j)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Features</head><p>Our main contribution in this paper is the addition of new kinds of features extracted using Bi-LSTMs. Our model uses four different kinds of embeddings for each sequence word. Three types of these embeddings are trained with Bi-LSTMs, they are concatenated with pretrained FastText word embeddings, and used as the input sequence to our main Bi-LSTM-CRF model. Figure <ref type="figure" target="#fig_1">2</ref> schematizes how the different types of embeddings are trained.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Word embedding</head><p>Word embeddings are vector representations trained on a large corpus of texts such as encyclopedias, news texts, or literature. There are many language modeling and feature learning techniques that help to map words to vectors that allow to represent the contextual proximity of different words. It is possible to train from scratch these vector representations on the particular task of keyword sequence tagging using our data. However, it turns out that the size of our data will not allow us to cover a large vocabulary. Therefore, we chose to use pre-trained word vectors for French, learned using fastText<ref type="foot" target="#foot_0">2</ref> on a Wikipedia dump. The French model we used contains 1.152.449 words mapped to 300-dimensional vectors <ref type="bibr" target="#b12">[13]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Syntactic Word Representations</head><p>Manual analysis of our data shows that syntax plays an important role to locate the user's need in a sequence. Hence the need to extract features representing syntax and dependency between words. This type of information (i.e., syntactic structure and dependencies) is important to complete the semantic information. For example, many sourcings requests contain verbs like (rechercher, souhaiter, chercher ...), in these cases recognizing the sentence object would help the model to recognize the customer's need.</p><p>We then trained part-of-speech (POS) embeddings, using a Bi-LSTM Model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Character-level Word Representations</head><p>Fasttext pretrained Word embedding allows to extract information about the meaning of words, but the covered vocabulary remains limited to the vocabulary of the training corpus. Therefore, rare words or words with spelling errors cannot be represented. Hence the importance of Characterlevel Word Representations since any word is made up of a number of characters and characters set is finite. For every word, we use a BiLSTM that takes as input the sequence of word's characters, and returns the last hidden states vector. We consider this vector as a character level based represen-tation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.4">Position Representation</head><p>This kind of information is important in our use case : based on manual analysis of our data, it turns out that the main subject is mentioned at the beginning of the text. We are convinced that this is generally valid for written text requests. Thus, it would be interesting if the model takes into account the position of words. This way, the model will be able to understand that it is highly likely that the words at the beginning of the text are relevant information. We use a Bi-LSTM model to extract this type of embedding. All these types of representations are concatenated and used as input of the main Bi-LSTM-CRF bloc as shown in Figure <ref type="figure" target="#fig_1">2</ref>.</p><p>4 Experiments -B-: to mark the beginning of an entity -I-: to mark the inside of an entity -E-: to mark the end of an entity -S-: to mark a single entity -O : to mark a token outside all of entities Table <ref type="table" target="#tab_1">1</ref> shows an example annotation using the BIOES format. In the field of NLP, it is difficult to increase text due to the complexity of natural language. Some methods in the literature use shuffling, which involves changing the order of sentences in the text or random deletion of certain words from the text. However, this augmentation techniques may change the whole context of the text. In natural language processing, it is not a good idea to augment data using signal transformation techniques (images <ref type="bibr" target="#b15">[16]</ref>, speech recognition...), because the order of characters is important to keep a strict syntactic and semantic meaning. Other augmentation techniques make more sense in the context of our work, such as injecting punctuation noise or modifying certain characters in words. Otherwise, it is difficult to add more semantics, the best way to do so is to use human sentence rephrasing, but it is an expensive operation. Therefore, the most natural choice in data augmentation for us is to replace words with their synonyms based on a dictionary of synonyms of the most frequent words in the field of sourcing.</p><p>We therefore chose to augment our training dataset by : -introducing punctuation noise -changing characters of some words (which simulates spelling errors in user-generated data) -replacing some words with their synonyms This way, we multiplied our training set size by three.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Implementation</head><p>To implement our model, we used the Pytorch library, which supports GPU computing. For all Bi-LSTMs in our model, we used the torch.nn.LSTM class, which allows to create LSTMs and to set parameters such as the number of layers, bidirectionality, input feature size, hidden state vector size. We implemented the CRF based on equations of section 3.1.2. We use the Viterbi algorithm for finding the most likely sequence of hidden states.  For the final Bi-LSTM bloc (see Figure <ref type="figure" target="#fig_1">2</ref>) used to combine all the features of Table <ref type="table" target="#tab_2">2</ref> with Fasttext word embeddings, we chose a hidden layer of size 400.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Hyper-parameters</head><p>In our experiments, we started by initializing the semantic word embedding layer with FastText pre-trained embeddings, and we updated them during training. We found that this causes overfitting because of a high number of parameters. We then chose to freeze the semantic word embedding layer, which helped us to improve the results.</p><p>In addition to increasing the data, to avoid over-fitting, we chose a high dropout value of 0.5.</p><p>In the literature, the number of Bi-LSTMs hidden layers is usually equal to the same number of embedding units. We compared the results with different units' numbers in the main Bi-LSTM and we were able to get better results when the size of the hidden layers is equal to 400.</p><p>In this paper, we conducted four experiments to compare the performance of four kinds of models :</p><p>- </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Result and Discussion</head><p>The problem we deal with is different from ordinary NER tasks, since we detect text segments summarizing a user's need. Let us note that even expert annotators find it sometimes hard to decide on the segment to annotate. Thus, we chose to evaluate the four models in two different ways : (i) Precision and Recall, (ii) Cosine similarity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Precision and Recall</head><p>We started with an evaluation based on precision, recall, and the F1 score as the basis for choosing the best model.</p><p>In our evaluation, we do not consider the complete annotated entity, but rather words of the entity separately. For example, in the sentence "I am looking for a plumber able to repair a faucet Sprinkle", we do not fully penalize the algorithm if it does not detect the complete repair a faucet Sprinkle segment. But we count words that it could detect in that segment. Indeed, if we suppose that the model detects only repair a faucet, this may be enough to understand the need's subject. We also ignore conjunctions, determinants and punctuation in the evaluation.      <ref type="table" target="#tab_4">3</ref>). We also note that with this model, we were able to get a similar F1 scores in dev and test data (difference of 0.39%). The switch from CPU to GPU computing allowed us to gain in terms of performance (more than 1 difference for the F 1 score, which is due to the difference between implementations of the libraries used on   We point out that the difference between dev and test scores is due to the small amount of annotated data. The disadvantage of this evaluation method in the particular case of our work is that it gives equal importance to all extracted words. However, it turns out that some words are more important than others, especially words for which meaning appears several times in a service request extracted segments. In the sentence "I am looking for a plumber able to repair a faucet Sprinkle", "plumber" and "faucet" are two words that generally appear in the same context and could be considered as the most important extracted words. "Sprinkle" ; the faucet brand, is the least important word. However, with precision and recall evaluation, all words are given the same weight, and the model will be penalized in the same way if it does not detect the word Sprinkle or the word plumber.</p><p>We mentioned earlier that the manual annotation was not obvious to the experts. Here is a second possible scenario : in the previous example, an annotator decides to annotate only "plumber" as a unique key phrase, and the model annotates only "repair a faucet", even if there is an important semantic similarity between the two segments, the model will be penalized in terms of Precision and Recall scores. Hence, to deal with this issue, we propose a meaning-based evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Evaluation using cosine similarity distribution</head><p>To address all of the above problems, we tried to present the results otherwise, which corresponds to the purpose of this work. Indeed, the goal is to be able to reduce a service request description into a set of keywords. So we thought of calculating the cosine similarity between embedding of expert annotated segments and embedding of segments detected by the algorithm for each service request, and then plot the histogram of the results. A service request extracted segments' embedding is computed by averaging on FastText pretrained word embeddings of their words. We ignore stop words and punctuation. In Figure <ref type="figure">5</ref>, we present four histograms each associated to a different model.</p><p>Note that each type of features added to the model helps to move the distribution a little bit to the right. We can clearly see the difference between the distribution of model I and the distribution of model IV : the distribution of model IV is tightest around 1, with a highest peak.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we proposed a method that relies on Bi-LSTM-CRF for sequence labeling to summarize sourcing requests. We combined several types of features to represent every word in a sequence. The key of success of our method is an original combination of input features.</p><p>In addition to word embedding, we extract three other kinds of embeddings : (i) character-level based embeddings, (ii) syntax based embeddings and (iii) position embeddings. These additional embeddings are extracted using Bi-LSTMs and are concatenated with word embedding. We have shown that syntax and position of words help to improve the quality of the information extraction in our use case. We also shared hyper-parameters that give us the best training and choices we made to overcome overfitting problems. Moreover, we have shown that Bi-LSTM-CRF architectures for information extraction can provide value even in a small data context. This work was integrated into Silex sourcing platform to recommend similar service requests, which considerably reduces the processing time in 60% of cases. This recommendation is based on semantic similarity between requests based on their extracted segments.</p><p>As future work we aim to experiment new extraction approaches based on transformers like BERT or Camembert for French texts. We also aim to evaluate the generality of our approach designed for the sourcing domain by experimenting it in a general benchmark.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>h 3 , ..., h n ] -P (y|H) the conditional probability of a sequence of tags y . -Y the set of possible tag sequences. score function is defined as follows :</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>FIGURE 2 -</head><label>2</label><figDesc>FIGURE 2 -Model architecture (Embeddings Extraction + Main BiLSTM-CRF)</figDesc><graphic coords="5,311.81,74.24,255.12,262.60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>I : Bi-LSTM model with only word embedding and logistic regression classification model -II : Bi-LSTM model with only word embedding and CRF classification model -III : Bi-LSTM-CRF model with word embedding, character-based representations, and Bi-LSTM position based representation. -IV : Bi-LSTM-CRF model with word embedding, character-based representations, Bi-LSTM position-based representation and syntactic word representations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 and</head><label>3</label><figDesc>Figure 3 and Figure 4 respectively show the evolution of</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>FIGURE 3 -FIGURE 4 -</head><label>34</label><figDesc>FIGURE 3 -Evolution of score F 1Dev</figDesc><graphic coords="7,311.81,291.90,255.13,170.09" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>Figure 4 shows that from epoch 30, the loss function continues to decrease without improving the F1 score. From this epoch, the model starts to overfit on training data. Using syntactic information with POS tagging significantly improves Dev and Test recall, and balances well precision and recall (see Table</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>CPU and GPU) and in terms of training time (from 350 seconds per epoch to 75 seconds per epoch for model IV) . With data augmentation (</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE 1 -</head><label>1</label><figDesc>Example of input training dataArtificial neural networks generally require a large corpus of training data in order to learn efficiently the task. To overcome this limitation of the training set size (594 service requests), we augmented the training set. We gene-rated new training examples by applying some transformations of the available ones. These transformations must preserve the entities' labels.</figDesc><table><row><cell>Word</cell><cell>POS</cell><cell>Label</cell></row><row><cell>Je</cell><cell>PRON</cell><cell>O</cell></row><row><cell>recherche</cell><cell>VERB</cell><cell>O</cell></row><row><cell>un</cell><cell>DET</cell><cell>O</cell></row><row><cell>plombier</cell><cell cols="2">NOUN S-</cell></row><row><cell>La</cell><cell>DET</cell><cell>O</cell></row><row><cell>société</cell><cell cols="2">NOUN O</cell></row><row><cell>BNB</cell><cell cols="2">NOUN O</cell></row><row><cell>souhaite</cell><cell>VERB</cell><cell>O</cell></row><row><cell>créer</cell><cell>VERB</cell><cell>O</cell></row><row><cell>des</cell><cell>DET</cell><cell>O</cell></row><row><cell>supports</cell><cell cols="2">NOUN B-</cell></row><row><cell>de</cell><cell>ADP</cell><cell>I-</cell></row><row><cell cols="3">communication NOUN E-</cell></row><row><cell>.</cell><cell>.</cell><cell>.</cell></row><row><cell>.</cell><cell>.</cell><cell>.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2</head><label>2</label><figDesc>shows the hyper-parameters we use in Bi-LSTM layers used for character-based, position and syntactic features extraction.</figDesc><table><row><cell>Features</cell><cell cols="2">Embedding Hidden vectors</cell></row><row><cell></cell><cell>dimensions</cell><cell>dimensions</cell></row><row><cell>Syntactic features</cell><cell>25</cell><cell>30</cell></row><row><cell>Character-level based features</cell><cell>25</cell><cell>50</cell></row><row><cell>Position features</cell><cell>25</cell><cell>30</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE 2 -</head><label>2</label><figDesc>Hyper-parameters for the features extraction layers</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3</head><label>3</label><figDesc>presents the results obtained in terms of precision, recall and F 1 score.</figDesc><table><row><cell></cell><cell cols="2">Recall</cell><cell cols="2">Precision</cell><cell>F1</cell></row><row><cell cols="2">Model Dev</cell><cell>Test</cell><cell>Dev</cell><cell>Test</cell><cell>Dev</cell><cell>Test</cell></row><row><cell>I</cell><cell cols="6">58.88 61.31 77.02 76.61 66.74 68.11</cell></row><row><cell>II</cell><cell cols="6">64.03 66.61 75.21 76.66 69.17 71.29</cell></row><row><cell>III</cell><cell cols="6">63.06 66.61 75.62 80.11 68.77 72.74</cell></row><row><cell>IV</cell><cell cols="6">67.57 70.20 76.03 73.77 71.55 71.94</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE 3 -</head><label>3</label><figDesc>Precision, Recall and F1-score without data augmentation</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4</head><label>4</label><figDesc></figDesc><table><row><cell></cell><cell cols="2">Recall</cell><cell cols="2">Precision</cell><cell>F1</cell></row><row><cell cols="2">Model Dev</cell><cell>Test</cell><cell>Dev</cell><cell>Test</cell><cell>Dev</cell><cell>Test</cell></row><row><cell>I</cell><cell cols="6">60.94 61.93 77.31 73.79 68.15 67.35</cell></row><row><cell>II</cell><cell cols="6">65.64 67.71 75.50 70.23 70.22 68.94</cell></row><row><cell>III</cell><cell cols="6">63.64 68.49 75.28 72.68 70.13 70.52</cell></row><row><cell>IV</cell><cell cols="6">67.05 68.02 75.34 77.58 70.96 72.49</cell></row></table><note><p>), we have mainly improved the F 1 Dev scores of the different models. The recall</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE 4 -</head><label>4</label><figDesc>Results</figDesc><table><row><cell>with data augmentation</cell></row><row><cell>associated with test dataset was relatively improved for the</cell></row><row><cell>first three models, but the precision decreased relatively,</cell></row><row><cell>except for model IV.</cell></row><row><cell>We can deduce that the best model in terms of Test and</cell></row><row><cell>Dev scores is model IV that uses a Bi-LSTM-CRF ap-</cell></row><row><cell>plied on the concatenation of semantic embedding, POS</cell></row><row><cell>embedding, Character based embedding, and Position em-</cell></row><row><cell>bedding.</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>https ://fasttext.cc/docs/en/pretrained-vectors.html</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">Daniil</forename><surname>Anastasyev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Gusev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eugene</forename><surname>Indenbom</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.00818</idno>
		<title level="m">Improving part-of-speech tagging via multitask learning and character-level word representations</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Named entity recognition with bidirectional lstm-cnns</title>
		<author>
			<persName><forename type="first">Jason Pc</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Nichols</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="357" to="370" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Natural language processing (almost) from scratch</title>
		<author>
			<persName><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Karlen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pavel</forename><surname>Kuksa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2493" to="2537" />
			<date type="published" when="2011-08">Aug. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Le sourcing</title>
		<author>
			<persName><forename type="first">Eschenlauer</forename><surname>Rémi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013-06">June 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Learning to forget : Continual prediction with lstm</title>
		<author>
			<persName><forename type="first">Jürgen</forename><surname>Felix A Gers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fred</forename><surname>Schmidhuber</surname></persName>
		</author>
		<author>
			<persName><surname>Cummins</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Speech recognition with deep recurrent neural networks</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abdel-Rahman</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2013 IEEE international conference (a) Model I (b) Model II (c) Model III (d) Model IV FIGURE 5 -Distribution of cosine similarity between segments automatically detected and segments annotated by human experts on acoustics, speech and signal processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="6645" to="6649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Framewise phoneme classification with bidirectional lstm and other neural network architectures</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural networks</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">5-6</biblScope>
			<biblScope unit="page" from="602" to="610" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Bidirectional lstm-crf models for sequence tagging</title>
		<author>
			<persName><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.01991</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Dependency-guided lstmcrf for named entity recognition</title>
		<author>
			<persName><forename type="first">Zhanming</forename><surname>Jie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Lu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.10148</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Conditional random fields : Probabilistic models for segmenting and labeling sequence data</title>
		<author>
			<persName><forename type="first">John</forename><surname>Lafferty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fernando Cn</forename><surname>Pereira</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Neural architectures for named entity recognition</title>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandeep</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kazuya</forename><surname>Kawakami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.01360</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">A survey on deep learning for named entity recognition</title>
		<author>
			<persName><forename type="first">Jing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aixin</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianglei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenliang</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.09449</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">Cicero</forename><surname>Nogueira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dos</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Guimaraes</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1505.05008</idno>
		<title level="m">Boosting named entity recognition with neural character</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">An introduction to conditional random fields</title>
		<author>
			<persName><forename type="first">Charles</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foundations and Trends R in Machine Learning</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="267" to="373" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Improving deep learning using generic data augmentation</title>
		<author>
			<persName><forename type="first">Luke</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoff</forename><surname>Nitschke</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.06020</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">A survey on recent advances in named entity recognition from deep learning models</title>
		<author>
			<persName><forename type="first">Vikas</forename><surname>Yadav</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Bethard</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.11470</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">Zenan</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dat</forename><surname>Quoc Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karin</forename><surname>Verspoor</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.08450</idno>
		<title level="m">Comparing cnn and lstm character-level embeddings in bilstm-crf models for chemical and disease named entity recognition</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
