<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="fr">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Benchmarking and challenges in security and privacy for voice biometrics</title>
				<funder>
					<orgName type="full">COMPRISE)</orgName>
				</funder>
				<funder ref="#_f6rF3cF">
					<orgName type="full">Japan Science and Technology Agency</orgName>
					<orgName type="abbreviated">JST</orgName>
				</funder>
				<funder ref="#_UU58R3D">
					<orgName type="full">Region Grand Est, France</orgName>
				</funder>
				<funder ref="#_C84xXTt">
					<orgName type="full">Academy of Finland</orgName>
				</funder>
				<funder ref="#_vKPwHRH">
					<orgName type="full">European Commission</orgName>
				</funder>
				<funder>
					<orgName type="full">French ANR (VoicePrivacy, VoicePersonae</orgName>
				</funder>
				<funder ref="#_XdHb7pz">
					<orgName type="full">JSPS</orgName>
				</funder>
				<funder>
					<orgName type="full">DEEP-PRIVACY, Harpocrates)</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Jean-Francois</forename><surname>Bonastre</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Hector</forename><surname>Delgado</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Nicholas</forename><surname>Evans</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Tomi</forename><surname>Kinnunen</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Aik</forename><surname>Kong</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Xuechen</forename><surname>Lee</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Andreas</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Paul-Gauthier</forename><surname>Nautsch</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Jose</forename><surname>Noe</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Md</forename><surname>Patino</surname></persName>
						</author>
						<author>
							<persName><surname>Sahidullah</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Hector</forename><surname>Delgado</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Héctor</forename><surname>Delgado</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Jose</forename><surname>Noé</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Brij</forename><surname>Sahidullah</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Lal</forename><surname>Mohan</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Massimiliano</forename><surname>Srivastava</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Natalia</forename><surname>Todisco</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Emmanuel</forename><surname>Tomashenko</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Xin</forename><surname>Vincent</surname></persName>
						</author>
						<author>
							<persName><surname>Wang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Yamagishi</forename><surname>Junichi</surname></persName>
						</author>
						<author>
							<persName><surname>Asvspoof</surname></persName>
						</author>
						<title level="a" type="main">Benchmarking and challenges in security and privacy for voice biometrics</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">72AE110C98CB04CA0A1784EC9ABD205D</idno>
					<idno type="DOI">10.21437/SPSC.2021-11</idno>
					<note type="submission">Submitted on 10 Apr 2024</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-04-12T14:46+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>niveau recherche, publiés ou non, émanant des établissements d'enseignement et de recherche français ou étrangers, des laboratoires publics ou privés.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="fr">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The voice is among the most natural and convenient means to human-machine interaction and biometric authentication. In some scenarios, particularly telephony or teleconferencing applications, voice can be the only available biometric. While automatic speaker verification (ASV) systems can provide a reliable means to authentication, like all biometric technologies, it is not without security and privacy concerns. Arguably, these concerns are potentially greater for voice biometrics than they can be for authentication systems that use alternative biometric characteristics.</p><p>Security concerns relate to the potential for ASV systems to be manipulated by adversaries through spoofing attacks <ref type="bibr" target="#b0">[1]</ref>, now referred to as presentation attacks <ref type="bibr" target="#b1">[2]</ref>. Fraudsters can launch spoofing attacks to gain illegitimate access to protected services or resources by presenting to the ASV system a speech recording which has been manipulated to sound<ref type="foot" target="#foot_0">1</ref> like another speaker. Without adequate protection, spoofing attacks can substantially degrade the reliability of almost any ASV system.</p><p>While not related specifically to voice biometrics, but to speech technology more generally, privacy concerns relate to the potential for speech data to be exploited for purposes other than those to which an individual might have given consent <ref type="bibr" target="#b2">[3]</ref>. Speech signals are a rich source of personal, private information. In providing recordings of speech to a particular voice service, the speaker usually furnishes the service provider with much more information than is strictly necessary in order to perform the expected task, hence the need for privacy preservation.</p><p>In this article we describe two specific benchmarking challenges launched by the speech processing research community to expedite solutions to security and privacy concerns. The first involves solutions to protect ASV systems from being manipulated by spoofing in the form of spoofing countermeasures or presentation attack detection systems <ref type="bibr" target="#b1">[2]</ref> whose development is spearheaded through the ASVspoof initiative launched in 2015 <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6]</ref>. The second relates to the VoicePrivacy initiative, launched in 2020 <ref type="bibr" target="#b6">[7]</ref>, which aims to promote the development of privacy preservation solutions for speech technology. The inaugural VoicePrivacy challenge focused upon anonymisation, namely techniques to manipulate speech data in order that it cannot be used with ASV systems to recognise the speaker.</p><p>The article is intended as a contribution to the efforts to build bridges between the speech community and, e.g., the legal and ethical communities and, in particular, to support the recently formed Security and Privacy in Speech Communication (SPSC) special interest group of the International Speech Communication Association (ISCA). It targets the non-specialist and is hence intentionally high-level, with a focus upon the essentials and clarity, rather than upon scientific and technical rigour. We describe the importance of benchmarking campaigns, classifier fundamentals and the early, classical approaches to performance estimation. After presenting a high-level overview of ASV systems, the remainder of the article introduces the ASVspoof and VoicePrivacy initiatives.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Evaluation-driven research</head><p>In the early days, researchers collected their own datasets to develop methods (algorithms and software). To make technological progress in complex tasks such as speech or speaker recognition, and to be able to meaningfully compare different methods, the need for commensurable performance benchmarking was quickly recognised. From the mid-1990s, the National Institute of Standards and Technology (NIST) -a US-based standardisation body -pioneered evaluation-driven research <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9]</ref>. The key ingredients are: (1) commonly agreed (often public) data, evaluation rules, and performance metrics; (2) disentangled roles for researchers (evaluees) and evaluators. Performance claims should not be reported by evaluees but instead by independent evaluators who provide common infrastructure (data, rules, metrics) in the form of an evaluation campaign or challenge. Only with such a level playing field can competing methods can be meaningfully compared. There are many such  challenges <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11]</ref> within the speech field. Typically, they require participants <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13]</ref> to design special-purpose software to solve some specific tasks. For many, the software takes the form of a binary classifier, namely a system which must choose between two mutually exclusive hypotheses.</p><p>Even if classifiers are based on well-established statistical principles and are trained objectively via numerical optimisation techniques, they can (and do) make errors. Errors are the result of over-simplified modelling assumptions, natural variability in the input data, sources of external nuisance variation, or as a result of limited training data, etc. The design of evermore reliable classifiers is the traditional bread and butter of machine learning and speech research.</p><p>A binary classifier makes two types of errors: misses (false rejections) and false alarms (false acceptances). Misses imply that the classifier rejects a positive input that should be accepted. A false alarm results from the classifier accepting a negative input that should be rejected. Classifier performance can be estimated by dividing the number of misses and false alarms by the number of tested positive and negative cases respectively. The resulting miss rate (Pmiss) and false alarm rate (PF A) can be seen as proxies for user convenience and security.</p><p>Binary classifier decisions are derived in two stages. The first involves computation of a 'soft decision' (the score)a real number that expresses the classifier's confidence in the positive case. Example raw score distributions for such a binary classifier are illustrated in Fig. <ref type="figure" target="#fig_0">1</ref>(a) were the positive case is the target class and the negative case is the impostor class. In practice, the score is often a logarithmic likelihood ratio (LLR) and expresses the relative strength between the two competing hypotheses. Second, the score is compared with a pre-set threshold δ. Scores above the threshold imply 'accept', whereas scores below the threshold imply 'reject'. By increasing the threshold, the false alarm rate (PF A, red shaded area in Fig. <ref type="figure" target="#fig_0">1</ref>) can be reduced at the expense of a higher miss rate (Pmiss, green shaded area) and vice versa. The trade-off can be readily visualised in so-called detection error trade-off (DET) plots <ref type="bibr" target="#b13">[14]</ref> such as that illustrated in Fig. <ref type="figure" target="#fig_2">2</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(described below).</head><p>Since there are two error rates, which do we report? Or do we report both? What threshold / how do we set it? The answer to these questions is rather subtle and a detailed treatment is outside the scope of this paper. One particular metric, namely the equal error rate (EER) is adopted for a broad range of tasks. It is computed using the threshold that makes miss and false alarm rates equal (see Fig. <ref type="figure" target="#fig_2">2</ref>) -thereby yielding a single number to report. The lower the EER, the more reliable the classifier.</p><p>The authors acknowledge that, even if the EER is deprecated in ISO standards <ref type="bibr" target="#b14">[15]</ref>, it provides a compact summary of the discrimination capabilities of a classifier -how well it is capable of observing (in speech, 'hearing') differences between positive and negative inputs. In practice, however, the EER does not provide a full picture. More comprehensive approaches to assessment have been developed and have been broadly adopted by the community. These provide a view of classifier performance through the lens of a formal decision policy which represents the effective trade-off between the two decision outcomes <ref type="bibr" target="#b15">[16]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Automatic speaker verification</head><p>ASV systems provide one of the most natural and convenient means to biometric person authentication. Test recordings (probes) are compared with enrolment recordings (references) to verify (or not) a claimed identity. The reference is used to create a model 2 which is stored in a reference database. At test time, the model corresponding to the claimed identity is compared to the test utterance resulting in a soft score. A hard accept/reject decision can be fully automated (e.g., online banking) or semi-automated (with some human intervention, e.g., when forensic practitioners present voice evidence in court). Scores should reflect reliably the extent to which the strengthof-evidence supports the same/different identity propositions: the higher the score, the greater the similarity and vice versa.</p><p>Detection error trade-off plots for two different ASV systems assessed using the ASVspoof 2019 logical access database and the VoicePrivacy 2020 database are illustrated in Figs. <ref type="figure" target="#fig_2">2</ref> and<ref type="figure">4</ref> and show EERs of 2.5% and 1.1% respectively (blue profiles). Each point on any one profile corresponds to a different decision threshold (operating point). The profiles show how misses can be traded off against false alarms in order to meet different application requirements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Security vulnerabilities: ASVspoof</head><p>Without adequate protections, the reliability of ASV systems can be compromised by the presentation of synthetic or converted voice, replayed speech and impersonation <ref type="bibr" target="#b5">[6]</ref>. Generated automatically from a text input, today's state-of-the-art synthesis systems are capable of producing speech that the human cannot distinguish from bona fide speech <ref type="bibr" target="#b16">[17]</ref>. Voice conversion systems operate directly upon an input speech signal and alter the voice to that of another speaker <ref type="bibr" target="#b17">[18]</ref>. Unlike synthetic and converted voice spoofing attacks, which both demand a certain technical expertise and suitable training and adaptation data, 2 On account of their dynamic nature and the variability in speech signals, we refer to models, not templates.  replay attacks can be launched by the layman, requiring only consumer-grade recording and replaying devices. All can substantially degrade ASV reliability. Impersonation, while still a threat <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20]</ref>, is less effective.</p><p>The impact of spoofing attacks upon an ASV system is illustrated in Fig. <ref type="figure" target="#fig_0">1(b)</ref>. Spoofing attacks introduce a third class of input so that the ASV system must now cope with target (matching), impostor (non-matching) and spoofed utterances. A successful spoofing attack circumvents the ASV system by provoking a score above the decision threshold. The score distribution for spoofing attacks is illustrated in the middle of Fig. <ref type="figure" target="#fig_0">1(b)</ref>. Spoofing attacks provoke a false alarm rate P spoof that is greater than the original false alarm rate PF A. One can think of spoofing attacks as a special case of impostors where there is a concerted effort to deceive the ASV system.</p><p>Without the capacity to distinguish between spoofed and bona fide speech, ASV reliability will degrade as a result of spoofing attacks, sometimes substantially. This degradation assessed using the ASVspoof 2019 logical access database (synthetic and converted voice spoofing attacks) is illustrated in Fig. <ref type="figure" target="#fig_2">2</ref>. The baseline EER (with no spoofing attacks) of 2.5% increases to over 50% when impostor trials are replaced by the most effective synthetic and converted voice spoofing attacks. These results should be interpreted with caution, however. In practice, one must consider the relative likelihood of the ASV system being presented with target and impostor trials, versus that of spoofing attacks. Without this consideration, the results in Fig. <ref type="figure" target="#fig_2">2</ref> might convey an overly pessimistic view of the vulnerabilities to spoofing. There is, in any case, potential to detect attacks automatically using countermeasures.</p><p>The series of ASVspoof challenges held bi-annually since 2015 have spearheaded the development of spoofing countermeasures or presentation attack detection (PAD) solutions for ASV. Spoofing countermeasures can be applied prior to ASV in order to detect attacks and prevent them from reaching the ASV system. The most recently completed challenge was held in 2019 and included separate logical access (synthetic and converted voice attacks) and physical access (replay attacks) Miss probability (in %)</p><p>Team 05 (EER=0.22 %) Team 45 (EER=1.86 %) Team 60 (EER=2.64 %) Team 24 (EER=3.45 %) Team 50 (EER=3.56 %)</p><p>Figure <ref type="figure">3</ref>: As for Fig. <ref type="figure" target="#fig_2">2</ref> except for spoofing countermeasures.</p><p>Profiles shown for the top five performing systems.</p><p>tasks <ref type="bibr" target="#b5">[6]</ref>. DET plots for the top-five performing spoofing countermeasures for the ASVspoof 2019 logical access task are shown in Fig. <ref type="figure">3</ref>. They show EERs of as low as 0.2%. Thus, while spoofing attacks can present a substantial threat to reliability, and while human listeners may not be able to detect spoofing attacks, countermeasures can be effective. Even so, while ASV countermeasures have proven potential to detect attacks, they can also degrade usability; they can erroneously classify bona fide speech as spoofed speech. Assessment and performance estimates should hence reflect the impact of both spoofing and countermeasures upon the ASV system; countermeasures should be assessed in tandem with ASV. Such more elaborate approaches to assessment, e.g. <ref type="bibr" target="#b20">[21]</ref>, are outside the scope of the current article.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Privacy implications: VoicePrivacy</head><p>Speech signals contain much more than just the spoken message or the voice identity <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b22">23]</ref>. The speaker's sex/gender, age, socio-economic and geographical background, emotion and health condition etc. can all be estimated automatically using recordings of speech <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b24">25]</ref>. With much of this information being personal and private there is hence an interest to develop privacy safeguards for speech technology. This is the goal of the VoicePrivacy initiative, founded in 2020. While solutions to privacy preservation can take many different forms, and while VoicePrivacy may explore different approaches in the future, the inaugural challenge focused upon the development of anonymization solutions <ref type="bibr" target="#b6">[7]</ref>.</p><p>The idea is to protect privacy by distorting speech data such that it cannot be used by ASV systems to recognise the speaker. Anonymisation is achieved by suppressing the information in speech signals that is typically used by machines to infer identity. On its own, this is relatively straightforward, e.g. by adding sufficient levels of background noise, or by replacing speech with silence. The challenge comes from the requirement to suppress personally identifiable attributes contained within the speech signal while leaving all other attributes intact. These requirements imply that anonymisation should not interfere with the application of some down stream tasks such as automatic speech recognition, nor should it introduce processing artefacts that might degrade subjective intelligibility or naturalness. Last, anonymised voices should remain distinctive, meaning the task ASV baseline (EER=1.11% ASV trained on anonymised data, anonymised test (EER=10.69%) Anonymised test (EER=52.12%</p><p>Figure <ref type="figure">4</ref>: Detection error trade-off plot for the VoicePrivacy 2020 challenge (LibriSpeech test, male trials). Profiles shown for the baseline ASV system (blue profile) and the same system presented with anonymised test utterances (orange profile) and anonymised test utterances when the ASV system is re-trained using similarly anonymised training data (red profile).</p><p>might better be referred to as pseudonymisation. Rather than operating upon the speech signal so that it reflects the voice of another, specific speaker, anonymisation aims to prevent the speaker identity from being recognised. Anonymisation acts to increase the confusion between utterances produced by the same and different speakers. For a perfect anonymisation system, the score distributions corresponding to impostor and target trials overlap. The desired effect of anonymisation upon an ASV system is illustrated in Fig. <ref type="figure" target="#fig_0">1(c</ref>). In this case, the ASV system cannot produce simultaneously both a low false alarm rate and a low miss rate, no matter what the decision threshold, and the EER is 50%. Real anonymisation solutions are less effective.</p><p>The first VoicePrivacy challenge was held in 2020 and attracted submissions from 7 independent teams. A DET plot for the primary challenge baseline <ref type="bibr" target="#b6">[7]</ref> is illustrated in Fig. <ref type="figure">4</ref>. It shows an increase in the EER from a baseline of 1% (blue profile) to 52% after anonymisation (orange profile). The EER of over 50% suggests that the anonymisation goal is met. However, if an anonymisation adversary were to adapt the ASV system in light of anonymisation, then performance is less effective; anonymised utterances still contain some personally identifiable information (PII) and the potential to re-identify the speaker remains. When the ASV system is re-trained or adapted using similarly anonymised training data, the result is a lower EER of 10.7% (red profile); true anonymisation remains elusive.</p><p>The above treatment does not reflect impacts upon intelligibility/naturalness, nor voice distinctiveness. In practice, the multiple objectives and complex nature of anonymisation means several different metrics are used in practice. The development of more suitable approaches to assessment are the focus of current research <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b28">29]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Reflections and further considerations</head><p>The community has made substantial progress to address the security and privacy implications of speech technology and rapid progress has been made in the last half-decade. We have: pro-posed definitions for a number of distinct tasks and solutions; established evaluation driven research initiatives as a vehicle to successfully raise the profile of security and privacy research and to build new research communities; developed protocols, criteria and metrics for the assessment of security and privacy safeguards. Nonetheless, we are perhaps still far from having a comprehensive appreciation of the implications as well as the potential of safeguards.</p><p>Will spoofing ever be a solved problem? Possibly not. Speech synthesis and voice conversion are long-established research fields with genuine applications, such as anonymisation, yet the same technology poses a threat to the security of ASV. The progress speech synthesis and voice conversion in recent years has been impressive. Today's technology produces synthetic speech that humans cannot distinguish from bona fide speech. Whether or not similar advances may one day result in machines that produce synthetic speech that other machines cannot detect is an intriguing question. For the time being it is clear that, if we seek reliable, secure approaches to person authentication using ASV, we must intensify our efforts in antispoofing. Future directions include a focus on more adversarial attacks, the development of countermeasures that function reliably in the wild, e.g. in the face of background noise and other sources of nuisance variation such as bandwidth and channel variability which typify telephony ASV scenarios, as well as the approach used to estimate performance.</p><p>The research effort in anonymisation is relatively embryonic. While there is an opportunity to provide some level of protection, current solutions fall short of delivering true anonymisation. Furthermore, we have observed differences in the level of privacy that a given anonymisation solution provides to different individuals. Whereas the protection for some can be strong, others are left with relatively little protection at all. Since personally identifiable information encapsulates far more than that used by most ASV systems to infer identity, since other alternative attributes can be used instead, and since current anonymisation solutions do not necessarily suppress them, is full anonymisation even technically possible?</p><p>Anonymisation solutions that focus only upon the attributes used by typical ASV systems already degrade intelligibility and naturalness. If all personally identifiable information can be successfully suppressed, then what remains? Is personally identifiable information so inextricably, indelibly embedded in speech that it cannot be (fully) removed? Even if a speech signal can be fully anonymised, will anything resembling speech remain? Or, for a given application, what level of intelligibility/naturalness can be sacrificed in order to achieve anonymisation? How should we even measure anonymisation performance and privacy? Are the methodology and metrics adequate? How does this research fit with broader solutions to privacy preservation, e.g. encryption and distributed learning?</p><p>That we have certainly raised more questions above than we have answered serves to show that our community's journey in security and privacy research is only just beginning. Our efforts must be redoubled looking to the future.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Illustrative score distributions for (a) automatic speaker verification (ASV), (b) an ASV system subjected to spoofing attacks and (c) an ASV system presented with anonymised data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>(EER=2.46 %) worst VC spoofing attack (EER=65.25 %) worst TTS spoofing attack (EER=66.42 %) EER=2.46%</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Detection error trade-off plot for the ASVspoof 2019 logical access task. Profiles shown for the baseline ASV system (blue profile) and the same system subjected to the most effective synthetic (text-to-speech, TTS) speech spoofing attack (orange profile) and the most effective voice conversion (VC) spoofing attack (red profile).</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>We refer to machine perception rather than human perception; humans and machines do not hear in the same way.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="7.">Acknowledgements</head><p>The work reported in this paper was supported by: the <rs type="funder">French ANR (VoicePrivacy, VoicePersonae</rs>, <rs type="funder">DEEP-PRIVACY, Harpocrates)</rs>; the <rs type="funder">European Commission</rs> (<rs type="grantNumber">H2020</rs> <rs type="funder">COMPRISE)</rs>; the <rs type="funder">Japan Science and Technology Agency (JST)</rs> with grant No. <rs type="grantNumber">JPMJCR18A6</rs>; <rs type="funder">JSPS</rs> <rs type="grantNumber">KAKENHI</rs> No.<rs type="grantNumber">21K17775</rs>; the <rs type="funder">Academy of Finland</rs> (proj. <rs type="grantNumber">309629</rs>); <rs type="funder">Region Grand Est, France</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_vKPwHRH">
					<idno type="grant-number">H2020</idno>
				</org>
				<org type="funding" xml:id="_f6rF3cF">
					<idno type="grant-number">JPMJCR18A6</idno>
				</org>
				<org type="funding" xml:id="_XdHb7pz">
					<idno type="grant-number">KAKENHI</idno>
				</org>
				<org type="funding" xml:id="_C84xXTt">
					<idno type="grant-number">21K17775</idno>
				</org>
				<org type="funding" xml:id="_UU58R3D">
					<idno type="grant-number">309629</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Introduction to voice presentation attack detection and recent advances</title>
		<author>
			<persName><forename type="first">M</forename><surname>Sahidullah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Delgado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Todisco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kinnunen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yamagishi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-A</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Handbook of Biometric Anti-Spoofing</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="321" to="361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<idno>ISO/IEC 30107</idno>
		<title level="m">Information Technology -Biometric presentation attack detection</title>
		<meeting><address><addrLine>Geneva, Switzerland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note>Standard, International Organization for Standardization</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The GDPR &amp; Speech Data: Reflections of Legal and Technology Communities, First Steps Towards a Common Understanding</title>
		<author>
			<persName><forename type="first">A</forename><surname>Nautsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Jasserand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Kindt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Todisco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Trancoso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Evans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech 2019</title>
		<meeting>Interspeech 2019</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3695" to="3699" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">ASVspoof 2015: the first automatic speaker verification spoofing and countermeasures challenge</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kinnunen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yamagishi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hanilc ¸i</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sahidullah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sizov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2037" to="2041" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The ASVspoof 2017 Challenge: Assessing the Limits of Replay Spoofing Attack Detection</title>
		<author>
			<persName><forename type="first">T</forename><surname>Kinnunen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sahidullah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Delgado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Todisco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yamagishi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">A</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">ASVspoof 2019: future horizons in spoofed and fake audio detection</title>
		<author>
			<persName><forename type="first">M</forename><surname>Todisco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vestman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sahidullah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Delgado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Nautsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yamagishi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">H</forename><surname>Kinnunen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">A</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1008" to="1012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Introducing the VoicePrivacy Initiative</title>
		<author>
			<persName><forename type="first">N</forename><surname>Tomashenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">M L</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Nautsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yamagishi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Patino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-F</forename><surname>Bonastre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-G</forename><surname>Noé</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Todisco</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2020-10">oct 2020</date>
			<biblScope unit="page" from="1693" to="1697" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Two decades into speaker recognition evaluation -are we there yet?</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">A</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Sadjadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Reynolds</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Speech &amp; Language</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="page">101058</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Two decades of speaker recognition evaluation at the National Institute of Standards and Technology</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">S</forename><surname>Greenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">P</forename><surname>Mason</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Speech &amp; Language</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="page">101032</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Voxceleb: Large-scale speaker verification in the wild</title>
		<author>
			<persName><forename type="first">A</forename><surname>Nagrani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Speech &amp; Language</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="page">101027</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">SdSV Challenge 2020: Large-Scale Evaluation of Short-Duration Speaker Verification</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zeinali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">A</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Alam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Burget</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech 2020</title>
		<meeting>Interspeech 2020</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="731" to="735" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">I4U submission to NIST SRE 2018: Leveraging from a decade of shared experiences</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">A</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Hautamaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kinnunen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yamamoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1497" to="1501" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">State-of-the-art speaker recognition for telephone and video speech: the JHU-MIT submission for NIST SRE18</title>
		<author>
			<persName><forename type="first">J</forename><surname>Villalba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Snyder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Garcia-Romero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1488" to="1492" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">The DET Curve in Assessment of Detection Task Performance</title>
		<author>
			<persName><forename type="first">A</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Doddington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kamm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ordowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Przybocki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eurospeech</title>
		<meeting>Eurospeech</meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="1895" to="1898" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<idno>ISO/IEC IS 19795-1</idno>
		<title level="m">Information Technology -Biometric performance testing and reporting -Part 1: Principles and framework</title>
		<meeting><address><addrLine>Geneva, Switzerland</addrLine></address></meeting>
		<imprint>
			<publisher>Standard, International Organization for Standardization</publisher>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Nautsch</surname></persName>
		</author>
		<title level="m">Speaker Recognition in Unconstrained Environments</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
		<respStmt>
			<orgName>Technische Universität Darmstadt</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Natural TTS synthesis by conditioning WaveNet on Mel spectrogram predictions</title>
		<author>
			<persName><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Weiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4779" to="4783" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Voice Conversion Challenge 2020 -Intra-lingual semi-parallel and cross-lingual voice conversion</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yamagishi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Joint Workshop for the Blizzard Challenge and Voice Conversion Challenge</title>
		<meeting>Joint Workshop for the Blizzard Challenge and Voice Conversion Challenge</meeting>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
			<biblScope unit="page" from="80" to="98" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">I-vectors meet imitators: on vulnerability of speaker verification systems against voice mimicry</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">G</forename><surname>Hautamäki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kinnunen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Hautamäki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Leino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A.-M</forename><surname>Laukkanen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="930" to="934" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Spoofing and countermeasures for speaker verification: A survey</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kinnunen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yamagishi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Alegre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Speech Communication</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="page" from="130" to="153" />
			<date type="published" when="2015-02">feb 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Tandem Assessment of Spoofing Countermeasures and Automatic Speaker Verification: Fundamentals</title>
		<author>
			<persName><forename type="first">T</forename><surname>Kinnunen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Delgado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">A</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="2195" to="2210" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Preserving privacy in speaker and speech characterisation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Nautsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jiménez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Treiber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kolberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Speech &amp; Language</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="page" from="441" to="480" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<author>
			<persName><surname>Comprise</surname></persName>
		</author>
		<ptr target="https://www.compriseh2020.eu/files/2019/06/d5.1.pdf" />
		<title level="m">Deliverable Nº5.1: Data protection and GDPR requirements</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Voice signatures</title>
		<author>
			<persName><forename type="first">I</forename><surname>Shafran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mohri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ASRU</title>
		<meeting>ASRU</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="31" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Speaker characteristics</title>
		<author>
			<persName><forename type="first">T</forename><surname>Schultz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Speaker classification I</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="47" to="74" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Privacy-Preserving Adversarial Representation Learning in ASR: Reality or Illusion?</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">M L</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bellet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tommasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Vincent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Interspeech 2019</title>
		<imprint>
			<date type="published" when="2019-09">sep 2019</date>
			<biblScope unit="page" from="3700" to="3704" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">The Privacy ZEBRA: Zero Evidence Biometric Recognition Assessment</title>
		<author>
			<persName><forename type="first">A</forename><surname>Nautsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Patino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Tomashenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yamagishi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-G</forename><surname>Noé</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-F</forename><surname>Bonastre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Todisco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Evans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1698" to="1702" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A Comparative Study of Speech Anonymization Metrics</title>
		<author>
			<persName><forename type="first">M</forename><surname>Maouche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">M L</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Vauquier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bellet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tommasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Vincent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1708" to="1712" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Speech Pseudonymisation Assessment Using Voice Similarity Matrices</title>
		<author>
			<persName><forename type="first">P.-G</forename><surname>Noé</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-F</forename><surname>Bonastre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Matrouf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Tomashenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Nautsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Evans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech 2020</title>
		<meeting>Interspeech 2020</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1718" to="1722" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
