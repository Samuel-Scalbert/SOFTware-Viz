<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Overview of BirdCLEF 2022: Endangered bird species recognition in soundscape recordings</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Stefan</forename><surname>Kahl</surname></persName>
							<email>stefan.kahl@cornell.edu</email>
							<affiliation key="aff0">
								<orgName type="laboratory" key="lab1">K. Lisa Yang Center for Conservation Bioacoustics</orgName>
								<orgName type="laboratory" key="lab2">Cornell Lab of Ornithology</orgName>
								<orgName type="institution">Cornell University</orgName>
								<address>
									<settlement>Ithaca</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Amanda</forename><surname>Navine</surname></persName>
							<email>navine@hawaii.edu</email>
							<affiliation key="aff1">
								<orgName type="department">Listening Observatory for Hawaiian Ecosystems</orgName>
								<orgName type="institution">University of Hawai&apos;i at Hilo</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tom</forename><surname>Denton</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Google LLC</orgName>
								<address>
									<settlement>San Francisco</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Holger</forename><surname>Klinck</surname></persName>
							<email>holger.klinck@cornell.edu</email>
							<affiliation key="aff0">
								<orgName type="laboratory" key="lab1">K. Lisa Yang Center for Conservation Bioacoustics</orgName>
								<orgName type="laboratory" key="lab2">Cornell Lab of Ornithology</orgName>
								<orgName type="institution">Cornell University</orgName>
								<address>
									<settlement>Ithaca</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Patrick</forename><surname>Hart</surname></persName>
							<email>pjhart@hawaii.edu</email>
							<affiliation key="aff1">
								<orgName type="department">Listening Observatory for Hawaiian Ecosystems</orgName>
								<orgName type="institution">University of Hawai&apos;i at Hilo</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hervé</forename><surname>Glotin</surname></persName>
							<email>herve.glotin@univ-tln.fr</email>
							<affiliation key="aff3">
								<orgName type="department">AMU</orgName>
								<orgName type="institution" key="instit1">University of Toulon</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
								<orgName type="institution" key="instit3">LIS</orgName>
								<address>
									<settlement>Marseille</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hervé</forename><surname>Goëau</surname></persName>
							<email>herve.goeau@cirad.fr</email>
							<affiliation key="aff4">
								<orgName type="laboratory">CIRAD</orgName>
								<orgName type="institution">UMR AMAP</orgName>
								<address>
									<settlement>Montpellier</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Willem-Pier</forename><surname>Vellinga</surname></persName>
							<affiliation key="aff5">
								<orgName type="institution">Xeno-canto Foundation</orgName>
								<address>
									<settlement>Groningen</settlement>
									<country key="NL">Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Robert</forename><surname>Planqué</surname></persName>
							<affiliation key="aff5">
								<orgName type="institution">Xeno-canto Foundation</orgName>
								<address>
									<settlement>Groningen</settlement>
									<country key="NL">Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Alexis</forename><surname>Joly</surname></persName>
							<email>alexis.joly@inria.fr</email>
							<affiliation key="aff6">
								<orgName type="laboratory" key="lab1">Inria</orgName>
								<orgName type="laboratory" key="lab2">LIRMM</orgName>
								<orgName type="institution" key="instit1">University of Montpellier</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
								<address>
									<settlement>Montpellier</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Overview of BirdCLEF 2022: Endangered bird species recognition in soundscape recordings</title>
					</analytic>
					<monogr>
						<idno type="ISSN">1613-0073</idno>
					</monogr>
					<idno type="MD5">4DFF72D328401F20EFFEB21CB6203C88</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-04-12T14:44+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>LifeCLEF</term>
					<term>bird</term>
					<term>song</term>
					<term>call</term>
					<term>species</term>
					<term>retrieval</term>
					<term>audio</term>
					<term>collection</term>
					<term>identification</term>
					<term>fine-grained classification</term>
					<term>evaluation</term>
					<term>benchmark</term>
					<term>bioacoustics</term>
					<term>passive acoustic monitoring</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>As the "extinction capital of the world", Hawai'i has lost 68% of its native bird species, the consequences of which can harm entire ecosystems. With physical monitoring difficult, scientists have turned to sound recordings, as this approach could provide a passive, low labor, and cost-effective strategy for monitoring endangered bird populations. Current methods for processing large bioacoustic datasets involve manual review of each recording. This requires specialized training and prohibitively large amounts of time. Recent advances in machine learning have made it possible to automatically identify bird songs for common species with ample training data. However, it remains challenging to develop such tools for rare and endangered species. The main goal of the 2022 edition of BirdCLEF was to advance automated detection of rare and endangered bird species that lack large amounts of training data. The competition challenged participants to develop reliable analysis frameworks to detect and identify the vocalizations of rare bird species in continuous Hawaiian soundscapes utilizing limited training data.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Passive acoustic monitoring (PAM), using autonomous sound recorders to monitor animals and their habitats at ecologically relevant scales, has become a critical survey tool in conservation <ref type="bibr" target="#b0">[1]</ref>. Inexpensive commercial off-the-shelf sound recorders are readily available to the community, making the data collection straightforward. Arrays of sound recorders are often deployed for extended periods (weeks to months) and collect vast amounts of data, providing valuable information on the abundance and distribution of vocalizing animals with high spatio-temporal resolution <ref type="bibr" target="#b1">[2]</ref>. However, several challenges with PAM remain. It is not uncommon that data collection efforts result in tens of Terabytes of acoustics data that need to be efficiently handled, stored, and analyzed <ref type="bibr" target="#b2">[3]</ref>. Especially the analysis, reliably extracting signals of interest in often complex soundscapes, is an active area of research. In addition, while common species are typically well represented in available training datasets, data for rare, listed, or endangered species is often sparse, requiring the development of new and innovative algorithmic approaches to monitor these species in need.</p><p>The Hawaiian Islands were once home to an estimated 152 species of terrestrial bird species, but today, habitat loss, habitat degradation by introduced plants and ungulates, and introduced diseases and predators have left only 45 endemic species remaining, 33 of which are listed as endangered <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6]</ref>. Despite their plight, island birds often receive less conservation attention and funding than continental species <ref type="bibr" target="#b3">[4]</ref>. Further, many of the endangered birds of Hawai'i only persist at low population densities in remote, high-elevation, and/or densely forested habitats, which are difficult to access and monitor. In light of these obstacles, Hawaiian birds are prime examples of species that could benefit greatly from PAM techniques that can reduce labor demands and cost. However, these species remain challenging to incorporate into automated detection frameworks because they are severely underrepresented in online acoustic data repositories such as Xeno-canto <ref type="foot" target="#foot_0">1</ref> .</p><p>The BirdCLEF 2022 competition challenged participants to develop reliable analysis frameworks to detect and identify the vocalizations of rare bird species in continuous Hawaiian soundscapes utilizing limited training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">BirdCLEF 2022 Competition Overview</head><p>In recent years, research in the domain of bioacoustics shifted towards deep neural networks for sound event recognition <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8]</ref>. In past editions, we have seen many attempts to utilize convolutional neural network (CNN) classifiers to identify bird calls based on visual representations of these sounds (i.e., spectrograms) <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11]</ref>. Despite their success for bird sound recognition in focal recordings, the classification performance of CNNs on continuous and omnidirectional soundscape recordings remained low. Passive acoustic monitoring can be a valuable sampling tool for habitat assessments and observations of environmental niches, which often are threatened. However, manual processing of large collections of soundscape data is not desirable, and automated attempts can help to advance this process <ref type="bibr" target="#b11">[12]</ref>. Yet, the lack of suitable validation and test data prevented the development of reliable techniques to solve this task.</p><p>Bridging the acoustic gap between high-quality training recordings and complex soundscapes with varying ambient noise levels is one of the most challenging tasks in the domain of audio event recognition. This is especially true when the amount of training data is insufficient, as is the case for many rare and endangered bird species around the globe. Despite the vast amounts of data collected on Xeno-canto and other online sound libraries, audio data for endangered birds is still sparse. However, those endangered species are most relevant for conservation, rendering acoustic monitoring of endangered birds particularly difficult.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Goal and Evaluation Protocol</head><p>The main goal of the 2022 edition of BirdCLEF was to advance automated detection of rare and endangered bird species that lack large amounts of training data. The competition was hosted on Kaggle<ref type="foot" target="#foot_1">2</ref> to attract machine learning experts from around the world to participate in the challenge. The overall task design was consistent with previous editions, but the focus was shifted towards species with very few training samples. Participants were asked to detect and identify 21 target bird species within the provided soundscape test set. Each soundscape was divided into segments of 5 seconds, and a list of audible species within each segment had to be reported by the participants.</p><p>The competition's evaluation metric was a weighted variant of the macro-averaged F1-score, which weighted all target classes equally, thus emphasizing rare acoustic events. In earlier editions, ranking metrics were used to assess the overall classification performance. However, when applying bird call identification systems to real-world data, confidence thresholds have to be set in order to provide meaningful results. The F1-score, a balanced metric between recall and precision, appears to better reflect this circumstance. For each 5-second segment, a binary call indication for all 21 scored species had to be reported. Participants had to apply a threshold to determine if a species is vocalizing during a given segment (True) or not (False).</p><p>To obtain a weighted F1 score, we compute a separate weight for each classification outputi.e., a separate weight for every possible label for every 5-second segment. These weights are chosen such that a) the total weight of positive and negative samples for each species is equal, and b) the total weight for each species is equal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Dataset</head><p>Current methods for processing large bioacoustic datasets involve manual annotation of each recording. This requires specialized training and prohibitively large amounts of time. Thankfully, recent advances in machine learning have made it possible to automatically identify bird songs for common species with ample training data. However, it remains challenging to develop such tools for rare and endangered species, such as those in Hawai'i (Figure <ref type="figure" target="#fig_0">1</ref>). Deploying a bird sound recognition system to a new recording and observation site requires classifiers that generalize well across different acoustic domains. Focal recordings of bird species form an excellent base to develop such a detection system. However, the lack of annotated soundscape data for a new deployment site poses a significant challenge. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1.">Training Data</head><p>As in previous editions, training data was provided by the Xeno-canto community and consisted of more than 14,800 recordings covering 152 species. Participants were allowed to use metadata to develop their systems. Most notably, we provided detailed location information on recording sites of focal and soundscape recordings, allowing participants to account for spatio-temporal occurrence patterns of bird species.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2.">Test Data</head><p>In this edition of the BirdCLEF competition, the test data consisted of 5,356 one-minute soundscapes (amounting to approximately 90 hours of recordings). This dataset was hidden and only accessible to participants during the inference process. These soundscapes were collected for various research projects by the Listening Observatory for Hawaiian Ecosystems (LOHE) at the University of Hawai'i at Hilo at 7 sites across the islands of Hawai'i, Maui, and Kaua'i. Because these data were not collected specifically to be used in the BirdCLEF 2022 contest, different acoustic recording equipment (Song Meter SM2 or SM4 <ref type="foot" target="#foot_2">3</ref> ) and recording settings (sampling rate, mono versus stereo microphones, audio gain, etc.) were used to collect the data. The recordings also varied in duration, time of year, and time of day. All soundscapes received some level of manual bird vocalization annotation by trained members of the LOHE lab using Raven Pro 1.5 software <ref type="foot" target="#foot_3">4</ref> , however some recordings had a select few target species annotated, while others were annotated for every detectable species (Figure <ref type="figure" target="#fig_1">2</ref>). In light of these annotation strategies, only the subset of species for which every vocalization was annotated were scored for any given file, which resulted in a total of 21 scored bird species, 15 species endemic to the Hawaiian Islands and 6 introduced species. This allowed us to include a much larger test dataset in the contest than would have otherwise been possible and exactly replicates real-world use cases where often whatever data is easily accessible is used to answer biological questions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Results</head><p>A total of 1,019 participants (801 teams) from 62 countries participated in the BirdCLEF 2022 competition and submitted 23,352 runs. In Figure <ref type="figure" target="#fig_2">3</ref> we report the performance achieved by the top 50 runs. The private leaderboard score is the primary metric and was revealed to participants after the submission deadline to avoid probing the hidden test data. Public leaderboard scores were visible to participants throughout the challenge.</p><p>The baseline weighted F1-score in this year's edition was 0.5112 (public 0.4849) with all scored birds marked as silent (False) for all segments, and 665 teams managed to score above this threshold. The best submission achieved a weighted F1-score of 0.8527 (public 0.9128) and the top 10 best performing submissions only differed by 7% in score. Most approaches were based on convolutional neural network ensembles and mainly differed in pre-and post-processing strategies and neural network backbone architectures. Interestingly, few-shot learning techniques were vastly underrepresented despite the fact that some target species only had a handful of training samples. Participants employed various sophisticated post-processing schemes, most notably a percentile-based thresholding approach established during the 2021 edition <ref type="bibr" target="#b12">[13]</ref>. Some participants experimented with different loss functions, focal loss being the most notable.Some teams used audio transformers, but the results were inconsistent and led to discussions about whether these methods were appropriate for the task of bird call identification.</p><p>In addition to code repositories and online write-ups, eight teams also submitted full working notes, which are summarized below:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conde &amp; Choi [14]:</head><p>This team explored a number of CNN backbones and evaluated their performance for this task, finding that EfficientNet architectures work best (i.e., EfficientNet-B0). The overall training process was adapted from previous attempts, predominantly last year's second-ranked solution by Henkel et al. <ref type="bibr" target="#b12">[13]</ref>. In addition, the authors experimented with various post-processing steps to improve results. In particular, a penalization which reduces output probabilities proportional to amount of training data was introduced. On top of that, quantile-based class-wise thresholds, tuned by grid search and leaderboard probing, were employed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sampathkumar &amp; Kowerko [15]: Data augmentation is an important processing step in bird sound recognition because of the domain shift between training and test recordings.</head><p>In their work, this team focused on evaluating the best augmentation scheme for this task. Most transformations focus on adding different patterns of noise to the source recording, thus emulating noisy soundscape recordings. While the authors find that all augmentations methods improve the baseline experiment, Gaussian noise, loudness normalization and tanh distortion appear to be most impactful. This team also evaluated different CNN backbones, settling on a EfficientNet-B0 for their final submission.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Martynov &amp; Uematsu [16]:</head><p>This team also employed a sophisticated augmentation scheme to enhance the overall performance. Methods like mix-up, cut-mix and spec augment were chosen. Backbone architecture and training process are largely modeled after last year's submission by Henkel et al. <ref type="bibr" target="#b12">[13]</ref> and adds species based thresholding as post-processing step. In order to overcome limitations of weakly-labeled training data, this team also decided to hand-label some source recordings. The final submission was an ensemble consisting of various CNN trained for different groups of birds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Krishnan et al. [17]:</head><p>Despite the dominance of CNN backbones for bird call detection, this team decided to exploit the sequential nature of audio data to train a prediction model based on Long Short-Term Memory (LSTM) units. Combined with a taxonomic sequence prediction task, this attempt consistently outperformed baseline setups based on multi-layer perceptrons. The authors conclude that the "expressive power of the hidden cells combined with the hierarchical set up for the task" can significantly improve performance. This might also apply, when added on top of other models (e.g., feeding logit outputs into LSTM units).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Miyaguchi et al. [18]:</head><p>Unsupervised representation learning can be key when facing a training dataset with vast class imbalances and underrepresented classes. In this year's competition, some bird species only had a mere two training recordings, forcing participants to develop novel strategies to cope with these limitations. This team decided to explore unsupervised training approaches in combination with species classifiers. Motif mining <ref type="bibr" target="#b18">[19]</ref> was used to extract features from training recordings, embeddings were generated by training a Tile2Vec model <ref type="bibr" target="#b19">[20]</ref> with triplet loss and multi-layer perceptron as well as decision trees were used as classifiers. The authors note that the overall performance on downstream prediction tasks is still not competitive, but propose changes to the training scheme that need to be explored further.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Per-Species Analysis</head><p>There was a wide variation in available Xeno-Canto training data for the various species. In Figure <ref type="figure" target="#fig_4">4</ref> a+b, we show the relationship between quantity of training data and unweighted model performance amongst the top competitors. Notice that quantity of training data does co-vary with model quality, but there are also significant per-species effects on model quality.</p><p>The competition metric was a weighted F1, chosen to equalize the importance of the 21 target species, and then within each species to equalize the importance of positive and negative labels. Because negative labels are far more common in the test data than positive labels, this weighting strongly emphasizes recall over precision.</p><p>A recent study of Hawaiian honeycreepers found 'cultural convergence' of birdsong in the face of population decline <ref type="bibr" target="#b20">[21]</ref>. In this study, three species of honeycreeper were found to have decreased intra-species song variation over a 40-year period, but also decreased inter-species variation amongst the three species studied. This has implications for the acoustic identification task. While decreased intra-species variation should make identification easier, decreased inter-species variation will increase the difficulty of separating similar species.</p><p>For downstream users of high-recall models, we would expect conflation of easily confused species. Define the co-firing rate of two species as the probability that a binary classifier fires for both species when either species is present. For the top 5 submissions, the average co-firing  rate across all pairs of species was 9.3% (Figure <ref type="figure" target="#fig_4">4 c</ref>). The species 'I'iwi and 'Apapane are difficult even for humans to distinguish, and the co-firing rate for this pair was the highest amongst all species pairs at 68%. The 95th percentiles for the co-firing rate was 31.6%. The co-firing rates above this threshold all occurred for a cluster of nine native honeycreepers and thrushes.</p><p>The preference for high recall also led to over-firing when none of the target species were present. In the test data, about one third of segments contained at least one target species. However, the top five submissions had, on average, at least one label for 94% of segments. This suggests that these species classifiers should be paired with an upstream binary bird detector.</p><p>Using these high-recall species classifiers for the identification of Hawaiian species could help reduce the amount of data requiring attention, but only when trying to isolate a specific species. This would ultimately still require significant expert labor to separate vocalizations by species, given the high co-firing rates between species with similar vocalizations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Conclusions and Lessons Learned</head><p>Despite being set up as a few-shot learning task, few teams decided to employ techniques other than CNNs. Pre-trained neural networks for image recognition still dominated the task, and participants tried to cope with the lack of training data through intensive data augmentation and transfer learning. Surprisingly, there was only a weak correlation between the number of training samples and overall per-species performance. This indicates that other factors -such as repertoire size and call patterns -might outweigh training data quantity. Automatic detection of endangered and rare species remains challenging. Still, this year's competition demonstrated that passive acoustic monitoring combined with machine learning could already be a powerful monitoring tool for some endangered species. BirdCLEF continues to engage a large number of data scientists from around the world to develop new and effective acoustic analysis solutions that aid avian conservation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Four of the threatened (a, d) and endangered (b, c) Hawai'i endemic species featured in BirdCLEF 2022. Photo credit: Ann Tanimoto-Johnson.</figDesc><graphic coords="5,89.29,256.22,204.18,136.17" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Expert ornithologists provided bounding box labels for all soundscape recordings indicating calling of 21 target species.In this example, all 'I'iwi calls were annotated, while vocalizations of other species were not labeled. This labeling scheme was applied to all test data soundscapes.</figDesc><graphic coords="6,89.29,84.19,416.68,113.53" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Scores achieved by the best systems evaluated within the primary bird identification task of LifeCLEF 2022. Public and private test data were split randomly, private scores remained hidden until the submission deadline. Participants were able to optimize the recognition performance of their systems based on public scores, which likely explains some differences in scores.</figDesc><graphic coords="7,89.29,84.19,416.69,180.01" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>(a) Per-species unweighted F1 for the top five submissions (b) Per-species precison, recall and weighted F1 (c) Co-firing rates</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Per-species unweighted F1 for the top five submissions (a). Each species is listed by its eBird code, and the number of Xeno-Canto training recordings is given in parentheses. (b) Per-species precision, recall, and unweighted F1, averaged across the top five submissions. (c) Co-firing rates for all species pairs. The matrix of co-firing rates is on the left, and the density histogram of co-firing rates is on the right.</figDesc><graphic coords="9,89.29,317.16,170.85,126.01" type="bitmap" /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>https://xeno-canto.org</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>https://www.kaggle.com/c/birdclef-2022</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>Wildlife Acoustics Inc., Maynard, Massachusetts</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"><p>The Cornell Lab of Ornithology, Ithaca, New York</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>In particular, we want to thank the members of the <rs type="institution">Listening Observatory for Hawaiian Ecosystems (LOHE)</rs>, led by <rs type="person">Patrick Hart</rs>, who collected and annotated the soundscapes used in the test dataset. Special thanks to LOHE lab manager <rs type="person">Ann Tanimoto-Johnson</rs> for gathering, organizing, annotating, and proof checking the soundscape data. Ann also provided photographs of the native birds shown on the competition website and in this paper.</p><p>We would also like to thank <rs type="institution">Kaggle</rs> for helping us host this competition. We are especially grateful for the incredible support and efforts of <rs type="person">Addison Howard</rs> and <rs type="person">Sohier Dane</rs>, who helped process the dataset and set up the competition website. Kaggle and Google jointly sponsored the prize money for this competition. Thanks to everyone who participated in this contest and shared their code base and write-ups with the <rs type="institution">Kaggle</rs> community.</p><p>All results, code notebooks and forum posts are publicly available at: https://www.kaggle.com/c/birdclef-2022</p></div>
			</div>			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Terrestrial passive acoustic monitoring: review and perspectives</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">S M</forename><surname>Sugai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S F</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Ribeiro</surname><genName>Jr</genName></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Llusia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BioScience</title>
		<imprint>
			<biblScope unit="volume">69</biblScope>
			<biblScope unit="page" from="15" to="25" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A roadmap for survey designs in terrestrial acoustic monitoring</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">S M</forename><surname>Sugai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Desjonqueres</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S F</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Llusia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Remote Sensing in Ecology and Conservation</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="220" to="235" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Perspectives in machine learning for wildlife conservation</title>
		<author>
			<persName><forename type="first">D</forename><surname>Tuia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kellenberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Beery</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">R</forename><surname>Costelloe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zuffi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Risse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mathis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">W</forename><surname>Mathis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Van Langevelde</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Burghardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature communications</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="1" to="15" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">K</forename><surname>Pratt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">T</forename><surname>Atkinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">C</forename><surname>Banko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">L</forename><surname>Woodworth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Jacobi</surname></persName>
		</author>
		<title level="m">Conservation Biology of Hawaiian Forest Birds: Implications for Island Avifauna</title>
		<imprint>
			<publisher>Yale University Press</publisher>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Extinct Birds of Hawaii</title>
		<author>
			<persName><forename type="first">M</forename><surname>Walther</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>Mutual Publishing, LLC</publisher>
		</imprint>
	</monogr>
	<note>first edition ed</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Research and management priorities for hawaiian forest birds</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">H</forename><surname>Paxton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Laut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Vetter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Kendall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Condor</title>
		<imprint>
			<biblScope unit="volume">120</biblScope>
			<biblScope unit="page" from="557" to="565" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">BirdNET: A deep learning solution for avian diversity monitoring</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kahl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Wood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Eibl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Klinck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ecological Informatics</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="page">101236</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep neural networks for automated detection of marine mammal species</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Shiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Palmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Roch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Fleishman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E.-M</forename><surname>Nosal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Helble</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cholewiak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Gillespie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Klinck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific reports</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Bird identification from timestamped, geotagged audio recordings</title>
		<author>
			<persName><forename type="first">J</forename><surname>Schlüter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CLEF: Conference and Labs of the Evaluation Forum</title>
		<meeting><address><addrLine>Avignon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-09">Sep. 2018. 2018</date>
		</imprint>
	</monogr>
	<note>CLEF working notes 2018</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Bird species identification in soundscapes</title>
		<author>
			<persName><forename type="first">M</forename><surname>Lasseck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CLEF: Conference and Labs of the Evaluation Forum</title>
		<meeting><address><addrLine>Lugano, Switzerland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-09">Sep. 2019. 2019</date>
		</imprint>
	</monogr>
	<note>CLEF working notes 2019</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Bird species recognition via neural architecture search</title>
		<author>
			<persName><forename type="first">M</forename><surname>Mühling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Franz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Korfhage</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Freisleben</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CLEF: Conference and Labs of the Evaluation Forum</title>
		<meeting><address><addrLine>Thessaloniki, Greece</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-09">Sep. 2020. 2020</date>
		</imprint>
	</monogr>
	<note>CLEF Working Notes 2020</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Survey coverage, recording duration and community composition affect observed species richness in passive acoustic surveys</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Wood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kahl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Chaon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">Z</forename><surname>Peery</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Klinck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Methods in Ecology and Evolution</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="885" to="896" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Recognizing bird species in diverse soundscapes under weak supervision</title>
		<author>
			<persName><forename type="first">C</forename><surname>Henkel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Pfeiffer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CLEF: Conference and Labs of the Evaluation Forum</title>
		<meeting><address><addrLine>Bucharest, Romania</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021-09">Sep. 2021. 2021</date>
		</imprint>
	</monogr>
	<note>CLEF Working Notes 2021</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Few-shot Long-Tailed Bird Audio Recognition</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">V</forename><surname>Conde</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U.-J</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CLEF: Conference and Labs of the Evaluation Forum</title>
		<meeting><address><addrLine>Bologna, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022-09">Sep. 2022. 2022</date>
		</imprint>
	</monogr>
	<note>CLEF Working Notes 2022</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Strategies in identifying bird sounds in a complex acoustic environment</title>
		<author>
			<persName><forename type="first">A</forename><surname>Sampathkumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kowerko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CLEF: Conference and Labs of the Evaluation Forum</title>
		<meeting><address><addrLine>Bologna, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022-09">2022. Sep. 2022. 2022</date>
		</imprint>
	</monogr>
	<note>CLEF Working Notes 2022</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Dealing with Class Imbalance in Bird Sound Classification</title>
		<author>
			<persName><forename type="first">E</forename><surname>Martynov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Uematsu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CLEF: Conference and Labs of the Evaluation Forum</title>
		<meeting><address><addrLine>Bologna, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022-09">Sep. 2022. 2022</date>
		</imprint>
	</monogr>
	<note>CLEF Working Notes 2022</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Bird Species Classification: One Step at a Time</title>
		<author>
			<persName><forename type="first">S</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Khandelwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Garg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CLEF: Conference and Labs of the Evaluation Forum</title>
		<meeting><address><addrLine>Bologna, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022-09">Sep. 2022. 2022</date>
		</imprint>
	</monogr>
	<note>CLEF Working Notes 2022</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Motif Mining and Unsupervised Representation Learning for BirdCLEF</title>
		<author>
			<persName><forename type="first">A</forename><surname>Miyaguchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Cheungvivatpant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Dudley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Swain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CLEF: Conference and Labs of the Evaluation Forum</title>
		<meeting><address><addrLine>Bologna, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022-09">2022. Sep. 2022. 2022</date>
		</imprint>
	</monogr>
	<note>CLEF Working Notes 2022</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Fast similarity matrix profile for music analysis and exploration</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">F</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-C</forename><forename type="middle">M</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Batista</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Keogh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="29" to="38" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Tile2vec: Unsupervised representation learning for spatially distributed data</title>
		<author>
			<persName><forename type="first">N</forename><surname>Jean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Samar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Azzari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lobell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ermon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="3967" to="3974" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Loss of cultural song diversity and the convergence of songs in a declining hawaiian forest bird community</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">L</forename><surname>Paxton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Sebastián-González</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Hite</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">H</forename><surname>Crampton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kuhn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Hart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Royal Society open science</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">190719</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
