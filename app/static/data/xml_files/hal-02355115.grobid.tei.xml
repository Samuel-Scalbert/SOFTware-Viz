<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">EVALUATING VOICE CONVERSION-BASED PRIVACY PROTECTION AGAINST INFORMED ATTACKERS</title>
				<funder ref="#_jhkSSvj">
					<orgName type="full">Agence Nationale de la Recherche</orgName>
					<orgName type="abbreviated">ANR</orgName>
				</funder>
				<funder>
					<orgName type="full">Inria</orgName>
				</funder>
				<funder>
					<orgName type="full">CNRS</orgName>
				</funder>
				<funder ref="#_8U7aQn9">
					<orgName type="full">European Union</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Brij</forename><surname>Mohan</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Lal</forename><surname>Srivastava</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">INRIA</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Nathalie</forename><surname>Vauquier</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">INRIA</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Md</forename><surname>Sahidullah</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution" key="instit1">Université de Lorraine</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
								<orgName type="institution" key="instit3">Inria</orgName>
								<address>
									<addrLine>Loria</addrLine>
									<postCode>F-54000</postCode>
									<settlement>Nancy</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Aurélien</forename><surname>Bellet</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">INRIA</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Marc</forename><surname>Tommasi</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Université de Lille</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Emmanuel</forename><surname>Vincent</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution" key="instit1">Université de Lorraine</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
								<orgName type="institution" key="instit3">Inria</orgName>
								<address>
									<addrLine>Loria</addrLine>
									<postCode>F-54000</postCode>
									<settlement>Nancy</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">EVALUATING VOICE CONVERSION-BASED PRIVACY PROTECTION AGAINST INFORMED ATTACKERS</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">782A811CBF819887A3E8B4411EAF1301</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-04-12T14:48+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>voice conversion</term>
					<term>speech recognition</term>
					<term>speaker verification</term>
					<term>linkage attack</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Speech data conveys sensitive speaker attributes like identity or accent. With a small amount of found data, such attributes can be inferred and exploited for malicious purposes: voice cloning, spoofing, etc. Anonymization aims to make the data unlinkable, i.e., ensure that no utterance can be linked to its original speaker. In this paper, we investigate anonymization methods based on voice conversion. In contrast to prior work, we argue that various linkage attacks can be designed depending on the attackers' knowledge about the anonymization scheme. We compare two frequency warping-based conversion methods and a deep learning based method in three attack scenarios. The utility of converted speech is measured via the word error rate achieved by automatic speech recognition, while privacy protection is assessed by the increase in equal error rate achieved by state-of-the-art i-vector or x-vector based speaker verification. Our results show that voice conversion schemes are unable to effectively protect against an attacker that has extensive knowledge of the type of conversion and how it has been applied, but may provide some protection against less knowledgeable attackers.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Speech is a behavioural biometric characteristic of human beings <ref type="bibr" target="#b0">[1]</ref>, which can produce distinguishing and repeatable biometric features. Dramatic improvements in speech synthesis <ref type="bibr" target="#b1">[2]</ref>, voice cloning <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref> and speaker recognition <ref type="bibr" target="#b4">[5]</ref> that leverage "found data" pose severe privacy threats to the users of speech interfaces <ref type="bibr" target="#b5">[6]</ref>. According to the ISO/IEC International Standard 24745 on biometric information protection <ref type="bibr" target="#b6">[7]</ref>, biometric references must be irreversible and unlinkable for full privacy protection. Anonymization or deidentification <ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref> refers to the task of concealing the speaker's identity while retaining the linguistic content, thereby making the data unlinkable <ref type="bibr" target="#b10">[11]</ref>. In this work, we consider the following threat model: given a public dataset of (supposedly) anonymized speech, an attacker records/finds a sample of speech of a target user and attempts to guess which utterances in the anonymized dataset are spoken by the target user. A good anonymization scheme should prevent such linkage attacks from being successful, while preserving the perceived speech naturalness and intelligibility and/or the performance of downstream tasks such as automatic speech recognition (ASR).</p><p>Fang et al. <ref type="bibr" target="#b11">[12]</ref> classify speaker anonymization methods into two categories: physical vs. logical. Physical methods perturb speech in the physical space by adding acoustic noise, while logical methods apply a transformation to the recorded signal. Among the latter, voice conversion (VC) methods have been traditionally exploited as a way to map the input voice (source) into that of another speaker (target) <ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref><ref type="bibr" target="#b14">[15]</ref>. In contrast to feature-domain approaches <ref type="bibr" target="#b9">[10]</ref>, the output of VC remains a speech waveform and it may be used for listening or transcription purposes. The anonymized speech should thus sound as natural and intelligible as possible <ref type="bibr" target="#b15">[16]</ref>.</p><p>Crucially, all past studies assumed a weak attack scenario where the attacker is unaware that an anonymization method has been applied to the found data <ref type="bibr" target="#b11">[12]</ref>. This raises the concern that the privacy protection may entirely rely on the secrecy of the design and implementation of the anonymization scheme, a principle known as "security by obscurity" <ref type="bibr" target="#b16">[17]</ref> that has long been rejected by the security community. There is therefore a strong need to evaluate the robustness of the anonymization to the knowledge that the adversary may have about the transformation. In practice, such knowledge may for instance be acquired by inspecting the code embedded in the user's device or in an open-source implementation.</p><p>As opposed to past studies, we consider different linkage attacks depending on the attacker's knowledge of the anonymization method. At one end of the spectrum, an Ignorant attacker is unaware of the speech transformation being applied, while at the other end an Informed attacker can leverage complete knowledge of the transformation algorithm. A Semi-Informed attacker may know the voice transformation algorithm but not its parameter values. In our experiments, we evaluate three VC methods with different target speaker selection strategies in various attack scenarios to study unlinkability in the spirit of ISO/IEC 30136 standard <ref type="bibr" target="#b17">[18]</ref>. In each scenario, we assess how well each method protects the speaker identity against attackers that leverage state-of-the-art speaker verification techniques based on i-vectors <ref type="bibr" target="#b18">[19]</ref> or x-vectors <ref type="bibr" target="#b4">[5]</ref> to design linkage attacks. We also report the word error rate (WER) achieved by a state-ofthe-art end-to-end automatic speech recognizer <ref type="bibr" target="#b19">[20]</ref>. While a formal listening test is beyond the scope of this paper, we make a few samples of converted speech available for informal comparison. <ref type="foot" target="#foot_0">1</ref>In Section 2, we describe the three VC methods we evaluate in the context of anonymization. Section 3 introduces the target speaker selection strategies and the attack scenarios. Section 4 presents the experimental settings and the results. We conclude in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">VOICE CONVERSION METHODS</head><p>The criteria for selecting the VC methods in our study are that they must be 1) non-parallel, i.e., do not require a parallel corpus of sentences uttered by both the source and target speakers for trainingthis is important from a privacy perspective since there exist few parallel corpora and selecting openly available targets would increase the risk of an inversion attack; 2) many-to-many, i.e., allow conversion between arbitrary sources and targets so that any speaker in a large corpus can be selected as the target ; 3) source-and languageindependent, i.e., do not require enrollment sentences for the source speaker and do not rely on language-specific ASR or phoneme classification -this is important from a usability perspective as it frees the user from the burden of enrolling and it is applicable to any language (including under-resourced ones), and from a privacy perspective since enrollment translates into the storage of a voiceprint which poses even greater privacy threats.</p><p>The third criterion is quite strict: many VC methods, such as StarGAN-VC <ref type="bibr" target="#b20">[21]</ref> or the ASR-based method in <ref type="bibr" target="#b11">[12]</ref>, do not satisfy it. We found that the vocal tract length normalization (VTLN) based methods in <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b21">22]</ref> and the one-shot method in <ref type="bibr" target="#b22">[23]</ref> satisfy all criteria. In this paper, we use models trained over English speech <ref type="bibr" target="#b23">[24]</ref> but do not use any other linguistic resources such as transcriptions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">VoiceMask</head><p>VoiceMask is described in <ref type="bibr" target="#b14">[15]</ref> as the frequency warping method based on the composition of a log-bilinear function, expressed as f (ω, α) = | -i ln z-α 1-αz |, and a quadratic function, given by g</p><formula xml:id="formula_0">(ω, β) = ω + β( ω π -( ω π ) 2 ).</formula><p>Here ω ∈ [0, π] is the normalized frequency, α ∈ [-1, 1] is the warping factor for the bilinear function, z = e iω , and β &gt; 0 is the warping factor for the quadratic function. Therefore, the warping function is of the form g(f (ω, α), β). The two parameters, α and β, are chosen uniformly at random from a predefined range which is found to produce intelligible speech while perceptually concealing the speaker identity. In the following, we apply this transform to the spectral envelope rather than the pitchsynchronous spectrum as in the original paper. In addition, we apply logarithm Gaussian normalized pitch transformation (see <ref type="bibr" target="#b24">[25]</ref>) so as to match the pitch statistics of a target speaker <ref type="foot" target="#foot_1">2</ref> .</p><p>The authors claim that this transformation is difficult to inverse when the parameter values are unknown because they are randomly selected from a large interval. However, VoiceMask uses the same parameter values to warp the spectra at each time step of the utterance. This approach is quite limited to conceal the identity of the source speaker and to mimic the target speaker because it warps the entire frequency axis in a single direction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">VTLN-based voice conversion</head><p>VTLN-based VC <ref type="bibr" target="#b21">[22]</ref> represents each speaker by a set of centroid spectra extracted using the CheapTrick <ref type="bibr" target="#b25">[26]</ref> algorithm for k pseudophonetic classes. These classes are learned in an unsupervised fashion by clustering all speech frames of all utterances from this speaker. For each class of the source speaker, the procedure finds the class of the target speaker and the warping parameters that minimize the distance between the transformed source centroid spectrum and the target centroid spectrum. All speech frames in that class are then warped using a power function. Similarly to above, we apply this warping to the spectral envelope and also perform Gaussian normalized pitch transformation so as to match the pitch statistics of the target. Compared to VoiceMask, this approach warps the frequency axis in different directions over time. The parameters of this method include the number of classes k and the chosen target speaker.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Disentangled representation based voice conversion</head><p>The third approach is based on disentangled representation of speech as proposed in <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b26">27]</ref>. The core idea is that speaker information is statically present throughout the utterance but content information is dynamic. This approach is based on a neural network transformation and uses a speaker encoder and a content encoder to separate the factors of variation corresponding to speaker and content information. The only parameter of this method is the chosen target speaker.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">TARGET SELECTION STRATEGIES AND ATTACKERS</head><p>In this study, we consider that the VC function and the sets of possible parameter values are known to all users. Each user records his/her voice on his/her device and applies a VC scheme locally before sending it to a public database. In the threat model we consider, an attacker then performs a linkage attack to try to identify which converted utterances in this public database are spoken by a particular user. To this end, we assume that the attacker has access to a small amount of found speech from this user (and potentially some additional public resources, such as benchmark speech processing datasets to train generic speaker models).</p><p>In the following, we define three parameter selection (a.k.a. target selection) strategies for the three VC methods above, which can be seen as key ingredients of a "private-by-design" speech processing system. We then describe the knowledge that an attacker trying to compromise the system could have about the VC function and the target selection strategy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Target selection strategies</head><p>We consider three possible target selection strategies. In strategy const, the VC function is constant across all users and all utterances. This means choosing a unique target speaker and, in the case of VoiceMask, fixed values for α and β. In strategy perm, the conversion parameters are chosen at random once by each user. In other words, when a user downloads the VC module on his/her device, he/she selects a personal target speaker and, in the case of Voice-Mask, personal random values for α and β. Finally, in the random strategy, each time a user applies VC to an utterance, a random set of parameters is drawn, i.e., a random target speaker is selected and, in the case of VoiceMask, random values are drawn for α and β.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Attackers' knowledge</head><p>We define the types of attackers based on the extent of their knowledge about the VC function and its parameters. An Ignorant attacker is not aware that VC has been applied at all. In contrast, an Informed attacker knows the VC method and its exact parameter values (i.e., the chosen target speaker and the values of α and β). One may argue that an Informed attacker is not very realistic (except for the const strategy), while an Ignorant attacker is very weak. Between these two extreme cases, various types of attackers can be defined. For instance, we consider a Semi-Informed attacker who knows the chosen VC method (VoiceMask, VTLN, or disentangled representation) and the target selection strategy (const, perm, or random), but not the actual target (i.e., the actual target speaker or the value of α and β). This is arguably more realistic since the VC algorithm and the target selection strategy may be open-source, while (except for the const strategy) the target chosen by the user is much less easily accessible.</p><p>It is important to note that many concrete instances of attackers of the above types can be designed, and finding out the "best" attacker of a particular type is a hard problem. In the experiments section, we propose attackers exploiting these different levels of knowledge based on the assumptions defined above. A more exhaustive investigation of the design of attackers is left for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">EXPERIMENTS AND RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Data and evaluation setup</head><p>All experiments are performed on the LibriSpeech corpus <ref type="bibr" target="#b23">[24]</ref>. We use the 460 h clean training set (train-clean-100 + train-clean-360), which contains 1,172 speakers, to train the disentanglement transform. Out of the test-clean set, we create an enrollment set (438 utterances) and a trial set (1,496 utterances) with different utterances from the same 29 speakers (13 male and 16 female, not in the training set) considered as source speakers. The target speakers for all three VC methods are randomly picked from the training and testclean sets. See <ref type="bibr" target="#b9">[10]</ref> for more details.</p><p>For each VC method and target selection strategy, all utterances in the trial set are mapped to possibly different target speakers in the training or trial set. The converted trial set serves as the public database that attackers want to de-anonymize by designing a linkage attack. To this end, attackers have access to the enrollment set which serves as the found data used to model the speakers in the trial set.</p><p>The attackers also have access to the 460 h training set to train state-of-the-art speaker verification methods based on x-vectors <ref type="bibr" target="#b4">[5]</ref> and i-vectors, which are stronger than the Gaussian mixture modeluniversal background model (GMM-UBM) based method used in the seminal work of <ref type="bibr" target="#b15">[16]</ref>. We adapt the sre16 Kaldi recipe for training x-vectors and i-vectors to LibriSpeech<ref type="foot" target="#foot_2">3</ref> . We use a smaller network architecture for x-vector computation than the original recipe. Specifically, compared to the architecture in [5, Table <ref type="table" target="#tab_0">1</ref>], we remove the frame4, frame5 and segment7 layers, thereby also reducing the stats pooling layer to 512T ×1024 and the segment6 layer to 1024×512. Here T refers to the utterance-level context. This reduced architecture performs slightly better on LibriSpeech than the architecture in the original recipe. We give more details on the different attackers in Section 4.3.</p><p>Finally, we evaluate the utility of each VC method in terms of the resulting ASR performance on the converted data. We use a hybrid connectionist temporal classification (CTC) and attention based encoder-decoder <ref type="bibr" target="#b19">[20]</ref> trained on the converted 460 h training set using the standard recipe for LibriSpeech provided in ESPnet<ref type="foot" target="#foot_3">4</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Voice conversion settings</head><p>VoiceMask. Pitch, aperiodicity and spectral envelope are extracted using the pyworld vocoder <ref type="foot" target="#foot_4">5</ref> . We follow strategy random only. We sample α uniformly such that |α| ∈ [0.08, 0.10] then β in [-2, 2] such that 0.32 ≤ dist f α,β ≤ 0.40 where dist f α,β = π 0 |f α,β (ω)-ω| is the distortion strength of the warping function. These ranges are provided by VoiceMask's authors in <ref type="bibr" target="#b14">[15]</ref> since they produce most intelligible output. A subset of 100 target speakers is randomly selected and, for every utterance, pitch is transformed so as to match a random speaker within that subset. Other target selections strategies have not been applied because fixed values for α and β (whether speaker-dependent or not) are prone to inversion attacks.</p><p>VTLN-based VC. Pitch, aperiodicity and spectral envelope are extracted using the pyworld vocoder. For each speaker, we collect speech frames using energy-based voice activity detection (VAD) with a threshold of 0.06, and we cluster their spectral envelopes via k-means with k = 8. In strategy const, only one target speaker is selected. In perm, we draw a random subset of 100 target speakers and, for each source speaker, we select a random target within it. In random, we draw a random subset of 100 target speakers and, for each source utterance, we select a random target within it.</p><p>Disentangled representation based VC. We use a publicly available implementation of this method <ref type="foot" target="#foot_5">6</ref> . As per the authors' suggestion in the preprocessing script, we train the disentanglement models (speaker encoder, content encoder, decoder) over the trainclean-100 subset of the LibriTTS corpus (itself a subset of the 460 h training set of LibriSpeech), with a batch size of 128 and learning rate of 0.0005 for 500,000 iterations. All three target selection strategies are applied similarly to VTLN-based VC except that only the source utterance and one random utterance from the target speaker are used as inputs to the content and speaker encoders, respectively. Other utterances from the source and targets speakers are unused.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Attackers</head><p>We have implemented several attackers depending on the choice of the VC algorithm and the target selection strategy as well as the extent of the attacker's knowledge (Informed, Semi-Informed or Ignorant). Our Ignorant attacker is unaware of the VC step: he/she simply trains x-vector/i-vector models on the untransformed training set, and applies them to the untransformed enrollment set. Our Semi-Informed attacker knows the VC algorithm and the target selection strategy (const, random or perm) but not the particular choices of targets. He/she applies this strategy to the training and enrollment sets by drawing random target speakers from the subset of 100 target speakers used by the VC method (we assume that the value of k in VTLN is known to the attacker). As a result, the training and enrollment data are converted in a similar way as the trial data, but the target speaker associated with every speaker in the enrollment set is typically different from that associated with the same speaker in the converted trial set. Finally, our Informed attacker has access to the actual VC models and target choices used to anonymize the trial set, so it converts the training and enrollment sets accordingly.</p><p>In our preliminary experiments, we also considered attackers who convert the enrollment set only and use x-vector/i-vector models trained on the untransformed training set. Unsurprisingly, we found that this leads to significantly larger equal error rates (EER) than retraining the x-vector/i-vector model (which can easily be done by the attacker using public benchmark data). Therefore, we do not report results for such attackers below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Results and discussion</head><p>We first train and apply the ASR and speaker verification systems on the original (untransformed) data for baseline performance. We  obtain an EER of 4.61% and 4.31% for i-vector and x-vector, respectively, and a WER of 9.4% for ASR. Tables <ref type="table" target="#tab_0">1</ref> and<ref type="table" target="#tab_1">2</ref> present the EER for x-vector and i-vector based speaker verification for the three attackers and the various VC methods and target selection strategies. Interestingly, the Informed attacker achieves similar or even slightly lower EER than the baseline. This indicates that, when the attacker has complete knowledge of the VC scheme and target speaker mapping, none of the VC methods is able to protect the speaker identity. While an attacker with such complete knowledge is not very realistic in most practical cases, our results show that speaker information has not been totally removed and is somehow still present in the converted speech.</p><p>For the more realistic Semi-Informed attacker, we observe that strategy perm is quite effective in protecting privacy and shows the highest gains in EER. This is due to the fact that the target speaker in the enrolled data may not be same as the one in trial, hence greater confusion is induced during inference. We also notice that strategy random is not much affected by the change of speaker mapping, which is intuitive because in this case the utterances are already being mapped randomly to different speakers. Such mapping would be ineffective due to averaging of randomness. Strategy const is also slightly affected by the change of mapping because the training and enrollment speaker is not same as that of test speaker, but the effect is not as significant as strategy perm.</p><p>Consistently with past results in the literature, the Ignorant attacker performs worst in terms of EER. This confirms that, when the attacker is oblivious to the privacy-preserving mechanism, we can protect speaker identity completely. Figure <ref type="figure" target="#fig_0">1</ref> shows the distribution of i-vector PLDA scores for genuine and impostor trials, i.e., the log-likelihood ratios between same-speaker and differentspeaker hypotheses. For full unlinkability, the distributions of genuine and impostor scores must be identical. We observe that the overlap between the two distributions decreases as we move from the Ignorant to the Informed attacker, hence increasing linkability.</p><p>Table <ref type="table" target="#tab_2">3</ref> gives the WER obtained for each VC method, which we use as a proxy for the usefulness of the converted speech. Note that there is no difference between converted data in different attack scenarios, hence the WER does not depend on the attacker. Voice-Mask and VTLN-based VC achieve reasonable WER compared to the untransformed data, while the disentangled representation based VC produces unreasonably high WER. Note that these WERs are achieved when ASR is trained solely using converted data. In prac-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Impostor trials</head><p>Genuine trials Informed Semi-Informed Ignorant tice, many techniques can be used optimize the WER, such as using converted data to augment clean data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">CONCLUSION AND FUTURE WORK</head><p>We investigated the use of VC methods to protect the privacy of speakers by concealing their identity. We formally defined target speaker selection strategies and linkage attack scenarios based on the knowledge of attacker. Our experimental results indicate that both aspects play an important role in the strength of the protection. Simple methods such as VTLN-based VC with appropriate target selection strategy can provide reasonable protection against linkage attacks with partial knowledge.</p><p>Our characterization of strategies and attack scenarios opens up several avenues for future research. To increase the naturalness of converted speech, we can explore intra-gender VC as well as the use of a supervised phonetic classifier in VTLN. We also plan to conduct experiments with a broader range of attackers and use standard local and global unlinkability metrics <ref type="bibr" target="#b10">[11]</ref> to precisely evaluate the privacy protection in various scenarios. More generally, designing a privacy-preserving transformation which induces a large overlap between genuine and impostor distributions even in the Informed attack scenario remains an open question. In the case of disentangled representations, this calls for avoiding any leakage of private attributes into the content embeddings.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. I-vector score distribution for trials conducted on VTLN (strategy random) converted data by Ignorant, Semi-Informed, or Informed attackers. The orange distribution indicates impostor scores, while the blue distribution indicates genuine scores. The crossing between the two curves indicates the threshold for EER. More overlap means greater confusion, hence greater privacy protection.</figDesc><graphic coords="5,394.71,287.10,84.43,63.32" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>EER (%) achieved using x-vector based speaker verification.</figDesc><table><row><cell></cell><cell>VoiceMask</cell><cell cols="3">VTLN-based VC</cell><cell cols="3">Disentangl.-based VC</cell></row><row><cell>Attackers ↓ / Strategies →</cell><cell>random</cell><cell>const</cell><cell>perm</cell><cell>random</cell><cell>const</cell><cell>perm</cell><cell>random</cell></row><row><cell>Informed</cell><cell>5.01</cell><cell>4.71</cell><cell>3.91</cell><cell>6.32</cell><cell>4.71</cell><cell>0.20</cell><cell>5.52</cell></row><row><cell>Semi-Informed</cell><cell>-</cell><cell>12.84</cell><cell>23.37</cell><cell>6.32</cell><cell>13.64</cell><cell>43.03</cell><cell>5.42</cell></row><row><cell>Ignorant</cell><cell>28.69</cell><cell>24.27</cell><cell>30.99</cell><cell>27.38</cell><cell>27.68</cell><cell>32.20</cell><cell>30.59</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>EER (%) achieved using i-vector based speaker verification.</figDesc><table><row><cell></cell><cell>VoiceMask</cell><cell cols="3">VTLN-based VC</cell><cell cols="3">Disentangl.-based VC</cell></row><row><cell>Attackers ↓ / Strategies →</cell><cell>random</cell><cell>const</cell><cell>perm</cell><cell>random</cell><cell>const</cell><cell>perm</cell><cell>random</cell></row><row><cell>Informed</cell><cell>8.22</cell><cell>6.22</cell><cell>10.23</cell><cell>9.84</cell><cell>4.71</cell><cell>0.20</cell><cell>11.03</cell></row><row><cell>Semi-Informed</cell><cell>-</cell><cell>18.25</cell><cell>31.49</cell><cell>18.76</cell><cell>15.65</cell><cell>43.93</cell><cell>10.53</cell></row><row><cell>Ignorant</cell><cell>50.55</cell><cell>26.08</cell><cell>49.15</cell><cell>49.15</cell><cell>49.95</cell><cell>47.74</cell><cell>49.85</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>WER (%) achieved using end-to-end ASR.</figDesc><table><row><cell></cell><cell>VoiceMask</cell><cell cols="3">VTLN-based VC</cell><cell cols="3">Disentangl.-based VC</cell></row><row><cell>Subset ↓ / Strategies →</cell><cell>random</cell><cell>const</cell><cell>perm</cell><cell cols="2">random const</cell><cell cols="2">perm random</cell></row><row><cell>test-clean</cell><cell>18.1</cell><cell>19.8</cell><cell>18.4</cell><cell>15.9</cell><cell>41.5</cell><cell>23.7</cell><cell>115.1</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>https://github.com/brijmohan/adaptive_voice_ conversion/tree/master/samples</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>Strictly speaking, VoiceMask is a voice transformation method rather than a VC method: pitch is converted from the source speaker to a target speaker, but the spectral envelope is not related to a particular target speaker.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>https://github.com/brijmohan/kaldi/tree/master/ egs/librispeech_spkv/v2</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"><p>https://espnet.github.io/espnet/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4"><p>https://github.com/JeremyCCHsu/ Python-Wrapper-for-World-Vocoder</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5"><p>https://github.com/jjery2243542/adaptive_voice_ conversion</p></note>
		</body>
		<back>

			<div type="funding">
<div><p>This work was supported in part by the <rs type="funder">European Union</rs>'s <rs type="programName">Horizon 2020 Research and Innovation Program</rs> under Grant Agreement No. <rs type="grantNumber">825081</rs> COMPRISE (https://www.compriseh2020.eu/) and by the <rs type="funder">French National Research Agency</rs> under project <rs type="projectName">DEEP-PRIVACY</rs> (<rs type="grantNumber">ANR-18-CE23-0018</rs>). Experiments presented in this paper were carried out using the Grid'5000 testbed, supported by a scientific interest group hosted by <rs type="funder">Inria</rs> and including <rs type="funder">CNRS</rs>, RENATER and several Universities as well as other organizations (see https://www.grid5000.fr).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_8U7aQn9">
					<idno type="grant-number">825081</idno>
					<orgName type="program" subtype="full">Horizon 2020 Research and Innovation Program</orgName>
				</org>
				<org type="funded-project" xml:id="_jhkSSvj">
					<idno type="grant-number">ANR-18-CE23-0018</idno>
					<orgName type="project" subtype="full">DEEP-PRIVACY</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Biometric identification</title>
		<author>
			<persName><forename type="first">Anil</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lin</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharath</forename><surname>Pankanti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="90" to="98" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Spontaneous conversational speech synthesis from found data</title>
		<author>
			<persName><forename type="first">Éva</forename><surname>Székely</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gustav</forename><forename type="middle">Eje</forename><surname>Henter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonas</forename><surname>Beskow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joakim</forename><surname>Gustafson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. INTERSPEECH</title>
		<meeting>INTERSPEECH</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4435" to="4439" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Voice mimicry attacks assisted by automatic speaker verification</title>
		<author>
			<persName><forename type="first">Ville</forename><surname>Vestman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomi</forename><surname>Kinnunen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rosa</forename><forename type="middle">González</forename><surname>Hautamäki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Md</forename><surname>Sahidullah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Speech &amp; Language</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="page" from="36" to="54" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Can we steal your vocal identity from the internet?: Initial investigation of cloning Obama&apos;s voice using GAN, WaveNet and low-quality found data</title>
		<author>
			<persName><forename type="first">Jaime</forename><surname>Lorenzo-Trueba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fuming</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Isao</forename><surname>Echizen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junichi</forename><surname>Yamagishi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomi</forename><surname>Kinnunen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Odyssey: The Speaker and Language Recognition Workshop</title>
		<meeting>Odyssey: The Speaker and Language Recognition Workshop</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="240" to="247" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">X-vectors: Robust DNN embeddings for speaker recognition</title>
		<author>
			<persName><forename type="first">David</forename><surname>Snyder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Garcia-Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><surname>Sell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjeev</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="5329" to="5333" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The ASVspoof 2017 challenge: Assessing the limits of replay spoofing attack detection</title>
		<author>
			<persName><forename type="first">Tomi</forename><surname>Kinnunen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Md</forename><surname>Sahidullah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Héctor</forename><surname>Delgado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Massimiliano</forename><surname>Todisco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junichi</forename><surname>Yamagishi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kong</forename><surname>Aik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lee</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IN-TERSPEECH</title>
		<meeting>IN-TERSPEECH</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<idno>Document ISO/IEC 24745:2011</idno>
		<title level="m">Information Technology-Security techniques-Biometric Information Protection</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
	<note>ISO/IEC JTCI SC27 Security Techniques</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Preserving privacy in speaker and speech characterisation</title>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Nautsch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Speech &amp; Language</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="page" from="441" to="480" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Convolutional neural network based speaker deidentification</title>
		<author>
			<persName><forename type="first">Fahimeh</forename><surname>Bahmaninezhad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunlei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John Hl</forename><surname>Hansen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Odyssey: The Speaker and Language Recognition Workshop</title>
		<meeting>Odyssey: The Speaker and Language Recognition Workshop</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="255" to="260" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Privacy-preserving adversarial representation learning in ASR: Reality or illusion?</title>
		<author>
			<persName><forename type="first">Brij</forename><surname>Mohan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lal</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aurélien</forename><surname>Bellet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Tommasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emmanuel</forename><surname>Vincent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. INTERPSPEECH</title>
		<meeting>INTERPSPEECH</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3700" to="3704" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">General framework to evaluate unlinkability in biometric template protection systems</title>
		<author>
			<persName><forename type="first">Marta</forename><surname>Gomez-Barrero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Javier</forename><surname>Galbally</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Rathgeb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christoph</forename><surname>Busch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Forensics and Security</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1406" to="1420" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Speaker anonymization using x-vector and neural waveform models</title>
		<author>
			<persName><forename type="first">Fuming</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junichi</forename><surname>Yamagishi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Isao</forename><surname>Echizen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Massimiliano</forename><surname>Todisco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean-Francois</forename><surname>Bonastre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 10th ISCA Speech Synthesis Workshop</title>
		<meeting>10th ISCA Speech Synthesis Workshop</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="155" to="160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">De-identification for privacy protection in multimedia content: A survey</title>
		<author>
			<persName><forename type="first">Slobodan</forename><surname>Ribaric</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aladdin</forename><surname>Ariyaeeinia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikola</forename><surname>Pavesic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Signal Processing: Image Communication</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="page" from="131" to="151" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Online speaker de-identification using voice transformation</title>
		<author>
			<persName><forename type="first">Miran</forename><surname>Pobar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivo</forename><surname>Ipšić</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 37th International convention on information and communication technology, electronics and microelectronics (mipro)</title>
		<meeting>37th International convention on information and communication technology, electronics and microelectronics (mipro)</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1264" to="1267" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Hidebehind: Enjoy voice input with voiceprint unclonability and anonymity</title>
		<author>
			<persName><forename type="first">Jianwei</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haohua</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiahui</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linlin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taeho</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang-Yang</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. the 16th ACM Conference on Embedded Networked Sensor Systems</title>
		<meeting>the 16th ACM Conference on Embedded Networked Sensor Systems</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="82" to="94" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Speaker de-identification via voice transformation</title>
		<author>
			<persName><forename type="first">Qin</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><forename type="middle">R</forename><surname>Toth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tanja</forename><surname>Schultz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><forename type="middle">W</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ASRU</title>
		<meeting>ASRU</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="529" to="533" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Security by obscurity</title>
		<author>
			<persName><forename type="first">T</forename><surname>Rebecca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">G</forename><surname>Mercuri</surname></persName>
		</author>
		<author>
			<persName><surname>Neumann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page">160</biblScope>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<idno>ISO/IEC FDIS 30136</idno>
		<title level="m">Information Technology-Performance Testing of Biometric Protection Schemes</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note>ISO/IEC JTCI SC37 Biometrics</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Front-end factor analysis for speaker verification</title>
		<author>
			<persName><forename type="first">Najim</forename><surname>Dehak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><forename type="middle">J</forename><surname>Kenny</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Réda</forename><surname>Dehak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Dumouchel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Ouellet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="788" to="798" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">ESPnet: End-to-end speech processing toolkit</title>
		<author>
			<persName><forename type="first">Shinji</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Takaaki</forename><surname>Hori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shigeki</forename><surname>Karita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomoki</forename><surname>Hayashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiro</forename><surname>Nishitoba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuya</forename><surname>Unno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nelson</forename><surname>Enrique</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yalta</forename><surname>Soplin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jahn</forename><surname>Heymann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Wiesner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nanxin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adithya</forename><surname>Renduchintala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tsubasa</forename><surname>Ochiai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. INTERPSPEECH</title>
		<meeting>INTERPSPEECH</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2207" to="2211" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">StarGAN-VC: Non-parallel many-to-many voice conversion using star generative adversarial networks</title>
		<author>
			<persName><forename type="first">Hirokazu</forename><surname>Kameoka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Takuhiro</forename><surname>Kaneko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kou</forename><surname>Tanaka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nobukatsu</forename><surname>Hojo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Spoken Language Technology Workshop (SLT)</title>
		<meeting>Spoken Language Technology Workshop (SLT)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="266" to="273" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">VTLN-based voice conversion</title>
		<author>
			<persName><forename type="first">David</forename><surname>Sundermann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hermann</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 3rd IEEE International Symposium on Signal Processing and Information Technology</title>
		<meeting>3rd IEEE International Symposium on Signal essing and Information Technology</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="556" to="559" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">One-Shot Voice Conversion by Separating Speaker and Content Representations with Instance Normalization</title>
		<author>
			<persName><forename type="first">Chou</forename><surname>Ju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hung-Yi</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. INTERSPEECH</title>
		<meeting>INTERSPEECH</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="664" to="668" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">LibriSpeech: an ASR corpus based on public domain audio books</title>
		<author>
			<persName><forename type="first">Vassil</forename><surname>Panayotov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guoguo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjeev</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="5206" to="5210" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">High quality voice conversion through phoneme-based linear mapping functions with STRAIGHT for Mandarin</title>
		<author>
			<persName><forename type="first">Kun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianping</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonghong</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Fourth International Conference on Fuzzy Systems and Knowledge Discovery (FSKD 2007)</title>
		<meeting>Fourth International Conference on Fuzzy Systems and Knowledge Discovery (FSKD 2007)</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="410" to="414" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">CheapTrick, a spectral envelope estimator for high-quality speech synthesis</title>
		<author>
			<persName><forename type="first">Masanori</forename><surname>Morise</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Speech Communication</title>
		<imprint>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="page" from="1" to="7" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Improved texture networks: Maximizing quality and diversity in feed-forward stylization and texture synthesis</title>
		<author>
			<persName><forename type="first">Dmitry</forename><surname>Ulyanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="6924" to="6932" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
