<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Stackless Processing of Streamed Trees</title>
				<funder ref="#_EdnfHMd">
					<orgName type="full">Poland&apos;s National Science Centre</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Corentin</forename><surname>Barloy</surname></persName>
							<email>cbarloy@clipper.ens.fr</email>
							<affiliation key="aff0">
								<orgName type="institution">ENS de Paris</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Filip</forename><surname>Murlak</surname></persName>
							<email>fmurlak@mimuw.edu.pl</email>
							<affiliation key="aff1">
								<orgName type="institution">University of Warsaw</orgName>
								<address>
									<country key="PL">Poland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Charles</forename><surname>Paperman</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Université de Lille &amp; INRIA</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Stackless Processing of Streamed Trees</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">574700D1C7B9F30ABA8497F79E486382</idno>
					<idno type="DOI">10.4230/LIPIcs</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-04-12T14:45+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>2012 ACM Subject Classification Theory of computation → Automata extensions; Theory streaming</term>
					<term>querying</term>
					<term>XML</term>
					<term>JSON</term>
					<term>automata</term>
					<term>weak validation Digital Object Identifier 10.4230/LIPIcs</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Processing tree-structured data in the streaming model is a challenge: capturing regular properties of streamed trees by means of a stack is costly in memory, but falling back to finite-state automata drastically limits the computational power. We propose an intermediate stackless model based on register automata equipped with a single counter, used to maintain the current depth in the tree. We explore the power of this model to validate and query streamed trees. Our main result is an effective characterization of regular path queries (RPQs) that can be evaluated stacklessly-with and without registers. In particular, we confirm the conjectured characterization of tree languages defined by DTDs that are recognizable without registers, by <ref type="bibr" target="#b24">Segoufin and Vianu (2002)</ref>, in the special case of tree languages defined by means of an RPQ.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>While graph is the new black, tree-structured data has not vanished. It is used both as a serialization format (Wikipedia, Wikidata, DBLP) and as an exchange format (WSDL and SOAP rely on XML, the more recent GraphQL prefers JSON). Querying and validation of tree-structured data continue to be both vital and challenging tasks in data management.</p><p>Particularly so, when documents grow too large to fit in memory, and it is time to switch to streaming; that is, to read the document sequentially, maintaining a concise internal representation sufficient for the realized task. According to Palkar et al. <ref type="bibr" target="#b17">[18]</ref>, exploratory big-data applications running over data in a semi-structured format, like JSON, can spend 80-90% of their execution time simply parsing the data. Performance improvements often rely on clever ways to reduce the cost of parsing. In systems research, two main strategies have been proposed. The first one relies on SAX (Simple API for XML) parsers: it outsources parsing to the API and deals only with the resulting events <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b25">26]</ref>. This allows to factor out the cost of parsing, and may lead to significant performance gains when multiple queries are executed over the same document <ref type="bibr" target="#b25">[26]</ref>. The second approach is to perform parsing and query execution simultaneously, applying push-down automata as the computation model <ref type="bibr" target="#b16">[17]</ref>, in the hope that the acquired semantic information would help reduce the cost of parsing. When a single query is executed over a huge document, this may also be highly beneficial <ref type="bibr" target="#b5">[6]</ref>.</p><p>The theoretical take on alleviating the cost of parsing is more radical: since it is so costly, let us assume that it has been already done for us and the input stream is guaranteed to be a well-formed document. This may be the case, for instance, if we trust the source of the document or if we have already processed the document for other purposes. Can this assumption help process the document more efficiently? This setting was introduced as weak validation in the seminal work of Segoufin and Vianu <ref type="bibr" target="#b24">[25]</ref> on validating a streamed XML document against a DTD by means of a finite automaton. Despite the significant progress made in the initial paper and in the follow-up work <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b23">24]</ref>, the general problem of deciding whether weak validation against a given regular tree language is feasible, remains open. Incidentally, this question is a special-but disturbingly generic-case of an undecidable separation problem <ref type="bibr" target="#b12">[13]</ref>.</p><p>A recent trend in data processing is to use hardware acceleration to exploit local parallelism. Most modern CPU architectures offer SIMD (single instruction multiple data) instructions, allowing to perform the same operation on multiple data points in one CPU cycle, leading to what is known as the vectorization of computation. Vectorization in used routinely in data-intensive applications like multimedia processing <ref type="bibr" target="#b22">[23]</ref> or deep learning <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b26">27]</ref>, and is finding its way to data management, particularly in the sub-field of in-memory databases <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b21">22]</ref>. Relevant examples from a related field are the performant regular expression engine Hyperscan <ref type="bibr" target="#b28">[29]</ref> and the competitive engine of the RUST language <ref type="bibr" target="#b9">[10]</ref>, both relying crucially on vectorization. In the context of streaming processing of tree-structured data, an early work on parabix by Cameron et al. studies the use of SIMD instructions to accelerate XML parsing <ref type="bibr" target="#b3">[4]</ref>. More recently, Langdale and Lemire illustrate how the performance of JSON parsers could be vastly improved by using vectorization <ref type="bibr" target="#b13">[14]</ref>. Their experiments confirm that the cost of parsing is a large fraction of the total cost of query execution, matching the performance loss with respect to regular expression matching. To get a better feeling of the room for improvement, let us look at some numbers: the experiments had different setups, but the orders of magnitude are still of interest. The standard C function memchr scans memory to find the first occurrence of a given byte; it has been hand-optimized for various architectures and can be assumed to display the best performance one could hope for in a streaming task. On a standard laptop computer, it easily reaches 20Gb/s. The Hyperscan regular expression engine reaches performance of 10Gb/s <ref type="bibr" target="#b28">[29]</ref>. Langdale et al. get up to 3Gb/s when parsing JSON files and selecting some nodes, but selecting alone reaches 10Gb/s <ref type="bibr" target="#b13">[14]</ref>. Palkar et al. explicitly put the blame on the incompatibility of pushdown automata and vectorization <ref type="bibr" target="#b17">[18]</ref>.</p><p>To some extent this is explained by theory. An abstract model of exploiting local parallelism in streaming algorithms was proposed in <ref type="bibr" target="#b14">[15]</ref>: the stream is read in blocks, each block is processed by a fixed boolean circuit, and the result is fed back to the circuit together with the next block of the stream. The degree of local parallelism of a language is measured by the complexity of the circuit needed to recognize the language in the above model: the higher the complexity, the less local parallelism. As shown in the paper, the degree of local parallelism of regular languages matches their classical circuit complexity, and it is plausible that the situation is similar for larger classes of languages. Assuming this is the case, successful vectorization of XML or JSON parsers might be more tricky than for regular expression engines: Dyck languages (well-formed multi-bracket expressions) are TC 0 -complete <ref type="bibr" target="#b1">[2]</ref>, but while regular languages may have even higher complexity, the ones appearing in benchmarks are typically much simpler (for instance, all examples in <ref type="bibr" target="#b9">[10]</ref> and <ref type="bibr" target="#b30">[31]</ref> are in AC 0 ).</p><p>All this evidences that stack-based computation is troublesome. At the same time falling back to finite automata severely limits expressivity, as revealed by the necessary conditions discovered by Segoufin and Vianu <ref type="bibr" target="#b24">[25]</ref>. As a middle ground, we propose to relax the computational model just so. We allow one counter for maintaining the current depth in the document, and registers for storing the current depth to be compared with the depths of later tags. In the resulting model, dubbed depth-register automaton, transitions are performed at a very low CPU cost with almost no external memory access. The latter depends on the number of registers; if the number is low enough, it is even possible to keep all the values within the CPU's registers and not use external memory at all. Unlike pushdown automata, the model appears amenable to vectorization and may be hoped to achieve high throughputs, but a systematic study of this aspect is a matter of future work.</p><p>We apply the proposed setting (predominantly) to querying, which-to the best of our knowledge-has not been studied before from this angle. After a preliminary expressivity study, we embark on characterizing node-selecting queries that can be realized in our model. Our first main result is an effective characterization of regular path queries (RPQs) that can be realized with depth-register automata. As a by-product, we reveal a connection with the languages of trees in which some (resp. each) leaf is selected by the RPQ. Our second main result is an analogous characterization for finite automata. The conditions for the unary query and the two associated tree languages do not conincide any more, but are elegantly related: the RPQ can be realized iff both languages can be (weakly) recognized. The setting of the second main result is exactly that of Segoufin and Vianu, and it allows to make some progress towards solving the original weak validation problem. We develop our results for the XML encoding of trees, but they adapt smoothly to the less verbose JSON-style encoding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Organization of the paper.</head><p>Section 2 introduces the computation model and gives a preliminary expressivity study. Section 3 establishes the characterization theorems. Section 4 explores the connection with the weak validation problem, explains the adaptation to JSON-style encoding, and points out key open problems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Computational model</head><p>We model tree-structured data as ordered unranked finite trees whose nodes are labelled with symbols from a finite alphabet Γ. We refer to them simply as trees over Γ. An immediate subtree of tree T is a subtree rooted at a child of the root of T . A tree language over Γ is a set of trees over Γ. If L is a language of trees (or words) over Γ, we write L c for the complement of this language.</p><p>The markup encoding represents trees over Γ as words over the alphabet Γ ∪ Γ, where Γ = ā a ∈ Γ . In the context of the encoding, the elements of Γ and Γ are referred to as opening and closing tags, respectively. If T is a tree whose root is labelled with a and whose immediate subtrees are T 1 , T 2 , . . . , T n , then</p><formula xml:id="formula_0">T = a • T 1 • T 2 • • • • • T n • ā .</formula><p>For example, aaāccā encodes the tree with a-labelled root whose first child has label a and second child has label c. If L is a tree language over Γ, we let L = T T ∈ L ⊆ (Γ ∪ Γ) * .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Depth-register automata</head><p>Under the markup encoding, finite automata are unable to check even the simplest properties of the input document: for instance, determining if one marked node is a child, descendant, or sibling of another marked node requires a stack-or at least a counter, used to compare depths of nodes. Realizing multiple such tasks simultaneously seems to lead to multi-counter automata, which are notoriously hard to analyze. We take a different path: we allow only one counter, used exclusively to maintain the current depth in the tree, but additionally equip the automaton with a bounded number of registers, which can be used to store depths of critical nodes, and compare them later with the current depth. To keep our automata executable efficiently, we assume that they are deterministic. Thus we arrive at deterministic input-driven 1-counter automata with registers. 'Input-driven' is the standard terminology for counters or stacks that evolve independently of the state <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b6">7]</ref>. Here it means that the counter increases by one with each opening tag read, and decreases by one with each closing tag read; such automata (without registers) are also called visibly counter automata <ref type="bibr" target="#b0">[1]</ref>. Importantly, the only tests allowed on the values stored in registers are order comparisons with the current depth. We shall refer to such devices as depth-register automata. A formal definition follows.</p><formula xml:id="formula_1">Definition 1. A depth-register automaton A is a tuple (Γ, Q, q init , F, Ξ, δ) ,</formula><p>where Γ is a finite alphabet, Q is a finite set of states, q init ∈ Q is the initial state, F ⊆ Q is the set of accepting (final) states, Ξ is a finite set of registers, and</p><formula xml:id="formula_2">δ : Q × (Γ ∪ Γ) × 2 Ξ × 2 Ξ → 2 Ξ × Q is the transition function. A configuration of A is a tuple (q, d, η) ∈ Q × Z × Z Ξ ,</formula><p>whose components specify the state, the current depth, and the values stored in the registers, respectively. We call a configuration (q, d, η) accepting if q ∈ F . The initial configuration is c init = (q init , 0, η init ) where η init (ξ) = 0 for all ξ ∈ Ξ.</p><p>The run of A over a word a 1 a 2 . . . a n ∈ (Γ ∪ Γ) * from a configuration (q 0 , d 0 , η 0 ) is the unique sequence of configurations</p><formula xml:id="formula_3">(q 0 , d 0 , η 0 )(q 1 , d 1 , η 1 ) . . . (q n , d n , η n ) ∈ Q × Z × Z Ξ * such that for each i ∈ {1, 2, . . . , n}, there exists Y i ⊆ Ξ such that d i = d i-1 + 1 if a i ∈ Γ , d i-1 -1 if a i ∈ Γ ; δ(q i-1 , a i , X ≤ i , X ≥ i ) = Y i , q i where X ≤ i = {ξ ∈ Ξ η i-1 (ξ) ≤ d i } , X ≥ i = {ξ ∈ Ξ η i-1 (ξ) ≥ d i } ;</formula><p>for each ξ ∈ Ξ,</p><formula xml:id="formula_4">η i (ξ) = d i if ξ ∈ Y i , η i-1 (ξ) if ξ / ∈ Y i .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Barloy, Murlak and Paperman</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>XX:5</head><p>We write c • w for the last configuration of the run on w from c. If c • w = c , we also write c w -→ c . By the run of A on w we understand the run on w from c init . We say that w is accepted by A if c init • w is accepting. The language recognized by A is the set of words accepted by A.</p><p>Depth-register automata without registers (that is, with Ξ = ∅) are a notational variant of deterministic finite automata over the alphabet Γ ∪ Γ. For such automata we streamline the notation introduced in Definition 1 by dropping the ingredients associated with Ξ. In particular, we use states instead of configurations, and write q • w = q and q w -→ q . The same notation will be applied to finite automata over Γ, not only over Γ ∪ Γ.</p><p>We shall give examples of depth-register automata in Section 2.2 (Examples 2, 5 and 6), once we have made precise how they are used to recognize tree languages.</p><p>To conclude the discussion of the automata model, let us point out that the kind of tests allowed on registers is a natural parameter of the definition. For instance, one could allow testing if the current depth differs from the content of a given register by a specified constant; this kind of test can be simulated in our model at the cost of using additional registers. An interesting proper extension is to allow semilinear conditions, like testing equality modulo a specified constant. Finally, forsaking any hope of decidability of emptiness (which might be tolerable), one could go up to full arithmetics. Owing to their determinizm, depth-register automata in all these variants would be efficiently executable in practice, using only a constant number of variables (possibly just CPU registers). Nevertheless, in this first study we stick to the minimalist approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Recognizing streamed tree languages</head><p>A tree language L over Γ is recognized (under the markup encoding) by an automaton A over the alphabet Γ ∪ Γ if for each tree t over Γ it holds that A accepts t iff t ∈ L. Equivalently, L is recognized by A if the language of words accepted by A separates L from L c . A tree language is stackless if it is recognized by a depth-register automaton, and registerless if it is recognized by a finite automaton.</p><p>Note that the automaton is allowed to accept or reject invalid encodings; that is, elements of Γ ∪ Γ * \ L ∪ L c . Requiring that all invalid encodings be rejected would lead to L ⊆ (Γ ∪ Γ) * being recognized by a finite (resp. depth-register) automaton, which is a much stronger property. In particular, the assumption that L is recognized by a finite automaton is prohibitively strong, as it implies that the depth of trees in L is bounded <ref type="bibr" target="#b24">[25]</ref>. In contrast, a registerless tree language may easily contain trees of unbounded depth: a very simple example is the set of trees with at least one a-labelled node, which can be recognized (under the markup encoding) by a finite automaton that moves to an all-accepting sink state upon reading the opening tag a for the first time.</p><p>Registerless tree languages are regular, because a tree automaton can simulate the run of a finite automaton over the encoding of the tree. Stackless tree languages, in contrast, need not be regular.</p><p>Example 2. The set of trees over the alphabet {a, b} in which all a-labelled nodes are at the same depth, can be recognized by a depth-register automaton. The first time the automaton sees a, it stores the current depth in its only register. Then, every time it sees a it checks if the current depth is equal to the stored value, and if it is not, it moves to a rejecting sink state.</p><p>Regularity can be enforced by applying a stack-like policy of using registers. We call a depth-register automaton restricted if each transition overwrites all stored values strictly greater than the current depth; that is, if δ(p, a, X ≤ , X ≥ ) = (Y, q), then X ≥ \ X ≤ ⊆ Y . Proposition 3. Restricted depth-register automata recognize regular tree languages.</p><p>We conjecture that restricted depth-register automata recognize all regular stackless tree languages, but it is conceivable that they do not. This is why we work with the unrestricted model and prove (potentially) stronger inexpressibility results. We stress, however, that all depth-register automata we construct are restricted. In particular, the characterization in Theorem 14 is identical for the restricted model, backing up the conjecture.</p><p>Regardless of the restriction, stackless tree languages retein the usual closure properties or regular languages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Lemma 4. The classes of registerless and stackless tree languages are both closed under intersection, union, and complementation.</head><p>How far do stackless tree languages go beyond registerless? As first examples, let us see how depth-register automata can deal with sequences of siblings and the descendent relation.</p><p>Example 5. Consider a regular language L ⊆ Γ * and the set HL of trees over Γ such that the sequence of labels read from the children of the root forms a word in L. Depending on L, the tree language HL may be registerless or not. For instance, for L = Γ * aΓ * , HL is not registerless, because a finite automaton cannot determine whether the current tag with label a belongs to a child of the root. This can be shown easily by pumping, but it also follows from our general characterization result, Theorem 15 (1), applied to the set of trees that contain a branch labelled by a word from ΓaΓ * . In contrast, HL is stackless for all regular L. Indeed, after reading the first tag (which must be an opening tag in a valid encoding), the automaton stores the current depth (which is 1) in its only register, and then simulates the finite automaton recognizing L over all closing tags for which the current depth is equal to the value stored in the register. This is correct, because in each valid encoding all closing tags with current depth 1 belong to the children of the root. Example 6. Consider the set of trees over the alphabet {a, b, c} where the first a-labelled node (in the document order) has a b-labelled descendent. To recognize this language, the automaton should read the input word until it sees a, load the current depth to its only register, and accept iff it sees the letter b before the current depth drops strictly below the stored value (this will indicate, that the corresponding closing tag has been read). Now, consider the set of trees over {a, b, c} where some a-labelled node has a b-labelled descendant. It suffices to test this property for minimal a-labelled nodes (that is, those without a-labelled ancestors): if a node has a b-labelled descendent, so do all its ancestors. Hence, to recognize the described language it suffices to run the automaton described above in a loop, returning to the initial state whenever the current depth drops strictly below the stored value, until it accepts.</p><p>The main weakness of depth-register automata when applied to processing trees is their limited ability to handle the child relation, as revealed by the following example.</p><p>Example 7. Consider the language of trees over the alphabet {a, b, c} where some a-labelled node has a b-labelled child. It might appear that this language is stackless because it is easy to identify an a-labelled node and a single register is sufficient to identify the tags of its children in the encoding. Indeed, this idea can be used to recognize the language of trees where some minimal a-labelled node has a b-labelled child, just like we did for b-labelled descendents in Example 6. Without the minimality assumption, however, the subautomaton searching for b-labelled children needs to be relaunched whenever the opening tag a is read, which may well happen before the previous instance of the subautomaton terminates. Each launch requires a new register to store the return point. Because the input tree may contain arbitrarily long chains of a-labelled nodes, this does not seem feasible with any fixed number of registers. That it is indeed infeasible follows from the general characterization result (Theorem 14) we establish in Section 3.</p><p>The method from Example 6 can be extended to test the existence of multiple nodes with specified labels and descendent relationships between them. By a descendent pattern we shall understand a finite tree over Γ. A tree T contains a descendent pattern π if there exists a matching function h that maps nodes of π to nodes of T such that for all nodes u, v of π:</p><p>the label of u coincides with the label of h(u);</p><formula xml:id="formula_5">if v is a child of u, then h(v) is a descendent of h(u).</formula><p>Proposition 8. For each descendent pattern π, the set of trees containing π is stackless.</p><p>Proof. By a slight abuse of the definition of depth-register automata, we shall allow automata that can stop; that is, in some configurations there may be no transition to take. We prove by induction on the height of π that there is an automaton A π that recognizes trees that contain π and stops upon reading the closing tag corresponding to the first opening tag of its input.</p><p>If π consists of a single node, the automaton loads into its only register the current depth before reading any tags, scans the input until the current depth again becomes equal to the stored value. Then it moves to a state without outgoing transitions that is accepting or not, depending on whether the automaton has detected a tag with the label from the root of π or not.</p><p>Suppose that the root of π has some children. By the inductive hypothesis, there is an automaton A π for each descendent pattern π corresponding to an immediate subtree of π. Let A be the synchronous product of all these automata, recognizing the intersection of the languages recognized by its components. Like in Example 6, we can assume that the root of π is matched to a minimal element with the desired label. The automaton A π loads the current depth before reading any tags into its first register, and then processes the input looking for the first opening tag with the same label as the root of π. If A π does not see one before the current depth is again equal to the stored value, it rejects. If it does find one, it calls the automaton A using a set of registers excluding the first one and waits until A stops. If A accepts, A π waits until the current depth becomes equal to the value stored in the first register, and accepts. If A rejects, A π moves on to the next opening tag with the same label as the root of π.</p><p>While the class of stackless tree languages is closed under complement by Lemma 4, it does not follow that we can handle negative information just as well as positive. We say that a tree T strictly contains a descendent pattern π if T contains π and the matching function h additionally satisfies the condition if v is not a descendent of u, then h(v) is not a descendent of h(u). The following example shows that Proposition 8 does not extend to this stronger notion.  . trees strictly containing π is recognized by a depth-register automaton B with m states and registers. We shall analyze the behaviour of the automaton B over trees conforming to the 'schema' shown in Figure <ref type="figure" target="#fig_1">1b</ref>, which for each n &gt; 2 defines the set K n of trees that have the main branch labelled by the word b n , and additionally each b-labelled node may have a c-labelled child to the right of the main branch, and each internal b-labelled node may have an a-labelled child to the left of the main branch. For a tree T like this, let w T be the prefix of T ending at the opening tag of the deepest b-labelled node. Let c init be the initial configuration of B.</p><p>For T ∈ K n , we have that c init • w T = (q, n, η) for some state q and some η : Ξ → {0, 1, . . . , n}. That is, m • (n + 1) configurations are possible. But there are 2 n-2 ways to choose which b-labelled non-leaf nodes have an a-labelled child, so w T T ∈ K n = 2 n-2 . Consequently, for sufficiently large n, there exist two different words u and v in {w</p><formula xml:id="formula_6">T T ∈ K n , such that c init • u = c init • v. Because u = v,</formula><p>there exists i ∈ {2, 3, . . . , n -1} such that for all S, T ∈ K n , if u = w S and v = w T , then the ith b-labelled node has an a-labelled child in S iff it does not have one in T . Let us choose S and T such that in both of them, the (i -1)st and the (i + 1)st b-labelled node has a c-labelled child and there are no other c-labelled nodes, as shown in Figures <ref type="figure" target="#fig_1">1c</ref> and<ref type="figure" target="#fig_1">1d</ref>. Clearly, the tree in Figure <ref type="figure" target="#fig_1">1c</ref> strictly contains π. It is not difficult to verify that the one in Figure <ref type="figure" target="#fig_1">1c</ref> does not. However, from the definition of S and T it follows that S = uw and T = vw for some w , and because c init • uw = c init • vw , we conclude that S and T are indistinguishable to B.</p><p>Finally, let us point out that the ability to deal with sequences of siblings, demonstrated in Example 5, is limited to nodes that are close to the root. The following example shows why.</p><p>Example 10. Even a finite automaton can check if the streamed tree contains two consecutive siblings with labels a and b: it suffices to check if the read encoding contains the closing tag ā followed immediately by the opening tag b. Consider, however, the set of trees that contain three consecutive siblings with labels a, b, c. Arguing like in Example 9 one can show that this language is not stackless. Dropping the assumption that the siblings are consecutive, or even that they are ordered as written, does not affect the argument. Thus, depth-register automata are able to express involved global properties of trees (Proposition 8), far out of reach of finite automata, yet they cannot handle many properties that appear local but lose their locality when seen as properties of the encodings (Examples 7 and 10). Characterizing stackless tree languages seems to be challenging, but in Section 3 we solve the special case of tree languages defined in terms of properties of branches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Querying streamed trees</head><p>So far we used automata as acceptors, defining languages of trees. However, we can also use them as node selectors, defining queries over trees. By a query Q of arity k we mean a function mapping each tree T to a set Q(T ) of k-tuples of nodes of T . In the streaming setting, higher-arity queries are problematic because a streaming algorithm using memory of size f (n) over inputs of length n cannot return asymptotically more than f (n) • n answers. This means that handling even very simple queries of arity larger than one in sublinear memory is impossible without compromising the semantics by applying restrictive selection strategies <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b8">9]</ref> or heuristics like load shedding <ref type="bibr" target="#b11">[12]</ref>. Moreover, popular query languages for tree-structured data, like XPath or JSONPath, focus on unary queries. We shall do the same.</p><p>Implementations of unary queries over streamed trees come in two distinct flavours, corresponding to the two natural moments when one may wish the selected nodes to be returned: at the opening tag or at the closing tag. Accordingly, we say that an automaton A pre-selects (resp. post-selects) a node v of a tree T if A is in an accepting state directly after reading the opening (resp. closing) tag of v. Both approaches have their merits. Post-selection is more expressive, because it allows to explore the subtree rooted at the given node. Pre-selection gives more flexibility in the subsequent stages of processing, allowing to return the whole subtree rooted at the selected node without additional memory cost. In this work we focus on pre-selection, and leave post-selection for the future. We say that an automaton A realizes a unary query Q if for every tree T , A pre-selects exactly those nodes of T that belong to Q(T ). We call a unary query stackless (resp. registerless) if it can be realized by a depth-register automaton (resp. finite automaton).</p><p>Practical declarative query formalisms for tree-structured data, like XPath or JSONPath, treat the context of a node in a symmetric fashion, even if siblings are considered ordered. The streaming setup, on the other hand, is inherently asymmetric: siblings to the left of the node to be selected can be accessed freely, but there is no way to access those on the right. While there exist meaningful queries that could exploit access to the siblings on the left, in this work we abstract away from this aspect and focus on queries invariant under sibling order. A query Q is invariant under sibling order if for each bijection f between the nodes of a tree T and the nodes of a tree T that preserves node labels and the child relation, it holds that Q(T ) = f (u) u ∈ Q(T ) . Unary queries invariant under sibling order form a rich class and capture an important segment of user queries, including all vertical XPath queries, built up from vertical axes (child, descendent, parent, ancestor), label tests, and filters. We aim at understanding which of them can be implemented over streamed trees using finite or depth-register automata.</p><p>The scope of this task can be narrowed down quickly, as all stackless queries invariant under sibling order fall within a well-known class of queries. With each regular language L ⊆ Γ * , we associate a unary query QL that selects all nodes v such that the path from the root to v is labelled by a word from L. We call queries of this form regular path queries (RPQs). They include all XPath queries built up from downward axes (child, descendent) and label tests, but not those using upward axes (parent, ancestor) or filters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proposition 11. The class of stackless queries invariant under sibling order is contained in the class of RPQs.</head><p>That is, if a unary query invariant under sibling order is not an RPQ, then it is not stackless either. In particular, vertical XPath queries cannot be realized by depth-register automata if they use upward axes or filters. Consequently, understanding which unary queries invariant under sibling order are stackless or registerless amounts to characterizing stackless and registerless queries among RPQs, which will be the focus of the remainder of this paper. The first one is registerless: the realizing finite automaton should check that the first opening tag has label a and then it should accept at each opening tag with label b. On the other hand, the last RPQ cannot be realized even by a depth-register automaton, because letting this automaton loop in each accepting state we would obtain an automaton recognizing the language from Example 7. What about the remaining two RPQs? It will follow from our general characterization results (Theorems 14 and 15) that they are stackless, but not registerless.</p><p>From the perspective taken in this paper, RPQs and depth-register automata play asymmetric roles: RPQs represent user queries, and depth-register automata represent their implementations in the streaming setting. Accordingly, the fundamental question is which user queries can be implemented; that is, which RPQs are stackless. Nevertheless, one can also ask which stackless queries are RPQs. This appears challenging in general, but if the query is given as a restricted depth-register automaton, it is a pleasent exercise in automata theory, reminiscent of the characterization of tree languages recognizable by deterministic top-down automata <ref type="bibr" target="#b15">[16]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proposition 13. It is decidable if the query realized by a given restricted depth-register automaton is an RPQ.</head><p>Unlike in graph databases, where RPQs are viewed as binary queries selecting suitably connected pairs of nodes <ref type="bibr" target="#b2">[3]</ref>, in our setting RPQs are treated primarily as unary queries selecting nodes suitably connected to the root. But we can also treat them as boolean queries, defining sets of trees that contain a node-or a leaf-suitably connected to the root. The leaf variant will be instrumental in the characterization results of Section 3. We write EL for the set of trees that contain a branch labelled by a word from L, and AL for the set of trees with all branches labelled by words from L. Note that (AL) c = E(L c ). Languages of the form AL can express useful and nontrivial schema restrictions, as they are able to specify which labels are allowed in the children of a node, depending on regular properties of the path from the root. This will allow us to shed more light on the framework of Segoufin and Vianu <ref type="bibr" target="#b24">[25]</ref> in Section 4.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Characterization theorems</head><p>The characterization theorems rely on four syntactic classes of regular languages: almostreversible, hierarchically almost-reversible, E-flat, and A-flat (Definitions 17, 19 and 22).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Barloy, Murlak and Paperman</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>XX:11</head><p>For now they can be treated as blackboxes, but let us highlight that their definitions are based on simple PTIME-testable properties of the minimal automaton, which makes the characterizations effective. Indeed, also the suitable automata for QL, AL, and EL can be computed in time polynomial in the size of the minimal automaton of L.</p><p>For each theorem we provide a proof outline explaining how to infer the theorem from the expressibility and inexpressibility results we establish in the remainder of this section. Theorem 14. For each regular language L, the following conditions are equivalent: 1. QL is a stackless unary query; 2. EL is a stackless tree language; 3. AL is a stackless tree language; 4. L is hierarchically almost-reversible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proof outline.</head><p>(1) implies ( <ref type="formula" target="#formula_22">2</ref>) because an automaton A realizing QL can be easily turned into an automaton A recognizing EL. A behaves like A, but it additionally remembers the previously read symbol; if the previous symbol was an opening tag, the state is accepting in A, and the current letter is a closing tag, then A moves to an all-accepting sink state.</p><p>(2) implies ( <ref type="formula" target="#formula_26">4</ref>) by Lemma 29, and ( <ref type="formula" target="#formula_26">4</ref>) implies (1) by Lemma 21. This shows that (1), (2), and (4) are equivalent. It follows that ( <ref type="formula" target="#formula_22">2</ref>) and ( <ref type="formula" target="#formula_25">3</ref>) are equivalent, because (AL) c = E(L c ), the class of stackless tree languages is closed under complementation, and, by Lemma 20, so is the class of hierarchically almost-reversible languages.</p><p>In the registerless case the picture is more complicated, reflecting the inherent duality of tree languages of the form AL and EL. Theorem 15. Let L be a regular language. 1. EL is a registerless tree language iff L is E-flat. 2. AL is a registerless tree language iff L is A-flat. We remark that Theorem 15 is fully compatible with the framework introduced by Segoufin and Vianu <ref type="bibr" target="#b24">[25]</ref>; we discuss the connection in detail in Section 4.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Almost-reversibility</head><p>How does one go about evaluating an RPQ with a finite automaton reading the markup encoding of a tree? Over the leftmost branch this is easy: as long as only opening tags are read, we simulate the automaton underlying the RPQ over the labels in the tags and accept whenever the simulated automaton accepts. When the first closing tag appears, the simulated automaton should revert to the state before reading the corresponding opening tag. Our simulation could store a bounded suffix of the run of the simulated automaton, and use it when closing tags occur, but what shall we do when it is used up? This is clearly not a sustainable strategy. The task does become feasible if we assume that the previous state can be determined based on the current state and the last read letter. Automata that have this property are called reversible.</p><p>Recall that in a deterministic automaton letters induce functions mapping states to states. A deterministic automaton is reversible if every letter induces an injective function (Figure <ref type="figure">2</ref>). Equivalently, one may assume that letters induce permutations of states, which implies that the monoid generated by these functions-with composition as the inner product-is a group. Reversibility can be studied as a separate notion upon extension to incomplete automata, where letters induce partial functions over states <ref type="bibr" target="#b19">[20]</ref>.</p><p>The simulation above captures RPQs given by reversible automata, but we can do a bit more. Consider the automata depicted in Figure <ref type="figure" target="#fig_11">3</ref>. None of them is reversible because the function induced by the letter a is not injective. However, as explained in Example 12, the automaton in Figure <ref type="figure" target="#fig_11">3a</ref> defines a registerless RPQ, while those in Figures <ref type="figure" target="#fig_11">3b to 3d</ref> do not. In order to capture registerless RPQs precisely, we carefully relax the notion of reversibility.</p><p>Unlike reversibility itself, its relaxed variant is dependent on which states are accepting. Let us fix a deterministic automaton A. We say that states p and q are equivalent if for every word w, p • w ∈ F iff q • w ∈ F . In a minimal automaton, equivalent states are equal. We say that states p and q are almost equivalent if for every non-empty word w, p • w ∈ F iff q • w ∈ F . That is, non-empty words do not distinguish almost equivalent states; it follows immediately that after reading any letter the states become indistinguishable. Lemma 16. If states p and q are almost equivalent, then for each letter a, the states p • a and q • a are equivalent.</p><p>We shall call a state p of automaton A internal if it is reachable from the initial state via a nonempty word. Note that if all states are reachable, only the initial state can be non-internal, and it happens only iff it has no incoming transitions. Definition 17 (Almost-reversibility). We say that states p and q meet in state r if there exists a word u such that p • u = q • u = r; we say p and q meet if they meet in some state r. A deterministic automaton is almost-reversible if every two internal states that meet are almost equivalent. We call a regular language almost-reversible if its minimal automaton is almost-reversible.</p><p>As intended, the automaton in Figure <ref type="figure" target="#fig_11">3a</ref> is almost-reversible, while those in Figures 3b to 3d are not.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Lemma 18.</head><p>If L is an almost-reversible language, then QL is a registerless query.</p><p>Proof. Let A be the minimal automaton of L. The simulating automaton B will use the same states as A together with an additional rejecting sink state ⊥; the initial state and the set of accepting states are also like in A. When reading opening tags, B follows the transition relation of A. Upon reading a closing tag ā in a state p, B moves to some internal p in A such that p • a is almost equivalent to p. To keep B deterministic, we take the minimal such p according to an arbitrarily chosen order on the states of A. If such a state p does not exists, B moves to ⊥.</p><p>Consider an input tree T . For each prefix w of T , let w be the word obtained from w by successively erasing all two-letter subwords of the form aā for a ∈ Γ. If w ends with the opening tag of a node x in T , then w is the sequence of labels on the shortest path from the root of T to x. If w ends with the closing tag of a node x in T , then w is the sequence of labels on the shortest path from the root of T to the parent of x (if x is the root of T , then the path is empty). We claim that for every proper nonempty prefix w of T , the state p w of B after reading w is a an internal state of A that is almost equivalent to the state q w of A after reading w, and if the last letter of w is an opening tag, then p w = q w . The claim immediately implies that B realizes QL, because the first and the last state of B in the run on T does not matter.</p><p>We prove the claim by induction on |w|. The automaton B begins the computation in the initial state of A. The first letter of T is some opening tag a. Because a = a, we have p a = q a and p a is clearly internal. Suppose now that the claim holds for w. If the next letter after w is an opening tag c, applying Lemma 16 to the almost equivalent states p w and q w of A, we get p wc = p w • c = q w • c = q wc , and we are done because q w • c is clearly internal. Suppose that the next letter read by B is a closing tag c. We need to prove that there exists an internal state p in A such that p • c is almost equivalent to p w , and that every such p is almost equivalent to q wc . Consider p = q wc . Because wc is a proper prefix of T , the word wc is nonempty; hence, q wc is a internal state of A. We also have q wc • c = q w , and we have assumed that p w and q w are almost equivalent; hence, q wc • c is almost equivalent to p w . So, indeed, q wc is a correct choice for p . Let us now take any internal p with p • c almost equivalent to p w , and prove that p is almost equivalent to q wc . As p w and q w are almost equivalent by the induction hypothesis, it follows that so are p • c and q w . By Lemma 16,</p><formula xml:id="formula_7">p • c • b = q w • b = q wc • c • b for each b ∈ Γ.</formula><p>Hence, p and q wc meet. We have already argued that q wc is internal, and p is internal by assumption. Because A is almost-reversible, we conclude that p is almost equivalent to q wc . XX:14 Stackless</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Hierarchical almost-reversibility</head><p>We have already developed intuitions on evaluating RPQs over markup encodings using finite automata. Can we do more using the depth information and the (limited) ability to process it offered by the registers? Using one register and an additional component in the state, we can store the configuration of the simulated automaton in one node on the path from the root to the current node: we store the depth of this node in the register and the state of the simulated automaton in the additional component of the state of the simulating automaton. When the simulation climbs up to this depth again, we know to which state the simulated automaton should be reverted, regardless of the reversibility assumptions.</p><p>Using this feature we can simulate automata whose strongly connected components (SCCs) are singletons (Figure <ref type="figure" target="#fig_11">3b</ref>). Recall that an SCC is a maximal subset X of the state-space such that every state in X is reachable from every other state in X. If each SCC is a singleton, then a run may loop in some states it visits, but it never revisits a state it has left. Hence, in each run there is a bounded number of state changes. The simulating automaton can then represent the whole run of the simulated automaton over the path from the root to the current node by means of the list of state changes and depths at which these changes occurred. Automata with only singleton SCCs capture exactly the class of R-trivial languages, named after one of Green's relations from algebraic formal language theory <ref type="bibr" target="#b20">[21]</ref>; the intensively studied piecewise testable languages <ref type="bibr" target="#b18">[19]</ref> form a prominent subclass of R-trivial languages.</p><p>As we shall see, the potential of register automata is exhausted by the combination of the above simulation method with the full power of finite automata to simulate a run inside a single SCC. The class of automata that can be simulated this way is captured by the following definition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Definition 19 (Hierarchical almost-reversibility). A deterministic automaton is hierarchically almost-reversible, abbreviated as HAR, if every two states from the same SCC that meet inside this SCC are almost equivalent. A regular language is HAR if its minimal automaton is HAR.</head><p>By design, HAR languages include all almost-reversible languages (Figure <ref type="figure" target="#fig_11">3a</ref>), and all R-trivial languages (Figure <ref type="figure" target="#fig_11">3b</ref>), but also the language in Figure <ref type="figure" target="#fig_11">3c</ref> which is neither almostreversible nor R-trivial. The language in Figure <ref type="figure" target="#fig_11">3d</ref>, is not HAR.</p><p>As Definition 19 is invariant under the complementation of the automaton, we obtain the following.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Lemma 20. The complement of a HAR language is HAR.</head><p>Let us see that HAR languages can indeed be handled by depth-register automata.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Lemma 21.</head><p>If L is a HAR language, then QL is a stackless query.</p><p>Proof. Let L be a HAR language and A its minimal automaton. Like before, we construct a depth-register automaton B that evaluates QL by maintaining a simulation of the run of A on the word w labelling the path π from the root to the current node. It applies the method used for R-trivial languages to keep track of the changes of SCCs of A during the simulated run, and an adaptation of the method for almost-reversible languages to deal with the segments of the simulated run within a single SCC. After processing a prefix w of the encoding of the input tree, for each SCC X of A visited during the run on w, except the current one, the automaton B stores the depth of the deepest node on the path π whose label was read in a state from X during the run on w; and some state from X that meets in X with the last state from X visited by A in the run on w. Additionally, if q is the current state of A after processing w and Y is the SCC of A that contains q, the automaton B stores some state p ∈ Y that meets with q in Y , and p = q after reading each opening tag. Initially, p is the initial state i of A, and nothing else is stored.</p><p>Suppose that B reads an opening tag a and the current depth is d. Because A is HAR, the states p and q mentioned above are almost equivalent. As A is minimal, it follows from Lemma 16 that p • a = q • a. Consequently, p • a is the next state of A. If p • a ∈ Y , we just replace p with p • a and proceed to the next tag. If p • a belongs to some SCC Z = Y , we also add Y to the list of remembered SCCs, with depth d (loaded to some unused register) and state p, and continue with Z as the current SCC.</p><p>Suppose now that B reads a closing tag ā and the current depth d is greater than or equal to the maximal recorded depth d . This indicates that the previous state of A also belongs to Y . We should now revert A to some state q ∈ Y such that q • a = q, but we do not know which one. Even worse, we do not have access to q, but only to some state p ∈ Y that meets with q in Y . Nevertheless, we can maintain the invariant by picking any state p ∈ Y such that p • a ∈ Y is almost equivalent to p. Note first that such states p exist because q is one of them: q • a = q and from the previous case we know that q and p are almost equivalent. To keep B deterministic we pick the minimal such p according to some arbitrarily fixed order on the states of A. To prove that every p is suitable it suffices to show that p meets with q in Y . We know that p</p><formula xml:id="formula_8">• u = q • u ∈ Y for some word u. Because p • a is almost equivalent to p and A is minimal, we get p • a • u = p • u = q • u = q • a • u,</formula><p>and we are done. Hence, B can replace p with p and proceed to the next tag.</p><p>Finally, suppose B reads a closing tag ā and the current depth is strictly smaller than the greatest recorded depth d . This indicates that the previous state of A belongs to the SCC X = Y , associated with depth d . The automaton A should be reverted to the last state q from X visited during the run. The simulation does not have access to q , but it has the state p recorded for X, and we know that p meets with q in X. This is sufficient to maintain the invariant: the automaton B simply replaces p with p , removes X from the list of remembered SCCs marking the register storing the associated depth d as unused, and proceeds to the next tag with X as the current SCC.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Flatness</head><p>Not all finite languages are almost-reversible, as witnessed by the one in Figure <ref type="figure" target="#fig_11">3b</ref>. Nevertheless, if L is finite, then AL is registerless. Indeed, a finite automaton can simply simulate the stack up to the depth bounded by the length of the longest word in L. If an opening tag is read when the stack is at its maximum depth, the automaton moves to an all-rejecting sink state. Symmetrically, if L is co-finite (that is, L c is finite), then EL is registerless. This motivates the following dual notions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Definition 22 (E-flatness and A-flatness).</head><p>We call a state q acceptive (resp. rejective) if q • w is accepting (resp. rejecting) for some w ∈ Γ * . A deterministic automaton is E-flat (resp. A-flat) if for every internal state p and every rejective (resp. acceptive) state q, if p meets with q in q, then p is almost equivalent to q. A E-flat (resp. A-flat) language is a regular language whose minimal automaton is E-flat (resp. A-flat).</p><p>Checking that all finite languages (including the one in Figure <ref type="figure" target="#fig_11">3b</ref>) are A-flat, and all co-finite ones are E-flat is an easy exercise. The following lemma, connecting flatness to almost-reversibility is not hard either (see Appendix D).  Lemma 23. Let L ⊆ Γ * be a regular language.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>XX:16 Stackless</head><formula xml:id="formula_9">1. L is A-flat iff L c is E-flat.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">L almost-reversible iff it is both A-flat and E-flat.</head><p>More effort is needed to show that E-flatness of L is sufficient to simulate its minimal automaton faithfully enough to support recognizing EL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Lemma 24.</head><p>If L is an E-flat language, then EL is a registerless tree language.</p><p>Like for almost-reversible automata, the high-level idea is to maintain the state of the simulated automaton up to almost equivalence, except that we should immediately accept if this state becomes non-rejective. Because the internal structure of E-flat automata is much richer, we additionally need to keep track of transitions that moved the simulated run from one SCC to the next one. Taking transitions backwards when processing closing tags introduces certain ambiguity: the origins of the stored transitions are not single states, but pairs of states guaranteed to be almost equivalent. Full details can be found in Appendix E.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Inexpressibility</head><p>The results established in this section are proved by pumping simultaneously at the level of trees and their encodings, which resembles pumping arguments for context free grammars. To simplify factorizing encodings of trees, for a word w = a 1 a 2 • • • a n ∈ Γ * we let w = ān • • • ā2 ā1 (note the reversed order). Consider the tree S shown in Figure <ref type="figure" target="#fig_5">4a</ref>, keeping in mind that s, t, u, x are words rather than single letters: each node labelled with a word w represents a chain of |w| nodes whose labels form the word w. Then,</p><formula xml:id="formula_10">S = su n! xxū n! t tu n! xxū n! s .</formula><p>We use S in the proof of the following lemma.</p><p>Lemma 25. For a regular language L, if EL is a registerless tree language, then L is E-flat.</p><p>Proof. Suppose that the minimal automaton A of L ⊆ Γ * is not E-flat. Let i be the initial state of A. Then, there exist words s, t, u ∈ Γ + , x ∈ Γ * and states p, q such that i • s = p, p • u = q • u = q, q • x is rejecting, and p • t is accepting iff q • t is rejecting. It follows that for each k &gt; 0, su k x ∈ L c , and st ∈ L iff su k t ∈ L c .</p><p>Consider a deterministic finite automaton B over Γ ∪ Γ with n states. It is well known that r • w n! = r • w 2•n! for each nonempty word w and each state r of B (this is also implied by Lemma 28 established later in this section).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>XX:17</head><p>Consider the trees S and S shown in Figure <ref type="figure" target="#fig_5">4</ref>. By the discussion above, exactly one of those trees belongs to EL. Consider the runs of B on S and S . Suppose that on S we have</p><formula xml:id="formula_11">q 0 su n! ---→ q 1 xx•ū n! •t t•u n! xxū n! ------------→ q 2 s -→ q 3 .</formula><p>Then, by the choice of n, we have</p><formula xml:id="formula_12">q 0 su n! ---→ q 1 u n! --→ q 1 xx•ū n! •t t•u n! xxū n! ------------→ q 2 ūn! --→ q 2 s -→ q 3 .</formula><p>It follows that B accepts S iff it accepts S . Consequently, B does not recognize EL.</p><p>The missing implication in Theorem 14 is also proved by pumping, but requires considerably more effort because this time we need to fool a depth-register automaton. Before we dive into it, we prepare some simple tools helping to analyze runs of such automata; proofs of the auxiliary lemmas can be found in Appendix F.</p><p>For configurations c = (q, d, η) and c = (q , d , η ) of a depth-register automaton B we write c ∼ c if q = q .</p><p>For -∞ ≤ i ≤ j ≤ ∞, we write c ≈ i,j c if c ∼ c and for each register ξ one of the following conditions holds:</p><formula xml:id="formula_13">η (ξ) -d = η(ξ) -d; η(ξ) -d &lt; i and η (ξ) -d &lt; i and η(ξ) = η (ξ); η(ξ) -d &gt; j and η (ξ) -d &gt; j.</formula><p>We let ε = 0 and inductively wa = w + 1 and wā = w -1 for all a ∈ Γ and w ∈ (Γ ∪ Γ) * . For nonempty w we also define</p><formula xml:id="formula_14">w = min ε =u w u , w = max ε =u w u ,</formula><p>where u w means that u is a prefix of w. Note that for all w, w ≤ w ≤ w .</p><p>Lemma 26. Suppose that c 1 ≈ i,j c 2 . For every word w such that i ≤ w ≤ w ≤ j, it holds that</p><formula xml:id="formula_15">c 1 • w ≈ i-w ,j-w c 2 • w. A word x ∈ (Γ ∪ Γ) + is descending if 1 = x ≤ x = x and it is ascending if -1 = x ≥ x = x .</formula><p>Descending words generalize words from Γ + , and ascending words generalize words from Γ+ . For i, j</p><formula xml:id="formula_16">∈ Z ∞ = Z ∪ {-∞, ∞} we let [i, j] = {k ∈ Z ∞ i ≤ k ≤ j} , (i, j] = {k ∈ Z ∞ i &lt; k ≤ j} ,</formula><p>and analogously for [i, j) and (i, j).</p><formula xml:id="formula_17">Lemma 27. Let c i = (q i , d i , η i ) with i ∈ [1; 4] be configurations of a depth-register automaton B and let y, z ∈ (Γ ∪ Γ) + be descending words such that c 1 y -→ c 2 z -→ c 3 y -→ c 4 . If img(η 1 ) ⊆ (-∞; d 1 ] and c 1 ∼ c 3 , then img(η 4 ) ∩ (d 1 ; d 2 ] = ∅.</formula><p>Lemma 28. Let B be a depth-register automaton with k states and registers, and let n ≥ k • ( + 1). For every configuration c = (q, d, η) of the automaton B and every descending or ascending word</p><formula xml:id="formula_18">x ∈ (Γ ∪ Γ) + , if img(η) ∩ d + x 3•n! ; d + x 3•n! = ∅ , then XX:18 Stackless 1. c • x n! ∼ c • x n! • x n! ; and 2. c • x n! • x n! ≈ x n! -x n! , x n! -x n! c • x n! • x n! • x n! .</formula><p>Lemma 29. For each regular language L, if EL is a stackless tree language, then L is HAR.</p><p>Proof. Again, we prove the contrapositive. Suppose L ⊆ Γ * is not HAR. Then, its minimal automaton A admits states p, q, and r in the same SCC Y such that for some word u and some non-empty word t, we have r = p • u = q • u and p • t is accepting and q • t is non-accepting (in particular, p = q). Then, there exist v and w such that r • v = p and r • w = q. Finally, by minimality, all states are reachable from the initial state, so there exists a word s such that i • s = r. Because Y contains two different states, it is a non-trivial SCC. Consequently, for each state p ∈ Y there exists a nonempty looping word; that is, a word w = ε such that p • w = p . By appending suitable looping words if necessary, we can assume that the words s, u, v, w are nonempty as well. Additionally, it will be convenient to assume that |u| ≥ |t|; this can be ensured by appending |t| copies of the appropriate looping word to u. The resulting fragment of the automaton A is shown in the top left corner of Figure <ref type="figure" target="#fig_6">5</ref>. We have</p><formula xml:id="formula_19">s(wu + vu) * vt ⊆ L , s(wu + vu) * wt ⊆ L c .</formula><p>Consider a depth-register automaton B over Γ ∪ Γ with k states and registers. Let n = k • ( + 1). We shall construct a fooling pair of trees by unravelling the fooling gadget. The trees are shown in Figure <ref type="figure" target="#fig_6">5</ref>. The original tree R, is build from: (i) a tree R 0 consisting of a single branch labelled by the word s, (ii) trees R 1 , . . . , R 2•n!+1 that are isomorphic copies of the same tree, and (iii) a tree R 2•n!+2 consisting of a single branch labelled by the word wt. Each branch of R is labelled by a word from s(wu + vu) * wt ⊆ L c , which means that R / ∈ EL. The pumped tree R is obtained by inserting an additional segment labelled by (uv) n! in R n!+1 , just before the branching; we will write R n!+1 for thus modified R n!+1 . The modification introduces a branch labelled by a word from s(wu + vu) * vt ⊆ L, which means that R ∈ EL. We will show that the automaton B cannot distinguish R from R , by analyzing the respective runs in parallel. The crucial moments of the analysis will be configurations c i = (q i , d i , η i ), c i = (q i , d i , η i ), and c i = (q i , d i , η i ), depicted (with the exception of c 6 ) in brown in Figure <ref type="figure" target="#fig_6">5</ref>: configurations to the left of edges are visited when going down and those to the right when going up.</p><p>Let x be the prefix of R 1 ending at the opening tag of the rightmost leaf of R 1 . Because |t| ≤ |u|, the rightmost branch of R 1 is at least as long as both other branches, which implies that x is descending. Clearly, so is y = wu(vu) 2•n! ∈ Γ + . Consider the following initial segments of the runs of B over R and R :</p><formula xml:id="formula_20">c 0 sx n! ---→ c 1 y n! --→ c 2 w -→ c 3 (uv) 2•n! -----→ c 4 u -→ c 5 y n!-1 ---→ c 6 y -→ c 7 , c 0 sx n! ---→ c 1 y n! --→ c 2 w -→ c 3 (uv) 3•n! -----→ c 4 u -→ c 5 y n!-1 ---→ c 6 y -→ c 7 .</formula><p>Let δ = |(uv) n! |. As all words over the arrows are descending, we have</p><formula xml:id="formula_21">img(η i ) ⊆ [-∞; d i ] , img(η j ) ⊆ [-∞; d j ] , d j = d j + δ (1)</formula><p>for all i ∈ [0; 7] and j ∈ <ref type="bibr">[4; 7]</ref>. Condition (1) allows us to apply Lemma 28 to configuration c 3 and the descending word uv, and conclude that c 4 ≈ 1-δ,0 c 4 . By <ref type="bibr" target="#b0">(1)</ref>, this can be strengthened to c 4 ≈ 1-δ,∞ c 4 . By Lemma 26, we get </p><formula xml:id="formula_22">c 7 ≈ 1-(uv) n! u•y n! ,∞ c 7 . (<label>2</label></formula><formula xml:id="formula_23">)</formula><formula xml:id="formula_24">s c0 c13 (wu(vu) 2•n! ) n! w t (1) u(vu) 2•n! (wu(vu) 2•n! ) n! wt u(vu) 2•n! (wu(vu) 2•n! ) n! wu . . . (wu(vu) 2•n! ) n! w t (n! + 1) (uv) 2•n! u (wu(vu) 2•n! ) n! wt (uv) 2•n! u (wu(vu) 2•n! ) n! wu c1 c2 c3 c4 c5 c7 c12 c11 . . . (wu(vu) 2•n! ) n! w t (2 • n! + 1) u(vu) 2•n! (wu(vu) 2•n! ) n! wt u(vu) 2•n! (wu(vu) 2•n! ) n! wu c8 c9 wt c10 s c0 c 13 (wu(vu) 2•n! ) n! w t u(vu) 2•n! (wu(vu) 2•n! ) n! wt (1) u(vu) 2•n! (wu(vu) 2•n! ) n! wu . . . (wu(vu) 2•n! ) n! w (uv) n! t (n! + 1) (uv) 2•n! u (wu(vu) 2•n! ) n! wt (uv) 2•n! u (wu(vu) 2•n! ) n! wu c1 c2 c 12 c3 c 4 c 5 c 7 c 12 c 11 . . . (wu(vu) 2•n! ) n! w t (2 • n! + 1) u(vu) 2•n! (wu(vu) 2•n! ) n! wt u(vu) 2•n! (wu(vu) 2•n! ) n!</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>XX:20 Stackless</head><p>Applying Lemma 28 to c 2 and y, we get c 2 ∼ c 6 . Hence, we can apply Lemma 27 to configurations c 2 , c 5 , c 6 , c 7 and descending words y and y n!-1 . Combining the result with (2), we get</p><formula xml:id="formula_25">img(η 7 ) ∩ d 2 ; d 5 = ∅ , img(η 7 ) ∩ d 2 ; d 5 = ∅ .<label>(3)</label></formula><p>Consequently, from (2) we can also conclude</p><formula xml:id="formula_26">c 7 ≈ 1-y n!+1 ,∞ c 7 .<label>(4)</label></formula><p>Let y = wu(vu) 3•n! and take x 0 such that</p><formula xml:id="formula_27">y 2•n!+1 • x 0 = x. Consider c 0 sx n! ---→ c 1 y n! •y•y n! ------→ c 7 x0 -→ c 8 x n!-1 ---→ c 9 x -→ c 10 , c 0 sx n! ---→ c 1 y n! •y •y n! ------→ c 7 x0 -→ c 8 x n!-1 ---→ c 9 x -→ c 10 .</formula><p>Note that condition (1) holds for all i, j ∈ [8; 10]. From (4) via Lemma 26 we get</p><formula xml:id="formula_28">c 10 ≈ 1-y n!+1 wux n! ,∞ c 10 .<label>(5)</label></formula><p>Applying Lemma 28 to configuration c 0 • s and the descending word x, we get c 1 ∼ c 9 .</p><p>Applying Lemma 27 to configurations c 1 , c 8 , c 9 , c 10 and the descending words x and x n! , and combining the result with (5), we get</p><formula xml:id="formula_29">img(η 10 ) ∩ (d 1 ; d 8 ] = ∅ , img(η 10 ) ∩ (d 1 ; d 8 ] = ∅ .<label>(6)</label></formula><p>Hence, we can strengthen (5) to</p><formula xml:id="formula_30">c 10 ≈ 1-x n!+1 ,∞ c 10 . (<label>7</label></formula><formula xml:id="formula_31">) Let x = ū wȳ 2•n!+1 ; that is, xx = R 1 . Consider c 10 wt t w•x n! •ū w ȳn! •ū -----------→ c 11 (v ū) 2•n! -----→ c 12 w•ȳ n! •x n! •s -------→ c 13 , c 10 wt t w•x n! •ū w ȳn! •ū -----------→ c 11 (v ū) 2•n! -----→ c 12 (v ū) n! ----→ c 12 w•ȳ n! •x n! •s -------→ c 13 .</formula><p>We have d i = d i + δ for i ∈ [10; 12] and d i = d i for i ∈ <ref type="bibr">[12; 13]</ref>. By Lemma 26, we have</p><formula xml:id="formula_32">c 12 ≈ 1-w ,∞ c 12 . As from (6) it follows that img(η 12 ) ∩ (d 1 ; d 12 ) = img(η 12 ) ∩ (d 1 ; d 12 ) = ∅,</formula><p>we also have</p><formula xml:id="formula_33">c 12 ≈ 0,∞ c 12 . (<label>8</label></formula><formula xml:id="formula_34">)</formula><p>Applying Lemma 28 to configuration c 11 and the ascending word vū, we get c 12 ≈ 0,δ-1 c 12 .</p><p>In combination with <ref type="bibr" target="#b7">(8)</ref> this implies c 12 ≈ 0,δ-1 c 12 . Because d 12 = d 12 , it follows that c 12 ≈ -∞,δ-1 c 12 . By Lemma 26, this implies c 13 ∼ c 13 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Discussion</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Tree languages defined by DTDs</head><p>Our characterization results shed some light on the registerlessness of DTDs, studied in <ref type="bibr" target="#b24">[25]</ref> (called recognizability there). A DTD D over Γ consists of an initial symbol a 0 ∈ Γ and, for each a ∈ Γ, a production of the form a → L a where L a is a regular language over Γ (typically  </p><formula xml:id="formula_35">+ ã) * , b → (a + b + ã) * , ã → c * , c → (a + b) * with alphabet projection a → a, ã → a, b → b, c → c.</formula><p>represented as a regular expression). It defines the set of trees T over Γ that have a 0 in the root and for each a-labelled node v in T , the labels of v's children read from left to right form a word in L a . A specialized DTD over Γ consists of a DTD D over Γ and an alphabet projection π : Γ → Γ; the language it defines is the projection of the language defined by D to the alphabet Γ.</p><p>Languages of the form AL capture (resp. capture precisely) all tree languages definable by DTDs (resp. specialized DTDs) using only productions of the forms</p><formula xml:id="formula_36">a → (b 1 + • • • + b n ) * , a → (b 1 + • • • + b n ) + .</formula><p>This is a severely restricted, yet non-trivial and practically relevant, special case of the setting considered in <ref type="bibr" target="#b24">[25]</ref>. Let us refer to such DTDs as path DTDs. A path DTD is almost an automaton recognizing allowed paths: use (specialized) symbols as states, add a transition from a to each b i over symbol b i (or its projection π(b i ) in the case of specialized DTDs), and let a be accepting if the production uses * , and non-accepting if it uses + (see Figure <ref type="figure" target="#fig_8">6a</ref>).</p><p>It can be shown that under restriction to path DTDs, the first necessary condition for registerlessness proposed in <ref type="bibr" target="#b24">[25]</ref> reduces to the assumption that the corresponding automaton is HAR, and the second one amounts to A-flatness. Segoufin and Vianu show that the first necessary condition is also sufficient under the restriction to fully-recursive DTDs, which correspond to automata that have only two non-trivial SCCs: one contains the initial state, and the other is an all-rejecting sink. For such automata, HAR is equivalent to A-flat, which makes their result a special case of Theorem 15 (2) (in the limited special case of path DTDs). Segoufin and Vianu also conjecture, that the two necessary conditions together are sufficient for all DTDs. Theorem 15 (2) confirms this conjecture in the special case of path DTDs. Let us remark that A-flatness works also for languages defined by specialized path DTDs, but the corresponding automaton must be determinized and minimized before the criterion is applied, as witnessed by the specialized DTD in Figure <ref type="figure" target="#fig_8">6</ref> which gives an A-flat non-deterministic automaton, which is not A-flat any more after determinizing and minimizing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">A different encoding of trees</head><p>An alternative way to serialize tree-structured data, used for instance in JSON, is the term encoding, in which the information about the label is included only in opening tags. For instance, instead of abaāaā bccā we would have a{b{a{}a{}}c{}}, where a{ , b{ , c{ are opening tags, and } is the universal closing tag. Streaming processing under this encoding is harder, but analyzing it is easier. An effective characterization of regular tree languages that are registerless under the term encoding is given in <ref type="bibr" target="#b0">[1]</ref>.</p><p>Our treatment can be easily adapted to the term encoding by adjusting the definition of when two states meet: we say that states p and q blindly meet in state r if there exist words</p><formula xml:id="formula_37">u 1 , u 2 ∈ Γ * such that |u 1 | = |u 2 | and p • u 1 = q • u 2 = r.</formula><p>By replacing 'meet' with 'blindly meet' in Definitions 17, 19 and 22, we get the definitions of the syntactic classes of blindly almost-reversible, blindly HAR, blindly A-flat, and blindly E-flat word languages. Theorems 14 and 15 then hold for the term encoding with all syntactic classes of word languages replaced by their blind analogues (see Appendix G). Based on this, it can be checked by direct examination of the automata in Figure <ref type="figure" target="#fig_11">3</ref> that also under the term encoding, the first RPQ from Example 12 is registerless, the following two are stackless but not registerless, and the last one is not stackless. Nevertheless, 'blind' classes are much more restricted than their originals: all R-trivial languages are blindly HAR, but the possibilities of backtracking inside an SCC are very limited. For example, the minimal automaton shown in Figure <ref type="figure">2</ref> is reversible, but not blindly-HAR; this means that the language (b * a b * a b * ) * this automaton recognizes is registerless under the markup encoding, but not even stackless under the term encoding. This is the cost of succinctness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Outlook</head><p>In this work we have proposed an intermediate model for processing streamed trees, increasing the expressive power of finite automata considerably while sparing us the maintenance of the stack. We have effectively characterized unary RPQs that can be realized in this model, and those that can be realized by finite automata. The latter leads to a partial solution of the weak validation problem posed by Segoufin and Vianu <ref type="bibr" target="#b24">[25]</ref>.</p><p>The weak validation problem remains the most intriguing theoretical challenge in the area. Other salient problems are to characterize (effectively) stackless tree languages among regular ones and, conversely, regular tree languages among stackless ones. The former is more relevant for query and schema processing, but the latter may provide some useful insights as well.</p><p>Solving these problems for all regular tree languages might be very hard, but for more restricted, yet practically relevant, subclasses it might be easier. For instance, it would be very useful to be able to decide if a given XPath expression is stackless or registerless (both as a boolean query and as a unary query). Examples 7, 9 and 10 suggest that stackless XPath expressions would have to use child, next-sibling, following-sibling, and negation extremely cautiously, but this might be alleviated by including schema information into the setting.</p><p>Applying our results on the term encoding to JSON would also involve incorporating rudimentary schema information, as in JSON siblings either have different labels, or have no labels at all.</p><p>Finally, a major question is how to vectorize. A first step would be to uncover the local parallelism of pushdown and depth-register automata, in the spirit of <ref type="bibr" target="#b14">[15]</ref>. Closing the distance to actual applications will require replacing circuits with a more faithful abstraction of the capabilities of CPUs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Proof of Proposition 3</head><p>Let us recall the statement.</p><p>Proposition 30. Restricted depth-register automata recognize regular tree languages.</p><p>Proof. Consider a restricted depth-register automaton</p><formula xml:id="formula_38">A = (Γ, Q, q init , F, Ξ, δ) .</formula><p>The run of A on a tree T can be represented by means of an auxiliary labelling of the nodes of T with elements of</p><formula xml:id="formula_39">2 Ξ × Q × 2 Ξ × 2 Ξ × Q</formula><p>where for each node v in T , if v gets auxiliary label (X, p), Y, (Z, q) then upon reading the opening tag of v, A loads the current depth to registers in X and moves to state p; when processing the infix of T delimited (exclusively) by the opening and closing tags of v, A loads some current depth to exactly those registers that belong to Y ; upon reading the closing tag of v, A loads the current depth to registers in Z and moves to state q. In what follows, we shall refer to q as the exit state of v.</p><p>The correctness of the auxiliary labelling can be equivalently expressed in a more local way, relying on the transition function of A. Suppose that a node v has label a in T and auxiliary label (X, p), Y, (Z, q) , and its children v 1 , v 2 , . . . , v n have labels a 1 , a 2 , . . . , a n in T and auxiliary labels (X i , p i ), Y i , (Z i , q i ) for i ∈ {1, 2, . . . , n}. Then,</p><formula xml:id="formula_40">Y = n i=1 X i ∪ Y i ∪ Z i</formula><p>and for all i ∈ {1, 2, . . . , n},</p><formula xml:id="formula_41">(X i , p i ) = δ(p i , a i , Ξ, ∅) , (Z i , q i ) = δ(q i , a i , Ξ \ (X i ∪ Y i ), X ∪ Z 1 ∪ . . . Z i-1 ∪ X i ∪ Y i ) ,</formula><p>where p 1 = p and p i = q i-1 for i ∈ {2, . . . , n}; q i = p i if v i is a leaf and otherwise q i is the exit state of the last child of v i . If v is the root of T , it must also hold that (X, p) = δ(q init , a, Ξ, ∅) and (Z, q) = δ(q , a, Ξ \ Y, Ξ) , where q = p if v is a leaf and otherwise q is the exist state of the last child of v (that is, q = q n ). The rephrased condition is equivalent to the original one precisely because in a restricted depth-register automaton we have the guarantee that X i ∪ Y i ⊆ Z i .</p><p>To show that the tree language recognized by A is regular it suffices to observe that it can be recognized by a nondeterministic tree automaton that guess an auxiliary labelling of the input tree, checks its correctness by verifying the rephrased condition, and accepts if the second state in the auxiliary label of the root belongs to F .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Proof of Proposition 11</head><p>Let us recall the statement of the proposition.</p><p>Proposition 31. The class of stackless queries invariant under sibling order is contained in the class of RPQs.</p><p>Proof. Consider a query Q invariant under sibling order, realized by a depth-register automaton B. It follows immediately, that Q is fully described by the answers it gives on the leftmost branch of every tree. But these answers are determined by the word on the path from the root to the current node. Hence, Q is a path query and it is fully described by its behaviour on single-branch trees. Consider the run of B on the prefix of the encoding of such a tree, consisting of all opening tags. In such a run, the current depth is always strictly greater than all values stored in the registers, so the registers can be eliminated from the automaton. Over single-branch trees, the resulting finite automaton over Γ ∪ Γ selects the same nodes as B. By restricting the alphabet to Γ, we obtain an automaton recognizing L.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Proof of Proposition 13</head><p>We recall the formulation of the proposition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proposition 32. It is decidable if the query realized by a given restricted depth-register automaton is an RPQ.</head><p>Proof. By Proposition 11, a stackless query is an RPQ iff it is a path query. We phrase the argument for the latter property.</p><p>A marked tree over Γ is a tree over Γ × {0, 1}; marked nodes in such a tree are those with labels from Γ × {1}. Let us fix a unary query Q. For a tree T over Γ we let T Q be the marked tree over Γ obtained from T by marking nodes from Q(T ). Let M Q be the set of all such T Q with T ranging over all trees over Γ. The query Q is a path query if and only if there exists a language L over Γ × {0, 1} such that M Q = M L , where M L is the set of marked trees over Γ where each direct path from the root to a marked node is labelled with a word from L. Moreover, if Q is a path query, then we can take for L the language L Q obtained by restricting M Q to trees that consist of a single branch with marked leaf. Hence, Q is a path query if and only if</p><formula xml:id="formula_42">M Q = M L Q .</formula><p>It is easy to turn this characterization into an algorithm. Suppose that we are given a restricted depth-register automaton A and let Q be the query it realizes. Based on (the proof of) Proposition 3, it is easy to construct a tree automaton B recognizing M Q . Next, we intersect B with a tree automaton recognizing single-branch trees with marked leaf, interpret the result as a word automaton, and thus obtain an automaton C that recognizes the language L Q . Finally, we easily turn C into a tree automaton D recognizing M L Q . Thus, testing if the query realized by A is a path query reduces to testing if the tree automata B and D are equivalent, which is well known to be decidable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Proof of Lemma 23</head><p>Let us recall the statement of the lemma.</p><p>Lemma 33. Let L ⊆ Γ * be a regular language.</p><p>Barloy, Murlak and Paperman XX:27</p><formula xml:id="formula_43">1. L is A-flat iff L c is E-flat.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">L almost-reversible iff it is both A-flat and E-flat.</head><p>Proof. Let A be the minimal automaton of L. Then A c , obtained from A by swapping accepting and rejecting states, is the minimal automaton of L c . A state q is acceptive in</p><formula xml:id="formula_44">A iff it is rejective in A c . It follows that A is A-flat iff A c is E-flat.</formula><p>For the second part, observe that states p and q in Definition 22 are internal, so every almost-reversible automaton is A-flat and E-flat. For the converse, consider a minimal automaton A that is A-flat and E-flat. We begin with an auxiliary claim.</p><p>We call an SCC X a sink if for each q ∈ X and each u ∈ Γ * , q • u ∈ X. We claim that if a sink SCC X is reachable from an internal state p, then X contains a state q that is almost equivalent to p. Indeed, suppose that p • w ∈ X. Because X is a sink, p • w n ∈ X for all n &gt; 0. Consequently, there exist n, k &gt; 0 such that p • w n = p • w n • w k . Moving n positions backwards in the cyclic list of states p • w n , p • w n+1 , . . . , p • w n+k-1 , starting from p • w n , we find a state q = p • w n+k-n mod k ∈ X that meets with p. Because X is a sink, p and q can only meet in some r ∈ X. But then p and q also meet in q. Because q is either rejective or acceptive, and A is both E-flat and A-flat, it follows that p and q are almost equivalent.</p><p>To see that A is almost-reversible, take two internal states p 1 and p 2 that meet. Then, p 1 and p 2 meet in some sink SCC X. Consequently, there exists a non-empty word w and state r ∈ X such that p 1 • w = p 2 • w = r. By the auxiliary claim, X contains states q 1 and q 2 that are almost equivalent to p 1 and p 2 , respectively. By Lemma 16 and the minimality of A, we get q</p><formula xml:id="formula_45">1 • w = p 1 • w = r = p 2 • w = q 2 •</formula><p>w; that is, the states q 1 , q 2 ∈ X meet in X. Consequently, q 1 meets with q 2 in q 2 , and because q 1 is obviously internal, it follows by E-flatness or A-flatness that q 1 and q 2 are almost equivalent. It follows that p 1 and p 2 are almost equivalent, too.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Proof of Lemma 24</head><p>Let us recall the statement of the lemma.</p><p>Lemma 34. If L is an E-flat language, then EL is a registerless tree language.</p><p>Proof. Let A be the minimal automaton of L. We first construct an automaton B simulating A in a certain precise sense, and then we turn B into an automaton recognizing EL. Like in the simulation of almost-reversible automata, the high-level idea is to maintain the state of A after processing w up to almost equivalence, except that if at any point the maintained state becomes non-rejective, the simulating automaton moves to an all-accepting sink state . But because the internal structure of E-flat automata is much richer then that of almost-reversible ones, the simulating automaton B needs more information.</p><p>After reading a prefix w of the encoding of the input tree, the simulating automaton B will store a synopsis of the run of A on w. The goal of the synopsis is to list the transitions that moved the run from one SCC of A to the next one. However, because the automaton A is not reversible, taking the transitions backwards when processing closing tags will introduce certain ambiguity into the stored transitions. Namely, the origins of the transitions will be split states, defined as pairs (p, q) such that q is rejective and either p = q or p is internal and meets with q in q. E-flatness guarantees that for each split state (p, q), the states p and q are almost equivalent. By minimality, transitions from split states have unambiguous targets.</p><p>A split transition is a tuple (p, q, a, r) such that (p, q) is a split state and p • a = q • a = r. A synopsis for A is an alternating sequence of state triples and letters, written as (r 0 , p 0 , q 0 ) a1</p><formula xml:id="formula_46">-→ (r 1 , p 1 , q 1 ) a2 -→ • • • a -→ (r , p , q ) ,<label>(9)</label></formula><p>such that r 0 is the initial state of A, each (p i , q i , a i+1 , r i+1 ) is a split transition in A, (p , q ) is a split state in A, and for each i &lt; , the states q i and r i+1 are in different SCCs; for each i ≤ , q i belongs to the SCC of r i and either p i belongs to the SCC of r i or i &gt; 0 and p i = p i-1 = q i-1 . Observe that the states q i represent a chain of different SCCs, so + 1 is bounded by the depth of the DAG of SCCs of A.</p><p>The empty word ε is compatible only with synopses (r 0 , p 0 , q 0 ) with r 0 ∈ {p 0 , q 0 }. For u ∈ Γ * and a ∈ Γ, the word ua is compatible with a synopsis σ of the form (9) if r 0 •ua ∈ {p , q } and one of the following holds: (a) r 0 • u is in the SCC of r 0 • ua, and u is compatible with the synopsis obtained from σ by replacing (r , p , q ) with (r , r 0</p><formula xml:id="formula_47">• u, r 0 • u); (b) &gt; 0, r 0 • u ∈ {p -1 , q -1 }, a = a ,</formula><p>and u is compatible with the synopsis obtained from σ by removing the suffix a -→ (r , p , q ); (c) &gt; 0, r 0 • ua = p = p -1 = q -1 , and ua is compatible with the synopsis obtained from σ by removing the suffix a -→ (r , p , q ). Note that if some u is compatible with σ and r 0 • u = p , then u is compatible with every synopsis obtained from σ by replacing q with some other state; similarly with p and q swapped.</p><p>The states of B include all synopses for A and two sink states: all-accepting and all-rejecting ⊥. The simulation invariant is that after processing a proper prefix w of the encoding of the input tree, either B is in the state and r 0 • v is non-rejective for some prefix v of w, or B is in a synopsis state σ and w is compatible with σ and if the last symbol of w is an opening tag then p = q .</p><p>Let r 0 be the initial state of A. If r 0 is rejective, the initial state of B is (r 0 , r 0 , r 0 ); otherwise, it is . The invariant clearly holds before the first tag is processed. Let us see how to define transitions from a synopsis state σ of the form (9) to propagate the invariant.</p><p>Suppose that an opening tag a is read and let s = p • a = q • a. If s is not rejective, move to . If s is rejective and belongs to the SCC of q , continue with (r , p , q ) replaced with (r , s, s) in σ. If s is rejective but does not belong to the SCC of q , continue with a -→ (s, s, s) appended to σ. The invariant propagates.</p><p>Suppose a closing tag ā is read. If p is not internal, then p = q = r 0 , which is only possible if σ = (r 0 , r 0 , r 0 ). The automaton B then moves to ⊥. Assume that the invariant holds before ā is processed. Then, r 0 • w = r 0 . Because r 0 is not internal, it follows that w is empty. Hence, wā = ā, which is not a prefix of the encoding of any tree, and the state of B after processing wā does not matter. If p is internal, we consider four cases depending on whether p and q are in the same SCC of A, and whether the shape of the synopsis allows backtracking via a transition that originates outside of the SCC of q .</p><p>Case A: p and q are in the same SCC X, and either r / ∈ {p , q } or a = a or p -1 is not internal; that is, we can only take (backward) transitions within X. Consider P = p ∈ X p • a ∈ {p , q } . Because X contains the internal state p and the rejective state q , all states in X are internal and rejective. The same holds for P ⊆ X. Pick any two p, q ∈ P . Because p and q meet inside X, so do p and q. It follows that p and q meet in q. Hence, (p, q) is a split state, and p and q are almost equivalent. In a minimal automaton there can be at most two different almost equivalent states, so |P | ≤ 2. If P = ∅, then B moves to ⊥. Otherwise, P = {p , q } for some p and q , and B continues, replacing (r , p , q ) with (r , p , q ). Suppose that the XX:31 Lemma 37. Let B be a depth-register automaton with k states and registers, and let n ≥ k • ( + 1). For every configuration c = (q, d, η) of the automaton B and every descending or ascending word x ∈ (Γ ∪ Γ) + , if</p><formula xml:id="formula_48">img(η) ∩ d + x 3•n! ; d + x 3•n! = ∅ , then 1. c • x n! ∼ c • x n! • x n! ; and 2. c • x n! • x n! ≈ x n! -x n! , x n! -x n! c • x n! • x n! • x n! .</formula><p>Proof. It is well known that for every deterministic finite automaton A over Γ ∪ Γ with at most n states, p • w n! = p • w n! • w n! for every state p and every word w. To see why this is the case, let us analyze the evolution of the state after processing successive copies of w. Already after processing at most n copies a state will repeat, and because A is deterministic, we will start looping around a cycle in A. After processing all n! copies we are still on the cycle, of course. After processing any number of copies that is divisible by the length of the cycle (measured in the number of w-steps, not single letters), we return to the same state. Because the length of the cycle is at most n, and n! is divisible by every number between 1 and n, the claim follows.</p><p>The lemma is proved in a similar fashion. Suppose x is descending; the argument for ascending x is entirely analogous. Throughout the run on x n! • x n! • x n! from c, the current depth stays within d + x 3•n! ; d + x 3•n! . Consequently, comparisons with values from img(η) give the same result at every step of this run. Moreover, because x is descending, depths stored when processing the ith copy of x are all strictly smaller than every depth that occurs when processing the jth copy of x for all j &gt; i. Consequently, the behaviour of B when processing the (i + 1)st copy of x is determined by the state and the set of registers storing values not greater than the current depth-after processing the ith copy of x. Because the set of registers can only grow as the successive copies of x are processed, after processing at most k • ( + 1) copies of x a state-set pair will repeat. Because the sets only grow, all state-pairs in between share the same set. It follows that when processing subsequent copies of x, this sequence of state-pairs will repeat in a cyclic fashion. Because the length of this sequence is at most k • ( + 1), it follows like before that the state-set pairs corresponding to c • x n! and c • x n! • x n! coincide. This implies item (1) of the lemma. In configuration c • x n! • x n! some registers store the same value from -∞; d + x n! ∪ d + x 3•n! ; ∞ that they stored in configuration c • x n! , and into the remaining registers some values from</p><formula xml:id="formula_49">d + x n! ; d + x 2•n!</formula><p>were loaded when the second copy of x n! was being processed. Because the state-set pairs corresponding to c • x n! and c • x n! • x n! coincide, processing the third copy of x n! will load into the same registers the corresponding (that is, shifted by x n! ) values, and no other load operations will be performed. This implies item (2) of the lemma.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G Blind classes</head><p>We  Correspondingly, for u = a 1 a 2 • • • a n ∈ Γ * we let ū = n . Like for the markup encoding, we let [L] = [T ] T ∈ L for every tree language L over Γ.</p><p>A tree language L over Γ is term-registerless (resp. term-stackless) if there exists a finite automaton (resp. depth-register automaton) over Γ ∪ { } that accepts all words from [L] and rejects all words from [L c ]. A unary query Q is term-registerless (resp. term-stackless) if there exists a finite automaton (resp. depth-register automaton) over Γ ∪ { } that pre-selects nodes in Q(T ) when running over <ref type="bibr">[T ]</ref>.</p><p>We say that states p and q blindly meet in state r if there exist words u 1 , u 2 ∈ Γ * such that |u 1 | = |u 2 | and p • u 1 = q • u 2 = r. By replacing 'meet' with 'blindly meet' in Definitions 17, 19 and 22, we get the definitions of the syntactic classes of blindly almost-reversible, blindly HAR, blindly A-flat, and blindly E-flat word languages.</p><p>Theorem 38. Let L be a regular language. 1. EL is a term-registerless tree language iff L is blindly E-flat. 2. AL is a term-registerless tree language iff L is blindly A-flat. Proof. The argument is fully analogous to that in Theorem 15, with Lemmas 18 and 23 to 25. replaced by their analogues for term-registerless, blindly E-flat, blindly A-flat, and blindly almost-reversible languages. The analogue of Lemma 18 states that if L is a blindly almost-reversible language, then QL is a term-registerless query. The proof is almost identical, except that when the closing tag is read in state p, we pick any state p such that p • a is almost equivalent to p for some a ∈ Γ; because L is blindly almost-reversible, the original argument now shows also that the choice of a does not matter.</p><p>The analogue of Lemma 23 states that a regular language is blindly A-flat iff its complement is blindly E-flat, and that it is blindly almost-reversible iff it is both blindly A-flat and blindly E-flat; it is proved just like the original.</p><p>The analogue of Lemma 24 states that if L is blindly E-flat, then EL is term-registerless. The proof is an adaptation of the original one to the blind setting. The states of the simulating finite automaton, the simulation invariant, the transitions over opening tags, and the transformation into an automaton recognizing EL are entirely analogous, with 'meet' replaced everywhere with 'blindly meet'; in particular, we keep the labels a 1 , . . . , a in the synopsis. However, the behaviour of the simulating automaton over the closing tag needs to</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Example 9 .</head><label>9</label><figDesc>Consider the pattern π shown in Figure1a. As is customary, we use double edges to indicate descendent relationships between nodes. Suppose that the languages of</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1</head><label>1</label><figDesc>Figure 1 Strict descendent patterns are not stackless.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Example 12 .</head><label>12</label><figDesc>Consider the following simple RPQs, expressed in XPath, JSONPath, and as regular expressions: XPath /a//b /a/b //a//b //a/b JSONPath $.a..b $.a.b $..a..b $..a.b RegEx a Γ * b a b Γ * a Γ * b Γ * a b Registerless? Stackless?</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 AFigure 3</head><label>23</label><figDesc>Figure 2 A reversible finite automaton.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4</head><label>4</label><figDesc>Figure 4 Fooling trees in Lemma 25.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5</head><label>5</label><figDesc>Figure 5 Non-HAR gadget and fooling trees in Lemma 29.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 6</head><label>6</label><figDesc>Figure 6 Automata corresponding to the specialized DTD a → (a + b + ã) * , b → (a + b + ã) * , ã → c * , c → (a + b) * with alphabet projection a → a, ã → a, b → b, c → c.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>shall use the symbol for the universal closing tag. The term encoding [T ] ∈ Γ ∪ { } * of a tree T with a-labelled root and immediate subtrees T 1 , T 2 , . . . , T n is [T ] = a [T 1 ] [T 2 ] . . . [T n ] .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 7</head><label>7</label><figDesc>Figure 7 Blind variants of fooling trees in Lemma 25.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>3 .</head><label>3</label><figDesc>The following conditions are equivalent: a. QL is a term-registerless unary query; b. EL and AL are term-registerless tree languages; c. L is blindly E-flat and blindly A-flat; d. L is blindly almost-reversible.</figDesc></figure>
		</body>
		<back>

			<div type="funding">
<div><p>Funding <rs type="person">Filip Murlak</rs>: This work was supported by <rs type="funder">Poland's National Science Centre</rs> grant <rs type="grantNumber">2018/30/E/ST6/00042</rs>. <rs type="person">© C. Barloy</rs>, <rs type="person">F. Murlak</rs> and <rs type="person">C. Paperman</rs>; licensed under Creative Commons</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_EdnfHMd">
					<idno type="grant-number">2018/30/E/ST6/00042</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>XX:29</head><p>invariant holds before ā is processed. If it holds by (a), then r 0 • wā ∈ P = {p , q } and wā is compatible with the synopsis obtained from σ by replacing (r , p , q ) with (r , r 0 • wā, r 0 • wā). Suppose that the invariant holds by (b). This implies that r ∈ {p , q } and a = a , so it must be the case that p is not internal. Then q is equal to p , so not internal either. By (b), r 0 • wā ∈ {p -1 , q -1 }, so it is non-internal too. Consequently, wā is the empty word, which is possible only if wā is the complete encoding of the input tree. But then the invariant is not required to hold. Finally, the invariant cannot hold by (c), because it would imply that q -1 and q are in the same SCC, which is forbidden by the definition of synopsis.</p><p>Case B: p and q are in the same SCC X, and also r ∈ {p , q }, a = a , and p -1 is internal; that is, we can also take (backward) transitions that leave X. Note that this is possible only if &gt; 0. Consider again the set P ⊆ X introduced above. If P = ∅, then B continues, removing the suffix a -→ (r , p , q ) from the synopsis. In this case, only the condition (b) of the invariant might hold before processing ā, so wā is compatible with the modified synopsis, and the invariant propagates. Assume that P is nonempty. Let p ∈ {p -1 , q -1 } and q ∈ P . We know that p • a and q • a belong to {p , q }, and that p and q meet in X, so we also have that p and q meet in X. Because q ∈ P ⊆ X, it follows that p and q meet in q . As p -1 is assumed to be internal, so is q -1 , and consequently also p . The state q is rejective because all states in P are. It follows that (p , q ) is a split state, so p and q are almost equivalent. Because p ∈ {p -1 , q -1 } ⊆ X c and q ∈ P ⊆ X, we conclude that p = q . Using again the fact that there are at most two different almost equivalent states in every minimal automaton, we get that p = p -1 = q -1 and {q } = P . The automaton B continues, replacing (r , p , q ) with (r , p , q ) in the synopsis σ. If the invariant holds before processing ā, then either (a) or (b) holds. If (a) holds, then r 0 • wā = q , and the invariant propagates like before. If (b) holds, then r 0 • wā = p = p -1 = q -1 , and after processing ā, (c) will hold.</p><p>Case C: q is in SCC X but p / ∈ X, and either r / ∈ {p , q } or a = a . We then have p = p -1 = q -1 . Suppose p • a = p for some internal p and q • a = q for some q ∈ X. Then it easily follows that p meets with q in q, and so p and q are almost equivalent. Consequently, p • a = p and q • a = q are equal, which is impossible because p / ∈ X. Thus, p and q cannot both exist.</p><p>If p does not exist, B moves to the state it would take from the synopsis σ obtained from the current one by replacing (r , p , q ) with (r , q , q ) in σ. Note that σ falls into Case A. Suppose that the invariant holds before processing ā. If it is by (a), then r 0 • w = q , so w will also be compatible with σ and the invariant will propagate as shown in Case A. The invariant cannot hold by (b), because this would imply that r ∈ {p , q } and a = a , and we have assumed the contrary. Suppose that the invariant holds by (c). Then (r 0 • wā) • a = r 0 • w = p . But, as we have shown, there are no internal states p such that p • a = p . Hence, r 0 • wā is a noninternal state. This is possible only if wā is empty. Then, wā is the whole encoding of the input tree, and the invariant is not required to hold any more.</p><p>If q does not exist, the state is chosen similarly, but this time we obtain σ by removing the suffix a -→ (r , p , q ) from σ. Note that σ falls into Case A or Case B: p -1 is internal because it is equal to p , and p -1 and q -1 are in the same SCC because they are equal. If the invariant holds before processing ā, then it must be by (c). Then, w will also be compatible with σ , and the invariant will propagate as shown in Cases A and B.</p><p>Case D: q is in SCC X but p / ∈ X, and both r ∈ {p , q } and a = a . It then follows that p = p -1 = q -1 and r = q . Consequently, p • a = q and, because p and q are almost equivalent, q • a = q . Suppose that p • a = p for some internal state p. Then, we have p • aa = q • aa = q ; that is, p meets with q in q . Since q is rejective, it follows XX:30 Stackless that p and q are almost equivalent. But that means that p = p • a = q • a = q , which is impossible because p / ∈ X. Hence, no such p exists. Suppose that q • a = q for some q ∈ X \ {q }. Then q • a = q • a = q and it follows that q is almost equivalent to q . But this is impossible because together with p / ∈ X this would give three different almost equivalent states. Hence, such q also does not exist. We let B continue with the same synopsis. Suppose that the invariant holds before processing ā. If it is by (a), then r 0 • wā = q , because it is the only state in X from which the transition over a leads to {p , q }, and the invariant propagates. If the invariant holds by (b), then r 0 • wā ∈ {p -1 , q -1 }, but p -1 = q -1 = p , so for wā and σ we will have (c). Finally, if the invariant holds by (c), it follows that wā is the whole encoding of the input tree, like in the first subcase of Case C, and the invariant is not required to hold any more.</p><p>This completes the construction of B and the proof that every run of B over the encoding of a tree T satisfies the invariant. Directly from the invariant it follows that after reading a prefix wa of T for a opening tag a, we have p = q = r 0 • wa. To recognize EL it suffices to enrich the synopsis states of B with the information about the most recently read tag, and move directly to whenever a closing tag ā is read in a state storing the opening tag a and a synopsis with p = q accepting in A. The resulting automaton B enters in the situation described above or if it encounters a prefix v of the encoding such that r 0 • v is not rejective. In the first case, the automaton B has detected a leaf such that the branch leading to it is labelled by a word from L. In the second case, B has detected a node such that each branch containing this node is labelled by a word from L. Correctness of B follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F</head><p>Proofs of lemmas from Section 3.4</p><p>We recall the formulations of the lemmas.</p><p>Lemma 35. Suppose that c 1 ≈ i,j c 2 . For every word w such that i ≤ w ≤ w ≤ j, it holds that c 1 • w ≈ i-w ,j-w c 2 • w.</p><p>Proof. It suffices to show the lemma for the case when w is a single letter; the general claim follows by straightforward induction on the length of w. Suppose that w = a ∈ Γ. Then, w = w = 1. Because c 1 ≈ i,j c 2 and i ≤ 1 ≤ j, it follows the same transition over a will be taken from c 1 and c 2 . After the transition is taken, the absolute thresholds between the three kinds of behaviour of registers listed in the definition of ≈ do not change, but because the current depth increases by one, the relative thresholds have to be adjusted. This gives precisely c 1 • a ≈ i-1,j-1 c 2 • a. For w = ā the argument is entirely analogous. Lemma 36. Let c i = (q i , d i , η i ) with i ∈ [1; 4] be configurations of a depth-register automaton B and let y, z ∈ (Γ ∪ Γ) + be descending words such that c 1</p><p>Proof. Because y and z are descending, from img(η 1 ) ⊆ (-∞; d 1 ] it follows that img(η 3 ) ⊆ (-∞; d 3 ]. Combining this with c 1 ∼ c 3 , we conclude that from configurations c 1 and c 3 the same sequence of transitions will be taken while processing y. But this implies that if a depth d ∈ (d 1 ; d 2 ] was stored in some register ξ while processing y from c 1 , the corresponding depth d ∈ (d 3 ; d 4 ] will be stored in ξ while processing y from c 3 . That is, each depth stored when the first copy of y was processed, is overwritten when the second copy of y is processed. Because img(η 1 ) ⊆ (-∞; d 1 ], and both y and z are descending, there is no other way of putting a value from the segment (d 1 ; d 2 ] into registers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>XX:33</head><p>be adjusted so that it does not rely on the label of the current node. We begin by dropping all references to the current label in the conditions defining Cases A-D, which gives Case A': p , q ∈ X but either r / ∈ {p , q } or p -1 is not internal; Case B': p , q ∈ X, r ∈ {p , q }, and p -1 is internal; Case C': q ∈ X, p / ∈ X, and r / ∈ {p , q }; Case D': q ∈ X, p / ∈ X, and r ∈ {p , q }. In each of these cases the simulating automaton needs to consider all possible values of the current label. That is, in Cases A' and B', the set P is now defined as</p><p>and in Case C' we look at p • a 1 = p and q • a 2 = q for arbitrary a 1 , a 2 ∈ Γ. Apart from these differences, the arguments in Cases A'-C' are analogous to the original ones. Let us have a closer look at Case D'. Like before we have p = p -1 = q -1 and r = q . Consequently, p • a = q and, because p and q are almost equivalent, q • a = q . Suppose that p • a = p for some internal state p and some a ∈ Γ. Then, we have p • aa = q • a a = q ; that is, p blindly meets with q in q . Since q is rejective, it follows from blind E-flatness that p and q are almost equivalent. Consequently, q • a = p • a = p . Because we also have that p • a = q , it follows that p ∈ X which is a contradiction. Hence, such p cannot exist. One then argues, like in the markup case, that there is no q ∈ X \ {q } for which there exists a ∈ Γ such that q • a = q , and that letting the simulating automaton continue with the same synopsis preserves the invariant.</p><p>Finally, the analogue of Lemma 25 states that for each regular language L, if EL is term-registerless, then L is blindly E-flat. This time there are important differences in the proof; we sketch it below.</p><p>We show that if L is not blindly E-flat, then [EL] cannot be separated from [(EL) c ] by a finite automaton. Suppose that the minimal automaton A of L ⊆ Γ * is not E-flat. Let i be the initial state of A. Then, there exist words s, t, u 1 , u 2 ∈ Γ + , x ∈ Γ * and states p, q such that |u</p><p>is rejecting, and p • t is accepting iff q • t is rejecting. It follows that for each k &gt; 0, su 1 (u 2 ) k x ∈ L c , and st ∈ L iff s(u 1 )(u 2 ) k t ∈ L c . Unlike for the markup encoding, the construction of the fooling trees depends on whether st ∈ L or st ∈ L c Suppose first that st ∈ L c . Then, the trees S, S used in Lemma 25 should be replaced with the ones in Figure <ref type="figure">7a</ref>. We have S / ∈ EL and S ∈ EL. Note that we have no control on whether the rightmost branch of S is labelled by a word from L or not, but it is irrelevant, because we know that the middle branch is. The term encodings of S and S satisfy the following:</p><p>The rest of the proof is identical.</p><p>If st ∈ L, in S we replace u 1 on the rightmost branch with u 2 , and we modify S accordingly. It then holds that S ∈ EL regardless of whether su 2 (u 2 ) n! x belongs to L or not, and S / ∈ EL; the proof again continues like in Lemma 25.</p><p>Theorem 39. For each regular language L, the following conditions are equivalent:</p><p>XX:34 Stackless 1. QL is a term-stackless unary query; 2. EL is a term-stackless tree language; 3. AL is a term-stackless tree language; 4. L is blindly HAR.</p><p>Proof. The argument is fully analogous to that in Theorem 14, with Lemmas 20, 21 and 29 replaced by their analogues for term-stackless and blindly HAR languages. The analogue of Lemma 20 states that the class of blindly HAR languages is closed under complement, which is immediate from the definition just like for HAR languages.</p><p>The analogue of Lemma 21 states that if L blindly HAR then QL is term-stackless. The proof is analogous, with the only modification being what we did with Lemma 18 in the proof of Theorem 38: when the closing tag is read in state p and the current depth is greater than or equal to the maximal stored depth, we pick any state p such that p • a is almost equivalent to p for some a ∈ Γ. Because L is blindly HAR, the original argument now shows also that the choice of a does not matter.</p><p>Finally, the analogue of Lemma 29 states that for each regular language L, if EL is a term-stackless tree language then L is blindly HAR. The proof is obtained by adjusting the proof of Lemma 29 just like the proof of Lemma 25 was adjusted in Theorem 38. This time there is only one case because we know that s(wu 1 + vu 2 ) * wt ⊆ L c and s(wu 1 + vu 2 ) * vt ⊆ L, and not the other way around. In the tree R shown in Figure <ref type="figure">5</ref>, the copies of u immediately following copies of w should be replaced by u 1 and those immediately following v should be replaced by u 2 . From there, the proof continues like before.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Regularity problems for visibly pushdown languages</title>
		<author>
			<persName><forename type="first">Vince</forename><surname>Bárány</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christof</forename><surname>Löding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olivier</forename><surname>Serre</surname></persName>
		</author>
		<idno type="DOI">10.1007/11672142_34</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. STACS 2006</title>
		<meeting>STACS 2006</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="420" to="431" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">On the relative complexity of some languages in NC 1</title>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">A Mix</forename><surname>Barrington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">C</forename><surname>Corbett</surname></persName>
		</author>
		<idno type="DOI">10.1016/0020-0190(89)90052-5</idno>
	</analytic>
	<monogr>
		<title level="j">Inf. Process. Lett</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="251" to="256" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Querying Graphs</title>
		<author>
			<persName><forename type="first">Angela</forename><surname>Bonifati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">L</forename><surname>George</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannes</forename><surname>Fletcher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikolay</forename><surname>Voigt</surname></persName>
		</author>
		<author>
			<persName><surname>Yakovets</surname></persName>
		</author>
		<idno type="DOI">10.2200/S00873ED1V01Y201808DTM051</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>Morgan &amp; Claypool Publishers</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Parallel scanning with bitstream addition: An XML case study</title>
		<author>
			<persName><forename type="first">Robert</forename><forename type="middle">D</forename><surname>Cameron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ehsan</forename><surname>Amiri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenneth</forename><forename type="middle">S</forename><surname>Herdy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">C</forename><surname>Shermer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fred</forename><surname>Popowich</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-642-23397-5_2</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. Euro-Par 2011</title>
		<meeting>Euro-Par 2011</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="2" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">On validation of XML streams using finite state machines</title>
		<author>
			<persName><forename type="first">Cristiana</forename><surname>Chitic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniela</forename><surname>Rosu</surname></persName>
		</author>
		<idno type="DOI">10.1145/1017074.1017096</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. WebDB 2004</title>
		<meeting>WebDB 2004</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="85" to="90" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Early nested word automata for XPath query answering on XML streams</title>
		<author>
			<persName><forename type="first">Denis</forename><surname>Debarbieux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olivier</forename><surname>Gauwin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joachim</forename><surname>Niehren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Sebastian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohamed</forename><surname>Zergaoui</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.tcs.2015.01.017</idno>
	</analytic>
	<monogr>
		<title level="j">Theor. Comput. Sci</title>
		<imprint>
			<biblScope unit="volume">578</biblScope>
			<biblScope unit="page" from="100" to="125" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Input-driven Languages Are in Log N Depth</title>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Dymond</surname></persName>
		</author>
		<idno type="DOI">10.1016/0020-0190(88)90148-2</idno>
		<ptr target="http://dx.doi.org/10.1016/0020-0190(88)90148-2,doi:10.1016/0020-0190(88)90148-2" />
	</analytic>
	<monogr>
		<title level="j">Inf. Process. Lett</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="247" to="250" />
			<date type="published" when="1988-01">January 1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Anatomy of high-performance deep learning convolutions on SIMD architectures</title>
		<author>
			<persName><forename type="first">Evangelos</forename><surname>Georganas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sasikanth</forename><surname>Avancha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kunal</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dhiraj</forename><forename type="middle">D</forename><surname>Kalamkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><surname>Henry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hans</forename><surname>Pabst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Heinecke</surname></persName>
		</author>
		<ptr target="http://dl.acm.org/citation.cfm?id=3291744" />
	</analytic>
	<monogr>
		<title level="m">Proc. SC</title>
		<meeting>SC</meeting>
		<imprint>
			<publisher>IEEE / ACM</publisher>
			<date type="published" when="2018">2018. 2018</date>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="page" from="1" to="66" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A formal framework for complex event processing</title>
		<author>
			<persName><forename type="first">Alejandro</forename><surname>Grez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cristian</forename><surname>Riveros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martín</forename><surname>Ugarte</surname></persName>
		</author>
		<idno type="DOI">10.4230/LIPIcs.ICDT.2019.5</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. ICDT 2019</title>
		<meeting>ICDT 2019</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1" to="5" />
		</imprint>
		<respStmt>
			<orgName>Schloss Dagstuhl -Leibniz-Zentrum für Informatik</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">A comparison of regex engines</title>
		<author>
			<persName><forename type="first">Sascha</forename><surname>Grunert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Schmidt</surname></persName>
		</author>
		<ptr target="https://rust-leipzig.github.io/regex/2017/03/28/comparison-of-regex-engines/" />
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Stream processing of XPath queries with predicates</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gupta</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Suciu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SIGMOD 2003</title>
		<meeting>SIGMOD 2003</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="419" to="430" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">On load shedding in complex event processing</title>
		<author>
			<persName><forename type="first">Yeye</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siddharth</forename><surname>Barman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><forename type="middle">F</forename><surname>Naughton</surname></persName>
		</author>
		<idno type="DOI">10.5441/002/icdt.2014.23</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. ICDT 2014</title>
		<meeting>ICDT 2014</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="213" to="224" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Invisible pushdown languages</title>
		<author>
			<persName><forename type="first">Eryk</forename><surname>Kopczynski</surname></persName>
		</author>
		<idno type="DOI">10.1145/2933575.2933579</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. LICS 2016</title>
		<meeting>LICS 2016</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="867" to="872" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Parsing gigabytes of JSON per second</title>
		<author>
			<persName><forename type="first">Geoff</forename><surname>Langdale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Lemire</surname></persName>
		</author>
		<idno type="DOI">10.1007/s00778-019-00578-5</idno>
	</analytic>
	<monogr>
		<title level="j">VLDB J</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="941" to="960" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Schema validation via streaming circuits</title>
		<author>
			<persName><forename type="first">Filip</forename><surname>Murlak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><surname>Paperman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michal</forename><surname>Pilipczuk</surname></persName>
		</author>
		<idno type="DOI">10.1145/2902251.2902299</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. PODS 2016</title>
		<meeting>PODS 2016</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="237" to="249" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A gap property of deterministic tree languages</title>
		<author>
			<persName><forename type="first">Damian</forename><surname>Niwinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Igor</forename><surname>Walukiewicz</surname></persName>
		</author>
		<idno type="DOI">10.1016/S0304-3975(02)00452-8</idno>
	</analytic>
	<monogr>
		<title level="j">Theor. Comput. Sci</title>
		<imprint>
			<biblScope unit="volume">303</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="215" to="231" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">SPEX: streamed and progressive evaluation of XPath</title>
		<author>
			<persName><forename type="first">Dan</forename><surname>Olteanu</surname></persName>
		</author>
		<idno type="DOI">10.1109/TKDE.2007.1063</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Knowl. Data Eng</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="934" to="949" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Filter before you parse: Faster analytics on raw data with sparser</title>
		<author>
			<persName><forename type="first">Shoumik</forename><surname>Palkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Firas</forename><surname>Abuzaid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Bailis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matei</forename><surname>Zaharia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. VLDB Endow</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1576" to="1589" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Proprietes syntactiques du produit non ambigu</title>
		<author>
			<persName><forename type="first">Jean-Eric</forename><surname>Pin</surname></persName>
		</author>
		<idno type="DOI">10.1007/3-540-10003-2_93</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. ICALP 1980</title>
		<meeting>ICALP 1980</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1980">1980</date>
			<biblScope unit="page" from="483" to="499" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">On reversible automata</title>
		<author>
			<persName><forename type="first">Jean-Eric</forename><surname>Pin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. LATIN 1992</title>
		<meeting>LATIN 1992</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1992">1992</date>
			<biblScope unit="page" from="401" to="416" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Varieties Of Formal Languages</title>
		<author>
			<persName><forename type="first">Eric</forename><surname>Jean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raymond</forename><forename type="middle">E</forename><surname>Pin</surname></persName>
		</author>
		<author>
			<persName><surname>Miller</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1986">1986</date>
			<publisher>Plenum Publishing Co</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Rethinking SIMD vectorization for in-memory databases</title>
		<author>
			<persName><forename type="first">Orestis</forename><surname>Polychroniou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arun</forename><surname>Raghavan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenneth</forename><forename type="middle">A</forename><surname>Ross</surname></persName>
		</author>
		<idno type="DOI">10.1145/2723372.2747645</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. SIGMOD 2015</title>
		<meeting>SIGMOD 2015</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1493" to="1508" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">An empirical study on the vectorization of multimedia applications for multimedia extensions</title>
		<author>
			<persName><forename type="first">Peng</forename><surname>Gang Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">A</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><surname>Padua</surname></persName>
		</author>
		<idno type="DOI">10.1109/IPDPS.2005.94</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. IPDPS 2005. IEEE</title>
		<meeting>IPDPS 2005. IEEE</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Constant-memory validation of streaming XML documents against DTDs</title>
		<author>
			<persName><forename type="first">Luc</forename><surname>Segoufin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cristina</forename><surname>Sirangelo</surname></persName>
		</author>
		<idno type="DOI">10.1007/11965893_21</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. ICDT 2007</title>
		<meeting>ICDT 2007</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="299" to="313" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Validating streaming XML documents</title>
		<author>
			<persName><forename type="first">Luc</forename><surname>Segoufin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Vianu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. PODS 2002</title>
		<meeting>PODS 2002</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="53" to="64" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">From searching text to querying XML streams</title>
		<author>
			<persName><forename type="first">Dan</forename><surname>Suciu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Discrete Algorithms</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="17" to="32" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Improving the speed of neural networks on CPUs</title>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><forename type="middle">Z</forename><surname>Mao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Deep Learning and Unsupervised Feature Learning Workshop @ NIPS</title>
		<imprint>
			<date type="published" when="2011">2011. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Input-driven languages are recognized in log n space</title>
		<author>
			<persName><forename type="first">Braunmühl</forename><surname>Burchard Von</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rutger</forename><surname>Verbeek</surname></persName>
		</author>
		<idno type="DOI">10.1007/3-540-12689-9_92</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. FCT</title>
		<meeting>FCT</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1983">1983. 1983</date>
			<biblScope unit="page" from="40" to="51" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Hyperscan: A fast multi-pattern regex matcher for modern CPUs</title>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Harry</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyoungsoo</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoff</forename><surname>Langdale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiayu</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heqing</forename><surname>Zhu</surname></persName>
		</author>
		<ptr target="https://www.usenix.org/conference/nsdi19/presentation/wang-xiang" />
	</analytic>
	<monogr>
		<title level="m">Proc. NSDI 2019</title>
		<meeting>NSDI 2019</meeting>
		<imprint>
			<publisher>USENIX Association</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="631" to="648" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">On complexity and optimization of expensive queries in complex event processing</title>
		<author>
			<persName><forename type="first">Haopeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanlei</forename><surname>Diao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neil</forename><surname>Immerman</surname></persName>
		</author>
		<idno type="DOI">10.1145/2588555.2593671</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. SIGMOD 2014</title>
		<meeting>SIGMOD 2014</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="217" to="228" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Regex engine matching speed benchmark</title>
		<author>
			<persName><forename type="first">Yichun</forename><surname>Zhang</surname></persName>
		</author>
		<ptr target="http://openresty.org/misc/re/bench/" />
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Implementing database operations using SIMD instructions</title>
		<author>
			<persName><forename type="first">Jingren</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenneth</forename><forename type="middle">A</forename><surname>Ross</surname></persName>
		</author>
		<idno type="DOI">10.1145/564691.564709</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. SIGMOD 2002</title>
		<meeting>SIGMOD 2002</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="145" to="156" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
