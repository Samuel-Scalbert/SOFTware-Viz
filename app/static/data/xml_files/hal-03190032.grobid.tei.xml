<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Efficient Batch-Incremental Classification Using UMAP for Evolving Data Streams</title>
				<funder ref="#_uwkgUa9">
					<orgName type="full">Labex DigiCosme</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Maroua</forename><surname>Bahri</surname></persName>
							<email>maroua.bahri@telecom-paris.fr</email>
							<affiliation key="aff0">
								<orgName type="laboratory">LTCI</orgName>
								<orgName type="institution" key="instit1">Télécom Paris</orgName>
								<orgName type="institution" key="instit2">IP</orgName>
								<address>
									<settlement>-Paris, Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution" key="instit1">Université Paris-Saclay</orgName>
								<orgName type="institution" key="instit2">LRI</orgName>
								<orgName type="institution" key="instit3">CNRS</orgName>
								<address>
									<settlement>Orsay</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Bernhard</forename><surname>Pfahringer</surname></persName>
							<email>bernhard@waikato.ac.nz</email>
							<affiliation key="aff1">
								<orgName type="institution">University of Waikato</orgName>
								<address>
									<settlement>Hamilton</settlement>
									<region>New Zealand</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Albert</forename><surname>Bifet</surname></persName>
							<email>albert.bifet@telecom-paris.fr</email>
							<affiliation key="aff0">
								<orgName type="laboratory">LTCI</orgName>
								<orgName type="institution" key="instit1">Télécom Paris</orgName>
								<orgName type="institution" key="instit2">IP</orgName>
								<address>
									<settlement>-Paris, Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Waikato</orgName>
								<address>
									<settlement>Hamilton</settlement>
									<region>New Zealand</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Silviu</forename><surname>Maniu</surname></persName>
							<email>silviu.maniu@lri.fr</email>
							<affiliation key="aff2">
								<orgName type="institution" key="instit1">Université Paris-Saclay</orgName>
								<orgName type="institution" key="instit2">LRI</orgName>
								<orgName type="institution" key="instit3">CNRS</orgName>
								<address>
									<settlement>Orsay</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">Inria</orgName>
								<address>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="department">École Normale Supérieure</orgName>
								<orgName type="laboratory">ENS DI</orgName>
								<orgName type="institution" key="instit1">CNRS</orgName>
								<orgName type="institution" key="instit2">Université PSL</orgName>
								<address>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Efficient Batch-Incremental Classification Using UMAP for Evolving Data Streams</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">82572361D02CF6FC831F6FED51FBEF64</idno>
					<idno type="DOI">10.1007/978-3-030-</idno>
					<note type="submission">Submitted on 5 Apr 2021</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-04-12T14:48+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Data stream</term>
					<term>k-Nearest Neighbors</term>
					<term>dimension reduction</term>
					<term>UMAP</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>HAL is</head></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>With the evolution of technology, several kinds of devices and applications are continuously generating large amounts of data in a fast-paced way as streams. Hence, the data stream mining area has become indispensable and ubiquitous in many real-world applications that require real-time -or near real-time-processing, e.g., social networks, weather forecast, spam filters, and more. Unlike traditional datasets, the dynamic environment and the tremendous volume of data streams make them impossible to store or to scan multiple times <ref type="bibr" target="#b11">[12]</ref>.</p><p>Classification is an active area of research in data stream mining field where several researchers are paying attention to develop new -or improve existingalgorithms <ref type="bibr" target="#b13">[14]</ref>. However, the dynamic nature of data streams has outpaced the capability of traditional classification algorithms to process data streams. In this context, a multitude of supervised algorithms for static datasets that have been widely studied in the offline processing, and proved to be of limited effectiveness on large data, have been extended to work within a streaming framework <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b17">18]</ref>. Data stream mining approaches can be divided into two main types <ref type="bibr" target="#b22">[23]</ref>: (i) instance-incremental approaches which update the model with each instance as soon as it arrives, such as Self-Adjusting Memory kNN (samkNN) <ref type="bibr" target="#b17">[18]</ref>, and Hoeffding Adaptive Tree (HAT) <ref type="bibr" target="#b3">[4]</ref>; and (ii) batch-incremental approaches which make no change/increment to their model until a batch is completed, e.g., support vector machines <ref type="bibr" target="#b9">[10]</ref>, and batch-incremental ensemble of decision trees <ref type="bibr" target="#b14">[15]</ref>. Nevertheless, the high dimensionality of data complicates the classification task for some algorithms and increases their computational resources, most notably the k-Nearest Neighbors (kNN) because it needs the entire dataset to predict the labels for test instances <ref type="bibr" target="#b22">[23]</ref>. To cope with this issue, a promising approach is feature transformation which transforms the input features into a new set of features, containing the most relevant components, in some lower-dimensional space.</p><p>In attempt to improve the performance of kNN, we incorporate a batchincremental feature transformation strategy to tackle potentially high-dimensional and possibly infinite batches of evolving data streams while ensuring effectiveness and quality of learning (e.g. accuracy). This is achieved via a new manifold technique that has attracted a lot of attention recently: Uniform Manifold Approximation and Projection (UMAP) <ref type="bibr" target="#b20">[21]</ref>, built upon rigorous mathematical foundations, namely Riemannian geometry. To the best of our knowledge, no incremental version of UMAP exists which makes it not applicable on large datasets. The main contributions are summarized as follows:</p><p>-Batch-Incremental UMAP : a new batch-incremental novel manifold learning technique, based on extending the UMAP algorithm to data streams. -UMAP-kNearest Neighbors (UMAP-kNN): a new batch-incremental kNN algorithm for data streams classification using UMAP.</p><p>-Empirical experiments: we provide an experimental study, on various datasets, that discusses the implication of parameters on the algorithms performance;</p><p>The paper is organized as follows. Section 2 reviews the prominent related work. Section 3 provides the background of UMAP, followed by the description of our approach. In Section 4 we present and discuss the results of experiments on diverse datasets. Finally, we draw our conclusions and present future directions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Dimensionality reduction (DR) is a powerful tool in data science to look for hidden structure in data and reduce the resources usage of learning algorithms.</p><p>The problem of dimensionality has been widely studied <ref type="bibr" target="#b24">[25]</ref> and used throughout different domains, such as image processing and face recognition. Dimensionality reduction techniques facilitate the classification task, by removing redundancies and extracting the most relevant features in the data, and permits a better data visualization. A common taxonomy divides these approaches into two major groups: matrix factorization and graph-based approaches.</p><p>Matrix factorization algorithms require matrix computation tools, such as Principal Components Analysis (PCA) <ref type="bibr" target="#b15">[16]</ref>. It is a well-known linear technique that uses singular value decomposition and aims to find a lower-dimensional basis by converting the data into features called principal components by computing the eigenvalues and eigenvectors of a covariance matrix. This straightforward technique is computationally cheap but ineffective with data streams since it relies on the whole dataset. Therefore, some incremental versions of PCA have been developed to handle streams of data <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b25">26]</ref>.</p><p>Graph/Neighborhood-based techniques are leveraged in the context of dimension reduction and visualization by using the insight that similar instances in a large space should be represented by close instances in a low-dimensional space, whereas dissimilar instances should be well separated. t-distributed Stochastic Neighbor Embedding (tSNE) <ref type="bibr" target="#b19">[20]</ref> is one of the most prominent DR techniques in the literature. It has been proposed to visualize high-dimensional data embedded in a lower space (typically 2 or 3 dimensions). In addition to the fact that it is computationally expensive, tSNE does not preserve distances between all instances and can affect any density-or distance-based algorithm and hence conserves more of the local structure than the global structure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Batch-Incremental Classification</head><p>In the following, we assume a data stream S is a sequence of instances X 1 , . . . , X N , where N denotes the number of available observations so far. Each instance X i is composed of a vector of d attributes X i = (x 1 i , . . . , x d i ). The dimensionality reduction of S comprises the process of finding a low-dimensional presentation S = Y 1 , . . . , Y N , where Y i = (y 1 i , . . . , y p i ) and p d.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Prior Work</head><p>Unlike tSNE <ref type="bibr" target="#b19">[20]</ref>, UMAP has no restriction on the projected space size making it useful not only for visualization but also as a general dimension reduction technique for machine learning algorithms. It starts by constructing open balls over all instances and building simplicial complexes. Dimension reduction is obtained by finding a representation, in a lower space, that closely resembles the topological structure in the original space. Given the new dimension, an equivalent fuzzy topological representation is then constructed <ref type="bibr" target="#b20">[21]</ref>. Then, UMAP optimizes it by minimizing the cross-entropy between these two fuzzy topological representations. UMAP offers better visualization quality than tSNE by preserving more of the global structure in a shorter running time. To the best of our knowledge, none of these techniques has a streaming version. Ultimately, both techniques are essentially transductive <ref type="foot" target="#foot_0">6</ref> and do not learn a mapping function from the input space. Hence, they need to process all the data for each new unseen instance, which prevents them from being usable in data streams classification models. Figure <ref type="figure" target="#fig_0">1</ref> shows the projection of CNAE dataset (see Table <ref type="table" target="#tab_0">1</ref>) into 2-dimensions in an offline/online fashions where each color represents a label. In Figure <ref type="figure" target="#fig_0">1a</ref>, we note that UMAP offers the most interesting visualization while separating classes (9 classes). The overlap in the new space, for instance with tSNE in Figure <ref type="figure" target="#fig_0">1b</ref>, can potentially affect later classification task, notably distance-based algorithms, since properties like global distances and density may be lost. On the other hand, linear transformation, such as PCA, cannot discriminate between instances which prevents them from being represented in the form of clusters (Figure <ref type="figure" target="#fig_0">1c</ref>). To motivate our choice, we project the same dataset using our batch-incremental strategy (more details in Section 3.2). Figure <ref type="figure" target="#fig_0">1d</ref> illustrates the change from the offline UMAP representation; this is not as drastic as the ones engendered by tSNE and PCA (Figures <ref type="figure" target="#fig_0">1e</ref> and<ref type="figure" target="#fig_0">1f</ref>, respectively) showing their limits on capturing information from data that arrives in a batch-incremental manner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Algorithm Description</head><p>A very efficient and simple scheme in supervised learning is lazy learning <ref type="bibr" target="#b0">[1]</ref>. Since lazy learning approaches are based on distances between every pair of instances, they unfortunately have a low performance in terms of execution time. The k-Nearest Neighbors (kNN) is a well-known lazy algorithm that does not require any work during training, so it uses the entire dataset to predict labels for test instances. However, it is impossible to store an evolving data stream which is potentially infinite -nor to scan it multiple times -due to its tremendous volume. To tackle this challenge, a basic incremental version of kNN has been proposed which uses a fixed-length window that slides through the stream and merges new arriving instances with the closest ones already in the window <ref type="bibr" target="#b22">[23]</ref>.</p><p>To predict the class label for an incoming instance, we take the majority class labels of its nearest neighbors inside the window using a defined distance metric (Equation <ref type="formula" target="#formula_1">2</ref>). Since we keep the recent arrived instances inside the sliding window for prediction, the search for the nearest neighbors is still costly in terms of memory and time <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b6">7]</ref> and high-dimensional streams require further resources.</p><p>Given a window w, the distance between X i and X j is defined as follows:</p><formula xml:id="formula_0">D Xj (X i ) = X i -X j 2 .<label>(1)</label></formula><p>Similarly, the k-Nearest Neighbors distance is defined as follows:</p><formula xml:id="formula_1">D w,k (X i ) = min ( w k ),Xj∈w k j=1 D Xj (X i ),<label>(2)</label></formula><p>where w k denotes the subset of the kNN to X i in w. When dealing with high-dimensional data, a pre-processing phase before applying a learning algorithm is imperative to avoid the curse of dimensionality from a computational point of view. The latter may increase the resources usage and decrease the performance of some algorithms, such as kNN. The main idea to mitigate this curse consists of using an efficient strategy with consistent and promising results such as UMAP.</p><p>Since UMAP is a transductive technique, an instance-incremental learning approach that includes UMAP does not work because the entire stream needs to be processed for each new incoming instance. By doing it this way, the process will be costly and will not respond to the streaming requirements. To alleviate the processing cost considering the framework within which several challenges shall be respected, including the memory constraint and the incremental behavior of data, we adopt a batch-incremental strategy. In the following, we introduce the procedure of our novel approach, batch-incremental UMAP-kNN.</p><p>Step 1: Partition of the stream. During this step, we assume that data arrive in batches -or chunks -by dividing the stream into disjoint partitions S 1 , S 2 , . . . of size s. The first part of Figure <ref type="figure">2</ref> shows a stream of instances divided into batches, so instead of having instances available one at a time, they will arrive as a group of instances simultaneously, S 1 , . . . , S q , where S q is the qth chunk. A</p><formula xml:id="formula_2">X 1 X 2 X 3 X 4 X 5 X 6 X 7 X 8 X 9 X 10 X 11 X 12 ... X r X r+1 X r+2 X r+3 ∊ℝ d Y 1 Y 2 Y 3 Y 4 Y 5 Y 6 Y 7 Y 8 Y 9 Y 10 Y 11 Y 12 ... Y r Y r+1 Y r+2 Y r+3 ∊ℝ p T 1 T 2 T 3 T q Y 1 Y 2 Y 1 Y 2 Y 3 Y 1 Y 2 Y 3 Y 4 Y 1 Y 2 Y 3 Y 4 Y 5 Y 6 Y 1 Y 2 Y 3 Y 4 Y 5 Y 6 Y 7 Y 8 Y 1 Y 2 Y 3 Y 4 Y 5 Y 6 Y 7 ... Y 8 Y 1 Y 2 Y 3 Y 4 Y 5 Y 6 Y 7 Y 9 Y 10 Y 11 Y 12 ... Y r Y r+1 Y r+2 Y r+3 kNN</formula><p>Fig. <ref type="figure">2</ref>: Batch-incremental UMAP-kNN scheme simple example of data stream is a video sequence where at each instant we have a succession of images.</p><p>Step 2: Data pre-processing. We aim to construct a low-dimensional Y i ∈ p, from an infinite stream of high-dimensional data X i ∈ d, where p d. As mentioned before, UMAP is unable to compress data incrementally and needs to transform more than one observation at a time because it builds a neighborhoodgraph on a set of instances and then lays it out in a lower dimensional space <ref type="bibr" target="#b20">[21]</ref>. Thus, our proposed approach operates on batches of the stream where a single batch S i of data is processed at a time T i . The two first steps in Figure <ref type="figure">2</ref> depict the application of UMAP on the disjoint batches. Once a batch is complete, throughout the second step, we apply UMAP on it independently from the chunks that have been already processed, so each S i ∈ R d will be transformed and represented by S i ∈ R p . This new representation is very likely devoid of redundancies, irrelevant attributes, and is obtained by finding potentially useful non-linear combinations of existing attributes, i.e. by repacking relevant information of the larger feature space and encoding it more compactly.</p><p>For UMAP to learn when moving from a batch to another, we seed each chunk's embedding with the outcome of the previous one, i.e., match the prior initial coordinates for instances in the current embedding to the final coordinates in the preceding one. This will help to avoid losing the topological information of the stream and to keep stability in successive embeddings as we transition from one batch to its successor. Afterwards, we use the compressed representation of the high-dimensional chunk for the next step that consists in supporting the incremental kNN classification algorithm.</p><p>Step 3: kNN classification. UMAP-kNN aims to decrease the computational costs of kNN on high-dimensional data stream by reducing the input space size using the dimension reducing UMAP, in a batch-incremental way. In addition to the prediction phase of the kNN algorithm that, based on the neighborhood<ref type="foot" target="#foot_1">7</ref> , UMAP operates on a k-nearest graph (topological representation) as well and optimizes the low-dimensional representation of the data using gradient descent. One nice takeaway is that UMAP, because of its solid theoretical backing as a manifold technique, keeps properties such as density and pairwise distances. Thus, it does not bias the neighborhood-based kNN performance.</p><p>This step consists of classifying the evolving data stream, where the learning task occurs on consecutive batches, i.e. we train incrementally kNN with instances becoming successively available in chunk buffers after pre-processing. Figure <ref type="figure">2</ref> shows the underlying batch-incremental learning scheme used which is built upon the divide-and-conquer strategy. Since UMAP is independently applied to batches, so once a chunk is complete and has been transformed in R p , we feed the half of the batch to the sliding window and we predict incrementally the class label for the second half (the rest of instances).</p><p>Given that kNN is adaptive, the main novelty of UMAP-kNN is in how it merges the current batch to previous ones. This is done by adding it to the instances from previous chunks inside the kNN window. Even if past chunks have been discarded, only some of them have been stored and maintained while the adaptive window scrolls. Thereafter, instances kept temporarily inside the window are going to be used to define the neighborhood and predict the class labels for later incoming instances. As presented in Figure <ref type="figure">2</ref>, the intuitive idea to combine results from different batches is to use the half of each batch for training and the second half for prediction. In general, due to the possibility of having often very different successive embeddings, one would expect that this may affect the global performance of our approach. Thus, we adopt this scheme to maintain a stability over an adaptive batch-incremental manifold classification approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In this section, we present a series of experiments carried out on various datasets based on three main results: the accuracy, the memory (MB), and the time (Sec).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>We use 3 synthetic and 6 real-world datasets from different scenarios that have been thoroughly used in the literature to evaluate the performance of data streams classifiers. Table <ref type="table" target="#tab_0">1</ref> presents a short description of each dataset, and further details are provided in what follows.</p><p>Tweets. The dataset was created using the tweets text data generator provided by MOA <ref type="bibr" target="#b5">[6]</ref> that simulates sentiment analysis on tweets, where messages can be classified depending on whether they convey positive or negative feelings. Tweets 1,2,3 produce instances of 500, 1,000 and 1,500 attributes respectively.</p><p>Har . Human Activity Recognition dataset <ref type="bibr" target="#b1">[2]</ref> built from several subjects performing daily living activities, such as walking, sitting, standing and laying, while wearing a waist-mounted smartphone equipped with sensors. The sensor signals were pre-processed using noise filters and attributes were normalized.</p><p>CNAE . CNAE is the national classification of economic activities dataset <ref type="bibr" target="#b8">[9]</ref>. Instances represent descriptions of Brazilian companies categorized into 9 classes. The original texts were pre-processed to obtain the current highly sparse data.</p><p>Enron. The Enron corpus dataset is a large set of email messages that was made public during the legal investigation concerning the Enron corporation <ref type="bibr" target="#b16">[17]</ref>. This cleaned version of Enron consists of 1, 702 instances and 1, 000 attributes. IMDB. IMDB movie reviews dataset was proposed for sentiment analysis <ref type="bibr" target="#b18">[19]</ref>, where each review is encoded as a sequence of word indexes (integers).</p><p>Nomao. Nomao dataset <ref type="bibr" target="#b7">[8]</ref> was provided by Nomao Labs where data come from several sources on the web about places (name, address, localization, etc).</p><p>Covt. The forest covertype dataset obtained from US forest service resource information system data where each class label presents a different cover type.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Results and Discussions</head><p>We compare our proposed classifier, UMAP-kNN, to various commonly-used baseline methods in dimensionality reduction and machine learning areas. PCA <ref type="bibr" target="#b23">[24]</ref>, tSNE (fixing the perplexity to 30, which is the best value as reported in <ref type="bibr" target="#b19">[20]</ref>), SAM-kNN (SkNN) <ref type="bibr" target="#b17">[18]</ref>. We use HAT, a classifier with a different structure based on trees <ref type="bibr" target="#b3">[4]</ref>, to assess its performance with the neighborhood-based UMAP. For fair comparison, we compare the performance of UMAP-kNN approach with a competitor using UMAP as well in the same batch-incremental manner. Actually, incremental kNN has two crucial parameters: (i) the number of neighbors k fixed to 5; and (ii) the window size w, that maintains the low-dimensional data, fixed to 1000. According to previous studies such as <ref type="bibr" target="#b6">[7]</ref>, a bigger window will increase the resources usage and smaller size will impact the accuracy.</p><p>The experiments were conducted on a machine equipped with an Intel Core i5 CPU and 4 GB of RAM. All experiments were implemented and evaluated in Python by extending the Scikit-multiflow framework<ref type="foot" target="#foot_2">8</ref>  <ref type="bibr" target="#b21">[22]</ref>. Figure <ref type="figure" target="#fig_1">3a</ref> depicts the influence of the chunk size on the accuracy using UMAP-kNN with some datasets. Generally, fixing the chunk size imposes the following dilemma: choosing a small size so that we obtain an accurate reflection of the current data or choosing a large size that may increase the accuracy since more data are available. The ideal would be to use a batch with the maximum of instances to represent as possible the whole stream. In practice, the chunk size needs to be small enough to fit in the main memory otherwise the running time of the approach will increase. Since UMAP is relatively slow, we choose small chunk sizes to overcome this issue with UMAP-kNN. Based on the obtained results, we fix the chunk size to 400 for the best trade-off accuracy-memory.</p><p>We investigate the behavior of a crucial parameter that controls UMAP, number of neighbors, via the classification performance of our approach. Based on the size of the neighborhood, UMAP constructs the manifold and focuses on preserving local and global structures. Figure <ref type="figure" target="#fig_1">3b</ref> shows the accuracy when the number of neighbors is varied on diverse datasets. We notice that for all datasets, the accuracy is consistently the same with no large differences, e.g. Har. Since a large neighborhood leads to a slower learning process, in the following we fix the neighborhood size to 15. tSNE is a visualization technique, so we are limited to project high-dimensional data into 2 or 3 dimensions. In order to evaluate the performance of our proposal in a fair comparison against each of tSNE-kNN and PCA-kNN, we project data into 3-dimensional space. We illustrate in Figure <ref type="figure" target="#fig_2">4a</ref> that UMAP-kNN makes significantly more accurate predictions beating consistently the best performing baselines (tSNE-kNN and PCA-kNN) notably with CNAE and the tweets datasets. Figure <ref type="figure" target="#fig_2">4b</ref> depicts the quantity of memory needed by the three algorithms which is practically the same for some datasets. Compared to kNN that uses the whole data without projection, we notice that UMAP-kNN consumes much less memory whilst sacrificing a bit in accuracy because we are removing many attributes. Figure <ref type="figure" target="#fig_2">4c</ref> shows that our approach is consistently faster than tSNE-kNN because tSNE computes the distances between every pair of instances to project. But PCA-kNN is a bit faster thanks to the simplicity of PCA. But with this trade-off our approach performs good on almost all datasets. In addition to its good classification performance in comparison with competitors, the batch-incremental UMAP-kNN did a better job of preserving density by capturing both of global and local structures, as shown in Figure <ref type="figure" target="#fig_0">1d</ref>. The fact that UMAP and kNN are both neighborhood-based methods arises as a key element in achieving a good accuracy. UMAP has not only the power of visualization but also the ability to reduce the dimensionality of data efficiently which makes it useful as pre-processing technique for machine learning.</p><p>Table <ref type="table" target="#tab_1">2</ref> reports the comparison of UMAP-kNN against state-of-the-art classifiers. We highlight that our approach performs better on almost all datasets. It achieves similar accuracies to UMAP-SkNN on several datasets but in terms of resources, the latter is slower because of its drift detection mechanism. UMAP-kNN has a better performance than PCA-kNN, e.g. the Tweets datasets at the cost of being slower. We also observe the UMAP-HAT failed to overcome our approach (in terms of accuracy, memory, and time) due to the integration of a neighborhood-based technique (UMAP) to a tree structure (HAT).</p><p>Figure <ref type="figure" target="#fig_3">5</ref> reports detailed results for Tweet 1 dataset with five output dimensions. Figure <ref type="figure" target="#fig_3">5a</ref> exhibits the accuracy of our approach which is consistently above competitors whilst ensuring stability for different manifolds. Figures <ref type="figure" target="#fig_3">5b</ref> and<ref type="figure" target="#fig_3">5c</ref> show that kNN-based classifiers use much less resources than the tree-based UMAP-HAT. We see that UMAP-kNN requires less time than UMAP-HAT and UMAP-SkNN to execute the stream but PCA-kNN is fastest thanks to its simplicity. Still, the gain in accuracy with UMAP-kNN is more significant. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Concluding Remarks and Future Work</head><p>In this paper, we presented a novel batch-incremental approach for mining data streams using the kNN algorithm. UMAP-kNN combines the simplicity of kNN and the high performance of UMAP which is used as an internal pre-processing step to reduce the feature space of data streams. We showed that UMAP is capable of embedding efficiently data streams within a batch-incremental strategy in an extensive evaluation with well-known state-of-the-art algorithms using various datasets. We further demonstrated that the batch-incremental approach is just as effective as the offline approach in visualization and its accuracy outperforms reputed baselines while using reasonable resources.</p><p>We would like to pursue our promising approach further to enhance its runtime performance by applying a fast dimension reduction before using of UMAP. Another area for future work could be the use of a different mechanism, such as the application of UMAP for each incoming data inside a sliding window. We believe that this may be slow but will be suited for instance-incremental learning.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Fig. 1: Projection of CNAE dataset in 2-dimensional space. Offline: (a) UMAP, (b) tSNE, and (c) PCA. Batch-incremental: (d) UMAP, (e) tSNE, and (f) PCA.</figDesc><graphic coords="5,148.86,296.66,96.49,95.32" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 :</head><label>3</label><figDesc>Fig. 3: (a) Varying the chunk size. (b) Varying the neighborhood size for UMAP.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 :</head><label>4</label><figDesc>Fig. 4: Comparison of UMAP-kNN, tSNE-kNN, PCA-kNN, and kNN (with the entire datasets) while projecting into 3-dimensions: (a) Accuracy. (b) Memory.</figDesc><graphic coords="10,380.27,536.59,94.03,73.55" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 :</head><label>5</label><figDesc>Fig. 5: Comparison of UMAP-kNN, PCA-kNN, UMAP-SkNN, and UMAP-HAT over different output dimensions on Tweet 1 : (a) Accuracy. (b) Memory. (c) Time.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Overview of the datasets</figDesc><table><row><cell>Dataset</cell><cell>#Instances</cell><cell>#Attributes</cell><cell>#Classes</cell><cell>Type</cell></row><row><cell>Tweets1</cell><cell>1,000,000</cell><cell>500</cell><cell>2</cell><cell>Synthetic</cell></row><row><cell>Tweets2</cell><cell>1,000,000</cell><cell>1,000</cell><cell>2</cell><cell>Synthetic</cell></row><row><cell>Tweets3</cell><cell>1,000,000</cell><cell>1,500</cell><cell>2</cell><cell>Synthetic</cell></row><row><cell>Har</cell><cell>10,299</cell><cell>561</cell><cell>6</cell><cell>Real</cell></row><row><cell>CNAE</cell><cell>1,080</cell><cell>856</cell><cell>9</cell><cell>Real</cell></row><row><cell>Enron</cell><cell>1,702</cell><cell>1,000</cell><cell>2</cell><cell>Real</cell></row><row><cell>IMDB</cell><cell>120,919</cell><cell>1,001</cell><cell>2</cell><cell>Real</cell></row><row><cell>Nomao</cell><cell>34,465</cell><cell>119</cell><cell>2</cell><cell>Real</cell></row><row><cell>Covt</cell><cell>581,012</cell><cell>54</cell><cell>7</cell><cell>Real</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Comparison of UMAP-kNN, PCA-kNN, UMAP-SkNN, and UMAP-HAT.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Accuracy (%)</cell><cell></cell></row><row><cell cols="5">Dataset UMAP-kNN PCA-kNN UMAP-SkNN UMAP-HAT</cell></row><row><cell>Tweets1</cell><cell>75.71</cell><cell>69.89</cell><cell>75.37</cell><cell>66.47</cell></row><row><cell>Tweets2</cell><cell>75.16</cell><cell>69.21</cell><cell>74.40</cell><cell>61.27</cell></row><row><cell>Tweets3</cell><cell>71.01</cell><cell>70.81</cell><cell>70.47</cell><cell>66.98</cell></row><row><cell>Har</cell><cell>75.30</cell><cell>70.50</cell><cell>64.09</cell><cell>84.89</cell></row><row><cell>CNAE</cell><cell>76.67</cell><cell>67.41</cell><cell>75.18</cell><cell>40.18</cell></row><row><cell>Enron</cell><cell>92.24</cell><cell>93.41</cell><cell>91.89</cell><cell>91.77</cell></row><row><cell>IMDB</cell><cell>67.38</cell><cell>67.28</cell><cell>67.43</cell><cell>64.52</cell></row><row><cell>Nomao</cell><cell>91.92</cell><cell>91.13</cell><cell>91.63</cell><cell>83.75</cell></row><row><cell>Covt</cell><cell>61.29</cell><cell>66.73</cell><cell>53.08</cell><cell>55.43</cell></row><row><cell></cell><cell></cell><cell cols="2">Memory (MB)</cell><cell></cell></row><row><cell cols="5">Dataset UMAP-kNN PCA-kNN UMAP-SkNN UMAP-HAT</cell></row><row><cell>Tweets1</cell><cell>1366.71</cell><cell>1354.24</cell><cell>1373.15</cell><cell>2738.32</cell></row><row><cell>Tweets2</cell><cell>2530.30</cell><cell>2518.76</cell><cell>2532.95</cell><cell>4891.23</cell></row><row><cell>Tweets3</cell><cell>3706.99</cell><cell>3706.55</cell><cell>3722.68</cell><cell>7144.77</cell></row><row><cell>Har</cell><cell>311.58</cell><cell>310.48</cell><cell>312.84</cell><cell>381.49</cell></row><row><cell>CNAE</cell><cell>254.17</cell><cell>246.94</cell><cell>260.29</cell><cell>262.52</cell></row><row><cell>Enron</cell><cell>269.00</cell><cell>267.31</cell><cell>271.56</cell><cell>288.74</cell></row><row><cell>IMDB</cell><cell>3012.85</cell><cell>3013.28</cell><cell>3018.04</cell><cell>7471.64</cell></row><row><cell>Nomao</cell><cell>289.81</cell><cell>285.50</cell><cell>290.60</cell><cell>508.50</cell></row><row><cell>Covt</cell><cell>700.69</cell><cell>689.97</cell><cell>704.46</cell><cell>3788.54</cell></row><row><cell></cell><cell></cell><cell>Time (Sec)</cell><cell></cell><cell></cell></row><row><cell cols="5">Dataset UMAP-kNN PCA-kNN UMAP-SkNN UMAP-HAT</cell></row><row><cell>Tweets1</cell><cell>558.56</cell><cell>217.44</cell><cell>1396.32</cell><cell>2163.14</cell></row><row><cell>Tweets2</cell><cell>616.50</cell><cell>350.63</cell><cell>908.59</cell><cell>3453.21</cell></row><row><cell>Tweets3</cell><cell>667.43</cell><cell>400.62</cell><cell>1066.98</cell><cell>6273.19</cell></row><row><cell>Har</cell><cell>75.20</cell><cell>24.37</cell><cell>77.99</cell><cell>82.47</cell></row><row><cell>CNAE</cell><cell>8.89</cell><cell>4.81</cell><cell>13.17</cell><cell>19.78</cell></row><row><cell>Enron</cell><cell>12.80</cell><cell>9.52</cell><cell>17.26</cell><cell>32.84</cell></row><row><cell>IMDB</cell><cell>715.68</cell><cell>407.60</cell><cell>1038.77</cell><cell>4691.07</cell></row><row><cell>Nomao</cell><cell>248.79</cell><cell>20.46</cell><cell>327.36</cell><cell>228.00</cell></row><row><cell>Covt</cell><cell>2311.21</cell><cell>137.62</cell><cell>3756.41</cell><cell>2297.01</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_0"><p>Transductive learning consists on learning on a full given dataset (including unknown label), but prediction is only made on the known set of unlabeled instances from the same dataset. This is achieved by clustering data instances.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_1"><p>The distances between the new incoming instance and the instances already available inside the adaptive window are computed in order to assign it to a particular class.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_2"><p>https://scikit-multiflow.github.io/</p></note>
		</body>
		<back>

			<div type="funding">
<div><p>This work was done in the context of <rs type="projectName">IoTA AAP Emergence DigiCosme Project</rs> and was funded by <rs type="funder">Labex DigiCosme</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_uwkgUa9">
					<orgName type="project" subtype="full">IoTA AAP Emergence DigiCosme Project</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">W</forename><surname>Aha</surname></persName>
		</author>
		<title level="m">Lazy learning</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Human activity recognition on smartphones using a multiclass hardware-friendly support vector machine</title>
		<author>
			<persName><forename type="first">D</forename><surname>Anguita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ghio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Oneto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Parra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Reyes-Ortiz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IWAAL</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="216" to="223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A sketch-based naive bayes algorithms for evolving data streams</title>
		<author>
			<persName><forename type="first">M</forename><surname>Bahri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Maniu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bifet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Big Data</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="604" to="613" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Adaptive learning from evolving data streams</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bifet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Gavaldà</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Intelligent Data Analysis</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="249" to="260" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Bifet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Gavaldà</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Holmes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Pfahringer</surname></persName>
		</author>
		<title level="m">Machine Learning for Data Streams: with Practical Examples in MOA</title>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Moa: Massive online analysis</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bifet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Holmes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kirkby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Pfahringer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="1601" to="1604" />
			<date type="published" when="2010-05">May. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Efficient data stream classification via probabilistic adaptive windows</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bifet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Pfahringer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Read</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Holmes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGAPP</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="801" to="806" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Design and analysis of the nomao challenge active learning in the real-world</title>
		<author>
			<persName><forename type="first">L</forename><surname>Candillier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Lemaire</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ALRA, Workshop ECML-PKDD. sn</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Agglomeration and elimination of terms for dimensionality reduction</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">M</forename><surname>Ciarelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Oliveira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISDA</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="547" to="552" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Support-vector networks</title>
		<author>
			<persName><forename type="first">C</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vapnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ML</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="273" to="297" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Mining high-speed data streams</title>
		<author>
			<persName><forename type="first">P</forename><surname>Domingos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hulten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="71" to="80" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Issues in evaluation of stream learning algorithms</title>
		<author>
			<persName><forename type="first">J</forename><surname>Gama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sebastião</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">P</forename><surname>Rodrigues</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="329" to="338" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Fast iterative kernel principal component analysis</title>
		<author>
			<persName><forename type="first">S</forename><surname>Günter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">N</forename><surname>Schraudolph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Vishwanathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1893" to="1918" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Hand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Mannila</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Smyth</surname></persName>
		</author>
		<title level="m">Principles of data mining</title>
		<imprint>
			<publisher>MIT press</publisher>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Batch-incremental learning for mining data streams</title>
		<author>
			<persName><forename type="first">G</forename><surname>Holmes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">B</forename><surname>Kirkby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bainbridge</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Analysis of a complex of statistical variables into principal components</title>
		<author>
			<persName><forename type="first">H</forename><surname>Hotelling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Educational Psychology</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">417</biblScope>
			<date type="published" when="1933">1933</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">The enron corpus: A new dataset for email classification research</title>
		<author>
			<persName><forename type="first">B</forename><surname>Klimt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECML</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="217" to="226" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Knn classifier with self adjusting memory for heterogeneous concept drift</title>
		<author>
			<persName><forename type="first">V</forename><surname>Losing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Hammer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wersing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDM</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="291" to="300" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning word vectors for sentiment analysis</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Maas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Daly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL-HLT</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="142" to="150" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Visualizing data using t-sne</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">V D</forename><surname>Maaten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="2579" to="2605" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Umap: Uniform manifold approximation and projection for dimension reduction</title>
		<author>
			<persName><forename type="first">L</forename><surname>Mcinnes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Healy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Melville</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.03426</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Scikit-multiflow: a multi-output streaming framework</title>
		<author>
			<persName><forename type="first">J</forename><surname>Montiel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Read</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bifet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Abdessalem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="2915" to="2914" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Batch-incremental versus instanceincremental learning in dynamic and evolving data</title>
		<author>
			<persName><forename type="first">J</forename><surname>Read</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bifet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Pfahringer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Holmes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IDA</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="313" to="323" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Incremental learning for robust visual tracking</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">77</biblScope>
			<biblScope unit="issue">1-3</biblScope>
			<biblScope unit="page" from="125" to="141" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">O S</forename><surname>Sorzano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Vargas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Montano</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1403.2877</idno>
		<title level="m">A survey of dimensionality reduction techniques</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Candid covariance-free incremental principal component analysis</title>
		<author>
			<persName><forename type="first">J</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">S</forename><surname>Hwang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1034" to="1040" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
