<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Performance-Explainability Framework to Benchmark Machine Learning Methods: Application to Multivariate Time Series Classifiers</title>
				<funder>
					<orgName type="full">&quot; (HyAIAI)</orgName>
				</funder>
				<funder ref="#_XAVj9V8">
					<orgName type="full">Inria Project Lab &quot;</orgName>
				</funder>
				<funder ref="#_H8tm7NP">
					<orgName type="full">Agence Nationale de la Recherche</orgName>
					<orgName type="abbreviated">ANR</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Kevin</forename><surname>Fauvel</surname></persName>
							<email>kevin.fauvel@inria.fr</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Univ Rennes</orgName>
								<orgName type="institution" key="instit2">Inria</orgName>
								<orgName type="institution" key="instit3">CNRS</orgName>
								<orgName type="institution" key="instit4">IRISA</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Univ Rennes</orgName>
								<orgName type="institution" key="instit2">Inria</orgName>
								<orgName type="institution" key="instit3">CNRS</orgName>
								<orgName type="institution" key="instit4">IRISA</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Véronique</forename><surname>Masson</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Univ Rennes</orgName>
								<orgName type="institution" key="instit2">Inria</orgName>
								<orgName type="institution" key="instit3">CNRS</orgName>
								<orgName type="institution" key="instit4">IRISA</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Univ Rennes</orgName>
								<orgName type="institution" key="instit2">Inria</orgName>
								<orgName type="institution" key="instit3">CNRS</orgName>
								<orgName type="institution" key="instit4">IRISA</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Elisa</forename><surname>Fromont</surname></persName>
							<email>elisa.fromont@irisa.fr</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Univ Rennes</orgName>
								<orgName type="institution" key="instit2">Inria</orgName>
								<orgName type="institution" key="instit3">CNRS</orgName>
								<orgName type="institution" key="instit4">IRISA</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Élisa</forename><surname>Fromont</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Univ Rennes</orgName>
								<orgName type="institution" key="instit2">Inria</orgName>
								<orgName type="institution" key="instit3">CNRS</orgName>
								<orgName type="institution" key="instit4">IRISA</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A Performance-Explainability Framework to Benchmark Machine Learning Methods: Application to Multivariate Time Series Classifiers</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">5384DAA61C3B40CBCAB91129A6FA56ED</idno>
					<note type="submission">Submitted on 20 Dec 2021</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-04-12T14:45+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>published or not. The documents may come from teaching and research institutions in France or abroad, or from public or private research centers.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>There has been an increasing trend in recent years to leverage machine learning methods to automate decision-making processes. However, for many applications, the adoption of such methods cannot rely solely on their prediction performance. For example, the European Union's General Data Protection Regulation, which became enforceable on 25 May 2018, introduces a right to explanation for all individuals so that they can obtain "meaningful explanations of the logic involved" when automated decision-making has "legal effects" on individuals or similarly "significantly affecting" them<ref type="foot" target="#foot_0">1</ref> . Therefore, in addition to their prediction performance, machine learning methods have to be assessed on how they can supply their decisions with explanations.</p><p>The performance of a machine learning method can be assessed by the extent to which it correctly predicts unseen instances. A metric like the accuracy score commonly measures the performance of a classification model. However, there is no standard approach to assess explainability. First, there is no mathematical definition of explainability. A definition proposed by <ref type="bibr">[Miller, 2019]</ref> states that the higher the explainability of a machine learning algorithm, the easier it is for someone to comprehend why certain decisions or predictions have been made. Second, there are several methods belonging to different categories (explainability-by-design, post-hoc model-specific explainability and post-hoc modelagnostic explainability) <ref type="bibr" target="#b16">[Du et al., 2020]</ref>, which provide their own form of explanations.</p><p>The requirements for explainable machine learning methods are dependent upon the application and to whom the explanations are intended for <ref type="bibr" target="#b64">[Tomsett et al., 2018;</ref><ref type="bibr">Bohlen-der and Kóhl, 2019]</ref>. In order to match these requirements and conduct experiments to validate the usefulness of the explanations by the end-users, there is a need to have a comprehensive assessment of the explainability of the existing methods. Doshi-Velez and Kim <ref type="bibr">[2017]</ref> claim that creating a shared language is essential for the evaluation and comparison of machine learning methods, which is currently challenging without a set of explanation characteristics. As far as we have seen, there is no existing framework which defines a set of explanation characteristics that systematize the assessment of the explainability of existing machine learning methods.</p><p>Hence, in this paper, we propose a new framework to assess and benchmark the performance-explainability characteristics of machine learning methods. The framework hypothesizes a set of explanation characteristics, and as emphasized in <ref type="bibr">[Wolf, 2019]</ref>, focuses on what people might need to understand about machine learning methods in order to act in concert with the model outputs. The framework does not claim to be exhaustive and excludes application-specific implementation constraints like time, memory usage and privacy. It could be a basis for the development of a comprehensive assessment of the machine learning methods with regards to their performance and explainability and for the design of new machine learning methods. Due to space constraint, we limit the illustration of the use of the framework to one category of machine learning methods and we choose the Multivariate Time Series (MTS) classifiers. Multivariate data which integrates temporal evolution has received significant interests over the past decade, driven by automatic and high-resolution monitoring applications (e.g. healthcare <ref type="bibr" target="#b39">[Li et al., 2018]</ref>, mobility <ref type="bibr" target="#b32">[Jiang et al., 2019]</ref>, natural disasters <ref type="bibr">[Fauvel et al., 2020a]</ref>). Moreover, the available explainability solutions to support the current state-of-the-art MTS classifiers remain limited, so this category of methods appears meaningful to assess for us.</p><p>The contributions of this paper are the following:</p><p>• We present a new performance-explainability analytical framework to assess and benchmark machine learning methods; • We detail a set of characteristics that systematize the performance-explainability assessment of existing machine learning methods; • We illustrate the use of the framework by benchmarking the current state-of-the-art MTS classifiers.</p><p>In this section, we first position this paper in the related work and introduce the different categories of explainability methods as a background to the notions that will be discussed in the framework. Then, we present the state-of-the-art machine learning methods that will be used to illustrate the framework, i.e. MTS classifiers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Explainability</head><p>Multiple taxonomies of explainability methods have been derived from different frameworks <ref type="bibr" target="#b24">[Guidotti et al., 2018;</ref><ref type="bibr" target="#b68">Ventocilla et al., 2018;</ref><ref type="bibr" target="#b16">Du et al., 2020]</ref>. However, none of them defines a set of explanation characteristics that systematize the assessment of the explainability of existing machine learning methods. <ref type="bibr" target="#b24">[Guidotti et al., 2018]</ref> provides a classification of the main problems addressed in the literature with respect to the notion of explanation and the type of machine learning systems. <ref type="bibr" target="#b68">[Ventocilla et al., 2018]</ref> proposes a high-level taxonomy of interpretable and interactive machine learning composed of six elements (Dataset, Optimizer, Model, Predictions, Evaluator and Goodness). And, <ref type="bibr" target="#b16">[Du et al., 2020]</ref> categorizes existing explainability methods of machine learning models into either by design or post-hoc explainability. As our framework aims to cover all types of methods, we do not present the frameworks focusing on a particular type of explainability methods (e.g. <ref type="bibr" target="#b43">[Lundberg and Lee, 2017;</ref><ref type="bibr" target="#b2">Ancona et al., 2018;</ref><ref type="bibr" target="#b30">Henin and Métayer, 2019]</ref>).</p><p>A five-step method to understand the requirements for explainable AI systems has been published in <ref type="bibr" target="#b28">[Hall et al., 2019]</ref>. The five steps are: explainee role definition, explanation characteristics identification, requirements collection, existing methods assessment and requirements/existing methods mapping. Our framework can be positioned as a further development of the fourth step of the method by detailing a set of explanations characteristics that systematize the assessment of existing methods. Our framework does not include application-specific implementation constraints like time, memory usage and privacy.</p><p>As a background to the notions that will be discussed in the framework, we introduce the three commonly recognized categories (explainability-by-design, post-hoc modelspecific explainability and post-hoc model-agnostic explainability) <ref type="bibr" target="#b16">[Du et al., 2020]</ref> to which all of the explainability methods are belonging to. First, some machine learning models provide explainability-by-design. These self-explanatory models incorporate explainability directly to their structures. This category includes, for example, decision trees, rulebased models and linear models. Next, post-hoc modelspecific explainability methods are specifically designed to extract explanations for a particular model. These methods usually derive explanations by examining internal model structures and parameters. For example, a method has been designed to measure the contribution of each feature in random forests <ref type="bibr" target="#b48">[Palczewska et al., 2013]</ref>; and another one has been designed to identify the regions of input data that are important for predictions in convolutional neural networks using the class-specific gradient information <ref type="bibr" target="#b58">[Selvaraju et al., 2019]</ref>. Finally, post-hoc model-agnostic explainability methods provide explanations from any machine learning model.</p><p>These methods treat the model as a black-box and does not inspect internal model parameters. For example, the permutation feature importance method <ref type="bibr" target="#b0">[Altmann et al., 2010]</ref> and the methods using an explainable surrogate model <ref type="bibr" target="#b37">[Lakkaraju et al., 2017;</ref><ref type="bibr" target="#b43">Lundberg and Lee, 2017;</ref><ref type="bibr" target="#b52">Ribeiro et al., 2018;</ref><ref type="bibr" target="#b26">Guidotti et al., 2019]</ref> belong to this category.</p><p>The explainability methods presented reflect the diversity of explanations generated to support model predictions, therefore the need for a framework in order to benchmark the machine learning methods explainability. The next section present the MTS classifiers that will be used to illustrate the framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Multivariate Time Series Classifiers</head><p>The state-of-the-art MTS classifiers consist of a diverse range of methods which can be categorized into three families: similarity-based, feature-based and deep learning methods.</p><p>Similarity-based methods make use of similarity measures to compare two MTS. Dynamic Time Warping (DTW) has been shown to be the best similarity measure to use along the k-Nearest Neighbors (k-NN) <ref type="bibr" target="#b60">[Seto et al., 2015]</ref>. There are two versions of kNN-DTW for MTS: dependent (DTW D ) and independent (DTW I ). Neither dominates over the other <ref type="bibr" target="#b62">[Shokoohi-Yekta et al., 2017]</ref> from an accuracy perspective but DTW I allows the analysis of distance differences at feature level.</p><p>Next, feature-based methods include shapelets (gRSF <ref type="bibr" target="#b36">[Karlsson et al., 2016]</ref>, UFS <ref type="bibr" target="#b70">[Wistuba et al., 2015]</ref>) and bag-of-words (LPS <ref type="bibr" target="#b8">[Baydogan and Runger, 2016]</ref>, mv-ARF <ref type="bibr" target="#b66">[Tuncel and Baydogan, 2018]</ref>, SMTS <ref type="bibr" target="#b6">[Baydogan and Runger, 2014]</ref>, WEASEL+MUSE <ref type="bibr" target="#b56">[Schäfer and Leser, 2017]</ref>) models. WEASEL+MUSE shows better results compared to gRSF, LPS, mv-ARF, SMTS and UFS on average (20 MTS datasets). WEASEL+MUSE generates a bag-of-words representation by applying various sliding windows with different sizes on each discretized dimension (Symbolic Fourier Approximation) to capture features (unigrams, bigrams, dimension identification). Following a feature selection with chi-square test, it classifies the MTS based on a logistic regression classifier.</p><p>Then, deep learning methods use Long-Short Term Memory (LSTM) and/or Convolutional Neural Networks (CNN). According to the results published, the current state-of-theart model (MLSTM-FCN) is proposed in <ref type="bibr" target="#b34">[Karim et al., 2019]</ref> and consists of a LSTM layer and a stacked CNN layer along with Squeeze-and-Excitation blocks to generate latent features.</p><p>Therefore, we choose to benchmark the performanceexplainability of the best-in-class for each similaritybased, feature-based and deep learning category (DTW I , WEASEL+MUSE and MLSTM-FCN classifiers). The next section introduces the performance-explainability framework, which is illustrated with the benchmark of the bestin-class MTS classifiers in section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Performance-Explainability Framework</head><p>The framework aims to respond to the different questions an end-user may ask to take an informed decision based on the predictions made by a machine learning model: What is the level of performance of the model? Is the model comprehensible? Is it possible to get an explanation for a particular instance? Which kind of information does the explanation provide? Can we trust the explanations? What is the target user category of the explanations? The performanceexplainability framework that we propose is composed of the following components, which will also be translated into terms specific to our application (MTS classifiers) whenever relevant:</p><p>Performance What is the level of performance of the model? The first component of the framework characterizes the performance of a machine learning model. Different methods (e.g. holdout, k-fold cross-validation) and metrics (e.g. accuracy, F-measure, Area Under the ROC Curve) exist to evaluate the performance of a machine learning model <ref type="bibr" target="#b72">[Witten et al., 2016]</ref>. However, there is no consensus on an evaluation procedure to assess the performance of a machine learning model. Recent work suggests that the definition of such an evaluation procedure necessitates the development of a measurement theory for machine learning <ref type="bibr">[Flach, 2019]</ref>. Many of the problems stem from a limited appreciation of the importance of the scale on which the evaluation measures are expressed.</p><p>Then, in current practices, the choice of a metric to evaluate the performance of a machine learning model depends on the application. According to the application, a metric aligned with the goal of the experiments is selected, which prevents the performance comparison of machine learning models across applications.</p><p>Therefore, the performance component in the framework is defined as a first step towards a standard procedure to assess the performance of machine learning models. It corresponds to the relative performance of a model on a particular application. More specifically, it indicates the relative performance of the models as compared to the state-of-the-art model on a particular application and an evaluation setting. This definition allows the categorization of the models' performance on an application and an evaluation setting. In the case of different applications with a similar machine learning task, the performance component can give the list of models which outperformed current state-of-the-art models on their respective application. Thus, it points to certain models that could be interesting to evaluate on a new application, without providing guarantee that these models would perform the same on this new application. We propose an assessment of the performance in three categories:</p><p>• Best: best performance. It corresponds to the performance of the first ranked model on the application following an evaluation setting (models, evaluation method, datasets); • Similar: performance similar to that of the state-of-theart models. Based on the same evaluation setting, it corresponds to all the models which do not show a statistically significant performance difference with the second ranked model. For example, the statistical comparison of multiple classifiers on multiple datasets is usually presented on a critical difference diagram <ref type="bibr">[Demšar, 2006]</ref>;</p><p>• Below: performance below that of the state-of-the-art models. It corresponds to the performance of the remaining models with the same evaluation setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model Comprehensibility Is the model comprehensible?</head><p>The model comprehensibility corresponds to the ability for the user to understand how the model works and produces certain predictions. Comprehensibility is tightly linked to the model complexity; yet, there is no consensus on model complexity assessment <ref type="bibr" target="#b24">[Guidotti et al., 2018]</ref>. Currently, two categories of models are commonly recognized: "whitebox" models, i.e. easy-to-understand models, and "blackbox" models, i.e. complicated-to-understand models <ref type="bibr">[Lipton, 2016]</ref>. For example, many rule-based models and decision trees are regarded as "white-box" models while ensemble methods and deep learning models are "black-box" models. Not all rule-based models or decision trees are "whitebox" models. Cognitive limitations of humans place restrictions on the complexity of the approximations that are understandable to humans. For example, a decision tree with a hundred levels cannot be considered as an easy-to-understand model <ref type="bibr" target="#b37">[Lakkaraju et al., 2017]</ref>.</p><p>Nevertheless, the distinction between "white-box" models and "black-box" models is clear among the machine learning methods of this paper. The state-of-the-art MTS classifiers are all "black-box" except one which is an easy-to-understand similarity-based approach. Therefore, due to space limitation, we propose a first assessment of the comprehensibility in two categories and we plan to further elaborate this component in future work:</p><p>• Black-Box: "black-box" model, i.e. complicated-tounderstand models; • White-Box: "white-box" model, i.e. easy-to-understand models.</p><p>Granularity of the Explanations Is it possible to get an explanation for a particular instance? The granularity indicates the level of possible explanations. Two levels are generally distinguished: global and local <ref type="bibr" target="#b16">[Du et al., 2020]</ref>. Global explainability means that explanations concern the overall behavior of the model across the full dataset, while local explainability informs the user about a particular prediction. Some methods can provide either global or local-only explainability while other methods can provide both (e.g. decision trees). Therefore, we propose an assessment of the granularity in three categories:</p><p>• Global: global explainability;</p><p>• Local: local explainability;</p><p>• Global &amp; Local: both global and local explainability. Information Type Which kind of information does the explanation provide? The information type informs the user about the kind of information communicated. The most valuable information is close to the language of human reasoning, with causal and counterfactual rules <ref type="bibr" target="#b50">[Pearl and Mackenzie, 2018]</ref>. Causal rules can tell the user that certain observed variables are the causes of specific model predictions. However, machine learning usually leverages statistical associations in the data and do not convey information about the causal relationships among the observed variables and the unobserved confounding variables. The usual statistical asso-ciations discovered by machine learning methods highly depend on the machine learning task. Therefore, we first give a generic high-level definition of the information type and then we detail and illustrate it for the application case of this paper (MTS classification). We propose a generic assessment of the information type in 3 categories from the least to the most informative:</p><p>• Importance: the explanations reveal the relative importance of each dataset variable on predictions. The importance indicates the statistical contribution of each variable to the underlying model when making decisions; • Patterns: the explanations provide the small conjunctions of symbols with a predefined semantic (patterns) associated with the predictions; • Causal: the most informative category corresponds to explanations under the form of causal rules;</p><p>In this paper, the issue of Multivariate Time Series (MTS) classification is addressed. A MTS M = {x 1 , ..., x d } ∈ R d * l is an ordered sequence of d ∈ N streams with x i = (x i,1 , ..., x i,l ), where l is the length of the time series and d is the number of multivariate dimensions. Thus, considering the MTS data type, the information can be structured around the features, i.e. the observed variables, and the time. We propose to decompose the 3 categories presented into 8 categories. In addition, we will illustrate each of these categories with an application in the medical field. Figure <ref type="figure" target="#fig_0">1</ref> shows the first MTS of the UEA Atrial Fibrilation <ref type="bibr" target="#b4">[Bagnall et al., 2018]</ref> test set that belongs to the class Non-Terminating Atrial Fibrilation. This MTS is composed of two dimensions (two channels ECG) with a length of 640 (5 second period with 128 samples per second). It is worth noting that the explanations provided to illustrate each category are assumptive rather than validated, they are given as illustrative in nature. • Features (Importance): the explanations reveal the relative importance of the features on predictions. For example, in order to support a model output from the MTS of the Figure <ref type="figure" target="#fig_0">1</ref>, the explanations could tell the user that the channel 2 has a greater importance on the prediction than the channel 1;</p><p>• Features + Time (Importance): the explanations provide the relative importance of the features and timestamps on predictions. For example, in order to support a model output from the MTS of the Figure <ref type="figure" target="#fig_0">1</ref>, the explanations could tell the user that the channel 2 has a greater importance on the prediction than the channel 1 and that the timestamps are in increasing order of importance on the prediction; • Features + Time + Values (Importance): in addition to the relative importance of the features and timestamps on predictions, the explanations indicate the discriminative values of a feature for each class. For example in Figure <ref type="figure" target="#fig_0">1</ref>, the explanations could give the same explanations as the previous category, plus, it could tell the user that the timestamps with the highest importance are associated with high values (values above 0.15) on the channel 2; • Uni Itemsets (Patterns): the explanations provide patterns under the form of groups of values, also called itemsets, which occur per feature and are associated with the prediction. For example, in order to support a model output from the MTS of the Figure <ref type="figure" target="#fig_0">1</ref>, the explanations could tell the user that the following itemsets are associated with the prediction: {channel 1: extremely high value (above 1); channel 1: low value (below -0.05)} and {channel 2: high value (above 0.15); channel 2: extremely low value (below -0.1)}. The first itemset can be read as: the prediction is associated with the occurence on the channel 1 of an extremely high value being above 1 and a low value being below -0.05 at another moment, without information on which one appears first; • Multi Itemsets (Patterns): the explanations provide patterns under the form of multidimensional itemsets, i.e. groups of values composed of different features, which are associated with the prediction. For example, in order to support a model output from the MTS of the Figure <ref type="figure" target="#fig_0">1</ref>, the explanations could tell the user that the following itemset is associated with the prediction: {channel 1: extremely high value (above 1); channel 2: high value (above 0.15)}; • Uni Sequences (Patterns): the explanations provide patterns under the form of ordered groups of values, also called sequences, which occur per feature and are associated with the prediction. For example, in order to support a model output from the MTS of the Figure <ref type="figure" target="#fig_0">1</ref>, the explanations could tell the user that the following sequences are associated with the prediction: &lt;channel 1: extremely high value (above 1); channel 1: low value (below -0.05)&gt; and &lt;channel 2: high values (above 0.15) with an increase during 1 second&gt;. The first sequence can be read as: the prediction is associated with the occurrence on the channel 1 of an extremely high value being above 1 followed by a low value being below -0.05; • Multi Sequences (Patterns): the explanations provide patterns under the form of multidimensional sequences, i.e. ordered groups of values composed of different features, which are associated with the prediction. For example, in order to support a model output from the MTS 1 Predefined train/test splits and an arithmetic mean of the accuracies on 35 public datasets <ref type="bibr" target="#b34">[Karim et al., 2019]</ref>. As presented in section 2.2, the models evaluated in the benchmark are: DTW D , DTW I , gRSF, LPS, MLSTM-FCN, mv-ARF, SMTS, UFS and WEASEL+MUSE.</p><p>of the Figure <ref type="figure" target="#fig_0">1</ref>, the explanations could tell the user that the following sequence is associated with the prediction: &lt;channel 1: extremely high value (above 1); channel 2: high values (above 0.15) with an increase during 1 second&gt;; • Causal: the last category corresponds to explanations under the form of causal rules. For example, in order to support a model output from the MTS of the Figure <ref type="figure" target="#fig_0">1</ref>, the explanations could tell the user that the following rule applies: if (channel 1: extremely high value (above 1)) &amp; (channel 2: high values (above 0.15) with an increase during 1 second), then the MTS belongs to the class Non-Terminating Atrial Fibrilation. Faithfulness Can we trust the explanations? The faithfulness corresponds to the level of trust an end-user can have in the explanations of model predictions, i.e. the level of relatedness of the explanations to what the model actually computes. An explanation extracted directly from the original model is faithful by definition. Some post-hoc explanation methods propose to approximate the behavior of the original "blackbox" model with an explainable surrogate model. The explanations from the surrogate models cannot be perfectly faithful with respect to the original model <ref type="bibr">[Rudin, 2019]</ref>. The fidelity criteria is used to quantify the faithfulness by the extent to which the surrogate model imitates the prediction score of the original model <ref type="bibr" target="#b24">[Guidotti et al., 2018]</ref>.</p><p>In this paper, two MTS classifiers use an explainable surrogate model among the three state-of-the-art methods presented in section 4. However, there is no need to distinguish between the degree of fidelity of the surrogate models for the purpose of the comparison in this paper. Therefore, due to space limitation, we propose a first assessment of the faithfulness in two categories and we plan to further elaborate this component in future work:</p><p>• Imperfect: imperfect faithfulness (use of an explainable surrogate model); • Perfect: perfect faithfulness. User category What is the target user category of the explanations? The user category indicates the audience to whom the explanations are accessible. The user's experience will affect what kind of cognitive chunks they have, that is, how they organize individual elements of information into collections <ref type="bibr" target="#b47">[Neath and Surprenant, 2003</ref>]. Thus, it could be interesting to categorize the user types and associate with the model to whom the explanations will be accessible to. The broader the audience, the better are the explanations. Therefore, we propose an assessment in three categories:</p><p>• Machine Learning Expert;</p><p>• Domain Expert: domain experts (e.g. professionals, researchers); • Broad Audience: non-domain experts (e.g. policy makers). In order to compare the methods visually using the proposed framework, the different aspects can be represented on a parallel coordinates plot. A parallel coordinate plot allows a 2-dimensional visualization of a high dimensional dataset and is suited for the categorical data of this framework. The next section presents an example of parallel coordinates plots comparing the state-of-the-art MTS classifiers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Application to Multivariate Time Series Classifiers</head><p>This section shows how the framework presented in the previous section can be used to assess and benchmark the stateof-the-art MTS classifiers. As introduced in section 2.2, the state-of-the-art MTS classifiers are: DTW I , MLSTM-FCN and WEASEL+MUSE. The results of the assessment are summarized in Table <ref type="table" target="#tab_0">1</ref>, illustrated in Figure <ref type="figure" target="#fig_1">2</ref> and detailed in the following paragraphs.</p><p>The first MTS classifier belongs to the similarity-based category and is the one-nearest neighbor MTS classifier with DTW distance (DTW I ). DTW I classifies MTS based on the label of the nearest sample and a similarity calculated as the cumulative distances of all dimensions independently measured under DTW. For each MTS, the explanation supporting the classification is the ranking of features and timestamps in decreasing order of their DTW distance with the nearest MTS. Based on predefined train/test splits and an arithmetic mean of the accuracies, DTW I underperforms the current state-of-the-art MTS classifiers on the 35 public datasets (Performance: Below). The results from <ref type="bibr" target="#b34">[Karim et al., 2019]</ref> shows that DTW I has a statistically significant lower performance than MLSTM-FCN and WEASEL+MUSE. Furthermore, DTW I supports its predictions with limited information (Information: Features+Time) that needs to be analyzed by a domain expert to ensure that it is relevant for the application (User: Domain Expert). However, DTW I is an easyto-understand model (Comprehensibility: White-Box) which provides faithful explanations (Faithfulness: Perfect) for each MTS (Granularity: Local).</p><p>Then, we can analyze MLSTM-FCN and WEASEL+ MUSE together. First, based on predefined train/test splits and an arithmetic mean of the accuracies, MLSTM-FCN exhibits the best performance on the 35 public datasets   <ref type="bibr" target="#b34">[Karim et al., 2019]</ref>. As presented in section 2.2, the models evaluated in the benchmark are: DTWD, DTWI , gRSF, LPS, MLSTM-FCN, mv-ARF, SMTS, UFS and WEASEL+MUSE.</p><p>(Performance: Best) <ref type="bibr" target="#b34">[Karim et al., 2019]</ref>, followed by WEASEL+MUSE (Performance: Similar). Second, both MLSTM-FCN and WEASEL+MUSE are "black-box" classifiers without being explainable-by-design or having a posthoc model-specific explainability method. Thus, the explainability characteristics of these models depend on the choice of the post-hoc model-agnostic explainability method. We have selected SHapley Additive exPlanations (SHAP) <ref type="bibr" target="#b43">[Lundberg and Lee, 2017]</ref>, a state-of-the-art post-hoc model-agnostic explainability method offering explanations at all granularity levels. SHAP method measures how much each variable (Features+Time) impacts predictions and comes up with a ranking of the variables which could be exploited by domain experts. The combination of MLSTM-FCN and WEASEL+MUSE with SHAP enables them to outperform DTW I while reaching explanations with a similar level of information (Information: Features+Time, DTW I : Fea-tures+Time), in the meantime remaining accessible to the same user category (User: Domain Expert, DTW I : Domain Expert). However, as opposed to DTW I , SHAP relies on a surrogate model which cannot provide perfectly faithful explanations (Faithfulness: Imperfect, DTW I : Perfect).</p><p>Therefore, based on the performance-explainability framework introduced, if a "white-box" model and perfect faithfulness are not required, it would be preferable to choose MLSTM-FCN with SHAP instead of the other state-of-the-art MTS classifiers on average on the 35 public datasets. In addition to its better level of performance, MLSTM-FCN with SHAP provides the same level of information and at all granularity levels.</p><p>However, the imperfect faithfulness of the explanations could prevent the use of MLSTM-FCN with a surrogate explainable model on numerous applications. In addition, the level of information provided to support the predictions remains limited (Information: Features+Time). Therefore, based on the assessment of the current state-of-the-art MTS classifiers with the framework proposed, it would be valuable for instance to design some new high-performing MTS classifiers which provide faithful and more informative explanations. For example, it could be interesting to work in the direction proposed in <ref type="bibr">[Fauvel et al., 2020b]</ref>. It presents a new MTS classifier (XEM) which reconciles performance (Performance: Best) and faithfulness while providing the time window used to classify the whole MTS (Information: Uni Sequences). XEM is based on a new hybrid ensemble method that combines an explicit approach to handle the biasvariance trade-off and an implicit approach to individualize classifier errors on different parts of the training data <ref type="bibr" target="#b18">[Fauvel et al., 2019]</ref>. Nevertheless, the explanations provided by XEM are only available per MTS (Granularity: Local) and the level of information could be further improved. As suggested by the authors, it could be interesting to analyze the time windows identified for each class to determine if they contain some common multidimensional sequences (Information: Multi Sequences, Granularity: Both Global &amp; Local). These patterns could also broaden the audience as they would summarize the key information in the discriminative time windows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We have presented a new performance-explainability analytical framework to assess and benchmark the machine learning methods. The framework details a set of characteristics that systematize the performance-explainability assessment of machine learning methods. In addition, it can be employed to identify ways to improve current machine learning methods and to design new ones. Finally, we have illustrated the use of the framework by benchmarking the current state-ofthe-art MTS classifiers. With regards to future work, we plan to further elaborate the definition of the different components of the framework (especially the Model Comprehensibility, the Information Type and the Faithfulness) and evaluate the relevance of integrating new components. Then, we plan to apply the framework extensively to assess the different types of existing machine learning methods.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The first MTS sample of the UEA Atrial Fibrilation test set. It belongs to the class Non-Terminating Atrial Fibrilation and is composed of two channels ECG on a 5 second period (128 samples per second).</figDesc><graphic coords="5,60.08,444.51,230.84,152.81" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure2: Parallel coordinates plot of the state-of-the-art MTS classifiers. Performance evaluation method: predefined train/test splits and an arithmetic mean of the accuracies on 35 public datasets<ref type="bibr" target="#b34">[Karim et al., 2019]</ref>. As presented in section 2.2, the models evaluated in the benchmark are: DTWD, DTWI , gRSF, LPS, MLSTM-FCN, mv-ARF, SMTS, UFS and WEASEL+MUSE.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Summary of framework results of the state-of-the-art MTS classifiers.</figDesc><table><row><cell></cell><cell>Similarity-Based</cell><cell>Feature-Based</cell><cell>Deep Learning</cell></row><row><cell></cell><cell>DTW I</cell><cell>WEASEL+MUSE with SHAP</cell><cell>MLSTM-FCN with SHAP</cell></row><row><cell>Performance</cell><cell>Below 1</cell><cell>Similar 1</cell><cell>Best 1</cell></row><row><cell>Comprehensibility</cell><cell>White-Box</cell><cell>Black-Box</cell><cell>Black-Box</cell></row><row><cell>Granularity</cell><cell>Local</cell><cell>Both Global &amp; Local</cell><cell>Both Global &amp; Local</cell></row><row><cell>Information</cell><cell>Features+Time</cell><cell>Features+Time</cell><cell>Features+Time</cell></row><row><cell>Faithfulness</cell><cell>Perfect</cell><cell>Imperfect</cell><cell>Imperfect</cell></row><row><cell>User</cell><cell>Domain Expert</cell><cell>Domain Expert</cell><cell>Domain Expert</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>https://ec.europa.eu/info/law/law-topic/data-protection en</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>This work was supported by the <rs type="funder">French National Research Agency</rs> under the <rs type="programName">Investments for the Future Program</rs> (<rs type="grantNumber">ANR-16-CONV-0004</rs>) and the <rs type="funder">Inria Project Lab "</rs><rs type="projectName">Hybrid Approaches for Interpretable AI</rs><rs type="funder">" (HyAIAI)</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_H8tm7NP">
					<idno type="grant-number">ANR-16-CONV-0004</idno>
					<orgName type="program" subtype="full">Investments for the Future Program</orgName>
				</org>
				<org type="funded-project" xml:id="_XAVj9V8">
					<orgName type="project" subtype="full">Hybrid Approaches for Interpretable AI</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><surname>Altmann</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Permutation Importance: A Corrected Feature Importance Measure</title>
		<author>
			<persName><forename type="first">A</forename><surname>Altmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Tolosi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Sander</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lengauer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1340" to="1347" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName><surname>Ancona</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Towards Better Understanding of Gradient-Based Attribution Methods for Deep Neural Networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ancona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Ceolini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Öztireli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gross</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th International Conference on Learning Representations</title>
		<meeting>the 6th International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title/>
		<author>
			<persName><surname>Bagnall</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">The UEA UCR Time Series Classification Archive</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bagnall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lines</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Keogh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Runger</forename><surname>Baydogan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning a Symbolic Representation for Multivariate Time Series Classification</title>
		<author>
			<persName><forename type="first">M</forename><surname>Baydogan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Runger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Data Mining and Knowledge Discovery</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="400" to="422" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Runger</forename><surname>Baydogan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Time Series Representation and Similarity Based on Local Autopatterns</title>
		<author>
			<persName><forename type="first">M</forename><surname>Baydogan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Runger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Data Mining and Knowledge Discovery</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="476" to="509" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Kóhl</forename><surname>Bohlender</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Towards a Characterization of Explainable Systems</title>
		<author>
			<persName><forename type="first">D</forename><surname>Bohlender</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kóhl</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">ArXiv</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title/>
		<author>
			<persName><surname>Demšar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Statistical Comparisons of Classifiers over Multiple Data Sets</title>
		<author>
			<persName><forename type="first">J</forename><surname>Demšar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="1" to="30" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Kim</forename><surname>Doshi-Velez</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Towards a Rigorous Science of Interpretable Machine Learning</title>
		<author>
			<persName><forename type="first">F</forename><surname>Doshi-Velez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kim</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">ArXiv</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title/>
		<author>
			<persName><surname>Du</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Techniques for Interpretable Machine Learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title/>
		<author>
			<persName><surname>Fauvel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Towards Sustainable Dairy Management -A Machine Learning Enhanced Method for Estrus Detection</title>
		<author>
			<persName><forename type="first">K</forename><surname>Fauvel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Masson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">É</forename><surname>Fromont</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Faverdin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Termier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 25th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A Distributed Multi-Sensor Machine Learning Approach to Earthquake Early Warning</title>
		<author>
			<persName><surname>Fauvel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th AAAI Conference on Artificial Intelligence</title>
		<meeting>the 34th AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">XEM: An Explainable-by-Design Ensemble Method for Multivariate Time Series Classification</title>
		<author>
			<persName><surname>Fauvel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note type="report_type">ArXiv</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title/>
		<author>
			<persName><surname>Flach</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Performance Evaluation in Machine Learning: The Good, the Bad, the Ugly, and the Way Forward</title>
		<author>
			<persName><forename type="first">P</forename><surname>Flach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33rd AAAI Conference on Artificial Intelligence</title>
		<meeting>the 33rd AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title/>
		<author>
			<persName><surname>Guidotti</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">A Survey of Methods for Explaining Black Box Models</title>
		<author>
			<persName><forename type="first">R</forename><surname>Guidotti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Monreale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ruggieri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Turini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Giannotti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Pedreschi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>ACM Computing Survey</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title/>
		<author>
			<persName><surname>Guidotti</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Factual and Counterfactual Explanations for Black Box Decision Making</title>
		<author>
			<persName><forename type="first">R</forename><surname>Guidotti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Monreale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Giannotti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Pedreschi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ruggieri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Turini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Intelligent Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="14" to="23" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title/>
		<author>
			<persName><surname>Hall</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A Systematic Method to Understand Requirements for Explainable AI (XAI) Systems</title>
		<author>
			<persName><forename type="first">M</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Harbone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Tomsett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Galetic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Quintana-Amate</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Nottle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Preece</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IJCAI Workshop on Explainable Artificial Intelligence</title>
		<meeting>the IJCAI Workshop on Explainable Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Métayer</forename><surname>Henin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Towards a Generic Framework for Black-Box Explanation Methods</title>
		<author>
			<persName><forename type="first">C</forename><surname>Henin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">Le</forename><surname>Métayer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IJCAI Workshop on Explainable Artificial Intelligence</title>
		<meeting>the IJCAI Workshop on Explainable Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title/>
		<author>
			<persName><surname>Jiang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Deep-UrbanEvent: A System for Predicting Citywide Crowd Dynamics at Big Events</title>
		<author>
			<persName><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><surname>Shibasaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 25th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title/>
		<author>
			<persName><surname>Karim</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Multivariate LSTM-FCNs for Time Series Classification</title>
		<author>
			<persName><forename type="first">F</forename><surname>Karim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Majumdar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Darabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Harford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">116</biblScope>
			<biblScope unit="page" from="237" to="245" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Generalized Random Shapelet Forests</title>
		<author>
			<persName><surname>Karlsson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Data Mining and Knowledge Discovery</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1053" to="1085" />
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title/>
		<author>
			<persName><surname>Lakkaraju</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Interpretable and Explorable Approximations of Black Box Models</title>
		<author>
			<persName><forename type="first">H</forename><surname>Lakkaraju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Kamar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Caruana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the KDD Workshop on Fairness, Accountability, and Transparency in Machine Learning</title>
		<meeting>the KDD Workshop on Fairness, Accountability, and Transparency in Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title/>
		<author>
			<persName><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">TATC: Predicting Alzheimer&apos;s Disease with Actigraphy Data</title>
		<author>
			<persName><forename type="first">Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kwok</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 24th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title/>
		<author>
			<persName><surname>Lipton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">The Mythos of Model Interpretability</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Lipton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ICML Workshop on Human Interpretability in Machine Learning</title>
		<meeting>the ICML Workshop on Human Interpretability in Machine Learning</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Lee</forename><surname>Lundberg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">A Unified Approach to Interpreting Model Predictions</title>
		<author>
			<persName><forename type="first">S</forename><surname>Lundberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on Neural Information Processing Systems</title>
		<meeting>the 31st International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title/>
		<author>
			<persName><surname>Miller</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Explanation in Artificial Intelligence: Insights from the Social Sciences</title>
		<author>
			<persName><forename type="first">T</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IJCAI-PRICAI 2020 Workshop on Explainable AI (XAI)</title>
		<meeting>the IJCAI-PRICAI 2020 Workshop on Explainable AI (XAI)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">267</biblScope>
			<biblScope unit="page" from="1" to="38" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Human Memory: An Introduction to Research, Data, and Theory</title>
		<author>
			<persName><forename type="first">Surprenant</forename><surname>Neath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Neath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Surprenant</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003">2003. 2003</date>
			<publisher>Thomson/Wadsworth</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title/>
		<author>
			<persName><surname>Palczewska</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Interpreting Random Forest Models Using a Feature Contribution Method</title>
		<author>
			<persName><forename type="first">A</forename><surname>Palczewska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Palczewski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Neagu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th IEEE International Conference on Information Reuse Integration</title>
		<meeting>the 14th IEEE International Conference on Information Reuse Integration</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Mackenzie</forename><surname>Pearl</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">The Book of Why: The New Science of Cause and Effect</title>
		<author>
			<persName><forename type="first">J</forename><surname>Pearl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mackenzie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>Basic Books</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title/>
		<author>
			<persName><surname>Ribeiro</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Anchors: High-Precision Model-Agnostic Explanations</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ribeiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Guestrin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd AAAI Conference on Artificial Intelligence</title>
		<meeting>the 32nd AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title/>
		<author>
			<persName><surname>Rudin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Stop Explaining Black Box Machine Learning Models for High Stakes Decisions and Use Interpretable Models Instead</title>
		<author>
			<persName><forename type="first">C</forename><surname>Rudin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="206" to="215" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Leser</forename><surname>Schäfer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Schäfer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Leser</surname></persName>
		</author>
		<title level="m">Multivariate Time Series Classification with WEASEL + MUSE</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">ArXiv</note>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title/>
		<author>
			<persName><surname>Selvaraju</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Grad-CAM: Visual Explanations from Deep Networks via Gradient-Based Localization</title>
		<author>
			<persName><forename type="first">R</forename><surname>Selvaraju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cogswell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">128</biblScope>
			<biblScope unit="page" from="336" to="359" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title/>
		<author>
			<persName><surname>Seto</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Multivariate Time Series Classification Using Dynamic Time Warping Template Selection for Human Activity Recognition</title>
		<author>
			<persName><forename type="first">S</forename><surname>Seto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Symposium Series on Computational Intelligence</title>
		<meeting>the IEEE Symposium Series on Computational Intelligence</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title/>
		<author>
			<persName><surname>Shokoohi-Yekta</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Generalizing DTW to the Multi-Dimensional Case Requires an Adaptive Approach</title>
		<author>
			<persName><forename type="first">M</forename><surname>Shokoohi-Yekta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Keogh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Data Mining and Knowledge Discovery</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="1" to="31" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title/>
		<author>
			<persName><surname>Tomsett</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Interpretable to Whom? A Role-Based Model for Analyzing Interpretable Machine Learning Systems</title>
		<author>
			<persName><forename type="first">R</forename><surname>Tomsett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Braines</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Harborne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Preece</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chakraborty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ICML Workshop on Human Interpretability in Machine Learning</title>
		<meeting>the ICML Workshop on Human Interpretability in Machine Learning</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Baydogan</forename><surname>Tuncel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Autoregressive Forests for Multivariate Time Series Modeling</title>
		<author>
			<persName><forename type="first">K</forename><surname>Tuncel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Baydogan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">73</biblScope>
			<biblScope unit="page" from="202" to="215" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<title/>
		<author>
			<persName><surname>Ventocilla</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Towards a Taxonomy for Interpretable and Interactive Machine Learning</title>
		<author>
			<persName><forename type="first">E</forename><surname>Ventocilla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Helldin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Riveiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bae</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Lavesson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IJCAI Workshop on Explainable Artificial Intelligence</title>
		<meeting>the IJCAI Workshop on Explainable Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
		<title/>
		<author>
			<persName><surname>Wistuba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
		<title level="m" type="main">Ultra-Fast Shapelets for Time Series Classification</title>
		<author>
			<persName><forename type="first">M</forename><surname>Wistuba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Grabocka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Schmidt-Thieme</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">ArXiv</note>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
		<title level="m" type="main">Data Mining: Practical Machine Learning Tools and Techniques</title>
		<author>
			<persName><surname>Witten</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
			<publisher>Morgan Kaufmann Series</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<monogr>
		<title/>
		<author>
			<persName><surname>Wolf</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Explainability Scenarios: Towards Scenario-Based XAI Design</title>
		<author>
			<persName><forename type="first">C</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th International Conference on Intelligent User Interfaces</title>
		<meeting>the 24th International Conference on Intelligent User Interfaces</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
