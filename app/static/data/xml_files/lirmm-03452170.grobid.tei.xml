<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">TSPred: A framework for nonstationary time series prediction</title>
				<funder>
					<orgName type="full">CNPq</orgName>
				</funder>
				<funder>
					<orgName type="full">FAPERJ</orgName>
				</funder>
				<funder ref="#_fqQ5DUk">
					<orgName type="full">CAPES</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Rebecca</forename><surname>Salles</surname></persName>
							<email>rebeccapsalles@acm.org</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">INRIA</orgName>
								<orgName type="institution" key="instit2">University of Montipellier</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Cefet</forename><forename type="middle">/</forename><surname>Rj</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">INRIA</orgName>
								<orgName type="institution" key="instit2">University of Montipellier</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Esther</forename><surname>Pacitti</surname></persName>
							<email>pacitti@lirmm.fr</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">INRIA</orgName>
								<orgName type="institution" key="instit2">University of Montipellier</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Eduardo</forename><surname>Bezerra</surname></persName>
							<email>ebezerra@cefet-rj.br</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">INRIA</orgName>
								<orgName type="institution" key="instit2">University of Montipellier</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Fabio</forename><surname>Porto</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">INRIA</orgName>
								<orgName type="institution" key="instit2">University of Montipellier</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Eduardo</forename><surname>Ogasawara</surname></persName>
							<email>eogasawara@ieee.org</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">INRIA</orgName>
								<orgName type="institution" key="instit2">University of Montipellier</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">TSPred: A framework for nonstationary time series prediction</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">9225E1DF5B527B452A3DAEB43651FA23</idno>
					<idno type="DOI">10.1016/j.neucom.2021.09.067</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-04-12T14:44+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>time series</term>
					<term>prediction</term>
					<term>nonstationarity</term>
					<term>machine learning</term>
					<term>preprocessing</term>
					<term>transform</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The nonstationary time series prediction is challenging since it demands knowledge of both data transformation and prediction methods. This paper presents TSPred, a framework for nonstationary time series prediction. It differs from the mainstream frameworks since it establishes a prediction process that seamlessly integrates nonstationary time series transformations with state-of-the-art statistical and machine learning methods. It is made available as an R-package, which provides functions for defining and conducting time series prediction, including data pre(post)processing, decomposition, modeling, prediction, and accuracy assessment. Besides, TSPred enables user-defined methods, which significantly expands the applicability of the framework.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The prediction of time series has gained more attention among researchers. Many time series prediction methods have been developed and can be found in the literature <ref type="bibr" target="#b1">[2]</ref>. Several state-of-the-art methods are based on machine learning, especially for long and potentially high-frequent time series. Unfortunately, most of these methods tend to be optimistic regarding their assumptions over the time series.</p><p>A common assumption is the property of stationarity <ref type="bibr" target="#b1">[2]</ref>. In a stationary time series, statistical properties (mean, variance and covariance) do not change over time, which is not common for most real datasets. Generally, nonstationarity demands transformation methods to coerce the time series into stationarity <ref type="bibr" target="#b17">[18]</ref>. Such methods can have different features depending on the implementation of the data properties they consider. Choosing an adequate method for a particular time series application is not a simple task.</p><p>Additionally, nonstationarity can lead to exploring many data transformation and prediction methods for building prediction models. Moreover, the choice of an appropriate transformation setup for a particular time series is not straightforward. The benchmarking of different candidate combinations helps select an appropriate setup for predicting a nonstationary time series.</p><p>In this context, this paper presents TSPred, a framework for nonstationary time series prediction. It specializes in integrating data transformation methods and machine learning methods (MLM) to predict long time series with potentially high frequency. TSPred is made available as an R-package <ref type="bibr" target="#b18">[19]</ref>. It is the first tool to seamlessly integrate a broad range of transformation methods <ref type="bibr" target="#b17">[18]</ref> and state-of-the-art statistical and machine learning prediction methods for addressing nonstationary time series. The package automates the time series prediction process and parameterization while also enabling user-defined prediction methods and data transformations. The features provided by TSPred are shown to be competitive regarding time series prediction accuracy.</p><p>Besides this introduction, Section 2 gives background on the time series prediction problem. Section 3 presents TSPred, while Sections 4 and 5 give empirical results and illustrate its usage. Finally, Section 6 concludes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>preprint 2 Problem and Background</head><p>A general nonstationary time series prediction process usually encompasses five steps <ref type="bibr" target="#b16">[17]</ref>. The first refers to acquiring the training and evaluation datasets and applying nonstationary time series transformation methods. The second and third refer to training the time series prediction model and applying it to an evaluation set to obtain predictions. Forth corresponds to reversing transformations applied in the first step. Finally, the fifth is the evaluation of prediction errors and model fitness.</p><p>This process provides a systematic way of predicting a time series based on a particular setup of preprocessing and prediction methods. It also focuses on prediction/model evaluation, that is, evaluating the accuracy of prediction and the fitness of a model. Such evaluation may indicate a demand for refining and perfecting the preprocessing-modeling setup and its parameters to obtain a more accurate model. This process may be repeated if the evaluated time series prediction model does not reach the desired accuracy. This process enables benchmarking different preprocessingmodeling setups.</p><p>Several authors focused on the task of benchmarking different models in many different domains. Ramey <ref type="bibr" target="#b15">[16]</ref> and Lessmann et al. <ref type="bibr" target="#b11">[12]</ref> developed frameworks for benchmarking classification models and algorithms. Moreover, Bischl et al. <ref type="bibr" target="#b0">[1]</ref> and Eugster and Leisch <ref type="bibr" target="#b5">[6]</ref> developed the R-packages mlr and benchmark, respectively, which provide tools for executing automated experiments when benchmarking a set of models for data mining tasks such as classification and regression. These packages are designed to support tabular data and focus on benchmarking based on plot visualization.</p><p>Hyndman and Khandakar <ref type="bibr" target="#b6">[7]</ref> and Hyndman et al. <ref type="bibr" target="#b7">[8]</ref> present frameworks for automatic forecasting using mainly statistical models such as autoregressive integrated moving average (ARIMA) and exponential smoothing state space model (ETS). Hyndman and Khandakar <ref type="bibr" target="#b6">[7]</ref> produced the well-known R-package named forecast, which can be used for automatic time series prediction. The R-package of Moreno, Rivas, and Godoy <ref type="bibr" target="#b13">[14]</ref> also facilitates time series prediction using simple differencing (DIF) and Box-Cox transform (BCT). Furthermore, we observed three worth mentioning works. Diebold and Mariano <ref type="bibr" target="#b4">[5]</ref> propose various tests to compare the predictive accuracy of two different prediction models. Diebold and Lopez <ref type="bibr" target="#b3">[4]</ref> propose an ensemble approach using different prediction models. Kumar et al. <ref type="bibr" target="#b9">[10]</ref> propose a class of analytics systems to manage model selection using key ideas from data management research.</p><p>All in all, several works present frameworks and tools for MLM performance assessment. Nonetheless, to the best of our knowledge, no work proposes and implements a framework for seamless integration of transformation and time series prediction methods focusing on addressing nonstationary properties. Such framework contributes to the benchmarking of preprocessing-modeling setups for a particular nonstationary time series prediction application.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Framework and Functionalities</head><p>The main functionality modules representing the concept and structure of the framework are depicted in Figure <ref type="figure" target="#fig_0">1</ref>. Together, they represent a particular time series prediction application and encapsulate the process described in Section 2 and all the elements and steps necessary to fulfill it. TSPred has three main functionality modules: Preprocessing &amp; Postprocessing, Modeling &amp; Predicting, and Evaluating.</p><p>The first module is responsible for preprocessing/transforming and postprocessing/reverse transforming time series. It includes the implementation of the main nonstationary time series transformation methods <ref type="bibr" target="#b17">[18]</ref>, being either mappingbased, namely the logarithmic transform (LT), BCT, percentage change transform (PCT), moving average smoother (MAS), and DIF, or splitting-based, such as empirical mode decomposition (EMD) and wavelet transform (WT). Furthermore, it implements other relevant preprocessing methods for time series prediction with MLM: partitioning time series data into training and evaluation datasets, subsetting the time series data into sliding windows, handling of missing values, and data normalization via Min-max normalization (MM) and Adaptive normalization (AN) methods <ref type="bibr" target="#b14">[15]</ref>.</p><p>The Modeling &amp; Predicting module is responsible for modeling and predicting a time series based on a particular time series model. These tasks are specialized for either statistical or machine learning models. For the latter, the framework is prepared to perform any necessary machine learning life-cycle tasks during the training and prediction steps, including coercing data into sliding windows and performing normalization and transformation of the input data. This module includes the implementation of the statistical models: ARIMA, Holt-Winter's exponential smoothing (HW), theta forecasting (TF), and ETS. MLM models include feed-forward neural network (NNET), random forest regression (RFrst), radial basis function network (RBF), support vector machine (SVM), multilayer perceptron network (MLP), and extreme learning machines network (ELM). Furthermore, the module provides deep learning Finally, the Evaluating module is responsible for assessing the model fitness and quality of predictions for the given time series based on a particular metric. These tasks are specialized for computing either prediction accuracy (error) measures or model fitting criteria. Among the available prediction accuracy measures, it includes Mean Square Error (MSE), symmetric MAPE (sMAPE), and maximal error. It also includes model fitness criteria such as Akaike Information Criterion (AIC), Bayesian Information Criterion (BIC), and log-likelihood <ref type="bibr" target="#b2">[3]</ref>.</p><p>TSPred can integrate the described modules in a workflow, encompassing the five steps described in Section 2. Ultimately, the package provides the means to perform the benchmarking of several prediction models. It is important to remark that although providing several pre-implemented options, TSPred design enables the user to define and apply customized time series prediction methods.</p><p>Moreover, the package provides several automatized features that can be useful for any time series prediction application. Among them, some of the main features are: (i) seamless recursive combination of two or more transformation methods; (ii) seamless integration of splitting-based transformation methods to the prediction process <ref type="bibr" target="#b17">[18]</ref>, which demands the combination of predictions for each component resulting from data decomposition (first package to include this approach); (iii) transformation/model parameter selection; (iv) multistep-ahead or one-step-ahead predictions; (v) rolling origin evaluation <ref type="bibr" target="#b8">[9]</ref> for both statistical and machine learning models; and (vi) machine-learning life-cycle tasks performed during training and prediction steps. It means that data normalization and transformation of sliding windows are seamlessly conducted during machine learning model training.</p><p>The framework is implemented in R using the S3 class system <ref type="bibr" target="#b19">[20]</ref>. TSPred is currently in version 5.1 and is available at The Comprehensive R Archive Network (CRAN). For more implementation details, please refer to the TSPred documentation <ref type="bibr" target="#b18">[19]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Empirical Results</head><p>The TSPred package includes datasets provided by time series prediction competitions to explore different time series prediction applications. The CATS Competition dataset <ref type="bibr" target="#b10">[11]</ref> is one of them. It presents an artificial time series with 5000 observations, among which 100 are missing. The missing observations are grouped into five non-consecutive gaps of 20. Each subset of known data that precedes a gap may be considered a different time series to be modeled. Although prediction techniques can be used in each gap, interpolation techniques might influence comparisons in the first four gaps. In contrast, the fifth gap can only be solved using a prediction process. Table <ref type="table" target="#tab_1">1</ref> presents the top-3 ranking of MSE prediction errors produced by the competitors of CATS for the fifth gap of missing observations. The same prediction experiment was performed using the TSPred package and its implemented resources.  <ref type="bibr" target="#b17">[18]</ref>. For that reason, we adopted a data preprocessing step based on applying splittingbased nonstationary time series transform EMD. Furthermore, we adopted the statistical ARIMA model as a baseline <ref type="bibr" target="#b16">[17]</ref>. The results from each approach are ranked based on MSE prediction errors.</p><p>TSPred is competitive when comparing them to the errors produced by state-of-the-art prediction models adopted by the top-ranked CATS competitors. Furthermore, the demand for adopting a suitable baseline model is noticeable given that the statistical ARIMA model produced smaller errors than the presented in Table <ref type="table" target="#tab_1">1</ref>. In particular, the deep learning model CNN could not outperform the baseline model. The same is valid for the SVM model by itself. However, introducing a preprocessing step designed to address the nonstationarity in the CATS time series resulted in the smaller prediction errors among the 22 presented approaches. The predictions from Table <ref type="table" target="#tab_0">2</ref> are depicted in Figure <ref type="figure" target="#fig_1">2</ref>. While the predictions from SVM and CNN seem closer to the baseline (ARIMA), their combination with EMD allowed a better representation of the actual observations. Nonetheless, CNN did not outperform SVM and yielded higher errors, especially for the first predictions.</p><p>TSPred enabled the application of all the different prediction approaches presented in Table <ref type="table" target="#tab_0">2</ref>. The results were obtained with few lines of code. The only parameter explicitly given was the length of sliding windows. All other parameters required by the models and the data transform were duly estimated by TSPred, resulting in more optimized prediction results. The package was designed to enable a seamless integration of prediction models, including stateof-the-art MLM and nonstationary time series preprocessing methods. The combination of preprocessing methods is also supported and may lead to even smaller prediction errors. For example, the BCT and EMD data transforms with SVM modeling led to a smaller MSE (120).</p><p>Besides the CATS time series, we present results obtained by TSPred over a time series dataset drawn from the M5 Accuracy competition held in 2020 <ref type="bibr" target="#b12">[13]</ref>. The dataset contains real-world daily observations of retail sales (1,969 days or approximately 5.4 years, from 2011 to 2016). It was asked to the competitors that they predict the next 28 preprint daily sales. The provided time series dataset can be grouped based on either location (store and state) or productrelated information (department and category). The competition allowed the participants to use cross-sectional levels of aggregation for prediction evaluation. Henceforth we discuss prediction results for the time series of unit sales aggregated for each of the ten stores available (aggregation level 3) <ref type="bibr" target="#b12">[13]</ref>.</p><p>The prediction of the selected M5 time series dataset was performed with TSPred, using both MLM models (NNET, ELM, SVM, CNN, LSTM) and statistical models (ETS, ARIMA).We adopted a rolling origin scheme and one-stepahead predictions for all adopted prediction models for a more robust evaluation. Moreover, different nonstationary time series transformation methods were used and combined to evaluate their impact on prediction. In that case, different transformation combination setups were adopted, including mapping-based methods (BCT, MAS, and DIF) and splitting-based methods (EMD and WT) <ref type="bibr" target="#b17">[18]</ref>. TSPred automatically selected the parameters for all transformation methods. Data normalization was performed with either MM or AN, and sliding window length was set to 30 (based on the central limit theorem). All other parameters were set to default values. Besides, for the sake of discussion, transformation methods were not applied to CNN and LSTM. According to the M5 competition guidelines, the best time series prediction setups were ranked based on the Weighted Root Mean Squared Scaled Error (WRMSSE) metric. Table <ref type="table" target="#tab_3">3</ref> presents some of the best evaluated results obtained by TSPred on the adopted M5 time series dataset. The simulated position of the results on the top-50 ranking of the M5 competition (regarding agg. level 3) is also given. Overall, better results were given by the use of MM normalization and mapping-based transform BCT. As observed from Table <ref type="table" target="#tab_3">3</ref>, the gap between prediction accuracy performances of statistical models and MLM models increased in comparison to the results from CATS (Table <ref type="table" target="#tab_0">2</ref>). It is expected since MLM models are more competitive in scenarios of long and high-frequency time series, such as the M5 time series dataset. Statistical models did not make it to the top-50 ranking of the M5 competition. Nonetheless, the use of deep learning MLM models (LSTM, CNN) without transformation methods could not climb to the top-50 ranking in M5, despite being computationally expensive. On the other hand, NNET, SVM, and ELM (faster and less complex) models combined with nonstationary time series transformation methods yielded better prediction results.</p><p>As presented in Table <ref type="table" target="#tab_3">3</ref>, even using default setups, TSPred prediction results were competitive, featuring in the top-50 ranked prediction results. In that case, hyperparameter optimization can probably result in even better performances and higher rank positions. TSPred furthers such optimization via the benchmarking of several different prediction setups. Moreover, TSPred can incorporate the best approaches by defining user-customized methods and benefiting from the broad range of transformation methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Illustrative Examples</head><p>This section gives examples of TSPred usage. The first example corresponds to a time series prediction using the NNET model combined with different data transformation methods. In Listing 1 the TSPred R-package and the CATS dataset are loaded into the R programming environment. The components for the time series process can be defined separately for enabling reuse. An example is presented in Listing 1. First, data processing objects are obtained for subsetting the time series and preprocessing it with the BCT and WT transformations. While parameters are set for WT, the parameters of BCT are automatically selected. Objects are also obtained for applying sliding windows technique (SW) (window length equal to 6 observations) and MM for normalizing the resulting windows of data during model training and prediction. Next, a modeling object is obtained for representing NNET modeling and prediction with five input units in a single hidden layer. proc_sw and proc_mm are given as parameters. At last an evaluating object respective to MSE is obtained. The previously defined objects are used to define the time series prediction process by MLM as shown in Listing 2.</p><p>The object tspred_mlm receives an instance of the tspred class. An instance of this class represents a particular time series prediction application. It encapsulates the process described in Section 2, including all its elements (attributes) and steps (class methods). This instance counts with (i) a subsetting process for dividing a time series into training and evaluation datasets, where the latter has 20 observations; (ii) an NNET modeling process with set parameters; and (iii) a list of evaluation metrics to be computed for prediction accuracy (MSE). The prediction process represented by tspred_mlm is applied for the fifth sequence of known values of the CATS series (CATS <ref type="bibr" target="#b4">[5]</ref>) and executed step-bystep as in Listing 2 (or by the function workflow). It returns a tspred class object similar to tspred_mlm updated with output elements and selected parameters tspred_mlm_res. Different tspred objects containing different prediction setups can be benchmarked and ranked based on prediction evaluation criteria using the function benchmark. The implementation of user-defined methods is done by extending the subclasses of TSPred. For example, the user specifies the functions for training and prediction of the model they wish to implement and their respective parameters to define a new MLM model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>This paper presented TSPred. It focuses on automatizing the entire time series prediction process. It is made available as an R Package <ref type="bibr" target="#b18">[19]</ref>. It seamlessly provides several automated features such as the combination of time series transformation methods in pipelines, prediction with decomposed time series, transformation and model parameter selection, multistep/one-step-ahead prediction, rolling origin evaluation, and the management of sliding windows. Several benchmark datasets from time series prediction competitions come bundled with TSPred, enabling users to practice data transformation and prediction methods, gaining confidence in the developed prediction models. Besides, the framework was designed to enable the user to implement their customized methods. Future updates will expand the range of implemented preprocessing methods, MLM, and evaluation metrics. The support for cross-learning and multivariate data is also envisaged.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: TSPred functionality modules and pre-implemented algorithms</figDesc><graphic coords="4,128.42,64.48,352.79,215.85" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Visualization of TSPred predictions for block 5</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>preprint Listing 1 :</head><label>1</label><figDesc>R example of the definition of components/steps of a time series prediction process in TSPred # i n s t a l l i n g and l o a d i n g TSPred package &gt; i n s t a l l . packages ( " TSPred " ) &gt; l i b r a r y ( " TSPred " ) # l o a d i n g CATS d a t a s e t &gt; data ( "CATS" ) # O b t a i n i n g o b j e c t s o f t h e p r o c e s s i n g c l a s s &gt; proc subset &lt;s u b s e t t i n g ( t e s t l e n = 20) &gt; proc b c t &lt;-BCT ( ) &gt; proc wt &lt;-WT( l e v e l = 1 , f i l t e r = " b l 1 4 " ) &gt; proc sw &lt;-SW( window l e n = 6 ) &gt; proc mm &lt;-MinMax ( ) # O b t a i n i n g o b j e c t s o f t h e modeling c l a s s &gt; modl nnet &lt;-NNET( s i z e = 5 , sw = proc sw , proc = l i s t (MM = proc mm) ) # O b t a i n i n g o b j e c t s o f t h e e v a l u a t i n g c l a s s &gt; eval mse &lt;-MSE( ) Listing 2: R example for an MLM prediction application using TSPred # D e f i n i n g a t i m e s e r i e s p r e d i c t i o n process &gt; t s p r e d mlm &lt;t s p r e d ( s u b s e t t i n g = proc subset , p r o c e s s i n g = l i s t (BCT = proc bct , WT = proc wt ) , modeling = modl nnet , e v a l u a t i n g = l i s t (MSE = eval mse ) )#Running t h e t i m e s e r i e s p r e d i c t i o n process and o b t a i n i n g r e s u l t s &gt; t s p r e d mlm r e s &lt;t s p r e d mlm %&gt;% subset ( data = CATS<ref type="bibr" target="#b4">[ 5 ]</ref> ) %&gt;% preprocess ( prep t e s t = TRUE) %&gt;% t r a i n ( ) %&gt;% p r e d i c t ( i n p u t t e s t data = TRUE) %&gt;% p os t p r oc e s s ( ) %&gt;% e v a l u a t e ( )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 2</head><label>2</label><figDesc>presents the adopted approaches combining data preprocessing and data modeling, either by the machine learning preprint</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>CATS top 3 methods for the fifth gap of observations<ref type="bibr" target="#b10">[11]</ref> </figDesc><table><row><cell>MSE Model</cell></row><row><cell>597 Recurrent Neural Networks</cell></row><row><cell>656 Kalman Smoother</cell></row><row><cell>672 BYY Harmony Learning Based Mixture of Experts Model</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>MSE for the fifth gap of observations using TSPred</figDesc><table><row><cell>MSE Nonstationary time series trans-</cell><cell>Model</cell></row><row><cell>form</cell><cell></cell></row><row><cell cols="2">130 Empirical Mode Decomposition Support Vector Machines</cell></row><row><cell>278 None</cell><cell>ARIMA (baseline)</cell></row><row><cell>325 None</cell><cell>Convolutional Neural Networks</cell></row><row><cell>358 None</cell><cell>Support Vector Machines</cell></row><row><cell cols="2">461 Empirical Mode Decomposition Convolutional Neural Networks</cell></row><row><cell cols="2">model SVM or CNN. It follows that the time series in CATS are mostly nonstationary, which knowingly poses a</cell></row><row><cell>challenge to prediction tasks</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>WRMSSE obtained by TSPred for the M5 time series (agg. level 3)</figDesc><table><row><cell cols="2">WRMSSE Nonstationary time</cell><cell>Model</cell><cell>Rank</cell></row><row><cell></cell><cell>series transform</cell><cell></cell><cell></cell></row><row><cell>0.455</cell><cell>Box-Cox</cell><cell>Feed-forward neural network (NNET)</cell><cell>35</cell></row><row><cell>0.472</cell><cell>Box-Cox</cell><cell>Support Vector Machines (SVM)</cell><cell>46</cell></row><row><cell>0.491</cell><cell>Box-Cox</cell><cell>Extreme learning machines network (ELM)</cell><cell>48</cell></row><row><cell>0.590</cell><cell>None</cell><cell>Convolutional Neural Networks (CNN)</cell><cell>-</cell></row><row><cell>0.596</cell><cell>None</cell><cell>Long short-term memory neural network (LSTM)</cell><cell>-</cell></row><row><cell>0.635</cell><cell>None</cell><cell>ARIMA (baseline)</cell><cell>-</cell></row><row><cell>0.909</cell><cell>None</cell><cell>Exponential smoothing model (ETS) (baseline)</cell><cell>-</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>The authors thank <rs type="funder">CNPq</rs>, <rs type="funder">CAPES</rs> (finance code <rs type="grantNumber">001</rs>), and <rs type="funder">FAPERJ</rs> for partially sponsoring this research.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_fqQ5DUk">
					<idno type="grant-number">001</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Mlr: Machine learning in R</title>
		<author>
			<persName><forename type="first">B</forename><surname>Bischl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Kotthoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schiffner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Richter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Studerus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Casalicchio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">M</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<idno type="ISSN">1533-7928</idno>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">170</biblScope>
			<biblScope unit="page" from="1" to="5" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Time series forecasting for nonlinear and non-stationary processes: A review and comparative study</title>
		<author>
			<persName><forename type="first">C</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sa-Ngasoongsong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Beyca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bukkapatnam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IIE Transactions (Institute of Industrial Engineers)</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1053" to="1071" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Measuring Forecasting Accuracy: The Case Of Judgmental Adjustments To Sku-Level Demand Forecasts</title>
		<author>
			<persName><forename type="first">A</forename><surname>Davydenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fildes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Forecasting</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="510" to="522" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">8 Forecast evaluation and combination</title>
		<author>
			<persName><forename type="first">F</forename><surname>Diebold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lopez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Handbook of Statistics</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="241" to="268" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Comparing predictive accuracy</title>
		<author>
			<persName><forename type="first">F</forename><surname>Diebold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mariano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Business and Economic Statistics</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="134" to="144" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Bench Plot and Mixed Effects Models: First Steps toward a Comprehensive Benchmark Analysis Toolbox</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J A</forename><surname>Eugster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Leisch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Compstat 2008</title>
		<editor>
			<persName><forename type="first">P</forename><surname>Brito</surname></persName>
		</editor>
		<meeting><address><addrLine>Heidelberg, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Physica Verlag</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="299" to="306" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Automatic time series forecasting: The forecast package for R</title>
		<author>
			<persName><forename type="first">R</forename><surname>Hyndman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Khandakar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Statistical Software</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1" to="22" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A state space framework for automatic forecasting using exponential smoothing methods</title>
		<author>
			<persName><forename type="first">R</forename><surname>Hyndman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Koehler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Snyder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Grose</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Forecasting</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="439" to="454" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Forecasting: principles and practice</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Hyndman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Athanasopoulos</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>OTexts</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Model Selection Management Systems: The Next Frontier of Advanced Analytics</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mccann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Naughton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Patel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGMOD Record</title>
		<idno type="ISSN">0163-5808</idno>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="17" to="22" />
			<date type="published" when="2016-05">May 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Time series prediction competition: The CATS benchmark</title>
		<author>
			<persName><forename type="first">A</forename><surname>Lendasse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Oja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Simula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Verleysen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="issue">13-15</biblScope>
			<biblScope unit="page" from="2325" to="2329" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Benchmarking classification models for software defect prediction: A proposed framework and novel findings</title>
		<author>
			<persName><forename type="first">S</forename><surname>Lessmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Baesens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Mues</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Pietsch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Software Engineering</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="485" to="496" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">The m5 accuracy competition: Results, findings and conclusions</title>
		<author>
			<persName><forename type="first">S</forename><surname>Makridakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Spiliotis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Assimakopoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Forecasting</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">V</forename><surname>Moreno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J R</forename><surname>Rivas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D P</forename><surname>Godoy</surname></persName>
		</author>
		<ptr target="https://cran.r-project.org/package=predtoolsTS" />
		<title level="m">predtoolsTS: Time Series Prediction Tools</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Adaptive Normalization: A novel data normalization approach for non-stationary time series</title>
		<author>
			<persName><forename type="first">E</forename><surname>Ogasawara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">De</forename><surname>Oliveira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zimbrão</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Pappa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mattoso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Joint Conference on Neural Networks</title>
		<meeting>the International Joint Conference on Neural Networks</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Ramey</surname></persName>
		</author>
		<ptr target="https://cran.r-project.org/web/packages/sortinghat/index.html" />
		<title level="m">sortinghat: sortinghat</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A framework for benchmarking machine learning methods using linear models for univariate time series prediction</title>
		<author>
			<persName><forename type="first">R</forename><surname>Salles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Assis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Guedes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Bezerra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Porto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Ogasawara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Joint Conference on Neural Networks</title>
		<meeting>the International Joint Conference on Neural Networks</meeting>
		<imprint>
			<date type="published" when="2017-05">2017-May. 2017</date>
			<biblScope unit="page" from="2338" to="2345" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Nonstationary time series transformation methods: An experimental review</title>
		<author>
			<persName><forename type="first">R</forename><surname>Salles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Belloze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Porto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Ogasawara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowledge-Based Systems</title>
		<imprint>
			<biblScope unit="volume">164</biblScope>
			<biblScope unit="page" from="274" to="291" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">TSPred: Functions for Benchmarking Time Series Prediction</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">P</forename><surname>Salles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Ogasawara</surname></persName>
		</author>
		<ptr target="https://cran.r-project.org/package=TSPred" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Advanced R</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wickham</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019-05">May 2019</date>
			<publisher>CRC Press</publisher>
		</imprint>
	</monogr>
	<note>second edition</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
