<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="fr">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">&quot;Don&apos;t discuss&quot;: Investigating Semantic and Argumentative Features for Supervised Propagandist Message Detection and Classification</title>
				<funder>
					<orgName type="full">3IA Côte d&apos;Azur Investments</orgName>
				</funder>
				<funder ref="#_akRmcdG">
					<orgName type="full">French government</orgName>
				</funder>
				<funder ref="#_UUaGRAq #_Vsu6tYP">
					<orgName type="full">unknown</orgName>
				</funder>
				<funder ref="#_VAPKsAc">
					<orgName type="full">Agence Nationale de la Recherche</orgName>
					<orgName type="abbreviated">ANR</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Vorakit</forename><surname>Vorakitphan</surname></persName>
							<email>vorakit.vorakitphan@inria.fr</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Université Côte d&apos;Azur</orgName>
								<orgName type="institution" key="instit2">Inria</orgName>
								<orgName type="institution" key="instit3">CNRS</orgName>
								<address>
									<region>I3S</region>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Elena</forename><surname>Cabrio</surname></persName>
							<email>elena.cabrio@univ-cotedazur.fr</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Université Côte d&apos;Azur</orgName>
								<orgName type="institution" key="instit2">Inria</orgName>
								<orgName type="institution" key="instit3">CNRS</orgName>
								<address>
									<region>I3S</region>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Serena</forename><surname>Villata</surname></persName>
							<email>villata@i3s.unice.fr</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Université Côte d&apos;Azur</orgName>
								<orgName type="institution" key="instit2">Inria</orgName>
								<orgName type="institution" key="instit3">CNRS</orgName>
								<address>
									<region>I3S</region>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">&quot;Don&apos;t discuss&quot;: Investigating Semantic and Argumentative Features for Supervised Propagandist Message Detection and Classification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">D844D21D1836240DD38D4B190F3EED66</idno>
					<note type="submission">Submitted on 5 Aug 2021</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-04-12T14:52+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>HAL is a multi-disciplinary open access archive for the deposit and dissemination of scientific research documents, whether they are published or not. The documents may come from teaching and research institutions in France or abroad, or from public or private research centers. L'archive ouverte pluridisciplinaire HAL, est destinée au dépôt et à la diffusion de documents scientifiques de niveau recherche, publiés ou non, émanant des établissements d'enseignement et de recherche français ou étrangers, des laboratoires publics ou privés.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="fr">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Propaganda represents an effective, even though often misleading, communication strategy to promote a cause or a viewpoint, for instance in the political context <ref type="bibr" target="#b19">(Lasswell, 1938;</ref><ref type="bibr" target="#b18">Koppang, 2009;</ref><ref type="bibr" target="#b11">Dillard and Pfau, 2009;</ref><ref type="bibr" target="#b21">Longpre et al., 2019)</ref>. Different communication means can be used to disseminate propaganda, i.e., textual documents, images, videos and oral speeches. The ability to effectively identify and manifestly label such kind of misleading and potentially harmful content is of primarily importance to restrain the spread of such information to avoid detrimental consequences for the society.</p><p>In this paper, we tackle this challenging issue <ref type="bibr">(Da San Martino et al., 2020b)</ref> by proposing a textual propaganda detection model. More precisely, we address the following research questions: (i) how to automatically identify propaganda in textual documents and further classify them into finegrained categories?, and (ii) what are the linguistic distinctive features of propaganda text snippets? The contribution of this paper consists not only in proposing a new effective neural architecture to automatically identify and classify propaganda in text, but we also present a detailed linguistic analysis of the features characterising propaganda messages.</p><p>Our work focuses on the propaganda detection and classification task, casting it both as a binary and as a multi-class classification task, and we address it both at sentence-level and at fragment-level. We investigate different architectures of recent language models (i.e., BERT, RoBERTa), combining them with a rich set of linguistic features ranging from sentiment and emotion to argumentation features, to rhetorical stylistic ones. The extensive experiments we conducted on two standard benchmarks <ref type="bibr">(i.e.,</ref> show that the proposed architectures achieve satisfying results, outperforming state-ofthe-art systems on most of the propaganda detection and classification subtasks. An error analysis discusses the main sources of misclassification. Furthermore, we analysed how the most relevant features for propaganda detection impact the finegrained classification of the different techniques employed in propagandist text, revealing the importance of semantic and argumentation features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>In the last years, there has been an increasing interest in investigating methods for textual propaganda detection and classification. Among them, <ref type="bibr" target="#b3">(Barrón-Cedeño et al., 2019)</ref> present a system to organize news events according to the level of propagandist content in the articles, and introduces a new corpus (QProp) annotated with the propaganda vs. trustworthy classes, providing information about the source of the news articles. <ref type="bibr" target="#b8">(Da San Martino et al., 2019)</ref> present the benchmark of the shared task NLP4IF'19<ref type="foot" target="#foot_0">1</ref> on fine-grained propaganda detection. As a follow up, in 2020 SemEval proposed a shared task (T11) <ref type="bibr">(Da San Martino et al., 2020a)</ref> reducing the number of propaganda categories with respect to NLP4IF'19, and proposing a more restrictive evaluation scheme. To evaluate the proposed approach, we rely on these two standard benchmarks, i.e., the NLP4IF'19 and SemEval'20 datasets.</p><p>The most recent approaches for propaganda detection are based on language models that mostly involve transformer-based architectures. The approach that performed best on the NLP4IF'19 sentence-level classification task relies on the BERT architecture with hyperparameters tuning without activation function <ref type="bibr" target="#b22">(Mapes et al., 2019)</ref>. <ref type="bibr" target="#b31">(Yoosuf and Yang, 2019)</ref> focused first on the pre-processing steps to provide more information regarding the language model along with existing propaganda techniques, then they employ the BERT architecture casting the task as a sequence labeling problem. The systems that took part in the SemEval 2020 Challenge -Task 11 represent the most recent approaches to identify propaganda techniques based on given propagandist spans. The most interesting and successful approach <ref type="bibr" target="#b17">(Jurkiewicz et al., 2020)</ref> proposes first to extend the training data from a free text corpus as a silver dataset, and second, an ensemble model that exploits both the gold and silver datasets during the training steps to achieve the highest scores. Notice that most of the most performing recent models heavily rely on transformer-based architectures.</p><p>In this paper, we also rely on language model architectures for the detection and classification of propaganda messages, empowering them with a rich set of features we identified as pivotal in propagandist text from the computational social science literature. In particular, <ref type="bibr" target="#b24">(Morris, 2012)</ref> discusses how emotional markers and affect at wordor phrase-level are employed in propaganda text, whilst <ref type="bibr" target="#b0">(Ahmad et al., 2019)</ref> show that the most effective technique to extract sentiment for the propaganda detection task is to rely on lexiconbased tailored dictionaries. Recent studies <ref type="bibr" target="#b20">(Li et al., 2017)</ref> show how to detect degrees of strength from calmness to exaggeration in press releases. Finally, <ref type="bibr" target="#b27">(Troiano et al., 2018)</ref> focus on feature extraction of text exaggeration and show that main factors include imageability, unexpectedness, and the polarity of a sentence.</p><p>3 Propaganda detection as a classification task <ref type="bibr" target="#b8">(Da San Martino et al., 2019)</ref> define the Fine-Grained Propaganda Detection task as two subtasks, with different granularities: i) Sentence-Level Classification task (SLC), which asks to predict whether a sentence contains at least one propaganda technique, and ii) Fragment-Level Classification task (FLC), which asks to identify both the spans and the type of propaganda technique. In the following example, "In a glaring sign of just how stupid and petty things have become in Washington these days, Manchin was invited on Fox News Tuesday morning to discuss how he was one of the only Democrats in the chamber for the State of the Union speech not looking as though Trump killed his grandma." the span "stupid and petty" carries some propagandist bias, and is labeled as "Loaded Language" , "not looking as though Trump killed his grandma" is considered as "Exaggeration and Minimisation", and "killed his grandma" is "Loaded Language". According to the SLC task, the whole sentence should be classified as a propaganda message given that it contains at least one token (e.g., "stupid and petty") considered as such.</p><p>As previously introduced, current systems address these tasks relying on word embedding models (e.g., BERT-embedding) and standard features (e.g., PoS, name-entity, n-grams), as representations to feed various RNN architectures <ref type="bibr" target="#b23">(Morio et al., 2020;</ref><ref type="bibr" target="#b5">Chernyavskiy et al., 2020)</ref>. Recently, the language model BERT <ref type="bibr" target="#b10">(Devlin et al., 2019)</ref> has been widely utilized to optimize the performances of classification tasks, but there is still room for improvement, in particular when applied to propaganda detection <ref type="bibr">(Da San Martino et al., 2020a</ref><ref type="bibr">, 2019)</ref>. In this work, we experiment with multiple architectures and language models to classify propagandist messages on both sentence and fragmentlevel. Prior to that, we conduct a detailed investigation of linguistic and argumentation features to capture propaganda strategies.</p><p>Propaganda strategies generally involve specific targets to be stimulated by the message. To better study such techniques from a computational point of view, we investigate a set of features that we assume to play a role in propaganda.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Persuasion</head><p>Speech style. To analyze the writing style of the messages, we apply the dictionary-based mapping tool "General Inquirer (v. 1.02 )" <ref type="bibr" target="#b15">(Gilman, 1968)</ref>. It relies on a list of lexicons from 26 domains (e.g., politician speeches, consumer protests) annotated according to 182 rating categories and dimensions (e.g., valence categories and words indicating overstatement and understatement)<ref type="foot" target="#foot_1">2</ref> . We apply such tool on our data and we sum the ratings of each token to obtain a global score for a sentence.</p><p>Lexical complexity. Given that pre-trained language models have shown to capture lexical and semantic complexities of words, we rely on BERT (base-uncased) <ref type="bibr" target="#b10">(Devlin et al., 2019)</ref> to extract lexical complexity features. We extract a vector of 768 dimensions per each token, then we average w.r.t. all tokens in a sentence, to obtain one vector of 768 dimensions to represent a sentence.</p><p>Concreteness. Propaganda messages tend to employ words with concrete meaning, that has more impact in conveying the intention of the message than using abstract words <ref type="bibr" target="#b12">(Eliasberg, 1957)</ref> We rely on the concreteness lexicon <ref type="bibr" target="#b4">(Brysbaert et al., 2013)</ref> and we sum the standardized score of each token in a sentence to obtain the global score.</p><p>Subjectivity. We rely on the subjectivity lexicon from <ref type="bibr" target="#b30">(Wilson et al., 2005)</ref>. We sum up the scores over all tokens in a sentence found in the lexicon as our extracted feature. Each word labeled as "weaksubj" is set to 0.5, and "strongsubj" to 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Sentiment</head><p>Sentiment labels. We use SentiWordNet 3.0 <ref type="bibr" target="#b2">(Baccianella et al., 2010)</ref> to obtain word-level sentiment labels (positive, negative, or neutral). We sum the sentiment scores of each word in a sentence, producing a vector with 3 dimensions (i..e, pos, neg, neu) for each sentence.</p><p>Emotion labels. We extract 8 emotions (i.e., afraid, amused, angry, annoyed, don't care, happy, inspired, sad) from DepecheMood++ lexicon <ref type="bibr" target="#b1">(Araque et al., 2019)</ref>. For each word that evokes emotions in a sentence, we produce our features by summing up each set of emotions evoked by each token, then find the average by emotions. Hence, we produce 8 emotion scores for a sentence.</p><p>VAD labels. In the three-dimensional model of affect, valence ranges from unhappiness to happiness and expresses the pleasant or unpleasant feeling about something, arousal expresses the level of affective activation, ranging from sleep to excitement, and dominance reflects the level of control of the emotional state, from submissive to dominant. We use Warriner lexicon <ref type="bibr" target="#b29">(Warriner et al., 2013)</ref> to match each word in a sentence to its VAD standardized word scores and sum up as our features.</p><p>Connotation. Propaganda can convey sentiment beyond its original meaning. Connotation lexicon <ref type="bibr" target="#b13">(Feng et al., 2013)</ref> provides positive, negative and neutral labels of each word. We count the frequencies of the three labels evoked in each sentence.</p><p>Politeness. Politeness evokes sentiment in readers. We use a lexicon of positive and negative words from <ref type="bibr" target="#b9">(Danescu-Niculescu-Mizil et al., 2013)</ref>, then we count the frequencies of both positive and negative words found in each sentence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Message Simplicity</head><p>To keep the message simple and picturable is one of main purposes of propaganda. We list the features we considered to extract the simplicity of message.</p><p>Exaggeration. We use imageability lexicon <ref type="bibr" target="#b28">(Tsvetkov et al., 2014)</ref> based on picturable vocabulary which mentally leads to an exaggerating state of mind. We consider the scores of abstraction and concreteness at each word token. We then sum up the scores for all the labels found in a sentence.</p><p>Length. "The less words used, the better to understand" can be a concept to easily interpret the propagandist message. We apply two strategies: i) we count the average char-length, actual char-length, word length, punctuation frequency, capital-case frequency per sentence <ref type="bibr" target="#b14">(Ferreira Cruz et al., 2019)</ref>; ii) we apply length encoding at character-level, plus one additional dimension for non-alphabetical char count.</p><p>Pronouns. Loaded language, name calling and labelling are the most used techniques in propaganda text <ref type="bibr" target="#b8">(Da San Martino et al., 2019)</ref>, and they all make use of pronouns. We create a lexicon of 123 pronouns in English 3 and perform one-hot encoding of common used pronouns in English.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Argumentation</head><p>We assume that argumentation plays an important role in propaganda. To extract argumentative features representing our data, we train a supervised classifier for the task of argumentative sentence classification on the persuasive essays dataset <ref type="bibr" target="#b26">(Stab and Gurevych, 2014)</ref>. First, we cast it as a binary classification task, merging premises, claims and major claims into the argumentative label, as opposed to the non-argumentative label. Then, for the argumentation component task, we rearrange the data to binary labels where the major claims and claims labels are merged, and they are opposed to premises. To address these tasks, we build and fine-tune a BERT classifier. We use a learning rate of 1e-5 with 80/20 split of the dataset. We run our classifier 3 times at different random states. The results for the argumentative sentence classification are (macro-average) F1 0.84, precision 0.86, recall 0.82, while for the component classification they are F1 0.77, precision 0.80, recall 0.75.</p><p>To extract argumentative features from the annotations provided by our classifiers, we use BERTbased features. After fine-tuning, we freeze the hidden states of these fine-tuned BERT models. To extract the argumentative and components features from each classifier, we take the [CLS] token of each sentence from the fine-tuned BERT model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Ablation Study</head><p>To investigate the impact of the proposed features (Section 4) for propaganda detection, we perform ablation tests by testing a supervised classifier relying on BERT + logistic regression. To the purpose, we use the NLP4IF'19 training and test sets <ref type="bibr" target="#b8">(Da San Martino et al., 2019)</ref>.</p><p>Table <ref type="table" target="#tab_0">1</ref> reports on the performances obtained while integrating groups of features to the proposed model. A logistic regression model is used as a baseline. Best results are obtained when adding all the proposed features, but the argumentation ones. Argumentation features alone perform almost identical as semantic features, therefore -unexpectedly -no added value can be demonstrated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Sentence-level classification</head><p>In the following, we describe the experiments we carried out to address the propaganda detection task at sentence level, investigating different architectures and leveraging both recent language models and the features that proved to play a role in propaganda messages. For the evaluation, we used the two available datasets for propaganda detection: </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Prediction Models</head><p>In the following, we first describe the baseline and the SOTA models we tested in our experiments, and then we present the three architectures we propose (underlined) integrating the propagandist features previously investigated (Section 4). BERT. Our baseline model relies on a pre-trained bidirectional transformer language model to encode context specific sentence tokens <ref type="bibr" target="#b10">(Devlin et al., 2019)</ref> (no fine tuning, default hyperparameters).</p><p>Fine-tuned BERT. We fine-tune the BERT model with a learning rate of 5e-5, and AdamW optimizer.</p><p>We set the gradients to zero at every training batch.</p><p>Then we use softmax activation to gate the output with the threshold of 0.5. Fine-tuned T5.</p><p>To fine-tune the text-totext transformer <ref type="bibr" target="#b25">(Raffel et al., 2020)</ref>, we use T5ForConditionalGeneration approach (equally to question-answering task) where the input is a sentence (as a question), and the output is an answer (as a label). We use a learning rate of 3e-4, with max sequence length of 512. Linear-Neuron Attention BERT. We replicate the winning approach of the NLP4IF'19 sharedtask <ref type="bibr" target="#b22">(Mapes et al., 2019)</ref>. It relies on BERT architecture with some modifications of hyperparameters (sentence length of 50 tokens, a learning rate of 1e-5, along with 12 attention heads and 12 transformer blocks). It uses the linear neuron without an activation function, and a threshold of 0.3 for the final prediction.</p><p>Multi-granularity BERT. This model <ref type="bibr" target="#b8">(Da San Martino et al., 2019)</ref> relies on BERT transformer with multi-granularity network on top that has multi-classifiers for different granularity levels of text (e.g., document, paragraph, sentence, word, subword, and character-level). We replicate this model with BertAdam optimizer and ReLU activation function.</p><p>Multi-granularity + Featured BERT. We integrate the proposed features (Section 4) into <ref type="bibr" target="#b8">(Da San Martino et al., 2019)</ref>, taking only the last layer of sentence-level granularity. We feed the proposed features to a BERT classifier to obtain logits which then aggregate with the last layer of sentence-level granularity to produce predictions.</p><p>BERT + Featured BiLSTM. We build a pretrained BERT transformer architecture, and Bidirectional Long Short-Term Memory (BiLSTM) architecture on top of the BERT model to handle the transformer architecture with our propaganda features. Firstly, the BERT model is used with learning rate of 0.001, with AdamW optimizer. We use the output of BERT that represents the [CLS] token of each sentence to combine with propaganda features as our input to the second model, the BiL-STM. For the BiLSTM model, after we feed our inputs of both [CLS] tokens combined with propaganda features, we train our BiLSTM model with hidden size of 256. Our BiLSTM hidden states con-sist of the last hidden states, and the last cell state for the BiLSTM layers. We then apply relu gate function, with a linear dense, then use a dropout function of 0.1. At the last layer, we use another linear dense layer to output final logits, then we apply a sigmoid activation function as final outputs. BERT + Featured Logistic Regression. We use pre-trained BERT transformer architecture to output [CLS] token, then use this output to stack with another prediction model, i.e., logistic regression. We build a linear classifier and feed it with propaganda features as a dense layer. We then combine these logits with [CLS] tokens as the input to logistic regression on top of BERT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Results and error analysis</head><p>Table <ref type="table" target="#tab_1">2</ref> reports on the results obtained for the SLC task (propaganda vs no propaganda). We run each experiment 5 times and report the macro-average of all metrics. Our proposed models achieve the highest F1-score of 0.72 using BERT + Featured Logistic Regression model (persuasion, sentiment, and message simplicity features), and the highest precision-score 0.80 using BERT + Featured BiL-STM model on NLP4IF'19 dataset, outperforming the state-of-the-art models. For SemEval'20-T11, we do not have the scores from the challenge (the binary task was not proposed), but we compare the obtained results with the replicated architectures of SOTA models. Our proposed architecture obtained the best F1-score using BERT+Featured Logistic Regression. Using semantic features alone perform slightly better than combining them with argumentation features.</p><p>Table <ref type="table" target="#tab_2">3</ref> reports on some misclassified examples of our best model on NLP4IF'19 dataset. Some short sentences containing strong intention keywords (e.g., "hate", "slave") have been missclas-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>False Positive</head><p>False Negative People who hate freedom will get unfettered access to the minds of 2 billion people.</p><p>The American people have a right to know, and those that engaged in this type of behavior do not have a right to hide. You are a slave to white America.</p><p>Hitler was a very great man. sified as false positives. As for false negatives, the underlined fragments are labeled propaganda in gold standard, but have not been recognized as such by the classifier (mainly informative statements).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Fragment-level Classification</head><p>In this section, we address the task of fragmentlevel classification, meaning that both the spans and the type of propaganda technique should be identified in the sentences. Again, to test the proposed methods, we use both NLP4IF'19 and Se-mEval'20 T11 datasets. However, in the two challenges, the FLC task was evaluated according to different strategies, explained in the following.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Task 1: FLC on NLP4IF'19 dataset</head><p>In the NLP4IF'19 dataset, 18 propaganda techniques are annotated. Prediction is expected to be at token-level. Multiple tokens can belong to the same span, and annotated with one propaganda type. Tokens that do not carry any propaganda bias are annotated as "no propaganda". To perform tokenization we run the tokenizer provided with the pretrained model of each transformer 4 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.1">Prediction Models</head><p>Fine-tuned BERT (baseline). Pretrained bertbase-uncased model and BERT architecture <ref type="bibr" target="#b10">(Devlin et al., 2019)</ref> with default hyperparameters. Our implementation is based on huggingface transformers. Settings: learning rate of 5e-5, padded length of 128, and batch size of 16. We use CrossEntropy-Loss as a loss function, and softmax activation function to gate output neurons.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fine-tuned RoBERTa (baseline).</head><p>We use roberta-base model with the same hyperparameters of loss and activation functions as the fine-tuned BERT model mentioned above. State-of-the-art Model. The winning team applied BERT architecture for token classification <ref type="bibr" target="#b31">(Yoosuf and Yang, 2019)</ref> on 20 labels (i.e., 18 propaganda classes, plus "background" as non propaganda, and "auxiliary" for fractions of previous tokens). They use a BERT language model, then 4 huggingface.co/transformers/ apply softmax function, followed by a linear multilabel classification layer to output their predictions. Transformer + CRF. We use a pre-trained model base-uncased with a learning rate of 3e-5 for BERT transformer, and a pre-trained model roberta-base with a learning rate of 2e-5 for RoBERTa transformers (hyperparameters: dropout of 0.1 with the max length of 128, batch size of 16 with AdamW optimizer and CrossEntropy loss function). We use CRF layer as the final layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.2">Results and error analysis</head><p>Table <ref type="table" target="#tab_3">4</ref> reports on the obtained performances. Evaluation is reported as the average of micro-F1 scores of 5 run-times (we use the evaluation scripts provided by <ref type="bibr" target="#b8">(Da San Martino et al., 2019)</ref>).</p><p>The proposed architecture based on transformers with CRF output layer at different learning gradients (epochs) outperforms SOTA model on several propaganda techniques at different learning gradient ranging from 5 to 15 epochs. We also tested other architectures such as Transformer+CRF with less learning gradients (3 epochs), Transformer architecture with semantic and/or argumentation features + CRF layer by adding extracted features from sentence-level (Section 4) to each token of its sentence to a linear layer before a loss function, with no major improvements.</p><p>In Table <ref type="table" target="#tab_3">4</ref>, we compare the performances of the proposed models w.r.t. the SOTA <ref type="bibr" target="#b31">(Yoosuf and Yang, 2019)</ref>, on the most frequent classes. Table <ref type="table" target="#tab_4">5</ref> reports examples of misclassification related to that technique. We observe that our proposed model does not capture well the articles (i.e., it, as, an, the), but rather focuses on capturing intentional word tokens (i.e., white, unbelievably, rude, wonderful, treasonous). As for future work to improve results on this specific category, we will investigate the work of <ref type="bibr" target="#b16">(Habernal et al., 2018)</ref> according to which a dedicated strategy is needed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Task 2: FLC on SemEval'20 T11 dataset</head><p>In SemEval'20 T11 dataset, 14 propaganda techniques are annotated. We focus here on the task called Technique-Classification task (TC). We cast it as a sentence-span classification problem, where (b) This is a man who follows a demonic religion, Islam, and supports textcolorredcop killers. (c) It was not widely recognized as an anti-Jewish organization during its early years (its early literature, though, focused on "the white man" as "the white devil"). we combine logits of tokenized elements from the sentence and the span, to learn the prediction. Moreover, we add the semantic and argumentation features to enhance the performance.</p><p>As pre-processing, both the tokenized sentence and the span are used to feed the transformer (Huggingface tokenizers) as follows: i) we input a sentence to the tokenizer where max length is set to 128 with padding; ii) we input the span provided by the propaganda span-template published by the workshop, and we set max length value of 20 with padding. If a sentence does not contain propaganda spans, it is labeled as a "none-propaganda".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.1">Prediction Models</head><p>Baseline. For all the tested architectures (BERT and RoBERTa), we use the same type of transformer model to produce logits (L) regarding the sentence-level and span-level individually. For BERT model, we use pre-trained model bert-baseuncased, learning rate of 5e-5, and α of 0.1. For RoBERTa, we take roberta-base pre-trained model with learning rate of 2e-5 with α of 0.5. All transformer models apply Adam optimizer, dropout 0.1, and CrossEntropy as a loss function per sentence (loss sentence ) and span (loss span ).</p><p>We arrange these alignment of L to calculate the average loss as joint loss (loss joint loss ) from each loss element. Here we introduce a loss joint loss function before back-propagation:</p><p>loss joint loss = α × (loss sentence +lossspan) N loss where N loss stands for a number of loss elements that are taken into the model. State-of-the-art Model. The winning team <ref type="bibr" target="#b17">(Jurkiewicz et al., 2020)</ref> applies RoBERTa (robertalarge) with pre-trained model. The training set is increased with silver annotation based on gold annotation, and then another RoBERTa model is stacked on top to output the predictions. Proposed Architecture. We propose another set of elements to feed the transformer by introducing the semantic and argumentation features into BiLSTM layer to produce L of proposed features, then we apply CrossEntropy as a loss function of our BiLSTM as loss proposed features then perform an addition with other loss in the loss joint loss function as follows: loss joint loss = α × (loss sentence +lossspan+loss proposed features ) N loss</p><p>Hyperparameters: 256 hidden size, 1 hidden layer, drop out of 0.1 with ReLU function at the last layer before the joint loss function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.2">Results and error analysis</head><p>As mentioned before, the gold labels of the test set of SemEval'20 T11 are not available, but it is possible to submit a system run to the challenge website and to obtain the evaluation score. Technique: Repetition False Negative All Features (1) When she arrived at Jean's door, Guyger entered a unique door key with an electronic chip into the keyhole, the affidavit says.</p><p>Bandwagon,Reductio ad hitlerum (2) She told the 911 operator as well as responding officers that she thought she was at her apartment when she shot Jean, according to the affidavit.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Doubt</head><p>Table <ref type="table">7</ref>: Misclassified Repetition spans (in red).</p><p>of span-templates of the test set (partial overlapping spans or missing spans are not accepted). Table <ref type="table" target="#tab_5">6</ref> reports on the obtained results (through such evaluation system) on 5 runs as micro-F1. Scores in bold are the ones for which significant improvement can be observed w.r.t. SOTA model. RoBERTa with argumentation features can outperform results on "Thought-terminating Cliches". Moreover, by using all semantic and argumentation features together, we can obtain some improvements over "Bandwagon,Reductio ad hitlerum" and "Casual-Oversimplification". Table <ref type="table">7</ref> shows some examples of missclassified instances. In general, we noticed that using different training epochs help detecting different propaganda techniques. In particular, it is observed that some techniques tend to be learnt best at low training epochs (i.e., "Bandwagon,Reductio ad hitlerum", "Thought-terminating Cliches"), some at high training epochs (i.e., "Casual-Oversimplification").</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Concluding remarks</head><p>In this paper, we proposed a new neural architecture combined with state-of-the-art language models and a rich set of linguistic features for the detection of propaganda messages in text, and their further classification along with standard propaganda tech-niques. Despite the boost in accuracy we achieved on two standard benchmarks for propaganda detection and classification (∼ 10% of F1 scores on sentence-level classification and on specific propaganda techniques on fragment-level classification), this task remains challenging, in particular regarding the fine-grained classification of the different propaganda classes. The state-of-the-art results on this subtask require further improvement to actually embed these solutions in real-world systems.</p><p>Future work goes in this direction, with the aim to improve the performance both of the disinformation detection task and of the classification of propaganda techniques. Moreover, we are currently investigating the propaganda classes we discussed in this paper in the context of political debates, with the aim of building a fallacy detection systems that relies on the identification of propagandist messages in political speeches.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>i) the NLP4IF'19 data set (Da San Martino et al., 2019) (293 articles for training and 101 for testing); and ii) the data from SemEval'20 T11 (Da San Martino et al., 2020a) (371 articles for training and 75 in the development set).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Ablation test on binary classification setting.</figDesc><table><row><cell>Persuasion</cell><cell>Sentiment</cell><cell>Message simplicity</cell><cell>Argumentation</cell><cell cols="3">Logistic Regression F1 Precision Recall F1 Precision Recall BERT + Featured LR</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.68</cell><cell>0.69</cell><cell>0.67</cell><cell>0.70</cell><cell>0.71</cell><cell>0.70</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.62</cell><cell>0.69</cell><cell>0.57</cell><cell>0.71</cell><cell>0.72</cell><cell>0.69</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.63</cell><cell>0.66</cell><cell>0.62</cell><cell>0.70</cell><cell>0.72</cell><cell>0.68</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.67</cell><cell>0.68</cell><cell>0.67</cell><cell>0.71</cell><cell>0.72</cell><cell>0.70</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.68</cell><cell>0.70</cell><cell>0.67</cell><cell>0.71</cell><cell>0.71</cell><cell>0.69</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.69</cell><cell>0.71</cell><cell>0.68</cell><cell>0.70</cell><cell>0.71</cell><cell>0.69</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.69</cell><cell>0.70</cell><cell>0.68</cell><cell>0.70</cell><cell>0.71</cell><cell>0.70</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.66</cell><cell>0.68</cell><cell>0.65</cell><cell>0.71</cell><cell>0.73</cell><cell>0.70</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.70</cell><cell>0.71</cell><cell>0.69</cell><cell>0.71</cell><cell>0.72</cell><cell>0.70</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.66</cell><cell>0.68</cell><cell>0.65</cell><cell>0.70</cell><cell>0.72</cell><cell>0.69</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.69</cell><cell>0.70</cell><cell>0.68</cell><cell>0.71</cell><cell>0.72</cell><cell>0.70</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.68</cell><cell>0.69</cell><cell>0.68</cell><cell>0.70</cell><cell>0.71</cell><cell>0.69</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.70</cell><cell>0.72</cell><cell>0.69</cell><cell>0.70</cell><cell>0.71</cell><cell>0.69</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.71</cell><cell>0.72</cell><cell>0.69</cell><cell>0.72</cell><cell>0.74</cell><cell>0.70</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.70</cell><cell>0.71</cell><cell>0.69</cell><cell>0.71</cell><cell>0.72</cell><cell>0.69</cell></row></table><note><p>type.php, https://www.thefreedictionary.com/List-ofpronouns.htm</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Results on the Sentence-level classification (SLC) task (binary task).</figDesc><table><row><cell>Model</cell><cell cols="6">NLP4IF'19 Test Set F1 Precision Recall F1 Precision Recall SemEval'20-T11 Dev. Set</cell></row><row><cell>BERT Baseline</cell><cell>0.52</cell><cell>0.53</cell><cell>0.50</cell><cell>0.48</cell><cell>0.48</cell><cell>0.48</cell></row><row><cell>SOTA</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Fine-tuned BERT</cell><cell>0.58</cell><cell>0.63</cell><cell>0.53</cell><cell>0.61</cell><cell>0.63</cell><cell>0.60</cell></row><row><cell>Fine-tuned T5</cell><cell>0.64</cell><cell>0.64</cell><cell>0.65</cell><cell>0.66</cell><cell>0.65</cell><cell>0.66</cell></row><row><cell>Linear-Neuron Attention BERT</cell><cell>0.63</cell><cell>0.60</cell><cell>0.67</cell><cell>0.66</cell><cell>0.69</cell><cell>0.63</cell></row><row><cell>Multi-granularity BERT</cell><cell>0.61</cell><cell>0.60</cell><cell>0.62</cell><cell>0.65</cell><cell>0.68</cell><cell>0.63</cell></row><row><cell>Proposed Architecture w/ Semantic Features</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Multi-granularity + Featured BERT</cell><cell>0.63</cell><cell>0.65</cell><cell>0.61</cell><cell>0.67</cell><cell>0.71</cell><cell>0.64</cell></row><row><cell>BERT + Featured BiLSTM</cell><cell>0.65</cell><cell>0.80</cell><cell>0.55</cell><cell>0.65</cell><cell>0.75</cell><cell>0.58</cell></row><row><cell>BERT + Featured Logistic Regression</cell><cell>0.72</cell><cell>0.74</cell><cell>0.70</cell><cell>0.68</cell><cell>0.71</cell><cell>0.66</cell></row><row><cell>Proposed Architecture w/ Semantic Features + Argumentation Features</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>BERT + Featured Logistic Regression</cell><cell>0.71</cell><cell>0.72</cell><cell>0.69</cell><cell>0.68</cell><cell>0.70</cell><cell>0.67</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Examples of misclassified sentences by the BERT + featured logistic regression model (NLP4IF'19)</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Experimental results on fragment-level classification on NLP4IF'19 test set. All scores are reported in micro-F1 (as in the original challenge). Scores in bold are the ones outperforming SOTA model.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">NLP4IF'19</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Average</cell><cell>Appeal-Fear</cell><cell>Black-White</cell><cell>Casual-Over.</cell><cell>Doubt</cell><cell>Exag.-Min.</cell><cell>Flag-Waving</cell><cell>Loaded-L.</cell><cell>Namecalling</cell><cell>Reductio-Hit.</cell><cell>Repetition</cell><cell>Slogans</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Misclassified NameCalling Labeling. False Negative (in red), False Positive (in blue), the correctly classified propaganda spans (underlined).</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>The evaluation system only accepts the exact list Results on span classification on SemEval'20 T11 test set (micro-F1).</figDesc><table><row><cell>SemEval'20 T11</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>https://propaganda.qcri.org/ nlp4if-shared-task/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>http://www.wjh.harvard.edu/ ˜inquirer/ homecat.htm</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>https://www.englishclub.com/vocabulary/pronouns-</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>This work is partially supported by the <rs type="projectName">AN-SWER</rs> project <rs type="projectName">PIA FSN2</rs> n.</p><p><rs type="grantNumber">P159564-2661789/DOS0060094</rs> between Inria and Qwant. Moreover, this work has been supported by the <rs type="funder">French government</rs>, through the <rs type="funder">3IA Côte d'Azur Investments</rs> in the Future project managed by the <rs type="funder">National Research Agency (ANR)</rs> with the reference number <rs type="grantNumber">ANR-19-P3IA-0002</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_UUaGRAq">
					<orgName type="project" subtype="full">AN-SWER</orgName>
				</org>
				<org type="funded-project" xml:id="_Vsu6tYP">
					<orgName type="project" subtype="full">PIA FSN2</orgName>
				</org>
				<org type="funding" xml:id="_akRmcdG">
					<idno type="grant-number">P159564-2661789/DOS0060094</idno>
				</org>
				<org type="funding" xml:id="_VAPKsAc">
					<idno type="grant-number">ANR-19-P3IA-0002</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A review of feature selection and sentiment analysis technique in issues of propaganda</title>
		<author>
			<persName><forename type="first">Ahmad</forename><surname>Siti Rohaidah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Muhammad</forename><surname>Zakwan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Muhammad</forename><surname>Rodzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nurlaila</forename><surname>Syafira Shapiei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nurhafizah</forename><surname>Moziyana Mohd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suhaila</forename><surname>Yusop</surname></persName>
		</author>
		<author>
			<persName><surname>Ismail</surname></persName>
		</author>
		<idno type="DOI">10.14569/IJACSA.2019.0101132</idno>
	</analytic>
	<monogr>
		<title level="j">International Journal of Advanced Computer Science and Applications</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">11</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Depechemood++: a bilingual emotion lexicon built through simple yet powerful techniques</title>
		<author>
			<persName><forename type="first">Oscar</forename><surname>Araque</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lorenzo</forename><surname>Gatti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacopo</forename><surname>Staiano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Guerini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Affective Computing</title>
		<imprint>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">SentiWordNet 3.0: An enhanced lexical resource for sentiment analysis and opinion mining</title>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Baccianella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Esuli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabrizio</forename><surname>Sebastiani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventh International Conference on Language Resources and Evaluation (LREC&apos;10)</title>
		<meeting>the Seventh International Conference on Language Resources and Evaluation (LREC&apos;10)<address><addrLine>Valletta, Malta</addrLine></address></meeting>
		<imprint>
			<publisher>European Language Resources Association (ELRA</publisher>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Proppy: Organizing the news based on their propagandistic content</title>
		<author>
			<persName><forename type="first">Alberto</forename><surname>Barrón-Cedeño</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Israa</forename><surname>Jaradat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Giovanni</forename><surname>Martino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Preslav</forename><surname>Nakov</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.ipm.2019.03.005</idno>
	</analytic>
	<monogr>
		<title level="j">formation Processing Management</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Concreteness ratings for 40 thousand generally known english word lemmas</title>
		<author>
			<persName><forename type="first">Marc</forename><surname>Brysbaert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amy</forename><surname>Warriner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Kuperman</surname></persName>
		</author>
		<idno type="DOI">10.3758/s13428-013-0403-5</idno>
	</analytic>
	<monogr>
		<title level="j">Behavior research methods</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Aschern at SemEval-2020 task 11: It takes three to tango: RoBERTa, CRF, and transfer learning</title>
		<author>
			<persName><forename type="first">Anton</forename><surname>Chernyavskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dmitry</forename><surname>Ilvovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Preslav</forename><surname>Nakov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourteenth Workshop on Semantic Evaluation</title>
		<meeting>the Fourteenth Workshop on Semantic Evaluation<address><addrLine>Barcelona (online</addrLine></address></meeting>
		<imprint>
			<publisher>International Committee for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1462" to="1468" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">SemEval-2020 task 11: Detection of propaganda techniques in news articles</title>
		<author>
			<persName><forename type="first">Giovanni</forename><surname>Da</surname></persName>
		</author>
		<author>
			<persName><forename type="first">San</forename><surname>Martino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alberto</forename><surname>Barrón-Cedeño</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Henning</forename><surname>Wachsmuth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rostislav</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Preslav</forename><surname>Nakov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th International Workshop on Semantic Evaluation</title>
		<meeting>the 14th International Workshop on Semantic Evaluation<address><addrLine>SemEval; Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A survey on computational propaganda detection</title>
		<author>
			<persName><forename type="first">Giovanni</forename><surname>Da</surname></persName>
		</author>
		<author>
			<persName><forename type="first">San</forename><surname>Martino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Cresci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alberto</forename><surname>Barron-Cedeno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seunghak</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roberto</forename><surname>Di</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Preslav</forename><surname>Nakov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 29th International Joint Conference on Artificial Intelligence and the 17th Pacific Rim International Conference on Artificial Intelligence (IJCAI-PRICAI2020)</title>
		<meeting>29th International Joint Conference on Artificial Intelligence and the 17th Pacific Rim International Conference on Artificial Intelligence (IJCAI-PRICAI2020)<address><addrLine>Yokohama, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note>IJCAI-PRICAI2020</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Fine-grained analysis of propaganda in news article</title>
		<author>
			<persName><forename type="first">Giovanni</forename><surname>Da</surname></persName>
		</author>
		<author>
			<persName><forename type="first">San</forename><surname>Martino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seunghak</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alberto</forename><surname>Barrón-Cedeño</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rostislav</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Preslav</forename><surname>Nakov</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1565</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="5636" to="5646" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A computational approach to politeness with application to social factors</title>
		<author>
			<persName><forename type="first">Cristian</forename><surname>Danescu-Niculescu-Mizil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moritz</forename><surname>Sudhof</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics<address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="250" to="259" />
		</imprint>
	</monogr>
	<note>: Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">The Persuasion Handbook: Developments in Theory and Practice</title>
		<author>
			<persName><forename type="first">James</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dillard</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Pfau</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>Sage Publications, Inc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Toward a philosophy of propaganda</title>
		<author>
			<persName><forename type="first">Wladimir</forename><surname>Gottlieb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eliasberg</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Jewish Social Studies</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">1/2</biblScope>
			<biblScope unit="page" from="51" to="63" />
			<date type="published" when="1957">1957</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Connotation lexicon: A dash of sentiment beneath the surface meaning</title>
		<author>
			<persName><forename type="first">Song</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seok</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Polina</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Kuznetsova</surname></persName>
		</author>
		<author>
			<persName><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics<address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1774" to="1784" />
		</imprint>
	</monogr>
	<note>: Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">On sentence representations for propaganda detection: From handcrafted features to word embeddings</title>
		<author>
			<persName><forename type="first">André</forename><surname>Ferreira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cruz</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Gil</forename><surname>Rocha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Henrique</forename><forename type="middle">Lopes</forename><surname>Cardoso</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-5015</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second Workshop on Natural Language Processing for Internet Freedom: Censorship, Disinformation, and Propaganda</title>
		<meeting>the Second Workshop on Natural Language Processing for Internet Freedom: Censorship, Disinformation, and Propaganda<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="107" to="112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">The general inquirer: A computer approach to content analysis. philip j. stone , dexter c. dunphy , marshall s. smith , daniel m. ogilvie</title>
		<author>
			<persName><forename type="first">C</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName><surname>Gilman</surname></persName>
		</author>
		<idno type="DOI">10.1086/224539</idno>
	</analytic>
	<monogr>
		<title level="j">American Journal of Sociology</title>
		<imprint>
			<biblScope unit="volume">73</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="634" to="635" />
			<date type="published" when="1968">1968</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Before name-calling: Dynamics and triggers of ad hominem fallacies in web argumentation</title>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Habernal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Henning</forename><surname>Wachsmuth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benno</forename><surname>Stein</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N18-1036</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>New Orleans, Louisiana</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="386" to="396" />
		</imprint>
	</monogr>
	<note>Long Papers</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">ApplicaAI at SemEval-2020 task 11: On RoBERTa-CRF, span CLS and whether self-training helps them</title>
		<author>
			<persName><forename type="first">Dawid</forename><surname>Jurkiewicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Łukasz</forename><surname>Borchmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Izabela</forename><surname>Kosmala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Filip</forename><surname>Graliński</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourteenth Workshop on Semantic Evaluation</title>
		<meeting>the Fourteenth Workshop on Semantic Evaluation<address><addrLine>Barcelona (online</addrLine></address></meeting>
		<imprint>
			<publisher>International Committee for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1415" to="1424" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Social influence by manipulation: A definition and case of propaganda</title>
		<author>
			<persName><forename type="first">Haavard</forename><surname>Koppang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Middle East Critique</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="117" to="143" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Propaganda technique in the world war</title>
		<author>
			<persName><forename type="first">Harold</forename><surname>Dwight</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lasswell</forename></persName>
		</author>
		<imprint>
			<date type="published" when="1938">1938</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">An NLP analysis of exaggerated claims in science news</title>
		<author>
			<persName><forename type="first">Yingya</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jieke</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bei</forename><surname>Yu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W17-4219</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 EMNLP Workshop: Natural Language Processing meets Journalism</title>
		<meeting>the 2017 EMNLP Workshop: Natural Language Processing meets Journalism<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="106" to="111" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Persuasion of the undecided: Language vs. the listener</title>
		<author>
			<persName><forename type="first">Liane</forename><surname>Longpre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Esin</forename><surname>Durmus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W19-4519</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th Workshop on Argument Mining</title>
		<meeting>the 6th Workshop on Argument Mining<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="167" to="176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Divisive language and propaganda detection using multi-head attention transformers with deep learning BERT-based language models for binary classification</title>
		<author>
			<persName><forename type="first">Norman</forename><surname>Mapes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anna</forename><surname>White</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Radhika</forename><surname>Medury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sumeet</forename><surname>Dua</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-5014</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second Workshop on Natural Language Processing for Internet Freedom: Censorship, Disinformation, and Propaganda</title>
		<meeting>the Second Workshop on Natural Language Processing for Internet Freedom: Censorship, Disinformation, and Propaganda<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="103" to="106" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Hitachi at SemEval-2020 task 11: An empirical study of pre-trained transformer family for propaganda detection</title>
		<author>
			<persName><forename type="first">Gaku</forename><surname>Morio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Terufumi</forename><surname>Morishita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hiroaki</forename><surname>Ozaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Toshinori</forename><surname>Miyoshi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourteenth Workshop on Semantic Evaluation</title>
		<meeting>the Fourteenth Workshop on Semantic Evaluation<address><addrLine>Barcelona (online</addrLine></address></meeting>
		<imprint>
			<publisher>International Committee for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1739" to="1748" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Extracting and networking emotions in extremist propaganda</title>
		<author>
			<persName><forename type="first">Travis</forename><surname>Morris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Intelligence and Security Informatics Conference</title>
		<imprint>
			<date type="published" when="2012">2012. 2012</date>
			<biblScope unit="page" from="53" to="59" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Exploring the limits of transfer learning with a unified text-totext transformer</title>
		<author>
			<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">140</biblScope>
			<biblScope unit="page" from="1" to="67" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Annotating argument components and relations in persuasive essays</title>
		<author>
			<persName><forename type="first">Christian</forename><surname>Stab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers</title>
		<meeting>COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<publisher>Dublin City University and Association for Computational Linguistics</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1501" to="1510" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A computational exploration of exaggeration</title>
		<author>
			<persName><forename type="first">Enrica</forename><surname>Troiano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlo</forename><surname>Strapparava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gözde</forename><surname>Özbal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Serra</forename><surname>Sinem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tekiroglu</forename></persName>
		</author>
		<idno type="DOI">10.18653/v1/D18-1367</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="3296" to="3304" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Metaphor detection with cross-lingual model transfer</title>
		<author>
			<persName><forename type="first">Yulia</forename><surname>Tsvetkov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leonid</forename><surname>Boytsov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anatole</forename><surname>Gershman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Nyberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<idno type="DOI">10.3115/v1/P14-1024</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics<address><addrLine>Baltimore, Maryland</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="248" to="258" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Norms of valence, arousal, and dominance for 13,915 english lemmas</title>
		<author>
			<persName><forename type="first">Amy</forename><surname>Warriner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Kuperman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Brysbaert</surname></persName>
		</author>
		<idno type="DOI">10.3758/s13428-012-0314-x</idno>
	</analytic>
	<monogr>
		<title level="j">Behavior research methods</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Recognizing contextual polarity in phraselevel sentiment analysis</title>
		<author>
			<persName><forename type="first">Theresa</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Janyce</forename><surname>Wiebe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Hoffmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing<address><addrLine>Vancouver, British Columbia, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="347" to="354" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Fine-grained propaganda detection with fine-tuned BERT</title>
		<author>
			<persName><forename type="first">Shehel</forename><surname>Yoosuf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yin</forename><surname>Yang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-5011</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second Workshop on Natural Language Processing for Internet Freedom: Censorship, Disinformation, and Propaganda</title>
		<meeting>the Second Workshop on Natural Language Processing for Internet Freedom: Censorship, Disinformation, and Propaganda<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="87" to="91" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
