<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Abstra: Toward Generic Abstractions for Data of Any Model</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Nelly</forename><surname>Barret</surname></persName>
							<email>nelly.barret@inria.fr</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Inria</orgName>
								<orgName type="institution" key="instit2">Institut Polytechnique de Paris</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ioana</forename><surname>Manolescu</surname></persName>
							<email>ioana.manolescu@inria.fr</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Inria</orgName>
								<orgName type="institution" key="instit2">Institut Polytechnique de Paris</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Prajna</forename><surname>Upadhyay</surname></persName>
							<email>prajna-devi.upadhyay@inria.fr</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Inria</orgName>
								<orgName type="institution" key="instit2">Institut Polytechnique de Paris</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Abstra: Toward Generic Abstractions for Data of Any Model</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">D8B96B958A2C339806B22BD2A87322BE</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-04-12T14:48+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Digital data sharing leads to unprecedented opportunities to develop data-driven systems for supporting economic activities (e.g., e-commerce or maps for tourism), the social and political life, and science. Many open-access datasets are RDF graphs, but others are CSV files, Neo4J property graphs, JSON or XML documents, etc.</p><p>Potential users need to understand a dataset in order to decide if it is useful for their goal. While some datasets come with a schema and/or documentation, this is not always the case. Data summaries or schema can be derived from the data, but their technical features may be hard to understand for non-IT specialist users, or they may overwhelm users with information.</p><p>We propose to demonstrate Abstra, a dataset abstraction system, which (𝑖) applies on a large variety of data models; (𝑖𝑖) computes a description meant for humans (as opposed to a schema meant for a parser), akin to an Entity-Relationship diagram; (𝑖𝑖𝑖) integrates Information Extraction data profiling to classify dataset content among a set of categories of interest to the user.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Open-access data being shared over the Internet has enormous positive impact. It enables the development of new businesses, economic opportunities and applications; it also leads to circulating knowledge on a variety of topics, from health to education, environment, the arts, science, news, etc.</p><p>The World Wide Web Consortium's recommended data sharing format is RDF graphs, and many datasets are shared this way. However, in practice, other formats are also widely used. For instance, CSV files are shared on portals such as Kaggle or the French public portal data.gouv.fr; hundreds of millions of bibliographic notices on PubMed, a leading medical scientific site, are available in XML; JSON is increasingly used, e.g., to document the activity of the French parliament on the websites NosDeputes.fr and NosSenateurs.fr, on Twitter, etc. Relational databases are sometimes shared as dumps, including schema constraints such as primary and foreign keys, etc., or as CSV files; property graphs (PGs, in short, such as pioneered by Neo4J) are used to share Offshore leaks, a journalistic database of offshore companies.</p><p>Users who must decide whether to use a dataset in an application need a basic understanding of its content and the suitability to their need.</p><p>Towards this goal, schemas may be available to describe the data structure, yet they have some limitations: (𝑖) schemas are often unavailable for semistructured datasets (XML, JSON, RDF, PGs). Even when a schema is supplied with or extracted from the data, e.g., <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b18">19]</ref>: (𝑖𝑖) schema syntactic details, such as regular expressions, etc., are hard to interpret for non-expert users; (𝑖𝑖𝑖) a schema focuses primarily on the dataset structure, not on its content. It does not exploit the linguistic information encoded in node names, in the string values the dataset may contain, etc.; (𝑖𝑣) schemas employ the data producer's terminology, not the categories of interest to users; (𝑣) schemas do not quantitatively reflect the dataset, whereas knowing "what is the main content of a dataset" can be very helpful for a first acquaintance with it. Data summarization has been studied for semistructured data models, e.g., <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b10">11]</ref>. In the particular case when the dataset is RDF, it may come with an ontology describing the semantics of the dataset, which is a step toward lifting limitation (𝑖𝑖𝑖) above; however, all the others still apply. Mining for patterns in the dataset <ref type="bibr" target="#b12">[13]</ref> allows to find popular motifs, e.g., items often purchased together, or small groups of strongly connected nodes in a graph, etc. This avoids shortcomings (𝑖) and (𝑣), but not the others. Dataset documentation, when well-written, is most helpful for users. However, it still suffers from limitations (𝑖) and (𝑖𝑣) above: it is often lacking, and it reflects the producer's view.</p><p>We propose to demonstrate Abstra, a all-in-one system for abstracting any relational, CSV, XML, JSON, RDF or PG dataset. Abstra is based on the idea that any dataset comprises some records, typically grouped in collections (which we view as sets). Records describe entities or relationships in the classical conceptual database design sense <ref type="bibr" target="#b16">[17]</ref>; Abstra entities can have deeply nested structure. When several collection of entities co-exist in a dataset, relationships typically connect them. To identify the entities and relationships, Abstra proceeds as follows.</p><p>(1). Given any dataset, Abstra models it as a graph, and identifies collections of equivalent nodes, leveraging graph structural summarization, as we describe in Section 2.</p><p>(2). Among the collections, Abstra detects the main ones, that is: a small number of collections, each comprising records that may be simple or very complex (i.e., with deeply nested structure), and such that these collections, together, hold a large part of the dataset contents. The challenge here is to detect, in the data graph, the nodes and edges that are "part of" each main collection record, and to do so efficiently even if the graph has complex, cyclic structure. This is addressed by introducing a notion of data weight and exploiting it as we describe in Section 3.</p><p>(3). Abstra attempts to classify each collection of entities into a given semantic category, such as Person, Product, Geographi-calPosition, etc., using a set of semantic properties, some of which we collect from well-known knowledge bases, while others can be elicited from users. The classification leverages Information Extraction to detect the presence of entities in the text fields of the data, then exploit them in our semantic properties (Section 4). It also uses language models to detect proximity between the dataset and the target categories.</p><p>Abstra outputs a natural-language, compact description of the main, classified collections of entities, together with the possible relationships in which they participate; this description is free of any data model-specific details. For instance, given an XMark <ref type="bibr" target="#b17">[18]</ref> XML document describing an online auctions site, containing 2.3M nodes, which, together, have 80 different labels, and are organized on 124 labeled paths, Abstra returns: "A collection of Person entities, a collection of Product, and a collection of category" (the latter are used to describe the items for sale). Users can explore them and inspect their internal structure through an interactive GUI (see Section 5). Below, we describe the abstraction steps and outline the demonstration scenarios, before concluding. Abstra examples and a video can be found at: https://team.inria.fr/cedar/projects/abstra/.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">BUILDING A COLLECTION GRAPH</head><p>We first explain how any dataset is converted in a graph representation (Section 2.1), before partitioning it and constructing the central tool of our method, the collection graph (Section 2.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Graph representation of any dataset</head><p>The graph representation we start from has been introduced in ConnectionLens [2, 8], a graph-based heterogeneous data integration system, to which we bring some modifications. Any relational, XML, JSON, RDF, or PG dataset is turned into a directed graph 𝐺 0 = (𝑁 0 , 𝐸 0 , 𝜆 0 ) where 𝐸 0 ⊆ 𝑁 0 × 𝑁 0 is a set of directed edges, and 𝜆 0 is a function labeling each node and edge with a string label, that could in particular be 𝜖 (the empty label).</p><p>XML trees and RDF graphs naturally map into this modeling. JSON documents are modeled as trees. A common model, also used in ConnectionLens, turns maps and arrays into unlabeled nodes. Figure <ref type="figure" target="#fig_0">1</ref> shows a sample JSON fragment and, at left, this tree model: the 𝜖-labeled nodes, from the top down, correspond respectively to the outermost map, the outermost array, the innermost map, and the innermost array. In Abstra, we are interested in recognizing groups of nodes that play similar roles in the dataset (see Section 2.2), and we facilitate that by attaching them more meaningful node names. As illustrated in Figure <ref type="figure" target="#fig_0">1</ref> at the bottom right, we (𝑖) move the labels of edges which connect a map parent to its children, on the child nodes; (𝑖𝑖) label the children of an array node with label of their parent, to which we concatenate . (a dot). In general, a node's label is computed from the closest non-empty label among its ancestors nodes, and concatenating a dot whenever going from an array to one of its children. This process ensures that every node but the root has a non-empty label.</p><p>A CSV file leads to a tree, whose root has an edge going to a node for each tuple; in turn, such a node has edges (labeled with the possible CSV attribute names) going toward each attribute value. A relational database is similarly modeled; in the presence of a primary key-foreign key constraint of the form "𝑅.𝑎 is a foreign key referencing 𝑆.𝑏", each node 𝑛 𝑟 corresponding to an 𝑅 tuple has an outgoing edge labeled 𝑎 pointing to the respective 𝑆 tuple node.</p><p>From a PG, we create a node for each PG node and for each of its attributes, with labeled edges connecting them. We also create a node for each PG edge, having one child node for each edge attribute (if any). Whenever the PG contains an edge 𝑒 from 𝑛𝑝 1 to 𝑛𝑝 2 , our graph has an edge from 𝑛𝑝 1 to the node 𝑛𝑒 representing 𝑒, and one from 𝑛𝑒 to 𝑛𝑝 2 .</p><p>In 𝐺 0 , some edges have empty (𝜖) labels, e.g., parent-child edges in XML, while other edges are labeled. For uniformity, Abstra transforms 𝐺 0 into a normalized graph 𝐺, copying all the nodes of 𝐺 0 and all its 𝜖-label edges, and replacing each 𝐺 0 edge of the form 𝑛 1 𝑙 -→ 𝑛 2 where 𝑙 ≠ 𝜖 by two unlabeled edges 𝑛 1 → 𝑥 𝑙 , 𝑥 𝑙 → 𝑛 2 where 𝑥 𝑙 is a new intermediary node labeled 𝑙. All subsequent Abstra steps apply on the normalized graph 𝐺.</p><p>Figure <ref type="figure" target="#fig_1">2</ref> shows a sample bibliographic data graph 𝐺. It depicts three papers (one partially shown), which are published in (pIn) conferences. The papers are written by (wb) authors, described by their name and email. Note the inverse "has written" (hW) edges going from authors to their papers. Author 21 is invited (inv) by the conference organizers. As Figure <ref type="figure" target="#fig_1">2</ref> shows, the graph may contain: (𝑖) nodes such as papers, whose information content is deeply nested, and (𝑖𝑖) several cycles (in-cycle edges are shown in red). Within each leaf (value) node, ConnectionLens extracts named entities such as: persons (highlighted in yellow), dates (pink highlight), emails (light blue), etc. Abstra leverages these in order to classify the main entity collections (Section 4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Partitioning nodes into collections</head><p>To leverage structural information present in 𝐺, we build a partition P = {𝐶 𝑖 } 𝑖 of the graph nodes 𝑁 , such that 𝑖 𝐶 𝑖 = 𝑁 and the 𝐶 𝑖 are pairwise disjoint. We say the nodes from a given set 𝐶 𝑖 are equivalent, and call 𝐶 𝑖 an equivalence class. Many node partitioning schemes, a.k.a. quotient summaries, exist <ref type="bibr" target="#b6">[7]</ref>. We need a method that is robust to heterogeneity, i.e., it can recognize the various papers in Figure <ref type="figure" target="#fig_1">2</ref> even though they have heterogeneous structure, and efficiently computed (ideally in linear time in the size of 𝐸).</p><p>For RDF graphs, we use Type Strong summarization <ref type="bibr" target="#b9">[10]</ref>, which satisfies these requirements; it leverages RDF types when available, but can also identify interesting equivalence classes without them. We extend it also to PGs and graphs derived from CSV files and relational databases. For graphs derived from XML or JSON as discussed in Section 2.1, we simply partition the nodes by their labels. On the graph in Figure <ref type="figure" target="#fig_1">2</ref>, the equivalence classes are: {1, 40}; {6, 8, 38}; {20, 21, 23}, etc.; there is one class for each distinct label of non-leaf nodes, and one class for each set of leaf nodes whose parents are equivalent.</p><p>We call collection graph the graph whose nodes are the collections 𝐶 𝑖 , and having an edge 𝐶 𝑖 → 𝐶 𝑘 if and only if for some nodes 𝑛 𝑖 ∈ 𝐶 𝑖 , 𝑛 𝑗 ∈ 𝐶 𝑗 , 𝑛 𝑖 → 𝑛 𝑗 ∈ 𝐸. Figure <ref type="figure" target="#fig_2">3</ref> shows the collection graph corresponding to the graph in Figure <ref type="figure" target="#fig_1">2</ref>. Here, in each collection, all nodes have the same label, shown in the collection; this does not hold in general, e.g., in a collection of RDF nodes, each node has a different label. The label year# is used to denote the collection of text children of the nodes from the collection with the label year and similarly for the others whose label end in #. The dw attributes will be discussed in Section 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">IDENTIFYING THE MAIN ENTITIES TO REPORT AND THEIR RELATIONSHIPS</head><p>Among the collections C, some are clearly better abstractions of the dataset than others, e.g., in Figure <ref type="figure" target="#fig_2">3</ref>, paper seems a better candidate than its child collection year. However, one cannot simply return "the parent (or root) collection(s)": the collection graph may have no root at all, if it is cyclic as in Figure <ref type="figure" target="#fig_2">3</ref> (red edges are part of cycles).</p><p>Even if a root collection exists, it may not be the best choice. For instance, consider XHTML search results grouped in pages, of the form ⟨top⟩ ⟨page⟩ ⟨result⟩...⟨/result⟩ ⟨result⟩... ⟨/result⟩ ⟨/page⟩ ⟨page⟩... ⟨/page⟩ ... ⟨/top⟩. Here, the top collection is that of pages, but the actual data is in the results, thus, "a collection of results" is a better abstraction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Overview of the method</head><p>A high-level view of our method is the following (concrete details will be provided below):</p><p>(1) Selecting the main entities (Section 3.2): (a) We assign to each collection a weight, and to each edge in the collection graph, a transfer factor. (b) We propagate weights in the collection graph, based on the weights and transfer factors, to assign to each collection a score that reflects not only its own weight, but also its position in the graph.</p><p>(c) In a greedy fashion, we select the main entities by repeating the following steps: (i) Select the collection node 𝐶 𝐸 currently having the highest score, as a root of a main entity; (ii) Determine the boundary of the entity 𝐶 𝐸 : this is a connected subgraph of the collection graph, containing 𝐶 𝐸 . We consider all this subgraph as part of 𝐶 𝐸 , which will be reported to users including all its boundary; (iii) Update the collection graph to reflect the selection of 𝐶 𝐸 and its boundaries, and recompute the collection scores; until a certain maximum number 𝐸 𝑚𝑎𝑥 of entities have been selected, or these entities together cover a sufficient fraction 𝑐𝑜𝑣 𝑚𝑖𝑛 of the data.</p><p>(2) Selecting relationships between the main entity collections. These relationships will also be reported as part of the abstraction (Section 3.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Main entity selection</head><p>We assign to each leaf node in 𝐺 an own data weight (𝑜𝑤) equal to the number of edges incoming that node. In tree data formats, 𝑜𝑤 is 1; in RDF, for instance, a literal that is the value of many triples may have 𝑜𝑤 &gt; 1. We leverage this to define the 𝑜𝑤 of a leaf collection as the sum of the 𝑜𝑤 of its nodes, e.g., in Figure <ref type="figure" target="#fig_2">3</ref>, 𝑜𝑤 (title#) = 2, 𝑜𝑤 (name#) = 5 etc.</p><p>For each edge 𝐶 𝑖 → 𝐶 𝑗 in the collection graph, we define the edge transfer factor 𝑓 𝑗,𝑖 as the fraction of nodes in 𝐶 𝑗 having a parent node in 𝐶 𝑖 ; 0 &lt; 𝑓 𝑗,𝑖 ≤ 1. Intuitively, 𝑓 𝑖,𝑗 of 𝐶 𝑗 's weight can also be seen as belonging to its parent 𝐶 𝑖 . For instance, there are 5 name nodes, but two belong to conferences, thus the transfer factor from name to conf is 𝑓 𝑛𝑎𝑚𝑒,𝑐𝑜𝑛𝑓 = 2/5.</p><p>We experiment with two weight propagation methods.</p><p>• We run the PageRank <ref type="bibr" target="#b5">[6]</ref> algorithm on the collection graph with the edge direction inverted, so that each child node transfers 𝑓 𝑖,𝑗 of its weight to the parent. Initially, only leaf collections have non-zero 𝑜𝑤, but successive PageRank iterations spread their weights across the graph. We denote this method PR 𝑜𝑤 . • Our second method propagates weights still backwards, but only outside of the collection graph cycles. Specifically, we assign to each collection a data weight 𝑑𝑤, which on leaf collection is initialized to 𝑜𝑤, and on others, to 0. Then, for each non-leaf 𝐶 𝑖 , and non cyclic path from 𝐶 𝑖 to a leaf collection 𝐶 𝑘 , we increase 𝑑𝑤 (𝐶 𝑖 ) by 𝑓 𝑘,𝑖 •𝑜𝑤 (𝐶 𝑘 ). We denote this method prop 𝑑𝑤 .</p><p>For instance, using the second method, the collection author in Figure <ref type="figure" target="#fig_2">3</ref> obtains 𝑑𝑤 = 6, corresponding to 3 transferred from mail, and 3 transferred from name. The intuition behind prop 𝑑𝑤 is that edges that are part of cycles may have a meaning closer to "symmetric relationships between entities", than to "including a collection in another collection's boundary".</p><p>To determine entity boundaries, we proceed as follows:</p><p>• When using prop 𝑑𝑤 , we consider part of the boundary of an entity 𝐶 𝑖 , any entity 𝐶 𝑘 that transferred some weight to 𝐶 𝑖 , and all the edges along which such transfers took place. For instance, in Figure <ref type="figure" target="#fig_2">3</ref>, mail and name are within the boundary of author.  • When using PR 𝑜𝑤 , to determine the boundary of 𝐶 𝑖 , we traverse the graph edges starting from 𝐶 𝑖 and include its neighbor node 𝐶 𝑗 if and only if (𝑖) the edge 𝐶 𝑖 → 𝐶 𝑗 has a transfer factor of at least 𝑓 𝑚𝑖𝑛 , or (𝑖𝑖) each node from 𝐶 𝑖 has at most one child in 𝐶 𝑗 . The intuition for (𝑖𝑖) is that such a child 𝐶 𝑗 "can be assimilated to an attribute of 𝐶 𝑖 ", rather than being "independent of it".</p><p>Finally, to update the graph after selecting one main entity 𝐶 𝐸 , each leaf collection in the boundary of 𝐶 𝐸 subtracts from its own weight 𝑜𝑤 the fraction (at most 1.0) that it propagated to 𝐶 𝐸 . For instance, once author is selected with name in its boundary, the 𝑜𝑤 of name decreases to 2. Then, the scores of all graph collections (𝑑𝑤, respectively, PageRank score based on 𝑜𝑤) are recomputed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Relationship selection</head><p>Having selected the main entities {𝐶<ref type="foot" target="#foot_0">1</ref> 𝐸 , . . . , 𝐶 𝑚𝑎𝑥 𝐸 𝐸 } and their boundaries, every oriented path in the collection graph that goes from a given 𝐶 𝑖 𝐸 to another 𝐶 𝑗 𝐸 is reported as a relationship. For instance, in Figure <ref type="figure" target="#fig_2">3</ref>, if the main entities are author (with mail and name in its boundary) and paper (with year, title and abstract in its boundary), the relationships reported are: paper wB ---→ author, author hW ---→ paper, and paper pIn.conf.inv ---------→ author. If the scores lead to reporting three main entities, the two above entities and also conf (with name and inv in its boundary), the rela- </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Discussion</head><p>Abstra may return different results on a given dataset, depending on the scoring method used (prop 𝑑𝑤 or PR 𝑜𝑤 ), as well as the parameters: 𝐸 𝑚𝑎𝑥 and 𝑐𝑜𝑣 𝑚𝑖𝑛 (Section 3.1), and 𝑓 𝑚𝑖𝑛 (Section 3.2). Empirically, we have used 𝐸 𝑚𝑎𝑥 ∈ {3, 5}, 𝑐𝑜𝑣 𝑚𝑖𝑛 = 0.8 and 𝑓 𝑚𝑖𝑛 = 0.3. More generally, classical Entity-Relationship (E-R) modeling is known to include a subjective factor, and for a given database, several E-R models may be correct. Our focus is on not missing any essential component of the dataset, while allowing users to limit the amount of information through 𝐸 𝑚𝑎𝑥 , and classifying the main entities into semantic categories, to make them as informative as possible, as we explain below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">MAIN ENTITY CLASSIFICATION</head><p>To each main entity 𝐶 thus identified, we want to associate a category from a predefined set K of semantic classes, and using also a set P of semantic properties, which have known domain and range constraints connecting them to the classes in K.</p><p>We bootstrapped our K and P by selecting six common classes (Person, Place, Organization, Creative Work, Event and Product), and associating them, based on WikiData and YAGO, properties they are likely to have, or to be values of. For instance, a Person has property birthPlace, while the value of birthPlace is a Place). Next, we rely on GitTables <ref type="bibr" target="#b13">[14]</ref>, a repository of 1.5M tables extracted from Github. For each attribute name encountered in a table, it provides candidate properties from DBPedia <ref type="bibr" target="#b2">[3]</ref> and/or schema.org 1 ; it also provides the domain and range triples corresponding to these properties. For instance, GitTable's entry for gender is:</p><p>"id":"schema:gender", "label":"gender", "description":"Gender of something, typically a Person, "domain":["schema:Person", "schema:SportsTeam"], "range": ["schema:GenderType", "schema:Text"] GitTables have been populated using SHERLOCK <ref type="bibr" target="#b14">[15]</ref>, a state-ofthe-art deep learning semantic annotation technique. From GitTables, we derive 4.187 P properties; 3.687 among them have domain information, and 3.898 have range statements.</p><p>The overall classification process is outlined in Figure <ref type="figure" target="#fig_3">4</ref>; solid arrows connect associated data items and trace the classification process, while dotted arrows go from a set to one of its elements.</p><p>At the top of the figure, if the collection contains RDF resources considered equivalent by RDFQuotient due to a common type 𝜏, we return that type, considering it is the most precise.</p><p>Otherwise, we exploit two kinds of information attached to 𝐶:</p><p>(1) We consider each data property 𝑑𝑝 𝑖 that 𝐶 has, such as mail for the author collection in Figure <ref type="figure" target="#fig_2">3</ref>. Out of all the values that 𝑑𝑝 𝑖 takes on a node from 𝐶, we compute an entity profile 𝜎 𝐶,𝑑𝑝 𝑖 , reflecting the entities extracted from these values. For instance, 𝜎 author, mail states that each value of the property mail contains an email (blue highlight in Figure <ref type="figure" target="#fig_1">2</ref>), and that overall, 100% of the length of these values is part of an email entity. In general, a profile may reflect the presence of entities of several types, which may span over only a small part of the property values.  Finally, 𝐶 is classified with the class 𝑘 * ∈ K having received the highest sum of votes. The process resembles domain inference using RDF Schema ontology constraints, with the difference that our "votes" are quantified by similarity and support, and, to keep things simple for the users, we select a single class, the one having the strongest support.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">SYSTEM AND SCENARIOS</head><p>Abstra is implemented in Java, leveraging the graph creation (including entity extraction) and Postgres-based store of Connection-Lens <ref type="bibr" target="#b1">[2]</ref>. All Abstra steps scale up linearly in the data size, which we experimentally verified on datasets of up to tens of millions of edges. The main memory needs are in TypedStrong partitioning, namely 𝑂 (|𝑁 |); other operations are implemented in SQL and benefit from Postgres' optimizations.</p><p>We have computed abstractions of dozens of synthetic benchmark datasets used in the data management literature, such as XMark <ref type="bibr" target="#b17">[18]</ref>, BSBM <ref type="bibr" target="#b4">[5]</ref>, LUBM <ref type="bibr" target="#b11">[12]</ref>, and real-life datasets about cooking, NASA flights, clean energy production, Nobel prizes, CSV datasets from Kaggle, etc. varying the data model, the number of collections and entity complexity, the presence of relations, etc. During the demonstration, users will be able to: (𝑖) change the system parameters, and see the impact on the classification results; (𝑖𝑖) edit the semantic information K and P to influence the entity classification; (𝑖𝑖𝑖) edit a set of small RDF, JSON and XML examples, then abstract them to see the impact.</p><p>Abstractions are shown both as HTML text and as a lightweight E-R diagram of the main entity collections and their relationships. Clicking on a collection launches a node-link GUI (JavaScript) enabling users to see records from the respective collection, navigate to neighbors, etc. (see Figure <ref type="figure" target="#fig_5">5</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>Abstra extracts the main entities and relationships from heterogeneousstructure datasets, leveraging Information Extraction, language models, knowledge bases, and user input to classify the main collections, so as to get close to the user's interest. The goal of our tool is to generate, as automatically as possible, compact dataset descriptions. Abstra complements schema extraction <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b18">19]</ref> or data profiling <ref type="bibr" target="#b0">[1]</ref>, aimed at more technical uses and users; Abstra aims to help novice, first-time users discover and start interacting with the data, through compact, graphical abstractions.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: JSON fragment (top), its direct tree model (bottom left), and the model used in Abstra (bottom right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Sample normalized graph.A relational database is similarly modeled; in the presence of a primary key-foreign key constraint of the form "𝑅.𝑎 is a foreign key referencing 𝑆.𝑏", each node 𝑛 𝑟 corresponding to an 𝑅 tuple has an outgoing edge labeled 𝑎 pointing to the respective 𝑆 tuple node.From a PG, we create a node for each PG node and for each of its attributes, with labeled edges connecting them. We also create a node for each PG edge, having one child node for each edge attribute (if any). Whenever the PG contains an edge 𝑒 from 𝑛𝑝 1 to 𝑛𝑝 2 , our graph has an edge from 𝑛𝑝 1 to the node 𝑛𝑒 representing 𝑒, and one from 𝑛𝑒 to 𝑛𝑝 2 .In 𝐺 0 , some edges have empty (𝜖) labels, e.g., parent-child edges in XML, while other edges are labeled. For uniformity, Abstra transforms 𝐺 0 into a normalized graph 𝐺, copying all the nodes of 𝐺 0 and all its 𝜖-label edges, and replacing each 𝐺 0 edge of the form</figDesc><graphic coords="3,317.96,83.69,240.23,125.26" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Sample collection graph corresponding to Figure 2.</figDesc><graphic coords="4,76.70,83.69,192.20,149.53" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Entity classification outline.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>tionships are: paper wB ---→ author, author hW ---→ paper, paper pIn ---→ conf, and conf inv --→ author.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Sample screenshots of the Abstra GUI. and use the result(s), weighted by their support among 𝐶 nodes, in a similar fashion.</figDesc><graphic coords="6,53.80,290.18,240.24,181.59" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>We compare 𝑑𝑝 𝑖 and 𝜎 𝐶,𝑑𝑝 𝑖 to each property 𝑝 ∈ P and the classes in the range of 𝑝. If the property name 𝑑𝑝 𝑖 is sufficiently similar (through word embeddings) with some property 𝑝 ∈ P, and that 𝜎 𝐶,𝑑𝑝 𝑖 is similarly sufficiently similar to a class 𝑘 𝑖 ∈ K, e.g., EmailAddress, that is in the range of 𝑝, this leads to a vote of 𝑑𝑝 𝑖 for classifying 𝐶, in every classes 𝑘 ∈ K such that 𝑘 is in the domain of 𝑝. Each property 𝑑𝑝 𝑖 may "vote" in favor of several classes, via different domain constraints; the higher the similarity between 𝑑𝑝 𝑖 and 𝑝, the more frequent 𝑑𝑝 𝑖 is on 𝐶 nodes, the fewer domain and range constraints 𝑝 has, the stronger the vote is.(2) The labels of 𝐶 nodes may also "vote" toward classifying collection 𝐶. All 𝐶 nodes may have the same label, e.g., author, which may resemble the name of a class, e.g., Author. Or, 𝐶 nodes may all have different names, e.g., RDF URIs of the form http://ns.com/Author123, from which we extract the component after the last /, eliminate all but alphabet letters,</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>https://schema.org/</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Data Profiling</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Abedjan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Golab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Naumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Papenbrock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Synthesis Lectures on Data Management</title>
		<imprint>
			<publisher>Morgan &amp; Claypool Publishers</publisher>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Graph integration of structured, semistructured and unstructured data for data journalism</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Anadiotis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Balalau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Conceicao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Galhardas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">Y</forename><surname>Haddad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Manolescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Merabti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>You</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021-07">July 2021</date>
		</imprint>
		<respStmt>
			<orgName>Information Systems</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Dbpedia: A nucleus for a web of open data</title>
		<author>
			<persName><forename type="first">S</forename><surname>Auer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Semantic Web</title>
		<meeting><address><addrLine>Berlin, Heidelberg; Berlin Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="722" to="735" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Human-in-the-loop schema inference for massive JSON datasets</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Baazizi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Berti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Colazzo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Ghelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Sartiani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EDBT</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The Berlin SPARQL benchmark</title>
		<author>
			<persName><forename type="first">C</forename><surname>Bizer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Schultz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJSWIS</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1" to="24" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The anatomy of a large-scale hypertextual web search engine</title>
		<author>
			<persName><forename type="first">S</forename><surname>Brin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Page</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Networks</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">1-7</biblScope>
			<biblScope unit="page" from="107" to="117" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Summarizing Semantic Graphs: A Survey</title>
		<author>
			<persName><forename type="first">S</forename><surname>Cebiric</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Goasdoué</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kondylakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kotzinos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Manolescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Troullinou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zneika</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The VLDB Journal</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2019-06">June 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">ConnectionLens: Finding connections across heterogeneous data sources (demonstration)</title>
		<author>
			<persName><forename type="first">C</forename><surname>Chanial</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Dziri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Galhardas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leblay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Manolescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PVLDB</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">12</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Schemas for safe and efficient XML processing</title>
		<author>
			<persName><forename type="first">D</forename><surname>Colazzo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Ghelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Sartiani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDE</title>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">RDF graph summarization for first-sight structure discovery</title>
		<author>
			<persName><forename type="first">F</forename><surname>Goasdoué</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Guzewicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Manolescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The VLDB Journal</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2020-04">Apr. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Dataguides: Enabling query formulation and optimization in semistructured databases</title>
		<author>
			<persName><forename type="first">R</forename><surname>Goldman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Widom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">VLDB</title>
		<imprint>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Lubm: A benchmark for owl knowledge base systems</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Heflin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Web Semantics</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">2-3</biblScope>
			<biblScope unit="page" from="158" to="182" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Data mining concepts and techniques</title>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kamber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<publisher>Morgan Kaufmann Publishers</publisher>
		</imprint>
	</monogr>
	<note>third edition</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Gittables: A large-scale corpus of relational tables</title>
		<author>
			<persName><forename type="first">M</forename><surname>Hulsebos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ç</forename><surname>Demiralp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Groth</surname></persName>
		</author>
		<idno>CoRR, abs/2106.07258</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Sherlock: A Deep Learning Approach to Semantic Data Type Detection</title>
		<author>
			<persName><forename type="first">M</forename><surname>Hulsebos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bakker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Zgraggen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Satyanarayan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kraska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ç</forename><surname>Demiralp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">A</forename><surname>Hidalgo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD explorations : newsletter of the Special Interest Group (SIG) on Knowledge Discovery &amp; Data Mining</title>
		<imprint>
			<date type="published" when="2019-05">May 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Schema inference for property graphs</title>
		<author>
			<persName><forename type="first">H</forename><surname>Lbath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bonifati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Harmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EDBT. OpenProceedings.org</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Database Management Systems</title>
		<author>
			<persName><forename type="first">R</forename><surname>Ramakhrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gehrke</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003">2003</date>
			<publisher>McGraw-Hill</publisher>
		</imprint>
	</monogr>
	<note>3rd edition</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Xmark: A benchmark for XML data management</title>
		<author>
			<persName><forename type="first">A</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Waas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Kersten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Carey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Manolescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Busse</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PVLDB</title>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Reducing ambiguity in JSON schema discovery</title>
		<author>
			<persName><forename type="first">W</forename><surname>Spoth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">A</forename><surname>Kennedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Hammerschmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">H</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGMOD</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
