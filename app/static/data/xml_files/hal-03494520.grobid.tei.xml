<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Discovering Useful Compact Sets of Sequential Rules in a Long Sequence</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Erwan</forename><surname>Bourrand</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Univ Rennes</orgName>
								<orgName type="institution" key="instit2">Inria</orgName>
								<orgName type="institution" key="instit3">CNRS</orgName>
								<orgName type="institution" key="instit4">IRISA</orgName>
								<address>
									<postCode>35000</postCode>
									<settlement>Rennes</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Advisor_SLA, Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Luis</forename><surname>Galárraga</surname></persName>
							<email>luis.galarraga@irisa.fr</email>
							<affiliation key="aff2">
								<orgName type="laboratory">Inria</orgName>
								<orgName type="institution" key="instit1">Univ Rennes</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
								<orgName type="institution" key="instit3">IRISA</orgName>
								<address>
									<postCode>35000</postCode>
									<settlement>Rennes</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Esther</forename><surname>Galbrun</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">University of Eastern Finland</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Elisa</forename><surname>Fromont</surname></persName>
							<email>elisa.fromont@irisa.fr</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Univ Rennes</orgName>
								<orgName type="institution" key="instit2">Inria</orgName>
								<orgName type="institution" key="instit3">CNRS</orgName>
								<orgName type="institution" key="instit4">IRISA</orgName>
								<address>
									<postCode>35000</postCode>
									<settlement>Rennes</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Alexandre</forename><surname>Termier</surname></persName>
							<email>alexandre.termier@irisa.fr</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Univ Rennes</orgName>
								<orgName type="institution" key="instit2">Inria</orgName>
								<orgName type="institution" key="instit3">CNRS</orgName>
								<orgName type="institution" key="instit4">IRISA</orgName>
								<address>
									<postCode>35000</postCode>
									<settlement>Rennes</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Discovering Useful Compact Sets of Sequential Rules in a Long Sequence</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">52401CD6F3A8E73382BFED72C40B2936</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-04-12T14:52+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We are interested in understanding the underlying generation process for long sequences of symbolic events. To do so, we propose COSSU, an algorithm to mine small and meaningful sets of sequential rules. The rules are selected using an MDL-inspired criterion that favors compactness and relies on a novel rule-based encoding scheme for sequences. Our evaluation shows that COSSU can successfully retrieve relevant sets of closed sequential rules from a long sequence. Such rules constitute an interpretable model that exhibits competitive accuracy for the tasks of next-element prediction and classification.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Long sequences of symbols are ubiquitous. Examples include DNA sequences, server logs, traces of network packets, and even long texts. Discovering regularities in such sequences allows for a better understanding of the sequence generation process, and can be useful e.g., for diagnostic and prediction.</p><p>Some models, such as LSTMs <ref type="bibr" target="#b0">[1]</ref>, excel at detecting those regularities and use them for accurate prediction. On the downside, such models are hardly understandable by human users. Models based on pattern mining, in contrast, provide highlevel human-readable descriptions of the underlying structure of the data. However, this interpretability typically comes at the cost of a restricted predictive power. We focus on the latter category of models, since we are interested in prediction but also in knowledge discovery.</p><p>In the realm of pattern mining, sequential rules are a wellestablished model for understanding and predicting sequential data. Sequential rules take the form A ⇒ C, where both the antecedent A and the consequent C are sequences of items, e.g., events, nucleotides, or words. A confidence score is often attached to the rule to measure how likely it is to observe C after A in the sequence. For instance, the rule A 1 A 2 A 3 ⇒ PowerFail with confidence 80% tell us that there is a four in five chance of undergoing a power failure after having observed the sequence of alarms A 1 , A 2 and A 3 . This can be used both for prediction, i.e., to issue an early warning, and for diagnostic, i.e., to identify the link between the alarms and the power failure.</p><p>While sequential rule mining has been extensively studied in the literature <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6]</ref>, two major issues remain. First, the vast majority of approaches consider a database of short sequences rather than a long sequence. While a long sequence can always be split into a database, this incurs a loss of information at the boundaries. Second, to the best of our knowledge, no approach for sequential rule mining addresses the so-called pattern explosion, the fact that millions of sequential rules may be produced due to the combinatorial search space.</p><p>We propose the first sequential rule mining approach that takes as input a long sequence and outputs a compact set of rules. The selection of the rules is based on the principle of Minimum Description Length (MDL), a paradigm that has proved effective for model selection in pattern mining <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8]</ref>. MDL approaches rely on an encoding scheme. Ours is presented in Section IV, along with our algorithm to mine rules, in Section V. Experiments, presented in Section VI, demonstrate the relevance of the rules found, as well as their predictive power for the tasks of next-element prediction, and classification for long sequences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. BACKGROUND</head><p>Our work stands at the crossroads of sequential and MDLbased pattern mining. Below we give brief overviews of these two fields, in turn. Sequential Pattern Mining. Mining sequential patterns on data is a well-studied problem that has given rise to a plethora of methods accounting for different notions of patterns. The vast majority of approaches assume a database of sequences as input, where sequences are ordered collections of itemsets. We refer the reader to the survey in <ref type="bibr" target="#b1">[2]</ref> for a detailed account.</p><p>Among the most recent works, Fumarola et al. <ref type="bibr" target="#b4">[5]</ref> mine frequent closed sequences, i.e. sequences of maximal length such that any extension will inevitably have lower support. Closed sequences can alleviate pattern explosion to some extent as they are a subset of frequent sequences. Other approaches <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b8">9]</ref> mine sequential rules of the form A ⇒ C where A and C may be itemsets <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b8">9]</ref> or sub-sequences <ref type="bibr" target="#b3">[4]</ref> with the constraint that C occurs after A. Itemsets are generally easier to mine than sequences, however they are less expressive because they do not account for items' order of appearance. Still, none of these methods is directly portable to long sequences as they all assume the data has been partitioned into small sequences (and hence often rely on a different definition of support). A few methods <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b9">10]</ref> can natively mine rules on long sequences. Unfortunately, such methods are not resilient to pattern explosion and are limited to association rules between itemsets, thus they do not capture the order between items within the antecedent and the consequent. Mining patterns with an MDL criterion. The principle of Minimum Description Length (MDL) <ref type="bibr" target="#b10">[11]</ref> is a criterion rooted in information theory that stipulates that the best model for a dataset is the model that compresses it best. For a dataset D and a family of models H, the best model for D, according to the (two-parts) MDL, is the one that minimizes the description length</p><formula xml:id="formula_0">H * = arg min H∈H L(H) + L(D|H) ,<label>(1)</label></formula><p>where L denotes code length in bits. Equation <ref type="bibr" target="#b0">(1)</ref> strikes a balance between model complexity, as measured by L(H), and fitness to the data, as measured by L(D|H). When applying MDL to pattern mining, a model is a collection of patterns, e.g., itemsets <ref type="bibr" target="#b6">[7]</ref>, sequences <ref type="bibr" target="#b7">[8]</ref> or, in our case, sequential rules. One of the main ingredients of any MDL-inspired approach is the encoding scheme, namely, the protocol to encode the data with the patterns and to encode the patterns themselves. Once an encoding mechanism is in place, we can generate candidate sets of patterns, calculate the corresponding code lengths, and select the set H * resulting in the shortest description length. KRIMP <ref type="bibr" target="#b6">[7]</ref> was one of the pioneering efforts to apply MDL to pattern mining, more specifically to itemset mining. Given a (large) collection of itemsets mined from the data, KRIMP selects a small representative subset. The selected itemsets were also empirically shown to be effective for classification. Recently, Fischer and Vreeken <ref type="bibr" target="#b11">[12]</ref> proposed an approach to mine compact sets of rules from data without the sequential dimension. The approaches proposed in <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14]</ref> extract sequences with gaps (but not rules) from univariate and multivariate long sequences. Unlike KRIMP <ref type="bibr" target="#b6">[7]</ref>, they combine the generation and selection of candidate patterns, which boosts efficiency and pattern quality. Shokoohi-Yekta et al. <ref type="bibr" target="#b14">[15]</ref> draw inspiration from the MDL principle to mine sequential rules from time series discretized into an integer domain. A mining approach tailored for the streaming setting is proposed in <ref type="bibr" target="#b15">[16]</ref>, but its compliance with the MDL principle is debatable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. DEFINITIONS AND NOTATION</head><p>A sequence S of length n over an alphabet Σ is an ordered collection of n occurrences of symbols from Σ. We denote by S[i] the i-th element in S (1 ≤ i ≤ n), and let S[i, j] denote the contiguous sequence S[i] . . . S[j] when 1 ≤ i ≤ j ≤ n or the empty sequence ∅ otherwise. For simplicity, we write the sequence σ 1 , σ 2 . . . σ k as σ 1 σ 2 . . . σ k . We denote the concatenation of two sequences by ⊕.</p><p>We say that sequence S is a subsequence of S (respectively S is a supersequence of S ), denoted as S S, iff there exist i and j in <ref type="bibr">[1, n]</ref> such that S = S[i, j]. Then, we call S[i, j] a match of S in S. More specifically, we denote by S i S the fact that S is a subsequence of S with a match starting at position i, i.e. S = S[i, i + |S | -1], and by S j S the fact that S is a subsequence of S with a match ending at position</p><formula xml:id="formula_1">j, i.e. S = S[j -|S | + 1, j].</formula><p>The support of a subsequence S in S, denoted by supp S (S ), is the number of distinct matches of S in S, that is, supp</p><formula xml:id="formula_2">S (S ) = |{i ∈ [1, n] : S i S}| = {j ∈ [1, n] : S j S} . Finally, a sequence S is closed in S if there is no supersequence S</formula><p>S in S such that supp S (S ) = supp S (S ).</p><p>Example 1: Sequences. Given the sequences S = abceabcadeab and S = abc, S 1 S, S 5 S, S 3 S and S 7 S are all true, hence supp S (S ) = 2.</p><p>A rule is a pair of sequences (A, C) over alphabet Σ, such that C = ∅. It is denoted as A ⇒ C. A and C are called the antecedent and the consequent of the rule, respectively. A rule such that A = ∅ and |C| = 1 is called a singleton rule. The set of singleton rules over alphabet Σ is</p><formula xml:id="formula_3">Ψ Σ = {∅ ⇒ σ : σ ∈ Σ}.</formula><p>A rule R : A ⇒ C triggers on sequence S at position i, denoted as R i S, iff the antecedent has a match in S ending at position i, i.e., iff A i S. The rule R applies on sequence S at position i, denoted by R i S, iff the antecedent has a match in S ending at position i and the consequent has a match in S starting at position i + 1, i.e. if A i S and C i+1 S.</p><formula xml:id="formula_4">Clearly, R i S implies R i S.</formula><p>The support of a rule is the number of times it applies in the sequence, whereas its confidence is the ratio of the number of times the rule applies to the number of times the rule triggers:</p><formula xml:id="formula_5">supp S (R) = |{i ∈ [1, n] : R i S}| and conf S (R) = |{i ∈ [1, n] : R i S}| |{i ∈ [1, n] : R i S}| .</formula><p>Example 2: Rules. In sequence S from Example 1, rule R : ab ⇒ c has support supp S (R) = 2 and confidence conf S (R) = 2/3, because R triggers at positions 1, 5 and 11 but only applies at positions 1 and 5.</p><p>Given a sequence S up to position m and a rule R : A ⇒ C, we say that R is active at stage j (0 ≤ j &lt; |C|) and predicts C</p><formula xml:id="formula_6">[j + 1], if A ⊕ C[1, j] = S[m -|A| -j, m]</formula><p>. This corresponds to cases where the antecedent of R is followed by the first j elements of its consequent (empty in case j = 0) in S. Formally, we define the predicates active(R, S, j), to indicate that rule R is active on sequence S at stage j, and predict(R, S, j), to return C[j + 1] if active(R, S, j) is true.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. ENCODING SCHEME</head><p>To illustrate our encoding scheme, we resort to a senderreceiver metaphor, where the sender first transmits a set of rules and their corresponding weights, followed by a sequence of code words, one for each element of the sequence, in such a way that the receiver can reconstruct the original sequence.</p><p>Encoding a Sequence via Sequential Rules. Assume we want to predict the next element after cabba, based on two rules R 1 : abba ⇒ b and R 2 : ba ⇒ cd. If the confidence of R 1 is higher than the confidence of R 2 , we would guess that b is more likely than c to appear next. We rely on this simple principle to encode a sequence element by element using a set of sequential rules. Assuming the receiver knows the set of rules and their confidence, as represented by weights, as well as the history of the sequence decoded so far, they can compute a probability distribution over the possible next elements. Hence, the sender can represent the next element as a code word chosen according to this probability distribution. If the set of rules effectively produces a probability distribution concentrated on the element that indeed occurs next, the code word for that element will be short, allowing for a concise representation of the input sequence.</p><p>Given a sequence S of m elements seen so far, and a set of weighted rules (A R ⇒ C R , w R ), such that A R , C R , and w R are respectively the antecedent, the consequent, and the weight of rule R, our goal is to compute the probability of the next element given this set of rules -denoted by R. This amounts to computing a probability distribution over Σ. For each possible next element σ ∈ Σ, we sum the weights of the active rules that predict σ and divide by the sum of weights of all active rules: Hence, the overall code length for the full sequence is</p><formula xml:id="formula_7">P R,S (σ) = (R,j)∈A S,σ w R (R,j)∈A S w R where A S = {(R, j), (R, w R ) ∈ R s.t. active(R</formula><formula xml:id="formula_8">L(S|R) = m∈[1,n] -log 2 P R,S[1,m-1] (S[m]) .<label>(2)</label></formula><p>The receiver, having the same collection of rules R and previous elements of the sequence, can perfom the same computation to dynamically reconstruct the code at his end, and decode the transmitted element, using an agreed canonical order on the alphabet to break ties if necessary.</p><p>To ensure comparability, the models must achieve lossless compression. For this reason, by construction our models always contain the singleton rule set Ψ Σ . This way, every symbol of the alphabet receives a non-zero probability of occurrence at any step, and can hence be transmitted. In fact, the singleton rule set constitutes the basis of the simplest possible model, what we call the empty model and denote by R ∅ , since it does not contain any proper rule. To each rule R σ ∈ Ψ Σ we associate a weight equal to the background probability of the predicted element, estimated as its frequency in the sequence,</p><formula xml:id="formula_9">f σ = |{i ∈ [1, n] : S[i] = σ}| /n.</formula><p>Encoding the Rules. For the receiver to have access to the rules R, the sender must transmit the antecedents, consequents, and weights at the start of the exchange. The antecedent and consequent are subsequences, hence they can be encoded simply by stating the length of the sequence then listing their elements using codes of length -log 2 (f σ ), in order. The weight, on the other hand, is a real number of bounded precision in (0, 1). Each weight is written as a finite list of decimals that we encode by applying universal coding to the integer value obtained by listing these decimals in reverse order. Put differently, we encode weight w = 0.d</p><formula xml:id="formula_10">1 d 2 d 3 ...d k with a code word of length L D (w) = L N (d k ...d 3 d 2 d 1 ). L N (z)</formula><p>is the length of the code word assigned by the universal code to integer z, which is such that L N (z) = log * 2 (z) + log 2 (c 0 ), with c 0 a constant adjusted to ensure the Kraft inequality is satisfied. This penalizes weights by the number of significant digits they contain rather than by their value. Overall, the code length for a rule R is the sum of the lengths of the code words for the antecedent, the consequent and the weight, respectively:</p><formula xml:id="formula_11">L(R) =   L N (|AR| + 1) + σ∈A R -log 2 (fσ)   +   L N (|CR|) + σ∈C R -log 2 (fσ)   + LD(wR) .</formula><p>We encode a rule table by stating the number of rules and then listing them:</p><formula xml:id="formula_12">L(R) = L N (|R|) + R∈R L(R) .<label>(3)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. THE COSSU ALGORITHM</head><p>We now introduce an algorithm to mine a set of sequential rules that compresses the input sequence well under the encoding scheme presented in Section IV. Finding such a set of rules is intractable in practice, thus we resort to heuristics.</p><p>Our COSSU (COmpact Sets of Sequential rUles) algorithm is outlined in Algorithm 1. Given a sequence S over alphabet Σ as input, COSSU returns a set of rules R in two phases. First, rule construction (lines 1-2) generates a collection of candidate rules C, which are then evaluated during rule selection (lines 3-10).</p><p>Rule Construction. COSSU starts by extracting closed frequent subsequences by applying an off-the-shelf sequence miner <ref type="bibr" target="#b16">[17]</ref> to the input sequence S with a minimum support threshold of 2. Sequential rules are then generated from the sequences by considering all possible partitionings into an antecedent and a non-empty consequent. We use the compression gain as a rough estimate of the individual ability of a rule to compress the sequence:</p><formula xml:id="formula_13">gain(R) = confS(R)•supp S (R)•L(CR)-(L(AR)+L(CR)) . (4)</formula><p>This score puts in balance the potential benefit and cost of adding the rule to the rule set. The benefit depends on how often the consequent is correctly predicted (first term), whereas the cost is the code length of the rule's antecedent and consequent (second term). The latter term constitutes a lower bound on the cost, with a weight yet undetermined. At if L(R , S) &lt; L(R, S) then</p><formula xml:id="formula_14">7: R ← R 8: for R ∈ R \ R ∅ do 9: if L(R \ {R }, S) ≤ L(R, S) then 10:</formula><p>R ← R \ {R } 11: return R the end of this first phase, the set of candidates consists of those rules that have a strictly positive compression gain.</p><p>Rule Selection. COSSU resorts to a greedy strategy to build the rule set R. Initially, R consists of the singleton rules (line 3). The algorithm then processes the candidate rules in C by order of decreasing compression gain, i.e. from most promising to least promising (line 4). The candidate rule is tentatively added to the rule set and the weights are re-adjusted (line 5). If the resulting rule set yields better compression than the current one, the current set is replaced (line 7) and the algorithm goes into a pruning loop. Otherwise, the rule is discarded and the algorithm moves on to the next candidate. The pruning loop (lines 8-10) checks whether the newly added rule makes any of the previously incorporated non-singleton rules obsolete. To do so, it tentatively removes each of the rules in turn and checks whether the compression improves as a result, in which case the rule is permanently removed from R.</p><p>Adjusting Rule Weights. Given a set of selected rules, COSSU must determine a suitable vector of weights w associated to the rules, so as to optimize the code length of both the model L(R) (complexity) and the sequence L(S|R) (fit). Because optimizing both aspects concurrently is very challenging, we assume that the code length of the weights is fixed, by fixing the floating point precision of the weights, and focus on the problem of minimizing L(S|R), i.e. solving arg min</p><formula xml:id="formula_15">w m∈[1,n] -log 2 P R,S[1,m-1] (S[m]) .</formula><p>We do so greedily, by adjusting the weight of each rule in turn, while keeping the value of the other weights fixed. The minimization resorts to the golden section search algorithm <ref type="bibr" target="#b17">[18]</ref>, initializing the weights to 1. In the end, we scale all weights to ensure that their values lie in the interval (0, 1). Designing an algorithm that is able to optimize both L(R) and L(S|R) is a major direction for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. EVALUATION</head><p>We evaluate our proposed algorithm on both synthetic and real-world datasets. For real data, we evaluate the rules learned by COSSU by using them as features in two classical machine learning tasks on long sequences: next event prediction and classification. While COSSU is not designed specifically for these tasks, it still yields acceptable performance and reports rules that capture useful patterns in the data. A full account of our experiments is available in an extended version <ref type="bibr" target="#b18">[19]</ref> of this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Evaluation on synthetic data</head><p>To study the behaviour of our algorithm in controlled settings, we generate synthetic random sequences of symbols and inject patterns that match a set of hand-crafted sequential rules. Then, we verify whether COSSU is able to recover the planted rules. We show in <ref type="bibr" target="#b18">[19]</ref> that COSSU can distinguish between regularities and background noise in long sequences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Evaluation on a prediction task</head><p>Next, we used the rules mined by COSSU to predict the next event in the sequence and showed that the achieved performance competes with interpretable methods for nextelement prediction such as bigrams and Hidden Markov Models (HMM).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Evaluation on a classification task</head><p>In this experiment, we use the following datasets: Quantified Awesome is a life log<ref type="foot" target="#foot_0">1</ref> that records the daily activities of its owner, an enthusiast of the "Quantified Self" movement, since 2011; we predict whether a sequence of events happened on a weekday or during the weekend; Presidential debates consists of 555 sentences uttered by Donald Trump and Hillary Clinton in the context of the 2016 US election; the goal is to predict the speaker given a sentence<ref type="foot" target="#foot_1">2</ref> ; Newsgroups is a dataset of 180 posts about the topics electronic and religion<ref type="foot" target="#foot_2">3</ref> ; Film critics is a dataset of 1800 movie reviews used for sentiment analysis <ref type="foot" target="#foot_3">4</ref> ; Stylometry is a database of 2000 writings from H.P. Lovecraft or E.A. Poe, where the goal is to predict the author based on its writing style <ref type="foot" target="#foot_4">5</ref> ; Stylometry (PoS) is the sequence of part-of-speech tags from the previous dataset. For all datasets (except Quantified Awesome), our alphabet Σ consists of all the words present in the corpus after removing special characters and stop words, and stemming of the remaining words. All datasets were split into training, validation, and test subsets. Experimental setup. In line with the MDL-based classifier proposed in <ref type="bibr" target="#b6">[7]</ref>, we can use COSSU to classify sequences as follows: For each class c, we build a training sequence by concatenating all the sequences labeled with c. We then extract sets of rules with COSSU on the resulting long sequence. This set of rules defines a model H c that characterizes the generation process for sequences in class c. To predict the class of an input sequence S, we compute the description length L(S|H c ) for each class c and return the class that compresses S best, i.e., the class that minimizes L(S|H c ).</p><p>We compare the COSSU classifier to (i) an SVM classifier trained on a bag-of-words representation of the text instances, (ii) an HMM-based classifier, and (iii) logistic regression trained on top of the BERT language model <ref type="bibr" target="#b19">[20]</ref>. Results. Despite not being a native classifier, COSSU achieves the best performance on three of the experimental datasets, and exhibits comparable performance to HMM and state-of-theart text classifiers. Furthermore, and unlike its competitors, COSSU is inherently interpretable because its rules serve as an explicit explanation for the outcome of classification. VII. CONCLUSION We have presented COSSU, an algorithm to mine sequential rules from a long sequence of symbolic events. Our approach is inspired by the MDL principle and retrieves a compact and relevant set of rules. Experiments on real-world data show that, in addition to providing an interpretable model for sequential data, the retrieved rules achieve competitive accuracy on the tasks of next event prediction and classification, as compared to other symbolic methods.</p><p>As future work, we would like to design a procedure to optimize the weights of the rules while dynamically adjusting their precision. We would also like to allow our method to handle gaps, perhaps by taking inspiration from <ref type="bibr" target="#b7">[8]</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>, S, j)}, and A S,σ = {(R, j) ∈ A S s.t. predict(R, S, j) = σ}. A full sequence S of length n is transmitted element by element. At each step m, we encode the next symbol S[m] with a code word chosen according to the probability assigned to it based on R and the portion of the sequence seen so far, S[1, m -1].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Algorithm 1</head><label>1</label><figDesc>COSSU: finding a compact set of sequential rules Require: A long sequence S of elements over an alphabet Σ Ensure: A compact set of sequential rules R 1: S ← MineClosedSequences(S) 2: C ← {R ∈ PrepareRules(S), R ∈ ΨΣ and gain(R) &gt; 0} 3: R ← R ∅ , with adjusted weights ˆ w = arg min w L(S|R ∅ ) 4: for R ∈ C, ordered by decreasing gain(R) do 5: R ← R ∪ {R}, with re-adjusted weights ˆ w = arg min w L(S|R ) 6:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>As anecdotal examples, we show a few rules characterizing the classes of the Stylometry dataset: E.A. Poe: rue → morge ourang → outang little old → gentleman H.P. Lovecraft: pnakotic → manuscript catch eight → clock coach arkham necronomicon mad → arab abdul alhazred</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>http://quantifiedawesome.com/records</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>https://www.kaggle.com/mrisdal/2016-us-presidential-debates</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>https://kdd.ics.uci.edu/databases/20newsgroups/20newsgroups.html</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"><p>https://is.gd/rdx39u</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4"><p>https://bit.ly/3wBqITA</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgements. The authors would like to thank <rs type="person">Nikolaj Tatti</rs> for providing a fast implementation of a closed gapless sequential pattern miner.</p></div>
			</div>			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Long Short-Term Memory</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computing</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A Survey of Sequential Pattern Mining</title>
		<author>
			<persName><forename type="first">P</forename><surname>Fournier-Viger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>-W. Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">U</forename><surname>Kiran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">S</forename><surname>Koh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Thomas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Data Science and Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="54" to="77" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Mining Association Rules in Long Sequences</title>
		<author>
			<persName><forename type="first">B</forename><surname>Cule</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Goethals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Knowledge Discovery and Data Mining</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="300" to="309" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Using Partially-Ordered Sequential Rules to Generate More Accurate Sequence Prediction</title>
		<author>
			<persName><forename type="first">P</forename><surname>Fournier-Viger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Gueniche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Tseng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advanced Data Mining and Applications</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="431" to="442" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">CloFAST: Closed Sequential Pattern Mining Using Sparse and Vertical Id-lists</title>
		<author>
			<persName><forename type="first">F</forename><surname>Fumarola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">F</forename><surname>Lanotte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ceci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Malerba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowledge and Information Systems</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="429" to="463" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">ERMiner: Sequential Rule Mining Using Equivalence Classes</title>
		<author>
			<persName><forename type="first">P</forename><surname>Fournier-Viger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Gueniche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zida</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">S</forename><surname>Tseng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Int. Symp. on Intelligent Data Analysis</title>
		<meeting>of the Int. Symp. on Intelligent Data Analysis</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="108" to="119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Krimp: Mining Itemsets that Compress</title>
		<author>
			<persName><forename type="first">J</forename><surname>Vreeken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Van Leeuwen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Siebes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Data Mining and Knowledge Discovery</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="169" to="214" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The Long and the Short of It: Summarising Event Sequences with Serial Episodes</title>
		<author>
			<persName><forename type="first">N</forename><surname>Tatti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Vreeken</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACM KDD</title>
		<meeting>of ACM KDD</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="462" to="470" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">RuleGrowth: Mining Sequential Rules Common to Several Sequences by Pattern-growth</title>
		<author>
			<persName><forename type="first">P</forename><surname>Fournier-Viger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Nkambou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">S</forename></persName>
		</author>
		<author>
			<persName><forename type="first">-M</forename><surname>Tseng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACM SAC</title>
		<meeting>of ACM SAC</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="956" to="961" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">N</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Mangal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Tiwari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mitra</surname></persName>
		</author>
		<title level="m">Mining Quantitative Association Rules in Protein Sequences</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="273" to="281" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Grünwald</surname></persName>
		</author>
		<title level="m">The Minimum Description Length Principle</title>
		<imprint>
			<publisher>The MIT Press</publisher>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Sets of Robust Rules, and How to Find Them</title>
		<author>
			<persName><forename type="first">J</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Vreeken</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ECML-PKDD</title>
		<meeting>of ECML-PKDD</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Keeping it Short and Simple: Summarising Complex Event Sequences with Multivariate Patterns</title>
		<author>
			<persName><forename type="first">R</forename><surname>Bertens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Vreeken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Siebes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACM KDD</title>
		<meeting>of ACM KDD</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="735" to="744" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Efficiently Summarising Event Sequences with Rich Interleaving Patterns</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bhattacharyya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Vreeken</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of SIAM ICDM</title>
		<meeting>of SIAM ICDM</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="795" to="803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Discovery of Meaningful Rules in Time Series</title>
		<author>
			<persName><forename type="first">M</forename><surname>Shokoohi-Yekta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Campana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zakaria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Keogh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACM KDD</title>
		<meeting>of ACM KDD</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1085" to="1094" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">SWIFT: Mining Representative Patterns from Large Event Streams</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Madden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">A</forename><surname>Rundensteiner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the VLDB Endow</title>
		<meeting>of the VLDB Endow</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="265" to="277" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Mining Closed Strict Episodes</title>
		<author>
			<persName><forename type="first">N</forename><surname>Tatti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Cule</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Data Min. Knowl. Discov</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="34" to="66" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Sequential Minimax Search for a Maximum</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kiefer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. of the American Mathematical Society</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="502" to="506" />
			<date type="published" when="1953">1953</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Discovering Useful Compact Sets of Sequential Rules in a Long Sequence</title>
		<author>
			<persName><forename type="first">E</forename><surname>Bourrand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Galárraga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Galbrun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Fromont</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Termier</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2109.07519" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</title>
		<author>
			<persName><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the NAACL Conference</title>
		<meeting>of the NAACL Conference</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
