<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Making Computer Music on the Web with JSPatcher</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Michel</forename><surname>Buffa</surname></persName>
							<email>michel.buffa@univ-cotedazur.fr</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Shanghai Conservatory of Music</orgName>
								<orgName type="institution" key="instit2">SKLMA</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">UniversitÂ e Jean Monnet</orgName>
								<address>
									<settlement>ECLLA</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="institution" key="instit1">CNRS</orgName>
								<orgName type="institution" key="instit2">INRIA</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Laurent</forename><surname>Pottier</surname></persName>
							<email>laurent.pottier@univ-st-etienne.fr</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Shanghai Conservatory of Music</orgName>
								<orgName type="institution" key="instit2">SKLMA</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">UniversitÂ e Jean Monnet</orgName>
								<address>
									<settlement>ECLLA</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="laboratory">Côte d&apos;Azur</orgName>
								<orgName type="institution">UniversitÂ e</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Shihong</forename><surname>Ren</surname></persName>
							<email>shihong.ren@univ-st-etienne.fr</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Shanghai Conservatory of Music</orgName>
								<orgName type="institution" key="instit2">SKLMA</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">UniversitÂ e Jean Monnet</orgName>
								<address>
									<settlement>ECLLA</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">UniversitÂ e Jean Monnet</orgName>
								<address>
									<settlement>ECLLA</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yang</forename><surname>Yu</surname></persName>
							<email>yuyang@shcmusic.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Shanghai Conservatory of Music</orgName>
								<orgName type="institution" key="instit2">SKLMA</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff5">
								<orgName type="institution">Shanghai Conservatory of Music</orgName>
								<address>
									<settlement>SKLMA</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yang</forename><forename type="middle">Yu</forename><surname>Making</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">UniversitÂ e Jean Monnet</orgName>
								<address>
									<settlement>ECLLA</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Making Computer Music on the Web with JSPatcher</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">6577E9912C701C034AF9426C11878113</idno>
					<note type="submission">Submitted on 13 Oct 2022</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-04-12T14:53+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract xml:lang="fr">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>niveau recherche, publiés ou non, émanant des établissements d'enseignement et de recherche français ou étrangers, des laboratoires publics ou privés.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>JSPatcher is a web-based visual programming language (VPL) originally designed for providing a user interface (UI) for Web Audio API. <ref type="bibr" target="#b0">1</ref> Since the API describes the audio processing flow as a graph of DSP nodes, it is convenient to have a patcher editing system [5] to manipulate the audio graph. JSPatcher is initially a WebAudio patcher editor that runs in a browser, where users can create boxes representing the DSP nodes and cables representing the connections in a canvas.</p><p>Since the whole JSPatcher platform is mainly developed in TypeScript and compiled to JavaScript which is the scripting language for the Web API, we have the possibility to fully import the language it self to the patcher system. Thus, in addition to the audio connection layer, in order to control DSP parameters with non-audio data, we added a dataflow layer that can distribute events between functions in real time. In this layer, various functions imported from the web environment, including JavaScript language built-ins and Web APIs, are available. These can be used to access the browser's high-level APIs, such as those for managing mouse and keyboard events, battery life or computer peripherals.</p><p>With these two layers in a single imperative patcher, the usage becomes similar to some VPLs available on native platforms like Max or PureData, while offering more flexible computational possibilities, taking advantages from the web community, as most of the JavaScript packages can be imported and used as functional boxes in a patcher. An additional patcher interpretation mode, FAUST compiled patcher, has also been developed to facilitate DSP design under the WebAudio AudioWorklet specification [6].</p><p>The FAUST [7] language ecosystem is used to transform patchers under a specific mode into FAUST code which can be compiled to a customized DSP. This kind of can be used in imperative patchers as a special WebAudio node called AudioWorkletNode that can be connected with other WebAudio nodes. With this interpreter, some Max's Gen patchers can also be interpreted to FAUST code and be compiled in the same way in JSPatcher.</p><p>These new features make JSPatcher not only a utility to interface graphs from the Web Audio API, but also an integrated development environment (IDE) for data computation and audio processing on the web. Various possibilities related to music composition and performance are thus open for exploration on the platform.</p><p>The following sections will present the improvements made to JSPatcher that contain important features for modern computer music practices, including a file manager, audio buffer manipulation, audio plugin support and computer-assisted composition functions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">FILE MANAGER</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Concept</head><p>One major limitation of the web environment is the access to the device's local files due to security reasons. According to the current web standards, web applications do not have permissions by default to read local files unless users explicitly allow them. The lack of a proper file manager API makes any web application difficult to maintain the structure of a file-based project.</p><p>To support sub-patchers and audio file manipulation in Figure <ref type="figure">1</ref>. JSPatcher with a file manager JSPatcher, we had to design a virtual file system in the browser that allows the following operations:</p><p>1. Make the file system persistent after closing the application, 2. Upload files from the user's machine, 3. Create new files, 4. Delete files, 5. Copy or move files, 6. Access file data using its path. Designing and implementing a persistent file storage system is a challenge in this work as there are few ways to keep reusable data in the browser. Our solution relies on IndexedDB,<ref type="foot" target="#foot_1">2</ref> a technology for client-side storage of significant amounts of structured data. Normally, in 2022 and on a desktop browser, up to two gigabytes of storage quota are available in the IndexedDB per site group, <ref type="bibr" target="#b2">3</ref> . It is sufficient for any lightweight project. We also use BrowserFS, a JavaScript module that can store persistently a whole structured file system into the IndexedDB and provide a JavaScript API for file management [8, 9].</p><p>To accelerate file reading and writing in runtime, we also built a higher-level file system that caches the file tree structure and data in memory. It acts as a bridge between the BrowserFS and the UI, when a change is made by the user to the file system, it calls the BrowserFS to store new change in IndexedDB; meanwhile, all event subscribers to the changed file, such as the file manager UI (Figure <ref type="figure">1</ref>), file editors or audio players will be alerted and they can react in real time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Temporary File System</head><p>Along with the persistent file system in IndexedDB, we also implemented another memory-only file system for temporary entries. It is internally a label and file content map with event emitters and an observation mechanism.</p><p>The purpose of the temporary file system is to allow users to create memory spaces and use them with an associated name just like a persistent project file. A similar behavior exists in Max. For example, when a user defines an audio buffer using a name that doesn't refer to a file from the hard disk, the system will allocate a part of the memory that is temporarily accessible with this name. Any object that uses the same name will then refer to the same part of the memory. When every object associated with this name is removed, the memory will be freed and erased.</p><p>In JSPatcher, this behavior is implemented by the temporary file system in which every file is under the root directory. We have provided an API for objects to create temporary files when a name cannot be resolved as a persistent file. Meanwhile, the object becomes an observer of the temporary file. When the last observer gets removed, the temporary file will automatically free the memory space and remove itself.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">File Structure of a Project</head><p>In JSPatcher, a project is a folder that contains subfolders or files in any format as in native operating systems. When a session is started, the project folder will automatically be initialized with a hidden file named .jspatproj under the root folder. This file contains metadata related to the project such as its name, its author, its dependencies and will be saved with the whole project.</p><p>Typically, a project will have one or multiple patchers with some asset files. To distinguish patchers under different modes, we use .jspat as the extension for regular imperative patcher, and .dsppat for FAUST compiled patcher. Patcher files can be uploaded to the file system or downloaded (exported) from it. Users can also upload or download the whole project for further exchange under the .zip format with every file compressed inside.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Usage in Patchers</head><p>Some types of project files can be recognized by JSPatcher and can be loaded into patchers. Typically, they will be used in a JavaScript patcher (using imperative and WebAudio layers) by different box objects. To load a specific file, the user should put the file path in Unix format (using slash for sub-directory) relative to the project root folder.</p><p>Table <ref type="table">1</ref> shows a list of file types that can be loaded by box objects in a patcher. Figure <ref type="figure" target="#fig_0">2</ref> shows a sub-patcher file, an audio file, a text file and an image file in a patcher.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">AUDIO EDITING</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Concept</head><p>Different kinds of web-based audio editors or digital audio workstations (DAW) have been developed by the community in recent years to bring desktop audio production experience on the web platform. However, for a long period, due to the inconsistent implementation of the Web Audio API between different web browsers, it has been hard to maintain related projects and ensure the same user experience across browsers and devices. Support for the AudioWorklet API on Safari Desktop and iOS browsers was added in April 2021, which as the final step in providing full Web Audio API coverage on all major browsers. Also, the Web Audio API version 1.0 became, after ten years of maturation, a W3C recommendation (a frozen standard) in June 2021. These two events mark a milestone of the standard and its implementation process. It motivated developers to adapt Web Audio applications to the recommended standard without worrying about compatibility issues. The AudioWorklet API is important for audio editors or DAWs as buffer recording, looping, editing regions require sample-accurate controls over the input signal and can be done in a customized DSP through the API.</p><p>In JSPatcher, using the recent API, two features related to audio files were added. Audio files can now be edited in non-real time from a single-track audio editor and can be manipulated as an audio buffer in real time in a patcher.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Audio Editor</head><p>In the workflow for some genres of electronic music, especially musique concrète or interactive electroacoustic music, raw audio materials need to be cut and ªcleaned upº before being put into a DAW project or a program. This process requires a tool that can directly modify the audio file with the following features:</p><p>1. Visualize any part of the waveform with rulers, 2. Play any part of the audio clip, 3. Select on the waveform with sample-accuracy, 4. Record audio in-place or after the clip, 5. Cut, copy and paste any part of the clip, 6. Adjust the volume of the selected section, 7. Silent the selected section or insert silence of any length, 8. Perform a phase inversion or reverse the selected section, 9. Fade-in and fade-out of any length with a flexible curve, 10. Resample the audio to any sample rate, 11. Create, delete, reorder channels, up-mix and downmix (i.e., mix a stereo audio to a mono one), 12. Apply audio effects, 13. Export the audio in some formats. Its implementation in JSPatcher is a dedicated window that can be opened by double-clicking an audio file listed in the file manager. (Figure <ref type="figure">1</ref>) When the user needs to open the editor, the system will firstly decode the given file to raw PCM data. Then, the data will be grouped into different levels of zoom that contain minimum and maximum values of each 16, 256, 4096 samples, etc. to optimize the waveform display for long audio files.</p><p>The layout of the editor's UI is similar to some desktop audio editors. A navigation bar with the waveform of the whole file is shown at the top of the window with a range of currently displayed and selected sections. Below the bar, a larger waveform viewer shows a section of the audio. Conventional features were implemented such as the cursor, the auto-scalable timing grid (vertical), energy (in dB) grid (horizontal), buttons to enable or disable channels for replay, fade-in and fade-out handlers, and an additional popup handler to adjust the gain when a section is selected.</p><p>Below the main viewer, a playback toolbar is presented. Users can play, pause or stop the playback from the cursor's position. Loop can also be enabled or disabled here. To record audio, users can choose an input device from a menu, enable the monitoring, and start recording using the red button.</p><p>At the bottom, a meter shows the current volume of the playing or the monitoring audio. A table with adjustable numbers shows the current displayed and selected section, and allows users to change it manually.</p><p>In the menu, a set of options are available to perform simple processing like silence, reverse, phase inversion, insert silence, resample, remix channels and export. To perform more complex processing using third-party DSPs, it is possible to load WebAudioModule (WAM) plugins [10, 11]  into a plugin rack. Users can choose to process the whole audio file or only the selected part. We will present the WAM support in the next sections of the paper.</p><p>In the ªRemix channelsº option, users have access in a popup window to an I/O matrix to decide the number of outputs and how much volume of signals coming from original channels for the resulting audio. (Figure <ref type="figure">3</ref>) Figure <ref type="figure">3</ref>. ªRemix channelsº and ªBounceº options</p><p>The audio export (ªBounceº option) supports .wav, .mp3 and .aac formats with different sample rates or bit-rates thanks to a web implementation of ffmpeg<ref type="foot" target="#foot_3">4</ref> through Emscripten<ref type="foot" target="#foot_4">5</ref> compilation and WebAssembly. <ref type="bibr">6 7</ref> (Figure <ref type="figure">3</ref>)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Audio Buffer Manipulation in a Patcher</head><p>Audio buffer manipulation is a key feature for any audio programming environment like Max or JSPatcher, as real time audio projects like some interactive music pieces often need to record and replay audio clips during the performance.</p><p>In the previous section, we mentioned that the bufferõ bject can refer to any audio file in the project or can create any temporary audio clip in the memory. The audio editor is also available when the user double-clicks the object. In addition, the editor can be ªdockedº beside the patcher.</p><p>The buffer~object accepts 4 possible arguments. The first argument is the identifier of the audio file or a temporary audio clip. If it is temporary, users can initialize it by specifying the buffer's number of channels, length in samples and its sample rate.</p><p>In fact, the object is associated with a PatcherAudio API in which the audio buffer is stored with its waveform data, with a set of methods related to the buffer editing. A Bang (a triggering event) from the first inlet of the buffer~object will trigger output of the PatcherAudio API.</p><p>Users can connect the buffer~object to some other objects to use the PatcherAudio API. For example, the wave-form~object can display the waveform of the audio clip, bufferSource~object is a player that wraps the Buffer-SourceNode from the Web Audio API that accepts the PatcherAudio as the input and can play the audio with loop and different playback speed.</p><p>The PatcherAudio API provides necessary information getters like its number of channels, length and sample rate; and manipulation methods like split, concatenate, pick, paste, remove, insert, etc. Figure <ref type="figure" target="#fig_1">4</ref> shows an example of picking a section (from sample 24000 to 25000) of the existing buffer with 48000 samples. Due to the limitation of the web platform, native audio plugins such as VSTs cannot be used in a web application without installing additional native software. Thus, for web-based DAWs, the need arose for a new standard of WebAudio plugins, offering similar functionality to their native counterparts.</p><p>In 2015, the first version of the Web Audio Modules (WAM) [10] standard is created, primarily for native plugins developers to port their existing plugins to the web. In 2018, they joined forces with other groups of people working on interoperable Web Audio plugins and plugin hosts to synchronize their efforts toward the beginnings of an open standard called Web Audio Plugins (WAP) [11, 12], covering a wider range of use cases. Recently, taking into account previous developments and the feedback received from developers over the past few years, a new version of the WAM standard has been created by emerging recent technological developments. Now, a WAM plugin can be fetched from the web using a URI, and be initialized using the current WebAudio context. The plugin comes with a standardized API that can create a UI, get or adjust parameters, schedule events like parameter automation or MIDI messages, and connect with other WAMs or native WebAudio nodes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">WAMs in the Audio Editor</head><p>The Audio Editor in JSPatcher has a dedicated effect ªrackº for WAMs. When the audio is loaded, the ªrackº is empty so that users can add WAMs from their URIs. The audio will be played and processed through a pre-gain, then through the ordered WAM effects, finally through a post-gain. Users can use the rack UI to add or delete WAMs or display the WAM's own UI. (Figure <ref type="figure" target="#fig_2">5</ref>) Web Audio's OfflineAudioContext is an audio context that, instead of process audio in real time, generates audio data from a WebAudio graph as fast as it can. When a user needs to export the entire audio file rendered with effects, or to apply the effects in-place, we will use a copy of the current ªrackº in a new OfflineAudioContext to render the audio. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">WAMs in Patchers</head><p>It is also important to be able to create interactive audio programs with WAMs in JSPatcher. Using the pluginõ bject, users can load WAMs via a URI into a patcher. When the plugin~receives a text string (as URI), it will remotely download the code from the URI and initialize it as a WAM. Then, it will automatically wrap the WAM and load its UI. Its inlets and outlets will be connectable with other WebAudio node objects. Additional inlets will be created to accept real time parameter change messages.</p><p>As WAMs support MIDI messages as input, the first inlet of the object accepts numbered arrays as MIDI event messages to the WAM inside. In addition, as WAMs have the ability to transmit non-audio events between each other, the patcher connection between two WAMs will be treated as a special one and make the necessary connection. Figure <ref type="figure">6</ref> is an example of a WAM loaded in a patcher with an audio player. Figure <ref type="figure">6</ref>. WAM with an audio player in a patcher</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">COMPUTER-AIDED COMPOSITION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Context</head><p>Algorithmic composition often needs computer programs to calculate its musical representation. To design such a computer-aided composition (CAC) system for composers, visual programming and musical score display are two key features. OpenMusic, developed at IRCAM, shows the power of such a VPL in algorithmic composition with its composer-oriented design. The web environment, with its strong accessibility from device to device and flexibility from the UI perspective, is already a common platform for the low-code development system. It is likely to be highly suitable for a CAC system.</p><p>High-quality digital musical scores can be dynamically rendered and displayed on a computer using the SVG (Scalable Vector Graphics) format. Libraries like VexFlow, <ref type="bibr" target="#b7">8</ref> Guido, <ref type="bibr" target="#b8">9</ref> Verovio<ref type="foot" target="#foot_9">10</ref> or abcjs <ref type="bibr" target="#b10">11</ref> can render music scores on the web. However, not all of them provide an API to interactively deal with the musical structure (model) behind the score.</p><p>The challenge we are facing is a need for a JavaScriptcompatible musical model system that is able to:</p><p>1. Calculate and compose abstract music (like a MIDI file), 2. Play via WAM synthesizers, 3. Be displayed as a musical score. Therefore, we created a JavaScript library called Sol for the calculation of different musical concepts. <ref type="bibr" target="#b11">12</ref> It aims to deal with the musical model issue in the web-based CAC system. Then, a WebAssembly version of Guido <ref type="bibr" target="#b12">13</ref> is used to render the score as it provides the AR (Abstract Representation) API to easily convert the musical model to the score. Finally, these feature are integrated as a package <ref type="bibr" target="#b13">14</ref> into JSPatcher.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Musical Model</head><p>Musical notation is basically a representation of the musical structure based on a set of different musical concepts and a composition of instances of these concepts. To facilitate the CAC, the modeling of these concepts, or generally music theory, should be implemented in a digital way to allow the calculation between the musical concepts.</p><p>Yet, modern music theory is so complex and diverse that its modeling cannot be all-inclusive. The Sol library is only a proof of concept with a covering of very basic but necessary concepts in a common CAC workflow.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">Pitch and Note</head><p>The difference between a pitch and a note can be ambiguous. In our library, ªnoteº means different ªpitch classesº in an octave. A note can be A, B, C, D, E, F and G with any number of sharp or flat accidentals.</p><p>Text strings can be used to construct a Note object. For example, ªC###º ªDbº or ªBxº are recognizable as ªnote C with 3 sharpsº ªnote D with 1 flatº and ªnote B with double sharps.º Internally, the note name and the accidentals will be preserved and can be used for further calculation. Integer numbers can also be used to construct a note as the offset in semitones from note C, in this case, the note name and the accidentals will be determined automatically.</p><p>Pitch is an extended Note object with additional octave information. It is then a more concrete concept as the frequency can be calculated. It is also possible to get a pitch from a frequency with an approximation of a semitone. The pitch's offset follows the C4 = 60 standard.</p><p>Both Note and Pitch object supports some kinds of mathematical calculations. Adding or subtracting a number from a Pitch will change by semitones its offset. The offset difference can also be calculated between two Pitches For example, the following code creates a C4 pitch, by adding 12 to get a C5 pitch, then by subtracting a C4 pitch to get 12.</p><p>const pitch = new Pitch("C4"); pitch.add (12).toString(); // =&gt; "C5" // Now pitch is C5 pitch.sub(new Pitch("C4")); // =&gt; 12</p><p>The multiplication between a pitch and a number is supported to calculate an approximated pitch based on the current pitch as the fundamental frequency and a multiplication ratio. The division is also possible between the pitch and a number or another pitch to calculate a new pitch or the frequency ratio between them. For example, the following code creates a C3 pitch, by multiplying 4 (to its frequency) to get a 2 octaves higher pitch C5, then by multiplying 1.5 to get a perfect fifth higher. const pitch = new Pitch("C3"); pitch.mul(4).toString(); // =&gt; "C5" // Now pitch is C5 pitch.mul(1.5).toString(); // =&gt; "G5"</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">Interval</head><p>Interval is the distance between two pitches, but it is more complex than just the number of semitones. With different note names and accidentals, the same distance, in equal temperament, can have different intervals such as diminished fourth and major third. Its calculation involves three properties: quality, degree and octave.</p><p>In Sol, an interval can be created with a text string, a frequency ratio, or from these properties. ªA4º ªd5-1º ªP5º ªM9º ªm10+1º are usable strings for ªaugmented fourthº ªdiminished fifth with one octave lowerº ªperfect fifthº ªmajor ninth (internally will be transformed to a major second with one octave higher)º ªmajor tenth with one octave higher (internally a major third with two octaves higher).º</p><p>Here, the distance between two pitches can be ªnegative.º In one octave, the interval between note F and note B is augmented fourth, and the interval between note B and note F will be ªdiminished fifth with one octave lower.º The operation can be executed using the following code: const b = new Note("B"); const f = new Note("F"); const i1 = f.getInterval(b); i1.toString(); // =&gt; "A4" const i2 = b.getInterval(f); i2.toString(); // =&gt; "d5-1" i1 and i2 from the code above are two intervals. They can be used for addition and subtraction from notes and pitches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.3">Chord</head><p>A chord is basically a set of pitches aligned vertically and executed at the same time. In our library, the Chord interface includes a base note or pitch and an array of intervals. It has methods such as inverse up and down, move up and down, etc. This model is chosen to easily move the chords, and also to solve the following question more easily: "How to find the missing fundamental frequency of a given chord, if any?" CAC is often used for spectral music composition algorithms, which involves the calculation of harmonics or the fundamental frequency. The missing fundamental of a sound [13] is an interesting topic in the scope. It has been demonstrated that the frequency of the pitch heard in response to a set of two or more successive harmonics corresponds to the greatest common divisor (GCD) of the harmonic set, even when there is no spectral energy at that frequency [14]. From a signal perspective, the autocorrelation algorithm can be used to find the periodicity of a sound and to determine the missing fundamental frequency. As we already have the frequencies from every pitch of the given chord, we can simply calculate the GCD of the frequencies.</p><p>However, the missing fundamental frequency should be an approximated value, as we are under the equal temperament, and human ears has a limited frequency discrimination capacity [15].</p><p>Therefore, we calculate a commonly approximated ratio between pitches from multiple ratios given by the intervals, we choose <ref type="bibr" target="#b0">1</ref> 3 of a semitone, which is nearly a 2% difference, as a threshold of the frequency discrimination and the temperament compensation.</p><p>As a result, the algorithm is able to recognize that a dominant seventh chord has a ratio of 4 : 5 : 6 : 7 and a missing fundamental on the two octaves lower dominant degree. const chord = new Chord( new Pitch("C4"), new Pitch("E4"), new Pitch("G4"), new Pitch("Bb4") ); chord.ratio; // =&gt; [4,5,6,7]  chord.phantomBase.toString(); // =&gt; "C2"</p><p>The code above calculates a dominant seventh chord on C4, it returns that its missing fundamental (phantomBase) is C2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.4">Duration</head><p>A musical value (duration) in fractions of beats can be expressed using the Duration object. Its constructor accepts a fraction (numerator and denominator) or a text string such as ª4nº ª8ndº or ª16ntº for ªa quarterº ªa dotted 8thº and ªa triplet 16th.º Mathematical operations like compare, add, subtract, multiply and division are also available. For example, we create in the following code the length of a quarter note (one beat), then multiply it by 1.5 to get a dotted quarter, divide it by 9 to get a triplet 16th, finally add a quarter to it to get a length of <ref type="bibr" target="#b6">7</ref> 24 beat. const dur = new Duration(1, 4); // 1 beat dur.mul(1.5); // 3/8 beat dur.div(9); // 1/24 beat dur.add(new Duration(1, 4)); // 7/24 beat</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.5">Sequence</head><p>Combining the presented models, a sequence can be formed with an array of chords or null values for a rest, and their duration. The sequence interface contains minimum data to render a score segment. It can also be compiled to a MIDI file with a time signature and BPM (beats per minute) information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.6">Others</head><p>Other musical concepts such as Velocity, Scale, Tonality, Track, Instrument, Genre, etc. are added to the library as well. However, these concepts are more complex and still need to be improved in future works.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Score Rendering</head><p>The Guido engine [16] is originally a C++ library aimed to compile plain text as GMN (Guido Music Notation) and to an SVG format file for the score. But it also has two intermediate steps, AR (Abstract Representation) and GR (Graphical Representation) that allow rendering a score from its API [17].</p><p>Through an Emscripten toolchain, the engine has been compiled to WebAssembly with most of its API and released as a JavaScript library at NPM (Node Packages Manager) online. To use the Sol library with the Guido API, we added a method toGuidoAR to the Note, Pitch, Chord and Sequence to perform function calls, construct the musical structure in the Guido AR.</p><p>In JSPatcher, the Guido engine runs in a separate thread using the Web Worker API. A guido.view object is available to accept a Guido AR instance and shows directly the compiled SVG score in the object UI.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Other CAC-related Objects</head><p>Manipulations of arrays (lists) of pitches, durations, velocities are common tasks in a CAC work. Like the numerous array-related functions in OpenMusic, we also need to provide them in JSPatcher.</p><p>Array versions of JavaScript operators are available as objects with the prefix ª[]º like []+, []*, []&amp;&amp;, which are array versions of ªaddº ªmultiplyº and ªlogical and.º They accept an array as the first input. When the second input is an array, values in two arrays will be applied to the function respectively according to their index; otherwise, every value in the first array will be applied to the functions with the second value. We also added some functions existing in OpenMusic like x2dx (calculate the difference between values in an array), dx2x (integrate an array), permute, combinations, arithmSer (create an arithmetic series), fiboSer (create a Fibonacci series), etc.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">EXAMPLE</head><p>The following patcher (Figure <ref type="figure" target="#fig_3">7</ref>) shows a CAC project combining the presented features. The first part of the patcher starts with a button that will create a Bang on click. The Bang generates an arithmetic series between 60 and 71, then randomly permutes the array to get the prime form of a dodecaphonic series. Its inversion is then calculated using the first value and the intervals between each value .The reversions of the two forms are calculated using a copy of the arrays. Using the Sequence and the Sequences functions, we now have a four-voice music segment. The score is displayed using the cac2guido object, and its MIDI file is generated and passed to a sequencer. Below the sequencer, a WAM synthesizer is loaded and waiting for real time MIDI messages. It produces audio data that is connected to the physical output. With the toggle, users can play or stop the sequence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">CONCLUSIONS</head><p>The scope of sound and music computing becomes wider today due to various possibilities to create music rapidly or even in real time. The development of machine learning and AI makes automatic composition possible and continuously inspires musicians to cooperate with the computer in the composition process. The accessibility offered by modern web technology with its highly active ecosystem is an opportunity for us to bring music programming and CAC systems to a new stage. For instance, an artwork created on JSPatcher can be easily shared with people by designing a presentation mode of the patcher in which only selected boxes is showing with specific position and size, and specifying in the URI the location of the project file and the patcher filename, with additional options such as hiding the editor UI or make the patcher read-only. <ref type="bibr" target="#b14">15</ref> In addition, UI components in patchers can also be interacted using touch devices which could make the design workflow simpler than existing VPLs on native platforms.</p><p>Today, audio processing in the web do have some limitations, especially the performance cost and the reliability compared to native C/C++ programs due to the implementation of different browsers, i.e. lack of optimization for multi-core CPUs. Even though, the web platform is still attractive to both developers and users. It has minimized the barrier for the deployment and use of any computer program from any device.</p><p>The described additions and improvements on JSPatcher are just the first steps towards a complete online IDE for sound and music computing. Feedbacks given by musicians from the community shows high interest in the future possibilities of JSPatcher such as supports for INScore, <ref type="bibr" target="#b15">16</ref> collaborative editing and messaging via the network. As the project is open-sourced <ref type="bibr" target="#b16">17</ref> with the examples and SDK (Software development kit) provided, contributions from third-party are feasible and welcome.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Files in a patcher</figDesc><graphic coords="3,309.61,597.35,223.16,137.29" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Audio buffer manipulation</figDesc><graphic coords="5,331.93,85.59,178.53,94.22" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. WAM plugins in the audio editor</figDesc><graphic coords="6,64.69,164.58,223.15,127.68" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 7 .</head><label>7</label><figDesc>Figure 7. An example of CAC with a MIDI player</figDesc><graphic coords="8,309.61,238.87,223.16,198.24" type="bitmap" /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>Proceedings of the 19th Sound and Music Computing Conference,June 5-12th, 2022, Saint-Étienne (France)   </p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>https://www.w3.org/TR/IndexedDB/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>https://tinyurl.com/muc8cmyx</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"><p>https://www.ffmpeg.org/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4"><p>https://emscripten.org/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5"><p>https://webassembly.org/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_6"><p>https://github.com/ffmpegwasm/ffmpeg.wasm</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_7"><p>https://www.vexflow.com/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_8"><p>https://guido.grame.fr/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10" xml:id="foot_9"><p>https://www.verovio.org/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="11" xml:id="foot_10"><p>https://www.abcjs.net/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="12" xml:id="foot_11"><p>Open-sourced on https://github.com/fr0stbyter/sol</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="13" xml:id="foot_12"><p>https://github.com/grame-cncm/guidolib</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="14" xml:id="foot_13"><p>https://github.com/jspatcher/package-cac Proceedings of the 19th Sound and Music Computing Conference, June 5-12th, 2022, Saint-Étienne (France)</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">ªBuild webaudio and javascript web applications using jspatcher: A webbased visual programming</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Pottier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Buffa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Web Audio Conference</title>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Correya</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Ramires</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Bogdanov</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">X</forename><surname>Faraldo</surname></persName>
		</editor>
		<editor>
			<persName><surname>Favory</surname></persName>
		</editor>
		<meeting>the International Web Audio Conference<address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<publisher>UPF</publisher>
			<date type="published" when="2021-07">July 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Puckette</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">E A</forename><surname>Zicarelli</surname></persName>
		</author>
		<title level="m">ªMax/msp,º Cycling</title>
		<imprint>
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">ªPure Data</title>
		<author>
			<persName><forename type="first">M</forename><surname>Puckette</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Computer Music Conference</title>
		<meeting>the International Computer Music Conference<address><addrLine>Thessaloniki, Hellas</addrLine></address></meeting>
		<imprint>
			<publisher>The International Computer Music Association</publisher>
			<date type="published" when="1997-09">Sep. 1997</date>
			<biblScope unit="page" from="224" to="227" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">ªOpenmusic: Visual programming environment for music composition, analysis and research</title>
		<author>
			<persName><forename type="first">J</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Agon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Assayag</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th ACM International Conference on Multimedia, ser. MM &apos;11</title>
		<meeting>the 19th ACM International Conference on Multimedia, ser. MM &apos;11<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="743" to="746" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Puckette</surname></persName>
		</author>
		<ptr target="https://fr0stbyter.github.io/jspatcher/dist/?projectZip=../../soundcraft/Soundcraft6.zip&amp;file=020.jspat&amp;runtime=116https://inscore.grame.fr/17https://github.com/Fr0stbyteR/jspatchercisco" />
		<title level="m">ªThe patcher,º in Proceedings of the International Computer Music Conference. San Fran-15 The</title>
		<imprint>
			<publisher>Computer Music Association</publisher>
			<date type="published" when="1986">1986</date>
			<biblScope unit="page" from="420" to="429" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">ªAudioworklet: the future of web audio</title>
		<author>
			<persName><forename type="first">H</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Computer Music Conference</title>
		<meeting>the International Computer Music Conference<address><addrLine>Daegu, South Korea</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-08">Aug. 2018</date>
			<biblScope unit="page" from="110" to="116" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">ªFAUST : an Efficient Functional Approach to DSP Programming</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Orlarey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Fober</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Letz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">New Computational Paradigms for Computer Music</title>
		<meeting><address><addrLine>E. D. France; Paris, France</addrLine></address></meeting>
		<imprint>
			<publisher>Delatour</publisher>
			<date type="published" when="2009-01">Jan. 2009</date>
			<biblScope unit="page" from="65" to="96" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">ªDoppio: Breaking the Browser Language Barrier</title>
		<author>
			<persName><forename type="first">J</forename><surname>Vilk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">D</forename><surname>Berger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th ACM SIGPLAN Conference on Programming Language Design and Implementation</title>
		<meeting>the 35th ACM SIGPLAN Conference on Programming Language Design and Implementation</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="508" to="518" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">ªBrowsix: Bridging the Gap Between Unix and the Browser,º in Proceedings of the Twenty-Second International Confer</title>
		<author>
			<persName><forename type="first">B</forename><surname>Powers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Vilk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">D</forename><surname>Berger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Architectural Support for Programming Languages and Operating Systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="253" to="266" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">ªWeb audio modules</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kleimola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Larkin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Sound and Music Computing Conference</title>
		<meeting>the Sound and Music Computing Conference</meeting>
		<imprint>
			<date type="published" when="2015-07">Jul. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">ªTowards an open web audio plugin standard</title>
		<author>
			<persName><forename type="first">M</forename><surname>Buffa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lebrun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kleimola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Larkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Letz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Companion Proceedings of the The Web Conference 2018</title>
		<meeting><address><addrLine>Lyon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-04">Apr. 2018</date>
			<biblScope unit="page" from="759" to="766" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">ªEmerging w3c apis opened up commercial opportunities for computer music applications</title>
		<author>
			<persName><forename type="first">M</forename><surname>Buffa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lebrun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Letz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Orlarey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Michon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Fober</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">º in The Web Conference 2020 DevTrack</title>
		<meeting><address><addrLine>Taipei</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-04">Apr. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">ªBeobachtungen È uber einige bedingungen der entstehung von tÈ onen</title>
		<author>
			<persName><forename type="first">A</forename><surname>Seebeck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">º Annalen der Physik</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="417" to="436" />
			<date type="published" when="1841">1841</date>
		</imprint>
	</monogr>
	<note>nd ser.</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">ªPitch is determined by naturally occurring periodic sounds</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Purves</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">º Hearing Research</title>
		<imprint>
			<biblScope unit="volume">194</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="31" to="46" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">ªFrequency difference limens for shortduration tones</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">C J</forename><surname>Moore</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">º The Journal of the Acoustical Society of America</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="610" to="619" />
			<date type="published" when="1973">1973</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">ªThe GUIDO notation format: A novel approach for adequately representing score-level music,º in Proceedings of the 1998</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">H</forename><surname>Hoos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hamel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Renz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kilian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Computer Music Conference, ICMC 1998</title>
		<meeting><address><addrLine>Ann Arbor, Michigan, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Michigan Publishing</publisher>
			<date type="published" when="1998">October 1-6, 1998. 1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">ªScores level composition based on the GUIDO music notation</title>
		<author>
			<persName><forename type="first">D</forename><surname>Fober</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Orlarey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Letz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Non-Cochlear Sound: Proceedings of the 38th International Computer Music Conference, ICMC 2012</title>
		<meeting><address><addrLine>Ljubljana, Slovenia</addrLine></address></meeting>
		<imprint>
			<publisher>Michigan Publishing</publisher>
			<date type="published" when="2012">September 9-14, 2012. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
