<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="fr">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Queries with Arithmetic on Incomplete Databases</title>
				<funder>
					<orgName type="full">Foundation Sciences Mathématiques de Paris</orgName>
				</funder>
				<funder ref="#_g7gVDPF #_rwZzDUX #_Mk8ktV6">
					<orgName type="full">EPSRC</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Marco</forename><surname>Console</surname></persName>
							<email>mconsole@inf.ed.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Edinburgh</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Edinburgh</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Matthias</forename><surname>Hofer</surname></persName>
							<email>mhofer@inf.ed.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Edinburgh</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">University of Edinburgh</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Leonid</forename><surname>Libkin</surname></persName>
							<email>libkin@inf.ed.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Edinburgh</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">ENS-Paris/PSL</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Queries with Arithmetic on Incomplete Databases</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">5145B34743528889471303733FE838F6</idno>
					<idno type="DOI">10.1145/3375395.3387666</idno>
					<note type="submission">Submitted on 1 Feb 2021</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-04-12T14:50+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Theory of computation → Incomplete, inconsistent, and uncertain databases</term>
					<term>Complexity theory and logic</term>
					<term>Stochastic approximation</term>
					<term>• Information systems → Incomplete data</term>
					<term>KEYWORDS incomplete information, numerical data, missing data, query answering, measure of certainty, first-order queries, approximate query answers, asymptotic behavior</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>published or not. The documents may come from teaching and research institutions in France or abroad, or from public or private research centers. L'archive ouverte pluridisciplinaire HAL, est destinée au dépôt et à la diffusion de documents scientifiques de niveau recherche, publiés ou non, émanant des établissements d'enseignement et de recherche français ou étrangers, des laboratoires publics ou privés.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="fr">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Handling incomplete or uncertain information is one of the central topics in databases, as well as in multiple applications of databases where uncertainty naturally arises. These applications include data integration <ref type="bibr" target="#b24">[25]</ref>, data exchange <ref type="bibr" target="#b4">[5]</ref>, ontology-based data access <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b29">30]</ref>, consistent query answering <ref type="bibr" target="#b5">[6]</ref>, probabilistic databases <ref type="bibr" target="#b30">[31]</ref>, and others. The standard approach to query answering in all of these is to find answers with some certainty guarantees <ref type="bibr" target="#b18">[19]</ref>. Most commonly, one looks for certain answers that are true under all possible interpretations of missing data, although weaker guarantees are possible. In the probabilistic model, for example, we can talk about the probability of a tuple being an answer <ref type="bibr" target="#b30">[31]</ref>, and can even do this in the absence of explicit distributions <ref type="bibr" target="#b26">[27]</ref>.</p><p>There is one element common to all these approaches. They all start by the standard assumption prevalent in the database literature, which essentially says: assume that there is one domain of values 𝑈 from which all database elements are drawn. In reality however database columns are typed, and one would need to look hard for a database that does not have numerical columns and arithmetic operations in queries. For example, in the TPC-H benchmark <ref type="bibr" target="#b31">[32]</ref>, 7 out of 8 tables have numerical attributes, while over 85% of queries use at least order comparisons, and over 70% of queries use standard arithmetic operations such as addition and multiplication. Modeling extra types theoretically is done by viewing relational databases as many-sorted structures. Usually this is a mild assumption for theoretical work, unless there are specific interpreted operations on different types, such as for instance +, •, &lt; on numerical types.</p><p>The expressive power and complexity of such languages was extensively studied in the 1990s; see <ref type="bibr" target="#b23">[24]</ref> for a summary. For handling incomplete information, however, very little is known. We do know however that the problem becomes considerably harder. For example, under the single-domain assumption, unions of conjunctive queries on databases with nulls can be evaluated efficiently if we only compare values for equality <ref type="bibr" target="#b18">[19]</ref>, but already in the presence of order comparisons data complexity jumps to coNP, cf. <ref type="bibr" target="#b0">[1]</ref>. With more complicated arithmetic, the complexity may jump even higher, all the way to undecidability (we shall give an example later).</p><p>At this point we should ask ourselves: what kind of query answers we would actually want in such scenarios. Look at a very simple example: a query 𝜎 𝐴&gt;𝐵 (𝑅) on relation 𝑅 with attributes 𝐴 and 𝐵 and a single tuple (⊥ 1 , ⊥ 2 ) with two nulls. Should the tuple be selected? If we know nothing about ⊥ 1 and ⊥ 2 , it seems reasonable to say that with probability 1/2 the tuple will be in the answer. We now give an example showing the utility of such answers.</p><p>Example. A team of sale analysts is asked to predict the effectiveness of the forthcoming sales campaign using data from a database with three relations:</p><p>• Products(id, seg, rrp, dis) has product ids, their market segment, recommended retail price (rrp), and intended discount dis; • Competition(id, seg, p) contains information about competing products on the market including their ids, market segment, and offered price p; • Excluded(id, seg) has products excluded from the campaign.</p><p>As often happens in any business environment, information in these tables may be incomplete while the details of the campaign are being worked out. Furthermore, Competition is likely to be populated by an (automated) web extraction algorithm, leading to a high chance of incomplete data in that relation. A sales analyst may want to compile a list of market segments where the company will have a strong competitive advantage, i.e., the price, after discount, of all the products in that segment is below the competition. This can be done with the following query, written in first-order (FO) notation: 𝑞(𝑠) = ∀𝑖, 𝑟, 𝑑, 𝑖 ′ 𝑝 P(𝑖, 𝑠, 𝑟, 𝑑) ∧ ¬E(𝑖, 𝑠) ∧ C(𝑖 ′ , 𝑠, 𝑝)) → ((𝑟 • 𝑑 ≤ 𝑝) ∧ (𝑟, 𝑑, 𝑝 ≥ 0))</p><p>Now consider a database 𝐷 in which Competition has tuple (𝑐, 𝑠, ⊥), where ⊥ indicates a null, Products has tuples (𝑖𝑑 1 , 𝑠, 10, 0.8) and (𝑖𝑑 2 , 𝑠, ⊥ ′ , 0.7), where again ⊥ ′ indicates a null (here we use the model of marked or labeled nulls), and Excluded has a tuple (⊥ ′′ , 𝑠) meaning that some product from the segment is excluded from the campaign but we do not yet know which one. We want to estimate how likely it is that 𝑠 is returned as the answer to 𝑞 over 𝐷. This will happen iff ⊥, ⊥ ′ are interpreted as numbers 𝛼, 𝛼 ′ and ⊥ ′′ as some value 𝑢 such that 𝑢 ≠ 𝑖𝑑 2 and (𝛼 ′ ≥ 0) ∧ (𝛼 ≥ 8) ∧ (0.7 • 𝛼 ′ ≥ 𝛼) .</p><p>(1) Thus, 𝑠 is not a certain answer to 𝑞 over 𝐷, but it is an answer under conditions (1) and we thus would want to estimate the chance of these conditions being true. If we know nothing at all about the values that ⊥, ⊥ ′ , ⊥ ′′ can take, we assume that any value is equally likely. Then, it is rather unlikely that 𝑖𝑑 2 = 𝑢 and we can dismiss that constraint. For (1) it seems reasonable then to say: pick 𝛼, 𝛼 ′ uniformly at random, and compute the probability of (1). The problem is that, of course, we cannot sample from all of R (or Z for that matter) uniformly, so we need to restrict the range of numbers to do so. We look then at vectors (𝛼, 𝛼 ′ ) whose length is at most 𝑟 , and estimate the fraction of the area that constraint (1) covers among all vectors of such length. This is shown in the picture below The fraction depends on 𝑟 , but as 𝑟 increases, one can calculate that it tends to ≈ 0.388 of the positive quadrant, which can be taken as our estimate. Putting a higher discount (e.g., changing 0.7 to 0.5) results in approximately half of the positive quadrant satisfying the constraint, thus increasing our confidence in 𝑠 being an answer. □ Our goal is to provide a framework in which we can reason about answers in the same manner as in the example. Note that above we did not make any assumptions about probability distributions on data stored in numerical attributes. That is a deliberate assumption, for this initial study. We would like the framework to be general enough to accommodate additional information such as distributions or constraints on data. But for now we follow what most of the work on usual incomplete data in relational databases does. There, the standard assumption is that we have a domain of values, and every null can be interpreted as any of the elements of the domain, cf. <ref type="bibr" target="#b18">[19]</ref>. We take it further by assuming that we now have a second numerical domain, and specifying that some columns take values in that numerical domain. This is similar to stating attribute types as int or float in create table statements. Then we let nulls in numerical columns be interpreted as arbitrary elements of the numerical domain.</p><p>Before outlining our contributions, we remark that dealing with missing numerical data is common in data analysis, and typically is dealt with via imputation, i.e., finding suitable replacements for missing data, cf. <ref type="bibr" target="#b13">[14]</ref>. There are many variations of the approach that go from simple discarding of tuples with missing values to the inference of missing values through complex statistical models. Crucially though once the imputation is done, the data set is treated as complete, and if it resides in a database, it will be queries using standard techniques for querying complete data. Query answers then will have no way of referring to the fact that some data was initially missing. Our goal is different though: we want to query an incomplete database as given to us, with the full knowledge that it is incomplete, and provide the user with the additional information about confidence levels for potential query answers. Moreover, we want to do it even in the case when there is no prior information about data, such as probability distribution.</p><p>Our main contributions are as follows.</p><p>(1) We define a model of relational databases with attributes of two distinct types -a base type and a numerical type -with nulls that may occur as values of attributes of both types. We use, as a query language, an extension of relational calculus (FO) with arithmetic operations and order comparisons. We define a measure 𝜇 (𝑞, 𝐷, ā) of certainty of a tuple ā as an answer to query 𝑞 on database 𝐷. It takes values in the interval [0, 1]. Value 1 means that the answer is almost surely certain (essentially being a correct answer with probability 1). ( <ref type="formula" target="#formula_8">2</ref>) The measure 𝜇 relies on computing volumes of sets given by arithmetic constraints, and analyzing their asymptotic behavior. Indeed, we cannot have a uniform distribution over the entire infinite numerical domain, so instead we first analyze all numerical constraints under a bound that all values are bounded by some number 𝑟 , but then, to avoid dependence on an ad hoc bound, we look at the limit behavior as 𝑟 grows. This approach poses the question whether the measure is well-defined (i.e., all volumes exist, and limit exists). We prove that it is indeed well-defined. (3) We outline two approaches to computing the measure 𝜇 (𝑞, 𝐷, ā): finding the exact value, and finding its approximation. The former could be computationally hard even for conjunctive queries with inequalities, which is not surprising in view of known complexity results. But with more complex constraints (e.g., linear constraints), values of 𝜇 could be irrational, so one needs to approximate. (4) We then look at the common way of finding approximations, namely constructing an FPRAS (fully polynomial-time randomized approximation scheme), and prove that it exists for conjunctive queries with linear constraints but does not exist for FO queries even with order constraints. <ref type="bibr" target="#b4">(5)</ref> We then notice that values of 𝜇 are in the interval [0, 1], and thus it is reasonable to look at slightly weaker approximations 𝑎 of 𝜇 so that |𝜇 -𝑎| &lt; 𝜖 for some small 𝜖 (under FPRAS, the condition is |𝑎/𝜇 -1| &lt; 𝜖). Then we prove the existence of such a polynomial-time approximation scheme for all FO queries, with the usual arithmetic (addition, multiplication, order comparisons). <ref type="bibr" target="#b5">(6)</ref> We not only prove the theoretical bounds but also conduct an experimental study to confirm the feasibility of the approach. We look at some decision support queries in an example similar to the one shown earlier, and implement the approximation algorithm for finding confidence levels for candidate tuple answers. We show that its performance is adequate, essentially a small number of seconds with 𝜖 ranging from 0.1 to 0.01.</p><p>Organization In Section 2 we define basic notations related to databases with nulls. In Section 3 we present the model of databases with numerical columns, and in Section 4 we define the measure of certainty 𝜇. We prove that it is well-defined in 5. In Section 6 we outline approaches to computing and approximating 𝜇. In Section 7 we present an FPRAS for conjunctive queries with linear constraints, and in Section 8 we give an approximation scheme with additive guarantees for all FO queries. The results of the experimental study are in Section 9, and conclusions are in Section 10. Due to space limitations, detailed proofs are in the appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">PRELIMINARIES</head><p>We now briefly recall the standard model of incomplete information, where incompleteness is represented by nulls, and query answering in this model. In this model, elements of the database come either from a domain C, often viewed as the domain of constants, or known values. They may also come from a domain N of nulls, which are viewed as currently unknown values. A database schema is a set of relation names and their arities. In a database 𝐷, a 𝑘-ary relation 𝑅 from the schema is interpreted as 𝑘-ary relation over C ∪ N, i.e., a finite set 𝑅 𝐷 ⊆ (C ∪ N) 𝑘 . Often 𝐷 is clear from the context, and we then write 𝑅 in place of 𝑅 𝐷 . We write C(𝐷) and N(𝐷) for the set of all elements of C and N that appear in relations of 𝐷.</p><p>Incomplete databases are interpreted by means of valuations. A valuation is a map 𝑣 : N(𝐷) → C that interprets nulls by constants. Thus, an incomplete database 𝐷 represents complete (i.e., having no nulls) databases 𝑣 (𝐷), where 𝑣 ranges over valuations, and 𝑣 (𝐷) is the result of replacing each null ⊥ ∈ N(𝐷) with 𝑣 (⊥).</p><p>Suppose 𝑞 is a 𝑛-ary query that, on each complete database 𝐷 returns 𝑞(𝐷) ⊆ C(𝐷) 𝑛 . The standard notion of query answering is that of certain answers <ref type="bibr" target="#b18">[19]</ref>, i.e., answers independent of the valuation of nulls. For a database 𝐷 that now may have nulls, an 𝑛-tuple ā over C(𝐷) ∪ N(𝐷) is a certain answer if 𝑣 ( ā) ∈ 𝑞 𝑣 (𝐷) for each valuation 𝑣 . This is the definition from <ref type="bibr" target="#b27">[28]</ref>. One typically sees a restricted form of it when ā has no nulls; then it says ā ∈ 𝑞 𝑣 (𝐷) , see <ref type="bibr" target="#b18">[19]</ref>. We use the more permissive definition of <ref type="bibr" target="#b27">[28]</ref>, as it allows tuples with nulls to be in the answer. Its advantages are explained, for example, in <ref type="bibr" target="#b25">[26]</ref>. As a simple example, if we have a relation 𝑅 with a single tuple (1, ⊥), and a query returning 𝑅, then the definition of <ref type="bibr" target="#b18">[19]</ref> will return ∅, forgetting certain information that we have one tuple in 𝑅, and its first component is 1. The definition of <ref type="bibr" target="#b27">[28]</ref> that we use here will, on the other hand, return (1, ⊥).</p><p>It is well-known that certainty comes at a computational price: while for conjunctive queries and their unions and some small extensions with restricted negation they can be computed efficiently <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b18">19]</ref>, for first-order queries (or full relational algebra queries) finding certain answers is coNP-hard in data complexity. This cost however can be significantly reduced if we are only interested in almost certain answers, using a simple idea from <ref type="bibr" target="#b26">[27]</ref>.</p><p>Assume the domain of constants comes with some enumeration, C = {𝑐 1 , 𝑐 2 , 𝑐 3 , . . .}. For a valuation 𝑣 : N(𝐷) → C, define</p><formula xml:id="formula_0">∥𝑣 ∥ = max{𝑟 | 𝑐 𝑟 is in the range of 𝑣 } .</formula><p>Next, for a query 𝑞 and a tuple ā, define the 𝑟 -support of a tuple ā as an answer to 𝑞 as the set of valuations 𝑣 with ∥𝑣 ∥ ≤ 𝑟 such that 𝑣 ( ā) ∈ 𝑞 𝑣 (𝐷) , i.e.,</p><formula xml:id="formula_1">Supp 𝑟 (𝑞, 𝐷, ā) = {𝑣 | 𝑣 ( ā) ∈ 𝑞 𝑣 (𝐷) and ∥𝑣 ∥ ≤ 𝑟 } .</formula><p>We then take 𝜇 𝑟 (𝐷, 𝑞, ā) to be the probability that a valuation 𝑣 with ∥𝑣 ∥ ≤ 𝑟 picked uniformly at random is in Supp 𝑟 (𝑞, 𝐷, ā). As there are only 𝑟 |N(𝐷) | valuations 𝑣 with ∥𝑣 ∥ ≤ 𝑟 on a database 𝐷, this is well defined. Finally, to get rid of the dependence on 𝑟 , one simply looks at the limit as 𝑟 grows and thus the set of valuations with ∥𝑣 ∥ ≤ 𝑟 becomes close to the set of all valuations; that is, 𝜇 (𝑞, 𝐷, ā) = lim 𝑟 →∞ 𝜇 𝑟 (𝑞, 𝐷, ā).</p><p>Then, <ref type="bibr" target="#b26">[27]</ref> established the following results:</p><p>• The value of 𝜇 (𝑞, 𝐷, ā) does not depend on a particular enumeration of C. • For a large class of queries the value of 𝜇 (𝑞, 𝐷, ā) is either 0 or 1. These queries are generic, i.e., commuting with permutations of their domain. All queries in languages such as FO, relational algebra, datalog, etc., are generic.</p><p>• 𝜇 (𝑞, 𝐷, ā) = 1 iff ā is returned by the naïve evaluation of 𝑞 on 𝐷, i.e., treating nulls as new distinct constants, different from all the elements of C(𝐷). Thus, a straightforward evaluation of a query on a database with nulls results in answers that are almost certainly true. This happens though in the absence of constraints on database entries, such as their types (e.g., numbers), and in the absence of type-specific operations in queries (e.g., addition and multiplication).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">DATA MODEL AND QUERY LANGUAGES</head><p>We now move to a model where different columns in relations can be of different types. This is a fairly standard multi-sorted model where we allow for nulls as entries.</p><p>We assume that database columns can be of two types: either a base type, denoted by base, which corresponds to the usual onedomain assumption in the database literature, or a numerical type, denoted by num. The domain of base type is denoted by C base and the domain of the numerical type by C num . Thus, what we previously viewed as the set of constants C is now a disjoint union of C base and C num . We assume that the domain of the numerical type C num is a subset of the real numbers R.</p><p>A schema then, instead of simply specifying the arity of each relation, specifies types of columns. We assume that such specifications are of the form 𝑅(base 𝑘 num 𝑚 ) saying that the first 𝑘 columns of 𝑅 are of base type, and the remaining 𝑚 are of the numerical type. Such columns, as declared in any real-life DDL, can be interspersed, but here we assume, only for the simplicity of the notation, that all base type columns come first.</p><p>Each column, whether of base type or numerical, may contain null values. Using the same marked nulls model, we assume two sets of nulls: N base = {⊥ 1 , . . .} of nulls that occur in base type columns, and N num = {⊤ 1 , ⊤ 2 , . . .} of numerical type nulls. Then a relation of type 𝑅(base 𝑘 num 𝑚 ) in a database 𝐷 is interpreted as a finite set</p><formula xml:id="formula_2">𝑅 𝐷 ⊂ (C base ∪ N base ) 𝑘 × (C num ∪ N num ) 𝑚 .</formula><p>That is, entries of each base type column come from C base ∪ N base and entries of each numerical type column come from C num ∪N num . We use the same notations to denote sets of elements of each kind in a database 𝐷: for example, C base (𝐷) is the set of all constants of base type that occur in 𝐷, and N num (𝐷) is the set of all nulls in numerical columns that occur in 𝐷.</p><p>Query languages. As our basic query language, we consider twosorted first-order logic with arithmetic, FO(+, •, &lt;). In this logic, every variable is typed (i.e., of base or numerical type). Its terms and formulae are defined inductively as follows.</p><p>Terms • a variable of a base type is a base type term;</p><p>• a variable of a numerical type is a numerical type term;</p><p>• every element 𝛼 ∈ C num is a numerical type term;</p><p>• if 𝑡 and 𝑡 ′ are numerical type terms, then so are 𝑡 + 𝑡 ′ and 𝑡 • 𝑡 ′ . Atomic formulae • for a relation 𝑅(base 𝑘 num 𝑚 ), if x is a 𝑘-tuple of base type variables and t is an 𝑚-tuple of numerical type terms, then 𝑅( x, t) is an atomic formula; • if 𝑥 and 𝑦 are variables of the base type, then 𝑥 = 𝑦 is an atomic formula;</p><p>• if 𝑡, 𝑡 ′ are numerical type terms, then 𝑡 &lt; 𝑡 ′ and 𝑡 = 𝑡 ′ are atomic formulae. Formulae are closed under Boolean connectives and quantifiers:</p><p>• If 𝜑,𝜓 are formulae then 𝜑 ∨𝜓, 𝜑 ∧𝜓 and ¬𝜑 are formulae;</p><p>• if 𝜑 is a formula then ∃𝑥 𝜑 and ∀𝑥 𝜑 are formulae.</p><p>We shall use the standard shortcuts, i.e., 𝑥 ≤ 𝑦 for 𝑥 &lt; 𝑦 ∨ 𝑥 = 𝑦 or 2𝑥 and 𝑥 2 for 𝑥 + 𝑥 and 𝑥 • 𝑥, etc. Since atomic formulae are of the form 𝑡 = 𝑡 ′ or 𝑡 &lt; 𝑡 ′ , we can assume that operationsand ÷ are allowed for building terms as well.</p><p>The notion of free variables and the semantics are standard. When queries are interpreted over complete databases 𝐷, quantifiers range over their domain. That is, ∃𝑥 means that a witness is found among elements of C base (𝐷) if 𝑥 is of base type, and among elements of C num (𝐷) if 𝑥 is of numerical type. For example, ∃𝑦, 𝑧 𝑅(𝑥, 𝑦) ∧ 𝑆 (𝑦, 𝑧) ∧ 𝑦 &gt; 𝑧 2 for relations 𝑅(base num) and 𝑆 (num 2 ) finds all 𝑥 of base type such that there are tuples (𝑥, 𝑦) ∈ 𝑅 and (𝑦, 𝑧) ∈ 𝑆 with 𝑦 &gt; 𝑧 2 .</p><p>Sublanguages. If we restrict the use of arithmetic, we explicitly indicate so; for example, FO(+, &lt;) means that arithmetic terms are linear functions, built with +, and comparisons 𝑡 &lt; 𝑡 ′ are allowed. Notice that with +, we also have subtraction: for example,</p><formula xml:id="formula_3">𝑡 1 -𝑡 2 &lt; 𝑡 3 is 𝑡 1 &lt; 𝑡 2 + 𝑡 3 .</formula><p>We shall refer to the ∃, ∧-fragment of the language as conjunctive queries and write CQ( ) listing allowed operations in parentheses. For example, CQ(&lt;) refers to conjunctive queries extended with order comparisons.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">QUERY ANSWERS: MEASURE OF CERTAINTY</head><p>We now describe the approach to query answering in the presence of arithmetic and nulls. As the first step, we need to provide the semantics of incomplete databases over two types. Recall that under the single-type assumption, the semantics was given by valuations, which are functions from nulls to constants C. Now that the set of constants is two-sorted, such valuations on an incomplete database 𝐷 are pairs 𝑣 = (𝑣 base , 𝑣 num ) where 𝑣 base : N base (𝐷) → C base and 𝑣 num : N num (𝐷) → C num . The database 𝑣 (𝐷) is, as before, obtained by substituting each null ⊥ ∈ N base (𝐷) with 𝑣 base (⊥) and each null ⊤ ∈ N num (𝐷) with 𝑣 num (⊤). Likewise, when we have a tuple ā, we write 𝑣 ( ā) to denote a tuple obtained from ā by replacing base type nulls according to 𝑣 base , numerical type nulls according to 𝑣 num , and leaving constants intact. For example,</p><formula xml:id="formula_4">𝑣 (⊥, ⊤, 2) = (𝑣 base (⊥), 𝑣 num (⊤), 2).</formula><p>Of course with this, we could define certain answers as before: a tuple ā is a certain answer to query 𝑞 if 𝑣 ( ā) ∈ 𝑞 𝑣 (𝐷) for each valuation 𝑣. However, it is well known that even adding simple order comparisons &lt; already leads to much higher complexity of query answering. For example, finding certain answers to conjunctive queries and their unions in the single-type scenario is well-known to be the same as query evaluation <ref type="bibr" target="#b18">[19]</ref>, i.e., AC 0 in data complexity and NP in combined complexity. However, with order comparisons, the complexity of the problem jumps coNP in data complexity and Π 𝑝 2 in combined complexity <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b34">35]</ref>. Order comparisons lead to further complications when considered in typical scenarios involving certain answers such as integrating and exchanging data <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref>.</p><p>When arithmetic operations are added, the situation is even worse; in fact we have the following easy observation. Proposition 4.1. If C num is Z, there is a CQ(+, •, &lt;) query 𝑞 such that computing certain answers to 𝑞 is undecidable, even on a database of a fixed schema, with a single relation consisting of a single tuple. proof sketch. Indeed, consider a polynomial 𝑝 ∈ Z[𝑥 1 , . . . , 𝑥 𝑘 ], a relation 𝑅(num 𝑘 ) with a single tuple (⊤ 1 , . . . , ⊤ 𝑘 ), and 𝑞 given by ∃ x 𝑅( x) ∧ 𝑝 2 &gt; 0. Then, a certain answer to this query is true iff 𝑝 has no integer roots. Since it is know that the problem of finding solutions to Diophantine equations is undecidable even for a fixed number of variable (in fact 𝑘 = 13 <ref type="bibr" target="#b28">[29]</ref>), the result follows. □ Thus, it is even more justified in the case of numerical constraints to pass from absolute certainty to measures of certainty, as was done in <ref type="bibr" target="#b26">[27]</ref>. This is what we do next.</p><p>Measure of certainty. Now consider a query 𝑞( x, ȳ), where x are of base type, ȳ are of numerical type, and let 𝐷 be a database. Let ā be a tuple of base type constants and nulls from 𝐷 of the same arity as x, and let s be a tuple of numerical type constants and nulls from 𝐷 of the same arity as ȳ. Our goal is to define a measure 𝜇 (𝑞, 𝐷, ( ā, s)) of the likelihood of a tuple ( ā, s) being an answer to 𝑞 on 𝐷.</p><p>Given a database 𝐷, let its base type nulls be N base (𝐷) = (⊥ 1 , . . . , ⊥ 𝑚 ) and its numerical type nulls be N num (𝐷) = (⊤ 1 , . . . , ⊤ 𝑘 ). Then, for a valuation 𝑣 = (𝑣 base , 𝑣 num ), we have</p><formula xml:id="formula_5">𝑣 (N base (𝐷)) ∈ C 𝑚 base 𝑣 (N num (𝐷)) ∈ C 𝑘 num ⊆ R 𝑘 since we assume always that C num ⊆ R.</formula><p>As we did in the previous section, assume that we have an enumeration of base type constants C base as {𝑐 1 , 𝑐 2 , . . .} (we shall see soon that the choice of a particular enumeration does not affect the measure we define). When we had only one non-numerical domain we looked at the proportion of valuations 𝑣 with ∥𝑣 ∥ ≤ 𝑟 that witness 𝑣 ( ā) ∈ 𝑞 𝑣 (𝐷) , and then analyzed its asymptotic behavior. Here we do essentially the same. However, notice that 𝑣 (N num (𝐷)) is now a vector in R 𝑘 , and thus there are infinitely many valuations 𝑣 with ∥𝑣 (N num (𝐷))∥ ≤ 𝑟 , where ∥ • ∥ refers to the usual (Euclidean) norm in R 𝑘 . Thus, to measure the number of valuations, instead of cardinality we now look at their volume, denoted by Vol.</p><p>Thus, we mimic the definition from the previous section. Define</p><formula xml:id="formula_6">Supp 𝑟 (𝑞, 𝐷, ( ā, s)) =        𝑣 𝑣 ( ā) ∈ 𝑞 𝑣 (𝐷) ∥𝑣 base ∥ ≤ 𝑟 ∥𝑣 num ∥ ≤ 𝑟       </formula><p>, where of course 𝑣 = (𝑣 base , 𝑣 num ), and where we write ∥𝑣 base ∥ for ∥𝑣 base (N base (𝐷))∥ and likewise for ∥𝑣 num ∥ when 𝐷 is clear from the context. Next, the idea is essentially the same as before: define the measure of certainty as the probability that a valuation picked uniformly at random belongs to Supp 𝑟 (𝑞, 𝐷, ( ā, s)). There is a problems though: the set of valuations 𝑣 base is discrete and there are finitely many valuations 𝑣 base with ∥𝑣 base ∥ ≤ 𝑟 , while for valuations 𝑣 num we need to measure their volume. We thus need to combine the two in one measure somehow.</p><p>To do this, we relativize Supp 𝑟 (𝑞, 𝐷, ( ā, s)) for each fixed valuation 𝑣 base on base-type nulls. That is, we define Supp 𝑟 (𝑞, 𝐷, ( ā, s) | 𝑣 base ) as {𝑣 num | (𝑣 base , 𝑣 num ) ∈ Supp 𝑟 (𝑞, 𝐷, ( ā, s))} .</p><p>Since we associate 𝑣 num with 𝑣 num (N num (𝐷)) ⊆ R 𝑘 , we have Supp 𝑟 (𝑞, 𝐷, ( ā, s) | 𝑣 base ) ⊆ R 𝑘 and thus, if it is measurable, we use its volume as its measure. Then we define</p><formula xml:id="formula_7">Vol 𝑟 (𝑞, 𝐷, ( ā, s)) = ∥𝑣 base ∥ ≤𝑟 Vol Supp 𝑟 (𝑞, 𝐷, ( ā, s) | 𝑣 base )</formula><p>as the combined measure of Supp 𝑟 (𝑞, 𝐷, ( ā, s)) taking into account the discrete nature of valuations on base type and continuous measure of valuations on the numerical type.</p><p>Of course as 𝑟 grows, so can Vol 𝑟 (𝑞, 𝐷, ( ā, s)), since vectors 𝑣 num (N num (𝐷)) come from 𝐵 𝑘 𝑟 , the 𝑘-dimensional ball of radius 𝑟 . So we normalize this measure, to make it a number between 0 and 1 that determines the likelihood of ( ā, s) being an answer. For that, we divide by the largest possible value of Vol 𝑟 (𝑞, 𝐷, ( ā, s)). We have at most ⌊𝑟 ⌋ 𝑚 valuations 𝑣 base with ∥𝑣 base ∥ ≤ 𝑟 (we take ⌊𝑟 ⌋ if 𝑟 is not an integer), and for each of them Supp 𝑟 (𝑞, 𝐷, ( ā, s) | 𝑣 base ) ⊆ 𝐵 𝑘 𝑟 and thus its volume is bounded by Vol(𝐵 𝑘 𝑟 ). Summing up, the maximum value of Vol 𝑟 (𝑞, 𝐷, ( ā, s)) is Vol(𝐵 𝑘 𝑟 ) • ⌊𝑟 ⌋ 𝑚 and thus we define the likelihood as</p><formula xml:id="formula_8">𝜇 𝑟 (𝑞, 𝐷, ( ā, s)) = Vol 𝑟 (𝑞, 𝐷, ( ā, s)) Vol(𝐵 𝑘 𝑟 ) • ⌊𝑟 ⌋ 𝑚 ∈ [0, 1]<label>(2)</label></formula><p>to be this normalized value. Finally, as before, we look at the asymptotic behavior of this measure:</p><p>𝜇 (𝑞, 𝐷, ( ā, s)) = lim 𝑟 →∞ 𝜇 𝑟 (𝑞, 𝐷, ( ā, s)) .</p><p>(</p><formula xml:id="formula_9">)<label>3</label></formula><p>Convention For Boolean queries 𝑞, we shall write 𝜇 (𝑞, 𝐷) instead of the more formal 𝜇 (𝑞, 𝐷, ()), i.e., omitting the empty tuple of arguments.</p><p>Remark What if there are no numerical variables? In this case Supp 𝑟 (𝑞, 𝐷, ( ā, s) | 𝑣 base ) is a subset of R 0 , i.e., just a point. If we assume that Vol(R 0 ) = 1, then in this case 𝜇 (𝑞, 𝐷, ā) can be easily seen to coincide with the measure defined in <ref type="bibr" target="#b26">[27]</ref> and presented in the previous section. Thus we indeed provide a proper generalization of the framework of measuring certainty of answers to the case of numerical domains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">THE MEASURE IS WELL DEFINED</head><p>We provided a formal definition of the measure of certainty 𝜇 (𝑞, 𝐷, ( ā, s)) as a number in the interval [0, 1], indicating how likely a tuple ( ā, s), consisting of elements of base and numerical types, is to be an answer to 𝑞 on an incomplete database 𝐷. The definition however took for granted the existence of certain quantities, and thus we need to prove that 𝜇 is actually well defined. Specifically, there are two elements of the definition of 𝜇 where well-definedness needs to be formally proved.</p><p>• We defined measure 𝜇 using volumes of sets Supp 𝑟 (𝑞, 𝐷, ( ā, s) | 𝑣 base ) ⊆ R 𝑘 . But even though Supp 𝑟 (𝑞, 𝐷, ( ā, s) | 𝑣 base ) is a bounded set, as a subset of 𝐵 𝑘 𝑟 , a priori it does not mean that it is measurable and the volume is defined.</p><p>• We then defined 𝜇 as the limit of some ratio (see equations</p><p>(2) and ( <ref type="formula" target="#formula_9">3</ref>)) as 𝑟 grows. However, it is not clear a priori that the limit always exists. Our goal now is to prove that 𝜇 is well-defined for all queries in FO(+, •, &lt;). In the process of showing this, we also reduce the problem of computing 𝜇 (𝑞, 𝐷, ( ā, s)) to the problem of analyzing asymptotic behavior of formulae in the first-order theory of the reals. This connection will be essential for obtaining complexity results.</p><p>The existence of volumes. For this, we need to prove that sets Supp 𝑟 (𝑞, 𝐷, ( ā, s) | 𝑣 base ) ⊆ R 𝑘 are Lebesgue-measurable. Proposition 5.1. For every query 𝑞 ∈ FO(+, •, &lt;), database 𝐷, tuples ā and s, and valuation 𝑣 base of base type nulls in 𝐷, the set Supp 𝑟 (𝑞, 𝐷, ( ā, s) | 𝑣 base ) is Lebesgue-measurable, i.e., its volume exits.</p><p>proof sketch. To see this, simply put in the definition of each relation 𝑅 in 𝑣 base (𝐷) (as an explicit disjunction of tuples) in 𝑞 to see that Supp 𝑟 (𝑞, 𝐷, ( ā, s) | 𝑣 base ) is definable in FO theory of the real field ⟨R, +, •, &lt;⟩. The latter, by cell decomposition, is a finite union of cells, each of which is open and thus measurable, cf. <ref type="bibr" target="#b33">[34]</ref>. □</p><p>The existence of limits. We prove this in two steps. First, we restate the definition of 𝜇 by fully eliminating its dependence on valuations over the base type. Then we show how to eliminate the database and reduce the question to the existence of limits of volumes of some definable sets over the real field ⟨R, +, •, &lt;⟩, which is actually known.</p><p>Eliminating base type. We now show that to compute the measure, we can completely disregard valuations 𝑣 base . Recall that when there is no numerical type, 𝜇 (𝑞, 𝐷, ā) ∈ {0, 1}, and 𝜇 (𝑞, 𝐷, ā) = 1 iff ā is returned by the naive evaluation of 𝑞 on 𝐷. This observation makes it possible to eliminate base type valuations from the consideration of the asymptotic behavior of 𝜇 𝑟 .</p><p>For 𝑞, 𝐷, and a tuple ā, we say that a valuation 𝑣 base is bijective (with respect to 𝑞, 𝐷, ā) if 𝑣 base is a bijection and its range is disjoint from C base (𝐷). Then it easy to see that for sufficiently large 𝑟 and two bijective valuations 𝑣 base and 𝑣 ′ base we have Vol Supp 𝑟 (𝑞, 𝐷, ( ā, s) | 𝑣 base ) = Vol Supp 𝑟 (𝑞, 𝐷, ( ā, s) | 𝑣 ′ base ) Furthermore, as 𝑟 grows, almost all (asymptotically) valuations on base type become bijective. This easily leads to the following. we conclude that for computing 𝜇 (𝑞, 𝐷, ( ā, s)) we only need to look at databases 𝐷 without base type nulls. And once we have such databases, we can further reduce the problem to that for formulae that do not even mention database. □ Eliminating database relations. We now look at databases that have only numerical nulls. Consider such a database 𝐷 with N num (𝐷) = {⊤ 1 , . . . , ⊤ 𝑘 }, an FO(+, •, &lt;) query 𝑞( x, ȳ), and tuples ā over C base (𝐷) and s over C num (𝐷) ∪ N num (𝐷) of the same length as x and ȳ respectively. Let R stand for the structure ⟨R, +, •, &lt;⟩, i.e., the real closed field. Proposition 5.3. Under the assumptions above, there exists, and can be constructed in time polynomial in the size of 𝐷, a quantifier-free formula 𝜑 𝑞,𝐷, ā,s (𝑧 1 , . . . , 𝑧 𝑘 ) over R such that R |= 𝜑 𝑞,𝐷, ā,s (𝑧 1 , . . . , 𝑧 𝑘 ) iff 𝑣 z ( ā, s) ∈ 𝑞 𝑣 z (𝐷) , where 𝑣 z assigns 𝑧 𝑖 to each ⊤ 𝑖 for 𝑖 ≤ 𝑘.</p><p>proof sketch. To see this, first associate with each numerical null ⊤ 𝑖 a new variable 𝑧 𝑖 . Then replace all quantifiers ∃𝑥 over base type variables by explicit disjunction 𝑎 ∈C base (𝐷) and quantifiers ∃𝑦 over numerical type variables by 𝑠 ∈C num (𝐷)∪N num (𝐷) . In the resulting formula all atomic relational formulae are of the form 𝑅(c, ū), where c is a tuple of base type constants, and ū is a tuple of numerical type constants and nulls of some length 𝑝. We then replace such formula by</p><formula xml:id="formula_10">𝑝 𝑗=1 𝑢 • 𝑗 = 𝑤 •</formula><p>𝑗 where the disjunction is taken over all tuples (c, w) that occur in relation 𝑅 in 𝐷 and 𝑐 • = 𝑐 for 𝑐 ∈ C num while ⊤ • 𝑖 = 𝑧 𝑖 . Next, using standard arithmetic, we can assume that each atom 𝑡 &lt; 𝑡 ′ is given as a Boolean combinations of statements 𝑝 ( ū){&lt;, = }𝑝 ′ ( ū), where 𝑝, 𝑝 ′ are polynomials. We then again replace such formulae by 𝑝 (𝑢</p><formula xml:id="formula_11">• 1 , . . . , 𝑢 • 𝑝 ){&lt;, =}𝑝 ′ (𝑢 • 1 , . . . , 𝑢 • 𝑝 )</formula><p>, resulting in the desired formula 𝜑 ( z). Verifying correctness of the translation is routine. Thus, the set of all valuations 𝑣 witnessing 𝑣 ( ā, s) ∈ 𝑞 𝑣 (𝐷) is defined by a formula over R. Now consider an arbitrary formula 𝜑 ( z) with | z| = 𝑘 over R and define</p><formula xml:id="formula_12">𝜈 (𝜑) = lim 𝑟 →∞ Vol 𝜑 (R 𝑘 ) ∩ 𝐵 𝑘 𝑟 Vol(𝐵 𝑘 𝑟 ) ,<label>(4)</label></formula><p>where</p><formula xml:id="formula_13">𝜑 (R 𝑘 ) = {s ∈ R 𝑘 | R |= 𝜑 ( ā)}. That is, the ratio Vol 𝜑 (R 𝑘 ) ∩ 𝐵 𝑘 𝑟 /Vol(𝐵 𝑘 𝑟 )</formula><p>shows the proportion of the 𝑘-dimensional ball of radius 𝑟 occupied by vectors satisfying 𝜑, and 𝜈 (𝜑) defines the asymptotic behavior of this ratio. □</p><p>Next we use a known fact, namely that 𝜈 (𝜑) exists for every formula 𝜑 over R. It was shown in <ref type="bibr" target="#b10">[11]</ref>, using results of <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b21">22]</ref>, how to definably approximate volumes of sets definable in R. Combining it with previous results, we obtain the main result of this section. Theorem 5.4. For an FO(+, •, &lt;) query 𝑞, a database 𝐷, and tuples ā, s of values of free variables of 𝑞, the measure 𝜇 (𝑞, 𝐷, ( ā, s)) is welldefined and its value is a number in in the interval [0, 1]. Moreover, one can construct in polynomial time in the size of 𝐷 a formula 𝜑 ( z) over R, where | z| is the number of numerical nulls in 𝐷, such that 𝜇 (𝑞, 𝐷, ( ā, s)) = 𝜈 (𝜑) .</p><p>We finish this section by returning to the example from the introduction and computing 𝜇 (𝑞, 𝐷, (𝑠)), where 𝑠 refers to the only market segment present in the database. Since a bijective valuation 𝑣 base will not map ⊥ ′′ to 𝑠, we conclude that by Proposition 5.2 and Theorem 5.4 that 𝜇 (𝑞, 𝐷, (𝑠)) = 𝜈 (𝜑) where 𝜑 is given by <ref type="bibr" target="#b0">(1)</ref>. Then an easy calculation shows that for 𝑟 &gt; 8, the ratio</p><formula xml:id="formula_14">Vol(𝜑 (R 2 ) ∩ 𝐵 2 𝑟 )/Vol(𝐵 2 𝑟 ) is in the interval ( 𝜋 2 -arctan( 10 7 )) 2𝜋 - 8 𝜋𝑟 , ( 𝜋 2 -arctan( 10 7 )) 2𝜋 which shows that 𝜈 (𝜑) = ( 𝜋 2 -arctan( 10<label>7</label></formula><p>))/2𝜋 ≈ 0.097, or, if we are only interested in the proportion of the positive quadrant satisfying this constraint, it is four times that, i.e., ≈ 0.388 as shown in the Introduction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">APPROACHES TO COMPUTING AND APPROXIMATING THE MEASURE</head><p>Now that we have the measure of certainty defined, the question is how to compute it. This would be an algorithm that computes a value of a function in the interval [0, 1], rather than a decision problem.</p><p>Of course one can attempt to compute the function 𝜇. We shall see though that there are two obstacles along the way. First, the value of 𝜇 may be irrational and thus needs to be approximated. Second, computing the value of 𝜇, even if it is rational, may be computationally hard. Ideally, we would like it to be in FP, the class of functions computable in polynomial time. However, we will see that even for simple queries, the problem could be hard for FP #𝑃 , the class of functions computable in polynomial time with calls to a #𝑃 (and thus intractable) oracle.</p><p>Starting with the first issue we show the following. Proposition 6.1. Consider the Boolean query 𝑞 = ∃𝑥, 𝑦 𝑅(𝑥, 𝑦) ∧ (𝑥 ≥ 0) ∧ (𝑦 ≤ 𝛼 • 𝑥) and a database 𝐷 in which 𝑅 has a single tuple 𝑅(⊤, ⊤ ′ ). Then 𝜇 (𝑞, 𝐷) ∈ Q iff 𝛼 = 0, ±1. proof idea. Indeed, a simple calculation shows that 𝜇 (𝑞, 𝐷) = arctan(𝛼)/2𝜋 + 1/2, and then one can show that it is irrational except for 𝛼 = 0 or 𝛼 = ±1. □</p><p>In some cases, we can guarantee rationality of 𝜇 (𝑞, 𝐷) but then the complexity jumps. Proposition 6.2. For queries in FO(&lt;), the value 𝜇 (𝑞, 𝐷) is always rational. However, there are queries for which computing 𝜇 (𝑞, 𝐷) for queries from CQ(&lt;) is FP #𝑃 -hard in data complexity. proof sketch. Following <ref type="bibr" target="#b12">[13]</ref>, computing the rational number 𝑛 𝑚 = 𝜇 (𝑞, 𝐷) is FP #𝑃 -hard if we can find a query 𝑞 and databases 𝐷 such that computing the denominator 𝑚 is done in polynomial time, and computing the numerator 𝑛 is #𝑃-hard. We can show how to use a CQ(&lt;) query to compute the number of satisfying assignments to a 3DNF in 𝑘 variables which is known to be #𝑃-hard <ref type="bibr" target="#b32">[33]</ref>; the denominator in this case is 2 𝑘 , the number of all assignments, and thus can be computed efficiently in binary. □</p><p>These propositions show that it is necessary to look for approximation schemes. Towards that, let us recall some basic definitions, cf. <ref type="bibr" target="#b35">[36]</ref>. Given a problem of computing a function 𝑓 ( x), we say that a randomized algorithm 𝐴( x, 𝜖) is an FPRAS (fully polynomial-time randomized approximation scheme) for it if 𝐴 runs in time polynomial in the size of x and 1/𝜖, and the value output[𝐴( x, 𝜖)] it produces satisfies</p><formula xml:id="formula_15">(1 -𝜖)𝑓 ( x) ≤ output[𝐴( x, 𝜖)] ≤ (1 + 𝜖) 𝑓 ( x) ,</formula><p>with probability at least 3  4 . The confidence level 3  4 can be changed to any arbitrary value 1-𝛿 for 𝛿 ∈ (0, 1) by running 𝐴 polynomially many times over the same input; alternatively 𝛿 can be set up as a parameter of the FPRAS and the algorithm 𝐴 then needs to run in time that is also polynomial in log(1/𝛿).</p><p>However, even with order constraints, we cannot get an FPRAS for general FO queries, under a widely believed complexitytheoretic assumption. Theorem 6.3. If NP ⊈ BPP, then there is no FPRAS for computing 𝜇 (𝑞, 𝐷) for FO(&lt;) queries.</p><p>proof sketch. In this case, we produce an FO query 𝑞 ∈ FO(&lt;) and, for each 3CNF formula 𝜓 with 𝑛 variables, a database 𝐷 𝜓 such that 𝜇 (𝑞, 𝐷 𝜓 ) = ♯𝜓 2 𝑛 , where ♯𝜓 is the number of satisfying assignments to 𝜓 . In turn, this proves that 𝜓 is satisfiable if and only if 𝜇 (𝑞, 𝐷 𝜓 ) &gt; 0, and therefore the associated decision problem is NP-hard. This observation together with a standard complexity theoretic argument allows us to conclude that, if an FPRAS for 𝜇 (𝑞, 𝐷) exists, then 𝑁 𝑃 ⊈ 𝐵𝑃𝑃. □</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Alternatives. Negative results shown earlier provide us with clear limitations as to what our options could be:</head><p>• We could try to find an FPRAS for conjunctive queries, as nothing in our results precludes that; or • We can try to find a weaker form of approximations for arbitrary FO(+, •, &lt;) queries.</p><p>This is exactly what we do. In the next section we prove the existence of an FPRAS for CQ(+, &lt;), conjunctive queries with linear constraints. Then, for a weaker form of approximation, we note that the function 𝜇 has its values in the interval [0, 1]. Thus, unlike FPRAS that provides multiplicative error guarantees, for small values of 𝜖 we can also settle for a randomized algorithm 𝐴(𝑞, 𝐷, ( ā, s)) that returns an answer with additive error guarantees, i.e., |𝜇 (𝑞, 𝐷)output[𝐴(𝑞, 𝐷, ( ā, s))]| &lt; 𝜖 .</p><p>We provide such an algorithm for all FO(+, •, &lt;) queries. The algorithm is polynomial in data complexity, i.e., it runs in time polynomial in the size of 𝐷 and 1/𝜖. We will then show experimentally that its performance is feasible not only in theory but also in practice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">FRPAS FOR CONJUNCTIVE QUERIES</head><p>While, in the general case, computing the likelihood of an answer is hard and cannot be efficiently approximated, in the important restricted case of conjunctive queries with linear constraints approximation is still possible. These queries are essentially joins of relations where join conditions can involve linear (in)equalities such as rrp ≤ 0.9 • p in the notation of the example in the Introduction, saying that our price is less than 90% of that of the competition.</p><p>We show that for this class of queries, CQ(+, &lt;), we can design an efficient approximation for the measure 𝜇 (𝑞, 𝐷, ( ā, s)) in terms of data complexity. This is based on reduction to formulae in the theory of addition over the reals, along the lines of Proposition 5.3, and then applying an FPRAS for computing the volume of a union of convex bodies based on the existence of some oracles <ref type="bibr" target="#b8">[9]</ref>. Given a query 𝑞 ∈ CQ(+, &lt;), we define the function problem 𝜇 𝑞 (𝐷, ( ā, s)) that asks for the values of 𝜇 (𝑞, 𝐷, ( ā, s), with input 𝐷 and ( ā, s). In other words, this reformulation corresponds to the function problem of computing 𝜇 with respect to data complexity. Theorem 7.1. Let 𝑞( x, ȳ) be a query in CQ(+, &lt;). There exists an FPRAS for 𝜇 𝑞 (𝐷, ( ā, s)), with input database 𝐷, tuple ( ā, s), and</p><formula xml:id="formula_16">𝜖 ∈ (0, 1].</formula><p>That is, there is a randomized algorithm 𝐴 𝑞 (𝐷, 𝜖) that runs in time polynomial in 𝐷 and 1/𝜖 and returns a value 𝑎 such that the ratio 𝑎/𝜇 (𝑞, 𝐷,</p><formula xml:id="formula_17">( ā, s)) is in the interval [1 -𝜖, 1 + 𝜖].</formula><p>proof sketch. To prove this, we take 𝑞( x, ȳ), a database 𝐷 with 𝑛 numerical nulls, and tuples ā, s and use Theorem 5.4 to produce a formula 𝜑 ( z) with 𝑛 free variables over R so that 𝜇 (𝑞, 𝐷, ( ā, s)) = 𝜈 (𝜑). To get to the setting where we can define an FPRAS for 𝜇 𝑞 (𝐷, ( ā, s)), we need to analyze this translation in a bit more detail. Recall that first we had to apply a bijective valuation 𝑣 base to 𝐷 to get base type nulls out of the way. Since this can be produced in linear time, we can just assume that such a valuation already was applied, and neither 𝐷 nor ā have base type nulls.</p><p>Next, analyzing the construction of 𝜑 given in Proposition 5.3 and given the fact that it is applied to a conjunctive query where existential quantifiers translate into disjunctions and the rest of the formula is a conjunction of predicates, we observe the following about formula 𝜑:</p><p>• 𝜑 is a formula over ⟨R, +, &lt;⟩, i.e., it does not use multiplication; • 𝜑 is of the form 𝑖 𝑗 𝛽 𝑖 𝑗 ( z) where each 𝛽 𝑖 𝑗 ( z) is an atomic formula over ⟨R, +, &lt;⟩, i.e., of the form c • z{&lt;, =}𝑐 ′ , where c • z is the scalar product, c ∈ R 𝑛 and 𝑐 ′ ∈ R. For such a formula 𝜑, let φ denote its homogenized version, i.e., the formula obtained from 𝜑 by replacing each atomic formula of numerical type c • z &lt; 𝑐 ′ by c • z &lt; 0. Then we know (see <ref type="bibr" target="#b10">[11]</ref>) that</p><formula xml:id="formula_18">𝜈 (𝜑) = Vol( φ (R 𝑛 ) ∩ 𝐵 𝑛 1 ) Vol(𝐵 𝑛 1 )</formula><p>.</p><p>Next, using the special form of 𝜑, we see that φ (R 𝑛 ) is a finite union of sets 𝑋 1 , . . . , 𝑋 𝑚 , corresponding to the disjuncts of 𝜑, and each such set is convex, as the intersection of half-spaces defined by c • z &lt; 0. Furthermore, the intersection of each of these sets 𝑋 𝑖 with 𝐵 𝑛 1 is convex. While the problem of computing the volume of a union of convex sets is intractable, in <ref type="bibr" target="#b8">[9]</ref> it is shown that it admits an FPRAS under the assumption that each of the sets is given by an individual membership oracle that, for a set 𝑋 ∈ R 𝑛 and a point ā ∈ R 𝑛 , tests whether ā ∈ 𝑋 . Since sets are represented as conjunctions of linear constraints and constraints ∥ ā∥ ≤ 𝑟 . We have these polynomialtime oracles and can apply the FPRAS of <ref type="bibr" target="#b8">[9]</ref> to approximate both Vol( φ (R 𝑛 ) ∩𝐵 𝑛 1 ) and Vol(𝐵 𝑛 1 ). It is then an easy observation that we have an FPRAS for their ratio, i.e., 𝜈 (𝜑), and thus for 𝜇 𝑞 (𝐷, ( ā, s)), completing the proof. □</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">FIRST-ORDER QUERIES AND ADDITIVE ERROR APPROXIMATIONS</head><p>In Section 7, we proved the existence of an FPRAS for conjunctive queries with linear constraints. This algorithm has two drawbacks:</p><p>it is limited to conjunctive queries in order to reduce them to a formula of a specific shape, and then it uses that formula in combination with computational geometry algorithms that may not be best suited for implementation inside a DBMS. In contrast, we now prove that, if one settles for an additive error, it is possible to devise an efficient algorithm for all first-order queries with polynomial constraints. This algorithm has the advantage that it is natural to implement, as will be shown in Section 9 that presents an initial experimental evaluation of our approach. First, we define what it means to have an additive fully polynomial-time randomized approximation scheme (AFPRAS) for a problem of computing a function 𝑓 ( x). An AFPRAS is an algorithm 𝐴( x, 𝜖) that produces a value output[𝐴( x, 𝜖)] in time polynomial in x and 1/𝜖 such that</p><formula xml:id="formula_19">𝑓 ( x) -𝜖 &lt; output[𝐴( x, 𝜖)] &lt; 𝑓 ( x) + 𝜖</formula><p>holds with probability at least 3/4.</p><p>Once again, we are interested in data complexity, and thus the problem we analyze is that of computing the function 𝜇 𝑞 (𝐷, ( ā, s)), as in the previous section: for a fixed query 𝑞( x, ȳ), on the input that consists of a database 𝐷 and values ā for base type variables x and s for numerical type variables ȳ, compute 𝜇 (𝑞, 𝐷, ( ā, s)). Theorem 8.1. If 𝑞 ∈ FO(+, •, &lt;), then there is an AFPRAS for the function problem 𝜇 𝑞 (𝐷, ( ā, s)).</p><p>In the rest of the section we explain how to construct this algorithm. As in Section 7, we start with 𝑞( x, ȳ), a database 𝐷 with 𝑛 numerical nulls, and tuples ā, s and use Theorem 5.4 to produce a formula 𝜑 ( z) with 𝑛 free variables over R so that 𝜇 (𝑞, 𝐷, ( ā, s)) = 𝜈 (𝜑). If 𝜑 only uses linear constraints, and is in disjunctive normal form, the existence of an AFPRAS follows from results of <ref type="bibr" target="#b10">[11]</ref>. Now, however, the query 𝑞 is in FO(+, •, &lt;) and thus 𝜑 could be a formula over R of arbitrary shape that also uses multiplication, which complicates the search for an AFPRAS considerably.</p><p>The idea of an AFPRAS is to test the asymptotic behavior of 𝜑 by testing the truth value of 𝜑 far away enough from the origin, along different directions. To formalize this, for a formula 𝜑 ( z) over R with 𝑛 free variables, and for ā ∈ R 𝑛 , define</p><formula xml:id="formula_20">𝑓 𝜑, ā (𝑘) = 1, if R |= 𝜑 (𝑘 • ā) 0, if R ̸ |= 𝜑 (𝑘 • ā)</formula><p>where 𝑘 • ā is the result of multiplying each component of ā by 𝑘. Intuitively 𝑓 𝜑, ā (𝑘) defines the behavior of 𝜑 along the direction ā. Functions built this way exhibit a monotonic behavior as the following lemma shows. proof sketch. To prove this, notice that the limit, if it exists, is either 0 or 1, as the function 𝑓 𝜑, ā (𝑘) assumes only these values. It remains to argue why lim 𝑘→∞ 𝑓 𝜑, ā (𝑘) exists for all ā ∈ R 𝑛 . Each atomic formula of 𝜑 is given as a (multivariate) polynomial constraint. Each of these possesses, into each particular direction represented by ā ∈ R 𝑛 , finitely many zeros. As a consequence, there is a largest such zero, say 𝑘 + , after which all constraints in 𝜑 are either fulfilled or unfulfilled forever. Hence, the value of 𝑓 𝜑, ā (𝑘) will be constant for all 𝑘 ≥ 𝑘 + . □</p><p>Using the result presented above, one can prove that the value of 𝜈 (𝜑) depends only on the asymptotic behavior of 𝑓 𝜑, ā , for ā taken from the unit ball 𝐵 𝑛 1 . Lemma 8.3. For 𝜑 ( z) as in Lemma 8.2,</p><formula xml:id="formula_21">𝜈 (𝜑) = Vol({ ā ∈ 𝐵 𝑛 1 | lim 𝑘→∞ 𝑓 𝜑, ā (𝑘) = 1}) Vol(𝐵 𝑛 1 )</formula><p>.</p><p>proof sketch. First note that by the proof of Lemma 8.2, the set in the numerator is definable in R and thus is measurable and its volume exists. By analogy with the proof of Lemma 8.2, the proportion of directions into which all constraints of 𝜑 are eventually always true or false, will be the same regardless of the radius of the ball, and thus to compute it we can use the unit ball and consider the asymptotic behavior of the constraints into each direction. □</p><p>The idea of the algorithm then is to sample sufficiently many directions ā ∈ 𝐵 𝑛 1 , and test whether lim 𝑘→∞ 𝑓 𝜑, ā (𝑘) = 1. To prove that this intuition gives us an efficient algorithm, we first need to prove that testing whether lim 𝑘→∞ 𝑓 𝜑, ā (𝑘) = 1 can be done efficiently. This is indeed the case, as the following statement shows. Lemma 8.4. Given 𝜑 ( z) as in previous lemmas, and ā ∈ 𝐵 𝑛 1 , checking whether lim 𝑘→∞ 𝑓 𝜑, ā (𝑘) = 1 can be done in polynomial time with respect to the size of ā and 𝜑. proof sketch. Indeed, from 𝜑, we first compute the formula 𝜑 x/𝑘 • ā in one free variable 𝑘 by substituting 𝑘 •𝑎 𝑖 for each variable 𝑧 𝑖 . Observe that each atomic formula of 𝜑 x/𝑘 • ā is of the form 𝑝 (𝑘){&lt;, = }0, where 𝑝 is a univariate polynomial. For each such polynomial, to test whether the corresponding constraint is asymptotically fulfilled, we only need to consider the coefficients of the terms of highest degree.</p><p>□</p><p>We finally use all these lemmas to outline the AFPRAS algorithm. Assume an input database 𝐷 with |N num (𝐷)| = 𝑛, a tuple ( ā, s) of the appropriate type, and a value 𝜖 ∈ (0, 1]. First, using Theorem 5.4 we produce, in polynomial time, a formula 𝜑 ( z) with 𝑛 free variables over R so that 𝜇 (𝑞, 𝐷, ( ā, s)) = 𝜈 (𝜑). Then, to compute 𝜈 (𝜑) approximately, we sample, uniformly at random, 𝑚 vectors ā1 , . . . , ā𝑚 from the unit 𝑛-dimensional ball (see <ref type="bibr" target="#b7">[8]</ref> for a description of such a sampling). Then, we compute the values of</p><formula xml:id="formula_22">𝐴 𝑚 (𝐷) = 𝑚 𝑖=1 lim 𝑘→∞ 𝑓 𝜑, ā𝑖 (𝑘) 𝑚 .</formula><p>Using the Chernoff bound, we can prove that, if 𝑚 ≥ 𝜖 -2 , then 𝐴 𝑚 (𝐷) ∈ [𝜇 𝑞 (𝐷, ( ā, s)) -𝜖, 𝜇 𝑞 (𝐷, ( ā, s)) + 𝜖] with probability at least 3  4 , which shows that for such 𝑚 the algorithm is the desired AFPRAS. □</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">EXPERIMENTAL EVALUATION</head><p>We have implemented the additive error approximation algorithm from Section 8. Its main goal was not to produce a fully fledged system for generating a confidence level for queries, but rather to make sure that the proposed theoretical scheme, in addition to having nice theoretical properties, is also feasible in practice.</p><p>Specifically, we used this implementation to evaluate a set of join queries with numerical conditions that are fairly typical decision support queries. We evaluated the algorithm for various precision levels ranging from 0.1 to 0.01, essentially corresponding to getting one or two decimal digits correctly. The main parameter to measure was the time overhead required to run the algorithm. The key conclusion is that for common decision queries, and even fairly small 𝜖 (i.e., high confidence level), getting approximate results is feasible at least for medium size databases.</p><p>Implementation. The implementation of the approximation algorithm itself is done in Python, using a standard library (Numpy) to perform the random sampling. It works on the output of a query, under naive evaluation, produced by the Postgres database, to evaluate confidence level in tuples it generates. Our implementation follows very closely the theoretical algorithm presented in Section 8. Given a conjunctive query 𝑞 and a database 𝐷, we first use Postgres to construct a compact representation of the formulae 𝜑 𝑞,𝐷, ā,s defined in Section 5. With it in place, we run the Monte-Carlo phase of our algorithm, with only one major difference.</p><p>In the theoretical algorithm, given a database 𝐷 with 𝑛 different numerical nulls and a query 𝑞, we sample a vector z uniformly at random from the 𝑛-dimensional unit ball and check the asymptotic behavior of 𝑞 along the direction of z. To sample z uniformly, we use the standard technique of sampling 𝑛 independent and normally distributed random variables, and then scale the vector obtained this way to fit the unit ball, see <ref type="bibr" target="#b7">[8]</ref>.</p><p>While this technique is efficient in theory, in practice sampling normally distributed random variables requires the use of a relatively slow Python routine. When there are many nulls in the database, this inevitably leads to poor performances. In our implementation, instead of sampling the whole z, we only sample as many many coordinates of z as needed to replace the nulls that affect the result of the input query. As the final scaling does not affect the direction represented by ā, we can safely use this partial vector for our asymptotic analysis. This optimization saves us a considerable amount of calls to the sampling routine and speeds up the computation substantially.</p><p>Dataset and Queries. To perform our experiments, we used synthetic data generated for a schema of a sales database resembling the one we used in the introduction. It has the following relations.</p><p>• Products(id, seg, rrp, dis) has product ids, their market segment (seg), recommended retail price (rrp), and the intended discount (dis). • Orders(id, pr, q, dis) contains information about possible future orders, including the product id (pr), the quantity ordered (q), and the extra discount applied on the order (dis). In this case, the final discount depends on the quantity, and is calculated as dis/q. • Market(seg, rrp, dis) stores the recommended retail price (rrp) and the forecast discount (dis) of the best competing product on the market.</p><p>To produce the data, we used DataFiller <ref type="bibr" target="#b9">[10]</ref>, a tool for the generation of synthetic data. We generated an SQL database of about 200K tuples, with null values. To use this data in our assessment, we replaced each SQL's null with a different string so to obtain the equivalent of a database with marked nulls. We then loaded the data into Postgres to perform our evaluation.</p><p>We used three typical decision support queries in our experiments. They ask for segments of the market where competitive advantage could be achieved, for having much better offers than the market, and for other companies offering unfair discounts. As is common in such decision support queries, we put a limit clause to present the analyst with an analyzable sample from the database. Below we give SQL code and brief descriptions of queries. Experimental Setup and Results. We ran our implementation for each of the above queries, using errors from 0.01 to 0.1 with a step of 0.005. This resulted in 19 different figures for each of the queries. All experiments ran on a machine with an Intel(R) Core(TM) i5-8500 CPU @ 3.00GHz processor, 16GB of RAM, 500GB hard-disk, running Xubuntu 19.04.</p><p>The results are shown in Figure <ref type="figure" target="#fig_2">1</ref>. Along the 𝑥 axis, we show error levels 𝜖, and along the 𝑦 axis we show running time of the algorithm. Smaller values of 𝜖 correspond to higher precision and thus more sampling and longer running times. If we are interested only in the precision up to the first decimal digit (𝜖 ∈ (0, 0.1]), then our implementation, for all of the queries, runs in less than a second. This is a very small overhead that produces valuable information on the likelihood of tuples returned by the naive implementation of a query, and in particular tells the analyst whether results that may be based on incomplete information warrant further investigation. It is also reasonable to expect that such a level of precision would be acceptable. After all, it should not matter that much whether the likelihood of a tuple is 0.8 or 0.7. As the precision increases, and 𝜖 decreases, the performance degrades as expected. But even in the case when the second decimal digits matters (𝜖 ∈ (0, 0.01]), the overhead time required to perform our analysis stays below 10 seconds for every single query.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10">FUTURE WORK</head><p>The good theoretical bounds as well as promising results of the experimental evaluation suggest pushing this approach further in at least several directions. The most important one is to move away from the totally agnostic approach where we say that each numerical null can be interpreted by an arbitrary element of the numerical domain. While this was motivated by the usual assumption in the literature on incomplete databases that nulls can denote arbitrary elements, and as such it was appropriate for this initial study, in real life with typed columns additional constraints are possible. Most commonly we have restrictions on ranges of numerical attributes. For example, price is expected to be positive, while discount is expected to be a number in [0, 1]. However the model we proposed here is very easily adaptable to such modifications. We can simply add such constraints in both the numerator and denominator of the ratio defining the measure of certainty. It then needs to be seen how the AFPRAS algorithm is to be adjusted. The model is also suitable for adding probability distributions associated with particular columns, which can simply replace uniform distributions over the 𝑛-dimensional ball.</p><p>So far we measured the likelihoods of answers by computing volumes in R 𝑛 . In the case of integer type, we can provide an alternative measure by counting the number of integer lattice points. In general one may expect to obtain similar results as the 𝑛-dimensional generalization of the Gauss circle problem says that the number of integer points inside 𝐵 𝑛 𝑟 is a good approximation of Vol(𝐵 𝑛 𝑟 ), i.e., the difference between such number and Vol(𝐵 𝑛 𝑟 ) is of the order 𝑜 (Vol(𝐵 𝑛 𝑟 )), cf. <ref type="bibr" target="#b22">[23]</ref>. Extending the measure in the integer case may also offer connections with the recent work on open world probabilistic databases over countable domains <ref type="bibr" target="#b16">[17]</ref>. Open world databases with uncountable domains <ref type="bibr" target="#b17">[18]</ref> offer another possible direction for further exploration.</p><p>As further extensions, one could look at extensions with aggregate queries, where reasoning about certainty is significantly more complicated <ref type="bibr" target="#b1">[2]</ref> but ideas related to approximation might perhaps be helpful in the case of numerical answers. Yet another connection worth exploring is with the study of the asymptotic behavior of queries over random databases under the constraint that the expected size of the database remains constant <ref type="bibr" target="#b11">[12]</ref>, as this is indeed close to the model we have with substituting values for nulls.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Proposition 5 . 2 .</head><label>52</label><figDesc>For a query 𝑞, a database 𝐷, and tuples ā, s of values of free variables of 𝑞, let 𝑣 base be a bijective valuation with respect to 𝑞, 𝐷, ā. Then 𝜇 (𝑞, 𝐷, ( ā, s)) = lim 𝑟 →∞ Vol Supp 𝑟 (𝑞, 𝐷, ( ā, s) | 𝑣 base ) Vol(𝐵 𝑘 𝑟 ) . proof sketch. Using Proposition 5.2 and the observation that Supp 𝑟 (𝑞, 𝐷, ( ā, s) | 𝑣 base ) = Supp 𝑟 (𝑞, 𝑣 base (𝐷), (𝑣 base ( ā), s)) ,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Lemma 8 . 2 .</head><label>82</label><figDesc>For each formula 𝜑 ( z) over R with 𝑛 free variables, and ā ∈ R 𝑛 , lim 𝑘→∞ 𝑓 𝜑, ā (𝑘) ∈ {0, 1} .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Experimental results</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>•</head><label></label><figDesc>Never Knowingly Undersold Query: What are the products that will sell for less than half of the best price on the market? SELECT P.id FROM Products P, Orders O, Market M WHERE P.seg = M.seg AND (P.rrp * P.dis * (O.q/O.dis) &lt;= 0.5 * M.id * M.dis) LIMIT 25• Unfair Discount Query: What are the orders that apply a discount at least 60% higher than the intended campaign discount?</figDesc><table><row><cell>• Competitive Advantage Query: What are the market seg-</cell></row><row><cell>ments where the company will have a competitive advan-</cell></row><row><cell>tage?</cell></row><row><cell>SELECT P.seg</cell></row><row><cell>FROM Products P, Market M</cell></row><row><cell>WHERE P.seg = M.seg AND</cell></row><row><cell>P.rrp * P.dis &lt;= M.rrp * M.dis</cell></row><row><cell>LIMIT 25</cell></row><row><cell>SELECT O.id</cell></row><row><cell>FROM Products P, Orders O, Market M</cell></row><row><cell>WHERE P.id = O.pr AND</cell></row><row><cell>(P.rrp * P.dis O.q) &lt;= (0.5 * M.id * M.dis)</cell></row><row><cell>LIMIT 25</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgment Work partially supported by <rs type="funder">EPSRC</rs> grants <rs type="grantNumber">M025268</rs>, <rs type="grantNumber">N023056</rs>, and <rs type="grantNumber">S003800</rs>. Part of this work was done while the third author was at <rs type="institution">IRIF, Université de Paris</rs>, and <rs type="institution">DI-ENS</rs>, supported by a grant from the <rs type="funder">Foundation Sciences Mathématiques de Paris</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_g7gVDPF">
					<idno type="grant-number">M025268</idno>
				</org>
				<org type="funding" xml:id="_rwZzDUX">
					<idno type="grant-number">N023056</idno>
				</org>
				<org type="funding" xml:id="_Mk8ktV6">
					<idno type="grant-number">S003800</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Complexity of answering queries using materialized views</title>
		<author>
			<persName><forename type="first">Serge</forename><surname>Abiteboul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oliver</forename><surname>Duschka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">PODS</title>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="254" to="263" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Answering aggregate queries in data exchange</title>
		<author>
			<persName><forename type="first">Foto</forename><surname>Afrati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phokion</forename><surname>Kolaitis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">PODS</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="129" to="138" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Answering Queries Using Views with Arithmetic Comparisons</title>
		<author>
			<persName><forename type="first">Foto</forename><surname>Afrati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prasenjit</forename><surname>Mitra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">PODS</title>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="209" to="220" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Data exchange in the presence of arithmetic comparisons</title>
		<author>
			<persName><forename type="first">Foto</forename><surname>Afrati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vassia</forename><surname>Pavlaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EDBT</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="487" to="498" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">Marcelo</forename><surname>Arenas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pablo</forename><surname>Barceló</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leonid</forename><surname>Libkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Filip</forename><surname>Murlak</surname></persName>
		</author>
		<title level="m">Foundations of Data Exchange</title>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Database Repairing and Consistent Query Answering</title>
		<author>
			<persName><forename type="first">Leopoldo</forename><surname>Bertossi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
			<publisher>Morgan&amp;Claypool Publishers</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Ontology-Mediated Query Answering with Data-Tractable Description Logics</title>
		<author>
			<persName><forename type="first">Meghyn</forename><surname>Bienvenu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Magdalena</forename><surname>Ortiz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Reasoning Web</title>
		<imprint>
			<biblScope unit="page" from="218" to="307" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Foundations of Data Science</title>
		<author>
			<persName><forename type="first">Avrim</forename><surname>Blum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Hopcroft</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ravindran</forename><surname>Kannan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<publisher>CUP</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Approximating the volume of unions and intersections of high-dimensional geometric objects</title>
		<author>
			<persName><forename type="first">Karl</forename><surname>Bringmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tobias</forename><surname>Friedrich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Geom</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">6-7</biblScope>
			<biblScope unit="page" from="601" to="610" />
			<date type="published" when="2010">2010. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">Fabien</forename><surname>Coelho</surname></persName>
		</author>
		<ptr target="https://www.cri.ensmp.fr/people/coelho/datafiller.html" />
		<title level="m">DataFiller -generate random data from database schema</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Measuring the Likelihood of Numerical Constraints</title>
		<author>
			<persName><forename type="first">Marco</forename><surname>Console</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Hofer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leonid</forename><surname>Libkin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Asymptotic Conditional Probabilities for Conjunctive Queries</title>
		<author>
			<persName><forename type="first">N</forename><surname>Nilesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gerome</forename><surname>Dalvi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Miklau</surname></persName>
		</author>
		<author>
			<persName><surname>Suciu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDT</title>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="289" to="305" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">On the Complexity of Probabilistic Abstract Argumentation Frameworks</title>
		<author>
			<persName><forename type="first">Bettina</forename><surname>Fazzinga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergio</forename><surname>Flesca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francesco</forename><surname>Parisi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Comput. Log</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1" to="22" />
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Query Processing over Incomplete Databases</title>
		<author>
			<persName><forename type="first">Yunjun</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoye</forename><surname>Miao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>Morgan &amp; Claypool Publishers</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Certain Answers over Incomplete XML Documents: Extending Tractability Boundary</title>
		<author>
			<persName><forename type="first">Amélie</forename><surname>Gheerbrant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leonid</forename><surname>Libkin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Theory Comput. Syst</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="892" to="926" />
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Naïve evaluation of queries over incomplete databases</title>
		<author>
			<persName><forename type="first">Amélie</forename><surname>Gheerbrant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leonid</forename><surname>Libkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cristina</forename><surname>Sirangelo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Database Syst</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="31" />
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Probabilistic Databases with an Infinite Open-World Assumption</title>
		<author>
			<persName><forename type="first">Martin</forename><surname>Grohe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Lindner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">PODS</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="17" to="31" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Infinite Probabilistic Databases</title>
		<author>
			<persName><forename type="first">Martin</forename><surname>Grohe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Lindner</surname></persName>
		</author>
		<idno type="DOI">10.4230/LIPIcs.ICDT.2020.16</idno>
		<ptr target="https://doi.org/10.4230/LIPIcs.ICDT.2020.16" />
	</analytic>
	<monogr>
		<title level="m">23rd International Conference on Database Theory (ICDT 2020) (Leibniz International Proceedings in Informatics (LIPIcs))</title>
		<editor>
			<persName><forename type="first">Carsten</forename><surname>Lutz</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Jean</forename><surname>Christoph</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Jung</forename></persName>
		</editor>
		<meeting><address><addrLine>Dagstuhl, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">155</biblScope>
			<biblScope unit="page" from="1" to="16" />
		</imprint>
		<respStmt>
			<orgName>Schloss Dagstuhl-Leibniz-Zentrum fuer Informatik</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Incomplete information in relational databases</title>
		<author>
			<persName><forename type="first">Tomasz</forename><surname>Imielinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Witold</forename><surname>Lipski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. ACM</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="761" to="791" />
			<date type="published" when="1984">1984. 1984</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Approximating the Volume of General Pfaffian Bodies</title>
		<author>
			<persName><forename type="first">Marek</forename><surname>Karpinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angus</forename><surname>Macintyre</surname></persName>
		</author>
		<idno type="DOI">10.1007/3-540-63246-8_10</idno>
		<ptr target="https://doi.org/10.1007/3-540-63246-8_10" />
	</analytic>
	<monogr>
		<title level="j">Structures in Logic and Computer Science, A Selection of Essays in Honor of Andrzej Ehrenfeucht</title>
		<imprint>
			<biblScope unit="page" from="162" to="173" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">On conjunctive queries containing inequalities</title>
		<author>
			<persName><forename type="first">C</forename><surname>Anthony</surname></persName>
		</author>
		<author>
			<persName><surname>Klug</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. ACM</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="146" to="160" />
			<date type="published" when="1988">1988. 1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Approximating the volume of definable sets</title>
		<author>
			<persName><forename type="first">Pascal</forename><surname>Koiran</surname></persName>
		</author>
		<idno type="DOI">10.1109/SFCS.1995.492470</idno>
		<ptr target="https://doi.org/10.1109/SFCS.1995.492470" />
	</analytic>
	<monogr>
		<title level="m">FOCS</title>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="134" to="141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<author>
			<persName><forename type="first">E</forename><surname>Krätzel</surname></persName>
		</author>
		<title level="m">Lattice Points</title>
		<imprint>
			<publisher>Kluwer</publisher>
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Constraint Databases</title>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Kuper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leonid</forename><surname>Libkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Paredaens</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000">2000</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Data integration: a theoretical perspective</title>
		<author>
			<persName><forename type="first">Maurizio</forename><surname>Lenzerini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Symposium on Principles of Database Systems (PODS)</title>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="233" to="246" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">SQL&apos;s Three-Valued Logic and Certain Answers</title>
		<author>
			<persName><forename type="first">Leonid</forename><surname>Libkin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Database Syst</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Certain Answers Meet Zero-One Laws</title>
		<author>
			<persName><forename type="first">Leonid</forename><surname>Libkin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">PODS</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="195" to="207" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">On Relational Algebra with Marked Nulls</title>
		<author>
			<persName><forename type="first">Witold</forename><surname>Lipski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">PODS</title>
		<imprint>
			<date type="published" when="1984">1984</date>
			<biblScope unit="page" from="201" to="203" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Reduction of an arbitrary diophantine equation to one in 13 unknowns</title>
		<author>
			<persName><forename type="middle">V</forename><surname>Ju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julia</forename><surname>Matijasevic</surname></persName>
		</author>
		<author>
			<persName><surname>Robinson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Acta Arith</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="521" to="553" />
			<date type="published" when="1975">1975. 1975</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Linking Data to Ontologies</title>
		<author>
			<persName><forename type="first">Antonella</forename><surname>Poggi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Domenico</forename><surname>Lembo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diego</forename><surname>Calvanese</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Giuseppe</forename><forename type="middle">De</forename><surname>Giacomo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maurizio</forename><surname>Lenzerini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Riccardo</forename><surname>Rosati</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Data Semantics</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="133" to="173" />
			<date type="published" when="2008">2008. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Probabilistic Databases</title>
		<author>
			<persName><forename type="first">D</forename><surname>Suciu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Olteanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Re</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Koch</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
			<publisher>Mor-gan&amp;Claypool Publishers</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Transaction Processing Performance Council</title>
	</analytic>
	<monogr>
		<title level="j">TPC Benchmark™ H Standard Specification. Transaction Processing Performance Council</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note>Revision 2.17.1</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">The Complexity of Enumeration and Reliability Problems</title>
		<author>
			<persName><forename type="first">Leslie</forename><forename type="middle">G</forename><surname>Valiant</surname></persName>
		</author>
		<idno type="DOI">10.1137/0208032</idno>
		<ptr target="https://doi.org/10.1137/0208032" />
	</analytic>
	<monogr>
		<title level="j">SIAM J. Comput</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="410" to="421" />
			<date type="published" when="1979">1979. 1979</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Tame Topology and o-minimal Structures</title>
		<author>
			<persName><forename type="first">Lou</forename><surname>Van Den Dries</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
			<publisher>Cambridge university press</publisher>
			<biblScope unit="volume">248</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">The Complexity of Querying Indefinite Data about Linearly Ordered Domains</title>
		<author>
			<persName><forename type="first">Ron</forename><surname>Van Der Meyden</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Comput. Syst. Sci</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="113" to="135" />
			<date type="published" when="1997">1997. 1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Approximation Algorithms</title>
		<author>
			<persName><forename type="first">Vijay</forename><forename type="middle">V</forename><surname>Vazirani</surname></persName>
		</author>
		<ptr target="http://www.springer.com/computer/theoretical+computer+science/book/978-3-540-65367-7" />
		<imprint>
			<date type="published" when="2001">2001</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
