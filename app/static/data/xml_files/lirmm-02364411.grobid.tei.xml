<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">High Dimensional Data Clustering by means of Distributed Dirichlet Process Mixture Models</title>
				<funder>
					<orgName type="full">CASDAR</orgName>
				</funder>
				<funder ref="#_xMAY3D5">
					<orgName type="full">(PIA)</orgName>
				</funder>
				<funder ref="#_mMTWHzq">
					<orgName type="full">European Union</orgName>
				</funder>
				<funder ref="#_2a6nFkq">
					<orgName type="full">Agence Nationale de la Recherche</orgName>
					<orgName type="abbreviated">ANR</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Khadidja</forename><surname>Meguelati</surname></persName>
							<email>khadidja.meguelati@inria.fr</email>
							<affiliation key="aff0">
								<orgName type="laboratory" key="lab1">Inria</orgName>
								<orgName type="laboratory" key="lab2">LIRMM</orgName>
								<orgName type="institution" key="instit1">Univ Montpellier</orgName>
								<orgName type="institution" key="instit2">CNRS Montpellier</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Benedicte</forename><surname>Fontez</surname></persName>
							<email>benedicte.fontez@supagro.fr</email>
							<affiliation key="aff1">
								<orgName type="department">Montpellier SupAgro</orgName>
								<orgName type="laboratory">MISTEA</orgName>
								<orgName type="institution">Univ Montpellier Montpellier</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Nadine</forename><surname>Hilgert</surname></persName>
							<email>nadine.hilgert@inra.fr</email>
							<affiliation key="aff2">
								<orgName type="laboratory">MISTEA</orgName>
								<orgName type="institution" key="instit1">INRA</orgName>
								<orgName type="institution" key="instit2">Univ Montpellier Montpellier</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Florent</forename><surname>Masseglia</surname></persName>
							<email>florent.masseglia@inria.fr</email>
							<affiliation key="aff3">
								<orgName type="laboratory" key="lab1">Inria</orgName>
								<orgName type="laboratory" key="lab2">LIRMM</orgName>
								<orgName type="institution" key="instit1">Univ Montpellier</orgName>
								<orgName type="institution" key="instit2">CNRS Montpellier</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">High Dimensional Data Clustering by means of Distributed Dirichlet Process Mixture Models</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">79536E88E31FA4A77A4040F63D26FBB2</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-04-12T14:52+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Gaussian random process</term>
					<term>Dirichlet Process Mixture Model</term>
					<term>Clustering</term>
					<term>Parallelism</term>
					<term>Reproducing Kernel Hilbert Space</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Clustering is a data mining technique intensively used for data analytics, with applications to marketing, security, text/document analysis, or sciences like biology, astronomy, and many more. Dirichlet Process Mixture (DPM) is a model used for multivariate clustering with the advantage of discovering the number of clusters automatically and offering favorable characteristics. However, in the case of high dimensional data, it becomes an important challenge with numerical and theoretical pitfalls. The advantages of DPM come at the price of prohibitive running times, which impair its adoption and makes centralized DPM approaches inefficient, especially with high dimensional data. We propose HD4C (High Dimensional Data Distributed Dirichlet Clustering), a parallel clustering solution that addresses the curse of dimensionality by two means. First it gracefully scales to massive datasets by distributed computing, while remaining DPM-compliant. Second, it performs clustering of high dimensional data such as time series (as a function of time), hyperspectral data (as a function of wavelength) etc. Our experiments, on both synthetic and real world data, illustrate the high performance of our approach.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Clustering is a data mining technique intensively used for data analytics, with applications in marketing <ref type="bibr" target="#b0">[1]</ref>, security <ref type="bibr" target="#b1">[2]</ref>, or sciences like astronomy <ref type="bibr" target="#b2">[3]</ref> and many more. Clustering may be used for identification in the new challenge of digital agriculture, where large amounts of complex data are collected: for example in herd monitoring, animal activity is monitored using a collar-mounted accelerometer, as illustrated in figure <ref type="figure" target="#fig_0">1</ref>. One of the main difficulties, for clustering, is the fact that we do not know, in advance, the number of clusters to be discovered. To help performing cluster analysis, despite the unknown tackled number of clusters, the field of statistics contains several suggestions:</p><p>1. Setting a number of clustering runs, with varying value of cluster number, and selecting the one that minimizes a goodness of fit criteria. It may be a quadratic risk or the Residual Mean Squared Error of Prediction (RMSEP) <ref type="bibr" target="#b3">[4]</ref>. This approach needs the implementation of a crossvalidation algorithm <ref type="bibr" target="#b3">[4]</ref>. The clustering approach may be a mixture model with an Expectation-Maximization (EM) algorithm <ref type="bibr" target="#b4">[5]</ref>, or K-means <ref type="bibr" target="#b3">[4]</ref> for instance. 2. Performing a hierarchical clustering and then cutting off the tree at a given depth, usually decided by the end-user. Different approaches for pruning with advantages and drawbacks exist, see <ref type="bibr" target="#b3">[4]</ref>. 3. Using a Dirichlet Process Mixture (DPM) which automatically detects the number of clusters <ref type="bibr" target="#b5">[6]</ref>. In this work, we focus on the DPM approach since it allows estimating the number of clusters and assigning observations to clusters, in the same process. Furthermore, its implementation is quite straightforward in a Bayesian framework. Such properties of DPM make it a very appealing solution for many use-cases.</p><p>Unfortunately, DPM relies on matrix computations and is highly time consuming, especially in the case of high dimensional data. Several attempts have been made to make it distributed, however these approaches are not adapted for high dimensional data (see the discussion in Section 3).</p><p>We propose HD4C (High Dimensional Data Distributed Dirichlet Clustering), a novel parallel clustering approach adapted for high dimensional data, based on a distributed algorithm for Dirichlet Process Mixture. HD4C takes advantage of the properties of Reproducible Kernel Hilbert Spaces to allow clustering on the whole data (the whole signal or curve or time series) <ref type="bibr" target="#b6">[7]</ref>. Other approaches that use feature selection and/or dimensionality reduction (like PCA or SVM) are often inappropriate because clusters generally lie in different subspaces <ref type="bibr" target="#b7">[8]</ref>.</p><p>The paper is organized as follows. The problem is stated in Section 2 with the necessary background. In Section 3 the related work is discussed. Our distributed solution for high dimensional data clustering by means of Dirichlet Process Mixture is detailed in Section 4. The efficiency and effectiveness of our approach are illustrated in Section 5 through an experimental evaluation. Finally, the conclusion is in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Problem Definition</head><p>The problem we address is as follows. Given a (potentially big) dataset of records find, by means of a parallel process, a partition of the dataset into disjoint subsets called clusters, such that:</p><p>• Similar records are assigned to the same cluster.</p><p>• Dissimilar records are assigned to different clusters.</p><p>• The union of the clusters is the original dataset.</p><p>3 Related Work</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">High Dimensional Data Clustering</head><p>We set our work in the context of high dimensional data clustering, which covers time series and functional data (signal) clustering.</p><p>For time series clustering, three categories are defined by <ref type="bibr" target="#b8">[9]</ref>: "Whole time-series clustering", "subsequence clustering", and "time point clustering". Subsequence clustering is performed on a set of subsequences extracted via a sliding window from a single long time-series, Keogh and Lin <ref type="bibr" target="#b9">[10]</ref> showed that this type of clustering is meaningless. Time-point clustering also is applied on a single time series, and it is similar to subsequences clustering. The focus of our work is "whole time-series clustering". The authors of <ref type="bibr" target="#b8">[9]</ref> and <ref type="bibr" target="#b10">[11]</ref> identified four different approaches to do this, respectively for time series and functional data: 1. Work directly with raw data, 2. Work indirectly with features extracted from the raw data. For example, in <ref type="bibr" target="#b11">[12]</ref> a symbolic representation of time series called SAX is presented, 3. Use a specific distance or dissimilarity, like Dynamic Time Warping (DTW) for time series or RKHS properties for functional data (allows to define an inner product and therefore a distance in a specific space), 4. Build a model to estimate features from the data and to cluster simultaneously. A K-means algorithm is often suggested with the first two identified approaches because of its fast convergence and its scalability (distributed versions of K-means for multivariate data are available). The third approach requires to adapt the K-means, which uses an Euclidean distance, to more complex cases. Time warp for temporal data does not define a true distance (no triangular inequality). <ref type="bibr" target="#b12">[13]</ref> proposed a generalized K-means based clustering for temporal data under time warp, but no version for distributed data is available. In a broader context, a clustering method for misaligned curves was developed by <ref type="bibr" target="#b13">[14]</ref> based on warping functions. But this approach assumes landmarks, or more generally known warping functions, to re-align the data and is not proposed for distributed data.</p><p>Finally, K-means algorithms for functional data were proposed by <ref type="bibr" target="#b10">[11]</ref> but again the algorithms are not presented for distributed data.</p><p>Moreover, one drawback of the K-means <ref type="bibr" target="#b14">[15]</ref> is that it requires the number of clusters k to be specified in advance. In comparison, the Dirichlet Process mixture (DPM) <ref type="bibr" target="#b5">[6]</ref> approach automatically detects the number of clusters and distributed versions for multivariate data are now available <ref type="bibr" target="#b15">[16]</ref><ref type="bibr" target="#b16">[17]</ref><ref type="bibr" target="#b17">[18]</ref><ref type="bibr" target="#b18">[19]</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Massive Datasets Clustering</head><p>There is a significant research on clustering of big data. Some efforts have focused on making the similarity measures faster, like, e.g., Zhu et al. <ref type="bibr" target="#b19">[20]</ref> who introduced a novel data-adaptive approximation to DTW which can be quickly computed. Other studies suggest to make the main clustering algorithms scalable by means of massive distribution.</p><p>In our work, we focused on algorithms inspired by the DPM which allows estimating the number of clusters and assigning observations to clusters, in the same process. Unfortunately, DPM is highly time consuming. Consequently, several attempts have been done to make it distributed. However, while being effectively distributed, these approaches usually suffer from convergence issues (imbalanced data distribution on computing nodes) <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18]</ref>, or do not fully benefit from DPM properties <ref type="bibr" target="#b18">[19]</ref>. Furthermore, making DPM parallel is not straightforward since it must compare each record to the set of existing clusters, a highly repeated number of times. That impairs the global performance of the approach in parallel, since comparing all the records to all the clusters would call for a high number of communications and make the process impractical.</p><p>In <ref type="bibr" target="#b15">[16]</ref> a distributed DPM algorithm called DC-DPM (Distributed Clustering by Dirichlet Process Mixture) was introduced. It allows each node to have a view on the local results of all the other nodes, while avoiding exhaustive data exchanges. The main novelty of this work was to propose a model and its estimation at the master level by exploiting the sufficient statistics from the workers, in a DPM compliant approach. It takes advantage of the computing power of distributed systems by using parallel frameworks such as Spark <ref type="bibr" target="#b20">[21]</ref>. As illustrated in figure <ref type="figure" target="#fig_1">2</ref>, the DC-DPM solution distributes the Dirichlet Process by identifying local clusters on the workers and synchronizing these clusters on the master. These clusters are then communicated as a basis among workers for local clustering consistency. The Dirichlet Process is modified to consider this basis in each worker. By iterating this process the global consistency of DPM is sought in a distributed environment. The experiments of DC-DPM, using real and synthetic datasets, illustrate both the high efficiency and linear scalability of the approach. They report significant gains in response time, compared to centralized DPM approaches, with processing times of a few minutes, compared to several days in the centralized case. The workflow of the DC-DPM approach is illustrated by Figure <ref type="figure" target="#fig_1">2</ref>.</p><p>Our goal is to propose a parallel DPM approach for high dimensional data clustering based on the DC-DPM approach <ref type="bibr" target="#b15">[16]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">HD4C: High Dimensional Data Distributed Dirichlet Clustering</head><p>In this section, we present a novel parallel clustering approach called HD4C, adapted for high dimensional data and based on DC-DPM <ref type="bibr" target="#b15">[16]</ref> described in section 3. Actually, DC-DPM is a solution proposed to this issue when data is multivariate. This solution is based on a distributed DPM. In our case, the records are high dimensional data or signals (infinite dimension). In the case of infinite dimension, matrix computation is no more feasible (no inverse for example, no matrix product). In HD4C, we are not using the Lebesgue measure. We assume that observations are the addition of two gaussian processes : signal and noise. Therefore, we define a Radom-Nikodym derivative of gaussian measures <ref type="bibr" target="#b21">[22]</ref> in order to compute a likelihood process.</p><p>A first attempt to work with this kind of data is to reduce their dimensionality, by sub-sampling the observations or projecting them into sub spaces like the one defined by a truncated basis of B-splines <ref type="bibr" target="#b22">[23]</ref> or a truncated basis of kernel principal composant analysis <ref type="bibr" target="#b23">[24]</ref>. Multivariate analysis, like SVM, k-means or DC-DPM, can then be applied.</p><p>A better approach is to continue working in infinite dimension to keep all information on the data. To compute a distributed DPM for high dimensional data or signals, we need to replace a matrix product by an inner product in an adequate space of functions and to find the adequate measure to compute the likelihood and the posterior. To do that, we used the properties of the Reproducible Kernel Hilbert Spaces (RKHS), as in <ref type="bibr" target="#b6">[7]</ref>. RKHS (used for example in the Support Vector Machine approach) are very popular in machine learning thanks to "the representer theorem which simplified an infinite dimensional empirical risk minimization problem into a finite dimensional problem where the solution is included in the linear span of the kernel function evaluated at the training points" <ref type="bibr" target="#b24">[25]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">RKHS of Gaussian Process and DPM</head><p>We assume that the random variable of interest takes its values in a space of infinite dimension. Therefore, high dimensional data will be seen as trajectories of a random process Y : Y =(Y (t)) t∈[0,T ] , where t stands for the general index of the Y function, t can be for example a time index in case of time series or a wavelength index in case of spectrum. In order to guarantee the existence of necessary conditional probabilities in the DPM algorithm, we will assume that the trajectories belong to the space of the integrable square functions (L 2 ([0,T ])) on [0,T ] (from <ref type="bibr" target="#b25">[26]</ref>). Our work focuses on Gaussian random process because "of its ability to avoid simple parametric assumptions and still build in a lot of structure", <ref type="bibr" target="#b26">[27]</ref>. In addition many calculations are facilitated in the Gaussian framework. For example, <ref type="bibr" target="#b27">[28]</ref> stated that using Gaussian process for machine learning "turn out to be much more accurate than for parametric models of equal flexibility (such as multilayer perceptrons)".</p><p>A Gaussian process GP (m,K) is entirely defined by its mean function m(t) and its covariance function K(s,t), for all t,s ∈ [0,T ]. The main idea behind the clustering with Gaussian Process is to use results from signal processing where the data is the sum of two Gaussian processes, namely a signal (a trajectory m i issued from a GP (m 0 ,K 0 )) and a noise (ε i issued from a GP (0,K)):</p><formula xml:id="formula_0">Y i =m i +ε i .</formula><p>We assume that the signal is smoother than the noise in order to be able to detect it. To extract the signals and cluster them, we use the following DPM:</p><formula xml:id="formula_1">Y i |m i ,K ∼ GP (m i ,K),i=1,...,N m i ∼ G G|m 0 ,K 0 ∼ DP (α,GP (m 0 ,K 0 ))</formula><p>DPM will create clusters of m i where for all observations in cluster c, m i = φ c . To run the DPM with algorithm 8 from Neal <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b28">29]</ref>, we need to define a posterior distribution GP (m * , K * ) for φ c and the likelihood process dGP (m i ,K)/dGP (0,K) for Y i . From <ref type="bibr" target="#b27">[28]</ref>, the Reproducing Kernel Hilbert Space with reproducing kernel K, denoted H K "will turn out to contain expected values of m i conditioned on a finite amount of information, thus the posterior mean function m * we are interested in".</p><p>Moreover, there exists a duality between a Gaussian process GP (m,K) and H K . H K is a space of real functions defined on [0, T ] which verifies the following property: ∀t ∈ [0,T ], ∀f ∈ H K , f(t) = (f,K(.,t)) K , where (.,.) K is the inner product of H K . From <ref type="bibr" target="#b29">[30]</ref>, we define the random variable (Y,f) K like a stochastic integral. The properties of H K allow to define the likelihood process <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b31">32]</ref>:</p><formula xml:id="formula_2">(Y,K(,t)) K =Y (t) (1) f,g ∈H K , (f,g) K =E[(Y,f) K (Y,g) K ]</formula><p>(2)</p><formula xml:id="formula_3">m i ∈H K , dGP (m i ,K) dGP (0,K) (Y i )=e (Yi,mi) K -1 2 (mi,mi) K (3)</formula><p>To ensure that m i ∈ H K , we must choose carefully the covariance function K 0 , because the differentiability of m i up to a given order (and therefore the smoothness of m i ) can be controlled via the covariance function.</p><p>Finally, following <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b33">34]</ref>, the posterior distribution for the signal of a cluster c is a Gaussian process, namely</p><formula xml:id="formula_4">φ c |(Y i ) ci=c ∼GP (m * ,K * ) with: m * (t) = m 0 (t)+(K 0 (.,t),( Ȳc -m 0 )) K/nc+K0 (4) K * (s,t) = K 0 (s,t)-(K 0 (.,s),K 0 (.,t)) K/nc+K0</formula><p>(5) where the covariance functions K and K 0 are weakly continuous functions on [0, T ] × [0, T ]; n c and Ȳc are respectively the number of observations and the mean function Ȳc = 1 nc ci=c Y i in cluster c. When K is non singular and weakly continuous, usual matrix approximations of the inner product results from <ref type="bibr" target="#b30">[31]</ref>:</p><formula xml:id="formula_5">lim L→∞ t f (L) K (L) -1 g (L) = (f,g) K lim L→∞ t Y (L) i K (L) -1 g (L) = (Y i ,g) K</formula><p>where (t l ) l=1...L is dense in [0, T ] and f (L) = (f(t 1 ), ... , f(t L )), g (L) = (g(t 1 ), ... , g(t L )) and K (L) is a L × L matrix whose elements are K(t l ,t j ) for 1 ≤ l,j ≤ L. Oya et al. <ref type="bibr" target="#b34">[35]</ref> proposed a generalised numerical approach to estimate the inner product in H k . In our approach (Section IV), we use a known analytical form for the inner product, which avoids matrix product or inversion and thus allows to escape the curse of dimensionality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Massive Distribution and Spark</head><p>Clustering via Dirichlet Process Mixture based on Gibbs Sampling is unable to scale to large datasets due to its high computational costs associated with Bayesian inference. For this reason, we aim to implement a parallel algorithm for DPM clustering in a massively distributed environment called Spark which is a parallel programming framework aiming to efficiently process large datasets. This programming model can perform analytics with in-memory techniques to overcome disk bottlenecks. Similar to MapReduce <ref type="bibr" target="#b35">[36]</ref>, Spark can be deployed on the Hadoop Distributed File System (HDFS) <ref type="bibr" target="#b36">[37]</ref>. Unlike traditional in-memory systems, the main feature of Spark is its distributed memory abstraction, called resilient distributed datasets (RDD), that is an efficient and fault-tolerant abstraction for distributing data in a cluster. With RDD, the data can be easily persisted in main memory as well as on the hard drive. Spark is designed to support the execution of iterative algorithms <ref type="bibr" target="#b20">[21]</ref>.</p><p>To execute a Spark job, we need a master node to coordinate job execution, and some worker nodes to execute a parallel operation. These parallel operations are summarized to two types: (i) Transformations: to create a new RDD from an existing one (e.g., Map, MapToPair, MapPartition, FlatMap); and (ii) Actions: to return a final value to the user (e.g., Reduce, Aggregate or Count) <ref type="bibr" target="#b20">[21]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">HD4C</head><p>Working in infinite dimension (functional data) allows to use information on the trajectories but also on their derivatives, which may reveal key information for the data clustering (see <ref type="bibr" target="#b37">[38]</ref>). Indeed an Hilbert space (like the RKHS) is a space of integrable square functions (L 2 ([0,T ])) on [0,T ], it is a special case of a Sobolev space. It means that a RKHS is a vector space of functions equipped with a norm that is a combination of L p -norms of the function itself and its derivatives up to a given order. The given order is conditioned by the differentiability of the trajectories and therefore by the covariance function K of the random process Y .</p><p>In our experiments, we defined Y i | θ i = m i , K as an autocorrelated Gaussian process called Ornstein-Uhlenbeck (OU) whose covariance function is defined as follows:</p><formula xml:id="formula_6">K(s,t)= σ 2 2β e -β|s-t| ,<label>(6)</label></formula><p>where σ and β are two positive real. Therefore, from <ref type="bibr" target="#b38">[39]</ref>, H K is a space of differentiable functions in [0,T ] with the scalar product (defining the norm):</p><formula xml:id="formula_7">(f,g) K = 1 σ 2 T 0 f (t)g (t)+β 2 f(t)g(t) dt + β σ 2 f(0)g(0)+f(T )g(T ) .<label>(7)</label></formula><p>To ensure that m i ∈ H K , we used the prior G=GP (m 0 ,K 0 ), where</p><formula xml:id="formula_8">K 0 (s,t)= σ 2 0 2β 0 e -β0(s-t) 2 .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>This covariance gives very smooth trajectories (infinitely differentiable).</head><p>Other choice of covariance functions are possible for non smooth observations (like a Wiener process). Defining the covariance function K on the observations is equivalent to defining the kernel covariance K of the RKHS H K . Defining a kernel K requires defining an inner product in H K , which is equivalent to defining a metric, a distance between two observations d(i,j) = (m i -m j ,m i -m j ) K . This led us to use a Sobolev metric for high dimensional Gaussian data (ie a distance between trajectories and their derivatives for OU Gaussian data) instead of the usual euclidean distance T 0 (m i (t)-m j (t)) 2 dt, see the "changing metrics" discussion in <ref type="bibr" target="#b39">[40]</ref>.</p><p>Implementating this algorithm requires: • The set of indexes used for computing the integrals in the inner product equation <ref type="bibr" target="#b6">(7)</ref>; for example in time series, it could be the observation time steps or not. • An interpolation of the observations (if needed) to simplify the computation of the inner product. This interpolation can be used to adapt the observations to the covariance function K. • Computation of the densities at the master and at the worker level, from equation ( <ref type="formula">3</ref>). This requires estimating the hyperparameters β and σ. To avoid overly complex modelling, we have chosen to fix them empirically. As the Y i curves are generated from Gaussian processes with covariance function K in (6), the parameters β and σ were determined from the empirical estimation of the intra-class variance-covariance matrix of the curves discretized in a few points. We provide below more specific details:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Worker level</head><p>In the Gaussian process framework, the likelihood process is defined with respect to the Gaussian measure from GP (0,K). Using <ref type="bibr" target="#b31">[32]</ref> we have F (y i ,φ c )=e (yi,φc) K -1 2 (φc,φc) K . As the density of the predictive prior cannot be expressed with respect to the same Gaussian measure (GP (0,K)) than the likelihood, we approximated the integral in the MCMC algorithm, as suggested in algorithm 8 of <ref type="bibr" target="#b28">[29]</ref>, by drawing m realisations of φ c .</p><p>To improve the variety of new candidate values of φ new c , we modified the original algorithm according to the following: φ new c (t) = m 0 (t)+ζ(t), where ζ(t) is a trajectory simulated from GP (m 0 ,K) and m 0 (t) is randomly simulated from a truncated polynomial basis (the basis order is also randomly chosen).</p><p>Following <ref type="bibr" target="#b40">[41]</ref>, we used an inverse Gamma prior to infer the parameter α j .</p><p>The following algorithm 1 summarizes the worker level.</p><p>Algorithm 1 DPM at worker j for each data</p><formula xml:id="formula_9">y i do Draw m values φ new c Draw individual cluster label c i in addition to existing φ 1 ,...,φ C P (c i =c|{c l } l =i ,y i ,{φ},{w},α j )∝ #(c)+αjwc Nj-1+αj e (yi,φc) K -1 2 (φc,φc) K ,c=1,...,C 1 m αjwu Nj-1+αj e (yi,φ new c ) K -1 2 (φ new c ,φ new c ) K ,c=1,...,m</formula><p>end for Update of α j where the weight w c is the proportion of observations from cluster c evaluated on the whole dataset and w u the proportion of non affected observations (awaiting the creation, innovation, discover of their real clusters), with w u + C c=1 w c =1. Therefore, these parameters are updated at the master level during the synchronization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Master level</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Instead of drawing new values φ new c</head><p>, the proposed algorithm reuses the center values of the clusters received from the workers, namely φ workerj k . The approximation of φ is updated by computing the posterior mean in each cluster, equation ( <ref type="formula">4</ref>), to which we add a noise drawn from a GP (0,K/n c ).</p><p>Following <ref type="bibr" target="#b15">[16]</ref>, we use a Dirichlet prior to infer (w 1 ,...,w K ,w u ).</p><p>The master lever is outlined in algorithm 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>The parallel experimental evaluation was conducted on a computing cluster of 32 machines, each operated by Linux, with 64 Gigabytes of main memory, Intel Xeon CPU with</p><p>Algorithm 2 DPM at master level for each cluster k from worker j do Draw cluster label z j,k from P (z j,k =c|{c} =j,k ,ȳ j,k ,{φ},γ)∝</p><formula xml:id="formula_10">#(c) N-1+γ e (ȳ j,k ,φc) K -1 2 (φc,φc) K ,c=1,...,C γ N-1+γ e (ȳ j,k ,φ workerj k ) K -1 2 (φ workerj k ,φ workerj k ) K</formula><p>end for Update of φ and (w 1 ,...,w K ,w u ) 8 cores and 250 Gigabytes hard disk. We compared our approach to K-means, which is one of the most commonly used clustering algorithms. We used an implementation available at Spark's machine learning library (MLlib) <ref type="bibr" target="#b41">[42]</ref>. The first step of HD4C is a distributed K-means that sets the initial state (usually we set K to be one tenth of the dataset size).</p><p>Reproducibility : All our experiments are fully reproducible. We make our code and data available at https://github.com/khadidjaM/HD4C.</p><p>In the rest of this section, we describe the datasets in Section 5.1 and our evaluation criteria in Section 5.2. Then, in Section 5.3, we measure the performances, in response time, of our approach by reporting its scalability and speed-up. We evaluate the clusters obtained by HD4C in the case of real and synthetic dataset in Section 5.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Datasets</head><p>We carried out our experiments on two real world datasets and many synthetic datasets.</p><p>Our synthetic data was generated using a two-steps principle. First we generated four cluster centers according to the following polynomials :</p><formula xml:id="formula_11">         s 1 (t)= 0.11t 3 -0.16t 2 +0.55t s 2 (t)= -0.75t 4 +1.49t 3 -0.91t 2 +0.17t s 3 (t)= 3.91t 5 -9.77t 4 +0.854t 3 -3.05t 2 +0.37t s 4 (t)= -20.09t 6 +60.26t 5 -68.22t 4 +36t 3 -8.71t 2 +0.76t</formula><p>In the second step, we generated the data corresponding to each center, by using a Gaussian process of mean s i and a covariance given by an Ornstein-Uhlenbeckh process parametrized by β =10 and σ =2.5. We independently generated a batch of 5 datasets having size 200K, 400, 600, 800K and 1M time series of 100 points, the latter dataset is about 2 Gigabytes. Figures <ref type="figure">3</ref> and<ref type="figure">4</ref> give a visual representation of our synthetic dataset. Each cluster is assigned a color and represented by 10 time series. This type of generator is widely used in statistics, where methods are first evaluated on synthetic data before being applied on real data.</p><p>The first real world dataset corresponds to more than five thousands accelerometer time series which have been measured by sensor on 13 sheep (as in figure <ref type="figure" target="#fig_0">1</ref>). Each time series is made of 500 observation times and has been visually assigned to one of six activities (STANDING-GRAZING, STANDING- EATING BRUSH, STANDING-RUMINATING, WALKING, RUNNING, STANDING-IMMOBILE). Accelerometers captured 3-axial acceleration at a constant rate of 100Hz. The sensor signals were pre-processed and for each activity of interest, sampled in fixed-width of 5 seconds (500 values / a time series). Each of the three axial acceleration gives a different information for the zoologist, so HD4C clustering was performed by axis (horizontals (x and y) and vertical (z)). The objective was to discover the underlying structures of each axis and then to link these structures to sheep activities. Figures <ref type="figure">5</ref> and<ref type="figure">6</ref> represent one axis of the accelerometers dataset. Each label of activity is assigned a color and represented by 5 time series.</p><p>The second real dataset corresponds to more than 4K spectrum of 680 dimensions representing a protein rate measured on 10 different products: rapeseed (CLZ), corn gluten (CNG), sun flower seed (SFG), grass silage (EHH), full fat soya (FFS),  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Clustering Evaluation Criteria</head><p>There are two cases for evaluating the results of a clustering algorithm. Either there is a ground truth available, or there is not. In the case of an available ground truth, there are measures allowing to compare the clustering results to the reference, such as the Adjusted Rand Index (ARI): it is the corrected-for-chance version of the Rand Index <ref type="bibr" target="#b42">[43]</ref>, which is a function that measures the similarity between two data clustering results, for example between the ground truth class assignments (if known) and the clustering algorithm assignments. ARI values are in the range [-1,1] with a best value of 1. It is usually exploited for experiments when one wants to check performances in a controlled environment, In the case where there is no ground-truth (which is the usual case, because we don't know what should be discovered in real world applications of a clustering algorithm) the validation of the results is not straightforward. In our experiment we have checked K, the number of discovered clusters versus the expected number of clusters according to expert.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Response Time</head><p>In this section we measure the clustering time in HD4C. Figure <ref type="figure">8</ref> reports the response times on our synthetic data, HD4C is run on a computing cluster of 16 nodes. The clustering time increases with the number of data, our approach benefits from linear scalability with the dataset size. For a dataset of 200K data points, HD4C performs the clustering in about 12 minutes, while a centralized approach does not scale and cannot execute on such dataset size, it needs several days on a single machine.</p><p>Figures 9, 10 and 11 illustrate the parallel speed-up of our approach on 200K time series from the synthetic dataset, on accelerometers data from the first real world dataset, and on spectrums from the second real dataset. These experiments are conducted on 4, 8 and 16 nodes which correspond to 32, 64 and 128 workers (each node has 8 cores). The results show optimal or near optimal gain. On the accelerometers dataset there is not a big difference between 8 and 16 nodes because this dataset is not big, and distributing it on 8 or 16 nodes is super fast at workers level while the synchronisation at the master level takes almost the same time. Another reason is that the computing nodes do not have the same performances, those who have finished must wait for the slower nodes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Clustering Evaluation</head><p>In the following experiments, we evaluate the clustering performance of HD4C and compare it to the K-means approach.</p><p>Table <ref type="table" target="#tab_0">1</ref> reports the ARI values computed between the clustering obtained and the ground truth, the estimated values of parameters σ and β, and the number of clusters, obtained with HD4C on our synthetic data while increasing the dataset size. The HD4C is run on a cluster of 16 nodes. HD4C performs well, the ARI values are almost equal to 1 (best value), the number of discovered clusters is equal to the real number of clusters, the estimated values of σ and β are close to the parameters used for simulating the data. Note also that the estimated ratio σ2 2 β converges to the true simulated ratio σ 2 2β , which corresponds to the variance on the diagonal of K in <ref type="bibr" target="#b5">(6)</ref>.</p><p>Figure <ref type="figure" target="#fig_8">12</ref> reports the Adjusted Rand Index values (described in section 5.2) obtained by performing K-means approach on 200K time series from the synthetic dataset as a function of the number of clusters, it is run on two nodes (16 workers). The K-means approach does not reach the best value 1, the peak of these values is 0.90 but with 9 clusters which is not the real number in the ground truth, while with the real number of clusters (4 clusters) the ARI value is 0.79.   K-means suffers from the convergence to a local minimum which may produce "wrong" results, as illustrated for example in Table <ref type="table" target="#tab_1">2</ref>. This table shows the results of K-means performed on 600K time series of the synthetic dataset with the right number of clusters (4 clusters, each containing 150K data) and run on 16 nodes. Each line of Table <ref type="table" target="#tab_1">2</ref> represents one cluster obtained by K-means and reports the number of data obtained in each cluster: the cluster 2 obtained by K-means regroups the two real clusters 1 and 3, while the real cluster 2 is divided between clusters 1 and 3 discovered by K-means. By comparison, when applying HD4C on the same dataset, the right number of clusters is discovered and all the data except a few ones are affected to the true clusters, as presented in Table <ref type="table" target="#tab_2">3</ref>.</p><p>Repeating the clustering on accelerometers data many times by HD4C and K-means, we obtained the ARI values showed on table 4. Our approach performs better than the K-means approach, the average value obtained by HD4C is 0.50 which is a good value regarding the shapes of data in clusters: STANDING-GRAZING, STANDING-EATING BRUSH, STANDING-RUMINATING, WALKING. The true labels have been visually assigned by the experts, by observing the three axes at the same time. It is difficult to label them by only analysing one axis at a time (see figure <ref type="figure">6</ref>). HD4C is not intended to cluster multidimensional time series. Table <ref type="table" target="#tab_3">4</ref> also represents the ARI values obtained with the real world datasets both for HD4C and K-means. K-means was processed with the number of clusters found by HD4C. Each time we repeat the HD4C clustering we find a number close to the number of labels given by the experts. We proposed HD4C, a novel and efficient parallel solution to perform clustering via DPM on large amount of infinite dimensional data. These infinite dimensional data include lengthy time series or spectral data for example. We evaluated the performance of our solution over real world and synthetic datasets. The experimental results illustrate the high performance of HD4C with results that are comparable to K-means, one of the most commonly used clustering algorithms. Overall, the experimental results show that by using our parallel techniques, the clustering of very large volumes of data can now be done in small execution times, which is impossible to achieve using a centralized DPM approach. A nice perspective of our approach is now to extend the HD4C algorithm to multivariate functional data, like the accelerometer dataset which contains time series recorded according to three axes.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: An accelerometer mounted on a sheep's collar.</figDesc><graphic coords="2,347.54,241.57,170.07,127.55" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Diagram/workflow of the DC-DPM approach.</figDesc><graphic coords="4,56.69,56.69,242.22,143.37" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 : 4 Figure 4 :</head><label>344</label><figDesc>Figure 3: Visual representation of the synthetic dataset clusters.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :Figure 6 :</head><label>56</label><figDesc>Figure 5: One axis visual representation of labeled accelerometers data</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Visual representation of the spectral dataset</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 :Figure 9 :</head><label>89</label><figDesc>Figure 8: Response time (minutes) of HD4C as a function of the dataset size.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 10 :Figure 11 :</head><label>1011</label><figDesc>Figure 10: Clustering time as a function of the number of computing nodes on the accelrometers data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 12 :</head><label>12</label><figDesc>Figure 12: ARI values of K-means as a function of the number of clusters.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Clustering evaluation criteria obtained with HD4C (synthetic data).</figDesc><table><row><cell></cell><cell>ARI</cell><cell>σ</cell><cell>β</cell><cell>σ2 /2 β Clusters</cell></row><row><cell cols="4">200K 1.00 2.57 10.59</cell><cell>0.31</cell><cell>4</cell></row><row><cell cols="4">400K 1.00 2.13 7.25</cell><cell>0.31</cell><cell>4</cell></row><row><cell cols="4">600K 0.99 2.15 7.44</cell><cell>0.31</cell><cell>4</cell></row><row><cell cols="4">800K 1.00 2.28 8.30</cell><cell>0.31</cell><cell>4</cell></row><row><cell>1M</cell><cell cols="3">0.99 2.13 7.25</cell><cell>0.31</cell><cell>4</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Example of K-means convergence to a local minimum.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Ground truth</cell><cell></cell></row><row><cell></cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell></row><row><cell>1</cell><cell>0</cell><cell>75945</cell><cell>0</cell><cell>0</cell></row><row><cell cols="2">2 150000</cell><cell>0</cell><cell>150000</cell><cell>0</cell></row><row><cell>3</cell><cell>0</cell><cell>74055</cell><cell>0</cell><cell>0</cell></row><row><cell>4</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>150000</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Number of data obtained by HD4C in each cluster compared to the ground truth.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Ground truth</cell><cell></cell></row><row><cell></cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell></row><row><cell>1</cell><cell>0</cell><cell>149989</cell><cell>0</cell><cell>0</cell></row><row><cell cols="2">2 150000</cell><cell>0</cell><cell>1</cell><cell>0</cell></row><row><cell>3</cell><cell>0</cell><cell>11</cell><cell>149999</cell><cell>0</cell></row><row><cell>4</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>150000</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Clustering evaluation criteria obtained with HD4C and K-means on real datasets.</figDesc><table><row><cell></cell><cell></cell><cell>HD4C</cell><cell>K-means</cell></row><row><cell></cell><cell cols="2">ARI Clusters</cell><cell>ARI</cell></row><row><cell cols="2">Accelerometers 0.50</cell><cell>8</cell><cell>0.11</cell></row><row><cell>Spectrums</cell><cell>0.34</cell><cell>9</cell><cell>0.32</cell></row><row><cell>6 Conclusion</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="7">Acknowledgements</head><p>The research leading to these results has received funds from the <rs type="funder">European Union</rs>'s <rs type="programName">Horizon 2020 Framework Programme for Research and Innovation</rs>, under grant agreement No. <rs type="grantNumber">732051</rs>.</p><p>This work was also supported by the '<rs type="programName">Infrastructure Biologie Santé</rs>' <rs type="projectName">PHENOME-EMPHASIS</rs> project (<rs type="grantNumber">ANR-11-INBS-0012</rs>) funded by the <rs type="funder">National Research Agency</rs> and the '<rs type="programName">Programme d'Investissements d'Avenir'</rs> <rs type="funder">(PIA)</rs>.</p><p>The accelerometer data come from CLOChèTE project, supported by <rs type="funder">CASDAR</rs> funds.</p><p>The spectrum dataset comes from <rs type="person">V. Baeten</rs>'s team at the <rs type="institution">Walloon Agricultural Research Centre (CRA-W)</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_mMTWHzq">
					<idno type="grant-number">732051</idno>
					<orgName type="program" subtype="full">Horizon 2020 Framework Programme for Research and Innovation</orgName>
				</org>
				<org type="funded-project" xml:id="_2a6nFkq">
					<idno type="grant-number">ANR-11-INBS-0012</idno>
					<orgName type="project" subtype="full">PHENOME-EMPHASIS</orgName>
					<orgName type="program" subtype="full">Infrastructure Biologie Santé</orgName>
				</org>
				<org type="funding" xml:id="_xMAY3D5">
					<orgName type="program" subtype="full">Programme d&apos;Investissements d&apos;Avenir&apos;</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Monte carlo simulation and clustering for customer segmentation in business organization</title>
		<author>
			<persName><forename type="first">A</forename><surname>Alamsyah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Nurriz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 3rd International Conference on Science and Technology -Computer (ICST)</title>
		<imprint>
			<date type="published" when="2017-07">July 2017</date>
			<biblScope unit="page" from="104" to="109" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A survey of outlier detection methodologies</title>
		<author>
			<persName><forename type="first">V</forename><surname>Hodge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Austin</surname></persName>
		</author>
		<idno type="DOI">10.1023/B:AIRE.0000045502.10941.a9</idno>
		<ptr target="https://doi.org/10.1023/B:AIRE.0000045502.10941.a9" />
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence Review</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="85" to="126" />
			<date type="published" when="2004-10">Oct 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A fast version of the k-means classification algorithm for astronomical applications</title>
		<author>
			<persName><forename type="first">I</forename><surname>Ordovás-Pascual</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sánchez Almeida</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Astronomy &amp; Astrophysics</title>
		<imprint>
			<biblScope unit="volume">565</biblScope>
			<biblScope unit="page">53</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">An introduction to statistical learning</title>
		<author>
			<persName><forename type="first">G</forename><surname>James</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Witten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hastie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Tibshirani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
			<publisher>Springer</publisher>
			<biblScope unit="volume">112</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Maximum likelihood from incomplete data via the em algorithm</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Dempster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">M</forename><surname>Laird</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Rubin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society. Series B</title>
		<imprint>
			<biblScope unit="page" from="1" to="38" />
			<date type="published" when="1977">1977</date>
		</imprint>
	</monogr>
	<note>methodological</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Estimating normal means with a dirichlet process prior</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Escobar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">89</biblScope>
			<biblScope unit="issue">425</biblScope>
			<biblScope unit="page" from="268" to="277" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Classification bayésienne non supervisée de données fonctionnelles</title>
		<author>
			<persName><forename type="first">D</forename><surname>Juery</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Abraham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Fontez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal de la Société Française de Statistique</title>
		<imprint>
			<biblScope unit="volume">155</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="185" to="201" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Limitations of principal components analysis for hyperspectral target recognition</title>
		<author>
			<persName><forename type="first">S</forename><surname>Prasad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">M</forename><surname>Bruce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geoscience and Remote Sensing Letters</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="625" to="629" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Time-series clustering-a decade review</title>
		<author>
			<persName><forename type="first">S</forename><surname>Aghabozorgi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Shirkhorshidi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">Y</forename><surname>Wah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Systems</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="page" from="16" to="38" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Clustering of time-series subsequences is meaningless: implications for previous and future research</title>
		<author>
			<persName><forename type="first">E</forename><surname>Keogh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowledge and Information Systems</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="154" to="177" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">K-means algorithms for functional data</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L L</forename><surname>García</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>García-Ródenas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Gómez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">151</biblScope>
			<biblScope unit="page" from="231" to="245" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Experiencing sax: a novel symbolic representation of time series</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Keogh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lonardi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Data Mining and Knowledge Discovery</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="107" to="144" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Generalized k-means-based clustering for temporal data under weighted and kernel time warp</title>
		<author>
			<persName><forename type="first">S</forename><surname>Soheily-Khah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Douzal-Chouakria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Gaussier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">75</biblScope>
			<biblScope unit="page" from="63" to="69" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">Y.-H</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-M</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-F</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.00382</idno>
		<title level="m">A clustering method for misaligned curves</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Some methods for classification and analysis of multivariate observations</title>
		<author>
			<persName><forename type="first">J</forename><surname>Macqueen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fifth Berkeley Symposium on Mathematical Statistics and Probability</title>
		<meeting>the Fifth Berkeley Symposium on Mathematical Statistics and Probability<address><addrLine>Oakland, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1967">1967</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="281" to="297" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Dirichlet process mixture models made scalable and effective by means of massive distribution</title>
		<author>
			<persName><forename type="first">K</forename><surname>Meguelati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Fontez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Hilgert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Masseglia</surname></persName>
		</author>
		<ptr target="https://hal.archives-ouvertes.fr/hal-01999453" />
	</analytic>
	<monogr>
		<title level="m">SAC: Symposium on Applied Computing</title>
		<meeting><address><addrLine>Limassol, Cyprus</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-04">Apr. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Parallel markov chain monte carlo for dirichlet process mixtures</title>
		<author>
			<persName><forename type="first">D</forename><surname>Lovell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">P</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Mansingka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Big Learning</title>
		<imprint>
			<publisher>NIPS</publisher>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Parallel markov chain monte carlo for nonparametric mixture models</title>
		<author>
			<persName><forename type="first">S</forename><surname>Williamson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Dubey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="98" to="106" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Scalable estimation of dirichlet process mixture models on distributed data</title>
		<author>
			<persName><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<ptr target="http://dl.acm.org/citation.cfm?id=3171837.3171935" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th International Joint Conference on Artificial Intelligence, ser. IJCAI&apos;17</title>
		<meeting>the 26th International Joint Conference on Artificial Intelligence, ser. IJCAI&apos;17</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="4632" to="4639" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A novel approximation to dynamic time warping allows anytime clustering of massive time series datasets</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Batista</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Rakthanmanon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Keogh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 SIAM international conference on data mining</title>
		<meeting>the 2012 SIAM international conference on data mining</meeting>
		<imprint>
			<publisher>SIAM</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="999" to="1010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Spark: Cluster computing with working sets</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zaharia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chowdhury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Franklin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shenker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Stoica</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HotCloud</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Radon-nikodym derivatives of gaussian measures</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Shepp</surname></persName>
		</author>
		<idno type="DOI">10.1214/aoms/1177699516</idno>
		<ptr target="https://doi.org/10.1214/aoms/1177699516" />
	</analytic>
	<monogr>
		<title level="j">Ann. Math. Statist</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="321" to="354" />
			<date type="published" when="1966-04">04 1966</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Unsupervised curve clustering using b-splines</title>
		<author>
			<persName><forename type="first">C</forename><surname>Abraham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Cornillon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Matzner-Løber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Molinari</surname></persName>
		</author>
		<idno type="DOI">10.1111/1467-9469.00350</idno>
		<ptr target="https://onlinelibrary.wiley.com/doi/abs/10.1111/1467-9469.00350" />
	</analytic>
	<monogr>
		<title level="j">Scandinavian Journal of Statistics</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="581" to="595" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Kernel principal component analysis for the classification of hyperspectral remote sensing data over urban areas</title>
		<author>
			<persName><forename type="first">M</forename><surname>Fauvel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chanussot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Benediktsson</surname></persName>
		</author>
		<idno type="DOI">10.1155/2009/783194</idno>
		<ptr target="http://dx.doi.org/10.1155/2009/783194" />
	</analytic>
	<monogr>
		<title level="j">EURASIP J. Adv. Signal Process</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="1" to="11" />
			<date type="published" when="2009-01">2009. Jan. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Time series cluster kernel for learning similarities between multivariate time series with missing data</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">Ø</forename><surname>Mikalsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">M</forename><surname>Bianchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Soguero-Ruiz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Jenssen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">76</biblScope>
			<biblScope unit="page" from="569" to="581" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Dudley</surname></persName>
		</author>
		<title level="m">Real analysis and probability. wadsworth &amp; brooks</title>
		<meeting><address><addrLine>California</addrLine></address></meeting>
		<imprint>
			<publisher>Cole, Pacific Groves</publisher>
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Gaussian Processes for Machine Learning</title>
		<author>
			<persName><forename type="first">C</forename><surname>Rasmussen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Williams</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
		<respStmt>
			<orgName>Massachusetts Institute of Technology</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Gaussian processes for machine learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Seeger</surname></persName>
		</author>
		<idno type="DOI">10.1142/S0129065704001899</idno>
		<idno>pMID: 15112367</idno>
		<ptr target="https://doi.org/10.1142/S0129065704001899" />
	</analytic>
	<monogr>
		<title level="j">International Journal of Neural Systems</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">02</biblScope>
			<biblScope unit="page" from="69" to="106" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Markov chain sampling methods for dirichlet process mixture models</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Neal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computational and Graphical Statistics</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="249" to="265" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Regression analysis of continuous parameter time series</title>
		<author>
			<persName><forename type="first">E</forename><surname>Parzen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. ISPASS</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Statistical inference on time series by hilbert space methods</title>
		<imprint>
			<date type="published" when="1959">1959</date>
		</imprint>
		<respStmt>
			<orgName>Stanford Univ CA Applied Mathematics and Statistics Labs, Tech. Rep.</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Probability density functionals and reproducing kernel hilbert spaces</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Symposium on Time Series Analysis</title>
		<meeting>the Symposium on Time Series Analysis<address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Wiley</publisher>
			<date type="published" when="1963">1963</date>
			<biblScope unit="volume">196</biblScope>
			<biblScope unit="page" from="155" to="169" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">The signal-noise problem-a solution for the case that signal and noise are gaussian and independent</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>Driscoll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Applied Probability</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="183" to="187" />
			<date type="published" when="1975">1975</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Reproducing kernel hilbert spaces of gaussian priors</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">W</forename><surname>Van Der Vaart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Van Zanten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pushing the limits of contemporary statistics: contributions in honor of Jayanta K. Ghosh</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="200" to="222" />
		</imprint>
		<respStmt>
			<orgName>Institute of Mathematical Statistics</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Numerical evaluation of reproducing kernel hilbert space inner products</title>
		<author>
			<persName><forename type="first">A</forename><surname>Oya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Navarro-Moreno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Ruiz-Molina</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Signal Processing</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1227" to="1233" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Mapreduce: Simplified data processing on large clusters</title>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ghemawat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="107" to="113" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">The hadoop distributed filesystem: Balancing portability and performance</title>
		<author>
			<persName><forename type="first">J</forename><surname>Shafer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Rixner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Cox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. ISPASS</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Analyzing time-course microarray data using functional data analysis-a review</title>
		<author>
			<persName><forename type="first">N</forename><surname>Coffey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hinde</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statistical Applications in Genetics and Molecular Biology</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Berlinet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Thomas-Agnan</surname></persName>
		</author>
		<title level="m">Reproducing kernel Hilbert spaces in probability and statistics</title>
		<imprint>
			<publisher>Springer Science &amp; Business Media</publisher>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Data analysis for numerical and categorical individual time-series</title>
		<author>
			<persName><forename type="first">G</forename><surname>Saporta</surname></persName>
		</author>
		<idno type="DOI">10.1002/asm.3150010204</idno>
		<ptr target="https://onlinelibrary.wiley.com/doi/abs/10.1002/asm.3150010204" />
	</analytic>
	<monogr>
		<title level="j">Applied Stochastic Models and Data Analysis</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="109" to="119" />
			<date type="published" when="1985">1985</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Bayesian density estimation and inference using mixtures</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Escobar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>West</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">90</biblScope>
			<biblScope unit="issue">430</biblScope>
			<biblScope unit="page" from="577" to="588" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Mllib: Machine learning in apache spark</title>
		<author>
			<persName><forename type="first">X</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bradley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Yavuz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Sparks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Venkataraman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Amde</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Owen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1235" to="1241" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Information theoretic measures for clusterings comparison: Variants, properties, normalization and correction for chance</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">X</forename><surname>Vinh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Epps</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bailey</surname></persName>
		</author>
		<ptr target="http://dl.acm.org/citation.cfm?id=1756006.1953024" />
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="2837" to="2854" />
			<date type="published" when="2010-12">Dec. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
