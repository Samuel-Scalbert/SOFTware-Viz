<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">On the Complexity of SHAP-Score-Based Explanations: Tractability via Knowledge Compilation and Non-Approximability Results</title>
				<funder ref="#_ap63UwZ">
					<orgName type="full">National Center for Artificial Intelligence CENIA</orgName>
				</funder>
				<funder ref="#_CTrsxkC #_yvNQT78 #_gK3s6Sq">
					<orgName type="full">Fondecyt</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2023-03-30">30 Mar 2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Marcelo</forename><surname>Arenas</surname></persName>
							<email>marenas@ing.puc.cl</email>
						</author>
						<author>
							<persName><forename type="first">Pablo</forename><surname>Barceló</surname></persName>
							<email>pbarcelo@uc.cl</email>
						</author>
						<author>
							<persName><forename type="first">Leopoldo</forename><surname>Bertossi</surname></persName>
							<email>leopoldo.bertossi@skema.edu</email>
						</author>
						<author>
							<persName><forename type="first">Mikaël</forename><surname>Monet</surname></persName>
							<email>mikael.monet@inria.fr</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Computer Science</orgName>
								<orgName type="department" key="dep2">Institute for Mathematical and Computational Engineering</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">School of Engineering &amp; Faculty of Mathematics</orgName>
								<orgName type="institution">Universidad Católica de Chile (</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">IMFD Chile</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">Institute for Mathematical and Computational Engineering</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="department">School of Engineering &amp; Faculty of Mathematics</orgName>
								<orgName type="institution">Universidad Católica de Chile (b) IMFD Chile</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="department">National Center for Artificial Intelligence</orgName>
								<orgName type="institution">CENIA Chile</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff6">
								<orgName type="department">SKEMA Business School</orgName>
								<address>
									<settlement>Montreal</settlement>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff7">
								<orgName type="laboratory">UMR 9189</orgName>
								<orgName type="institution" key="instit1">Univ. Lille</orgName>
								<orgName type="institution" key="instit2">Inria</orgName>
								<orgName type="institution" key="instit3">CNRS</orgName>
								<orgName type="institution" key="instit4">Centrale Lille</orgName>
								<address>
									<addrLine>-CRIStAL</addrLine>
									<postCode>F-59000</postCode>
									<settlement>Lille</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">On the Complexity of SHAP-Score-Based Explanations: Tractability via Knowledge Compilation and Non-Approximability Results</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2023-03-30">30 Mar 2023</date>
						</imprint>
					</monogr>
					<idno type="MD5">2CCC3B3069978ECCEBD1FE2F245B473B</idno>
					<idno type="arXiv">arXiv:2104.08015v2[cs.AI]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-04-12T14:47+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Explainable AI</term>
					<term>Shapley values</term>
					<term>SHAP score</term>
					<term>knowledge compilation</term>
					<term>FPRAS</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Scores based on Shapley values are widely used for providing explanations to classification results over machine learning models. A prime example of this is the influential SHAPscore, a version of the Shapley value that can help explain the result of a learned model on a specific entity by assigning a score to every feature. While in general computing Shapley values is a computationally intractable problem, we prove a strong positive result stating that the SHAP-score can be computed in polynomial time over deterministic and decomposable Boolean circuits under the so-called product distributions on entities. Such circuits are studied in the field of Knowledge Compilation and generalize a wide range of Boolean circuits and binary decision diagrams classes, including binary decision trees, Ordered Binary Decision Diagrams (OBDDs) and Free Binary Decision Diagrams (FBDDs). Our positive result extends even beyond binary classifiers, as it continues to hold if each feature is associated with a finite domain of possible values.</p><p>We also establish the computational limits of the notion of SHAP-score by observing that, under a mild condition, computing it over a class of Boolean models is always polynomially as hard as the model counting problem for that class. This implies that both determinism and decomposability are essential properties for the circuits that we consider, as removing one or the other renders the problem of computing the SHAP-score intractable (namely, #P-hard). It also implies that computing SHAP-scores is #P-hard even over the class of propositional formulas in DNF. Based on this negative result, we look for the existence of fully-polynomial randomized approximation schemes (FPRAS) for computing SHAP-scores over such class. In stark contrast to the model counting problem for DNF formulas, which admits an FPRAS, we prove that no such FPRAS exists (under widely</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Context. Explainable artificial intelligence has become an active area of research. Central to it is the observation that artificial intelligence (AI) and machine learning (ML) models cannot always be blindly applied without being able to interpret or explain their results. For example, someone who applies for a loan and sees the application rejected by an algorithmic decision-making system would like to receive from the system an explanation for this decision. In ML, explanations have been commonly considered for classification algorithms, and there are different approaches. In particular, explanations can be global -focusing on the general input/output relation of the model -, or local -focusing on how individual features affect the decision of the model for a specific input, as in the loan example above <ref type="bibr" target="#b42">(Ribeiro et al., 2016;</ref><ref type="bibr" target="#b31">Lundberg and Lee, 2017;</ref><ref type="bibr" target="#b11">Chen et al., 2018;</ref><ref type="bibr" target="#b8">Bertossi et al., 2020)</ref>. Recent literature has strengthened the importance of the latter by showing their ability to provide explanations that are often overlooked by global explanations <ref type="bibr" target="#b37">(Molnar, 2020)</ref>. In this work we concentrate on local explanations.</p><p>One way to define local explanations is by considering feature values as players in a coalition game that jointly contribute to the outcome. More concretely, one treats a feature value's contribution from the viewpoint of game theory, and ties it to the question of how to properly distribute wealth (profit) among collaborating players. This problem was approached in general terms in <ref type="bibr" target="#b47">Shapley (1953)</ref>. One can use the established concepts and techniques that he introduced in the context of cooperative game theory; and, more specifically, use the popular Shapley value as a measure of the contribution of a player to the common wealth associated with a multi-player game. It is well known that the Shapley value possesses properties that cast it as natural and intuitive. Actually, the Shapley value emerges as the only function that enjoys those desirable properties <ref type="bibr" target="#b43">(Roth, 1988)</ref>. The Shapley value has been widely applied in different disciplines, in particular in computer science <ref type="bibr" target="#b25">(Hunter and Konieczny, 2010;</ref><ref type="bibr" target="#b35">Michalak et al., 2013;</ref><ref type="bibr" target="#b10">Cesari et al., 2018;</ref><ref type="bibr" target="#b29">Livshits et al., 2020</ref><ref type="bibr" target="#b30">Livshits et al., , 2021))</ref>; and in machine learning it has been applied to the explanation of classification results, in its incarnation as the SHAP-score <ref type="bibr" target="#b31">(Lundberg and Lee, 2017;</ref><ref type="bibr" target="#b32">Lundberg et al., 2020)</ref>. Here, the players are the feature values of an entity under classification.</p><p>In this paper, we concentrate on the SHAP-score for classification models. It has a clear, intuitive, combinatorial meaning, and inherits all the good properties of the Shapley value. Accordingly, an explanation for a classification result takes the form of a set of feature values that have a high, hopefully maximum, SHAP-score. We remark that SHAP-scores have attracted the attention of the ML community and have found several applications and extensions <ref type="bibr" target="#b41">(Rathi, 2019;</ref><ref type="bibr" target="#b23">Fidel et al., 2020;</ref><ref type="bibr" target="#b8">Bertossi et al., 2020;</ref><ref type="bibr" target="#b34">Merrick and Taly, 2020;</ref><ref type="bibr" target="#b56">Takeishi and Kawahara, 2020;</ref><ref type="bibr" target="#b13">Covert and Lee, 2021;</ref><ref type="bibr" target="#b28">Kumar et al., 2020)</ref>. However, its fundamental and computational properties have not been investigated much.</p><p>Problems studied in the paper. For a given classification model M , entity e and feature x, the SHAP-score SHAP(M, e, x) intuitively represents the importance of the feature value e(x) to the classification result M (e). In its general formulation, SHAP(M, e, x) is a weighted average of differences of expected values of the outcomes (c.f. Section 2 for its formal definition). Unfortunately, computing quantities that are based on the notion of Shapley value is in general intractable. Indeed, in many scenarios the computation turns out to be #P-hard <ref type="bibr" target="#b21">(Faigle and Kern, 1992;</ref><ref type="bibr" target="#b18">Deng and Papadimitriou, 1994;</ref><ref type="bibr" target="#b30">Livshits et al., 2021;</ref><ref type="bibr" target="#b8">Bertossi et al., 2020)</ref>, which makes the notion difficult to use -if not impossible -for practical purposes <ref type="bibr" target="#b3">(Arora and Barak, 2009)</ref>. Therefore, natural questions are: "For what kinds of classification models the computation of the SHAP-score can be done efficiently?", "Can one obtain lower computational complexity if one has access to the internals of the classification model?", or again "In cases where exact computation is intractable, can we efficiently approximate the SHAP-score?".</p><p>In <ref type="bibr" target="#b32">Lundberg et al. (2020)</ref> the claim is made that for certain models based on decision trees the computation of the SHAP-score is tractable. In this work we go deeper into these results, in particular, formulating and establishing them in precise terms, and extending them for a larger class of classification models. We also identify classes of models for which SHAP-score computation is intractable. In such cases, we investigate the problem of existence and computation of a good approximation in the form of a fully polynomial-time randomized approximation scheme (FPRAS). Recall that FPRAS are tractable procedures that return an answer that is, with high probability, close to the correct answer.</p><p>Given the high computational complexity of the SHAP-score, one might try to solve related problems, other than the exact and approximate computation of all the features' scores, that could still be useful in practice. For instance, we consider the problem that consists in deciding, for a pair of feature values, which of the two has the highest score. This problem could indeed be used to compute a ranking of (all or the highest) SHAP-scores, without computing them explicitly. We also address this problem in this paper.</p><p>Model studied in the paper. We focus mainly on binary classifiers with binary feature values (i.e., propositional features that can take the values 0 or 1), and that return 1 (accept) or 0 (reject) for each entity. We will call these Boolean classifiers. The restriction to binary inputs can be relevant in many practical scenarios where the features are of a propositional nature. Still, we consider classifiers with possibly non-binary features, but binary outcomes, in Section 4. The second assumption that we make is that the underlying probability distributions on the population of entities are what we call product distributions, where each binary feature x has a probability p(x) of being equal to 1, independently of the other feature values. This includes, as a special case, the uniform probability distribution when each p(x) is 1 2 . Product distributions are also known as fully-factorized in the literature ( <ref type="bibr" target="#b59">Van den Broeck et al., 2021</ref><ref type="bibr" target="#b33">, 2022)</ref>. They have received considerable attention in the context of computing score-based explanations, as they combine good computational properties with enough flexibility to model relevant practical scenarios <ref type="bibr" target="#b54">(Strumbelj and Kononenko, 2010;</ref><ref type="bibr" target="#b17">Datta et al., 2016;</ref><ref type="bibr" target="#b31">Lundberg and Lee, 2017)</ref>.</p><p>Positive results on the complexity of computation of SHAP-scores in the paper are obtained for Boolean classifiers defined as deterministic and decomposable Boolean circuits. This is a widely studied model in knowledge compilation <ref type="bibr" target="#b14">(Darwiche, 2001;</ref><ref type="bibr" target="#b16">Darwiche and Marquis, 2002)</ref>. Such circuits encompass a wide range of Boolean circuits and binary decision diagrams classes that are considered in knowledge compilation, and more generally in AI. For instance, they generalize binary decision trees, ordered binary decision diagrams (OBDDs), free binary decision diagrams (FBDDs), and deterministic and decomposable negation normal norms (d-DNNFs) <ref type="bibr" target="#b14">(Darwiche, 2001;</ref><ref type="bibr" target="#b1">Amarilli et al., 2020;</ref><ref type="bibr" target="#b15">Darwiche and Hirth, 2020)</ref>. These circuits are also known under the name of tractable Boolean circuits, that is used in recent literature <ref type="bibr">(Shih et al., 2019a;</ref><ref type="bibr" target="#b48">Shi et al., 2020;</ref><ref type="bibr">Shih et al., 2018a</ref><ref type="bibr">Shih et al., ,b, 2019b;;</ref><ref type="bibr" target="#b39">Peharz et al., 2020)</ref>. Readers who are not familiar with knowledge compilation can simply think about deterministic and decomposable circuits as a tool for analyzing in a uniform manner the computational complexity of the SHAP-score on several Boolean classifier classes.</p><p>In turn, our negative results on the complexity of computation of SHAP-scores in the paper are obtained over the class of propositional formulas in DNF. In addition to being a well-known restriction of the class of propositional formulas for which the satisfiability problem is tractable, DNF formulas define an extension of deterministic and decomposable Boolean circuits. In fact, DNF formulas can be seen as decomposable, although not necessarily deterministic, Boolean circuits.</p><p>Our results. Our main contributions are the following.</p><p>1. Tractability for a large class of Boolean classifiers. We provide a polynomial time algorithm that computes the SHAP-score for deterministic and decomposable Boolean circuits under product distributions over the entity population (Theorem 2). We obtain as a corollary that the SHAP-score for Boolean classifiers given as binary decision trees, OBDDs, FBDDs and d-DNNFs can be computed in polynomial time.</p><p>2. Tractability for non-binary classifiers. We extend the aforementioned tractability result to the case of classifiers with non-binary features, which take the form of nonbinary Boolean circuits (Theorem 5). In these circuits, the nodes may now contain equalities of the form x = v, where x is a feature, and v is a value in the domain of x.</p><p>The outcome of the classifier is still binary.</p><p>3. Limits of tractability. We observe that, under a mild condition, computing the SHAPscore on Boolean classifiers in a class is always polynomially as hard as the model counting problem for that class (Lemma 7). This leads to intractability for the problem of computing the SHAP-score for all classes of Boolean classifiers for which model counting is intractable. An important example of this corresponds to the class of propositional formulas in DNF. As a corollary, we obtain that each one of the determinism assumption and the decomposability assumption is necessary for tractability (Theorem 6). These results even hold for the uniform distribution.</p><p>4. Non-approximability for DNF formulas. We give a simple proof that, under widely believed complexity assumptions, there is no FPRAS for the computation of the SHAPscore with Boolean classifiers represented as DNF formulas (Proposition 8). This holds even under the uniform distribution. This result establishes a stark contrast with the model counting problem for DNF formulas, which admits an FPRAS <ref type="bibr" target="#b26">(Karp et al., 1989)</ref>. We further strengthen this by showing that the non-approximability result even holds for Boolean formulas represented as 2-POS-DNF, i.e., with every conjunct containing at most two positive literals (and no negative literal), and for the uniform distribution (Theorem 9). The proof of this last result is quite involved and is based on a non-approximability result in relation to the size of cliques in graphs <ref type="bibr" target="#b22">(Feige et al., 1996;</ref><ref type="bibr" target="#b4">Arora and Safra, 1998;</ref><ref type="bibr">Arora et al., 1998)</ref>.</p><p>5. Impossibility of comparing SHAP-scores for DNF formulas. Consider the problem of verifying, given a DNF formula ϕ and features x, y, whether the SHAP-score of x in ϕ is smaller than the SHAP-score of y in ϕ. Under widely believed complexity assumptions, we establish that this problem of comparing two SHAP-scores cannot be approximated in polynomial time, even for the case of monotone DNF formulas (Theorem 16).</p><p>the SHAP-score. Besides, these studies do not usually contain lower bounds, since their focus is on obtaining tractable approximations via Monte Carlo or other sampling schemes. By contrast, obtaining lower bounds is the topic of our Sections 6 and 7, where we show the non-existence of an FPRAS for approximating the SHAP-score of DNF formulas, as well as the non-membership in BPP of comparing SHAP-scores for such formulas. This paper is a considerable extension of the conference paper <ref type="bibr" target="#b2">(Arenas et al., 2021)</ref>. In addition to containing full proofs of all the results of <ref type="bibr" target="#b2">(Arenas et al., 2021)</ref>, we present here several new results. Among them, we provide a detailed analysis of approximability of the SHAP-score, a complexity analysis of the problem of comparing SHAP-scores, and an extension of the results for deterministic and decomposable Boolean circuits to the case of non-binary features.</p><p>Paper structure. We give preliminaries in Section 2. In Section 3, we prove that the SHAP-score can be computed in polynomial time for deterministic and decomposable Boolean circuits under product probability distributions. We extend this tractability result in Section 4 to non-binary deterministic and decomposable Boolean circuits. In Section 5, we establish the computational limits of the exact computation of the SHAP-score, while Section 6 studies the (non-) approximability properties of this score. The problem of comparing the SHAP-scores of different features is studied in Section 7. We conclude and discuss future work in Section 8.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Preliminaries</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Entities, distributions and classifiers</head><p>Let X be a finite set of binary 1 features, also called variables. An entity over X is a function e : X → {0, 1}.<ref type="foot" target="#foot_0">2</ref> We denote by ent(X) the set of all entities over X. On this set, we consider probability distributions that we call product distributions, defined as follows. Let p : X → [0, 1] be a function that associates to every feature x ∈ X a probability value p(x) ∈ [0, 1]. Then, the product distribution generated by p is the probability distribution Π p over ent(X) such that, for every e ∈ ent(X) we have</p><formula xml:id="formula_0">Π p (e) := x∈X e(x)=1 p(x) • x∈X e(x)=0 (1 -p(x)) .</formula><p>That is, Π p is the product distribution that is determined by pre-specified marginal distributions, and that makes the features take values independently from each other. We denote by U the uniform probability distribution, i.e., for every entity e ∈ ent(X), we have that U(e) := 1 2 |X| . Note that the uniform distribution can be obtained as a special case of product distribution, with Π p invoking p(x) := 1 /2 for every x ∈ X.</p><p>A Boolean classifier M over X is a function M : ent(X) → {0, 1} that maps every entity over X to 0 or 1. We say that M accepts an entity e when M (e) = 1, and that it rejects it if M (e) = 0. Since we consider ent(X) to be a probability space, M can be regarded as a random variable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2.2</head><p>The SHAP-score over Boolean classifiers Let M : ent(X) → {0, 1} be a Boolean classifier over the set X of features. Given an entity e over X and a subset S ⊆ X of features, we define the set cw(e, S) of entities that are consistent with e on S as cw(e, S) := {e ∈ ent(X) | e (x) = e(x) for each x ∈ S}. Then, given an entity e ∈ ent(X), a probability distribution D over ent(X), and S ⊆ X, we define the expected value of M over X \ S with respect to e under D as In other words, φ D (M, e, S) is the expected value of M , conditioned on the inputs to coincide with e over each feature in S. For instance, if we take D to be the uniform distribution U over ent(X), this expression simplifies to φ U (M, e, S) = e ∈cw(e,S)</p><formula xml:id="formula_1">1 2 |X\S| M (e ).</formula><p>The function φ D is then used in the general formula of the Shapley value <ref type="bibr" target="#b47">(Shapley, 1953;</ref><ref type="bibr" target="#b43">Roth, 1988)</ref> to obtain the SHAP-score for feature values in e, as follows.</p><p>Definition 1 (Lundberg and Lee (2017)) Given a Boolean classifier M over a set of features X, a probability distribution D on ent(X), an entity e over X, and a feature x ∈ X, the SHAP score of feature x on e with respect to M under D is defined as</p><formula xml:id="formula_2">SHAP D (M, e, x) := S⊆X\{x} |S|! (|X| -|S| -1)! |X|! φ D (M, e, S∪{x})-φ D (M, e, S) . (1)</formula><p>In Section 5, we will use another equivalent expression of the SHAP-score, that we introduce now. For a permutation π : X → {1, . . . , n} and x ∈ X, let S x π denote the set of features that appear before x in π. Formally, S x π := {y ∈ X | π(y) &lt; π(x)}. Then, letting Π(X) be the set of all permutations π : X → {1, . . . , n}, observe that Equation (1) can be rewritten as</p><formula xml:id="formula_3">SHAP D (M, e, x) = 1 |X|! π∈Π(X) φ D (M, e, S x π ∪ {x}) -φ D (M, e, S x π ) .<label>(2)</label></formula><p>Thus, SHAP D (M, e, x) is a weighted average of the contribution of feature x on e to the classification result, i.e., of the differences between having it and not, under all possible permutations of the other feature values. Observe that, from this definition, a high positive value of SHAP D (M, e, x) intuitively means that setting x to e(x) strongly leans the classifier towards acceptance, while a high negative value of SHAP D (M, e, x) means that setting x to e(x) strongly leans the classifier towards rejection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Deterministic and decomposable Boolean circuits</head><p>A Boolean circuit over a set of variables X is a directed acyclic graph C such that (i) Every node without incoming edges is either a variable gate or a constant gate. A variable gate is labeled with a variable from X, and a constant gate is labeled with either 0 or 1;</p><p>(ii) Every node with incoming edges is a logic gate, and is labeled with a symbol ∧, ∨ or ¬. If it is labeled with the symbol ¬, then it has exactly one incoming edge; 3</p><p>(iii) Exactly one node does not have any outgoing edges, and this node is called the output gate of C.</p><p>Such a Boolean circuit C represents a Boolean classifier in the expected way -we assume the reader to be familiar with Boolean logic -, and we write C(e) for the value in {0, 1} of the output gate of C when we evaluate C over the entity e. We consider the size |C| of the circuit to be its number of edges. Observe that, thanks to condition (iii), the number of gates of C is at most its number of edges plus one.</p><p>Several restrictions of Boolean circuits with good computational properties have been studied. Let C be a Boolean circuit over a set of variables X and g a gate of C. The Boolean circuit C g over X is defined by considering the subgraph of C induced by the set of gates g in C for which there exists a path from g to g in C. Notice that g is the output gate of C g . Then, an ∨-gate g of C is said to be deterministic if for every pair g 1 , g 2 of distinct input gates of g, the Boolean circuits C g 1 and C g 2 are disjoint in the sense that there is no entity e that is accepted by both C g 1 and C g 2 (that is, there is no entity e ∈ ent(X) such that C g 1 (e) = C g 2 (e) = 1). The circuit C is called deterministic if every ∨-gate of C is deterministic. The set var(g) is defined as the set of variables x ∈ X such that there exists a variable gate with label x in C g . An ∧-gate g of C is said to be decomposable, if for every pair g 1 , g 2 of distinct input gates of g, we have that var(g 1 ) ∩ var(g 2 ) = ∅. Then C is called decomposable if every ∧-gate of C is decomposable.</p><p>Example 1 We want to classify papers submitted to a conference as rejected (Boolean value 0) or accepted (Boolean value 1). Papers are described by propositional features fg, dtr, nf and na, which stand for "follows guidelines", "deep theoretical result", "new framework" and "nice applications", respectively. The Boolean classifier for the papers is given by the Boolean circuit in Figure <ref type="figure" target="#fig_1">1</ref>. The input of this circuit are the features fg, dtr, nf and na, each of which can take value either 0 or 1, depending on whether the feature is present (1) or absent (0). The nodes with labels ¬, ∨ or ∧ are logic gates, and the associated Boolean value of each one of them depends on the logical connective represented by its label and the Boolean values of its inputs. The output value of the circuit is given by the top node in the figure . 
The Boolean circuit in Figure <ref type="figure" target="#fig_1">1</ref> is decomposable, because for each ∧-gate the sets of features of its inputs are pairwise disjoint. For instance, in the case of the top node in Figure <ref type="figure" target="#fig_1">1</ref>, the left-hand side input has {fg} as its set of features, while its right-hand side input 3. Recall that the fan-in of a gate is the number of its input gates. In our definition of Boolean circuits, we allow unbounded fan-in ∧-and ∨-gates. has {dtr, nf, na} as its set of features, which are disjoint. Also, this circuit is deterministic as for every ∨-gate two (or more) of its inputs cannot be given value 1 by the same Boolean assignment for the features. For instance, in the case of the only ∨-gate in Figure <ref type="figure" target="#fig_1">1</ref>, if a Boolean assignment for the features gives value 1 to its left-hand side input, then feature dtr has to be given value 1 and, thus, such an assignment gives value 0 to the right-hand side input of the ∨-gate. In the same way, it can be shown that if a Boolean assignment for the features gives value 1 to the right-hand side input of this ∨-gate, then it gives value 0 to its left-hand side input.</p><p>We will use the fact that deterministic and decomposable Boolean circuits are closed under conditioning. Let C be a Boolean circuit over variables X, and let x ∈ X. We denote by C +x (resp., C -x ) the Boolean circuit that is obtained from C by replacing every variable gate that is labeled with x by a constant 1-gate (resp, by a constant 0-gate). In the literature, C +x (resp., C -x ) is called the conditioning by x (resp., by ¬x) of C. It is easy to check that, if C is deterministic and decomposable, then so are C +x and C -x .</p><p>As mentioned in the introduction, deterministic and decomposable Boolean circuits generalize many decision diagrams and Boolean circuits classes. We refer to <ref type="bibr" target="#b14">(Darwiche, 2001;</ref><ref type="bibr" target="#b1">Amarilli et al., 2020)</ref> for detailed studies of knowledge compilation classes and of their precise relationships. For the reader's convenience, we explain in Appendix A how FBDDs and binary decision trees can be encoded in linear time as deterministic and decomposable Boolean circuits.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Complexity classes and encoding of probability values</head><p>In Section 5, we will consider the counting complexity class #P <ref type="bibr" target="#b57">(Valiant, 1979)</ref> of problems that can be expressed as the number of accepting paths of a nondeterministic Turing machine running in polynomial time. Following <ref type="bibr" target="#b57">Valiant (1979)</ref>, we define #P-hardness using Turing reductions. Prototypical examples of #P-complete problems are counting the number of assignments that satisfy a propositional formula and counting the number of three-colorings of a graph. While it is known that FPTIME ⊆ #P, where FPTIME is the class of functions that can be computed in polynomial time, this inclusion is widely believed to be strict. Therefore, proving that a problem is #P-hard implies, under such an assumption, that it cannot be solved in polynomial time.</p><p>In Sections 6 and 7 we will use the complexity classes RP and BPP. Recall that RP is the class of decision problems L for which there exists a polynomial-time probabilistic Turing Machine M such that: (a) if x ∈ L, then M accepts with probability at least 3 /4; and (b) if x ∈ L, then M does not accept x. Moreover, BPP is defined exactly as RP but with condition (b) replaced by: (b') if x ∈ L, then M accepts with probability at most 1 /4. Thus, BPP is defined as RP but allowing errors for both the elements that are and are not in L. Hence, PTIME ⊆ RP ⊆ BPP by definition. Besides, it is known that RP ⊆ NP, and this inclusion is widely believed to be strict. Finally, it is not known whether BPP ⊆ NP or NP ⊆ BPP, but it is widely believed that NP is not included in BPP.</p><p>Finally, when considering problems where probabilities can be part of the input (such as in Theorem 2 or Theorem 5), it will always be implicit that such probabilities are given as rational numbers p /q for p, q ∈ N, encoded as ordered pairs (p, q) where p and q are themselves encoded in binary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Tractable Computation of the SHAP-Score</head><p>In this section we prove our main tractability result, namely, that computing the SHAPscore for Boolean classifiers given as deterministic and decomposable Boolean circuits can be done in polynomial time for product probability distributions.</p><p>Theorem 2 Given as input a deterministic and decomposable circuit C over a set of features X, rational probability values p(x) for every feature x ∈ X, an entity e : X → {0, 1}, and a feature x ∈ X, the value SHAP Πp (C, e, x) can be computed in polynomial time.</p><p>In particular, since binary decision trees, OBDDs, FBDDs and d-DNNFs are all restricted types of deterministic and decomposable circuits, we obtain as a consequence of Theorem 2 that this problem is also in polynomial time for these classes. For instance, for binary decision trees we obtain the following result.</p><p>Corollary 3 Given as input a binary decision tree T over a set of features X, rational probability values p(x) for every feature x ∈ X, an entity e : X → {0, 1}, and a feature x ∈ X, the value SHAP Πp (T, e, x) can be computed in polynomial time.</p><p>The authors of <ref type="bibr" target="#b32">(Lundberg et al., 2020)</ref> provide an algorithm for computing the SHAPscore in polynomial time for decision trees, but, unfortunately, as stated in <ref type="bibr" target="#b59">(Van den Broeck et al., 2021)</ref>, this algorithm is not correct. Moreover, it is important to notice that Theorem 2 is a nontrivial extension of the result for decision trees, as it is known that deterministic and decomposable circuits can be exponentially more succinct than binary decision trees (in fact, than FBDDs) at representing Boolean classifiers <ref type="bibr" target="#b14">(Darwiche, 2001;</ref><ref type="bibr" target="#b1">Amarilli et al., 2020)</ref>.</p><p>We prove Theorem 2 in the remaining of this section. First, we prove in Section 3.1 that the problem can be solved in polynomial time, and we extract from this proof a first version of our algorithm. Then, in Section 3.2, we provide an optimized version of this algorithm, and argue why the proposed optimization reduces the complexity of the algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Polynomial time computability proof</head><p>In this section we prove that the SHAP-score can be computed in polynomial time. For a Boolean classifier M over a set of variables X, probability distribution D over ent(X), entity e ∈ ent(X), and natural number k ≤ |X|, we define the quantity</p><formula xml:id="formula_4">H D (M, e, k) := S⊆X |S|=k E e ∼D [M (e ) | e ∈ cw(e, S)].</formula><p>Our proof of Theorem 2 is divided into two modular parts. The first part, which is developed in Section 3.1.1, consists in showing that the problem of computing</p><formula xml:id="formula_5">SHAP Π• (•, •, •) can be reduced in polynomial time to that of computing H Π• (•, •, •</formula><p>). This part of the proof is a sequence of formula manipulations, and it only uses the fact that deterministic and decomposable circuits are closed under conditioning on a variable value (recall the definition of conditioning from Section 2.3). In the second part of the proof, which is developed in Section 3.1.2, we show that computing H Π• (•, •, •) can be done in polynomial time for deterministic and decomposable Boolean circuits. It is in this part that the properties of deterministic and decomposable circuits are used. Finally, we extract Algorithm 1 from this proof in Section 3.1.3.</p><formula xml:id="formula_6">3.1.1 Reducing in polynomial-time from SHAP Π• (•, •, •) to H Π• (•, •, •)</formula><p>In this section we show that for deterministic and decomposable Boolean circuits, and under product distributions, the computation of the SHAP-score can be reduced in polynomial time to the computation of</p><formula xml:id="formula_7">H Π• (•, •, •).</formula><p>Suppose then that we wish to compute SHAP Πp (C, e, x), for a given deterministic and decomposable circuit C over a set of variables X, probability mapping p : X → [0, 1], entity e ∈ ent(X), and feature x ∈ X. Define</p><formula xml:id="formula_8">Diff k (C, e, x) := S⊆X\{x} |S|=k (φ Πp (C, e, S ∪ {x}) -φ Πp (C, e, S)),</formula><p>and let n = |X|. We then have, by definition, that</p><formula xml:id="formula_9">SHAP Πp (C, e, x) = S⊆X\{x} |S|!(n -|S| -1)! n! (φ Πp (C, e, S ∪ {x}) -φ Πp (C, e, S)) = n-1 k=0 S⊆X\{x} |S|=k k!(n -k -1)! n! (φ Πp (C, e, S ∪ {x}) -φ Πp (C, e, S)) = n-1 k=0 k!(n -k -1)! n! Diff k (C, e, x).<label>(3)</label></formula><p>Observe that all arithmetical terms (such as k! or n!) can be computed in polynomial time: this is simply because n is given in unary, as it is bounded by the size of the cir- To continue, we need to introduce some notation. For a set of features X and S ⊆ X, we write p |S : S → [0, 1] for the mapping that is the restriction of p to S, and Π p |S : ent(S) → [0, 1] for the corresponding product distribution on ent(S). Similarly, for an entity e ∈ ent(X) and S ⊆ X, let e |S be the entity over S that is obtained by restricting e to the domain S (that is, formally e |S ∈ ent(S) and e |S (y) := e(y) for every y ∈ S). Now, remembering from Section 2.3 the definition of C +x and C -x , we obtain that</p><formula xml:id="formula_10">R = p(x) • S⊆X\{x} |S|=k E e ∼Πp |X\{x} [C +x (e ) | e ∈ cw(e |X\{x} , S)] + (1 -p(x)) • S⊆X\{x} |S|=k E e ∼Πp |X\{x} [C -x (e ) | e ∈ cw(e |X\{x} , S)] = p(x) • H Πp |X\{x} (C +x , e |X\{x} , k) + (1 -p(x)) • H Πp |X\{x} (C -x , e |X\{x} , k),<label>(5)</label></formula><p>where the last equality is obtained simply by using the definition of</p><formula xml:id="formula_11">H Π• (•, •, •). Hence, if we could compute in polynomial time H Π• (•, •, •)</formula><p>for deterministic and decomposable Boolean circuits, then we could compute R in polynomial time as C +x and C -x can be computed in linear time from C, and they are deterministic and decomposable Boolean circuits as well.</p><p>We now inspect the term L, which we recall is</p><formula xml:id="formula_12">L = S⊆X\{x} |S|=k E e ∼Πp [C(e ) | e ∈ cw(e, S ∪ {x})].</formula><p>Observe that, for S ⊆ X \ {x} and e ∈ cw(e, S ∪ {x}), it holds that</p><formula xml:id="formula_13">C(e ) = C +x (e |X\{x} ) if e(x) = 1 C -x (e |X\{x} ) if e(x) = 0 .</formula><p>Therefore, if e(x) = 1, we have that</p><formula xml:id="formula_14">L = S⊆X\{x} |S|=k E e ∼Πp |X\{x} [C +x (e ) | e ∈ cw(e |X\{x} , S)] = H Πp |X\{x} (C +x , e |X\{x} , k)<label>(6)</label></formula><p>whereas if e(x) = 0, we have that</p><formula xml:id="formula_15">L = H Πp |X\{x} (C -x , e |X\{x} , k).<label>(7)</label></formula><p>Hence, again, if we were able to compute in polynomial time H Π• (•, •, •) for deterministic and decomposable Boolean circuits, then we could compute L in polynomial time (as deterministic and decomposable Boolean circuits C +x and C -x can be computed in linear time from C). But then we deduce from (4) that Diff k (C, e, x) could be computed in polynomial time for each k ∈ {0, . . . , n -1}, from which we have that SHAP Πp (C, e, x) could be computed in polynomial time (by Equation ( <ref type="formula" target="#formula_9">3</ref>)), therefore concluding the existence of the reduction claimed in this section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Computing H Π• (•, •, •) in polynomial time</head><p>We now take care of the second part of the proof of tractability, i.e., proving that computing H Π• (•, •, •) for deterministic and decomposable Boolean circuits can be done in polynomial time. Formally, in this section we prove the following lemma:</p><p>Lemma 4 The following problem can be solved in polynomial time. Given as input a deterministic and decomposable Boolean circuit C over a set of variables X, rational probability values p(x) for each x ∈ X, an entity e ∈ ent(X), and a natural number k ≤ |X|, compute the quantity H Πp (C, e, k).</p><p>We first perform two preprocessing steps on C.</p><p>Rewriting to fan-in 2. First, we modify the circuit C so that the fan-in of every ∨-and ∧gate is exactly 2. This can be done in linear time simply by rewriting every ∧-gate (resp., and ∨-gate) of fan-in m &gt; 2 with a chain of m -1 ∧-gates (resp., ∨-gates) of fan-in 2, and by attaching to each ∧ or ∨-gate of fan-in 1 a constant gate of the appropriate type. It is clear that the resulting Boolean circuit is deterministic and decomposable. Hence, from now on we assume that the fan-in of every ∨-and ∧-gate of C is exactly 2.</p><p>Smoothing the circuit. A deterministic and decomposable circuit C is smooth <ref type="bibr" target="#b14">(Darwiche, 2001;</ref><ref type="bibr">Shih et al., 2019b)</ref> if for every ∨-gate g and input gates g 1 , g 2 of g, we have that var(g 1 ) = var(g 2 ) (we call such an ∨-gate smooth). Recall that by the previous paragraph, we assume that the fan-in of every ∨-gate is exactly 2. We then repeat the following operation until C becomes smooth. For an ∨-gate g of C having two input gates g 1 , g 2 violating the smoothness condition, define S 1 := var(g 1 ) \ var(g 2 ) and S 2 := var(g 2 ) \ var(g 1 ), and let d S 1 , d S 2 be Boolean circuits defined as follows.</p><p>If </p><formula xml:id="formula_16">S 1 = ∅, then d S 1 consist of the</formula><formula xml:id="formula_17">∧ d S 2 ) = var(g 2 ∧ d S 1 ) = var(g 1 ) ∪ var(g 2 ),</formula><p>we have that g is now smooth. Therefore, the circuit that we obtain after this operation is equivalent to the one we started from, is again deterministic and decomposable, and contains one less non-smooth ∨-gate. Hence, by repeating this operation for each non-smooth ∨-gate, which we can do in polynomial time, we obtain an equivalent smooth deterministic and decomposable circuit where each ∨-and ∧-gate has fan-in exactly 2. Thus, from now on we also assume that C is smooth.</p><p>For a gate g of C, let R g be the Boolean circuit over var(g) that is defined by considering the subgraph of C induced by the set of gates g in C for which there exists a path from g to g in C. 4 Notice that R g is a deterministic and decomposable Boolean circuit with output gate g. Moreover, for a gate g and natural number ≤ |var(g)|, define α g := H Πp |var(g) (R g , e |var(g) , ), which we recall is equal, by definition of</p><formula xml:id="formula_18">H Π• (•, •, •), to H Πp |var(g) (R g , e |var(g) , ) = S⊆var(g) |S|= E e ∼Πp |var(g) [R g (e ) | e ∈ cw(e |var(g) , S)].</formula><p>We will show how to compute all the values α g for every gate g of C and ∈ {0, . . . , |var(g)|} in polynomial time. This will conclude the proof of Lemma 4 since, for the output gate g out of C, we have that α k gout = H Πp (C, e, k). Next we explain how to compute these values by bottom-up induction on C.</p><p>Variable gate. g is a variable gate with label y ∈ X, so that var(g) = {y}. Then for e ∈ ent({y}) we have R g (e ) = e (y), therefore </p><formula xml:id="formula_19">α 0 g = S⊆{y} |S|=0 E e ∼Πp |{y} [</formula><p>Constant gate. g is a constant gate with label a ∈ {0, 1}, and var(g) = ∅. We recall the mathematical convention that there is a unique function with the empty domain and, hence, a unique entity over ∅. But then</p><formula xml:id="formula_21">α 0 g = S⊆∅ |S|=0 E e ∼Πp |∅ [a | e ∈ cw(e |∅ , S)] = E e ∼Πp |∅ [a | e ∈ cw(e |∅ , ∅)] = a.<label>(10)</label></formula><p>¬-gate. g is a ¬-gate with input gate g . Notice that var(g) = var(g ). Then, since for e ∈ ent(var(g)) we have that R g (e ) = 1 -R g (e ), we have</p><formula xml:id="formula_22">α g = S⊆var(g) |S|= E e ∼Πp |var(g) [1 -R g (e ) | e ∈ cw(e |var(g) , S)].</formula><p>By linearity of expectations we deduce that</p><formula xml:id="formula_23">α g = S⊆var(g) |S|= E e ∼Πp |var(g) [1 | e ∈ cw(e |var(g) , S)] - S⊆var(g) |S|= E e ∼Πp |var(g) [R g (e ) | e ∈ cw(e |var(g) , S)] = S⊆var(g) |S|= 1 -α g = |var(g)| -α g (11)</formula><p>for every ∈ {0, . . . , |var(g)|}. By induction, the values α g for ∈ {0, . . . , |var(g)|} have already been computed. Thus, we can compute all the values α g for ∈ {0, . . . , |var(g)|} in polynomial time.</p><p>∨-gate. g is an ∨-gate. By assumption, recall that g is deterministic, smooth, and has fanin exactly 2. Let g 1 and g 2 be the input gates of g, and recall that var(g 1 ) = var(g 2 ) = var(g), because g is smooth. Given that g is deterministic, observe that for every e ∈ ent(var(g)) we have R g (e ) = R g 1 (e ) + R g 2 (e ). But then for ∈ {0, . . . , |var(g)|} we have</p><formula xml:id="formula_24">α g = S⊆var(g) |S|= E e ∼Πp |var(g) [R g 1 (e ) + R g 2 (e ) | e ∈ cw(e |var(g) , S)] = S⊆var(g) |S|= E e ∼Πp |var(g) [R g 1 (e ) | e ∈ cw(e |var(g) , S)] + S⊆var(g) |S|= E e ∼Πp |var(g) [R g 2 (e ) | e ∈ cw(e |var(g) , S)] = α g 1 + α g 2 , (<label>12</label></formula><formula xml:id="formula_25">)</formula><p>where the second equality is by linearity of the expectation, and the last equality is valid because g is smooth (in particular, we have that var(g 1 ) = var(g 2 ) = var(g)). By induction, the values α g 1 and α g 2 , for each ∈ {0, . . . , |var(g)|}, have already been computed. Therefore, we can compute all the values α g for ∈ {0, . . . , |var(g)|} in polynomial time.</p><p>∧-gate. g is an ∧-gate. By assumption, recall that g is decomposable and has fan-in exactly 2. Let g 1 and g 2 be the input gates of g. For e ∈ ent(var(g)) we have that R g (e ) = R g 1 (e |var(g 1 ) ) • R g 2 (e |var(g 2 ) ). Moreover, since var(g) = var(g 1 ) ∪ var(g 2 ) and var(g 1 ) ∩ var(g 2 ) = ∅ (because g is decomposable), observe that every S ⊆ var(g) can be uniquely decomposed into S 1 ⊆ var(g 1 ), S 2 ⊆ var(g 2 ) such that S = S 1 ∪ S 2 and S 1 ∩ S 2 = ∅. Therefore, for ∈ {0, . . . , |var(g)|} we have</p><formula xml:id="formula_26">α g = S 1 ⊆var(g 1 ) |S 1 |≤ S 2 ⊆var(g 2 ) |S 2 |= -|S 1 | E e ∼Πp |var(g) [R g 1 (e |var(g 1 ) ) • R g 2 (e |var(g 2 ) ) | e ∈ cw(e |var(g) , S 1 ∪ S 2 )].</formula><p>But, by definition of the product distribution Π p |var(g) and because g is decomposable, we have that R g 1 (e |var(g 1 ) ) and R g 2 (e |var(g 2 ) ) are independent random variables, hence we deduce</p><formula xml:id="formula_27">α g = S 1 ⊆var(g 1 ) |S 1 |≤ S 2 ⊆var(g 2 ) |S 2 |= -|S 1 | E e ∼Πp |var(g) [R g 1 (e |var(g 1 ) ) | e ∈ cw(e |var(g) , S 1 ∪ S 2 )] • E e ∼Πp |var(g) [R g 2 (e |var(g 2 ) ) | e ∈ cw(e |var(g) , S 1 ∪ S 2 )] . = S 1 ⊆var(g 1 ) |S 1 |≤ S 2 ⊆var(g 2 ) |S 2 |= -|S 1 | E e ∼Πp |var(g 1 ) [R g 1 (e ) | e ∈ cw(e |var(g 1 ) , S 1 )] • E e ∼Πp |var(g 2 ) [R g 2 (e ) | e ∈ cw(e |var(g 2 ) , S 2 )] ,</formula><p>where the last equality is simply by definition of the product distributions, and because R g 1 (e |var(g 1 ) ) is independent of the value e |var(g 2 ) , and similarly for R g 2 (e |var(g 2 ) ). But then, using the convention that α i g i = 0 when i &gt; |var(g i )|, for i = 1, 2, we obtain that</p><formula xml:id="formula_28">α g = S 1 ⊆var(g 1 ) |S 1 |≤ E e ∼Πp |var(g 1 ) [R g 1 (e ) | e ∈ cw(e |var(g 1 ) , S 1 )] • S 2 ⊆var(g 2 ) |S 2 |= -|S 1 | E e ∼Πp |var(g 2 ) [R g 2 (e ) | e ∈ cw(e |var(g 2 ) , S 2 )] = S 1 ⊆var(g 1 ) |S 1 |≤ E e ∼Πp |var(g 1 ) [R g 1 (e ) | e ∈ cw(e |var(g 1 ) , S 1 )] • α -|S 1 | g 2 = 1 =0 α -1 g 2 • S 1 ⊆var(g 1 ) |S 1 |= 1 E e ∼Πp |var(g 1 ) [R g 1 (e ) | e ∈ cw(e |var(g 1 ) , S 1 )] = 1 =0 α -1 g 2 • α 1 g 1 = 1 ∈{0,...,min( ,|var(g 1 )|)} 2 ∈{0,...,min( ,|var(g 2 )|)} 1 + 2 = α 1 g 1 • α 2 g 2 . (<label>13</label></formula><formula xml:id="formula_29">)</formula><p>By induction, the values α 1 g 1 and α 2 g 2 , for each 1 ∈ {0, . . . , |var(g 1 )|} and 2 ∈ {0, . . . , |var(g 2 )|}, have already been computed. Therefore, we can compute all the values α g for ∈ {0, . . . , |var(g)|} in polynomial time.</p><p>This concludes the proof of Lemma 4 and, hence, the proof that SHAP-score can be computed in polynomial time for our circuits.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.3">Extracting an algorithm from the proof</head><p>From the previous proof, it is possible to extract Algorithm 1, which is more amenable to implementation. The main idea of this algorithm is that values γ g correspond to values α g of the proof for the circuit D +x , while values δ g correspond to values α g of the proof for the circuit D -x . In lines 3-27, these values are computed by bottom-up induction over the circuit D, following the relations that we obtained in Equations ( <ref type="formula">8</ref>)-( <ref type="formula" target="#formula_28">13</ref>) (but specialized to the circuits D +x and D -x , as can be seen from lines 6-8, and by the fact that indices are always in {0, . . . , |var(g) \ {x}|}). Hence, it only remains to show that the returned value of the algorithm is correct. To see that, observe that we can rewrite Equations ( <ref type="formula" target="#formula_14">6</ref>) and ( <ref type="formula" target="#formula_15">7</ref>) into</p><formula xml:id="formula_30">L = e(x) • H Πp |X\{x} (D +x , e |X\{x} , k) + (1 -e(x)) • H Πp |X\{x} (D -x , e |X\{x} , k),</formula><p>Algorithm 1: SHAP-scores for deterministic and decomposable Boolean circuits (intermediate)</p><p>Input : Deterministic and decomposable Boolean circuit C over features X with output gate g out , rational probability values p(x) for all x ∈ X, entity e ∈ ent(X), and feature x ∈ X. Output: The value SHAP(C, e, x) under the probability distribution Π p .</p><p>1 Transform C into an equivalent smooth circuit D where each ∨-gate and ∧-gate has fan-in exactly 2;</p><p>2 Compute the set var(g) for every gate g in D;</p><p>3 Compute values γ g and δ g for every gate g in D and ∈ {0, . . . , |var(g) \ {x}|} by bottom-up induction on D as follows:</p><formula xml:id="formula_31">4 if g is a constant gate with label a ∈ {0, 1} then 5 γ 0 g , δ 0 g ← a;</formula><p>6 else if g is a variable gate with var(g) = {x} then</p><formula xml:id="formula_32">7 γ 0 g ← 1; 8 δ 0 g ← 0;</formula><p>9 else if g is a variable gate with var(g) = {y} and y = x then γ 0 g , δ 0 g ← p(y);</p><formula xml:id="formula_33">γ 1 g , δ 1 g ← e(y);</formula><p>else if g is a ¬-gate with input gate g then for ∈ {0, . . . , |var(g) \ {x}|} do</p><formula xml:id="formula_34">γ g ← |var(g)\{x}| -γ g ; δ g ← |var(g)\{x}| -δ g ;</formula><p>end else if g is an ∨-gate with input gates g 1 , g 2 then for ∈ {0, . . . , |var(g) \ {x}|} do</p><formula xml:id="formula_35">γ g ← γ g 1 + γ g 2 ; δ g ← δ g 1 + δ g 2 ; end else if g is an ∧-gate with input gates g 1 , g 2 then for ∈ {0, . . . , |var(g) \ {x}|} do γ g ← 1 ∈{0,...,min( ,|var(g 1 )\{x}|)} 2 ∈{0,...,min( ,|var(g 2 )\{x}|)} 1 + 2 = γ 1 g 1 • γ 2 g 2 ; δ g ← 1 ∈{0,...,min( ,|var(g 1 )\{x}|)} 2 ∈{0,...,min( ,|var(g 2 )\{x}|)} 1 + 2 = δ 1 g 1 • δ 2 g 2 ; end end return |X|-1 k=0 k! (|X| -k -1)! |X|! • (e(x) -p(x))(γ k gout -δ k gout ) ;</formula><p>which works no matter the value of e(x) ∈ {0, 1}. We can then directly combine this expression for L with Equation ( <ref type="formula">4</ref>) and the expression of R from Equation ( <ref type="formula" target="#formula_10">5</ref>) to obtain</p><formula xml:id="formula_36">Diff k (D, e, x) = L -R = (e(x) -p(x)) H Πp |X\{x} (D +x , e |X\{x} , k) -H Πp |X\{x} (D -x , e |X\{x} , k) .</formula><p>But the rightmost factor is precisely (γ k gout -δ k gout ) in Algorithm 1, so that the returned value is indeed correct by Equation (3).</p><p>Example 2 We describe in this example a complete execution of Algorithm 1 over the deterministic and decomposable Boolean circuit C in depicted Figure <ref type="figure" target="#fig_1">1</ref> (see Example 1). More precisely, we show in Figure <ref type="figure" target="#fig_4">2</ref> how SHAP(C, e, nf) is computed under the uniform distribution (that is, p(fg) = p(dtr) = p(nf) = p(na) = 1 /2), and assuming that e(fg) = 1, e(dtr) = 0, e(nf) = 1, and e(na) = 1. Notice that C(e) = 1.</p><p>In the first step of the algorithm, the gate in gray in Figure <ref type="figure" target="#fig_4">2</ref> is added to C so that the fan-in of every ∨-and ∧-gate is exactly 2, and the gates in blue in Figure <ref type="figure" target="#fig_4">2</ref> are added to obtain a deterministic, decomposable and smooth circuit D (in particular, for every ∨-gate g of D with input gates g 1 and g 2 , it holds that var(g 1 ) = var(g 2 )). Then in the main loop of the algorithm, the values of γ i g and δ i g are computed in a bottom-up fashion for every gate g and i ∈ {0, . . . , |var(g) \ {nf}|}. Finally, assuming that g out is the top ∧-gate in D, the value SHAP(C, e, nf) is computed as follows:</p><formula xml:id="formula_37">SHAP(C, e, nf) = 3 k=0 k! (3 -k)! 4! • (e(nf) -p(nf))(γ k gout -δ k gout ) = 1 2 • 1 4 • 1 8 + 1 12 • 3 4 + 1 12 • 3 2 + 1 4 • 1 = 15 64 .</formula><p>As a final comment, we notice that by following the same procedure it can be shown that SHAP(C, e, fg) = 23 /64, SHAP(C, e, dtr) = -9 /64, and SHAP(C, e, na) = 15 /64.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">An optimized version of the algorithm</head><p>In this section, we present an optimized version of the algorithm to compute the SHAPscore. The observation behind this algorithm is that it is possible to bypass the smoothing step and directly work on the input circuit C.<ref type="foot" target="#foot_2">5</ref> Our procedure can be found in Algorithm 2.</p><p>Observe that Algorithm 2 is identical to Algorithm 1, except that: (1) we do not smooth the circuit on line 1; and (2) the expressions for ∨-gates on lines 19 and 20 have changed. Before showing the correctness of Algorithm 2, notice that the smoothing step on line 1 of Algorithm 1 takes time O(|C| • |X|), as described in Section 3.1.2, and the resulting circuit, called D, has size O(|C| • |X|). As Algorithm 2 does not execute such a smoothing step, it works directly with a circuit of size O(|C|), obtained after preprocessing input circuit C to ensure that all ∨-and ∧-gate have fan-in 2, and it has a lower complexity.</p><formula xml:id="formula_38">dtr γ 0 = 1 2 γ 1 = 0 δ 0 = 1 2 δ 1 = 0 ∧ γ 0 = 1 2 δ 0 = 1 2 γ 1 = 1 2 δ 1 = 1 2 γ 2 = 0 δ 2 = 0 ∧ γ 0 = 1 δ 0 = 1 γ 1 = 1 δ 1 = 1 ∨ γ 0 = 1 δ 0 = 1 γ 1 = 1 δ 1 = 1 na γ 0 = 1 2 γ 1 = 1 δ 0 = 1 2 δ 1 = 1 ¬ γ 0 = 1 2 γ 1 = 0 δ 0 = 1 2 δ 1 = 0 na γ 0 = 1 2 γ 1 = 1 δ 0 = 1 2 δ 1 = 1 ∨ γ 0 = 1 δ 0 = 1 nf γ 0 = 1 δ 0 = 0 ¬ γ 0 = 0 δ 0 = 1 nf γ 0 = 1 δ 0 = 0 fg γ 0 = 1 2 γ 1 = 1 δ 0 = 1 2 δ 1 = 1 ∧ γ 0 = 1 2 γ 1 = 1 δ 0 = 0 δ 1 = 0 na γ 0 = 1 2 γ 1 = 1 δ 0 = 1 2 δ 1 = 1 nf γ 0 = 1 δ 0 = 0 ∧ ¬ γ 0 = 1 2 γ 1 = 1 δ 0 = 1 2 δ 1 = 1 γ 0 = 1 4 δ 0 = 0 γ 1 = 1 δ 1 = 0 γ 2 = 1 δ 2 = 0 ∨ γ 0 = 3 4 δ 0 = 1 2 γ 1 = 3 2 δ 1 = 1 2 γ 2 = 1 δ 2 = 0 ∧ γ 0 = 3 8 δ 0 = 1 4 γ 1 = 3 2 δ 1 = 3 4 γ 2 = 2 δ 2 = 1 2 γ 3 = 1 δ 3 = 0</formula><p>The only thing that changed between Algorithm 1 and 2 is how we treat an ∨-gate, which can now be non-smoothed. Therefore, to show that Algorithm 2 is correct, we need to revisit Equation ( <ref type="formula" target="#formula_24">12</ref>). The relation is still true for an arbitrary deterministic ∨-gate g with children g 1 , g 2 , however this is not equal to α g 1 +α g 2 anymore when g is not smooth. This is because, in this case, one of var(g 1 ) or var(g 2 ) (or both) is not equal to var(g). Assuming without loss of generality that we have var(g 1 ) = var(g), by induction hypothesis we have α g 1 = H Πp |var(g 1 ) (R g 1 , e |var(g 1 ) , ), which is not the expression ( <ref type="formula">14</ref>) that we obtain above. To fix this, we will do as if the gate had been smoothed, by considering the virtual circuits d S 1 and d S 2 that we use in the smoothing process, but without materializing them. We recall that S 1 := var(g 1 ) \ var(g 2 ) and S 2 := var(g 2 ) \ var(g 1 ), and that d S 1 and d S 2 are Boolean circuits that always evaluate to 1 over sets of variables S 1 and S 2 , respectively. Let g 1 and g 2 be the output gate of those circuits. We will show how to compute the values α 1 g 1 and α 2 g 2</p><formula xml:id="formula_39">α g = S⊆var(g) |S|= E e ∼Πp |var(g) [R g 1 (e ) + R</formula><p>, for 1 ∈ {0, . . . , |S 1 |} and 2 ∈ {0, . . . , |S 2 |}, directly with a closed form expression and use Equation ( <ref type="formula" target="#formula_28">13</ref>) for ∧gates to fix the algorithm. We have</p><formula xml:id="formula_40">α 1 g 1 = H Πp |var(g 1 ) (R g 1 , e |var(g 1 ) , ) = S⊆S 1 |S|= 1 E e ∼Πp |S 1 [R g 1 (e ) | e ∈ cw(e |S 1 , S)] = S⊆S 1 |S|= 1 E e ∼Πp |S 1 [1 | e ∈ cw(e |S 1 , S)] = S⊆S 1 |S|= 1 1 = |S 1 | 1 .</formula><p>Hence, by virtually considering that the circuit has been smoothed (that is, that we replaced gate g 1 with (g 1 ∧ d S 2 ) and gate g 2 with (g 2 ∧ d S 1 )), and by using the relation for ∧-gates we can correct Equation (12) as follows:</p><formula xml:id="formula_41">α g = ∈{0,...,min( ,|var(g 1 )|)} α g 1 • |var(g 2 ) \ var(g 1 )| - + ∈{0,...,min( ,|var(g 2 )|)} α g 2 • |var(g 1 ) \ var(g 2 )| - .</formula><p>And this is precisely what we use in Algorithm 2 for ∨-gates, so this concludes the proof. We note that, if we ignore the complexity of arithmetic operations (that is, if we consider that arithmetic operations over rationals take constant time and that rationals can be stored in constant space), Algorithm 2 runs in time O(|C| • |X| 2 ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Extension to Non-Binary Deterministic and Decomposable Circuits</head><p>In this section, we show how to extend the result of the previous section to non-binary classifiers. First, we need to redefine the notions of entities, product distributions and SHAPscore to account for non-binary features. We point out that these new definitions are to be considered for this section only, as in the rest of the paper we will again consider binary classifiers. Let X be a finite set of features, and dom be a function that associates to every feature x ∈ X a finite domain dom(x). An entity over (X, dom) is a function e that associates to every feature an element e(x) ∈ dom(X). We denote by ent(X, dom) the set of all entities over (X, dom). We then consider product distributions on ent(X, dom) as follows.</p><p>For every x ∈ X, let p x : dom(X) → [0, 1] be such that v∈dom(X) p x (v) = 1. Then the product distribution generated by (p x ) x∈X is the probability distribution Π p over ent(X, dom) such that, for every e ∈ ent(X, dom) we have</p><formula xml:id="formula_42">Π p (e) := x∈X p x (e(x)).</formula><p>That is, again, Π p is the product distribution that is determined by pre-specified marginal distributions, and that makes the features take values independently from each other. A Boolean classifier M over X is a function M : ent(X, dom) → {0, 1} that maps every entity over X to 0 or 1. The SHAP score over such classifiers is then defined just like in Section 2.2.</p><p>Non-binary Boolean circuits. We now define a variant of deterministic and decomposable circuits for non-binary variables, that we will call deterministic and decomposable non-binary Boolean circuits. Just like a Boolean circuit captures a set of binary entities (those that satisfy the circuit), a non-binary Boolean circuit will capture a set of entities over (X, dom). We define a non-binary Boolean circuit over (X, dom) like a Boolean circuit (recall the definition from Section 2.3), except that each variable gate is now labeled with an equality of the form x = v, where x ∈ X and v ∈ dom(X). Such a circuit then maps every entity over X to 0 or 1 in the expected way, and can thus be seen as a Boolean classifier. Again, an ∧-gate is decomposable if for every pair g 1 , g 2 of distinct input gates of g, we have that var(g 1 ) ∩ var(g 2 ) = ∅; an ∨-gate is deterministic if for every pair g 1 , g 2 of distinct input gates of g there is no entity that satisfies them both; and the circuit is called deterministic and decomposable if all its ∧-gates are decomposable and all its ∨-gates are deterministic.</p><p>The main result of this section is that we can generalize Theorem 2 to these kind of circuits (we presented the result for Boolean circuits over binary variables first for clarity of presentation):</p><p>Theorem 5 Given as input a set of features X, finite domains dom(x) for every x ∈ X, a deterministic and decomposable non-binary Boolean circuit C over (X, dom), rational probability values p x (v) for every x ∈ X and v ∈ dom(x), an entity e ∈ ent(X, dom), and a feature x ∈ X, the value SHAP Πp (C, e, x) can be computed in polynomial time.</p><p>Algorithm 2: SHAP-scores for deterministic and decomposable Boolean circuits Input : Deterministic and decomposable Boolean circuit C over features X with output gate g out , rational probability values p(x) for all x ∈ X, entity e ∈ ent(X), and feature x ∈ X. Output: The value SHAP(C, e, x) under the probability distribution Π p .</p><p>1 Preprocess C so that each ∨-gate and ∧-gate has fan-in exactly 2;</p><p>2 Compute the set var(g) for every gate g in C;</p><p>3 Compute values γ g and δ g for every gate g in C and ∈ {0, . . . , |var(g) \ {x}|} by bottom-up induction on C as follows:</p><formula xml:id="formula_43">4 if g is a constant gate with label a ∈ {0, 1} then 5 γ 0 g , δ 0 g ← a;</formula><p>6 else if g is a variable gate with var(g) = {x} then</p><formula xml:id="formula_44">7 γ 0 g ← 1; 8 δ 0 g ← 0;</formula><p>9 else if g is a variable gate with var(g) = {y} and y = x then γ 0 g , δ 0 g ← p(y); </p><formula xml:id="formula_45">γ 1 g , δ 1 g ← e(</formula><formula xml:id="formula_46">)\{x}|)} 1 + 2 = γ 1 g 1 • γ 2 g 2 ; δ g ← 1 ∈{0,...,min( ,|var(g 1 )\{x}|)} 2 ∈{0,...,min( ,|var(g 2 )\{x}|)} 1 + 2 = δ 1 g 1 • δ 2 g 2 ; end end return |X|-1 k=0 k! (|X| -k -1)! |X|! • (e(x) -p(x))(γ k gout -δ k gout ) ;</formula><p>Proof First, notice that, since probability values p x (v) for every x ∈ X and v ∈ dom(x) are anyway given as part of the input, it can safely be considered that dom(x) is of linear size. We then go through the proof of Theorem 2 and only explain what changes. For x ∈ X and v ∈ dom(x), we denote by C x=v the non-binary Boolean circuit that is obtained from C by replacing every variable gate that is labeled with x = v by a constant 1-gate, and every variable gate that is labeled with x = v for v = v by a constant 0-gate (it is clear that C x=v is again deterministic and decomposable if C satisfies these properties). Then, the reduction from We now look at the computation of</p><formula xml:id="formula_47">SHAP Π• (•, •, •) to H Π• (•, •, •) from</formula><formula xml:id="formula_48">H Π• (•, •, •)</formula><p>and inspect what changes in Lemma 4. We can rewrite to fan-in 2 and smooth the circuit in the same way,<ref type="foot" target="#foot_3">6</ref> and the quantities α g are also defined in the same way. The relations that we used to compute the values α g do not change, except the ones for variable gates: for a variable gate g labeled with x = v, Equations ( <ref type="formula">8</ref>) and ( <ref type="formula" target="#formula_20">9</ref>) become, respectively, α 0 g = p x (v) and</p><formula xml:id="formula_49">α 1 g = 1 if e(x) = v 0 otherwise .</formula><p>This in particular allows us to prove that the SHAP-score can be computed in polynomial time for (not necessarily binary) decision trees, or for variants of OBDDs/FBDDs that use non-binary features, as deterministic and decomposable non-binary Boolean circuits generalize them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Limits on the Tractable Computation of the SHAP-Score</head><p>We have shown in the previous sections that the SHAP-score can be computed in polynomial time for deterministic and decomposable circuits under product distributions. A natural question, then, is whether both determinism and decomposability are necessary for this positive result to hold. In this section we show that this is the case, at least under standard complexity assumptions, and even when we consider the uniform distribution. Recall that we are now back to considering Boolean classifiers. Formally, we prove the following: Theorem 6 The following problems are #P-hard.</p><p>1. Given as input a decomposable (but not necessarily deterministic) Boolean circuit C over a set of features X, an entity e : X → {0, 1}, and a feature x ∈ X, compute the value SHAP U (C, e, x).</p><p>2. Given as input a deterministic (but not necessarily decomposable) Boolean circuit C over a set of features X, an entity e : X → {0, 1}, and a feature x ∈ X, compute the value SHAP U (C, e, x).</p><p>Intuitively, for the first item, this comes from the fact that an arbitrary Boolean circuit can always be transformed in polynomial time (in fact in linear time) into an equivalent decomposable (but not necessarily deterministic) Boolean circuit, simply by applying De Morgan's laws to eliminate all ∧-gates. Hence the problem on those circuits it at least as hard as on unrestricted Boolean circuits; and the argument is similar for the second item. Therefore, to show Theorem 6, it is good enough to prove that the problem is indeed intractable over unrestricted Boolean circuits. We now prove these claims formally.</p><p>We start by showing that there is a general polynomial-time reduction from the problem of computing the number of entities that satisfy M , for M an arbitrary Boolean classifier, to the problem of computing the SHAP-score over M under the uniform distribution. This holds under the mild condition that M (e) can be computed in polynomial time for an input entity e, which is satisfied for all the Boolean circuits and binary decision diagrams classes considered in this paper. The proof of this result follows from well-known properties of Shapley values, and a closely related result can be found as Theorem 5.1 in <ref type="bibr" target="#b8">(Bertossi et al., 2020)</ref>.</p><p>Lemma 7 Let M be a Boolean classifier over a set of features X, and let #SAT(M ) := |{e ∈ ent(X) | M (e) = 1}|. Then for every e ∈ ent(X) we have:</p><formula xml:id="formula_50">#SAT(M ) = 2 |X| M (e) - x∈X SHAP U (M, e, x) .</formula><p>Proof The validity of this equation will be a consequence of the following property of the SHAP-score: for every Boolean classifier M over X, entity e ∈ ent(X) and feature x ∈ X, it holds that</p><formula xml:id="formula_51">x∈X SHAP U (M, e, x) = φ U (M, e, X) -φ U (M, e, ∅). (<label>15</label></formula><formula xml:id="formula_52">)</formula><p>This property is often called the efficiency property of the Shapley value. Although this is folklore, we prove Equation ( <ref type="formula" target="#formula_51">15</ref>) here for the reader's convenience. Recall from Equation ( <ref type="formula" target="#formula_3">2</ref>) that the SHAP-score can be written as</p><formula xml:id="formula_53">SHAP U (M, e, x) = 1 |X|! π∈Π(X) φ U (M, e, S x π ∪ {x}) -φ U (M, e, S x π ) .</formula><p>Hence, we have that</p><formula xml:id="formula_54">x∈X SHAP U (M, e, x) = 1 |X|! x∈X π∈Π(X) φ U (M, e, S x π ∪ {x}) -φ U (M, e, S x π ) = 1 |X|! π∈Π(X) x∈X φ U (M, e, S x π ∪ {x}) -φ U (M, e, S x π ) = 1 |X|! π∈Π(X) φ U (M, e, X) -φ U (M, e, ∅) = φ U (M, e, X) -φ U (M, e, ∅),</formula><p>where the second to last equality is obtained by noticing that the inner sum is a telescoping sum. This establishes Equation ( <ref type="formula" target="#formula_51">15</ref>). Now, by definition of φ U (•, •, •) we have that φ U (M, e, X) = M (e) and φ U (M, e, ∅) = 1 2 |X| e ∈ent(X) M (e ), so we obtain</p><formula xml:id="formula_55">x∈X SHAP U (M, e, x) = M (e) - #SAT(M ) 2 |X| ,</formula><p>thus proving Lemma 7.</p><p>We stress out here that this result holds for any Boolean classifier and is not restricted to the classes of Boolean circuits that we consider in this paper.</p><p>Theorem 6 can then easily be deduced from Lemma 7 and the following two facts: (a) counting the number of satisfying assignments of an arbitrary Boolean circuit is a #P-hard problem <ref type="bibr" target="#b40">(Provan and Ball, 1983)</ref>; and (b) every Boolean circuit can be transformed in linear time into a Boolean circuit that is deterministic (resp, decomposable), simply by using De Morgan's laws to get rid of the ∨-gates (resp., ∧-gates). For instance, the reduction to prove the first item of Theorem 6 is as follows: on input an arbitrary Boolean circuit C, use De Morgan's laws to remove all ∧-gates, thus obtaining an equivalent circuit C which is vacuously decomposable (since it does not have any ∧-gates), and then use the oracle to computing SHAP-scores together with Lemma 7 with an arbitrary entity (for instance, the one that assigns zero to all features) in order to compute #SAT(C).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Non-Approximability of the SHAP-Score</head><p>We now study the approximability of the SHAP-score. As we have shown in the previous section with Lemma 7, computing the SHAP-score is generally intractable for classes of models for which model counting is intractable. Yet, it could be the case that one can efficiently approximate the SHAP-score, just like in some cases one can efficiently approximate the number of models of a formula even though computing this quantity exactly is intractable. For instance, model counting of DNF formulas is #P-hard (Provan and Ball, 1983) but admits a Fully Polynomial-time Randomized Approximation Scheme, or FPRAS <ref type="bibr" target="#b26">(Karp et al., 1989)</ref>. Unfortunately, as we show next, intractability of computing the SHAP-score continues to hold when one considers approximability, and this even for very restricted kinds of Boolean classifiers and under the uniform probability distribution.</p><p>To simplify the notation in this section, we will drop the subscript U for the uniform distribution, and write SHAP(</p><formula xml:id="formula_56">•, •, •) instead of SHAP U (•, •, •), and φ(•, •, •) instead of φ U (•, •, •).</formula><p>Let C be a class of Boolean classifiers and ε ∈ (0, 1). We say that the problem of computing the SHAP score for C admits an ε polynomial-time randomized approximation (ε-PRA), if there exists a randomized algorithm A satisfying the following conditions. For every Boolean classifier M ∈ C over a set of features X, entity e over X, and feature x ∈ X, it holds that:</p><formula xml:id="formula_57">Pr |A(M, e, x) -SHAP(M, e, x)| ≤ ε • |SHAP(M, e, x)| ≥ 3 4 .</formula><p>Moreover, there exists a polynomial p(•) such that A(M, e, x) works in time O(p( M + e )), where M and e are the sizes of M and e represented as input strings, respectively. We recall that the notion of FPRAS mentioned before is defined as the concept of PRA, but imposing the stronger requirements that ε is part of the input and the algorithm is polynomial in 1 ε as well. We start by presenting a simple proof that for every ε ∈ (0, 1), the problem of computing the SHAP score for Boolean classifiers given as DNF formulas does not admit an ε-PRA, unless NP = RP. 7</p><p>Proposition 8 For every ε ∈ (0, 1), the problem of computing the SHAP score for Boolean classifiers given as DNF formulas does not admit an ε-PRA, unless NP = RP. This result holds even if we restrict to the uniform distributions on the entities.</p><p>Proof We first recall the following fact: if a function f admits an ε-PRA, then the problem of determining, given a string x, whether f (x) = 0 is in BPP (although this is folklore, we provide a proof in Appendix B). We use this to prove that if we could approximate the SHAP-score over DNF formulas, then we could solve the validity problem over DNF formulas in BPP. Recall that the validity problem over DNF formulas is the decision problem that, given as input a DNF formula ϕ, accepts if all valuations satisfy ϕ, and rejects otherwise (in other words, it rejects if ¬ϕ is satisfiable and accepts otherwise). Since this problem is coNP-complete and BPP is closed under complement, this would imply that NP ⊆ BPP, and thus that NP = RP <ref type="bibr" target="#b27">(Ko, 1982)</ref>. Let ϕ be a DNF formula over a set of variables X. We consider the DNF formula ϕ := ϕ ∨ x with x / ∈ X, and the uniform probability distribution over ent(X ∪ {x}). Let e be an arbitrary entity over X ∪ {x} such that e(x) = 1. We show that ϕ is valid if and only if SHAP(ϕ , e, x) = 0, which, by the previous remarks, is good enough to conclude the proof. By definition we have</p><formula xml:id="formula_58">SHAP(ϕ , e, x) = S⊆X |S|! (|X| -|S|)! (|X| + 1)! φ(ϕ , e, S ∪ {x}) -φ(ϕ , e, S) .</formula><p>Observe that for each S ⊆ X, it holds that φ(ϕ , e, S ∪ {x}) = 1 (given the definition of ϕ and the fact that e(x) = 1), and that 0 ≤ φ(ϕ , e, S) ≤ 1. Now, if ϕ is valid, then it is clear that φ(ϕ , e, S) = 1 for every S ⊆ X, so that indeed SHAP(ϕ , e, x) = 0. Assume now that ϕ is not valid. By what preceded, it is good enough to show that for some S ⊆ X we have φ(ϕ , e, S) &lt; 1. But this clearly holds for S = ∅, because ϕ is not valid and all entities have the same probability (so that no entity has probability zero).</p><p>Hence, this result already establishes an important difference with model counting: for the case of DNF formulas, the model counting problem admits an FPRAS <ref type="bibr" target="#b26">(Karp et al., 1989)</ref> and, thus, an ε-PRA for every ε ∈ (0, 1).</p><p>7. Recall that it is widely believed that RP is properly contained in NP, as mentioned in Section 2.</p><p>It is important to mention that the proof of Proposition 8 uses, in a crucial way, the fact that the validity problem for DNF formulas is intractable. We prove next a strong negative result, which establishes that not even for positive DNF formulas -for which the validity problem is trivial -it is possible to obtain an ε-PRA for computing the SHAP-score. Let ϕ = D 1 ∨ D 2 ∨ • • • ∨ D k be a formula in DNF, that is, each formula D i is a conjunction of positive literals (propositional variables) and negative literals (negations of propositional variables). Then ϕ is said to be in POS-DNF if each formula D i is a conjunction of positive literals, (hence, ϕ is a monotone formula), and ϕ is said to be in 2-POS-DNF if ϕ is in POS-DNF and each formula D i contains at most two (positive) literals. Our main result of this section is the following.</p><p>Theorem 9 For every ε ∈ (0, 1), the problem of computing the SHAP score for Boolean classifiers given as 2-POS-DNF formulas does not admit an ε-PRA, unless NP = RP. This result holds even if we restrict to the uniform distributions on the entities.</p><p>Furthermore, we can show that the same intractability result also holds for closely related classes of Boolean classifiers. As before, let</p><formula xml:id="formula_59">ϕ = D 1 ∨ D 2 ∨ • • • ∨ D k be a formula in DNF.</formula><p>Then ϕ is in NEG-DNF if each formula D i is a conjunction of negative literals, and ϕ is in 2-NEG-DNF if it is in NEG-DNF and each formula D i is a conjunction of at most two (negative) literals. We define similarly formulas in 2-POS-CNF and in 2-NEG-CNF. We then obtain the following result.</p><p>Corollary 10 Let C ∈ {2-POS-DNF, 2-NEG-DNF, 2-POS-CNF, 2-NEG-CNF}. Then for every ε ∈ (0, 1), the problem of computing the SHAP score for Boolean classifiers given as formulas in C does not admit an ε-PRA, unless NP = RP. This result holds even if we restrict to the uniform distributions on the entities.</p><p>It should be noticed that 2-NEG-CNF formulas are special cases of HORN-SAT formulas, so that the previous result also applies for HORN-SAT.</p><p>We prove Theorem 9 in Section 6.1, and then show how the proof can be adapted to prove Corollary 10 in Section 6.2</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Proof of Theorem 9</head><p>We will use some results and techniques related to approximating cliques in graphs. More specifically, all graphs G = (N, E) considered in this proof are assumed to be undirected and loop-free (that is, edges of the form (a, a) are not allowed). Besides, we assume that each graph G = (N, E) satisfies the following condition:</p><p>(A) there exist at least two isolated nodes in G, that is, two distinct nodes a, b ∈ N such that (a, c) ∈ E and (b, c) ∈ E for every node c ∈ N .</p><p>The assumption that condition (A) is satisfied will allow us to simplify some calculations. Besides, it is clear that condition (A) can be checked in polynomial time.</p><p>We define the problem GapClique as follows. The input of GapClique is a graph G = (N, E) and a number m ≤ |N |, and its output is:</p><formula xml:id="formula_60">• yes if G contains a clique with m nodes,</formula><p>• no if every clique of G contains at most m 3 nodes. From the PCP theorem and its applications to hardness of approximation <ref type="bibr" target="#b22">(Feige et al., 1996;</ref><ref type="bibr" target="#b4">Arora and Safra, 1998;</ref><ref type="bibr">Arora et al., 1998)</ref>, it is known that GapClique is NP-hard (in fact even if 1 3 is replaced by any δ ∈ (0, 1)). In other words, there exists a polynomial-time reduction that takes as input a Boolean formula ϕ and that outputs a graph G and an integer m such that (1) if ϕ is satisfiable then G contains a clique with m nodes; and (2) if ϕ is not satisfiable then every clique of G contains at most m 3 nodes. The idea of our proof of Theorem 9 is then to show that if the problem of computing the SHAP score for Boolean classifiers given as 2-POS-DNF formulas admits an ε-PRA, then there exists a BPP algorithm for GapClique. Hence, we would conclude that NP ⊆ BPP, which in turn implies that NP = RP <ref type="bibr" target="#b27">(Ko, 1982)</ref>. The proof is technical, and it is divided into five modular parts, highlighted in bold in the remaining of this section.</p><p>The SHAP-score of a Boolean classifier of the form M ∨x. Given a set of features X, a Boolean classifier M over X, and an S ⊆ X, define #SAT(M, S) := |{e ∈ ent(X) | M (e) = 1 and e(y) = 1 for every y ∈ S}|.</p><p>We can relate the SHAP-score of a Boolean classifier of the form M ∨ x to the quantities #SAT(¬M, S) as follows:</p><p>Lemma 11 Let X be a set of features, n = |X|, x ∈ X, M be a Boolean classifier over X \ {x}, and 1 be the entity over X such that 1(y) = 1 for every y ∈ X. Then</p><formula xml:id="formula_61">SHAP(M ∨ x, 1, x) = n-1 k=0 k!(n -k -1)! n! • 1 2 n-k S⊆X\{x} : |S|=k #SAT(¬M, S).</formula><p>Proof Let M be a Boolean classifier over X. Given S ⊆ X \ {x}, we have that:</p><formula xml:id="formula_62">φ(M , 1, S ∪ {x}) = e∈cw(1,S∪{x}) 1 2 |X\(S∪{x})| • M (e) = 1 2 |X\(S∪{x})| e ∈cw(1 |X\{x} ,S) M +x (e ) = 2 2 |X\S| e ∈cw(1 |X\{x} ,S) M +x (e ).</formula><p>Besides, we have that:</p><formula xml:id="formula_63">φ(M , 1, S) = e∈cw(1,S) 1 2 |X\S| • M (e) = e∈cw(1,S) : e(x)=0 1 2 |X\S| • M (e) + e∈cw(1,S) : e(x)=1 1 2 |X\S| • M (e) = 1 2 |X\S| • e ∈cw(1 |X\{x} ,S) M -x (e ) + e ∈cw(1 |X\{x} ,S) M +x (e ) .</formula><p>Therefore, we conclude that:</p><formula xml:id="formula_64">φ(M , 1, S ∪ {x}) -φ(M , 1, S) = 1 2 |X\S| • e∈cw(1 |X\{x} ,S) M +x (e) - e∈cw(1 |X\{x} ,S) M -x (e) .</formula><p>Considering this equation with M := M ∨ x, we deduce that:</p><formula xml:id="formula_65">φ(M ∨ x, 1, S ∪ {x}) -φ(M ∨ x, 1, S) = 1 2 |X\S| • e∈cw(1 |X\{x} ,S) (M ∨ x) +x (e) - e∈cw(1 |X\{x} ,S) (M ∨ x) -x (e) = 1 2 |X\S| • e∈cw(1 |X\{x} ,S) 1 - e∈cw(1 |X\{x} ,S) M (e) = 1 2 |X\S| • e∈cw(1 |X\{x} ,S) (1 -M (e)) = 1 2 |X\S| • #SAT(¬M, S).</formula><p>By considering the definition of the SHAP score, we obtain that:</p><formula xml:id="formula_66">SHAP(M ∨ x, 1, x) = S⊆X\{x} |S|!(|X| -|S| -1)! |X|! • φ(M ∨ x, 1, S ∪ {x}) -φ(M ∨ x, 1, S) = n-1 k=0 S⊆X\{x} : |S|=k |S|!(|X| -|S| -1)! |X|! • 1 2 |X\S| • #SAT(¬M, S) = n-1 k=0 k!(n -k -1)! n! • 1 2 n-k S⊆X\{x} : |S|=k #SAT(¬M, S).</formula><p>This concludes the proof of Lemma 11. Using Lemma 11, we can relate the SHAP-score of the 2-POS-DNF formula θ(G) ∨ x to the quantities #CLIQUE(G, S) for S ⊆ N as follows.</p><p>Lemma 12 For every graph G = (N, E) and x / ∈ N , letting n = |N | and 1 be the entity over N ∪ {x} such that 1(y) = 1 for every y ∈ N ∪ {x}, we have:</p><formula xml:id="formula_67">SHAP(θ(G) ∨ x, 1, x) = 1 2(n + 1) n k=0 k!(n -k)! n! • 1 2 n-k S⊆N : |S|=k #CLIQUE(G, S).</formula><p>Proof The crucial observation is that the following relation holds for every S ⊆ N :</p><formula xml:id="formula_68">#CLIQUE(G, S) = #SAT(¬θ(G), S).</formula><p>Indeed, this can be seen by defining the bijection that to every subset</p><formula xml:id="formula_69">Y of N associates the valuation ν Y of θ(G) such that ν Y (x) = 1 if and only if x ∈ Y , and then checking that [Y is a clique of G with S ⊆ Y ] if and only if [ν Y |= ¬θ(G) and ν Y (x) = 1 for all x ∈ S].</formula><p>Therefore, we have from Lemma 11 that</p><formula xml:id="formula_70">SHAP(θ(G) ∨ x, 1, x) = n k=0 k!(n -k)! (n + 1)! • 1 2 n+1-k S⊆N : |S|=k #SAT(¬θ(G), S) = 1 2(n + 1) n k=0 k!(n -k)! n! • 1 2 n-k S⊆N : |S|=k #CLIQUE(G, S).</formula><p>Working with amplified graphs. In this proof, we consider an amplification technique from <ref type="bibr" target="#b53">(Sinclair, 1993)</ref>. More precisely, given a graph G = (N, E) and a natural number r ≥ 1, define the amplified graph G r = (N r , E r ) of G as follows. For each a ∈ N , let N a := {a 1 , . . . , a r } and N r := a∈N N a , and let E r be the following set of edges:</p><formula xml:id="formula_71">E r := {(a i , a j ) | a i , a j ∈ N a and i = j} ∪ {(a i , b j ) | a i ∈ N a , b j ∈ N b and (a, b) ∈ E}.</formula><p>Thus, the amplified graph G r is constructed by copying r times each node of G, by connecting for each node of G all of its copies as a clique, and finally by connecting copies a i , b j of nodes a, b of G, respectively, whenever a and b are connected in G by an edge. Notice in particular that G 1 is simply G. An example of this construction for r = 2 is illustrated in Figure <ref type="figure">3</ref>.</p><p>Let then T be a clique of G r . We say that T is a witness of the clique {a ∈ N | N a ∩T = ∅} of G. Observe that a clique of G r witnesses a unique clique of G by definition, but that a clique of G can have multiple witnessing cliques in G r . Moreover, letting S be a clique of G, we write Wit(G r , S) for the set of cliques of G r that are witnesses of S. We then show the following three properties of G r :</p><formula xml:id="formula_72">(i) if S 1 , S 2 are distinct cliques of G, then Wit(G r , S 1 )∩Wit(G r , S 2 ) = ∅. Indeed, assuming</formula><p>without loss of generality that S 1 ⊆ S 2 (the case S 2 ⊆ S 1 being symmetrical) and letting a ∈ S 1 \ S 2 , it is clear by that, by definition of being a witness, for any T ∈ Wit(G r , S 1 ) we must have N a ∩ T = ∅, whereas for any T ∈ Wit(G r , S 2 ) we have</p><formula xml:id="formula_73">N a ∩ T = ∅. a b c d a 1 a 2 b 1 b 2 c 1 c 2 d 1 d 2</formula><p>Figure <ref type="figure">3</ref>: Illustration of the amplification construction for r = 2. On the left a graph G, on the right the corresponding graph G 2 . We only illustrate the construction for r = 2, as for r ≥ 3 this very quickly becomes unreadable.</p><p>(ii</p><formula xml:id="formula_74">) if S is a clique of G, then |Wit(G r , S)| = (2 r -1) |S| . Indeed, let T ∈ Wit(G r , S)</formula><p>. By definition of being a witness, T cannot contain any node of the form a i for a / ∈ S. Moreover, for every a ∈ S, by definition of being a witness again, the set T ∩ N a must be a non-empty subset of N a . Since for every a ∈ N there are exactly (2 r -1) non-empty subsets of N a , this shows that |Wit(G r , S)| ≤ (2 r -1) |S| . Additionally, any set of the form a∈S S a where each S a is a non-empty subset of N a is in fact in Wit(G r , S): this is simply because S itself is a clique of S and by definition of the graph G r . This shows that |Wit(G r , S)| ≥ (2 r -1) |S| , hence |Wit(G r , S)| = (2 r -1) |S| indeed.</p><p>(iii) for every natural number , if all cliques of G have size at most , then all cliques of G r have size at most • r. Indeed, for any clique T of G r , letting S be the clique of S that T witnesses, it is clear that |T | ≤ r • |S| (with the equality being reached when T ∩ N a = N a for every a ∈ S).</p><p>We use these properties to prove our next lemma.</p><p>Lemma 13 Let G = (N, E) be a graph, x / ∈ N , n := |N |, and m, r be natural numbers such that 1 ≤ m ≤ n and r ≥ 1. Then: (a) If every clique of G contains at most m 3 nodes, then</p><formula xml:id="formula_75">SHAP(θ(G r ) ∨ x, 1, x) ≤ 2 2 m 3 r+n 2 n•r+1 . (b) If G contains a clique with m nodes, then SHAP(θ(G r ) ∨ x, 1, x) ≥ 1 (n • r + 1) • 2 mr 2 n•r+1 . Proof (a) Given that |N r | = n • r, we have from Lemma 12 that: SHAP(θ(G r ) ∨ x, 1, x) = 1 2(n • r + 1) n•r k=0 k!(n • r -k)! (n • r)! • 1 2 n•r-k S⊆N r : |S|=k #CLIQUE(G r , S).</formula><p>Now, since every clique of G contains at most m 3 nodes, we have by Item (iii) that all cliques of G r have size at most m 3 • r. Hence when k &gt; m 3 • r it holds that #CLIQUE(G r , S) = 0 for any S ⊆ N r with |S| = k. Therefore, we have that</p><formula xml:id="formula_76">SHAP(θ(G r ) ∨ x, 1, x) = 1 2(n • r + 1) m 3 •r k=0 k!(n • r -k)! (n • r)! • 1 2 n•r-k S⊆N r : |S|=k #CLIQUE(G r , S) ≤ 1 2(n • r + 1) • 1 2 n•r-m 3 r m 3 •r k=0 k!(n • r -k)! (n • r)! S⊆N r : |S|=k #CLIQUE(G r , S) = 1 2(n • r + 1) • 1 2 n•r-m 3 r m 3 •r k=0 1 n•r k S⊆N r : |S|=k #CLIQUE(G r , S). Now, defining #CLIQUE(G, ) := |{Y | Y is a clique of G and |Y | = }|, observe that we have S⊆N r : |S|=k #CLIQUE(G r , S) = S⊆N r : |S|=k Y clique of G r S⊆Y 1 = S⊆N r : |S|=k |N r | =k Y clique of G r |Y |= S⊆Y 1 = S⊆N r : |S|=k m 3 •r =k Y clique of G r |Y |= S⊆Y 1 = m 3 •r =k S⊆N r : |S|=k Y clique of G r |Y |= S⊆Y 1 = m 3 •r =k Y clique of G r |Y |= S⊆Y : |S|=k 1 = m 3 •r =k Y clique of G r |Y |= |Y | k = m 3 •r =k k Y clique of G r |Y |= 1 = m 3 •r =k k #CLIQUE(G r , ),</formula><p>where the third equality is simply because all cliques of G r have size at most m 3 • r, again by Item (iii). Hence, we have that</p><formula xml:id="formula_77">SHAP(θ(G r ) ∨ x, 1, x) ≤ 1 2(n • r + 1) • 1 2 n•r-m 3 r m 3 •r k=0 1 n•r k m 3 •r =k k #CLIQUE(G r , ) ≤ 1 2(n • r + 1) • 1 2 n•r-m 3 r m 3 •r k=0 1 n•r k m 3 •r =k n • r k #CLIQUE(G r , ) = 1 2(n • r + 1) • 1 2 n•r-m 3 r m 3 •r k=0 m 3 •r =k #CLIQUE(G r , ) ≤ 1 2(n • r + 1) • 1 2 n•r-m 3 r m 3 •r k=0 m 3 •r =0 #CLIQUE(G r , ) ≤ ( m 3 • r + 1) 2(n • r + 1) • 1 2 n•r-m 3 r m 3 •r =0 #CLIQUE(G r , ) ≤ n • r + 1 2(n • r + 1) • 1 2 n•r-m 3 r m 3 •r =0 #CLIQUE(G r , ) = 1 2 • 1 2 n•r-m 3 r m 3 •r =0 #CLIQUE(G r , ). But notice that m 3 •r =0 #CLIQUE(G r ,</formula><p>) is simply the total number of cliques of G r . Given a clique S of G with elements, remember that by Item (ii) we have that |Wit(G r , S)| = (2 r -1) . Hence, given that each clique of G has at most m 3 nodes, that G has at most n cliques with nodes, and that every clique of G r witnesses a unique clique of G, we have that the total number of cliques of G r is bounded as follows:</p><formula xml:id="formula_78">m 3 •r =0 #CLIQUE(G r , ) ≤ m 3 =0 n (2 r -1) .</formula><p>Therefore, we conclude that:</p><formula xml:id="formula_79">SHAP(θ(G r ) ∨ x, 1, x) ≤ 1 2 • 1 2 n•r-m 3 r m 3 =0 n (2 r -1) ≤ 1 2 • 1 2 n•r-m 3 r m 3 =0 n (2 r -1) m 3 = 1 2 • 1 2 n•r-m 3 r • (2 r -1) m 3 m 3 =0 n ≤ 1 2 • 1 2 n•r-m 3 r • (2 r -1) m 3 n =0 n = 1 2 • 1 2 n•r-m 3 r • (2 r -1) m 3 • 2 n ≤ 1 2 • 1 2 n•r-m 3 r • 2 m 3 r • 2 n = 2 2 m 3 r+n 2 n•r+1 .</formula><p>(b) First, we claim that G r contains at least 2 mr cliques. Indeed, let S be a clique with m nodes that G contains (by hypothesis). Every subset S of S is also a clique of G. Now, by Item (i) and Item (ii), we have that the number of witnesses of all the subcliques of S is equal to</p><formula xml:id="formula_80">S ⊆S Wit(G r , S ) = S ⊆S |Wit(G r , S )| = m =0 m (2 r -1) = m =0 m (2 r -1) 1 m- = (2 r -1 + 1) m = 2 mr .</formula><p>All terms in the summation of SHAP(θ(G r ) ∨ x, 1, x) are non-negative. Hence, we have that:</p><formula xml:id="formula_81">SHAP(θ(G r ) ∨ x, 1, x) = 1 2(n • r + 1) n•r k=0 k!(n • r -k)! n • r! • 1 2 n•r-k S⊆N r : |S|=k #CLIQUE(G r , S) ≥ 1 2(n • r + 1) • 0!(n • r)! (n • r)! • 1 2 n•r S⊆N r : |S|=0 #CLIQUE(G r , S) = 1 2(n • r + 1) • 1 2 n•r • #CLIQUE(G r , ∅).</formula><p>But #CLIQUE(G r , ∅) is the total number of cliques of G r , which we have just proved to be greater than or equal to 2 mr . Therefore</p><formula xml:id="formula_82">SHAP(θ(G r ) ∨ x, 1, x) ≥ 1 (n • r + 1) • 2 mr 2 n•r+1 ,</formula><p>which concludes the proof of Lemma 13.</p><p>A technical lemma. Last, we will need the following technical lemma.</p><p>Lemma 14 For every ε ∈ (0, 1), there exists n ε ∈ N such that for every n, m ∈ N with n ≥ n ε and m ≥ 1, it holds that:</p><formula xml:id="formula_83">(1 + ε) • 2 2 m 3 n 2 +n &lt; (1 -ε) • 2 mn 2 (n 3 + 1)</formula><p>.</p><p>The proof is straightforward and can be found in Appendix C.1.</p><p>Putting it all together. We now have all the necessary ingredients to prove Theorem 9.</p><p>Assume that the problem of computing the SHAP score for Boolean classifiers given as 2-POS-DNF formulas admits an ε-PRA, for some fixed ε ∈ (0, 1), that we will denote by A.</p><p>By using A, we define the following BPP algorithm B for GapClique. Let G = (N, E) be a graph with n = |N | and m ∈ {0, . . . , n}. Then B(G, m) performs the following steps:</p><p>1. If m = 0, then return yes.</p><p>2. Let n ε be the constant in Lemma 14. If n ≤ n ε , then by performing an exhaustive search, check whether G contains a clique with m nodes or if every clique of G contains at most m 3 nodes. In the former case return yes, in the latter case return no.</p><p>3. Construct the formula θ(G n 2 ) ∨ x, where x is a fresh feature (not occurring in N n 2 ). Notice that θ(G n 2 ) ∨ x is a formula in 2-POS-DNF.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Use algorithm</head><formula xml:id="formula_84">A to compute s := A(θ(G n 2 ) ∨ x, 1, x). 5. If s &gt; (1 + ε) • 2 2 m 3 n 2 +n</formula><p>2 n 3 +1 , then return yes; otherwise return no. Algorithm B works in polynomial time since n ε is a fixed natural number (given that ε is a fixed value in (0, 1)), amplified graph G n 2 can be computed in polynomial time from G, formula θ(G n 2 ) ∨ x can be constructed in polynomial time from G n 2 , algorithm A(θ(G n 2 ) ∨ x, 1, x) works in time p( θ(G n 2 ) ∨ x + 1 ) for a polynomial p(•), and bound</p><formula xml:id="formula_85">(1 + ε) • 2 2 m 3 n 2 +n 2 n 3 +1</formula><p>can be computed in polynomial time in n. Therefore, to conclude the proof we need to show that the error probability of algorithm B is bounded by 1 4 .</p><p>• Assume that every clique of G contains at most m 3 nodes. Then the error probability of algorithm B is equal to Pr(B(G, m) returns yes). By using the definition of B and Lemma 13 (a) with r = n 2 , we obtain that:</p><formula xml:id="formula_86">Pr(B(G, m) returns yes) = Pr A(θ(G n 2 ) ∨ x, 1, x) &gt; (1 + ε) • 2 2 m 3 n 2 +n 2 n 3 +1 ≤ Pr A(θ(G n 2 ) ∨ x, 1, x) &gt; (1 + ε) • SHAP(θ(G n 2 ) ∨ x, 1, x) ≤ Pr A(θ(G n 2 ) ∨ x, 1, x) &gt; (1 + ε) • SHAP(θ(G n 2 ) ∨ x, 1, x) ∨ A(θ(G n 2 ) ∨ x, 1, x) &lt; (1 -ε) • SHAP(θ(G n 2 ) ∨ x, 1, x) = 1 -Pr A(θ(G n 2 ) ∨ x, 1, x) ≤ (1 + ε) • SHAP(θ(G n 2 ) ∨ x, 1, x) ∧ A(θ(G n 2 ) ∨ x, 1, x) ≥ (1 -ε) • SHAP(θ(G n 2 ) ∨ x, 1, x) = 1 -Pr A(θ(G n 2 ) ∨ x, 1, x) -SHAP(θ(G n 2 ) ∨ x, 1, x) ≤ ε • SHAP(θ(G n 2 ) ∨ x, 1, x) ≤ 1 - 3 4 = 1 4 ,</formula><p>where in the last step we use the fact that A is an ε-PRA for the problem of computing the SHAP score for Boolean classifiers given as 2-POS-DNF formulas, and the fact that SHAP(θ(G n 2 ) ∨ x, 1, x) ≥ 0 (as can be seen from Lemma 12).</p><p>• Assume now that G contains a clique with m nodes. We can assume that n &gt; n ε , since otherwise we know that B returns the correct answer in Step 2. Given that n &gt; n ε , we know from Lemma 14 that:</p><formula xml:id="formula_87">(1 + ε) • 2 2 m 3 n 2 +n 2 n 3 +1 &lt; (1 -ε) • 1 (n 3 + 1) • 2 mn 2 2 n 3 +1 . (<label>17</label></formula><formula xml:id="formula_88">)</formula><p>The error probability of algorithm B is equal to Pr(B(G, m) returns no). By using the definition of B, Lemma 13 (b) with r = n 2 , and inequality (17), we obtain that:</p><formula xml:id="formula_89">Pr(B(G, m) returns no) = Pr A(θ(G n 2 ) ∨ x, 1, x) ≤ (1 + ε) • 2 2 m 3 n 2 +n 2 n 3 +1 ≤ Pr A(θ(G n 2 ) ∨ x, 1, x) &lt; (1 -ε) • 1 (n 3 + 1) • 2 mn 2 2 n 3 +1 ≤ Pr A(θ(G n 2 ) ∨ x, 1, x) &lt; (1 -ε) • SHAP(θ(G n 2 ) ∨ x, 1, x) ≤</formula><p>which proves the lemma.</p><p>But it is clear that if M is a Boolean classifier defined with a 2-POS-DNF formula ϕ, then InvPol(M ) is the Boolean classifier defined by the formula obtained from ϕ by replacing every occurrence of a variable x by the literal ¬x; that is, a formula in 2-NEG-DNF.</p><p>Combining Theorem 9 with Lemma 15 then establishes Corollary 10 for the case of 2-NEG-DNF. The same argument shows that the results for 2-POS-CNF and 2-NEG-CNF are equivalent, so all that remains to do in this section is to prove the result, say, for 2-NEG-CNF. But this simply comes from the fact that the negation of a 2-POS-DNF formula is a 2-NEG-CNF formula, and from the fact that, for a Boolean classifier M we have that SHAP(M, e, x) = -SHAP(¬M, e, x).</p><p>This last property can be easily shown by considering that φ(M, e, S)+φ(¬M, e, S) = 1, for every Boolean classifier M over a set of features X and every subset S of X. This concludes the proof of Corollary 10.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">On Comparing the Values of the SHAP-Score</head><p>We now turn our attention to the problem of comparing the SHAP-score of the features of an entity. The reason we are interested in this problem is that it could be the case that computing the SHAP-score exactly or approximately is intractable (as we have shown in the last two sections), but yet that we are able to compare the relevance of features.</p><p>In this section we will again use the uniform distribution and drop the subscripts U. We now define the problem that we consider. Given a class C of Boolean classifiers, the input of the problem CompSHAP(C) is a Boolean classifier M ∈ C over a set of features X, an entity e over X and two features x, y ∈ X, and the question to answer is whether SHAP(M, e, x) &gt; SHAP(M, e, y). <ref type="foot" target="#foot_4">8</ref>We prove that this problem is unlikely to be tractable, even for very restricted Boolean classifiers. A formula ϕ is said to be in 3-POS-DNF if ϕ is in POS-DNF and every disjunct in ϕ contains at most three variables. Then we have that: Hence, all we have to do is to prove Theorem 16. The proof is again technical and divided in two parts, highlighted in bold in the rest of this section.</p><p>Proof Let S ⊆ X \ {x, y}. We have that: Similarly we have that:</p><formula xml:id="formula_91">φ((¬M ∧ x) ∨ (¬M ∧ y), e, S ∪ {x}) = e ∈cw(e,S∪{x}) 1 2 |X\(S∪{x})| • ((¬M ∧ x) ∨ (¬M ∧ y))(e ) = 1 2 |X\(S∪{x})|</formula><formula xml:id="formula_92">φ((¬M ∧ x) ∨ (¬M ∧ y), e, S ∪ {y}) = 1 2 |X\(S∪{y})| • #SAT(¬M , S).</formula><p>Hence, we have from Lemma 19 (applied to the classifier (¬M ∧ x) ∨ (¬M ∧ y)) that:</p><formula xml:id="formula_93">SHAP((¬M ∧ x) ∨ (¬M ∧ y), e, y) -SHAP((¬M ∧ x) ∨ (¬M ∧ y), e, x) = S⊆X\{x,y} |S|!(|X| -|S| -2)! (|X| -1)! • 1 2 |X\(S∪{y})| • #SAT(¬M , S) - 1 2 |X\(S∪{x})| • #SAT(¬M, S) = n-2 k=0 k!(n -k -2)! (n -1)! • 1 2 n-k-1 S⊆X\{x,y} : |S|=k #SAT(¬M , S) - n-2 k=0 k!(n -k -2)! (n -1)! • 1 2 n-k-1 S⊆X\{x,y} : |S|=k #SAT(¬M, S) = n-2 k=0 k!(n -k -2)! (n -1)! • 1 2 n-k-1 S⊆X\{x,y} : |S|=k 2 n-2-k -#SAT(M , S) - n-2 k=0 k!(n -k -2)! (n -1)! • 1 2 n-k-1 S⊆X\{x,y} : |S|=k 2 n-2-k -#SAT(M, S) = n-2 k=0 k!(n -k -2)! (n -1)! • 1 2 n-k-1 S⊆X\{x,y} : |S|=k #SAT(M, S) - n-2 k=0 k!(n -k -2)! (n -1)! • 1 2 n-k-1 S⊆X\{x,y} : |S|=k #SAT(M , S))</formula><p>We can now use Lemma 20 to conclude the proof.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proof of Lemma 18</head><p>Let M, M be Boolean classifiers in C 2-NEG-CNF over a set of variables X, which are inputs of DistCompSHAP. Remember that we want to decide whether SHAP(M ∧ ¬x, e, x) &gt; SHAP(M ∧ ¬y, e , y), where x, y are two features not occurring in X, e is the entity over X ∪ {x} such that e(x) = 0 and e(z) = 1 for every z ∈ X, and e is the entity over X ∪ {y} such that e (y) = 0 and e (z) = 1 for every z ∈ X. Observe that ¬M is a formula in 2-POS-DNF, and that ¬M ∧ y is a formula in 3-POS-DNF (obtained by adding y to every term of ¬M ). Similarly, ¬M ∧ x is a formula in 3-POS-DNF. Therefore, (¬M ∧ x) ∨ (¬M ∧ y) is also a formula in 3-POS-DNF, and then Lemma 21 can be used to establish the polynomial-time many-one reduction. This concludes the first part of the proof. Next, we show that DistCompSHAP is unlikely to be in BPP for the class of Boolean classifiers given as 2-NEG-CNF formulas.</p><p>Intractability of DistCompSHAP over 2-NEG-CNF. We now prove that if DistCompSHAP(C 2-NEG-CNF ) ∈ BPP, then NP = RP. Notice that this implies that Theorem 16 holds, given that in the previous section we prove that if</p><formula xml:id="formula_94">CompSHAP(C 3-POS-DNF ) ∈ BPP, then DistCompSHAP(C 2-NEG-CNF ) ∈ BPP.</formula><p>In what follows, we consider the same class of graphs as in the proof of Theorem 9, that is, the class of all undirected and loop-free graphs G = (N, E) containing at least two isolated nodes (see Condition (A)). Besides, recall the definition of the 2-POS-DNF formula θ(G) from Equation ( <ref type="formula">16</ref>). We start our proof with the following lemma:</p><p>Lemma 22 Let G = (N, E) be a graph with n = |N | nodes, x be a propositional variable not occurring in θ(G), and e be an entity such that e(x) = 0 and e(y) = 1 for each variable y occurring in θ(G). Then we have that:</p><formula xml:id="formula_95">SHAP(¬θ(G) ∧ ¬x, e, x) = 1 2(n + 1) n k=0 k!(n -k)! n! • 1 2 n-k S⊆N : |S|=k #CLIQUE(G, S).</formula><p>Proof The proof is similar to that of Lemma 12, but this time we use Lemma 20 instead of Lemma 11.</p><p>We will also need the fact that this quantity cannot be approximated: Lemma 23 For every ε ∈ (0, 1), the following problem does not admit an ε-PRA, unless NP = RP. Given as input a graph G, compute SHAP(¬θ(G) ∧ ¬x, e, x), where x is a fresh variable not occurring in θ(G) and e is an entity such that e(x) = 0 and e(y) = 1 for each variable y occurring in θ(G).</p><p>Proof From the proof of Theorem 9, we get that there is no ε-PRA for the following problem, unless NP = RP. Given as input a graph G, compute the quantity:</p><formula xml:id="formula_96">1 2(n + 1) n k=0 k!(n -k)! n! • 1 2 n-k S⊆N : |S|=k #CLIQUE(G, S).</formula><p>Hence, Lemma 22 allows us to conclude that Lemma 23 holds.</p><p>The idea of our proof is then the following. We will prove that if DistCompSHAP(C 2-NEG-CNF ) is in BPP, then there exists a 9 10 -PRA for the problem in Lemma 23, hence deducing that NP = RP. To this end, we introduce a class of graphs and calculate the values of SHAP(¬θ(G) ∧ ¬x, e, x) over them. Given n, t ∈ N with t ≤ n, we define the graph G n,t = (N n,t , E n,t ) with N n,t = {a 1 , . . . , a t } ∪ {b 1 , . . . , b n-t } and E n,t = {(b i , b j ) | i, j ∈ {1, . . . n -t} and i = j}. In other words, G n,t is the disjoint union of a clique with n -t nodes and of a graph consisting of t isolated nodes. For instance, the graph G 8,3 is illustrated in Figure <ref type="figure" target="#fig_10">4</ref>. We calculate the values SHAP(¬θ(G n,t ) ∧ ¬x, e, x) in the next lemma.</p><p>Lemma 24 Let n, t ∈ N such that t ≤ n, x be a variable not occurring in θ(G n,t ) and e an entity such e(x) = 0 and e(y) = 1 for every variable y occurring in θ(G n,t ). Then:</p><formula xml:id="formula_97">SHAP(¬θ(G n,t ) ∧ ¬x, e, x) = t n(n + 1)2 n + 1 (t + 1)2 t+1 .</formula><p>Proof By Lemma 22 we have</p><formula xml:id="formula_98">SHAP(¬θ(G n,t ) ∧ ¬x, e, x) = 1 2(n + 1) n k=0 k!(n -k)! n! • 1 2 n-k S⊆Nn,t : |S|=k #CLIQUE(G, S).</formula><p>Let A be the set of the t isolated nodes of G n,t and B be the set of the n -t nodes of G n,t that form a clique. In the above expression, it is clear that if S intersects both A and B then #CLIQUE(G, S) is empty (because S is already not a clique). Moreover, if S is included in A and is not empty, then #CLIQUE(G, S) is 1 if |S| = 1 and is empty otherwise. Finally, if S is included in B then we have #CLIQUE(G, S) = 2 n-t-|S| . Thus, we obtain</p><formula xml:id="formula_99">SHAP(¬θ(G n,t ) ∧ ¬x, e, x) = 1 2(n + 1) 1!(n -1)! n! • 1 2 n-1 S⊆A : |S|=1 1 + 1 2(n + 1) n-t k=0 k!(n -k)! n! • 1 2 n-k S⊆B : |S|=k 2 n-t-k = t n(n + 1)2 n + 1 2(n + 1) n-t k=0 k!(n -k)! n! • 1 2 n-k n -t k 2 n-t-k = t n(n + 1)2 n + 1 2(n + 1)2 t n-t k=0 (n -t)!(n -k)! n!(n -t -k)!</formula><p>We prove in Appendix C.3 that the sum on the right is equal to n+1 t+1 , so we get</p><formula xml:id="formula_100">SHAP(¬θ(G n,t ) ∧ ¬x, e, x) = t n(n + 1)2 n + 1 2(n + 1)2 t • n + 1 t + 1 = t n(n + 1)2 n + 1 (t + 1)2 t+1 ,</formula><p>which was to be shown.</p><p>As a final ingredient, we will also need the following simple observation:</p><p>Lemma 25 Let G = (N, E) be a graph with n = |N |, x be a variable not occurring in θ(G) and e an entity such e(x) = 0 and e(y) = 1 for every variable y occurring in θ(G). Assuming, without loss of generality, that G n,t has the same nodes as G, for each t ∈ {0, . . . , n}, we have that:</p><formula xml:id="formula_101">SHAP(¬θ(G n,n ) ∧ ¬x, e, x) ≤ SHAP(¬θ(G) ∧ ¬x, e, x) ≤ SHAP(¬θ(G n,2 ) ∧ ¬x, e, x).</formula><p>Proof By looking at Lemma 22, we see that SHAP(¬θ(G) ∧ ¬x, e, x) can only increase when edges are added to the graph G. Therefore, this quantity is lower bounded by the quantity for the graph with n isolated nodes (that is, G n,n ) and upper bounded by the quantity for the graph G n,2 , given that we only consider graphs with at least two isolated nodes.</p><p>We have the necessary ingredients to finish the proof of Theorem 16. Assume that there exists a BPP algorithm A for the problem DistCompSHAP(C 2-NEG-CNF ). Then the input of A is a pair of Boolean classifiers M, M over a set of features X given as 2-NEG-CNF formulas, and the task is to verify whether SHAP(M ∧ ¬x, e, x) &gt; SHAP(M ∧ ¬y, e , y), where x, y are two features not occurring in X, e is an entity over X ∪{x} such that e(x) = 0 and e(z) = 1 for every z ∈ X, and e is an entity over X ∪ {y} such that e (y) = 0 and e (z) = 1 for every z ∈ X. Using the amplification lemma of BPP <ref type="bibr">(Goldreich, 2008, p. 231)</ref>, we can ensure that A satisfies the following conditions:</p><formula xml:id="formula_102">• if SHAP(M ∧ ¬x, e, x) &gt; SHAP(M ∧ ¬y, e , y), then Pr(A(M, M ) outputs yes) ≥ 1 - 1 4( M + M ) . • If SHAP(M ∧ ¬x, e, x) ≤ SHAP(M ∧ ¬y, e , y), then Pr(A(M, M ) outputs no) ≥ 1 - 1 4( M + M ) .</formula><p>Moreover, A(M, M ) works in time O(p( M + M )), where p(•) is a fixed polynomial, and M , M are the sizes of M and M represented as input strings, respectively. By using BPP algorithm A, we will define an algorithm B for approximating, given a graph G = (N, E) containing at least 2 isolated nodes, the value SHAP(¬θ(G) ∧ ¬x, e, x), where x is a feature not occurring in N and e is the entity over N ∪ {x} such that e(x) = 0 and e(z) = 1 for every z ∈ N . More precisely, we will show that B is a 9 10 -PRA for this problem. As mentioned above, this concludes the proof of Theorem 16 by Lemma 23 and Lemma 18. We now define B.</p><p>Given a graph G = (N, E) containing at least 2 isolated nodes, and assuming that n = |N |, algorithm B(G) performs the following steps:</p><p>1. If n &lt; 4 then compute the exact value of SHAP(¬θ(G) ∧ ¬x, e, x) by using an exhaustive approach.</p><p>2. For t = 2 to n: </p><p>We can assume without loss of generality that n ≥ 4, since otherwise the algorithm has computed the exact value. Assume initially that every call A(¬θ(G), ¬θ(G n,t )) in algorithm B returns the correct result (we will come back to this assumption later). Because the values SHAP(¬θ(G n,t )∧¬x, e, x) are strictly decreasing with t (this is clear by looking at the expression in Lemma 22), by Lemma 25 we have that either SHAP(¬θ(G) ∧ ¬x, e, x) = SHAP(¬θ(G n,n )∧¬x, e, x) or there exists t ∈ {3, . . . , n} such that A(¬θ(G), ¬θ(G n,t )) = yes and A(¬θ(G), ¬θ(G n,t-1 )) = no (notice that A(¬θ(G), ¬θ(G n,2 )) = no since G contains at least 2 isolated nodes). In the former case, the third step of our algorithm ensures that we return the correct value, so let us focus on the latter case. For some t ∈ {3, . . . , n}, we have that: </p><formula xml:id="formula_104">SHAP</formula><p>where the last inequality comes from Equation (20).</p><p>on its input/output relation (c.f. <ref type="bibr" target="#b45">(Samek et al., 2021)</ref> for a recent and thorough review). Quite recent approaches to the identification and modeling of causal structures in deep learning should open the door for new methodologies for interpretation and explanation <ref type="bibr" target="#b0">(Ahmed et al., 2020;</ref><ref type="bibr" target="#b46">Schölkopf et al., 2021)</ref>.</p><p>To a large extent, explanations in ML are based on different forms of counterfactual-or causality-based approaches; and the concept of explanation as such is left rather implicit. However, explanations have been treated in more explicit terms in many disciplines, and, in particular, in AI, under consistency-based and abductive diagnosis, the main two forms of model-based diagnosis <ref type="bibr" target="#b55">(Struss, 2008)</ref>. These are explanations in knowledge representation with open models. Recent progress has been made in applying model-based diagnosis in Explainable ML. C.f. <ref type="bibr" target="#b6">Bertossi (2023)</ref> and Marques-Silva (2022) for technical details and references. However, a deeper investigation of this connection in the context of explanations for classification is open.</p><p>We conclude this discussion by mentioning a few research directions that our work opens. First, it would be interesting to extend our tractability results of Sections 3 and 4 to a setting that could allow for continuous variables, instead of the discrete variables that we have considered so far. A starting point for such an investigation could for instance be the framework of probabilistic circuits proposed by <ref type="bibr">Van den Broeck et al. (2019)</ref>: these are data structures that can be used to represent probability distributions (possibly with continuous variables) so as to ensure the tractability of certain tasks, such as expectation computation or probabilistic inference. Since these circuits are very similar in nature to the deterministic and decomposable circuit classes that we consider here, it seems that they could also be used for the SHAP-score. Another natural direction would be to study the complexity of approximately computing the SHAP-score under so-called empirical distributions, as is done in <ref type="bibr" target="#b59">Van den Broeck et al. (2021</ref><ref type="bibr" target="#b33">, 2022)</ref> for the case of exact computation where they show that it is intractable in general. For instance, is it the case that there is still no FPRAS for approximating the SHAP-score of DNF formulas (or monotone DNFs) under these distributions? Last, we leave open the question of finding a natural class of Boolean classifiers for which we could efficiently approximate the SHAP-score (via an FPRAS), or efficiently compare the SHAP-score of different features, whereas computing this score exactly would be intractable. As we mentioned in Section 6, given that exact model counting for DNF formulas is intractable but has an FPRAS, and given the strong connections between model counting and the SHAP-score established in Section 5 or by <ref type="bibr" target="#b59">Van den Broeck et al. (2021</ref><ref type="bibr" target="#b33">, 2022)</ref>, DNF formulas were a natural candidate for this. But, quite surprisingly, our non-approximability results indicate that this is not the case.</p><p>Therefore, we conclude that the inequality in the statement of the lemma holds from the fact that there exists z 0 such that:</p><formula xml:id="formula_106">1 + ε 1 -ε • (z 3 + 1) • 2 z &lt; 2 z 2 3</formula><p>for every z ≥ z 0 .</p><p>This is indeed clear, as the dominating factor of the left term is 2 O(z) whereas that of the the right term is 2 O(z 2 ) .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 Proof of Lemma 19</head><p>We prove the more general claim that for any Boolean classifier M over features X, probability distribution D over ent(X), entity e ∈ ent(X) and features x, y ∈ X, we have We cut the first sum in half by considering (1) those S that are ⊆ X \ {x, y} and (2) those S ⊆ X such that with y ∈ S and x / ∈ S; and do the same for the second sum. We end up with This concludes the proof.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3 The summation in Lemma 24</head><p>To ease the proof, we let s = n -t. Then we want to compute We have that: </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>φ</head><label></label><figDesc>D (M, e, S) := E e ∼D M (e ) | e ∈ cw(e, S) .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: A deterministic and decomposable Boolean Circuit as a classifier.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>cuit. Therefore, it is good enough to show how to compute in polynomial time the quantities Diff k (C, e, x) for each k ∈ {0, . . . , n -1}. By definition of φ Π• (•, •, •) we have that Diff k (C, e, x) = S⊆X\{x} |S|=k E e ∼Πp [C(e ) | e ∈ cw(e, S ∪ {x})] -S⊆X\{x} |S|=k E e ∼Πp [C(e ) | e ∈ cw(e, S)] . (4) In this expression, let L and R be the left-and right-hand side terms in the subtraction. Looking closer at R, we have by standard properties of conditional expectation that R = S⊆X\{x} |S|=k E e ∼Πp [C(e ) | e ∈ cw(e, S)] = p(x) • S⊆X\{x} |S|=k E e ∼Πp [C(e ) | e ∈ cw(e, S) and e (x) = 1] + (1 -p(x)) • S⊆X\{x} |S|=k E e ∼Πp [C(e ) | e ∈ cw(e, S) and e (x) = 0].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>E</head><label></label><figDesc>e (y) | e ∈ cw(e |{y} , S)] = E e ∼Πp |{y} [e (y) | e ∈ cw(e |{y} , ∅)] 4. The only difference between Rg and Cg (defined in Section 2) is that we formally regard Rg as a Boolean classifier over var(g), while we formally regarded Cg as a Boolean classifier over X. = E e ∼Πp |{y} [e (y)] = 1 • p(y) + 0 • (1 -p(y)) e ∼Πp |{y} [e (y) | e ∈ cw(e |{y} , S)] = E e ∼Πp |{y} [e (y) | e ∈ cw(e |{y} , {y})]= e(y).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Execution of Algorithm 1 over the deterministic and decomposable Boolean circuit depicted in Figure 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>g 2 (e ) | e ∈ cw(e |var(g) , S)] = S⊆var(g) |S|= E e ∼Πp |var(g) [R g 1 (e ) | e ∈ cw(e |var(g) , S)] |var(g) [R g 2 (e ) | e ∈ cw(e |var(g) , S)]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>Section 3.1.1 still works, for instance the term R from Equation (4) becomes R = S⊆X\{x} |S|=k E e ∼Πp [C(e ) | e ∈ cw(e, S)] = v∈dom(X) p x (v) • H Πp |X\{x} (C x=v , e |X\{x} , k).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>A 2 -</head><label>2</label><figDesc>POS-DNF formula for cliques. Given a graph G = (N, E) we define the formula θ(G) in 2-POS-DNF by θ(G) := (a,b)∈(N ×N ) a =b and (a,b) ∈E a ∧ b. (16) Notice that the set of propositional variables occurring in θ(G) is the same as the set N of nodes of G, given that G satisfies condition (A). For S ⊆ N we define #CLIQUE(G, S) := |{Y | Y is a clique of G and S ⊆ Y }|.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Theorem 16</head><label>16</label><figDesc>Let C 3-POS-DNF be the class of Boolean classifiers given as 3-POS-DNF formulas. If CompSHAP(C 3-POS-DNF ) ∈ BPP, then NP = RP. Moreover, as in the previous section, Lemma 15 and Equation (18) directly imply that this result extends to the closure of 3-POS-DNF formulas by duals and negations: Corollary 17 Let F ∈ {3-POS-DNF, 3-NEG-DNF, 3-POS-CNF, 3-NEG-CNF} and C F be the class of Boolean classifiers given as formulas in F. If CompSHAP(C F ) ∈ BPP, then NP = RP.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>e,S∪{x}) : e (y)=1 (¬M ∧ y)(e ) + e ∈cw(e,S∪{x}) : e (y)=0 (¬M ∧ y)(e ) = 1 2 |X\(S∪{x})| e ∈cw(e,S∪{x}) : e (y)=1 (¬M )(e ) = 1 2 |X\(S∪{x})| e ∈cw(e |X\{x,y} ,S) (¬M )(e ) = 1 2 |X\(S∪{x})| • #SAT(¬M, S).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Illustration of the graph G 8,3 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head></head><label></label><figDesc>(a) if A(¬θ(G), ¬θ(G n,t )) = yes, then return1 2 SHAP(¬θ(G n,t-1 ) ∧ ¬x, e, x) + SHAP(¬θ(G n,t ) ∧ ¬x, e, x)3. Return SHAP(¬θ(G n,n ) ∧ ¬x, e, x). Algorithm B works in polynomial time since each graph G n,t can be constructed in polynomial time in n, 2-NEG-CNF formulas ¬θ(G) and ¬θ(G n,t ) can be constructed in polynomial time from graphs G and G n,t , algorithm A(¬θ(G), ¬θ(G n,t )) works in time O(p( ¬θ(G) + ¬θ(G n,t ) )), and the value returned in either Step 2a or Step 3 can be computed in polynomial time in n by Lemma 24. As the final step of this proof, we need to show that: Pr B(G) -SHAP(¬θ(G) ∧ ¬x, e, x) ≤ 9 10 • SHAP(¬θ(G) ∧ ¬x, e, x) ≥ 3 4 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head></head><label></label><figDesc>SHAP D (M, e, x) -SHAP D (M, e, y) = S⊆X\{x,y} |S|! (|X| -|S| -2)! (|X| -1)! φ D (M, e, S ∪ {x}) -φ D (M, e, S ∪ {y}) .Indeed, we have:SHAP D (M, e, x) -SHAP D (M, e, y) = S⊆X\{x} |S|! (|X| -|S| -1)! |X|! φ D (M, e, S ∪ {x}) -φ D (M, e, S)-S⊆X\{y} |S|! (|X| -|S| -1)! |X|! φ D (M, e, S ∪ {y}) -φ D (M, e, S)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head></head><label></label><figDesc>SHAP D (M, e, x) -SHAP D (M, e, y) = S⊆X\{x,y} |S|! (|X| -|S| -1)! |X|! φ D (M, e, S ∪ {x}) -φ D (M, e, S ∪ {y}) + S⊆X\{x,y} (|S| + 1)! (|X| -|S| -2)! |X|! φ D (M, e, S ∪ {x, y}) -φ D (M, e, S ∪ {y}) -S⊆X\{x,y} (|S| + 1)! (|X| -|S| -2)! |X|! φ D (M,e, S ∪ {x, y}) -φ D (M, e, S ∪ {x}) = S⊆X\{x,y} |S|! (|X| -|S| -1)! |X|! φ D (M, e, S ∪ {x}) -φ D (M, e, S ∪ {y}) + S⊆X\{x,y} (|S| + 1)! (|X| -|S| -2)! |X|! φ D (M, e, S ∪ {x}) -φ D (M, e, S ∪ {y}) = S⊆X\{x,y} |S|! (|X| -|S| -2)! (|X| -1)! φ D (M, e, S ∪ {x}) -φ D (M, e, S ∪ {y}) .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head></head><label></label><figDesc>s + t -k)! (s + t)!(s -k)!.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>single constant 1-gate. Otherwise, d S 1 encodes the propositional formula ∧ x∈S 1 (x∨¬x) but it is constructed as a circuit in such a way that every ∧-and ∨-gate has fan-in exactly 2. Boolean circuit d S 2 is constructed exactly as d S 1 but considering the set of variables S 2 instead of S 1 . Observe that var(d S 1 ) = S 1 , var(d S 2 ) = S 2 , that d S 1 and d S 2 always evaluate to 1, and that all ∨-gates appearing in d S 1 and in d S 2 are deterministic. Then, we transform g into a smooth ∨-gate by replacing gate g 1 by a decomposable ∧-gate (g 1 ∧d S 2 ), and gate g 2 by a decomposable ∧gate (g 2 ∧ d S 1 ). Clearly, this does not change the Boolean classifier computed, and g is again deterministic because g 1 and (g 1 ∧d S</figDesc><table /><note><p>2 ) (resp., g 2 and (g 2 ∧d S 1 )) capture the same Boolean classifier. Moreover, since var(g 1</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>(¬θ(G n,t ) ∧ ¬x, e, x) &lt; SHAP(¬θ(G) ∧ ¬x, e, x) ≤ SHAP(¬θ(G n,t-1 ) ∧ ¬x, e, x). (20)Therefore, by considering that the returned value in Step 2a of invocation B(G) is the average of the left and right terms of the above interval, we obtain that:</figDesc><table><row><cell cols="5">Now, we have by Lemma 24 that:</cell></row><row><cell>1 2</cell><cell cols="4">SHAP(¬θ(G t -1 n(n + 1)2 n+1 +</cell><cell>1 t2 t+1 -</cell><cell>t n(n + 1)2 n+1 -</cell><cell>1 (t + 1)2 t+2 =</cell></row><row><cell></cell><cell cols="3">1 t2 t+1 -</cell><cell cols="2">1 (t + 1)2 t+2 -</cell><cell>1 n(n + 1)2 n+1 ≤</cell></row><row><cell></cell><cell cols="3">1 t2 t+1 -</cell><cell cols="2">1 (t + 1)2 t+2 =</cell></row><row><cell></cell><cell>t + 2 2t</cell><cell>•</cell><cell cols="3">1 (t + 1)2 t+1</cell></row><row><cell cols="6">But notice that t+2 2t = 1 2 + 1 t ≤ 9 10 since t ≥ 3. Therefore we have</cell></row><row><cell cols="6">B(G) -SHAP(¬θ(G) ∧ ¬x, e, x) ≤</cell><cell>9 10</cell><cell>•</cell><cell>1 (t + 1)2 t+1</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>≤</cell><cell>9 10</cell><cell>•</cell><cell>t n(n + 1)2 n +</cell><cell>1 (t + 1)2 t+1</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>=</cell><cell>9 10</cell><cell>• SHAP(¬θ(G n,t ) ∧ ¬x, e, x)</cell></row></table><note><p><p>B(G) -SHAP(¬θ(G) ∧ ¬x, e, x) ≤ 1 2 SHAP(¬θ(G n,t-1 ) ∧ ¬x, e, x) -SHAP(¬θ(G n,t ) ∧ ¬x, e, x) . n,t-1 ) ∧ ¬x, e, x) -SHAP(¬θ(G n,t ) ∧ ¬x, e, x) = ≤ 9 10</p>• SHAP(¬θ(G) ∧ ¬x, e, x),</p></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>Equivalently, one can see an entity as a vector of binary values, with each coordinate corresponding to a given feature. We will however always use the functional point of view as it simplifies the notation in our proofs.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_1"><p>Arenas, Barceló, Bertossi, Monet   </p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_2"><p>We first presented a version that used smoothing because it made the tractability proof easier to understand.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_3"><p>For smoothing, we use the circuits dS := x∈S ( v∈dom(x) x = v).</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_4"><p>Note that &gt; can be replaced by any of ≥, &lt;, or ≤, as this does not change the problem.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>Part of this work has been funded by <rs type="programName">ANID -Millennium Science Initiative Program</rs> -Code <rs type="grantNumber">ICN17002</rs>. M. Arenas has been funded by <rs type="funder">Fondecyt</rs> grant <rs type="grantNumber">1191337</rs>. <rs type="person">P. Barceló</rs> has been funded by <rs type="funder">Fondecyt</rs> Grant <rs type="grantNumber">1200967</rs>, and also by the <rs type="funder">National Center for Artificial Intelligence CENIA</rs> <rs type="grantNumber">FB210017</rs>, <rs type="person">Basal ANID. L. Bertossi</rs> is a Senior Researcher at the <rs type="institution">IMFD, Chile</rs>, and a Professor Emeritus at <rs type="institution">Carleton University, Ottawa, Canada</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_CTrsxkC">
					<idno type="grant-number">ICN17002</idno>
					<orgName type="program" subtype="full">ANID -Millennium Science Initiative Program</orgName>
				</org>
				<org type="funding" xml:id="_yvNQT78">
					<idno type="grant-number">1191337</idno>
				</org>
				<org type="funding" xml:id="_gK3s6Sq">
					<idno type="grant-number">1200967</idno>
				</org>
				<org type="funding" xml:id="_ap63UwZ">
					<idno type="grant-number">FB210017</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>where the last step is obtained as in the previous case. This concludes the proof of Theorem 9.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Proof of Corollary 10</head><p>First, we explain how the result for 2-NEG-DNF can be obtained from the result for 2-POS-DNF (Theorem 9). For an entity e over a set of features X, let us write InvPol(e) the entity defined by InvPol(e)(x) := 1 -e(x) for every x ∈ X. For a Boolean classifier M over variables X, we write InvPol(M ) the Boolean classifier defined by InvPol(M )(e) := M (InvPol(e)); in other words, we have changed the polarity of all the features. Then the following holds:</p><p>Lemma 15 Let M be a Boolean classifier over X, e an entity over X and x ∈ X. We have SHAP(M, e, x) = SHAP(InvPol(M ), InvPol(e), x).</p><p>Proof For every S ⊆ X, we have that where x, y are two features not occurring in X, e is the entity over X ∪ {x} such that e(x) = 0 and e(z) = 1 for every z ∈ X, and e is the entity over X ∪ {y} such that e (y) = 0 and e (z) = 1 for every z ∈ X. We claim that DistCompSHAP over 3-NEG-CNF formulas can be reduced in polynomial time to CompSHAP over 2-POS-DNF formulas:</p><p>Lemma 18 Let C 2-NEG-CNF be the class of Boolean classifiers given as 2-NEG-CNF formulas. There exists a polynomial-time many-one reduction from DistCompSHAP(C 2-NEG-CNF ) to CompSHAP(C 3-POS-DNF ).</p><p>This in particular implies that if CompSHAP(C 3-POS-DNF ) is in BPP then so is DistCompSHAP(C 2-NEG-CNF ). We need three lemmas to prove Lemma 18.</p><p>Lemma 19 Given a Boolean classifier M over a set of features X, an entity e over X and features x, y ∈ X, we have that:</p><p>Proof We prove this in Appendix C.2.</p><p>Lemma 20 Let X be a set of features, n = |X|, x ∈ X, M be a Boolean classifier over X \ {x} and e be the entity over X such that e(x) = 0 and e(y) = 1 for every y ∈ X \ {x}.</p><p>Then we have that:</p><p>Proof This can be established in the same way as Lemma 11 is proved.</p><p>Lemma 21 Let X be a set of features, n = |X|, x, y ∈ X, M, M be Boolean classifiers over X \ {x, y} and e be the entity over X such that e(x) = 0, e(y) = 0 and e(z) = 1 for every z ∈ X \ {x, y}. Then we have that:</p><p>To finish with the proof, we need to remove the assumption that every call A(¬θ(G), ¬θ(G n,t )) in algorithm B returns the correct result, and instead compute a lower bound on the probability that this assumption is true. More precisely, we need to show that the probability that every call A(¬θ(G), ¬θ(G n,t )) in algorithm B returns the correct result is at least 3 4 , as this lower bound together with (21) imply that ( <ref type="formula">19</ref>) holds (given that SHAP(¬θ(G)∧¬x, e, x) = |SHAP(¬θ(G)∧¬x, e, x)| by Lemma 25). For every t ∈ {2, . . . , n}, we have that:</p><p>x is strictly increasing for x ≥ 1, and that f (4) &gt; This concludes the proof of Theorem 16.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Final Remarks</head><p>Our algorithm for computing the SHAP-score could be used in practical scenarios. Indeed, some classes of classifiers can be compiled into tractable Boolean circuits. This is the case, for instance, of Bayesian Classifiers <ref type="bibr">(Shih et al., 2018b)</ref>, Binary Neural Networks <ref type="bibr" target="#b48">(Shi et al., 2020)</ref>, and Random Forests <ref type="bibr" target="#b12">(Choi et al., 2020)</ref>. The idea is to start with a Boolean classifier M given in a formalism that is hard to interpret -for instance a binary neural network -and to compute a tractable Boolean circuit M that is equivalent to M (this computation can be expensive). One can then use M and the nice properties of tractable Boolean circuits to explain the decisions of the model. Hence, this makes it possible to apply the results in this paper on the SHAP-score to those classes of classifiers.</p><p>A Boolean circuit representing (or compiled from) another opaque classifier, as those just mentioned, is also more interpretable <ref type="bibr" target="#b44">(Rudin, 2019)</ref>, adding a useful property to that of tractability. Still, it would be interesting to compare the explanations obtained from the compiled circuit with those that can be obtain directly from, say, the original neural network. After all, the SHAP-score can be applied to both. First and recent experiments of this kind are reported in <ref type="bibr" target="#b7">Bertossi and Leon (2023)</ref>. We should mention, however, that there are some recent methods to explain the results from a neural network that do not rely only</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix A. Encoding Binary Decision Trees and FBDDs into Deterministic and Decomposable Boolean Circuits</head><p>In this section we explain how FBDDs and binary decision trees can be encoded in linear time as deterministic and decomposable Boolean circuits. First we need to define these formalisms.</p><p>Binary Decision Diagrams A binary decision diagram (BDD) over a set of variables X is a rooted directed acyclic graph D such that: (i) each internal node is labeled with a variable from X, and has exactly two outgoing edges: one labeled 0, the other one labeled 1; and (ii) each leaf is labeled either 0 or 1. Such a BDD represents a Boolean classifier in the following way. Let e be an entity over X, and let π e = u 1 , . . . , u m be the unique path in D satisfying the following conditions: (a) u 1 is the root of D; (b) u m is a leaf of D; and (c) for every i ∈ {1, . . . , m -1}, if the label of u i is x ∈ X, then the label of the edge (u i , u i+1 ) is equal to e(x). Then the value of e in D, denoted by D(e), is defined as the label of the leaf u m . Moreover, a binary decision diagram D is free (FBDD) if for every path from the root to a leaf, no two nodes on that path have the same label, and a binary decision tree is an FBDD whose underlying graph is a tree.</p><p>Encoding FBDDs and binary decision trees into deterministic and decomposable Boolean circuits (Folklore). Given an FBDD D over a set of variables X, we explain how D can be encoded as a deterministic and decomposable Boolean circuit C over X.</p><p>Notice that the technique used in this example also apply to binary decision trees, as they are a particular case of FBDDs. The construction of C is done by traversing the structure of D in a bottom-up manner. In particular, for every node u of D, we construct a deterministic and decomposable circuit α(u) that is equivalent to the FBDD represented by the subgraph of D rooted at u. More precisely, for a leaf u of D that is labeled with ∈ {0, 1}, we define α(u) to be the Boolean circuit consisting of only one constant gate with label . For an internal node u of D labeled with variable x ∈ X, let u 0 and u 1 be the nodes that we reach from u by following the 0-and 1-labeled edge, respectively. Then α(u) is the Boolean circuit depicted in the following figure:</p><p>x It is clear that the circuit that we obtain is equivalent to the input FBDD. We now argue that this circuit is deterministic and decomposable. For the ∨-gate shown in the figure, if an entity e is accepted by the Boolean circuit in its left-hand size, then e(x) = 0, while if an entity e is accepted by the Boolean circuit in its right-hand size, then e(x) = 1. Hence, we have that this ∨-gate is deterministic, from which we conclude that α(u) is deterministic, as α(u 0 ) and α(u 1 ) are also deterministic by construction. Moreover, the ∧-gates shown in the figure are decomposable as variable x is mentioned neither in α(u 0 ) nor in α(u 1 ): this is because D is a free BDD. Thus, we conclude that α(u) is decomposable, as α(u 0 ) and α(u 1 ) are decomposable by construction. Finally, if u root is the root of D, then by construction we have that α(u root ) is a deterministic and decomposable Boolean circuit equivalent to D. Note that this encoding can trivially be done in linear time. Thus, we often say, by abuse of terminology, that "FBDDs (or binary decision trees) are restricted kinds of deterministic and decomposable circuits".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix B. Proof of a folklore fact</head><p>Fact 26 (Folklore) Let f be a function that admits an ε-PRA for ε ∈ (0, 1). Then the problem of determining, given a string x, whether f (x) = 0 is in BPP.</p><p>Proof Let A be an ε-PRA for f , that is, a randomized algorithm that takes as input a string x, and computes in polynomial time in the size of x a value A(x) such that (</p><p>4 . We claim that the following algorithm is a BPP algorithm for deciding if f (x) = 0: compute A(x), and if this is equal to zero then accept, otherwise reject. We will assume without loss of generality that f (x) ≥ 0, as the case f (x) ≤ 0 can be handled in the same way. Observe that ( ) can be equivalently rewritten as ( †) Pr (1 -ε)f (x) ≤ A(x) ≤ (1 + ε)f (x) ≥ 3 4 . But then:</p><p>• Assume first that f (x) = 0. Then by ( †) we have that Pr A(x) = 0 ≥ 3 4 as well, so that we accept with probability at least 3 4 .</p><p>• Assume now that f (x) = 0. Then ( †) gives us Pr A(x) ≥ (1 -ε)f (x) &gt; 0 ≥ 3 4 , so that we reject with probability at least 3 4 .</p><p>This concludes the proof.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix C. Proofs of Intermediate Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1 Proof of Lemma 14</head><p>We notice that the inequality holds if and only if</p><p>Moreover, given that m ≥ 1, we have that:</p><p>Hence, we conclude from ( <ref type="formula">22</ref>) and 23 that: </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Causalworld: A robotic manipulation benchmark for causal structure and transfer learning</title>
		<author>
			<persName><forename type="first">Ossama</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frederik</forename><surname>Träuble</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anirudh</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Neitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manuel</forename><surname>Wüthrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Bauer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.04296</idno>
		<ptr target="https://arxiv.org/abs/2010.04296" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Connecting knowledge compilation classes and width parameters</title>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Amarilli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Florent</forename><surname>Capelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mikaël</forename><surname>Monet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Senellart</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1811.02944" />
	</analytic>
	<monogr>
		<title level="j">Theory Comput. Syst</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="861" to="914" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The tractability of SHAP-score-based explanations over deterministic and decomposable boolean circuits</title>
		<author>
			<persName><forename type="first">Marcelo</forename><surname>Arenas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pablo</forename><surname>Barceló</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leopoldo</forename><surname>Bertossi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mikaël</forename><surname>Monet</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2007.14045" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI</title>
		<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">Sanjeev</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Boaz</forename><surname>Barak</surname></persName>
		</author>
		<ptr target="https://theory.cs.princeton.edu/complexity/book.pdf" />
		<title level="m">Computational Complexity -A Modern Approach</title>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Probabilistic checking of proofs: A new characterization of NP</title>
		<author>
			<persName><forename type="first">Sanjeev</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shmuel</forename><surname>Safra</surname></persName>
		</author>
		<ptr target="https://www.cs.umd.edu/users/gasarch/TOPICS/pcp/AS.pdf" />
	</analytic>
	<monogr>
		<title level="j">Journal of the ACM (JACM)</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="70" to="122" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Proof verification and the hardness of approximation problems</title>
		<author>
			<persName><forename type="first">Sanjeev</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carsten</forename><surname>Lund</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rajeev</forename><surname>Motwani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Madhu</forename><surname>Sudan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mario</forename><surname>Szegedy</surname></persName>
		</author>
		<ptr target="https://madhu.seas.harvard.edu/papers/1992/almss-conf.pdf" />
	</analytic>
	<monogr>
		<title level="j">Journal of the ACM (JACM)</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="501" to="555" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Attribution-scores and causal counterfactuals as explanations in artificial intelligence</title>
		<author>
			<persName><forename type="first">Leopoldo</forename><surname>Bertossi</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2303.02829</idno>
		<ptr target="https://doi.org/10.48550/arXiv.2303.02829" />
	</analytic>
	<monogr>
		<title level="m">Reasoning Web. Causality, Explanations and Declarative Knowledge</title>
		<title level="s">Springer LNCS</title>
		<imprint>
			<date type="published" when="2023">2023. 2023</date>
			<biblScope unit="volume">13759</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Opening up the neural network classifier for Shap score computation</title>
		<author>
			<persName><forename type="first">Leopoldo</forename><surname>Bertossi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jorge</forename><forename type="middle">E</forename><surname>Leon</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2303.06516</idno>
		<ptr target="https://doi.org/10.48550/arXiv.2303.06516" />
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Causality-based explanation of classification outcomes</title>
		<author>
			<persName><forename type="first">Leopoldo</forename><surname>Bertossi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jordan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maximilian</forename><surname>Schleich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Suciu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zografoula</forename><surname>Vagena</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2003.06868" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourth International Workshop on Data Management for End-to-End Machine Learning</title>
		<meeting>the Fourth International Workshop on Data Management for End-to-End Machine Learning</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Polynomial calculation of the Shapley value based on sampling</title>
		<author>
			<persName><forename type="first">Javier</forename><surname>Castro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Gómez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juan</forename><surname>Tejada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computers &amp; Operations Research</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1726" to="1730" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">An application of the Shapley value to the analysis of co-expression networks</title>
		<author>
			<persName><forename type="first">Giulia</forename><surname>Cesari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Encarnación</forename><surname>Algaba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Moretti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juan</forename><forename type="middle">A</forename><surname>Nepomuceno</surname></persName>
		</author>
		<ptr target="https://hal.archives-ouvertes.fr/hal-02103359" />
	</analytic>
	<monogr>
		<title level="j">Applied Network Science</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">35</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">An interpretable model with globally consistent explanations for credit risk</title>
		<author>
			<persName><forename type="first">Chaofan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kangcheng</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cynthia</forename><surname>Rudin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaron</forename><surname>Shaposhnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sijia</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tong</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.12615</idno>
		<ptr target="https://arxiv.org/abs/1811.12615" />
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">On symbolically encoding the behavior of random forests</title>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><surname>Shih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anchal</forename><surname>Goyanka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adnan</forename><surname>Darwiche</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.01493</idno>
		<ptr target="https://arxiv.org/abs/2007.01493" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Improving KernelSHAP: Practical Shapley value estimation using linear regression</title>
		<author>
			<persName><forename type="first">Ian</forename><surname>Covert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Su-In</forename><surname>Lee</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2012.01536" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="3457" to="3465" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">On the tractable counting of theory models and its application to truth maintenance and belief revision</title>
		<author>
			<persName><forename type="first">Adnan</forename><surname>Darwiche</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/cs/0003044" />
	</analytic>
	<monogr>
		<title level="j">Journal of Applied Non-Classical Logics</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="11" to="34" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">On the reasons behind decisions</title>
		<author>
			<persName><forename type="first">Adnan</forename><surname>Darwiche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Auguste</forename><surname>Hirth</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2002.09284" />
	</analytic>
	<monogr>
		<title level="m">ECAI</title>
		<imprint>
			<publisher>IOS Press</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="712" to="720" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A knowledge compilation map</title>
		<author>
			<persName><forename type="first">Adnan</forename><surname>Darwiche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Marquis</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1106.1819" />
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="229" to="264" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Algorithmic transparency via quantitative input influence: Theory and experiments with learning systems</title>
		<author>
			<persName><forename type="first">Anupam</forename><surname>Datta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shayak</forename><surname>Sen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yair</forename><surname>Zick</surname></persName>
		</author>
		<ptr target="https://www.andrew.cmu.edu/user/danupam/datta-sen-zick-oakland16.pdf" />
	</analytic>
	<monogr>
		<title level="m">2016 IEEE symposium on security and privacy (SP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="598" to="617" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">On the complexity of cooperative solution concepts</title>
		<author>
			<persName><forename type="first">Xiaotie</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christos</forename><forename type="middle">H</forename><surname>Papadimitriou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematics of Operations Research</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="257" to="266" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Explanations for data repair through Shapley values</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Deutch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nave</forename><surname>Frost</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amir</forename><surname>Gilad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oren</forename><surname>Sheffer</surname></persName>
		</author>
		<ptr target="https://amirgilad.github.io/publication/cikm21/CIKM21.pdf" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th ACM International Conference on Information &amp; Knowledge Management</title>
		<meeting>the 30th ACM International Conference on Information &amp; Knowledge Management</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="362" to="371" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Computing the Shapley value of facts in query answering</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Deutch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nave</forename><surname>Frost</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benny</forename><surname>Kimelfeld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mikaël</forename><surname>Monet</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2112.08874" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2022 International Conference on Management of Data</title>
		<meeting>the 2022 International Conference on Management of Data</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="1570" to="1583" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">The Shapley value for cooperative games under precedence constraints</title>
		<author>
			<persName><forename type="first">Ulrich</forename><surname>Faigle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Walter</forename><surname>Kern</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Game Theory</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="249" to="266" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Interactive proofs and the hardness of approximating cliques</title>
		<author>
			<persName><forename type="first">Uriel</forename><surname>Feige</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shafi</forename><surname>Goldwasser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">László</forename><surname>Lovász</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shmuel</forename><surname>Safra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mario</forename><surname>Szegedy</surname></persName>
		</author>
		<ptr target="http://ftp.cs.elte.hu/~lovasz/morepapers/fglss.pdf" />
	</analytic>
	<monogr>
		<title level="j">Journal of the ACM (JACM)</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="268" to="292" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">When explainability meets adversarial learning: Detecting adversarial examples using SHAP signatures</title>
		<author>
			<persName><forename type="first">Gil</forename><surname>Fidel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ron</forename><surname>Bitton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Asaf</forename><surname>Shabtai</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1909.03418" />
	</analytic>
	<monogr>
		<title level="m">2020 International Joint Conference on Neural Networks (IJCNN)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<author>
			<persName><forename type="first">Oded</forename><surname>Goldreich</surname></persName>
		</author>
		<ptr target="http://www.wisdom.weizmann.ac.il/~oded/cc-book.html" />
		<title level="m">Computational complexity: a conceptual perspective</title>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">On the measure of conflicts: Shapley inconsistency values</title>
		<author>
			<persName><forename type="first">Anthony</forename><surname>Hunter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sébastien</forename><surname>Konieczny</surname></persName>
		</author>
		<ptr target="http://www.cril.univ-artois.fr/~konieczny/papers/aij10a.pdf" />
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">174</biblScope>
			<biblScope unit="issue">14</biblScope>
			<biblScope unit="page" from="1007" to="1026" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Monte-carlo approximation algorithms for enumeration problems</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Richard M Karp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neal</forename><surname>Luby</surname></persName>
		</author>
		<author>
			<persName><surname>Madras</surname></persName>
		</author>
		<ptr target="https://www.math.cmu.edu/~af1p/Teaching/MCC17/Papers/KLM.pdf" />
	</analytic>
	<monogr>
		<title level="j">Journal of algorithms</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="429" to="448" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Some observations on the probabilistic algorithms and NP-hard problems</title>
		<author>
			<persName><forename type="first">-I</forename><surname>Ker</surname></persName>
		</author>
		<author>
			<persName><surname>Ko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">formation Processing Letters</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="39" to="43" />
			<date type="published" when="1982">1982</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Problems with Shapley-value-based explanations as feature importance measures</title>
		<author>
			<persName><forename type="first">Elizabeth</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suresh</forename><surname>Venkatasubramanian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlos</forename><surname>Scheidegger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sorelle</forename><surname>Friedler</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2002.11097" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="5491" to="5500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">The Shapley value of tuples in query answering</title>
		<author>
			<persName><forename type="first">Ester</forename><surname>Livshits</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leopoldo</forename><surname>Bertossi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benny</forename><surname>Kimelfeld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moshe</forename><surname>Sebag</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1904.08679" />
	</analytic>
	<monogr>
		<title level="m">ICDT</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">155</biblScope>
			<biblScope unit="page" from="1" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">The Shapley value of tuples in query answering</title>
		<author>
			<persName><forename type="first">Ester</forename><surname>Livshits</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leopoldo</forename><surname>Bertossi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benny</forename><surname>Kimelfeld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moshe</forename><surname>Sebag</surname></persName>
		</author>
		<idno type="DOI">10.46298/lmcs-17(3:22)2021</idno>
		<ptr target="https://doi.org/10.46298/lmcs-17(3:22)2021" />
	</analytic>
	<monogr>
		<title level="j">Log. Methods Comput. Sci</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A unified approach to interpreting model predictions</title>
		<author>
			<persName><forename type="first">M</forename><surname>Scott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Su-In</forename><surname>Lundberg</surname></persName>
		</author>
		<author>
			<persName><surname>Lee</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1705.07874" />
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="4765" to="4774" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">From local explanations to global understanding with explainable AI for trees</title>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Scott M Lundberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hugh</forename><surname>Erion</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><surname>Degrave</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bala</forename><surname>Jordan M Prutkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ronit</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Katz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nisha</forename><surname>Himmelfarb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Su-In</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName><surname>Lee</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1905.04610" />
	</analytic>
	<monogr>
		<title level="j">Nature Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="56" to="67" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Logic-based explainability in machine learning</title>
		<author>
			<persName><forename type="first">João</forename><surname>Marques-Silva</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2211.00541</idno>
		<ptr target="https://doi.org/10.48550/arXiv.2211.00541" />
	</analytic>
	<monogr>
		<title level="m">Reasoning Web. Causality, Explanations and Declarative Knowledge</title>
		<title level="s">Springer LNCS</title>
		<imprint>
			<date type="published" when="2022">2022. 2023</date>
			<biblScope unit="volume">13759</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">The explanation game: Explaining machine learning models using Shapley values</title>
		<author>
			<persName><forename type="first">Luke</forename><surname>Merrick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ankur</forename><surname>Taly</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1909.08128" />
	</analytic>
	<monogr>
		<title level="s">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">Cd-Make</forename></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">12279</biblScope>
			<biblScope unit="page" from="17" to="38" />
			<date type="published" when="2020">2020</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Efficient computation of the Shapley value for game-theoretic network centrality</title>
		<author>
			<persName><surname>Tomasz P Michalak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Karthik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><forename type="middle">L</forename><surname>Aadithya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Balaraman</forename><surname>Szczepanski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><forename type="middle">R</forename><surname>Ravindran</surname></persName>
		</author>
		<author>
			<persName><surname>Jennings</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1402.0567" />
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="page" from="607" to="650" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Sampling permutations for Shapley value estimation</title>
		<author>
			<persName><forename type="first">Rory</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><surname>Cooper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eibe</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Holmes</surname></persName>
		</author>
		<ptr target="https://www.jmlr.org/papers/volume23/21-0439/21-0439.pdf" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Interpretable machine learning: A guide for making black box models explainable</title>
		<author>
			<persName><forename type="first">C</forename><surname>Molnar</surname></persName>
		</author>
		<ptr target="https://christophm.github.io/interpretable-ml-book/" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">A multilinear sampling algorithm to estimate Shapley values</title>
		<author>
			<persName><forename type="first">Ramin</forename><surname>Okhrati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aldo</forename><surname>Lipani</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2010.12082" />
	</analytic>
	<monogr>
		<title level="m">2020 25th International Conference on Pattern Recognition (ICPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="7992" to="7999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Einsum networks: Fast and scalable learning of tractable probabilistic circuits</title>
		<author>
			<persName><forename type="first">Robert</forename><surname>Peharz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antonio</forename><surname>Vergari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karl</forename><surname>Stelzner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alejandro</forename><surname>Molina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Trapp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guy</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristian</forename><surname>Broeck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zoubin</forename><surname>Kersting</surname></persName>
		</author>
		<author>
			<persName><surname>Ghahramani</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.06231</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">The complexity of counting cuts and of computing the probability that a graph is connected</title>
		<author>
			<persName><forename type="first">Provan</forename><surname>Scott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">O</forename><surname>Ball</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Computing</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="777" to="788" />
			<date type="published" when="1983">1983</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Generating counterfactual and contrastive explanations using SHAP</title>
		<author>
			<persName><forename type="first">Shubham</forename><surname>Rathi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.09293</idno>
		<ptr target="http://arxiv.org/abs/1906.09293" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Explaining the predictions of any classifier</title>
		<author>
			<persName><forename type="first">Marco</forename><surname>Túlio Ribeiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sameer</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlos</forename><surname>Guestrin</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1602.04938" />
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1135" to="1144" />
		</imprint>
	</monogr>
	<note>Why should I trust you?</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<author>
			<persName><forename type="first">Alvin</forename><forename type="middle">E</forename><surname>Roth</surname></persName>
		</author>
		<ptr target="http://www.library.fa.ru/files/Roth2.pdf" />
		<title level="m">The Shapley value: essays in honor of Lloyd S. Shapley</title>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead</title>
		<author>
			<persName><forename type="first">Cynthia</forename><forename type="middle">C</forename><surname>Rudin</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1811.10154" />
	</analytic>
	<monogr>
		<title level="j">Nature Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="206" to="215" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Explaining deep neural networks and beyond: A review of methods and applications</title>
		<author>
			<persName><forename type="first">Wojciech</forename><surname>Samek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Grégoire</forename><surname>Montavon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Lapuschkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">J</forename><surname>Anders</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Klaus-Robert</forename><surname>Müller</surname></persName>
		</author>
		<idno type="DOI">10.1109/JPROC.2021.3060483</idno>
		<ptr target="https://doi.org/10.1109/JPROC.2021.3060483" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE</title>
		<meeting>the IEEE</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">109</biblScope>
			<biblScope unit="page" from="247" to="278" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Toward causal representation learning</title>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francesco</forename><surname>Locatello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><forename type="middle">Rosemary</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anirudh</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2102.11107" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE</title>
		<meeting>the IEEE</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">A value for n-person games</title>
		<author>
			<persName><forename type="first">Lloyd</forename><forename type="middle">S</forename><surname>Shapley</surname></persName>
		</author>
		<ptr target="http://www.library.fa.ru/files/Roth2.pdf#page=39" />
	</analytic>
	<monogr>
		<title level="j">Contributions to the Theory of Games</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">28</biblScope>
			<biblScope unit="page" from="307" to="317" />
			<date type="published" when="1953">1953</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">On tractable representations of binary neural networks</title>
		<author>
			<persName><forename type="first">Weijia</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><surname>Shih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adnan</forename><surname>Darwiche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Choi</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2004.02082" />
	</analytic>
	<monogr>
		<title level="m">KR</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="882" to="892" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Formal verification of Bayesian network classifiers</title>
		<author>
			<persName><forename type="first">Andy</forename><surname>Shih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adnan</forename><surname>Darwiche</surname></persName>
		</author>
		<ptr target="http://proceedings.mlr.press/v72/shih18a.html" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Probabilistic Graphical Models</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="427" to="438" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">A symbolic approach to explaining Bayesian network classifiers</title>
		<author>
			<persName><forename type="first">Andy</forename><surname>Shih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adnan</forename><surname>Darwiche</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1805.03364" />
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="5103" to="5111" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Verifying binarized neural networks by Angluin-style learning</title>
		<author>
			<persName><forename type="first">Andy</forename><surname>Shih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adnan</forename><surname>Darwiche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Choi</surname></persName>
		</author>
		<ptr target="http://web.cs.ucla.edu/~andyshih/assets/pdf/SDCsat19.pdf" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Theory and Applications of Satisfiability Testing</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="354" to="370" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Smoothing structured decomposable circuits</title>
		<author>
			<persName><forename type="first">Andy</forename><surname>Shih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guy</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Broeck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Beame</surname></persName>
		</author>
		<author>
			<persName><surname>Amarilli</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1906.00311" />
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="11412" to="11422" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Algorithms for random generation and counting -a Markov chain approach</title>
		<author>
			<persName><forename type="first">Alistair</forename><surname>Sinclair</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Progress in theoretical computer science</title>
		<imprint>
			<date type="published" when="1993">1993</date>
			<publisher>Birkhäuser</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">An efficient explanation of individual classifications using game theory</title>
		<author>
			<persName><forename type="first">Erik</forename><surname>Strumbelj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Igor</forename><surname>Kononenko</surname></persName>
		</author>
		<ptr target="https://www.jmlr.org/papers/volume11/strumbelj10a/strumbelj10a.pdf" />
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="1" to="18" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Model-based problem solving</title>
		<author>
			<persName><forename type="first">Peter</forename><surname>Struss</surname></persName>
		</author>
		<ptr target="https://cse.sc.edu/~mgv/csce781sp13/presentations/struss-handbook-chapter.pdf" />
	</analytic>
	<monogr>
		<title level="j">Foundations of Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="395" to="465" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<author>
			<persName><forename type="first">Naoya</forename><surname>Takeishi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshinobu</forename><surname>Kawahara</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.04464</idno>
		<ptr target="https://arxiv.org/abs/2004.04464" />
		<title level="m">On anomaly interpretation via Shapley values</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">The complexity of computing the permanent</title>
		<author>
			<persName><forename type="first">G</forename><surname>Leslie</surname></persName>
		</author>
		<author>
			<persName><surname>Valiant</surname></persName>
		</author>
		<ptr target="https://core.ac.uk/download/pdf/82500417.pdf" />
	</analytic>
	<monogr>
		<title level="j">Theoretical computer science</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="189" to="201" />
			<date type="published" when="1979">1979</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Tractable probabilistic models: Representations, algorithms, learning, and applications</title>
		<author>
			<persName><forename type="first">Guy</forename><surname>Van Den Broeck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicola</forename><forename type="middle">Di</forename><surname>Mauro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antonio</forename><surname>Vergari</surname></persName>
		</author>
		<ptr target="http://web.cs.ucla.edu/~guyvdb/slides/TPMTutorialUAI19.pdf" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note>Tutorial at UAI 2019</note>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">On the tractability of SHAP explanations</title>
		<author>
			<persName><forename type="first">Guy</forename><surname>Van Den Broeck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anton</forename><surname>Lykov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maximilian</forename><surname>Schleich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Suciu</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2009.08634" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI</title>
		<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">On the tractability of SHAP explanations</title>
		<author>
			<persName><forename type="first">Guy</forename><surname>Van Den Broeck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anton</forename><surname>Lykov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maximilian</forename><surname>Schleich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Suciu</surname></persName>
		</author>
		<ptr target="https://www.jair.org/index.php/jair/article/view/13283" />
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">74</biblScope>
			<biblScope unit="page" from="851" to="886" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
