<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Question Answering System For Interacting with SDMX Databases</title>
				<funder ref="#_cpB6WWz">
					<orgName type="full">Agence Nationale de la Recherche</orgName>
					<orgName type="abbreviated">ANR</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Guillaume</forename><surname>Thiry</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Institut Polytechnique de Paris</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Inria</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ioana</forename><surname>Manolescu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Institut Polytechnique de Paris</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Inria</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Leo</forename><surname>Liberti</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Institut Polytechnique de Paris</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">CNRS</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">A Question Answering System For Interacting with SDMX Databases</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">A82177CA5329B37A1BF1BD39A6C92E60</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-04-12T14:51+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>statistical databases</term>
					<term>Question Answering</term>
					<term>SDMX Dataflow Data Structure Definition Constraints Categorisation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Among existing sources of Open Data, statistical databases published by national and international organizations such as the International Monetary Fund, the United Nations, OECD etc. stand out for their high quality and valuable insights. However, technical means to interact easily with such sources are currently lacking. This article presents an effort to build an interactive Question Answering system for accessing statistical databases structured according to the SDMX (Statistical Data and Metadata Exchange) standard promoted by the abovementioned institutions. We describe the system architectures, its main technical choices, and present a preliminary evaluation. The system is available online.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Open Data has the potential of enabling societal changes of significant impact. On the one hand, the availability of data allows the creation of innovative products and applications. On the other hand, data that documents various aspects of a societal functioning is a crucial ingredient for its self-understanding, and it enables citizens to make informed choices, from personal consumer decisions to voting in elections. Such statistical databases are also valuable as references to be used in fact-checking <ref type="bibr" target="#b6">[7]</ref>. Indeed, claims about statistical quantities such as unemployment or immigration figures are a hot topic of political debate in many countries.</p><p>Statistical databases published by national and international institutions such as the United Nations (UN), the Organisation for Economic Co-operation and Development (OECD), the International Monetary Fund (IMF), EuroStat, etc. stand out among Open Data sources for the quality and comprehensiveness of their data, gathered at a significant expense of qualified personnel and carefully curated, structured and organized. To enable interoperability among their data sources, these and other organizations have established a standard for describing their statistical data, named SDMX (Statistical Data and Metadata eXchange) <ref type="bibr" target="#b0">[1]</ref>. The standard specification describes the various elements involved in specifying the (mostly numerical) data, and in describing it according to the relevant dimensions. Several institutions have adopted the standard, and published Web sites online with data sheets together with textual descriptions and sometimes automated visualization means for human users to interact with the data. As an example on which we build our presentation, we used the data and metadata provided by OECD, our partner, through its Web Service: http://de-staging-oecd.redpelicans.com/.</p><p>As this example shows, currently, interaction with an SDMX dataset takes the following form. (i) Users locate one or more pages on the Internet hosting the dataset(s) they are interested in; (ii) Users read the descriptions to check the expected content and semantics; (iii) they visualize the numerical data, possibly with the help of the visualization tools included in the pages.</p><p>Discussions with OECD experts working on Smart Data Practices and Solutions highlighted that this current usage scenario suffers from several limitations. First, searching for the right dataset is not easy, when multiple datasets may contain interesting answers, or when users' terms specifying their question(s) do not match those used by statisticians describing their data. Second, having to face tables of numbers may be cumbersome for users interested in point queries, e.g., "what is the unemployment rate in Spain in Q3 of 2018?" Third, this scenario gives no support for more advanced queries of the form "where is unemployment higher than in Spain, in Q3 of 2018?" Motivated by this observation, we have developed a tool which allows users to interact in an easier, more natural manner with SDMX statistical databases, by means of a QA system. In the remainder of the paper, we describe the SDMX standard (Section 2), before delving into our contribution: an architecture and algorithms for an interactive, English-speaking agent allowing natural language interaction with SDMX databases (Section 3 to Section 6). We present an evaluation in Section 7, before discussing related work and concluding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">The SDMX standard</head><p>The SDMX standard enables the description of: statistical data (typically under the form of a multidimensional array whose cells contain numbers, e.g., sales volume or percentage of people unemployed), and metadata, that is, the set of dimensions characterizing the data, as well as the values that these dimensions actually take in the data. Sample dimensions are for instance geographic (what area does a certain statistic correspond to), temporal (time period) or more specific to the data being described, e.g., tourist stays in a certain area can be split in hotel stays, bed-and-breakfast stays, and camping stays.</p><p>Concretely, SDMX advocates representing both data and metadata in XML<ref type="foot" target="#foot_0">1</ref> , a self-describing syntax which has several advantages. First, it lends itself easily to publishing data sets online, since XML can be easily turned into a Web page with the help of a style sheet. Second, XML is the syntax favored by Web service technologies such as SOAP <ref type="bibr">[18]</ref> (for message exchange) and WSDL [19] (for Web service communications). This simplifies the implementation of an SDMX endpoint which allows querying an SDMX database (or repository) remotely, through a Web service client application.</p><p>The SDMX metadata is split into several parts, each of them describing one specific aspect of the data. These different pieces point to each other, as well as interact with the data, which could not be readable without its metadata. An overview of the design of the SDMX standard is given in Figure <ref type="figure" target="#fig_0">1</ref>.</p><p>Data are organized in data tables which consist of cells. Each cell corresponds to a combination of values along each dimension present in the data table; each cell contains a numerical value (in green in the figure). A data table points to its metadata description, which is necessary in order to interpret its content. Specifically, the data table points to a Data Structure Definition (DSD), which is the central piece of the metadata architecture (in purple in the figure). The DSD lists the different dimensions used for this dataset, and attaches to each dimension a concept and a codelist (in blue in the figure). The concept gives more details about the dimension (an explicit name and a description) while the codelist is a dictionary that can be used to decode information about this dimension. For instance, the codelist of the dimension AREA may contain the pair "AU" = "AUSTRALIA" to state that "AU" may be used as a dimension value to denote "Australia" compactly. The DSD also lists other characteristics of the table, for instance whether its values are predicted, estimated, or actually computed.</p><p>Other important metadata components are the Constraints (in red) and the Categorisation (yellow). The Constraints specify the legal values for each dimension of the table, to guarantee the integrity of the dataset. The Categorisation characterizes the way in which the data is classified within the Web Service that makes it available to users. For example, the table "Domestic tourism" is found in the category "Tourism", itself a subcategory of "Industry". These two parts are ultimately linked to the DSD via a part called the Dataflow (orange).</p><p>The SDMX metadata comprises more fields, for example some which are designed to ensure the proper construction of the data sets themselves etc. The  modules mentioned above are those that we exploit for building our system, assuming that the data is correctly organized and described.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">System overview</head><p>The main problem addressed by our QA agent is to understand information requests, when expressed in natural language. A typical question we aim at answering is: "What was the population of Japan in 2010?" A successful interaction with the system, thus, is a sequence in which a query was asked in natural language, understood correctly by the algorithm, and answered through appropriate queries on the SDMX data.</p><p>Our QA system is implemented in Python, based on the nltk toolkit and parsers from the Stanford NLP Group. It consists of 12 files and about 1800 lines of code; it is available online at https://github.com/guillaume-thiry/OECD-Chatbot. The general analysis pipeline, outlined in Figure <ref type="figure" target="#fig_1">2</ref>, is as follows.</p><p>First, the user query, a natural language sentence, is analyzed by three different parsers, each gathering specific information from the query. A lexical parser assigns to each word a Part Of Speech (POS) Tag and captures how they form grammatical groups (such as a Noun Phrase [NP] or a Prepositional Phrase [PP]). The dependency parser identifies connections between non-consecutive words. These two first parsers are supplied by the Stanford CoreNLP toolkit <ref type="bibr" target="#b11">[12]</ref> (interfacing with nltk). Lastly, a Named Entity Recognition (NER) parser recognizes specific entities such as dates or places; we rely on the Stanford NER module <ref type="bibr" target="#b7">[8]</ref>. The results of these parsers are then used as input for different analysis functions (see below).</p><p>From the nature of SDMX data it follows that we expect user's input to be questions concerning a statistical measure over one or more countries, at a given time. We have therefore implemented functions to extract dimensions such as the location(s) and date(s) from the phrase, when present; a separate module is tasked with understanding the type of the question asked. These modules are described more specifically in Section 4.</p><p>Concerning the query type (or structure), beyond direct questions asking for the value of a certain metric in a certain time and place, we also considered questions featuring comparisons, e.g., "Countries with more than 100 M inhabitants", and aggregation, e.g., "What are the five largest countries in term of GDP per capita ?". The words used in such questions are relatively few and easy to detect ("more", "less", "higher", "than", "above" ...). However, because the syntactic structure of such questions is inherently more complex than that of simple questions, detecting and properly handling such structure is slightly more complex than understanding simple dimensions (date, place) in direct questions. We detail this in Section 5.</p><p>Last but not least, each question refers to a specific topic, e.g., unemployment rate, public debt or health investments, and specific dimensions, e.g., a question about a metric defined on the general population can be accompanied by a qualifier that restricts the questions to males or females. That depends on the user's information needs. This part of the question would naturally change depending on the content of the database and the user interest; a generic approach needs to be taken for understanding it, based on the known metadata of the SDMX endpoint behind the system. This is discussed in Section 6.</p><p>Once all these questions have been answered by the different parts of our algorithm (with potentially some requests of precision addressed to the user), a final section of the system uses the SDMX REST API to get the correct table with the correct dimensions, applies potential comparisons and aggregations, and displays the answer in a appropriate way (a list, a graph, a map ...).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Understanding general dimensions of user questions</head><p>We discuss here the first steps for analysing the question: what is the quantity of interest to the user, and (if specified), in which time frame, and in which spatial location? Each of these tasks is handled by a specific module as explained below.</p><p>Assumptions on the grammatical structure. We decided to focus on two very common question forms. First, the so-called WH-questions, such as "What is the population of Russia in 2012 ?", include an explicit question indicator such as "Who", "What" etc. Second, we also consider Nominal Phrase questions, e.g., "French GDP per capita". Other questions types exist, such as Yes/No questions, or imperative statements ("Give me the surface of Peru"); in the current project state, these are not tackled, although we believe the extension would be quite easy.</p><p>We identify the question form based on the grammatical structure given by the parsers: a WH question usually corresponds to a WH Noun Phrase (WHNP, in short), while a nominal question is specified through a Nominal Phrase (NP).</p><p>The question form can be used to determine the type of result the user wants. We have identified three main types of result: (i) The user could ask for the value of a specific metric or statistic, e.g., "What is the employment rate in Canada ?". In such questions, the geographical area (e.g., Canada) and the time associated to the query metric may or may not be present. (ii) Users can ask for one or more geographical areas, e.g., "Largest nations of the world". (iii) Users can ask for one or more time periods, e.g., "How many years with more unemployment than 2014".</p><p>We detect those, depending on the structure, by looking for specific words. For instance, a WH question is signaled by the presence of a specific WH word; a geographical question is signaled by a "where", and a set of time periods by a "when". Finally, if the WH-word is "what", we need to identify the word connected to it: "what" in conjunction with "countries" has a different meaning than when used with "population". This connection is captured by the dependency parser. While these two question shapes do not exhaust the set of possibilities, they are those that occur most frequently in real-world journalistic fact-checks, as noted for instance in <ref type="bibr" target="#b4">[5]</ref>.</p><p>Time search. Time is a central dimension characterizing statistical data. We implemented a function that detects time questions at the granularity of the year, since this appeared sufficient for the SDMX data we used. This could be easily extended to quarter or month granularity. Our function is also capable of understanding time intervals such as "from 1930 to 1950" or "since 1950". We can also detect time expressions such as "nowadays" or "over the last 10 years".</p><p>We rely on the NER component to signal DATE occurrences in the text. For instance, 1950 is tagged by the NER in all the sample occurrences mentioned above. However, NER does not capture the other qualifiers such as "from... to" or "since", which are important to correctly understand the question. Therefore, after identifying a year such as "1950" in a sentence, we examine the Prepositional Phrase [PP] (in the grammatical structure) containing it. In this PP, we consider specific words such as "since", "till" or "between" to know the meaning of that year. It is crucial to focus only on the PP of "1950" because words such as "since" can appear elsewhere in the sentence, even near "1950", without having any relation with it.</p><p>Based on this analysis, we fill two parameters, FROM DATE and TO DATE, specifying the time period of interest for the user. We also keep a variable THAN DATE in memory, for the specific case of a year linked with the word "than". This can be useful when working with comparisons, as we explain in Section 5.</p><p>Locations search. Another essential information for our statistical data is the location. The granularity adopted by most of the international organizations using SDMX is that of the countries, and that is our choice as well. This could be easily refined to regions or cities.</p><p>Experimenting with the NER component has shown that it is not sufficiently powerful in identifying occurrences of countries. Specifically, problems were raised by abbreviations such as "USA" or "UK" and with adjectives denoting countries, such as "French", "Finnish" etc. Instead, we created a set of dictionaries containing every country, with the corresponding adjectives and their multiple spellings. To achieve this, we took the list of member states from the UN website, and obtained the related linguistic information (abbreviations, alternative spellings, adjectival forms) from the Wikipedia pages of each country in the list. Another dictionary contains the names of several regions of the world with the corresponding countries, under the UN M49 standard<ref type="foot" target="#foot_1">2</ref> . This enables us to correctly understand expressions like "Western Europe" or "Latin America" and know exactly what countries are concerned.</p><p>Once countries or regions have been detected in the sentence, we analyze the context in the same way as for the dates. This enables us to correctly handle questions over statistical metrics between two countries, e.g., tourism or trade "from country A to country B". These are detected by examining the contextual words in the PP of each country.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Understanding advanced search criteria</head><p>Comparisons and aggregations are the next more complex class of queries we consider; they occur quite naturally, even if not as often as the simpler queries discussed above. Typical examples are the questions "Countries with more than ...", "Five countries with most ..." Such comparisons and aggregations are also basic features of database languages, such as SQL. We therefore decided to enhance the QA agent with the ability to handle them too.</p><p>Comparisons. We detect comparison questions that satisfy one of the structural patterns described below.</p><p>First, we look for a comparative adjective that can be specified by one word (e.g., "larger") or two words (e.g., "more populous"), followed by the word "than". This form is easy to detect, as the words expressing the exact comparison ("more", "less", "richer", "higher" etc.) are identified by a specific POS tag, and the word "than" is only used in English to signal such comparisons. We only need to verify that all the words that are necessary to form a correct comparative question are present and placed correctly in the sentence. If the word "than" is missing or appears in a position that does not correspond to the pattern, we do not recognize the question (thus we can ask for a reformulation of the query).</p><p>Second, in the specific case of a comparison with a numerical value (that we name threshold below), one can use words like "over" or "below", e.g., "What countries have a population below 50 M". A difficulty raised by these words is that they can appear in other contexts in a sentence. For instance, in the timerelated expression "over the last 5 years", the word "over" does not contribute to expressing a comparison. To avoid such confusions, every time one of these words appears, we check if it is linked to a numerical value and if this value is not already interpreted as a date. If so, we consider that the sentence expresses a comparison, and the numerical value is the threshold. Multiple comparisons in a phrase Next, we extended the algorithm to also handle multiple comparisons. Our system tries to split the sentence in as many clauses as there are comparisons, each one of them containing one comparison only; the splitting is based on separative words such as "and", "but" or "while". Therefore, a sentence like "Countries with a population above 10 M but below 50 M in 2012" is split into two clauses: "Countries with a population above 10 M" and "below 50 M in 2012". This separation is only used to interpret each comparison properly: the other functions, such as for the detection of the time ("2012" here), are applied to the whole sentence.</p><p>Each clause thus extracted falls in one of the following three cases:</p><p>-Comparison with a threshold (discussed above). If the comparative structure uses "above" or "under" for example, we check the threshold as explained before. If it uses "more ... than", then we search for the threshold after "than". Observe that the function detecting the thresholds also looks for multiples like "M" (millions) or "bn" (billions) attached to the numbers. At the end, we know the exact value involved in the comparison. -Comparison with a country or a year, e.g., "What countries have a higher GDP than Spain ?" Answering this requires the comparison of the GDP value for each country with the Spanish one. The same can happen with a list of years. This is the reason why we had a "THAN" category when looking for countries and years (Section 4). In such cases, we know that the user asks for a list of countries, and we know that a comparison is made, therefore we check in the detected countries of the sentence if one of them was linked to the word "than". If so, we know that the comparison is of this country against all the others. -Otherwise, we consider that two different values of the table are compared, e.g., "Countries with more male population than female population" or "Countries with less population in 2020 than in 1990". In such cases, we inspect the phrase before and after the word "than" to determine what the two values compared are. This is performed on different levels, such as when filling the dimensions of the table (see Section 6), as a dimension can have one value before "than" and another "after".</p><p>Aggregations Such patterns are easier to handle than the comparisons, because there are fewer possible variations in the respective linguistic structures. We are able to detect three ways to aggregate a list of countries or of years: through counting, maximum and minimum. Counting queries, such as "How many countries have ...", can already be detected when analyzing the type of sentence (Section 4), as we have to treat structures like "How many" or "Number of" and inspect the sequel of the phrase to determine the type of information wanted: "How many countries ..." aims at measuring a list of countries, while "How many inhabitants ..." aims at a value. We detect queries interested in a maximum (or minimum) through the presence of certain words like superlatives ("highest", "biggest", "most" ...) or a few other specific words ("maximum", "top", ...). Subsequently, the algorithm checks if a number is linked to this word, as it would indicate the number of items the user wants to see ("Best 5 countries ...", "Top 10 highest ..."). If no such number is found, a default value is used: 1 if the word is singular ("Country with highest ...") and 10 otherwise. This analysis outputs: if an aggregation was found; and, if yes, whether it is a maximum or minimum, and the number of items to return.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Understanding the question topic and dimensions</head><p>While we described above a set of methods for identifying some query elements specific to the kind of question, we now explain how we detect question topics. Specifically, we aim at answering the question "Which dataset should we use to answer this query?".</p><p>Find the topic. Before looking for more details in the query, we have to determine the general topic of the query. Indeed, the details highly depend on the topic: if the query is about "population", we may have to identify "male" or "female" as categories, but these do not apply if the query is about the "GDP". For each table, we build a set of keywords using the SDMX metadata. We do the same for the query, removing query words that are too generic (e.g., "country") or stop words (such as "in", "to", "with" etc.). Next, we compute a proximity score between the words of the query and the set of words corresponding to each table, in order to determine the tables most related to the query.</p><p>Our proximity function compares the stems of the input words, and finds their proximity according to word2vec <ref type="bibr" target="#b12">[13]</ref> and WordNet <ref type="bibr" target="#b13">[14]</ref>. We consider a set of different cases, shown in Table <ref type="table" target="#tab_2">1</ref>; to each there corresponds a certain similarity score. When two cases hold simultaneously, the higher similarity is kept.</p><p>To calculate the proximity between the query and a given table, we sum the proximity of each meaningful query word with the keywords of the table. Keywords characterizing a table can be found in four different places in the table metadata. Depending on the position, we give them different weights in the proximity computation formula, as shown in Table <ref type="table" target="#tab_3">2</ref>. [These weights were found by hand through trial and error; a Machine Learning approach could be used instead to learn their values] Finally, the proximity between a query and a table is computed as:</p><formula xml:id="formula_0">prox(query, table) = q∈QueryWords max t∈TableWords prox(q, t) weight(t)<label>(1)</label></formula><p>Once we are able to compute the proximity score between our query and a given table, we can rank the tables according to their scores to find the most relevant ones. As shown in Section 7, this technique is not perfect, as some tables are very similar. The table with the highest proximity score may not always be the most relevant one to answer the question. The solution we have found is to display a list of the tables with the highest scores to the user for him to choose. Indeed, the correct table usually belongs to this list in practice.</p><p>Filling the dimensions. Our last task is to detect possible dimension values in the user query, that is, e.g., "women" in the question "Number of French women". Given a table that we identified as described above and which could contain relevant query results (for instance, a table about "French population"), we compute, for each dimension of the table and each value of a dimension, a proximity score between the query's useful words and the name of the value. For each dimension, the value name most similar to the query keywords is finally retained, if the similarity is above a certain threshold. In our example, the query's useful words are "French" and "women" and the table has a dimension "sex" with two values "male" and "female". Because the similarity between "women" and "male" is lower than between "women" and "female", the dimension "sex" gets the value "female".</p><p>If, for a given dimension, no value is close enough to the query, three cases are possible. (i) If the dimension has only one value (e.g., "FREQUENCY" has "ANNUAL" as its only value in many SDMX datasets), this is systematically the default value. (ii) The default value can be the "total" value, as this is quite frequent in the data ("SEX" has "MALE", "FEMALE" and "TOTAL" as values). (iii) Otherwise, no default value is identified, and the user is presented the possible values for the dimension in order to clarify the request. In our example, if the table also has a dimension "age" (with values "0-20 years old", "20-40 years old" ...), no value is close enough, and "total" is used instead.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Experimental evaluation</head><p>Below, we describe qualitative evaluation results on each of the modules we developed and described above. Some of these (in particular those described in Section 6) can only be evaluated on an actual dataset; it should be noted that the only SDMX endpoint we had access to during this work (mentioned in Section 3) had relatively little data. Some others (e.g., detecting the question type, or extracting the time interval etc.) can be evaluated independently of an actual SDMX endpoint. All the queries used in the evaluation are available in the Github deposit of our code.</p><p>To perform our tests, we wrote for every module a set of test cases (queries) that we strived to make as realistic as possible, and also to highlight the capabilities and limitations of our approach. For each query, we manually specified the expected results, then we ran our code and measured how many correct results we obtained. Some query topics come from the available OECD data; the other topics are mainly taken from the "most popular tables" section of Eurostat <ref type="foot" target="#foot_2">3</ref> . Overall, the queries cover 20 to 30 topics. To test topic and dimension extraction, we had to write queries related to the tables for which we had actual metadata. Using the available OECD Web service, we selected 12 tables over 3 different categories (outlined in Table <ref type="table" target="#tab_4">3</ref>) as targets of our queries.</p><p>Detecting question type To test this, we created a set of 100 queries, half of which are WH-questions while the other half are Nominal Phrases (NP). These queries were asking for a value, a list of countries or a list of years. Half of them also contained comparisons or aggregations. We assessed the precision of four query understanding tasks: finding the sentence type (WH or NP), understanding the kind of result wanted (Value/Countries/Years), and detecting comparisons and aggregations. The result are as follows:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Parameter</head><p>Accuracy Parameter Accuracy Type of sentence 100% Comparison 97% Return 94% Aggregation 94%</p><p>It is interesting to see where the algorithm failed and why: in our case, this is due to our linguistic patterns not being flexible enough. For example, in the sentence "Highest mortality rate before 40 years in the world", the code falsely detects, based on "highest" and "years", that we seek an aggregation over a list of years. Another example is the query "What is the minimum wage in Canada ?" because here "minimum" does not indicate an aggregation but belongs to the expression "minimum wage". While these errors point to possible avenues for improvement, the accuracy results are already quite high.</p><p>Detecting time and location For this test, we wrote 30 queries with time and location indicators. These queries specify values to be extracted for FROM, TO and THAN, sometimes several in a single query. We assess for each query if the time, respectively, location was correctly found. If one of the three information is incorrect (e.g., the FROM and TO are good, but the THAN was not detected), then the query is considered wrongly predicted. We obtained:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Parameter</head><p>Accuracy Parameter Accuracy Time 97% Location 100%</p><p>Understanding comparisons and aggregations. For this evaluation, we created 35 queries with comparisons and 30 with aggregations. Most of the comparisons had just one clause, but 6 of them were written with two to test the split. The thing being compared could be a numerical value (threshold), a country, a year, or something else. In the latter case, we only check if a comparison is actually detected (not its details) as the correct understanding of details (e.g., dimensions) is evaluated separately. In the former cases, we are also able to check whether all the details are correctly understood.</p><p>For aggregation queries, we assess whether the code detects (i) the aggregate (count, minimum or maximum) and (ii) the number of items to display, e.g., from "5 best countries with ..." we should detect (max, 5) while from "12 years with least ..." the expected result is be (min, 12). We obtained:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Parameter</head><p>Accuracy Comparison (total) 91% Aggregation (aggregate) 97% Aggregation (value) 90%</p><p>The accuracy is slightly lower than the one obtained in the assessment of the type of the sentence because then, we only checked if a comparison or aggregation was actually detected; now, we look more into the details and count a test correct only if all the information was correctly extracted.</p><p>Our algorithm failed for comparisons such as "How many countries with falling GDP in 2020" as it is an implicit comparison of the GDP growth value with the value 0. Among aggregation queries, the algorithm is confused by queries like "Highest minimum wages in the world". These failures are due to the lack of general knowledge which could have enabled, e.g., to answer with the highest minimum wage in the database (even if it did not reflect the whole world); even more domain knowledge is necessary to verify whether it does.</p><p>Understanding query topics For this test, we used the 12 SDMX tables (from 3 different categories) described earlier in this Section. We wrote a total of 48 queries: 36 are about one of the 12 known topics (3 per topic), while the 12 remaining ones are on unknown topics (for which we do not have any data or metadata). The latter were introduced to check the behavior of the proximity score. For each query, we measured the accuracy for: Top 1 table (that or those with the best score), Top 3 tables (those with the 3 best scores) and Top category (the category with the best score).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Parameter</head><p>Accuracy Top 1 58% Top 3 78% Top Category 94%</p><p>The highest-score table is the best one only 58% of the time, but the topthree tables are correctly found 80% of the time. The category identification accuracy is quite high (94%). A likely explanation for the difference between these accuracy values is that some table names are very similar, e.g., "Inbound Tourism" and "Domestic Tourism", while category names are quite different from each other, thus easier to separate with a proximity function.</p><p>It is also interesting to examine the proximity score distribution for queries with known and unknown topics. Figure <ref type="figure" target="#fig_2">3</ref> shows that the scores for Top 1 and Top 3 (average of 3 scores) are lower when the topic is not present in the database. This is of course expected. However, in some cases, the proximity score for a known topic may be low, while on the opposite, an unknown topic may still reach a relatively high score. For instance, the best score for a query about "Government investment" (in a table we have) is 0.42, while the best score for "GDP per capita" (on which we do not have data) is 0.53. Thus, our current scores, alone, do not suffice to confidently determine whether our database does contain the information requested in a query. Thus, while our algorithm works generally well, the best approach appears to be to show the user the highest-rank datasets and let her chose the one(s) that look more promising, similarly to the way in which Web search engines work.</p><p>Dimensions To test our identification of data dimensions in queries, we wrote 40 queries on 10 tables (4 queries per table ). Each query specifies one value for one dimension of the table. For example, the query "Female population in Russia" would concern the dimension "SEX" with the value "FEMALE". Furthermore, 6 queries contained a comparison of dimension values, introduced through "than" (e.g., "Countries with more male population than female"). The value of a dimension can be specified ("MALE" or "FEMALE") or can default to "TOTAL". For each query, we assess if this specific dimension was given the correct value by the algorithm: we obtained 78% accuracy with no notable differences across query groups. Compared with other accuracy results, these are a bit lower, but they still show relatively good performance.</p><p>Another question we study is the utility of a threshold similarity score (recall Section 6). Figure <ref type="figure" target="#fig_3">4</ref>  query, against the value of this threshold. The most accurate results are obtained with the lowest threshold, i.e., in general, the best value for the dimension is the one with the highest score, even if this score is quite low. However, it is worth noting that our test queries only used one dimension, e.g., they do not simultaneously constrain the gender and the age of people (mostly because the data we had access to did not provide interesting, meaningful combinations of dimensions). Thus, our observations on the threshold are based on such singledimension queries. In general, care should be taken not to set the threshold too low, so as not to miss dimension(s) matched with lower similarity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Related work</head><p>Chatbots have become ubiquitous tools for human-computer interaction <ref type="bibr" target="#b10">[11]</ref>. While some are designed to handle conversations with the user, others are built for specific tasks, e.g., book plane tickets). Our work falls into the second category. The general framework for this kind of systems is to use the user's inputs to fill a set of frames (origin city, destination city, departure time...) and once everything is complete, execute the task (book the tickets). Some tools like Di-alogFlow (Google) or Wit.ai (Facebook) offers turnkey solutions for developers to specify the desired frames and let AI models fill them automatically.</p><p>To facilitate the task of querying relational databases, several works proposed natural-language query interfaces which are given user queries in natural language and translate them into SQL queries, e.g., <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16]</ref>. Prior efforts in facilitating access to high-quality statistical data have focused first, on extracting the data out of HTML pages and HTML or Excel tables <ref type="bibr" target="#b3">[4]</ref> and then on developing a specific keyword search algorithm for finding the datasets (and, if possible, dataset cells) most pertinent for answering a query <ref type="bibr" target="#b4">[5]</ref>. A claim extraction tool <ref type="bibr" target="#b5">[6]</ref> focused on extracting, from a natural language phrase making some statement about a statistical quantity, the quantity name and possible dimensions.</p><p>The need for question answering systems over Linked Data has grown in sync with the popularity of the RDF<ref type="foot" target="#foot_3">4</ref> format and the development of its associated query language, i.e. SPARQL<ref type="foot" target="#foot_4">5</ref> . Several projects have aimed at implementing QA systems over large linked datasets, for instance DBpedia <ref type="bibr" target="#b1">[2]</ref>. The RDF Data Cube Vocabulary<ref type="foot" target="#foot_5">6</ref> gives the RDF format additional means to deal with multidimensional linked data, such as statistics. A lot of research works have been done on the Question Answering on Data Cubes <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b8">9]</ref>. Challenges also take place regularly to assess state of the art techniques in this field <ref type="bibr" target="#b16">[17]</ref>.</p><p>We tried to build our system using a different approach compared to these existing works. First, because this high-quality data is specifically available in SDMX, we wanted to stick to this format. Even though a lot of similarities exist between SDMX and Data Cubes, we wanted to take advantage of the SDMX specificities (like the extended metadata) rather than just doing a format translation and using another system. Moreover, these works usually include machine learning methods based on neural networks. In this study, given the relatively simple shape of the questions we expect over the statistical databases, we have not relied upon such methods. Instead, we used a rule-based approach based on the question syntax to classify and understand the question, and relied on linguistic distance to determine the dataset that is closest to the query.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">Conclusion and future work</head><p>Among existing agents and natural language interfaces to databases, the novelty of our work is to be the first to consider the development of a system over SDMX data. Sponsored by major international institutions such as the European Central Bank, Eurostat, the International Monetary Fund, the United Nations and the World Bank, SDMX allows sharing datasets across institutions and facilitates their processing by stakeholders (government at all levels, the press, interested citizens). Large volumes of SDMX data are published and exchanged by these and other institutions. In this study, we only had access to an endpoint (Web service capable of answering structured queries sent by the algorithm) publishing a limited amount of data. Future work could test and consolidate our methods on more data, improve the accuracy of our topic understanding, and implement the conversational capabilities of a chatbot.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. SDMX data organisation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Main structure.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Score distribution for known and unknown topics</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>78%Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Accuracy of dimension understanding from user queries vs. similarity threshold.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>extraction Distance computing Request filler</head><label></label><figDesc></figDesc><table><row><cell>-Table to select</cell></row><row><cell>-Dimension</cell></row><row><cell>values</cell></row><row><cell>-Comparisons</cell></row><row><cell>-Aggregations</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Data Extractor Complete Request Data Extraction REST API Display the results</head><label></label><figDesc></figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 .</head><label>1</label><figDesc>Word similarity computation.</figDesc><table><row><cell>Case</cell><cell>Score Case</cell><cell>Score</cell></row><row><cell>Equal stems</cell><cell>1 WordNet hyponym or hypernym</cell><cell>0.6</cell></row><row><cell>WordNet synonyms</cell><cell>1 Word2vec similary s with s &gt; 0.2</cell><cell>s</cell></row><row><cell>WordNet noun-adjective relation</cell><cell>0.7 None</cell><cell>0</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 .</head><label>2</label><figDesc>Query-table similarity computation.</figDesc><table><row><cell>Name</cell><cell>Details</cell><cell>Weight</cell><cell>Example</cell></row><row><cell>Title</cell><cell>Table title keywords</cell><cell>1</cell><cell>population</cell></row><row><cell>Description</cell><cell>Description keywords if any</cell><cell>0.6</cell><cell>number, inhabitants</cell></row><row><cell cols="2">Dimensions Dimension name/value keywords</cell><cell>0.6</cell><cell>sex, female, male</cell></row><row><cell cols="2">Categories Table category name keywords</cell><cell>0.6</cell><cell>society, demography</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 .</head><label>3</label><figDesc>OECD datasets used in our tests.</figDesc><table><row><cell cols="2">Categories Tables</cell></row><row><cell></cell><cell>Domestic Tourism</cell></row><row><cell>Industry</cell><cell>Inbound Tourism</cell></row><row><cell>[IND]</cell><cell>Employment in tourism</cell></row><row><cell></cell><cell>Key tourism indicators</cell></row><row><cell></cell><cell>Open govt data</cell></row><row><cell cols="2">Government Public finance and economics</cell></row><row><cell>[GOV]</cell><cell>Structure of govt expenditures by function and govt type</cell></row><row><cell></cell><cell>Structure of govt investment by function</cell></row><row><cell></cell><cell>Enrolment by field</cell></row><row><cell cols="2">Education Educational attainment</cell></row><row><cell>[EDU]</cell><cell>Enrol by gender, program orientation and mode of study</cell></row><row><cell></cell><cell>Enrolment by age</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>XML being the main standard, it is however useful to note that SDMX is also compatible with JSON and CSV</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>https://unstats.un.org/unsd/methodology/m49/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>https://ec.europa.eu/eurostat</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"><p>https://www.w3.org/RDF/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4"><p>https://www.w3.org/TR/sparql11-query/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5"><p>https://www.w3.org/TR/vocab-data-cube/</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgments We thank <rs type="person">Eric Anvar</rs>, Head of Smart Data Practices and Solutions at OECD, <rs type="person">Jonathan Challener</rs>, Partnerships and Community Manager at <rs type="institution">OECD</rs>, for their suggestions and support. This work was partially funded by the <rs type="funder">ANR Project ContentCheck</rs> (<rs type="grantNumber">ANR-15-CE23-0025</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_cpB6WWz">
					<idno type="grant-number">ANR-15-CE23-0025</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<ptr target="https://sdmx.org/" />
		<title level="m">SDMX (Statistical Data and Metadata eXchange</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Enhancing community interactions with data-driven chatbots-the DBpedia chatbot</title>
		<author>
			<persName><forename type="first">R</forename><surname>Athreya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Ngonga Ngomo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Usbeck</surname></persName>
		</author>
		<idno type="DOI">10.1145/3184558.3186964</idno>
		<ptr target="https://doi.org/10.1145/3184558.3186964" />
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="143" to="146" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Querying rdf data cubes through natural language</title>
		<author>
			<persName><forename type="first">M</forename><surname>Atzori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">M</forename><surname>Mazzeo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zaniolo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SEBD</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Extracting linked data from statistic spreadsheets</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">D</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Manolescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tannier</surname></persName>
		</author>
		<idno type="DOI">10.1145/3066911.3066914</idno>
		<ptr target="https://hal.inria.fr/hal-01583975" />
	</analytic>
	<monogr>
		<title level="m">Int&apos;l. Workshop on Semantic Big Data</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Searching for Truth in a Database of Statistics</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">D</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Manolescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tannier</surname></persName>
		</author>
		<ptr target="https://hal.inria.fr/hal-01745768" />
	</analytic>
	<monogr>
		<title level="m">WebDB</title>
		<imprint>
			<date type="published" when="2018-06">Jun 2018</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Extracting statistical mentions from textual claims to provide trusted content</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">D</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Manolescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tannier</surname></persName>
		</author>
		<ptr target="https://hal.inria.fr/hal-02121389" />
	</analytic>
	<monogr>
		<title level="m">NLDB</title>
		<imprint>
			<date type="published" when="2019-06">Jun 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A Content Management Perspective on Fact-Checking</title>
		<author>
			<persName><forename type="first">S</forename><surname>Cazalens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lamarre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leblay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Manolescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tannier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Web Conference 2018 -alternate paper tracks &quot;Journalism, Misinformation and Fact Checking</title>
		<imprint>
			<biblScope unit="page" from="565" to="574" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Incorporating non-local information into information extraction systems by Gibbs sampling</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Finkel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Grenager</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Manning</surname></persName>
		</author>
		<ptr target="http://nlp.stanford.edu/~manning/papers/gibbscrf3.pdf" />
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="363" to="370" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Towards question answering on statistical linked data</title>
		<author>
			<persName><forename type="first">K</forename><surname>Höffner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lehmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">tl Conf. on Semantic Systems (SEM)</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="61" to="64" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Cubeqa-question answering on rdf data cubes</title>
		<author>
			<persName><forename type="first">K</forename><surname>Höffner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lehmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Usbeck</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-46523-4_20</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-46523-420" />
	</analytic>
	<monogr>
		<title level="m">ISWC</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Martin</surname></persName>
		</author>
		<ptr target="http://web.stanford.edu/~jurafsky/slp3/" />
		<title level="m">Chapter 26 : Dialog Systems and Chatbot</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note>Speech and Language Processing</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The Stanford CoreNLP natural language processing toolkit</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Finkel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Bethard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mcclosky</surname></persName>
		</author>
		<ptr target="http://www.aclweb.org/anthology/P/P14/P14-5010" />
	</analytic>
	<monogr>
		<title level="m">ACL (System Demonstrations)</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="55" to="60" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<ptr target="https://arxiv.org/pdf/1301.3781.pdf" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of Workshop at ICLR</title>
		<meeting>Workshop at ICLR</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Wordnet: A lexical database for english</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">A</forename><surname>Miller</surname></persName>
		</author>
		<ptr target="https://arxiv.org/pdf/1301.3781.pdf" />
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="39" to="41" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">An ontology-based conversation system for knowledge bases</title>
		<author>
			<persName><forename type="first">A</forename><surname>Quamar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Ozcan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kreulen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Efthymiou</surname></persName>
		</author>
		<idno type="DOI">10.1145/3318464.3386139</idno>
		<ptr target="https://doi.org/10.1145/3318464.3386139" />
	</analytic>
	<monogr>
		<title level="j">ACM SIGMOD</title>
		<imprint>
			<biblScope unit="page" from="361" to="376" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">ATHENA: an ontology-driven system for natural language querying over relational data stores</title>
		<author>
			<persName><forename type="first">D</forename><surname>Saha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Floratou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Sankaranarayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><forename type="middle">F</forename><surname>Minhas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Özcan</surname></persName>
		</author>
		<ptr target="http://www.vldb.org/pvldb/vol9/p1209-saha.pdf" />
	</analytic>
	<monogr>
		<title level="j">Proc. VLDB Endow</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1209" to="1220" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">9th challenge on question answering over linked data (qald-9) (11 2018) 18</title>
		<author>
			<persName><forename type="first">R</forename><surname>Usbeck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Gusmita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Saleem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Ngonga Ngomo</surname></persName>
		</author>
		<ptr target="https://www.w3.org/TR/wsdl/" />
		<imprint/>
	</monogr>
	<note>SOAP Version 1.2</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
