<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Optimal Tensor Transport</title>
				<funder ref="#_WvFaqqR">
					<orgName type="full">region Auvergne-Rhône-Alpes (France)</orgName>
				</funder>
				<funder ref="#_7wGmRqX">
					<orgName type="full">Pack Ambition Recherche</orgName>
				</funder>
				<funder ref="#_ue9Hd9R">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Tanguy</forename><surname>Kerdoncuff</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institut d Optique Graduate School</orgName>
								<orgName type="laboratory">Laboratoire Hubert Curien UMR 5516</orgName>
								<orgName type="institution" key="instit1">Univ Lyon</orgName>
								<orgName type="institution" key="instit2">UJM-Saint-Etienne</orgName>
								<orgName type="institution" key="instit3">CNRS</orgName>
								<address>
									<postCode>F-42023</postCode>
									<settlement>Saint-Etienne</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Michaël</forename><surname>Perrot</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">UMR 9189</orgName>
								<orgName type="institution" key="instit1">Univ. Lille</orgName>
								<orgName type="institution" key="instit2">Inria</orgName>
								<orgName type="institution" key="instit3">CNRS</orgName>
								<orgName type="institution" key="instit4">Centrale Lille</orgName>
								<address>
									<addrLine>-CRIStAL</addrLine>
									<postCode>F-59000</postCode>
									<settlement>Lille</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Rémi</forename><surname>Emonet</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institut d Optique Graduate School</orgName>
								<orgName type="laboratory">Laboratoire Hubert Curien UMR 5516</orgName>
								<orgName type="institution" key="instit1">Univ Lyon</orgName>
								<orgName type="institution" key="instit2">UJM-Saint-Etienne</orgName>
								<orgName type="institution" key="instit3">CNRS</orgName>
								<address>
									<postCode>F-42023</postCode>
									<settlement>Saint-Etienne</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Marc</forename><surname>Sebban</surname></persName>
							<email>marc.sebban@univ-st-etienne.frmichael.perrot@inria.fr</email>
							<affiliation key="aff0">
								<orgName type="department">Institut d Optique Graduate School</orgName>
								<orgName type="laboratory">Laboratoire Hubert Curien UMR 5516</orgName>
								<orgName type="institution" key="instit1">Univ Lyon</orgName>
								<orgName type="institution" key="instit2">UJM-Saint-Etienne</orgName>
								<orgName type="institution" key="instit3">CNRS</orgName>
								<address>
									<postCode>F-42023</postCode>
									<settlement>Saint-Etienne</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Optimal Tensor Transport</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">87D20CD5DE3E12EE8C3C5A4B1DBF5799</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-04-12T14:52+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Optimal Transport (OT) has become a popular tool in machine learning to align finite datasets typically lying in the same vector space. To expand the range of possible applications, Co-Optimal Transport (Co-OT) jointly estimates two distinct transport plans, one for the rows (points) and one for the columns (features), to match two data matrices that might use different features. On the other hand, Gromov Wasserstein (GW) looks for a single transport plan from two pairwise intra-domain distance matrices. Both Co-OT and GW can be seen as specific extensions of OT to more complex data. In this paper, we propose a unified framework, called Optimal Tensor Transport (OTT), which takes the form of a generic formulation that encompasses OT, GW and Co-OT and can handle tensors of any order by learning possibly multiple transport plans. We derive theoretical results for the resulting new distance and present an efficient way for computing it. We further illustrate the interest of such a formulation in Domain Adaptation and Comparison-based Clustering.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Comparing two probability measures in the form of empirical distributions is at the core of many machine learning tasks. Optimal Transport (OT) <ref type="bibr">(Villani 2008;</ref><ref type="bibr" target="#b39">Peyré, Cuturi et al. 2019</ref>) is a popular tool that allows such comparisons for datasets typically lying in a common vector space. Given two point clouds and a metric allowing to evaluate the transportation cost between two samples, the goal of OT is to learn the transport plan that minimizes the alignment cost between the two sets, resulting in the so-called Wasserstein distance. OT has been shown to be of great interest when dealing with machine learning tasks. For example, unsupervised Domain Adaptation (DA) aims at benefiting from labeled data of a source domain to classify examples drawn from a different but related target domain. The DA theory prompts us to reduce the shift between the source and the target distributions, a task that can be addressed by aligning the two datasets using OT <ref type="bibr">(Courty et al. 2016</ref><ref type="bibr" target="#b9">(Courty et al. , 2017;;</ref><ref type="bibr" target="#b46">Shen et al. 2018;</ref><ref type="bibr" target="#b12">Damodaran et al. 2018)</ref>. OT has also been successfully used in generative adversarial networks (GAN) <ref type="bibr">(Goodfellow et al. 2014)</ref> to minimize the divergence between training data and samples drawn from a generative model, leading to the WGAN <ref type="bibr" target="#b1">(Arjovsky, Chintala, and Bottou 2017;</ref><ref type="bibr">Gulrajani et al. 2017)</ref>.</p><p>In order to expand the range of possible applications, different variants have been proposed in the OT literature to tackle more complex settings. While the standard OT scenario assumes that the two datasets lie in the same feature space, Gromov Wasserstein (GW) <ref type="bibr" target="#b31">(Memoli 2007;</ref><ref type="bibr" target="#b32">Mémoli 2011;</ref><ref type="bibr">Peyré, Cuturi, and Solomon 2016)</ref> extends the framework to incomparable spaces by allowing the alignment of two distributions when only the within-dataset pairwise distances are available. This approach is particularly well suited to deal with graphs described by their adjacency matrices <ref type="bibr">(Xu et al. 2019;</ref><ref type="bibr" target="#b56">Xu, Luo, and Carin 2019;</ref><ref type="bibr">Chowdhury and Mémoli 2019)</ref>. The GW discrepancy has been used efficiently in various applications such as heterogeneous DA <ref type="bibr" target="#b58">(Yan et al. 2018)</ref>, word translation (Alvarez-Melis and Jaakkola 2018) or GAN <ref type="bibr" target="#b52">(Vayer et al. 2019;</ref><ref type="bibr" target="#b5">Bunne et al. 2019)</ref>. Recently, Co-Optimal Transport (Co-OT) <ref type="bibr">(Redko et al. 2020</ref>) extended the OT theory to datasets lying in different vector spaces. The idea is to jointly learn two transport plans. The first one aligns the examples as in standard OT while the second one aligns the most similar features. This has been shown to be of particular interest in heterogeneous DA and co-clustering.</p><p>Motivation and Contribution. While GW and Co-OT already cover a wide range of problems, we claim that many other scenarios are not covered by these two extensions. Let us suppose that both the source and target distributions are represented by a collection of graphs of the same size (in terms of nodes) but of different structure (in terms of edges). This is typically the case of two graphs evolving over time. In this case, the goal of OT would be to jointly align both the two collections of graphs and the nodes. It turns out that GW would be only able to handle the special case where there exist a known one-to-one correspondence between the graphs of the two collections. Another application is inspired from comparison-based learning. Let us consider a source and a target distribution represented by a set of users who watched movies, users providing a list of triplet comparisons of the form "movie x i is closer to x j than to x k ". In this case, neither GW nor Co-OT is able to align the two distributions because of the nature of this triplet-based representation. A last example comes from computer vision, where one may want to align two collections of images while preserving the OT plan T 3 between the columns of MNIST and USPS; the arrows explain how to match the pixels between the two datasets using T 2 and T 3 obtained with OTT.</p><p>some inner structural information in rows and columns. It is worth noting that these three applications share a common characteristic: they can be represented in the form of thirdorder tensors. To solve OT tasks on such complex structures, it is necessary to design a framework generalizing the OT theory. This is the main contribution of this paper.</p><p>We propose Optimal Tensor Transport (OTT), a new OT formulation that can handle datasets represented as tensors of any order by potentially learning multiple transport plans. The underlying idea is to jointly match the different dimensions of each tensor with respect to their weights. Figure <ref type="figure" target="#fig_0">1</ref> illustrates this on a transportation problem between images from the <ref type="bibr">MNIST (LeCun et al. 1998)</ref> to the USPS dataset <ref type="bibr" target="#b17">(Friedman et al. 2001)</ref>. Three transport plans are optimized in this scenario. T 1 is used to match the points (on the left), T 2 and T 3 preserve the structure by respectively mapping the pixel rows and pixel columns jointly (figure on the right). OTT effectively matches digits of the same class while only using supervision from the MNIST dataset. Note that the pixel-level transport plans are both close to the identity meaning that the structure of the images is automatically retrieved. We further illustrate this behaviour by extending this experiment in the supplementary material. From a theoretical perspective, we show that OTT encompasses both Co-OT and GW as well as standard OT. We also show that OTT can be seen as a distance between tensors of any order and thus it can be used to compute tensor barycenters. From an algorithmic point of view, we propose an efficient optimization scheme based on a stochastic mirror descent that allows a drastic reduction of the computational complexity.</p><p>The rest of this paper is organized as follows: Section 2 recalls some preliminary knowledge on OT, Co-OT and GW. Section 3 is dedicated to the introduction of our optimal tensor transport (OTT) setting. Section 4 proposes an efficient algorithm for solving OTT. We derive theoretical properties in Section 5 before presenting experimental results on DA and Comparison-based Clustering tasks in Section 6 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Preliminary Knowledge</head><p>In this section, we recall the standard OT <ref type="bibr">(Villani 2008;</ref><ref type="bibr" target="#b39">Peyré, Cuturi et al. 2019)</ref>, the GW <ref type="bibr" target="#b31">(Memoli 2007;</ref><ref type="bibr">Peyré, Cuturi, and Solomon 2016)</ref>, and the Co-OT <ref type="bibr">(Redko et al. 2020)</ref> formulations. Let p and q be two histograms of respective dimensions I and K. The set of coupling transport plans is defined as U pq = {T ∈ R I×K + |T 1 K = p, T 1 I = q} where 1 R is a vector of ones of dimension R. The goal in discrete OT is to learn one (in standard OT and GW) or two (in Co-OT) transport plans. Note that for the sake of clarity, we only consider the discrete case here. Nevertheless, all the formulations presented in this section, as well as OTT, can be straightforwardly extended to the continuous case by replacing the sums by integrals over the compared distributions. In this case, the transport plans take the form of joint continuous measures. To prepare for our generalization, we unify the formulations below. In particular, we introduce subscripts and superscripts that are usually not used in the standard formulations. We denote the (R -1)-simplex</p><formula xml:id="formula_0">∆ R = {(x r ) r∈ 1,R ∈ R R + | R r=1 x r = 1}.</formula><p>Optimal Transport <ref type="bibr">(Villani 2008)</ref>. Let X and Y be two datasets defined over the same feature space X (e.g. X = R F ), with respectively I 1 ∈ N and K 1 ∈ N points with weights p 1 ∈ ∆ I1 and q 1 ∈ ∆ K1 . The optimal transport plan between X and Y is obtained by solving:</p><formula xml:id="formula_1">min T 1 ∈U p 1 q 1 I1 i1=1 K1 k1=1 L(X i1 , Y k1 )T 1 i1k1 (1) where X i1 is example i 1 in dataset X.</formula><p>Here, L is a loss function which measures the cost of aligning two examples X i and Y k . An extension of OT which is conceptually different from what is covered in this article is the multi-marginal OT <ref type="bibr" target="#b6">(Carlier 2003;</ref><ref type="bibr" target="#b33">Moameni 2014;</ref><ref type="bibr" target="#b35">Pass 2015;</ref><ref type="bibr" target="#b16">Friedland 2020</ref>) that aligns R ≥ 3 datasets simultaneously: L becomes a function of R parameters and T 1 an R-order tensor.</p><p>Co-Optimal Transport <ref type="bibr">(Redko et al. 2020)</ref>. Co-Optimal Transport also aims at transporting points from two datasets X and Y . However, contrary to standard OT, these datasets may have different feature spaces X ⊆ R I2 and Y ⊆ R K2 of respective dimensions I 2 and K 2 and equipped with weights p 2 ∈ ∆ I2 and q 2 ∈ ∆ K2 . The goal is to jointly match the points with a first transport plan T 1 and the features with a second one T 2 . The Co-OT formulation is as follows:</p><formula xml:id="formula_2">min T 1 ∈U p 1 q 1 T 2 ∈U p 2 q 2 I1,I2 i1,i2=1 K1,K2 k1,k2=1 L(X i1i2 , Y k1k2 )T 1 i1k1 T 2 i2k2 (2)</formula><p>where X i1i2 is the value of feature i 2 for example i 1 .</p><p>Gromov Wasserstein <ref type="bibr" target="#b31">(Memoli 2007)</ref>. Instead of having features describing the examples, let us consider that we only have access to within-dataset pairwise similarities or dissimilarities, that is X and Y are now square matrices of dimensions I 1 × I 1 and K 1 × K 1 . It means that the two datasets may have different feature spaces, as in Co-OT, but</p><formula xml:id="formula_3">c) OTT 11 (GW) b) OTT 12 (Co-OT) d) OTT 111 (triplets) f) OTT 112 (GW collections) e) OTT 123 (triCo-OT) T 1 T 1 T 2 T 1 T 1 T 1 T 1 T 1 T 1 T 2 compact representation T 1 T 1 full representation a) OTT 1 (OT) (F=5 features) T 1 T 3 T 2</formula><p>Figure 2: Various formulations of OTT with, each time, the two datasets and the different transport plans (best viewed in color). The subscripts of OTT correspond to the indices of the transport plans used in each dimension. since these feature spaces are implicit, it is sufficient to learn a single transport plan T 1 . The GW formulation is:</p><formula xml:id="formula_4">min T 1 ∈U p 1 q 1 I1,I1 i1,i2=1 K1,K1 k1,k2=1 L(X i1i2 , Y k1k2 )T 1 i1k1 T 1 i2k2 (3)</formula><p>where X i1i2 is the (dis)similarity between examples i 1 and i 2 . It is typical, for both Co-OT and GW, to use a comparison/loss function L (often the squared difference) that operates on two numbers. In Co-OT, L compares the value of a feature from one point in X with one feature from a point in Y. In GW, it compares an entry of the pairwise matrix X to one in Y . Both formulations can be extended by allowing L to compare more complex entries such as F -dimensional vectors in R F . As illustrated in the top row of Figure <ref type="figure" target="#fig_3">2</ref>, corresponding to the formulations of Equations ( <ref type="formula">1</ref>), (2), and</p><p>(3), although OT, Co-OT and GW solve different problems, they still share common principles. Below, we propose a new generalized OT formulation that encompasses all of them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Optimal Tensor Transport (OTT)</head><p>Given the notational complexity involved in our generic formulation, let us first explain the intuition behind the subscripts associated with OTT as illustrated in Figure <ref type="figure" target="#fig_3">2</ref>. Both Co-OT and GW work on matrices (that is tensors of order D = 2) and thus will be represented with 2 digits. Since Co-OT uses A = 2 different transport plans T 1 and T 2 , computing Co-OT boils down to solving OTT 12 as defined below. On the other hand, GW uses the same plan T 1 for both dimensions, thus corresponding to OTT 11 . Note that dimensions that share a transport plan must have the same sizes. Thus, GW (OTT 11 ) deals with square matrices. Starting to generalize, when working with tensors of order D, a given OT extension considers A ≤ D transport plans and associates a transport plan (index) to each dimension. This is done by specifying an affectation function f : 1, D 1, A or equivalently, a D-tuple of transport plan indices, that is f ∈ 1, A D . For instance, Co-OT uses f = (1, 2) which corresponds to the subscript in OTT 12 .</p><p>For a given f ∈ 1, A D , we can now detail our OTT f formulation (denoted OTT when no ambiguity arises) that defines a distance between two datasets X and Y , represented as order D+1 tensors of respective size (I f (1) ...I f (D) , F ) and (K f (1) ...K f (D) , F ). The first D dimensions will be matched between the two datasets using the transport plans, while the last dimension (F ) is the feature dimension used to compare 2 points with the loss L. To simplify the rest of the paper, we will suppose that F = 1, as done in Co-OT and GW above. The OTT distance between X and Y relies on finding a list of optimal transport plans (T a ) a∈ 1,A under constraints on the marginals defined respectively by the weight vectors (p a ) a∈ 1,A and (q a ) a∈ 1,A . OTT is defined as:</p><formula xml:id="formula_5">OTT f (X, Y, (p a ) a , (q a ) a ) = min ∀a T a ∈U p a q a E f (X, Y, (T a ) a )</formula><p>where E f X, Y, (T a ) a∈ 1,A = (4)</p><formula xml:id="formula_6">I f (1) ,...,I f (D) i1,...,i D =1 K f (1) ,...,K f (D) k1,...,k D =1 L(X i1...i D , Y k1...k D ) D d=1 T f (d) i d k d</formula><p>where X i1...i D is the entry at position i 1 ...i D in the tensor X.</p><p>From this general formulation and looking at Equations 1, 2 and 3 with the support of Figure <ref type="figure" target="#fig_3">2</ref>, one can check that OT corresponds to OTT 1 (with F possibly &gt; 1), Co-OT is equivalent to OTT 12 and GW corresponds to OTT 11 . Our OTT formulation makes it possible to handle new forms of datasets as illustrated in the second row of Figure <ref type="figure" target="#fig_3">2</ref>. In the experiments (see Section 6), we will specifically consider two versions of OTT, each with order 3 tensors: (i) OTT 111 corresponds to datasets of triplets (like GW but with triplets instead of pairs); (ii) OTT 112 works with datasets that are collections of adjacency matrices. Figure <ref type="figure" target="#fig_0">1</ref> gives an illustration of a third kind of datasets, where OTT 123 has been applied to collections of images, like Co-OT but with three dimensions.</p><p>It is worth mentioning that the question that we tackle here is reminiscing of another problem in the literature: the Dregular hypergraphs <ref type="bibr">(Berge 1984)</ref> matching. Such a problem is indeed equivalent to OTT in the particular case where all the transport plans are identical. But it uses either a different formulation or different constraints on the matching. <ref type="bibr" target="#b60">Zass and Shashua (2008)</ref> proposes to find a soft matching between D-regular hypergraphs, with uniform inequality constraints, using a Kullback-Leibler objective function. <ref type="bibr" target="#b14">Duchenne et al. (2011)</ref> also matches hypergraphs, with a formulation similar to OTT but uses only row constraints on the matching matrix. Finally, <ref type="bibr">(Peyré et al. 2016</ref>) and <ref type="bibr" target="#b34">(Ning and Georgiou 2014)</ref> propose to represent examples as PSD matrices and to align those matrices using a single transport plan where each entry is also a PSD matrix instead of a real value.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Algorithm to Solve OTT</head><p>In this section, we detail how to efficiently solve the main optimization problem behind Equation <ref type="formula">4</ref>. The most used method for solving GW is called EGW <ref type="bibr">(Peyré, Cuturi, and Solomon 2016)</ref>. It can be seen as a Mirror Descent scheme <ref type="bibr" target="#b2">(Beck and Teboulle 2003)</ref> with the Kullback-Leibler divergence on a regularized version of GW: min T ∈U p 1 q 1 E(T ) + KL(T, p 1 q 1 ). The idea of the Mirror Descent algorithm is to interpret the usual gradient descent, at a point x, as a minimization of the sum of a linearization of the desired function h: ∇ x h, • plus a regularization term x -• 2 2 . Instead of using the Euclidean distance, <ref type="bibr">Peyré, Cuturi, and Solomon (2016)</ref> use the KL divergence. Thus, at a point T 1 , <ref type="bibr">Peyré, Cuturi, and Solomon (2016)</ref> show that the minimization becomes equivalent to the entropy regularized OT problem <ref type="bibr" target="#b11">(Cuturi 2013</ref>):</p><formula xml:id="formula_7">min T ∈U p 1 q 1 ∇ T 1 E, T + KL(T, p 1 q 1 ).</formula><p>(5) <ref type="bibr">Xu et al. (2019)</ref> based on the work of <ref type="bibr" target="#b55">Xie et al. (2020)</ref> change the uniform distribution p 1 q 1 in Equation ( <ref type="formula" target="#formula_21">5</ref>) to the previous transport plan T 1 . In fact, this is equivalent to applying a Mirror Descent algorithm on the original GW problem (see Equation (3)) instead of the regularized one (see supplementary material for more details). Thus, to solve the OTT problem, we use a Mirror Descent algorithm with the KL divergence. When the goal is to find multiple transport plans, we propose to use an alternating approach, similar to the Co-OT solver, where each transport plan is optimized in turn while the others remain fixed. In summary, we combine the ideas of existing solvers for Co-OT and GW and apply an alternate Mirror Descent algorithm with the KL divergence for OTT, with the main bottleneck being the computation of the gradient of E. The pseudo-code of our approach is presented in Algorithm 1. The main steps are the following:</p><p>Algorithm 1: OTT Require: datasets X, Y , weights (p a ) a∈ 1,A , (q a ) a∈ 1,A , loss function L, nb. of samples M , regularization 1: ∀a ∈ 1, A , T a = p a q a 2: for s= 0 to S-1 do 3:</p><p>for a= 1 to A do 4:</p><p>∇T a E = M samples of the gradient using Equation (7).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5:</head><p>T a = minT ∈U p a q a ∇T a E, T + KL(T, T a ) 6: end for 7: end for</p><p>Step 1: We initialize the transport plans (line 1) with the marginal product.</p><p>Step 2: We compute the gradient of E. For the sake of clarity, we assume that the aligned tensors are "cubic", that is all their dimensions are of the same size N . In this case, the overall gradient with respect to T a is a N 2 matrix:</p><formula xml:id="formula_8">∇ T a E = {d |f (d )=a} I f (1) ,...,I f (d -1) I f (d +1) ,...,I f (D) i1,...,i d -1 =1 i d +1 ,...,i D =1 K f (1) ,...,K f (d -1) K f (d +1) ,...,K f (D) k1,...,k d -1 =1 k d +1 ,...,k D =1 L X i1...i d -1 , • ,i d +1 ...i D , Y k1...k d -1 , • ,k d +1 ...k D D d=1|d =d T f (d) i d k d . (6)</formula><p>Note that computing the overall gradient exactly would be too expensive. Indeed, a naive approach leads to O(N 2D ) operations which is prohibitively high. To simplify the computation, a first idea would be to generalize the approach used for GW by <ref type="bibr">Peyré, Cuturi, and Solomon (2016)</ref> to our problem. This would reduce the complexity to O(N D+1 ) for a particular class of functions L, notably the square loss.</p><p>We provide a proof of this approach in the supplementary material. Nevertheless, this remains too expensive as soon as D = 3. Thus, instead, we propose to use a stochastic Mirror Descent <ref type="bibr" target="#b62">(Zhou et al. 2017;</ref><ref type="bibr" target="#b61">Zhang and He 2018;</ref><ref type="bibr" target="#b23">Hanzely and Richtárik 2021)</ref>. This idea was used for the GW problem by <ref type="bibr">Kerdoncuff, Emonet, and Sebban (2021)</ref> and we generalize it to our OTT problem. The main idea is to notice that the gradient of E with respect to T a can be seen as a sum of expectations over matrices of size N 2 , denoted (C d ) {d |f (d )=a} , such that:</p><formula xml:id="formula_9">P C d = L X i1...i d -1 , • ,i d +1 ...i D , Y k1...k d -1 , • ,k d +1 ...k D = D d=1|d =d T f (d) i d k d with I f (1) ,...,I f (d -1) I f (d +1) ,...,I f (D) i1,...,i d -1 =1 i d +1 ,...,i D =1 K f (1) ,...,K f (d -1) K f (d +1) ,...,K f (D) k1,...,k d -1 =1 k d +1 ,...,k D =1 D d=1 d =d T f (d) i d k d = 1 since ∀a ∈ 1, A , Ia,Ka i,k=1 T a ik = 1.</formula><p>The gradient can then be reformulated as:</p><formula xml:id="formula_10">∇ T a E = {d |f (d )=a} E C d .<label>(7)</label></formula><p>It means that one may obtain an unbiased estimate of the gradient in O(M N 2 ) operations where M is the number of samples to estimate the expectations.</p><p>Step 3: The last step (line 5) requires to solve a regularized OT problem, that can be efficiently solved using a Sinkhorn solver <ref type="bibr">(Xu et al. 2019;</ref><ref type="bibr" target="#b11">Cuturi 2013)</ref>.</p><p>We refer the interested reader to <ref type="bibr">Peyré, Cuturi, and Solomon (2016)</ref> and <ref type="bibr">Xu et al. (2019)</ref> for an analysis of the efficiency of the Mirror Descent algorithm, and to <ref type="bibr">Kerdoncuff, Emonet, and Sebban (2021)</ref> for investigations on the precision of the stochastic approximation of the gradient. We also provide in the supplementary material an experiment specific to our new formulation to show how well the gradient is approximated with an increasing order D of the tensors.</p><p>In this section, we derive two main theoretical results. Theorem 1 shows that as long as the cost function is a proper distance, then OTT is a distance between D-order tensors. Thus, we can naturally define an OTT barycenter between tensors. Theorem 2 states that the optimal barycenter can be found in closed form for particular loss functions. Theorem 1. OTT is a distance between weighted tensors (X, (p a ) a∈ 1,A ) and (Y , (q a ) a∈ 1,A ) represented in canonical form (Definition 1 in the supplementary material), for any affectation function f , as long as L is a proper distance.</p><p>The proof is provided in the supplementary material. This result notably extends the distance proof of Co-OT <ref type="bibr">(Redko et al. 2020)</ref> to matrices of different sizes and to non-uniform weights. Even though their comparison with OTT is out of the scope of this paper, notice that other distances exist between higher-order tensors <ref type="bibr" target="#b13">(De Lathauwer, De Moor, and Vandewalle 2000;</ref><ref type="bibr" target="#b29">Liu, Liu, and Chan 2010;</ref><ref type="bibr" target="#b27">Lai et al. 2013)</ref>.</p><p>Since OTT is a distance, we can define an OTT barycenter between several tensors with any affectation function f . Definition 1. (OTT barycenter) Assume that we are given B ≥ 1 weighted tensors of sizes</p><formula xml:id="formula_11">((K b a ) a∈ 1,A ) b∈ 1,B de- noted X b ∈ R K b f (1) ...K b f (D) , (q a,b ∈ ∆ K b a ) a∈ 1,A b∈ 1,B</formula><p>.</p><p>Let λ ∈ ∆ B be the weights quantifying the importance of each tensor. For fixed size (I a ) a∈ 1,A and weights (p a ∈ ∆ Ia ) a∈ 1,A , the OTT barycenter is defined as</p><formula xml:id="formula_12">min X∈R I f (1) ...I f (D) B b=1 λ b OTT(X, X b , (p a ) a , (q a,b ) a ).<label>(8)</label></formula><p>Note that the barycenter could also be defined in a similar manner with the marginals (p a ) a∈ 1,A not fixed.</p><p>To solve Problem (8), we propose to minimize alternatively the objective function w.r.t. X and (T a,b ) a∈ 1,A , the transport plans between X and X b . The latters can be found independently for each b ∈ 1, B using Algorithm 1. Interestingly, X can be obtained in closed form for particular loss functions, which generalizes, in particular to Co-OT, a known result for OT and GW <ref type="bibr">(Peyré, Cuturi, and Solomon 2016)</ref>. This is summarized in the next theorem. Theorem 2. Assume that the loss L is continuous and can be written as</p><formula xml:id="formula_13">L(x, y) = f 1 (x) + f 2 (y) -h 1 (x)h 2 (y) with four functions (f 1 , f 2 , h 1 , h 2 ) such that f 1 h 1 is invert- ible. Further assume that L(x, y) -→ x→±∞ +∞. For fixed ((T a,b ) a∈ 1,A ) b∈ 1,B , for all (i d ∈ 1, I f (d) ) d∈ 1,D , the optimal solution X * i1,...,i D of Problem (8) is equal to f 1 h 1 -1   B b=1 λ b K b f (1) ,...,K b f (D) k1,...,k D =1 h 2 (X b k1...k D ) D d=1 T f (d),b i d k d p f (d) i d   .</formula><p>In particular, when L is the squared euclidean distance,</p><formula xml:id="formula_14">X * i1,...,i D = B b=1 λ b K b f (1) ,...,K b f (D) k1,...,k D =1 X b k1...k D D d=1 T f (d),b i d k d p f (d) i d .</formula><p>Note that to obtain a barycenter using loss functions that are not covered by Theorem 2, for example the absolute loss, one can resort to a gradient based optimization scheme.</p><p>In the next section, we present experiments focused on 3D-tensors alignments. Nevertheless, it is worth noticing that our theoretical results and Algorithm 1 hold for any tensor order and thus might be used with D = 4, for example in comparison based learning tasks <ref type="bibr" target="#b18">(Ghoshdastidar, Perrot, and von Luxburg 2019)</ref> or to match hypergraphs <ref type="bibr">(Berge 1984)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experiments</head><p>In this section, we illustrate the interest of OTT on two different tasks<ref type="foot" target="#foot_0">1</ref> . First, following the success of OT in Domain Adaptation <ref type="bibr">(Courty et al. 2016)</ref>, we propose to predict the genres of recent movies based on labeled older movies by relying only on users preferences. We advantageously use a 3D-tensor formulation to take into account the particularity of each user. In a second experiment, we use the OTT barycenter in a Comparison-Based Clustering task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Domain Adaptation (DA)</head><p>We consider a DA task on the Movielens dataset <ref type="bibr" target="#b24">(Harper and Konstan 2015)</ref>. The goal is to adapt a model learned on old movies (source) to predict the genres of new movies (target).</p><p>Datasets. We build two 3-orders tensor X s (source) and X t (target) based on the ratings of the users. The entry (i, j, k) in X s (and similarly for X t ) is 1 if the user i preferred the movie j over the movie k, -1 if the movie k is preferred over the movie j and 0 if the user i cannot choose. As the users did not rate every movie, we use the 0.33 percentile of their personal rates as a default rating. For both the new and old movies, we identify 4 different groups of movies: Thriller/Crime/Drama (T ), Fantasy/Sci-Fi (F ), War/Western (W ), and Children's/Animation (C). We then create 6 pairwise binary classification datasets of 200 movies each by selecting 2 classes among the four aforementioned ones. We assume that we have access to all the labels for the old movies (source) but only to a single random label per class for the new movies (target). The goal is to learn a model that is as accurate as possible on the target. Since many movies have a small number of ratings and many users only rated a few movies, we focus on the 100 users with the highest number of ratings and the 200 most rated films for those users.</p><p>Baselines. Even though OTT 122 is, to the best of our knowledge, the first algorithm that allows direct DA on such tensor-based datasets, we still propose various baselines by reducing the X s and X t tensors into matrices by averaging along one dimension. Rdm is a first naive baseline that simply outputs random labels. For the next three baselines, we average over the user dimension. Then, SVM applies a SVM <ref type="bibr" target="#b8">(Cortes and Vapnik 1995)</ref> classifier only on the target domain, using the columns of the matrix as features. S-GWL <ref type="bibr" target="#b56">(Xu, Luo, and Carin 2019)</ref> interprets the obtained matrices as adjacency matrices of graphs and matches the nodes of the two graphs. GW <ref type="bibr">(Peyré, Cuturi, and Solomon</ref> Table <ref type="table" target="#tab_3">1</ref>: Accuracy on 6 DA tasks with the hyperparameters found using the unsupervised proposed method. To evaluate the best possible performance reachable by each method, AVG best displays the accuracy with the best hyperparameters using the ground truth of the target domain. 2016) solves the GW problem directly on the obtained matrices. The last baseline, Co-OT, uses a matrix obtained by averaging over one movie dimension, which leads to a matrix (users, movies). The two axes are then mapped jointly between the new and old movies. For all the methods that provide a transport plan T between the movies, the class of a target movie y t j is predicted via label propagation <ref type="bibr">(Redko et al. 2019a</ref>) of the source label y s , that is y t k = y s T • k . The stochastic methods are run 10 times and the mean and standard deviation are reported.</p><p>Experimental setup and hyperparameter tuning. As the initialization is key to avoid local minima, we take advantage of both the labels and our stochastic algorithm by sampling only the labelled points in the source and target for the first gradient estimation. The squared euclidean loss is used for L and we estimate the gradient of OTT using M = 1000 samples. S is set to 1000 iterations in Algorithm 1. For each method that uses the OT Sinkhorn solver, notably OTT, we replace it with the semi-supervised algorithm OTDA proposed by <ref type="bibr">Courty et al. (2016)</ref> which adds a l p -l 1 regularization to take advantage of the available source labels. In DA, tuning the hyperparameters is often key as there is not enough target labeled movies. As the goal of DA is to reduce the divergence between the two datasets <ref type="bibr" target="#b3">(Ben-David et al. 2007;</ref><ref type="bibr">Redko et al. 2019b)</ref>, we can use the distance between the source and the target as a criterion to choose the hyperparameters for each method. To compute the OTT distance, we resort to the sampling scheme already used to approximate the GW distance in <ref type="bibr">Kerdoncuff, Emonet, and Sebban (2021)</ref>. The Kullback-Leibler regularization parameter of the Sinkhorn method <ref type="bibr" target="#b11">(Cuturi 2013</ref>) is selected in the range [10 -5 , 10 2 ] and the class regularization η of OTDA <ref type="bibr">(Courty et al. 2016)</ref> in [10 -4 , 10 1 ]. The hyperparameters selection is limited to 24 hours for each method and dataset.</p><p>Results. The accuracy of each method is reported in Table 1. OTT achieves better performances than the other baselines on 5 out of 6 datasets. This result was expected as OTT is the only method which takes full advantage of the 3D structure of the data. Interestingly, OTT still behaves better than the baselines even when one uses the ground truth over the target domain to tune their hyperparameters (that would be cheating) as shown in the line AVG best of Table <ref type="table" target="#tab_3">1</ref>.</p><p>We now analyze the impact of the different hyperparameters on the accuracy. We report the results on each dataset in the supplementary material and only consider the global average in Figure <ref type="figure" target="#fig_1">3</ref>. The leftmost plot displays the accuracy for increasing values of the KL regularization parameter . The black markers correspond to the lowest achieved distance for each method. It is worth noting that this usually corresponds to a reasonable accuracy, which supports our hyperparameter tuning procedure. We notice a similar behaviour for the η parameter of OTDA as reported in the supplementary material. In Figure <ref type="figure" target="#fig_1">3</ref> (middle), we report the target accuracy with respect to the number of target labels available. We can notice that OTT is always better, even in the completely unsupervised scenario. Lastly, in the experiments reported in Table <ref type="table" target="#tab_3">1</ref>, we never use the fact that the users comparing the movies are the same for both old and new movies. Here, we study the impact of making this information available. To this end, we fix the transport plan for an increasing number of users. Figure <ref type="figure" target="#fig_1">3</ref> (right) shows that this information greatly improves the target accuracy of the methods that can handle it, especially OTT. Interestingly, as indicated with the black marker, the smallest distance is achieved with the highest number of known pairings, which corresponds to the highest number of constraints on the users transport plan. This supports the key assumption of this experiment: a good matching between users leads to a better matching of similar movies. This also highlights a limit of a mirror descent-based solver as it struggles to find the global minimum without this additional information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Comparison Based Clustering</head><p>In this second series of experiments, we show that OTT barycenters can be used competitively to address an unbalanced comparison-based clustering task.</p><p>Comparison-based learning deals with the problem of learning from examples when neither an explicit representation nor a pairwise distance matrix is available <ref type="bibr" target="#b53">(Vikram and Dasgupta 2016;</ref><ref type="bibr" target="#b49">Ukkonen 2017;</ref><ref type="bibr" target="#b15">Emamjomeh-Zadeh and Kempe 2018;</ref><ref type="bibr" target="#b36">Perrot, Esser, and Ghoshdastidar 2020)</ref>. Instead, it is assumed that only triplet comparisons of the form "example x i is closer to x j than to x k " are available. This field stems from the fact that relative judgments are usually easier than absolute ones for human observers <ref type="bibr" target="#b47">(Shepard 1962;</ref><ref type="bibr" target="#b59">Young 1987;</ref><ref type="bibr" target="#b48">Stewart, Brown, and Chater 2005)</ref>. For example, triplet-based queries are easier to answer than exact distance estimations. Given a set of examples and a given number of triplet comparisons, a dataset can be represented as a third order tensor where the entry (i, j, k) contains 1 if example x i is closer to x j than to x k and -1 otherwise. In comparison based clustering, the goal is to identify relevant groups in the examples, using only the information contained in the aforementioned tensor. As the three dimensions of the cubic tensor correspond to the same points we will use the same tranport plan for all the dimensions, that is OT T 111 .</p><p>Setting. To show the interest of our method for clustering unbalanced triplet datasets, we take inspiration from the experimental setup of <ref type="bibr" target="#b36">Perrot, Esser, and Ghoshdastidar (2020)</ref>.  For a given dataset, we find the OTT 111 barycenter (b = 1) of size (I 1 , I 1 , I 1 ) where I 1 is the number of clusters that we are looking for. The intuition is that similar examples should be sent by the transport plan to the same point in the barycenter since the latter summarizes the initial points.</p><p>Datasets. We consider some 3-class unbalanced subsamples of the MNIST dataset <ref type="bibr" target="#b28">(LeCun et al. 1998)</ref>. For a given number of examples per class <ref type="bibr">(for example, 200,20,20)</ref>, we consider 10 random draws for the 3 classes and, for each of these, we further consider 10 random draws for the actual images. Given N points in each unbalanced dataset, we randomly select N log(N ) 3 triplets of the form d(x i , x j ) &gt; d(x i , x k ) as suggested by <ref type="bibr" target="#b36">Perrot, Esser, and Ghoshdastidar (2020)</ref>. The distance between two digits is the euclidean distance after an UMAP projection in 2 dimensions. To simulate a real dataset, some noise is added by randomly flipping</p><formula xml:id="formula_15">d(x i , x j ) &gt; d(x i , x k ) to d(x i , x j ) &lt; d(x i , x k ) with proba- bility 0.1 for each triplet selected.</formula><p>Baselines. We use two main triplet clustering baselines: (i) t-STE (Van Der Maaten and Weinberger 2012) which projects the triplets into a vector space followed by kmeans <ref type="bibr" target="#b30">(Lloyd 1982)</ref>, and (ii) AddS3 (Perrot, Esser, and Ghoshdastidar 2020) which estimates a pairwise similarity matrix also followed by k-means. Moreover, as the OT formulation requires the marginal as a prior, we assume that the proportions of the clusters are known. To stay fair, we propose two variants (AddS3 s , t-STE s ) of the previous baselines where we replace the k-means step by an OT barycenter step which takes the marginal information into account.</p><p>Hyperparameters. We use default hyperparameters, reported in the supplementary material, for t-STE, AddS3, and OTT with the KL regularization parameter set to = 0.1. To ensure convergence, we also set the number of samples M = 100 and the number of iteration S = 500 between each of the 20 barycenter updates. Finally, to take advantage of the closed form derived in Theorem 2, we use the squared euclidean loss for OTT.</p><p>Results. The Adjusted Rand Index (ARI) <ref type="bibr" target="#b25">(Hubert and Arabie 1985)</ref> between the predicted clusters and the ground truth is displayed in Table <ref type="table" target="#tab_1">2</ref>. Overall, OTT has better performances than AddS3 s on average on all datasets while being slightly worse than t-STE s . Furthermore, for both AddS3 and t-STE, using the unbalancedness information improves the performances. The closeness between our approach and t-STE s is further investigated in the supplementary material, where we show a theoretical connection between t-STE and the OTT barycenter. The choice of the unbalanced setting is motivated by the fact that the two other baselines do not take into account this information during their first step, while OTT directly uses it in its unique step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>We presented OTT, a new OT formulation that can be used to align high dimensional tensors using potentially several transport plans. OTT generalizes various existing OT problems, such as GW and Co-OT, by defining a new tensor distance. We proposed an efficient algorithm to solve the underlying problem and demonstrated the competitiveness of OTT in DA and Comparison-based clustering. While our new approach unlocks new applications, this comes with a cost. First, despite having access to a solver that drastically reduces the computational complexity of the formulation, it still does not scale well on large datasets with high order tensors. Finally, we leave for future work a natural extension, Fused-OTT, inspired by <ref type="bibr">Vayer et al. (2020)</ref>, that would combine several OTT problems together. This approach could allow us to align datasets that are independently represented by multiple tensors of potentially different orders.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Material: Optimal Tensor Transport</head><p>In this supplementary material, we provide details on the various results presented in the main paper as well as complementary experiments. It is organised as follows. First, in Section 1 we illustrate the interest of our method compared to Co-OT. Then, in Section 2, we formally show that the approach of <ref type="bibr">Xu et al. (2019)</ref> used for GW is a Mirror Descent algorithm. In Section 3, we formally investigate the computational complexity of the sampling approach for the gradient approximation with D ≤ 2. In Section 4 we provide the proofs of the different theoretical results. Finally, in Section 5, we provide details on the hyperparameters used in the experiments as well as additional results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Notations</head><p>To simplify the notations, we will use the following shortcuts:</p><formula xml:id="formula_16">∀d i d = I f (1) ,...,I f (D) i1,...,i D =1</formula><p>,</p><formula xml:id="formula_17">∀d =d i d = I f (1) ,...,I f (d -1) I f (d +1) ,...,I f (D) i1,...,i d -1 =1 i d +1 ,...,i D =1 and d =d = D d=1 d =d</formula><p>.</p><p>(1)</p><p>1 Illustration of the difference between OTT and Co-OT</p><p>In this section, we provide the same pixel transportation image as the one provided in <ref type="bibr">(Redko et al. 2020)</ref> for Co-OT, but for both Co-OT and OTT 123 . The idea is to create an image of size 28 by 28 (the same size as the MNIST images) with a different color in each pixel and then to use the transport plans learned to map MNIST onto USPS to transform this image. It gives us new images (one for Co-OT and one for OTT) of the same size as the USPS images that illustrate how both methods alter the pictures when performing transportation. This is illustrated Figure <ref type="figure" target="#fig_0">1</ref>. Note that the transport plans used for OTT are the ones used in Figure <ref type="figure" target="#fig_0">1</ref> in the main paper.</p><p>For Co-OT, many pixels carry no information as we use only 0 and 1 labels, thus the colored USPS image has some coherence only in the middle of the image. On the other hand, with OTT 123 we force columns to be mapped on (whole) columns and rows to be mapped on (whole) rows. By doing so, OTT can extrapolate using the row/column structure inherent to images and the color visualisation is much smoother than for Co-OT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Mirror Descent</head><p>In this section, we prove that the method proposed by <ref type="bibr">(Xu et al. 2019</ref>) is a Mirror Descent algorithm on the original GW problem. More specifically, we prove that Equation (7) in their Section 3.1 is the same step as the step used in a Mirror Descent algorithm <ref type="bibr" target="#b2">(Beck and Teboulle 2003)</ref> with Kullback-Leibler divergence. The Mirror Descent method, at a given point T s at the iteration s, searches for the next minimum T s+1 with,</p><formula xml:id="formula_18">T s+1 = argmin T ∈Uµν ∇ Ts E, T + KL(T, T s ).<label>(2)</label></formula><p>Which is equivalent to,</p><formula xml:id="formula_19">T s+1 = argmin T ∈Uµν ∇ Ts E -log(T s ), T + T, log(T ) .<label>(3)</label></formula><p>The only difference is the missing factor 2 in <ref type="bibr">(Peyré, Cuturi, and Solomon 2016)</ref> that should be here due to the derivative, but both problems are equivalent with a re-scaling of by a factor 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Complexity of the gradient computation of OTT</head><p>We prove in this section that the gradient of OTT, which is necessary for the Mirror Descent algorithm <ref type="bibr" target="#b2">(Beck and Teboulle 2003)</ref>, can be computed in a time complexity of O(N D+1 ) for particular loss functions. This generalizes a known result for GW <ref type="bibr">(Peyré, Cuturi, and Solomon 2016)</ref>. Note that, in the main paper, we propose a more efficient approach based on sampling. We only mention this result for the sake of comparison with state of the art approaches.</p><p>Here, we assume that the feature dimension F is small and that every other dimension of the tensor is N to simplify the notations. We also assume that the order of the tensor D is fixed and small and we analyze the time complexity only with respect to N . First, we recall the gradient of E using the notations introduced at the start of this supplementary material:</p><formula xml:id="formula_20">∇ T a E = {d |f (d )=a} ∀d =d i d ∀d =d k d L(X i1...i d -1 , • ,i d +1 ...i D , Y k1...k d -1 , • ,k d +1 ...k D ) d =d T f (d) i d k d .</formula><p>(4) We suppose that the loss L is continuous and can be written as L(x, y) = f 1 (x) + f 2 (y) -h 1 (x)h 2 (y) with four functions (f 1 , f 2 : R F R) and (h 1 , h 2 : R F R F ). This is notably the case for the squared Euclidean distance or the Kullback-Leibler divergence. As each element of the first sum in Equation ( <ref type="formula">4</ref>) will be computed independently, we can fix d . We thus have to compute the following term,</p><formula xml:id="formula_21">∀d =d i d ∀d =d k d f 1 (X i1...i d -1 , • ,i d +1 ...i D ) d =d T f (d) i d k d + ∀d =d i d ∀d =d k d f 2 (Y k1...k d -1 , • ,k d +1 ...k D ) d =d T f (d) i d k d + ∀d =d i d ∀d =d k d h 1 (X i1...i d -1 , • ,i d +1 ...i D )h 2 (Y k1...k d -1 , • ,k d +1 ...k D ) d =d T f (d) i d k d .<label>(5)</label></formula><p>Note that X i1...i d -1 , • ,i d +1 ...i D is a vector of size (N, 1, F ) and as f 1 is applied only on the features dimension,</p><formula xml:id="formula_22">f 1 (X i1...i d -1 , • ,i d +1 ...i D ) is a vector of size (N, 1). Similarly, f 2 (Y k1...k d -1 , • ,k d +1 ...k D ) is a vector of size (1, N ).</formula><p>As the gradient is a (N × N ) matrix, the sum between the first two terms should be understood as a broadcasting sum. The same holds for h 1 (X i1..</p><formula xml:id="formula_23">.i d -1 , • ,i d +1 ...i D ) and h 2 (Y k1...k d -1 , • ,k d +1 ...k D ) of size (N × F</formula><p>) and (F × N ), the product is a matrix of size (N × N ). The first two double sums can be computed as,</p><formula xml:id="formula_24">∀d =d i d f 1 (X i1...i d -1 , • ,i d +1 ...i D ) d =d p f (d) i d + ∀d =d k d f 2 (Y k1...k d -1 , • ,k d +1 ...k D ) d =d q f (d) k d .<label>(6)</label></formula><p>There is D -1 sums, each of them operating over N indices. Hence, the complexity of each sum is O(N D-1 N ). As a consequence, the overall complexity of Equation ( <ref type="formula" target="#formula_24">6</ref>) is O(N D ).</p><p>The last term requires several tensor/matrix multiplications as it can be reformulated as,</p><formula xml:id="formula_25">∀d =d i d h 1 (X i1...i d -1 , • ,i d +1 ...i D )   ∀d =d k d h 2 (Y k1...k d -1 , • ,k d +1 ...k D ) d =d T f (d) i d k d   .<label>(7)</label></formula><p>The D -1 internal sums can be seen as D -1 tensor/matrices multiplications, each between the index d of the tensor and the second index of the matrix T f (d) . Each of these multiplications have a time complexity of O(N D+1 ). We provide an example after the proof, for D = 3. If we call this new tensor H, we can reformulate the problem as,</p><formula xml:id="formula_26">∀d =d i d h 1 (X i1...i d -1 , • ,i d +1 ...i D )H i1...i d -1 , • ,i d +1 ...i D .<label>(8)</label></formula><p>This can be seen as a standard multiplication of two tensors on every dimension except on the dimension d for both tensors. This is equivalent to the multiplication of two matrices of size (N, N D-1 ) and (N D-1 , N ). The time complexity is again</p><formula xml:id="formula_27">O(N × N D-1 × N ) = O(N D+1 ).</formula><p>Finally, the entire time complexity is O(N D+1 ), which is better than the naive O(N 2D ).</p><p>Example for D = 3. We explain how to compute Equation ( <ref type="formula" target="#formula_10">7</ref>) with D = 3 with a O(N 3+1 ) time complexity. We will suppose, without loss of generality, that d = 3 in this example. We are interested in the tensor H of Equation ( <ref type="formula" target="#formula_12">8</ref>) or equivalently of the inner sum of Equation ( <ref type="formula" target="#formula_10">7</ref>) for all</p><formula xml:id="formula_28">(i 1 , i 2 ) ∈ 1, N 2 , N k1=1 N k2=1 h 2 (Y k1,k2, • )T f (1) i1,k1 T f (2) i2,k2 = H i1,i2, • .<label>(9)</label></formula><p>We rearrange the terms to make the multiplication between a 3-order tensor and a transport plan appear,</p><formula xml:id="formula_29">∀(i 1 , i 2 ) ∈ 1, N 2 N k1=1 N k2=1 h 2 (Y k1,k2, • )T f (2) i2,k2 T f (1) i1,k1 . (<label>10</label></formula><formula xml:id="formula_30">) For all (k 1 , i 2 ) ∈ 1, N 2 we note H k1,i2, • = N k2=1 h 2 (Y k1,k2, • )T f (2)</formula><p>i2,k2 . H can be computed with N 2 × N × N operations as a multiplication between a tensor (N, N, N, F ) and a matrix (N, N ) along the second dimension on both sides. We now have a similar formulation as in Equation ( <ref type="formula" target="#formula_29">10</ref>) but with only one transport plan left,</p><formula xml:id="formula_31">∀(i 1 , i 2 ) ∈ 1, N 2 N k1=1 H k1,i2, • T f (1) i1,k1 .<label>(11)</label></formula><p>We apply the same process, and define for all</p><formula xml:id="formula_32">(i 1 , i 2 ) ∈ 1, N 2 , H i1,i2, • = N k1=1 H k1,i2, • T f (1) i1,k1</formula><p>. This new tensor H can be computed with N 2 × N × N operations. The difference is that the dimension of the sum is the first one for the tensor H .</p><p>We finally have to compute,</p><formula xml:id="formula_33">N i1=1 N i2=1 h 1 (X i1,i2, • )H i1,i2, • ,<label>(12)</label></formula><p>which is equivalent to Equation ( <ref type="formula" target="#formula_10">7</ref>). We can see it as a multiplication between two tensors of size (N, N, N, F ) along the first two dimensions as well as the last dimension. Thus the time complexity is</p><formula xml:id="formula_34">O(N × N 2 × N ), that is O(N 4 ).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Theoretical results</head><p>We now prove that OTT is a distance for weighted tensors. Theorem 1. OTT is a distance between weighted tensors represented in canonical form (X, (p a ) a∈ 1,A ) and (Y , (q a ) a∈ 1,A ) for any affectation function f , as long as L is a proper distance.</p><p>First we properly define the canonical form of a tensor. Note that, here, we will assume that two tensors are equal if, and only if, they are equal in canonical form. This is quite natural in an OT context where the goal is to align datasets. Nevertheless, if we now consider that two tensors are equal if, and only if, they are equal in their original forms then OTT is only a pseudo-distance (as is GW <ref type="bibr">(Chowdhury and Mémoli 2019</ref>)) since the identity of indiscernibles would not hold anymore. Definition 1 (Inspired by the work of <ref type="bibr">Chowdhury and Mémoli (2019)</ref> ). A weighted tensor (X, (p a ) a∈ 1,A ) associated with an affectation function f : 1, D 1, A , is in canonical form if it respects the three following rules. • All the weights should be strictly positive. If there is a a ∈ 1, A and i ∈ 1, I a such that p a i = 0 this weight is deleted along all the corresponding values in the tensor X. This is a natural reduction as if the weight is equal to 0 this is equivalent to not having a point.</p><p>• There is no duplicated points. If two points are equal, they should be merged together. Two points</p><formula xml:id="formula_35">(i a , i a ) ∈ 1, I a 2 for a fixed a ∈ 1, A are equal if, ∀d ∈ 1, d |f (d ) = a, X • 1,..., • d -1 ,ia, • d +1 ,..., • D = X • 1,..., • d -1 ,i a , • d +1 ,..., • D .</formula><p>(13) Those extracted (D -1)-order tensors define entirely each of those points. If two points are equal, then we delete one of them and add the two probabilities p a ia and p a i a . Notice that we look at every dimension of the tensor associated with the weight p a and delete simultaneously the points in every dimensions. This result is quite logical in a vector space, if two points are in the same location, they should be merged.</p><p>• Note that, for a given dataset, one may, in each dimension a ∈ 1, A , permute the objects without changing the nature of the dataset. The tensor Y is a permutation of X if there exist a permutation function σ a for all a ∈ 1, A such that</p><formula xml:id="formula_36">Y σ f (1) (i1),..,σ f (D) (i D ) = X i1,..,i D .</formula><p>Then, assuming that we have access to a strict total order on tensors, a weighted dataset is in canonical form if it is the smallest permutation with respect to the given order. An example of strict total order that can be used for this purpose is the lexicographic order. Note that, none of those three modifications of the tensor X will change the transport plan nor the OTT distance that depends on X. It means that we never need to explicitly transform a tensor into its canonical form. Instead, it is a theoretical construction that simplifies the proof of Theorem 1.</p><p>Proof. We will now prove that OTT is a distance.</p><p>Symmetry As L is symmetric, OTT is also symmetric.</p><p>Positiveness As L and T are always positive, OTT is always positive.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Identity of indiscernibles</head><p>To prove that OT T (X, X, (p a ) a∈ 1,A , (p a ) a∈ 1,A ) = 0, we set every transport plan T a to the identity matrix. As ∀x L(x, x) = 0, the entire sum is 0. But because the minimum is smaller than this particular case and also always positive, we have the desired result OT T (X, X, (p a ) a∈ 1,A , (p a ) a∈ 1,A ) = 0.</p><p>We will now prove the opposite, let (X, Y ) be D-order tensors datasets of sizes (I f (1) ×...×I f (D) ×F, K f (1) ×...×K f (D) ×F ) with weights (p a ∈ ∆ Ia , q a ∈ ∆ Ka ) a∈ 1,A in a canonical form (Definition 1). The canonical form is inspired from the proof that GW is a distance for graphs <ref type="bibr">(Chowdhury and Mémoli 2019)</ref>. We suppose that OT T (X, Y ) = 0, we will show that X = Y and for all a ∈ 1, A , p a = q a . We will note (T a ) a∈ 1,A the optimal transport plans.</p><p>We will proceed by contradiction and suppose that there exist two strictly positive values in the same row of a transport plan, more precisely we suppose that there exist a ∈ 1,</p><formula xml:id="formula_37">A , i ∈ 1, I a , (k, k ) ∈ 1, K a 2 such that T a i,k &gt; 0 and T a i,k &gt; 0. We fix d ∈ {d ∈ 1, D |f (d ) = a} and all the indices (k 1 ...k d -1 , k d +1 ...k D ) ∈ ( 1, K f (1) ... 1, K f (d -1) , 1, K f (d +1) ... 1, K f (D)</formula><p>). As all the marginals are strictly positive, there exist</p><formula xml:id="formula_38">(i 1 ...i d -1 , i d +1 ...i D ) ∈ ( 1, I f (1) ... 1, I f (d -1) , 1, I f (d +1) ... 1, I f (D) ) such that T f (d) i d ,k d &gt; 0 for all d ∈ 1, D with d = d .</formula><p>As the transport plans are strictly positive on those indices and T a i,k &gt; 0 and T a i,k &gt; 0, the transport plans product is strictly positive, thus the loss should be equal to 0,</p><formula xml:id="formula_39">L X i1...i d -1 ,i,i d +1 ...i D , Y k1...k d -1 ,k,k d +1 ...k D = 0, (14) L X i1...i d -1 ,i,i d +1 ...i D , Y k1...k d -1 ,k ,k d +1 ...k D = 0. (<label>15</label></formula><formula xml:id="formula_40">)</formula><p>As the loss is a distance, we have</p><formula xml:id="formula_41">Y k1...k d -1 ,k,k d +1 ...k D = Y k1...k d -1 ,k ,k d +1 ...k D . This is true for all d ∈ {d ∈ 1, D |f (d ) = a} and all the indices (k 1 ...k d -1 , k d +1 ...k D ) ∈ ( 1, K f (1) ... 1, K f (d -1) , 1, K f (d +1) ... 1, K f (D) )</formula><p>, thus those two points should have been merged, this is in contradiction with the canonical form assumption. We can do the same for the columns of the transport plans instead of the rows. We know that a transport plan has only one element in each of its rows and columns, thus it is necessarily a square matrix as all the marginals are strictly positive. Let α be the smallest strictly positive value of the transport plan (T a ) a∈ 1,A . We also define the permutation matrices P a associated to each T a by replacing each strictly positive value in T a to 1 in P a . We have α I f (a) P a ≤ T a elements-wise, thus</p><formula xml:id="formula_42">0 ≤ ∀d i d ∀d k d L(X i1,...,i D , Y k1,...,k D ) D d=1 α I f (d) P f (d) i d ,k d (16) ≤ ∀d i d ∀d k d L(X i1,...,i D , Y k1,...,k D ) D d=1 T f (d) i d ,k d (17) =0. (<label>18</label></formula><formula xml:id="formula_43">)</formula><p>If we note σ a the permutation function associated with P a , we have,</p><formula xml:id="formula_44">0 = ∀d i d L(X i1,...,i D , Y σ f (1) (i1),...,σ f (D) (i D ) ). (<label>19</label></formula><formula xml:id="formula_45">)</formula><p>Since datasets are invariant to points permutations, if both X and Y are in canonical form, they are equal. Thus the transport plans (T a ) a∈ 1,A are diagonal matrices. Therefore, the weights p a and q a are also equal for all a ∈ 1, A . Thus, we have OT T (X, Y, (p a ) a∈ 1,A , (q a ) a∈ 1,A ) = 0 ⇐⇒ (X, (p a ) a∈ 1,A ) = (Y, (q a ) a∈ 1,A ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Triangle inequality</head><p>We now prove the triangle inequality. Let (X, Y , Z) be D-order tensors datasets of sizes d) &gt; 0 as we can delete the corresponding (D -1)-order tensor in the tensor Z if one term is 0 and it won't change the transport plan nor the distance. We note (T a xy ) a∈ 1,A the optimal transport plans between X and Y and (T a yz ) a∈ 1,A the optimal transport plans between Y and Z. Similarly to <ref type="bibr">Redko et al. (2020)</ref> we will use the gluing lemma <ref type="bibr">(Villani 2008)</ref> to construct a coupling between X and Z,</p><formula xml:id="formula_46">(I f (1) × ... × I f (D) × F, K f (1) × ... × K f (D) × F, J f (1) × ... × J f (D) × F ) with weights (p a ∈ ∆ Ia , q a ∈ ∆ Ka , r a ∈ ∆ Ja ) a∈ 1,A . We suppose ∀d ∈ 1, D r f (</formula><formula xml:id="formula_47">∀a ∈ 1, A (T a xz ) = (T a xy ) diag 1 q a (T a yz ). (<label>20</label></formula><formula xml:id="formula_48">) Let a ∈ 1, A , we show that (T a xz ) ∈ U p a r a : ∀j ∈ 1, J a Ia i=1 (T a xz ) ij = Ia i=1 Ka k=1 (T a xy ) ik 1 q a k (T a yz ) kj (21) = Ka k=1 (T a yz ) kj (22) = r a j , (<label>23</label></formula><formula xml:id="formula_49">) ∀i ∈ 1, I a Ja j=1 (T a xz ) ij = Ja j=1 Ka k=1 (T a xy ) ik 1 q a k (T a yz ) kj (24) = Ka k=1 (T a xy ) ik (25) = p a i .</formula><p>(26) We now prove the triangle inequality:</p><p>OT T (X, Z, (p a ) a∈ 1,A , (r a ) a∈ 1,A ) (27)</p><formula xml:id="formula_50">≤ I f (1) ,...,I f (D) i1,...,i D =1 J f (1) ,...,J f (D) j1,...,j D =1 L(X i1,...,i D , Z j1...,j D ) D d=1 (T f (d) xz ) i d ,j d (28) = I f (1) ,...,I f (D) i1,...,i D =1 J f (1) ,...,J f (D) j1,...,j D =1 K f (1) ,...,K f (D) k1,...,k D =1 L (X i1,...,i D , Z j1,...,j D ) D d=1 (T f (d) xy ) i d k d (T f (d) yz ) k d j d q f (d) k d (29) ≤ ∀d i d ∀d j d ∀d k d L (X i1,...,i D , Y k1,...,k D ) D d=1 (T f (d) xy ) i d k d (T f (d) yz ) k d j d q f (d) k d + ∀d i d ∀d j d ∀d k d L (Y k1,...,k D , Z j1,...,j D ) D d=1 (T f (d) xy ) i d k d (T f (d) yz ) k d j d q f (d) k d (30) = ∀d i d ∀d k d L (X i1,...,i D , Y k1,...,k D ) D d=1 (T f (d) xy ) i d k d ∀d j d D d=1 (T f (d) yz ) k d j d q f (d) k d + ∀d j d ∀d k d L (Y k1,...,k D , Z j1,...,j D ) D d=1 (T f (d) yz ) k d j d ∀d i d D d=1 (T f (d) xy ) i d k d q f (d) k d (31) = ∀d i d ∀d k d L (X i1,...,i D , Y k1,...,k D ) D d=1 (T f (d) xy ) i d k d D d=1 J f (d) j d =1 (T f (d) yz ) k d j d q f (d) k d + ∀d j d ∀d k d L (Y k1,...,k D , Z j1,...,j D ) D d=1 (T f (d) yz ) k d j d D d=1 I f (d) i d =1 (T f (d) xy ) i d k d q f (d) k d (32) = ∀d i d ∀d k d L (X i1,...,i D , Y k1,...,k D ) D d=1 (T f (d) xy ) i d k d + ∀d j d ∀d k d L (Y k1,...,k D , Z j1,...,j D ) D d=1 (T f (d) yz ) k d j d (33) =OT T (X, Y, (p a ) a∈ 1,A , (q a ) a∈ 1,A ) + OT T (Y, Z, (q a ) a∈ 1,A , (r a ) a∈ 1,A ). (<label>34</label></formula><formula xml:id="formula_51">)</formula><p>In Equation ( <ref type="formula">31</ref>), the product/sum inversion is possible as no element in the product depends on (j d ) d∈ 1,D . Similarly, in Equation ( <ref type="formula">32</ref>), the sum/product inversion is allowed as only one term in the product depends on the sum. In addition, each of those sums are equal to 1 by definition of (T a xy ) a∈ 1,A , this leads to Equation (33). OTT respects the triangle inequality, thus it is a distance.</p><p>We now prove Theorem 2 with a more general formulation that the one proposed in the paper. More precisely, we extend it to the case where the loss is a function of R F R with F not necessarily restricted to 1 as supposed in the paper to simplify the notations. First we recall the definition of an OTT barycenter. Definition 2. (OTT barycenter) Given B ∈ N weighted tensors of sizes</p><formula xml:id="formula_52">((K b a ) a∈ 1,A ) b∈ 1,B , X b ∈ R K b f (1) ...K b f (D) ×F , (q a,b ∈ ∆ K b a ) a∈ 1,A b∈ 1,B</formula><p>. Let λ ∈ ∆ B be the weights quantifying the importance of each tensor. For fixed size (I a ) a∈ 1,A and weights (p a ∈ ∆ Ia ) a∈ 1,A , the OTT barycenter is defined as follows:</p><formula xml:id="formula_53">min X∈R I f (1) ...I f (D) ×F B b=1 λ b OTT(X, X b , (p a ) a∈ 1,A , (q a,b ) a∈ 1,A ). (<label>35</label></formula><formula xml:id="formula_54">)</formula><p>Theorem 2. Assume that the loss L is continuous and can be written as L(x, y) = f 1 (x) + f 2 (y) -h 1 (x)h 2 (y) with four functions (f 1 , f 2 : R F R) and (h 1 , h 2 : R F R F ) such that the function (∇h 1 ) -1l ∇f 1 : R F -→ R F is invertible, were the F × F matrix (∇h 1 ) -1l (x) is the left inverse of the F × F matrix ∇h 1 (x), ∀x ∈ R F . We also suppose that, ∀r ∈ 1, F L(x, y) -→ xr→±∞ +∞. For fixed (T a,b ) a∈ 1,A , the optimal solution X * of Problem (8) reads,</p><formula xml:id="formula_55">X * i1,...,i D = (∇h 1 ) -1l ∇f 1 -1   B b=1 λ b K b f (1) ,...,K b f (D) k1,...,k D =1 h 2 (X b k1,...,k D ) D d=1 T f (d),b i d k d p f (d) i d   (36) for all (i d ∈ 1, I f (d) ) d∈ 1,D .</formula><p>In particular, when L is the squared euclidean distance,</p><formula xml:id="formula_56">X * i1,...,i D = B b=1 λ b K b f (1) ,...,K b f (D) k1,...,k D =1 X b k1...k D D d=1 T f (d),b i d k d p f (d) i d .<label>(37)</label></formula><p>Proof. For fixed ((T a,b ) a∈ 1,A ) b∈ 1,B , we are looking for the derivative of Equation ( <ref type="formula" target="#formula_53">35</ref>) with respect to X i1,...,i D and we equate it to 0 to find the optimal solution. X i1,...,i D might be a vector if the features dimension is not reduced to 1. Each gradient in the following equation is a vector of size F .</p><formula xml:id="formula_57">0 = B b=1 λ b K b f (1) ,...,K b f (D) k1,...,k D =1 ∇ Xi 1 ,...,i D L(X i1,...,i D , X b k1,...,k D ) D d=1 T f (d),b i d k d (38) ⇐⇒ 0 = B b=1 λ b ∀d k d ∇f 1 (X i1,...,i D ) -∇h 1 (X i1,...,i D )h 2 (X b k1,...,k D ) D d=1 T f (d),b i d k d (39) ⇐⇒ 0 = B b=1 λ b ∀d k d ∇f 1 (X i1,...,i D ) -∇h 1 (X i1,...,i D )h 2 (X b k1,...,k D ) D d=1 T f (d),b i d k d (40) ⇐⇒ 0 = ∇f 1 (X i1,...,i D ) B b=1 λ b ∀d k d D d=1 T f (d),b i d k d (41) -∇h 1 (X i1,...,i D ) B b=1 λ b ∀d k d h 2 (X b k1,...,k D ) D d=1 T f (d),b i d k d (42) ⇐⇒ ∇f 1 (X i1,...,i D ) B b=1 λ b D d=1 p f (d) i d = ∇h 1 (X i1,...,i D ) B b=1 λ b ∀d k d h 2 (X b k1,...,k D ) D d=1 T f (d),b i d k d (43) ⇐⇒ (∇h 1 ) -1l (X i1,...,i D )∇f 1 (X i1,...,i D ) D d=1 p f (d) i d = B b=1 λ b ∀d k d h 2 (X b k1,...,k D ) D d=1 T f (d),b i d k d (44) ⇐⇒ X i1,...,i D = (∇h 1 ) -1l ∇f 1 -1 B b=1 λ b ∀d k d h 2 (X b k1,...,k D ) D d=1 T f (d),b i d k d p f (d) i d<label>(45)</label></formula><p>There is only one vector X i1,...,i D found as (∇h 1 ) -1l ∇f 1 is invertible. Thus, if we suppose that the gradient is a maximum or an inflection point, then there is no minimum in R I f (1) ×...×I f (D) ×F as OTT is continuous. This is in contradiction with the hypothesis that the loss tends to +∞ when x r -→ +∞ for any r ∈ 1, F . Thus the value obtained is a minimum. For the squared euclidean distance, for any x, y ∈ R F , ∇h 1 (x) is the identity matrix, h 2 (y) = 2y and ∇f 1 (x) = 2x.</p><p>All the experiments were conducted on a linux internal cluster using only CPUs with 16GB of RAM. As some experiments depend on randomness, the seeds to reproduce them were fixed using the numpy.seed function. The exact seeds that were randomly fixed are available in the provided code to reproduce the experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Impact of the order of the tensors on the gradient approximation</head><p>In this section, we will analyze the impact of the order of the tensors X and Y on the gradient approximation that would lead to a discussion on the choice of the number of samples M . Note that experiments for the choice of M were already conducted by <ref type="bibr">Kerdoncuff, Emonet, and Sebban (2021)</ref>, for the Gromov Wasserstein problem. They concluded that choosing M equal to 1 was sufficient to have a good approximation of the gradient, we will see that this is no longer true for D &gt; 2.</p><p>In these experiments, we generate a pair of graphs, a pair of 3-regular hypergraphs <ref type="bibr">(Berge 1984</ref>) and a pair of 4-regular hypergraphs. To do so, we sample 100 points in a 10 dimensional space using 3 Gaussians with random mean and variance, this correspond to the nodes of the hypergraphs. Then the edges of the hypergraphs are created by the D-tuple formed by the D nearest neighbors of each point and we symmetrize the hypergraphs. We give an example for D = 3 and the tensor X that represents the hypergraph. For a point x i , we find the 3 nearest neighbors, noted (x i , x j , x l ) and add a value in the tensor X at the position (i, j, l), (i, l, j), (j, i, l), (j, l, i), (l, i, j) and (l, j, i) to ensure the symmetry of the tensor X.</p><p>The OTT algorithm, with random marginals, is computed to the iteration S. Then we look at the relative difference in Frobenius norm between the estimated gradient ∇E for different values of M and the real one ∇E:</p><formula xml:id="formula_58">∇E-∇E F ∇E F</formula><p>. As most of the time the real gradient cannot be computed in a reasonable time, we use the value found with 10 6 samples as a reference. Figure <ref type="figure" target="#fig_3">2</ref> shows a clear correlation (with log-log axis) between the number of samples and the estimate of the gradient, with the exception of the very noisy first points. Some points are even omitted if they correspond to a gradient of 0. The latter may happen when M is too small and for high order tensors as the tensors X and Y become sparser. Thus, the 1 sample approximation proposed by <ref type="bibr">Kerdoncuff, Emonet, and Sebban (2021)</ref> is no longer possible for D &gt; 2, as it often corresponds to a null gradient.</p><p>From a time complexity point of view, the main bottleneck in terms of efficiency (excluding the gradient approximation) is the Sinkhorn algorithm with a complexity O(SN 2 ), independent of D, where S is the number of Sinkhorn iterations which is usually of the order of 100 or 1000. Thus, since the gradient approximation step has a complexity of O(M N 2 ), M can be chosen of the same order as S to avoid any increase in terms of time complexity. As shown in Figure <ref type="figure" target="#fig_3">2</ref>, this is acceptable for D = 3 (as in the experiments bellow) but might not be enough for D = 4. This difference between all three orders is expected as there is a sum of 10 4 matrices in the gradient for D = 2, 10 8 for D = 3 and 10 12 for D = 4. On both figures, to have the same relative error of 0.5 on the gradient, it requires approximately 10 5 samples for D = 4, 5 × 10 3 samples for D = 3 and 2 × 10 2 for D = 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Domain Adaptation (DA)</head><p>Hyperparameters This section will describe the key hyperparameters used by all the methods for reproducibility purpose. Except for the Kullback-Leibler regularization and class regularization η, we keep the default parameters provided with the code of each algorithm. Note that the code is also attached to this supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S-GWL</head><p>• Loss: squared euclidean distance • Outer iteration: 4000 </p><formula xml:id="formula_59">•</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figures for each parameter and datasets</head><p>In this section we present the Figures evoked in Section 6.1 for each dataset instead of the average. We also display in Figures 4 to 10 the values of the various computed distances rescaled between 0 and 1 for every transport method to more clearly demonstrate the correlation between the distance and the target accuracy. This supports the choice of using the distance to select the hyperparameters on this Domain Adaptation task. In addition we also plot similar figures related to the class regularization η and quickly analyze the performance for an increasing number of samples for the gradient estimation.</p><p>Increasing number of samples Figure <ref type="figure" target="#fig_1">3</ref> shows that having a higher number of samples M for the estimation of the gradient improves the performances of OTT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Kullback-Leibler and classes regularization</head><p>We show on Figures 4 to 10 the impact of the Kullback-Leibler and classes regularizations.</p><p>Increasing the supervision We show in Figures 11 to 17 the impact of an increase in terms of supervision with respect to the pairwise information that the same users rated old and new movies and the number of labels available for the target.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Comparison based clustering using OTT barycenter</head><p>Theoretical link between t-STE and OTT In this section, we show that t-STE (Van Der Maaten and Weinberger 2012) is just a OTT 111 barycenter with a fixed transport plan. While in the closed form presented in Theorem 2 we minimized directly          the value of the tensor X, we could also minimize another related variable, such as points in a vector space x which generate X(x). This is the principle of many triplet embedding methods which are looking for points x in a vector space that respect as closely as possible the triplets provided in X 1 . Theorem 3 shows that a widely used method for triplet embedding, t-STE (Van Der Maaten and Weinberger 2012), is a particular case of OTT barycenter.</p><p>Theorem 3. We suppose that T is a list of triplets which can also be represented with a cubic 3-order tensor X 1 of size (I 1 , I 1 , I 1 ) with, at the position i 1 , i 2 , i 3 the number of occurrences of the triplet (i 1 , i 2 , i 3 ) in T . Let x = (x i ) i∈ 1,I1 be I 1 points in a vector space R q . We can then set the tensor X to the t-STE or the STE formula as given in (Van Der Maaten and Weinberger 2012), for STE:</p><formula xml:id="formula_60">X i1,i2,i3 = exp(-xi 1 -xi 2 2 ) exp(-xi 1 -xi 2 2 )+exp(-xi 1 -xi<label>3</label></formula><p>2 ) . If L is the cross-entropy and f is the constant function (all the 3 transport plans are similar), then STE is a particular case of OTT with the identity matrix Id (divided by I 1 ) of size I 1 as the transport plan, L X 1 i1,i2,i3 , X k1,k2,k3 (x)</p><formula xml:id="formula_61">Id i1,k1 I 1 Id i2,k2 I 1 Id i3,k3 I 1 . (<label>52</label></formula><formula xml:id="formula_62">)</formula><p>Note that, any permutation are equivalent to the identity as x i and x j can be exchanged. In addition, the identity matrix is not necessarily the optimal value, thus the OTT barycenter might lead to a better optimum, but loses the pairwise connection that can be useful for interpretation.</p><p>Interestingly, this new interpretation of t-STE in the light of Theorem 3 gives a good theoretical justification of the choice of the log function in t-STE which is nothing more than a cross entropy between 3D-tensor. One specificity of OTT barycenter, compared to t-STE, is that the size of the barycenter X is not necessarily the size of X 1 . Thus, it is notably possible to use a small size for X which will aggregate similar points from X 1 . This idea has been used advantageously in the experiment to obtain a direct clustering of a triplet dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Hyperparameters used</head><p>In this section we details the hyperparameters used in the experiment.  <ref type="table" target="#tab_3">1</ref> which is similar to the one provided in the paper, without averaging the different classes. This table shows a similar behaviour, OTT is very often better than AddS3 s while being comparable to t-STE s on most datasets.</p><p>Additionally, we present several experiments in the balanced case in Table <ref type="table" target="#tab_1">2</ref> where the proportion of classes are similar. In this case, OTT is slightly worse than the two other baselines while still being competitive. This is not surprising since OTT, contrary to AddS3 and t-STE, was not specifically designed to handle triplet comparisons. Instead, it is a general purpose Optimal Transport formulation between tensors of potentially high order that can be used to solve multiple kind of tasks. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: (Left) Transport plan T 1 between 400 images (only digits 0 and 1) of MNIST and USPS datasets; (Right) (top left) An example from MNIST and (bottom right) an example from USPS with a 90°right rotation; (top right) the OT plan T 2 between the rows of MNIST and USPS; (bottom left)the OT plan T 3 between the columns of MNIST and USPS; the arrows explain how to match the pixels between the two datasets using T 2 and T 3 obtained with OTT.</figDesc><graphic coords="3,185.00,63.27,59.34,59.34" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Target accuracy averaged over all the datasets. The shadow area represents the standard deviation for the stochastic methods. When relevant, the black symbols correspond to the parameter values achieving, for each method, the lowest distances between the datasets on average. (Left) Target accuracy for various values of . (Middle) Target accuracy for an increasing target supervision. (Right) Target accuracy for an increasing number of similar known users who rated both old and new movies.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: (Left) MNIST image with a color associated with each pixel. (Middle and Right) USPS image colored by the transportation of the left image by the transport plans found with Co-OT and OTT 123 respectively. Few rows and columns are deleted in this representation as they have no mass.</figDesc><graphic coords="12,74.20,119.56,94.78,94.78" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Figure2: Relative error of the gradient approximation for an increasing number of samples M . The gradient was approximated after 100 (left) and 1000 (right) iterations of the algorithm OTT. The mean and standard deviation over 10 runs are displayed. The points corresponding to a gradient of 0 are omitted. This may happen with a small number of samples M and for high order tensors as the tensors X and Y become sparser.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :Figure 4 :Figure 5 :Figure 6 :</head><label>3456</label><figDesc>Figure3: Average target accuracy and distance over all the dataset for an increasing number of samples for the estimation of the gradient.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>10 - 5 Figure 7 :Figure 8 :</head><label>578</label><figDesc>Figure 7: (Top row) Target accuracy for an increasing Kullback-Leibler and classes regularization values with the dataset composed of the two classes Thriller/Crime/Drama and War/Western. (Bottom row) Distances of the different methods for an increasing Kullback-Leibler and classes regularization values with the dataset composed of the two classes Thriller/Crime/Drama and War/Western. The distances have been re-scaled between 0 and 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>10 - 5 Figure 9 :Figure 10</head><label>5910</label><figDesc>Figure 9: (Top row) Target accuracy for an increasing Kullback-Leibler and classes regularization values with the dataset composed of the two classes Fantasy/Sci-Fi and War/Western. (Bottom row) Distances of the different methods for an increasing Kullback-Leibler and classes regularization values with the dataset composed of the two classes Fantasy/Sci-Fi and War/Western. The distances have been re-scaled between 0 and 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 11</head><label>11</label><figDesc>Figure 11: (Top row) Average target accuracy for an increasing number of users known and label available in the target domain. (Bottom row) Distances of the different methods for an increasing number of users known and label available in the target domain. The distances have been re-scaled between 0 and 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 12</head><label>12</label><figDesc>Figure 12: (Top row) Target accuracy for an increasing increasing number of users known and label available in the target domain with the dataset composed of the two classes Thriller/Crime/Drama and Fantasy/Sci-Fi. (Bottom row) Distances of the different methods for an increasing increasing number of users known and label available in the target domain with the dataset composed of the two classes Thriller/Crime/Drama and Fantasy/Sci-Fi. The distances have been re-scaled between 0 and 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>Figure 13: (Top row) Target accuracy for an increasing increasing number of users known and label available in the target domain with the dataset composed of the two classes Children's/Animation and Fantasy/Sci-Fi. (Bottom row) Distances of the different methods for an increasing increasing number of users known and label available in the target domain with the dataset composed of the two classes Children's/Animation and Fantasy/Sci-Fi. The distances have been re-scaled between 0 and 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 14</head><label>14</label><figDesc>Figure 14: (Top row) Target accuracy for an increasing increasing number of users known and label available in the target domain with the dataset composed of the two classes Thriller/Crime/Drama and War/Western. (Bottom row) Distances of the different methods for an increasing increasing number of users known and label available in the target domain with the dataset composed of the two classes Thriller/Crime/Drama and War/Western. The distances have been re-scaled between 0 and 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head></head><label></label><figDesc>Figure 15: (Top row) Target accuracy for an increasing increasing number of users known and label available in the target domain with the dataset composed of the two classes Fantasy/Sci-Fi and Children's/Animation. (Bottom row) Distances of the different methods for an increasing increasing number of users known and label available in the target domain with the dataset composed of the two classes Fantasy/Sci-Fi and Children's/Animation. The distances have been re-scaled between 0 and 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 16</head><label>16</label><figDesc>Figure 16: (Top row) Target accuracy for an increasing increasing number of users known and label available in the target domain with the dataset composed of the two classes Fantasy/Sci-Fi and War/Western. (Bottom row) Distances of the different methods for an increasing increasing number of users known and label available in the target domain with the dataset composed of the two classes Fantasy/Sci-Fi and War/Western. The distances have been re-scaled between 0 and 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head></head><label></label><figDesc>Figure 17: (Top row) Target accuracy for an increasing increasing number of users known and label available in the target domain with the dataset composed of the two classes Children's/Animation and War/Western. (Bottom row) Distances of the different methods for an increasing increasing number of users known and label available in the target domain with the dataset composed of the two classes Children's/Animation and War/Western. The distances have been re-scaled between 0 and 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head></head><label></label><figDesc>max x∈R I 1 ×q (i1,i2,i3)∈T log(X i1i2i3 (x)) = I 3 1 min x∈R I 1 ×q 1 b=1 E X(x), X 1 , Id I 1 . (46)Proof. We start from the STE formulation and reformulate the problem,max x∈R I 1 ×q (i1,i2,i3)∈T log(X i1,i2,i3 (x)) ,i3 log(X k1,k2,k3 (x))Id i1,k1 Id i2,k2 Id i3,k3 ,i3 , X k1,k2,k3 (x) Id i1,k1 Id i2,k2 Id i3,k3</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>OTT•</head><label></label><figDesc>Loss type: squared euclidean distance • Number of samples M : 100 • Number of iterations S: 500 • Kullback-Leibler regularization: 0.1 AddS3 • Number of iterations of the k-means: 300 t-STE • Degrees of freedom in student T kernel: 0 • Number of iterations of k-means: 300 • Maximum number of iterations: 1000 • L2 regularization constant: 0 Detailed tables for the experiment We display the ARI for comparison based clustering in Table</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>ARI (in percentage) for unbalanced comparisonbased clustering on MNIST. Each line corresponds to the average over 10 combinations of classes, each run 10 times.</figDesc><table><row><cell>nb. points per class</cell><cell cols="3">AddS3 AddS3s t-STE</cell><cell>t-STEs OTT</cell></row><row><cell cols="3">200,20,20 43±12 80±17</cell><cell>56±7</cell><cell>91±4</cell><cell>91±4</cell></row><row><cell cols="2">30,3,1 28±9</cell><cell>82±17</cell><cell cols="2">49±14 91±14 89±14</cell></row><row><cell cols="3">30,3,3 37±13 78±23</cell><cell cols="2">52±09 93±19 87±19</cell></row><row><cell cols="2">300,30,10 28±4</cell><cell>83±07</cell><cell cols="2">48±10 89±4</cell><cell>89±04</cell></row><row><cell cols="2">AVG 34±9</cell><cell>81±16</cell><cell cols="2">51±10 91±8</cell><cell>89±10</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1 :</head><label>1</label><figDesc>ARI for unbalanced comparison-based clustering tasks on MNIST dataset. Each line corresponds to the average over 10 runs.</figDesc><table><row><cell>nb. examples per class-classes AddS3</cell><cell>AddS3 s</cell><cell>t-STE</cell><cell>t-STE s</cell><cell>OTT</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>ARI for balanced comparison-based clustering tasks on MNIST dataset. Each line corresponds to the average over 10 different combinations of classes, each run 10 times. 88±0.13 0.93±0.11 0.9±0.11 20,20,20 0.88±0.06 0.87±0.07 0.86±0.11 0.91±0.08 0.86±0.08 30,30,30 0.91±0.05 0.9±0.05 0.87±0.1 0.92±0.07 0.91±0.07 40,40,40 0.92±0.06 0.91±0.05 0.86±0.1 0.92±0.06 0.9±0.06 50,50,50 0.92±0.06 0.92±0.06 0.85±0.11 0.92±0.06 0.9±0.06 60,60,60 0.92±0.05 0.91±0.04 0.86±0.09 0.92±0.05 0.9±0.05 70,70,70 0.92±0.05 0.92±0.05 0.85±0.07 0.92±0.05 0.88±0.05 80,80,80 0.92±0.06 0.91±0.03 0.85±0.1 0.92±0.06 0.86±0.06 90,90,90 0.92±0.11 0.92±0.11 0.87±0.09 0.92±0.11 0.8±0.11 100,100,100 0.92±0.13 0.92±0.13 0.85±0.09 0.92±0.13 0.73±0.13 AVG 0.91±0.04 0.91±0.05 0.86±0.1 0.92±0.05 0.86±0.08</figDesc><table><row><cell>nb. examples per class AddS3</cell><cell>AddS3 s</cell><cell>t-STE</cell><cell>t-STE s</cell><cell>OTT</cell></row><row><cell>10,10,10 0.9±0.08</cell><cell>0.9±0.1</cell><cell>0.</cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>The code to reproduce all the experiments is available online: https://github.com/Hv0nnus/Optimal Tensor Transport</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>This paper is part of the <rs type="projectName">TADALoT Project</rs> funded by the <rs type="funder">region Auvergne-Rhône-Alpes (France)</rs> with the <rs type="funder">Pack Ambition Recherche</rs> (<rs type="grantNumber">2017</rs>, <rs type="grantNumber">17 011047 01</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_WvFaqqR">
					<orgName type="project" subtype="full">TADALoT Project</orgName>
				</org>
				<org type="funding" xml:id="_7wGmRqX">
					<idno type="grant-number">2017</idno>
				</org>
				<org type="funding" xml:id="_ue9Hd9R">
					<idno type="grant-number">17 011047 01</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Gromov-Wasserstein Alignment of Word Embedding Spaces</title>
		<author>
			<persName><forename type="first">D</forename><surname>Alvarez-Melis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Jaakkola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Wasserstein generative adversarial networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">70</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Mirror descent and nonlinear projected subgradient methods for convex optimization</title>
		<author>
			<persName><forename type="first">A</forename><surname>Beck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Teboulle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Operations Research Letters</title>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Analysis of representations for domain adaptation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ben-David</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Crammer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Hypergraphs: combinatorics of finite sets</title>
		<author>
			<persName><forename type="first">C</forename><surname>Berge</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1984">1984</date>
			<publisher>Elsevier</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning Generative Models across Incomparable Spaces</title>
		<author>
			<persName><forename type="first">C</forename><surname>Bunne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Alvarez-Melis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">On a class of multidimensional optimal transportation problems</title>
		<author>
			<persName><forename type="first">G</forename><surname>Carlier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of convex analysis</title>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The Gromov-Wasserstein distance between networks and stable network invariants</title>
		<author>
			<persName><forename type="first">S</forename><surname>Chowdhury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Mémoli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information and Inference: A Journal of the IMA</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Support-vector networks</title>
		<author>
			<persName><forename type="first">C</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vapnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Joint distribution optimal transportation for domain adaptation</title>
		<author>
			<persName><forename type="first">N</forename><surname>Courty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Flamary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Habrard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rakotomamonjy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Optimal transport for domain adaptation</title>
		<author>
			<persName><forename type="first">N</forename><surname>Courty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Flamary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tuia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rakotomamonjy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1853" to="1865" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Sinkhorn distances: Lightspeed computation of optimal transport</title>
		<author>
			<persName><forename type="first">M</forename><surname>Cuturi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">DeepJDOT: Deep Joint Distribution Optimal Transport for Unsupervised Domain Adaptation</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">B</forename><surname>Damodaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kellenberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Flamary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tuia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Courty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A multilinear singular value decomposition</title>
		<author>
			<persName><forename type="first">L</forename><surname>De Lathauwer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>De Moor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Vandewalle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM journal on Matrix Analysis and Applications</title>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A tensor-based algorithm for high-order graph matching</title>
		<author>
			<persName><forename type="first">O</forename><surname>Duchenne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I.-S</forename><surname>Kweon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Adaptive Hierarchical Clustering Using Ordinal Queries</title>
		<author>
			<persName><forename type="first">E</forename><surname>Emamjomeh-Zadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kempe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Symposium on Discrete Algorithms</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Friedland</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.00945</idno>
		<title level="m">Tensor optimal transport, distance between sets of measures and tensor scaling</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<author>
			<persName><forename type="first">J</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hastie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Tibshirani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The elements of statistical learning</title>
		<title level="s">Springer series in statistics</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Foundations of Comparison-Based Hierarchical Clustering</title>
		<author>
			<persName><forename type="first">D</forename><surname>Ghoshdastidar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Perrot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Luxburg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">I</forename><surname>Gulrajani</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Improved Training of Wasserstein GANs</title>
		<author>
			<persName><forename type="first">F</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Fastest rates for stochastic mirror descent methods</title>
		<author>
			<persName><forename type="first">F</forename><surname>Hanzely</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Richtárik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Optimization and Applications</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">The movielens datasets: History and context</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">M</forename><surname>Harper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Konstan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Acm transactions on</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note>interactive intelligent systems</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Comparing partitions</title>
		<author>
			<persName><forename type="first">L</forename><surname>Hubert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Arabie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of classification</title>
		<imprint>
			<date type="published" when="1985">1985</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Sampled Gromov Wasserstein</title>
		<author>
			<persName><forename type="first">T</forename><surname>Kerdoncuff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Emonet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sebban</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Sparse tensor discriminant analysis</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on Image processing</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE</title>
		<meeting>the IEEE</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Tensor distance based multilinear locality-preserved maximum information embedding</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">C</forename><surname>Chan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on neural networks</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Least squares quantization in PCM</title>
		<author>
			<persName><forename type="first">S</forename><surname>Lloyd</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on information theory</title>
		<imprint>
			<date type="published" when="1982">1982</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">On the use of Gromov-Hausdorff Distances for Shape Comparison</title>
		<author>
			<persName><forename type="first">F</forename><surname>Memoli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eurographics Symposium on Point-Based Graphics</title>
		<imprint>
			<publisher>The Eurographics Association</publisher>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Gromov-Wasserstein distances and the metric approach to object matching</title>
		<author>
			<persName><forename type="first">F</forename><surname>Mémoli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Foundations of computational mathematics</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Multi-marginal Monge-Kantorovich transport problems: A characterization of solutions</title>
		<author>
			<persName><forename type="first">A</forename><surname>Moameni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comptes Rendus Mathematique</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Metrics for matrix-valued measures via test functions</title>
		<author>
			<persName><forename type="first">L</forename><surname>Ning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">T</forename><surname>Georgiou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">53rd IEEE Conference on Decision and Control</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="2642" to="2647" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Multi-marginal optimal transport: theory and applications</title>
		<author>
			<persName><forename type="first">B</forename><surname>Pass</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ESAIM: Mathematical Modelling and Numerical Analysis</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Near-optimal comparison based clustering</title>
		<author>
			<persName><forename type="first">M</forename><surname>Perrot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">M</forename><surname>Esser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ghoshdastidar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.03918</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Quantum optimal transport for tensor field processing</title>
		<author>
			<persName><forename type="first">G</forename><surname>Peyré</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Chizat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F.-X</forename><surname>Vialard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Solomon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.08731</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Gromovwasserstein averaging of kernel and distance matrices</title>
		<author>
			<persName><forename type="first">G</forename><surname>Peyré</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cuturi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Solomon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Computational optimal transport</title>
		<author>
			<persName><forename type="first">G</forename><surname>Peyré</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cuturi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foundations and Trends® in Machine Learning</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">I</forename><surname>Redko</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Optimal transport for multi-source domain adaptation under target shift</title>
		<author>
			<persName><forename type="first">N</forename><surname>Courty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Flamary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tuia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 22nd International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">I</forename><surname>Redko</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Advances in domain adaptation theory</title>
		<author>
			<persName><forename type="first">E</forename><surname>Morvant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Habrard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sebban</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bennani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019b</date>
		</imprint>
	</monogr>
	<note type="report_type">Elsevier</note>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">I</forename><surname>Redko</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">CO-Optimal Transport</title>
		<author>
			<persName><forename type="first">T</forename><surname>Vayer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Flamary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Courty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Wasserstein distance guided representation learning for domain adaptation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Second AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">The analysis of proximities: Multidimensional scaling with an unknown distance function</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">N</forename><surname>Shepard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">I. Psychometrika</title>
		<imprint>
			<date type="published" when="1962">1962</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Absolute identification by relative judgment</title>
		<author>
			<persName><forename type="first">N</forename><surname>Stewart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">D A</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Chater</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological review</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Ukkonen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.08459</idno>
		<title level="m">Crowdsourced correlation clustering with relative distance comparisons</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Stochastic triplet embedding</title>
		<author>
			<persName><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Workshop on Machine Learning for Signal Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012">2012. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Fused Gromov-Wasserstein distance for structured objects</title>
		<author>
			<persName><forename type="first">T</forename><surname>Vayer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Chapel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Flamary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Tavenard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Courty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Algorithms</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Sliced Gromov-Wasserstein</title>
		<author>
			<persName><forename type="first">T</forename><surname>Vayer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Flamary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Tavenard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Chapel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Courty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Interactive bayesian hierarchical clustering</title>
		<author>
			<persName><forename type="first">S</forename><surname>Vikram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Dasgupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Optimal transport: old and new</title>
		<author>
			<persName><forename type="first">C</forename><surname>Villani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
			<publisher>Springer Science &amp; Business Media</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">A fast proximal point method for computing exact wasserstein distance</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Uncertainty in Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Scalable gromov-Wasserstein learning for graph partitioning and matching</title>
		<author>
			<persName><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carin</forename></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Gromov-Wasserstein Learning for Graph Matching and Node Embedding</title>
		<author>
			<persName><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">C</forename><surname>Duke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Semi-Supervised Optimal Transport for Heterogeneous Domain Adaptation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Multidimensional scaling: History, theory, and applications</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">W</forename><surname>Young</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Probabilistic graph and hypergraph matching</title>
		<author>
			<persName><forename type="first">R</forename><surname>Zass</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Shashua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008">2008. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">On the convergence rate of stochastic mirror descent for nonsmooth nonconvex optimization</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.04781</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Stochastic mirror descent in variationally coherent optimization problems</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mertikopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Bambos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Boyd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">W</forename><surname>Glynn</surname></persName>
		</author>
		<idno>12 0.93±0.12 30</idno>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">200</biblScope>
			<biblScope unit="issue">20</biblScope>
			<biblScope unit="page" from="35" to="35" />
			<date type="published" when="0320">2017. 20,20-1,3,4 0.35±0.02 0. 20,20-1,9,4 0.35±0.02 0.92±0. 20,20-2,9,8 0.32±0.03 0. 20,20-3,4,0 0.72±0.32 0.52±0. 20,20-3,4,9 0.38±0.06 0. 20,20-5,7,0 0.57±0.34 0.76±0.35. 20,20-6,4,8 0.35±0.03 0. 20,20-6,8,5 0.36±0.03 0. 20,20-7,1,9 0.53±0.35 0. 05 30,3,1-0,1,2 0.32</date>
		</imprint>
	</monogr>
	<note>.8±0.05 0.72±0.21 0.82±0.06 0.8±0.06 200. .83±0.18 0.39±0.16 0.9±0.05 0.9±0.04 200. .89±0.2 0.33±0.01 0.96±0.02 0.97±0.02 200. .81±0.24 0.4±0.18 0.93±0.04 0.94±0.03 200. .69±0.22 0.86±0.05 0.8±0.06 0.8±0.. ±0.08 0.8±0.33 0.35±0.17 0.93±0.</note>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Mirror descent and nonlinear projected subgradient methods for convex optimization</title>
		<author>
			<persName><forename type="first">References</forename><surname>Beck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Teboulle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Operations Research Letters</title>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">Hypergraphs: combinatorics of finite sets</title>
		<author>
			<persName><forename type="first">C</forename><surname>Berge</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1984">1984</date>
			<publisher>Elsevier</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">The Gromov-Wasserstein distance between networks and stable network invariants</title>
		<author>
			<persName><forename type="first">S</forename><surname>Chowdhury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Mémoli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information and Inference: A Journal of the IMA</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Optimal transport for domain adaptation</title>
		<author>
			<persName><forename type="first">N</forename><surname>Courty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Flamary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tuia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rakotomamonjy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1853" to="1865" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Sampled Gromov Wasserstein</title>
		<author>
			<persName><forename type="first">T</forename><surname>Kerdoncuff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Emonet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sebban</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Gromov-wasserstein averaging of kernel and distance matrices</title>
		<author>
			<persName><forename type="first">G</forename><surname>Peyré</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cuturi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Solomon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">I</forename><surname>Redko</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">CO-Optimal Transport</title>
		<author>
			<persName><forename type="first">T</forename><surname>Vayer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Flamary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Courty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Stochastic triplet embedding</title>
		<author>
			<persName><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Workshop on Machine Learning for Signal Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012">2012. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
		<title level="m" type="main">Optimal transport: old and new</title>
		<author>
			<persName><forename type="first">C</forename><surname>Villani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
			<publisher>Springer Science &amp; Business Media</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Gromov-Wasserstein Learning for Graph Matching and Node Embedding</title>
		<author>
			<persName><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">C</forename><surname>Duke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
