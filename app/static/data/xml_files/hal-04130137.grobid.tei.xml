<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="fr">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Globale des Modèles de TAL par la Génération Coopérative</title>
				<funder ref="#_TcXPhwV">
					<orgName type="full">Agence Nationale de la Recherche</orgName>
					<orgName type="abbreviated">ANR</orgName>
				</funder>
				<funder ref="#_bubH82z">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Antoine</forename><surname>Chaffin</surname></persName>
							<email>antoine.chaffin@irisa.fr</email>
							<affiliation key="aff1">
								<orgName type="institution">IRISA</orgName>
								<address>
									<addrLine>263 Avenue du Général Leclerc</addrLine>
									<postCode>35000</postCode>
									<settlement>Rennes</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">IMATAG</orgName>
								<address>
									<addrLine>13 Rue Dupont-des-Loges</addrLine>
									<postCode>35000</postCode>
									<settlement>Rennes</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Julien</forename><forename type="middle">"</forename><surname>Delaunay</surname></persName>
							<email>julien.delaunay@inria.fr</email>
							<affiliation key="aff3">
								<orgName type="institution">Inria</orgName>
								<address>
									<addrLine>263 Avenue du Général Leclerc</addrLine>
									<postCode>35000</postCode>
									<settlement>Rennes</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><roleName>Tell Me What'</roleName><surname>Honey</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Wrong&quot;</orgName>
								<orgName type="department" key="dep2">Explicabilité Globale des Modèles de TAL par la</orgName>
								<orgName type="laboratory" key="lab1">Génération Coopérative. CORIA TALN RJCRI RECITAL 2023</orgName>
								<orgName type="laboratory" key="lab2">Conférence en Recherche d&apos;Information et Applications 16e Rencontres Jeunes Chercheurs en RI 30e Conférence sur le Traitement Automatique des Langues Naturelles 25e Rencontre des Étudiants Chercheurs en Informatique pour le Traitement Automatique des Langues</orgName>
								<address>
									<postCode>2023</postCode>
									<settlement>Jun, Paris</settlement>
									<country>France. pp</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Globale des Modèles de TAL par la Génération Coopérative</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">44FA960E78D7F830E5E88DC6D0AA83B3</idno>
					<note type="submission">Submitted on 2 Oct 2023</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-04-12T14:46+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>explicabilité</term>
					<term>génération coopérative</term>
					<term>traitement automatique des langues explainability</term>
					<term>cooperative generation</term>
					<term>natural language processing</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>à la diffusion de documents scientifiques de niveau recherche, publiés ou non, émanant des établissements d'enseignement et de recherche français ou étrangers, des laboratoires publics ou privés.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="fr">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>L'émergence des modèles basés sur l'apprentissage automatique à permis leur adoption dans des domaines variés, allant de la simple recommandation à des secteurs critiques tels que la santé <ref type="bibr" target="#b8">(Buch et al., 2018;</ref><ref type="bibr" target="#b12">Esteva et al., 2017;</ref><ref type="bibr" target="#b16">Karatza et al., 2021)</ref> et le droit <ref type="bibr" target="#b2">(Araszkiewicz et al., 2022;</ref><ref type="bibr" target="#b37">Tagarelli &amp; Simeri, 2022)</ref>. Le besoin grandissant de précision induit une augmentation de la complexité de ces modèles, accentuant leur dénomination de boite noire. Ce manque de transparence limite (et empêche) leur déploiement dans différents domaines, en raison par exemple de l'augmentation significative de modèles souffrant de biais. Entre autres, certains chatbots ont été déployés bien que comportant des biais liés aux minorités religieuses <ref type="bibr" target="#b0">(Abid et al., 2021)</ref> ou de genre (Lucy &amp; <ref type="bibr" target="#b19">Bamman, 2021)</ref> et expliquer leur fonctionnement reste un problème ouvert.</p><p>Parmi les méthodes proposées pour répondre à ces problèmes, celles qui sont modèle-agnostiques sont préférées car applicables à tout type de modèle d'apprentissage automatique. Parmi ces dernières, les explications locales ont obtenu un fort succès car elles fournissent un bon compromis entre précision et utilité des explications. Les explications locales sont générées en perturbant une entrée afin de construire un voisinage autour de cette entrée et en étudiant comment le modèle réagit à ces petites différences. Cela permet de mettre en évidence les caractéristiques importantes pour le modèle et de fournir des éléments d'explication sur la décision du modèle pour cette entrée (e.g., les mots les plus importants de chaque classe). Selon une étude récente sur les tendances dans le domaine de l'explicabilité <ref type="bibr" target="#b15">(Jacovi, 2023)</ref>, les méthodes d'explication locale agnostique au modèle, telles que LIME <ref type="bibr" target="#b27">(Ribeiro et al., 2016)</ref> sont les plus utilisées.</p><p>Cependant, l'explication d'un modèle à partir d'un exemple précis présente trois défauts. Premièrement, il faut évidemment disposer d'entrées à expliquer ; ce qui peut être impossible pour des raisons de confidentialité ou de respect de la vie privée par exemple. Deuxièmement, choisir des entrées qui sont représentatives du modèle et/ou des données sur lesquelles le modèle sera utilisé est une tâche complexe. Troisièmement, cela donne une explication de la décision pour cette entrée et pour cette entrée uniquement. Ainsi, cela ne fournit que des informations très locales sur le comportement du modèle, qui ne représentent qu'une petite partie du domaine d'entrée du modèle. Pour cette raison, certaines méthodes d'explications ont proposé d'agréger plusieurs explications locales afin de fournir une explication plus globale. Néanmoins, ces explications restent fortement liées aux données d'entrée et ne fournissent des informations que sur le voisinage de ces échantillons. Ces méthodes nécessitent donc que les différentes données d'entrée couvrent la partie de l'espace la plus large possible. Avec l'objectif de supprimer cette dépendance aux données d'entrée et de générer une explication globale du modèle, nous proposons Therapy, une méthode qui utilise la génération coopérative <ref type="bibr" target="#b14">(Holtzman et al., 2018;</ref><ref type="bibr" target="#b32">Scialom et al., 2020;</ref><ref type="bibr" target="#b10">Chen et al., 2020;</ref><ref type="bibr" target="#b4">Bakhtin et al., 2021;</ref><ref type="bibr" target="#b9">Chaffin et al., 2022)</ref> pour générer des textes suivant la distribution d'un classifieur. La distribution des textes générés permet ensuite d'étudier les caractéristiques importantes du modèle, fournissant ainsi une explication globale agnostique au modèle. Dans ce papier, nous présentons d'abord les travaux connexes dans la Section 2, la Section 3 introduit ensuite des notions sur la génération de textes et plus particulièrement la génération coopérative. La Section 4 détaille quant à elle le fonctionnement de la méthode tandis que la Section 5 présente les expériences réalisées pour comparer les performances de Therapy avec les méthodes d'explication usuelles.</p><p>Générer une explication pour des données textuelles est une tâche ardue qui nécessite de prendre en compte à la fois la sémantique du texte mais également le domaine de la tâche (e.g. analyse de sentiment, détection de spam). De plus, il est fréquent de devoir évaluer un modèle déjà déployé pour des raisons d'équité ou de détection de biais par exemple mais que les données ne soient plus accessibles pour des raisons de sécurité ou de confidentialité. Afin de résoudre ce problème, les chercheurs se sont concentrés sur les méthodes d'explications post-hoc <ref type="bibr" target="#b15">(Jacovi, 2023)</ref>. Suivant la catégorisation <ref type="bibr">de Bodria et al. (2021)</ref>, nous différencions les explications sous forme d'exemples de celles par attribution de caractéristiques.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Explications sous forme d'exemples</head><p>Les méthodes d'explications sous forme d'exemples tirent leur racine des sciences sociales <ref type="bibr" target="#b22">(Miller, 2019)</ref> et montrent des contrefactuels qui indiquent le changement minimum requis pour modifier une prédiction, ou des prototypes, des exemples représentatif d'une classe. Les méthodes contrefactuelles perturbent le document cible jusqu'à trouver le document le plus proche qui soit classé différemment par le modèle complexe. À l'inverse, les méthodes qui génèrent des prototypes sélectionnent les instances qui représentent le plus une classe cible. Parmi les méthodes post-hoc, certaines proposent des codes de contrôle permettant de surveiller la perturbation du texte en entrée, tandis que d'autres entraînent des mécanismes complexes pour générer des phrases réalistes en perturbant une instance dans un espace latent. Polyjuice <ref type="bibr" target="#b39">(Wu et al., 2021)</ref> et GYC <ref type="bibr" target="#b21">(Madaan et al., 2021)</ref> feront partie de la première catégorie et proposent des codes de contrôle allant du changement de sentiment ou de temps jusqu'à l'ajout ou le retrait de mots. xSPELLS (S. <ref type="bibr">Punla et al., 2022)</ref> et CounterfactualGAN <ref type="bibr" target="#b28">(Robeer et al., 2021)</ref>, sont des méthodes qui entraînent respectivement un auto-encodeur variationnel et un réseau adversarial génératif pour convertir les textes en entrée dans un espace latent. Des modifications y sont ensuite réalisées afin de retourner des contrefactuels réalistes proches de l'exemple original.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Explications par attribution</head><p>Parmi les méthodes post-hoc, les explications par attribution associent un poids aux termes en entrée afin d'indiquer leur impact positif ou négatif sur la prédiction finale. Les méthodes telles que SHAP <ref type="bibr" target="#b20">(Lundberg &amp; Lee, 2017)</ref>, LIME <ref type="bibr" target="#b27">(Ribeiro et al., 2016)</ref> et ses variantes <ref type="bibr" target="#b13">(Gaudel et al., 2022;</ref><ref type="bibr" target="#b35">Shankaranarayana &amp; Runje, 2019;</ref><ref type="bibr" target="#b40">Zafar &amp; Khan, 2019;</ref><ref type="bibr" target="#b38">Visani et al., 2020;</ref><ref type="bibr" target="#b11">ElShawi et al., 2019;</ref><ref type="bibr" target="#b6">Bramhall et al., 2020)</ref> restent les plus utilisées pour générer des explications <ref type="bibr" target="#b15">(Jacovi, 2023)</ref>. Les explications sont dites locales puisque ces méthodes perturbent un document en entrée en modifiant légèrement les valeurs et en observant le comportement du modèle complexe dans cette localité. Pour les données texte, LIME masque aléatoirement les mots du document en entrée afin d'en créer diverses variations et entraîne un modèle linéaire sur ces exemples. Les coefficients du modèle linéaire, associés aux différents mots, sont ensuite retournés et utilisés comme explication. Bien que la majorité des études <ref type="bibr" target="#b3">(Arrieta et al., 2020;</ref><ref type="bibr" target="#b5">Bodria et al., 2021)</ref> fasse une différence entre les explications locales et globales, LIME introduit LIME-SP (pour sélection sous modulaire), une méthode qui génère une explication globale à partir de n explications locales. Ces n explications sont choisies parmi un plus grand ensemble afin de couvrir le plus possible l'espace d'entrée tout en réduisant la redondance.</p><p>3 Génération de textes</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Génération coopérative</head><p>Les modèles de langue génératifs (LM) tels que la famille des GPT <ref type="bibr" target="#b25">(Radford et al., 2018</ref><ref type="bibr" target="#b26">(Radford et al., , 2019;;</ref><ref type="bibr" target="#b7">Brown et al., 2020)</ref> apprennent la distribution de probabilité de séquences de symboles x 1 , x 2 , • • • , x T (souvent appelés tokens) appartenant à des séquences de taille variable T sur un vocabulaire V. La probabilité d'une séquence x (aussi appelée vraisemblance) est définie comme la probabilité jointe de chacun de ses tokens. Cette probabilité peut être factorisée en utilisant la formule des probabilités composées : p(x 1:T ) = T t=1 p(x t | x 1:t-1 ). Le LM est entraîné à produire une distribution de probabilité sur le dictionnaire pour le prochain token sachant ceux en entrée, i.e, p θ (x t | x 1:t-1 ) à un pas de temps donné t. Cela permet d'obtenir un modèle de langue auto-régressif qui génère des séquences itérativement. Le modèle utilise les distributions apprises pour émettre un token x t et l'ajouter au contexte x 1:t-1 , qui sera utilisé pour la prochaine itération. Le processus de génération -ou décodage-démarre généralement en utilisant une petite séquence initiale appelée l'amorce. Les grands modèles de langue apprennent une très bonne approximation de la distribution de leurs données d'apprentissage, ce qui permet de générer des textes plausibles en maximisant la vraisemblance du modèle p(x). Cependant, cette approche offre très peu de contrôle sur le texte généré à l'exception de l'amorce.</p><p>Les approches de génération coopérative <ref type="bibr" target="#b14">(Holtzman et al., 2018;</ref><ref type="bibr" target="#b32">Scialom et al., 2020;</ref><ref type="bibr" target="#b10">Chen et al., 2020;</ref><ref type="bibr" target="#b4">Bakhtin et al., 2021;</ref><ref type="bibr" target="#b9">Chaffin et al., 2022)</ref> permettent de résoudre ce problème en utilisant des modèles discriminatifs pour guider le modèle de langue durant la génération. Elles utilisent l'information du modèle externe pour guider le modèle de langue afin de générer des textes qui possèdent une propriété reconnue par le modèle discriminatif. Dans le cas où le modèle est un classifieur qui apprend à prédire la probabilité D(c | x) qu'une séquence x d'appartenir à la classe c, le but est de générer un texte qui maximise la probabilité d'appartenir à la classe cible. En raison de la taille de l'espace de toutes les séquences possibles (|V| n pour une séquence de longueur n) ; il est infaisable d'évaluer D(c | x) pour toutes les séquences possibles. Ainsi, les méthodes coopératives utilisent la distribution du modèle de langue génératif pour restreindre l'exploration aux séquences plausibles uniquement. De cette manière, la séquence produite maximise p(x) * D(c | x) ∝ p(x | c), résultant en une séquence qui est à la fois bien écrite et qui appartient à la classe cible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Décodage guidé par Monte Carlo Tree Seach</head><p>Parmi ces approches coopératives, celles qui utilisent le Monte Carlo Tree Search (MCTS) pour guider le décodage des modèles de langue ont obtenu de très bons résultats <ref type="bibr" target="#b33">(Scialom et al., 2021a;</ref><ref type="bibr" target="#b9">Chaffin et al., 2022;</ref><ref type="bibr" target="#b18">Leblond et al., 2021;</ref><ref type="bibr" target="#b17">Lamprier et al., 2022)</ref>. Le MCTS est un algorithme itératif qui cherche une solution dans un arbre trop grand pour être parcouru de façon exhaustive. Il est adapté à la génération de texte car l'espace de recherche créé durant le décodage correspond à un arbre : la racine correspond à l'amorce et les enfants d'un noeud correspondent aux séquences construites en augmentant le préfixe de leur parent d'un token supplémentaire. La boucle du MCTS est composée de 4 étapes : sélection, expansion, simulation et rétro-propagation.</p><p>1. Sélection. Une exploration de la racine de l'arbre à une feuille non explorée. Le chemin vers la feuille est défini en sélectionnant, à chaque noeud, l'enfant qui maximise la Polynomial Upper</p><p>Confidence Trees (PUCT) <ref type="bibr" target="#b29">(Rosin, 2011;</ref><ref type="bibr" target="#b36">Silver et al., 2017)</ref> qui est définie, pour un noeud i par : </p><formula xml:id="formula_0">P U CT (i) = s i n i + c puct p(x i | x 1:t-1 ) √ N i 1 + n i (1)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Expériences</head><p>Dans cette section, nous présentons d'abord les détails expérimentaux de l'évaluation de Therapy (Section 5.1). Nous évaluons ensuite Therapy au travers de 3 expériences. La première expérience (Section 5.2), mesure la corrélation de Spearman des explications avec les poids de la boite transparente et étudie l'impact de la quantité de textes générés sur la qualité de l'explication retournée par le modèle linéaire. Nous comparons ensuite la capacité de Therapy à identifier les mots les plus importants pour le classifieur avec celles de LIME et SHAP dans la Section 5.3. Enfin, nous observons si les termes retournées par les différentes approches sont suffisants pour modifier la prédiction du classifieur. Le code de Therapy ainsi que celui des expériences sont disponibles publiquement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Configuration expérimentale</head><p>Explication de boite transparente. Puisqu'il n'y a pas de vérité terrain disponible pouvant être utilisée comme objectif pour les méthodes d'explication évaluées, nous utilisons un modèle boite transparente. Un modèle est dit boite transparente lorsque ses paramètres utilisés pour faire une prédiction sont connus, on dit aussi que le modèle est explicable par conception. Tout au long de nos expérimentations, nous utilisons comme modèle boite transparente une régression logistique implémentée en utilisant sklearn <ref type="bibr" target="#b23">(Pedregosa et al., 2011)</ref>. Les poids de ce modèle représentent le score d'importance associé à chacun des termes du vocabulaire.</p><p>Implémentation de Therapy. Lors de l'évaluation de la méthode proposée, nous utilisons l'implémentation de PPL-MCTS <ref type="bibr" target="#b9">(Chaffin et al., 2022)</ref>  Jeu de données utilisés. Toutes les expériences ont été réalisées sur deux jeux de données différents issus de <ref type="bibr" target="#b42">(Zhang et al., 2015)</ref>. Le premier, amazon_polarity : est un jeu de données de classification binaire composé de commentaires de produits Amazon labelisés comme positifs ou négatifs. Les textes qui le composent sont relativement courts et possèdent des champs lexicaux très caricaturaux. Le second, AG_news est un jeu de classification thématique à 4 classes (world, sport, business et sci/tech). Les textes de ce jeu sont plus longs et plus variés, mais ils possèdent différents indicateurs caractéristiques liés au fait qu'ils sont extraits d'articles de presse en ligne. Des exemples de textes générés par Therapy pour chacun des jeux de données ainsi que les premiers top-mots retournés sont disponibles en Annexe A.</p><p>Méthodes comparées. Nous comparons dans les Section 5.3 et 5.4, les résultats de Therapy avec les deux approches par attribution de caractéristiques les plus répandues : LIME <ref type="bibr" target="#b27">(Ribeiro et al., 2016)</ref> et SHAP <ref type="bibr" target="#b20">(Lundberg &amp; Lee, 2017)</ref> en utilisant les implémentations publiques. La différence principale entre LIME et SHAP est que la première génère un échantillon en perturbant une instance puis entraîne localement un modèle de régression linéaire tandis que la seconde utilise la théorie des jeux pour calculer l'importance de chaque élément. Nous avons utilisé les versions globales de ces méthodes sur 500 textes du jeu de test. Pour SHAP, nous avons décidé de garder les 10000 mots les plus importants pour chacun des jeux de données tandis que pour LIME, nous avons retourné les 500 explications locales avec les 35 mots les plus importants et regroupé les couples mots-valeurs dans un dictionnaire composé de 4592 mots pour amazon_polarity et 5770 pour ag_news.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Corrélation de Spearman</head><p>Une bonne explication de boite transparente est une liste de mots qui contient à la fois ses mots importants (i.e, a une bonne couverture) et les relie à des scores d'importance similaires. Ainsi, nous calculons la corrélation de Spearman entre les top mots de la boite transparente (ceux ayant un poids &gt; 1) et leur score attribué par la méthode d'explication. La corrélation de Spearman à été préférée à celle de Pearson car les scores retournés par LIME et SHAP peuvent être très différents des poids de </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">Influence du nombre de textes générés</head><p>Un des paramètres critiques de la méthode proposée est le nombre de textes à générer. En effet, un nombre élevé de tokens permet une plus grande couverture mais nécessite plus de calculs. Nous reportons la corrélation de Spearman en fonction du nombre de textes générées par classe dans la Figure <ref type="figure" target="#fig_0">1</ref>. Nous pouvons voir que la corrélation augmente rapidement jusqu'à atteindre un plateau, ce qui signifie qu'une petite quantité de textes permet d'obtenir un bon aperçu du comportement du modèle et que la méthode ne nécessite pas énormément de calculs pour fonctionner. Pour la suite des expérimentations, nous fixons le nombre de textes générés par Therapy à 3000 par classe.     </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Précision et rappel</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>Les méthodes d'explicabilité usuelles s'appuient fortement sur des données en entrée, qui ne sont pas forcément disponibles et peuvent ne pas contenir les biais et caractéristiques importantes du modèle. Nous proposons Therapy, une méthode qui emploie la génération de texte coopérative afin de générer des données synthétiques qui suivent la distribution apprise par le modèle étudié. Ainsi, la recherche est dirigée par un modèle de langue génératif pré-entraîné et permet une exploration plus large que celle restreinte au voisinage des données d'entrée. Cela permet de relaxer la plupart des contraintes et aprioris induits par les méthodes qui se basent sur des exemples. Dans le cas extrême où des données très représentatives (comme le jeu de test d'un jeu de données) des caractéristiques importantes du modèle sont disponibles, Therapy obtient des résultats légèrement moins bon que la méthode état-de-l'art SHAP, tout en restant compétitif avec LIME. Cependant, si on considère des cas d'usage plus réalistes dans lesquels les caractéristiques importantes ne sont pas explicitement données en entrée de la méthode d'explication, alors les performances des autres méthodes s'effondrent et Therapy devient la meilleure approche. Ainsi, Therapy est un outil pertinent pour explorer comportement d'un modèle lorsque la collecte des données sur lesquelles le modèle sera utilisée n'est pas possible ou très complexe.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>FIGURE 1 -</head><label>1</label><figDesc>FIGURE 1 -Corrélation de Spearman en fonction du nombre de textes générés par classe pour amazon_polarity et ag_news</figDesc><graphic coords="9,53.86,14.17,317.49,114.26" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>FIGURE 2 -</head><label>2</label><figDesc>FIGURE 2 -Courbes de précision/rappel sur les top mots de la boite transparente (régression logistique) pour les différentes méthodes d'explications</figDesc><graphic coords="10,14.17,14.17,396.86,103.82" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>FIGURE 3 -</head><label>3</label><figDesc>FIGURE 3 -Proportion de textes dont la classification change en fonction du nombre de mots importants utilisés pour effectuer le remplacement.</figDesc><graphic coords="11,14.17,14.17,198.42,124.01" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>avec n i le nombre de simulations jouées après le noeud i, s i le score agrégé du noeud, N i le nombre de simulations jouées après son parent et c puct une constante qui définit le compromis entre exploitation (se focaliser sur des noeuds avec des bons scores) et exploration (explorer des noeuds prometteurs). Rétro-propagation. L'évaluation de la séquence x associée au noeud terminal et l'agrégation de son score à tous les parents jusqu'à la racine. Pour guider la génération vers des textes qui Contrairement aux méthodes traditionnelles qui décodent de gauche à droite et peuvent manquer des séquences qui deviennent meilleures après quelques étapes de génération ou se retrouver bloquer dans des séquences sous optimales, le MCTS brise le décodage myope en définissant le score d'un token sur la base des continuations possibles de la séquence. En plus d'être plug-and-play, c'est à dire que n'importe quel modèle de langue génératif (auto-régressif) peut être guidé durant le décodage par n'importe quel classifieur, cette approche a obtenu des résultats à l'état de l'art dans la tâche de génération contrainte, qui consiste à générer des textes qui maximisent D(c | x) tout en maintenant une qualité d'écriture élevée. plus importants et leurs termes associés comme explication. Comme p(x) est le même pour toutes les classes, l'utilisation des tf-idf sur le corpus entier (i.e, les textes générés pour toutes les classes) filtre les mots qui sont fréquents uniquement à cause de p(x) ou pour plusieurs classes. Les termes résultants sont donc dûs à la partie de la distribution venant du classifieur. L'entraînement d'un modèle de régression logistique sur les tf-idf permet d'extraire les termes les plus importants et d'étudier leur importance relative pour chaque classe. Therapy offre donc le niveau d'explicabilité d'une régression logistique basée sur des n-grams. Enfin, grâce à l'aspect plug-and-play de la génération guidée par MCTS, Therapy est une méthode agnostique au modèle qui peut expliquer tout type de modèle via n'importe quel modèle de langue et retourner une explication du fonctionnement global du classifieur.En substance, la méthode est similaire à l'utilisation de LIME combinée à un modèle de langue qui remplace des tokens masqués lorsque le nombre de tokens remplacés tend vers l'infini mais avec deux avantages. Premièrement, la méthode ne repose pas sur des exemples en entrée mais génère ces exemples à partir de rien en utilisant le modèle de langue auto-régressif. Ceci est particulièrement utile pour les cas où les données ne peuvent pas être partagées pour des raisons de confidentialité<ref type="bibr" target="#b1">(Amin- Nejad et al., 2020)</ref>. De plus, au lieu d'explorer le voisinage de ces exemples (et donc de conditionner les explications au contexte de ces exemples), le domaine d'exploration est défini par le modèle de langue. Ce modèle de langue peut être générique ou spécifique à un domaine sur lequel sera utilisé le classifieur pour s'assurer qu'il fonctionne correctement sur ce type de données précis.</figDesc><table><row><cell>2. Expansion. La création des enfants du noeud sélectionné s'il n'est pas terminal (i.e, correspondant</cell></row><row><cell>au token de fin de séquence).</cell></row><row><cell>3. Simulation (roll-out). Le tirage de tokens additionnels (en utilisant la distribution du modèle de</cell></row><row><cell>langue) jusqu'à un noeud terminal.</cell></row><row><cell>4. 4 Méthode</cell></row></table><note><p><p><p><p><p><p><p><p>appartiennent à une classe donnée, le score d'une séquence x associée à une feuille peut être défini par la probabilité D(c | x) donnée par le classifieur. Différentes méthodes d'agrégation peuvent être utilisées,</p><ref type="bibr" target="#b9">Chaffin et al. (2022)</ref> </p>calcule la moyenne du score actuel et de celui du noeud terminal alors que</p>(Scialom et al., 2021b;<ref type="bibr" target="#b17">Lamprier et al., 2022)</ref> </p>prennent le maximum des deux.</p>Cette boucle est répétée un certain nombre de fois, puis le token à ajouter à l'amorce est choisi grâce à l'arbre construit. Parmi les noeuds enfants de la racine, deux choix sont possibles : soit celui ayant été sélectionné le plus de fois, soit celui ayant un score agrégé le plus élevé. Comme nous souhaitons obtenir des séquences aussi stéréotypiques des classes du modèle discriminatif que possible, nous choisissons celui ayant le score le plus élevé. Ce noeud devient ensuite la nouvelle racine, et le processus est répété jusqu'à produire la séquence finale.</p>Dans cet article, nous présentons Therapy, une méthode d'explication agnostique au modèle qui n'utilise pas de données en entrée. Therapy utilise un modèle de langue guidé par le classifieur à expliquer pour générer des textes représentatifs des classes apprises par le classifieur. Pour ce faire, Therapy extrait les mots importants pour le classifieur en l'utilisant pour guider le modèle de langue via la génération coopérative. Puisque les textes générés coopérativement suivent la distribution p(x)D(c | x), leur distribution peut ensuite être utilisée pour étudier le classifieur D : les mots avec des fréquences élevées sont susceptibles d'être importants pour le classifieur. Ainsi, Therapy entraîne un modèle de régression logistique sur les représentations tf-idf des textes générés et retourne les coefficients les Deuxièmement, la méthode ne génère pas avant de classifier mais utilise le classifieur durant la génération. Ainsi, au lieu de générer des textes "au hasard" et espérer que les caractéristiques importantes apparaissent, la méthode interroge explicitement le modèle pour des caractéristiques discriminantes via la maximisation de D(c | x). Cela rend la méthode plus efficace et réduit la probabilité de générer (a) des mots rares mais qui ne sont pas importants pour le modèle, et (b) des textes "entre-deux" qui possèdent les caractéristiques de plusieurs classes et peuvent être perturbants. Par ailleurs, notre méthode s'appuie directement sur la distribution apprise par le classifieur étudié pour guider la génération, contrairement aux méthodes comme Polyjuice et GYC, qui en plus d'utiliser des exemples en entrée, s'appuient sur une distribution apprise par le modèle de langue pour biaiser la génération vers certaines caractéristiques (via les codes de contrôle).</p>Nous avons décidé d'appeler cette approche Therapy puisque nous associons son fonctionnement à celui d'un thérapeute (le LM). Ce thérapeute interroge son patient (le classifieur) afin de comprendre son comportement et éventuellement découvrir des comportements pathologiques (des biais).</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>disponible sur GitHub. L'utilisation de la boite transparente dans PPL-MCTS se fait simplement en définissant la fonction qui prend en entrée une séquence et retourne son score. Le choix du modèle de langue génératif définit le domaine sur lequel nous voulons des explications pour le comportement du classifieur. Pour montrer que la méthode fonctionne bien avec un modèle de langue général (sans domaine particulier), nous utilisons OPT-125m<ref type="bibr" target="#b41">(Zhang et al., 2022)</ref>. Une régression logistique est ensuite apprise sur la représentation tf-idf des textes générés et les coefficients de la régression logistique sont finalement retournés comme scores d'importance des différents tokens.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE 1 -</head><label>1</label><figDesc>Corrélation de Spearman (p-valeur) entre les top mots de la boite transparente et les différentes méthodes d'explications. Les résultats sont donnés par classe et par jeu de données. Le suffixe 'other' indique que les explications sont générées à partir de l'autre jeu de données.Les corrélations de Spearman pour chacune des approches évaluées sont disponibles dans la Table1. Les résultats obtenus par Therapy sont meilleurs que ceux de LIME sur ag_news mais moins bon sur amazon_polarity tandis que SHAP donne des résultats meilleurs que les autres méthodes sur les deux jeux de données. Ces résultats sont bons pour Therapy puisque LIME et SHAP génèrent des explications à partir du jeu de données de test, garantissant ainsi que les caractéristiques cibles se trouvent dans les exemples en entrée. Or, lorsque cette hypothèse ne tient plus, par exemple en</figDesc><table><row><cell>5.2.2 Comparaison avec les autres méthodes</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>Outre l'attribution correcte de scores aux caractéristiques importantes du modèle, il est également nécessaire que l'explication fournisse une sortie informative en pratique. Il faut donc s'assurer que les mots retournés dans l'explication (i.e, les mots avec les scores les plus élevés) soient effectivement des mots importants pour le modèle étudié et que ses mots les plus importants soient trouvés. Ainsi, la Figure2montre pour différents nombres de mots importants retournés, les valeurs de précision et de rappel moyennées sur toutes les classes. Le nombre k de mots retournés varie de 10 à 1500. La précision est obtenue en mesurant la proportion de mots retournés dans l'explication qui appartiennent au top mots de la boite transparente tandis que le rappel est la proportion des top mots de la boite transparente qui sont présents dans l'explication.On observe que Therapy obtient de moins bons résultats que LIME (bien qu'obtenant un meilleur rappel sur ag_news) tandis que SHAP est meilleur que les deux méthodes sur les deux jeux de données. À nouveau, lorsque les données ne contiennent plus nécessairement les caractéristiques importantes pour le modèle (-other), les résultats s'écroulent et Therapy surpasse les deux approches. Cette limitation est visible par le plateau au niveau des scores de rappels pour ces méthodes : elles trouvent effectivement bien les caractéristiques importantes présentes dans les données, mais sont limitées à celles-ci uniquement, fixant la limite supérieure des caractéristiques pouvant être trouvées. En pratique, les biais contenus dans le modèle peuvent être suffisamment subtils pour ne</figDesc><table /><note><p>pas être présents dans le jeu de données à disposition, auquel cas, LIME et SHAP ne peuvent pas les détecter. Therapy, au contraire, obtient de bons résultats en utilisant le même LM générique pour les deux jeux de données, sans utiliser d'apriori. La méthode permet donc d'obtenir un très bon aperçu du comportement du modèle lorsqu'aucune donnée, ou plus largement, lorsqu'aucune donnée représentative des caractéristiques importantes du modèle n'est disponible. Dans ce dernier cas de figure, Therapy permet d'offrir une recherche plus exhaustive que celles se basant sur des textes existants, obtenant un score de rappel important.</p></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Remerciements</head><p>Ces travaux sont financés par l'<rs type="funder">Agence Nationale de la Recherche (ANR)</rs> dans le cadre de la convention de subvention <rs type="grantNumber">ANR-19-CE23-0019-01</rs> et par le réseau <rs type="projectName">TAILOR</rs> (<rs type="programName">EU Horizon 2020 programme d'innovation et de recherche avec la convention de</rs> subvention <rs type="grantNumber">952215</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_TcXPhwV">
					<idno type="grant-number">ANR-19-CE23-0019-01</idno>
					<orgName type="project" subtype="full">TAILOR</orgName>
					<orgName type="program" subtype="full">EU Horizon 2020 programme d&apos;innovation et de recherche avec la convention de</orgName>
				</org>
				<org type="funding" xml:id="_bubH82z">
					<idno type="grant-number">952215</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Résultats qualitatifs</head><p>Nous donnons dans cette annexe, quelques exemples d'échantillons générés par Therapy pour les différentes classes des deux jeux de données étudiés ainsi que les 20 premiers top-mots retournés par la méthode. Veuillez noter que certains "mots" correspondent à des "sous-mots", dû à la décomposition en unigrammes <ref type="bibr">(ve, ll, ...)</ref>. L'approche proposée permet d'étudier l'impact de n-grammes, mais ceci n'étant pas possible avec LIME et SHAP (avec le code disponible), nous avons limité l'étude aux unigrammes.</p><p>A.1 amazon_polarity, classe "positive" Échantillons :</p><p>-"The amount of support they gave was amazing. I'm so happy with the game and it's a great way for me to play my favorite role! Also, I love that they're giving us a new character every week, which is awesome :)I think they've been pretty good lately too. It's nice when we see someone who really loves their team and has fun doing things" </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 ag_news, classe "sport"</head><p>-I think it's pretty safe to assume that the guy has a lot of experience with the game. He played in the NHL, and he was a top 10 player on the team for most games last season (he had a goal in his final three playoff series), and he won a Stanley Cup as a rookie this past season (he finished third in the league in points scored, which was good for second in the league) and is still one of the best players in hockey at this stage in the year -We've got to keep playing this game. This team needs to win games and we need to play the best basketball that's been in our league all season, every night. And it is time to get out there and do that.""The Warriors have won three straight games at home and are 2-0 against the NBA's best team on both sides of the ball this season with a record of 21-1 (13.7 points per game)</p><p>Top <ref type="bibr">-mots : time, game, back, season, play, didn, team, guy, field, night, games, left, 12, title, won, saturday, playing, great, day, wasn</ref> A.5 ag_news, classe "business"</p><p>Échantillons :</p><p>-I am still in shock after hearing of that.It's a pretty big deal. It happened last month. They are trying to get the money out of the company by selling their stock for profit so they can sell more shares and buy more shares at higher prices (which I think would have helped with the stock market) and it was reported as an ""investment fraud"" by the SEC which has been going on all over this subreddit for months, but no one ever seems to care much -Biden is planning to spend millions of dollars to buy a new home, but the real estate market in America is still struggling with the housing shortage. The average house sale cost $1 billion and was up by nearly 50 percent from the previous year's price of about $800 million --according to the Real Estate Board of New York (RBE).The RBE estimates that the average house sales prices are expected to rise 1,000 per month this fiscal year as the economy continues its rebound <ref type="bibr">Top-mots : money, buy, care, doesn, things, deal, pay, worth, business, car, biggest, interested, month, trade, don, compagny, happened, store, kind</ref> We have a lot in our hands. The internet can help us connect to others through video chat and online games.""The company will launch a mobile game called 'Gangster', where it plans to offer ""an interactive experience"" with its users, according to the company. The game has been developed for the Apple iPad and Android phones that use Apple TV, which also uses Google Chromecast, according to a release. <ref type="bibr">-mots : ve, ll, idea, phone, internet, make, system, video, online, life, understand, version, pc, found, 13, thing, computer, lot, hard, issue, people, work, information, future</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Top</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Persistent anti-muslim bias in large language models</title>
		<author>
			<persName><forename type="first">A</forename><surname>Abid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Farooqi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AIES &apos;21 : AAAI/ACM Conference on AI, Ethics, and Society</title>
		<meeting><address><addrLine>Virtual Event, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2021-05-19">2021. May 19-21, 2021</date>
			<biblScope unit="page" from="298" to="306" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Exploring transformer text generation for medical dataset augmentation</title>
		<author>
			<persName><surname>Amin-Nejad A</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ive</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Velupillai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twelfth Language Resources and Evaluation Conference</title>
		<meeting>the Twelfth Language Resources and Evaluation Conference<address><addrLine>Marseille, France</addrLine></address></meeting>
		<imprint>
			<publisher>European Language Resources Association</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="4699" to="4708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Thirty years of artificial intelligence and law : overviews</title>
		<author>
			<persName><forename type="first">M</forename><surname>Araszkiewicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Bench-Capon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Francesconi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lauritsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rotolo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence and Law</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Explainable artificial intelligence (XAI) : concepts, taxonomies, opportunities and challenges toward responsible AI</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">B</forename><surname>Arrieta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">D</forename><surname>Rodríguez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Ser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bennetot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tabik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Barbado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>García</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gil-Lopez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Molina</surname></persName>
		</author>
		<author>
			<persName><surname>Benjamins R</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chatila</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Herrera</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inf. Fusion</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="page" from="82" to="115" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Residual energy-based models for text</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bakhtin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Szlam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">40</biblScope>
			<biblScope unit="page" from="1" to="41" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Benchmarking and survey of explanation methods for black box models</title>
		<author>
			<persName><forename type="first">F</forename><surname>Bodria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Giannotti</surname></persName>
		</author>
		<author>
			<persName><surname>Guidotti R</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Naretto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Pedreschi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Rinzivillo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">QLIME-A : Quadratic Local Interpretable Model-Agnostic Explanation Approach</title>
		<author>
			<persName><forename type="first">S</forename><surname>Bramhall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Horn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tieu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Lohia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SMU Data Science Rev</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Language models are few-shot learners</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">B</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><surname>Herbert-Voss A</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ramesh</forename><forename type="middle">A</forename><surname>Child R</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Winter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Sigler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Litwin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Chess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Berner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mccandlish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Amodei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 33 : Annual Conference on Neural Information Processing Systems 2020</title>
		<editor>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Balcan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Lin</surname></persName>
		</editor>
		<meeting><address><addrLine>NeurIPS; virtual</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-12-06">2020. 2020. December 6-12, 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Artificial intelligence in medicine : current trends and future possibilities</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">H</forename><surname>Buch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ahmed</forename><forename type="middle">I</forename><surname>Maruthappu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Br. J. Gen. Pract</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="issue">668</biblScope>
			<biblScope unit="page" from="143" to="144" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">PPL-MCTS : constrained textual generation through discriminator-guided MCTS decoding</title>
		<author>
			<persName><forename type="first">A</forename><surname>Chaffin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Claveau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Kijak</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2022.naacl-main.215</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics : Human Language Technologies, NAACL 2022</title>
		<title level="s">Association for Computational Linguistics</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Carpuat</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>De</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">I</forename><forename type="middle">V M</forename><surname>Ruíz</surname></persName>
		</editor>
		<meeting>the 2022 Conference of the North American Chapter of the Association for Computational Linguistics : Human Language Technologies, NAACL 2022<address><addrLine>Seattle, WA, United States</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022-07-10">2022. July 10-15, 2022</date>
			<biblScope unit="page" from="2953" to="2967" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jin</forename><forename type="middle">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename></persName>
		</author>
		<idno type="arXiv">arXiv:2004.02135</idno>
		<title level="m">Adding a filter based on the discriminator to improve unconditional text generation</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">ILIME : Local and Global Interpretable Model-Agnostic Explainer of Black-Box Decision</title>
		<author>
			<persName><surname>Elshawi R</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sherif</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Al-Mallah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sakr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ADBIS</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Dermatologist-level classification of skin cancer with deep neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Esteva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kuprel</surname></persName>
		</author>
		<author>
			<persName><forename type="middle">A</forename><surname>Novoa R</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Swetter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">M</forename><surname>Blau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Thrun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">nature</title>
		<imprint>
			<biblScope unit="volume">542</biblScope>
			<biblScope unit="issue">7639</biblScope>
			<biblScope unit="page" from="115" to="118" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">s-LIME : Reconciling locality and fidelity in linear explanations</title>
		<author>
			<persName><surname>Gaudel R</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Galárraga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Delaunay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Rozé</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Bhargava</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Intelligent Data Analysis XX -20th International Symposium on Intelligent Data Analysis, IDA 2022</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<meeting><address><addrLine>Rennes, France</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022-04-20">2022. April 20-22, 2022</date>
			<biblScope unit="volume">13205</biblScope>
			<biblScope unit="page" from="102" to="114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning to write with cooperative discriminators</title>
		<author>
			<persName><forename type="first">A</forename><surname>Holtzman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Buys</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Forbes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bosselut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Golub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Choi</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P18-1152</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, ACL 2018</title>
		<title level="s">Association for Computational Linguistics</title>
		<editor>
			<persName><forename type="first">I</forename><surname>Gurevych</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><surname>Miyao</surname></persName>
		</editor>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics, ACL 2018<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-07-15">2018. July 15-20, 2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1638" to="1649" />
		</imprint>
	</monogr>
	<note>Long Papers</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Trends in explainable AI (XAI) literature</title>
		<author>
			<persName><forename type="first">A</forename><surname>Jacovi</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2301.05433</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Interpretability methods of machine learning algorithms with applications in breast cancer diagnosis</title>
		<author>
			<persName><forename type="first">P</forename><surname>Karatza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Dalakleidi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Athanasiou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Nikita</surname></persName>
		</author>
		<idno type="DOI">10.1109/EMBC46164.2021.9630556</idno>
	</analytic>
	<monogr>
		<title level="m">2021 43rd Annual International Conference of the IEEE Engineering in Medicine &amp; Biology Society (EMBC)</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2310" to="2313" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Generative cooperative networks for natural language generation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Lamprier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Scialom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Chaffin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Claveau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Kijak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Staiano</surname></persName>
		</author>
		<author>
			<persName><surname>Piwowarski B. ; K</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chaudhuri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jegelka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning, ICML 2022</title>
		<title level="s">Proceedings of Machine Learning Research</title>
		<editor>
			<persName><forename type="first">C</forename><surname>Song</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Szepesvári</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Niu</surname></persName>
		</editor>
		<editor>
			<persName><surname>Sabato</surname></persName>
		</editor>
		<meeting><address><addrLine>Baltimore, Maryland, USA</addrLine></address></meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2022-07">2022. 17-23 July 2022</date>
			<biblScope unit="volume">162</biblScope>
			<biblScope unit="page" from="11891" to="11905" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Machine translation decoding beyond beam search</title>
		<author>
			<persName><surname>Leblond R</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Alayrac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Sifre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pislar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lespiau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Antonoglou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.emnlp-main.662</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, Virtual Event</title>
		<title level="s">Association for Computational Linguistics</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Moens</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">X</forename><surname>Huang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Specia</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><forename type="middle">W</forename><surname>Yih</surname></persName>
		</editor>
		<meeting>the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, Virtual Event<address><addrLine>Punta Cana, Dominican Republic</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021-07-11">2021. 7-11 November, 2021</date>
			<biblScope unit="page" from="8410" to="8434" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Gender and representation bias in GPT-3 generated stories</title>
		<author>
			<persName><forename type="first">Lucy</forename><forename type="middle">L</forename><surname>Bamman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.nuse-1.5</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Third Workshop on Narrative Understanding</title>
		<meeting>the Third Workshop on Narrative Understanding</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="48" to="55" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A unified approach to interpreting model predictions</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Lundberg</surname></persName>
		</author>
		<author>
			<persName><surname>Lee S. ; I</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName><surname>Von</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.nuse-1.5</idno>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 30 : Annual Conference on Neural Information Processing Systems 2017</title>
		<editor>
			<persName><forename type="first">S</forename><forename type="middle">V N</forename><surname>Fergus</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Vishwa-Nathan</surname></persName>
		</editor>
		<editor>
			<persName><surname>Garnett</surname></persName>
		</editor>
		<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-09">2017. December 4-9, 2017</date>
			<biblScope unit="page" from="4765" to="4774" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Generate your counterfactuals : Towards controlled counterfactual generation for text</title>
		<author>
			<persName><forename type="first">N</forename><surname>Madaan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Padhi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Panwar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Saha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Fifth AAAI Conference on Artificial Intelligence, AAAI 2021, Thirty-Third Conference on Innovative Applications of Artificial Intelligence, IAAI 2021, The Eleventh Symposium on Educational Advances in Artificial Intelligence, EAAI 2021, Virtual Event</title>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2021-02-02">2021. February 2-9, 2021</date>
			<biblScope unit="page" from="13516" to="13524" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Explanation in artificial intelligence : Insights from the social sciences</title>
		<author>
			<persName><forename type="first">T</forename><surname>Miller</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.artint.2018.07.007</idno>
	</analytic>
	<monogr>
		<title level="j">Artif. Intell</title>
		<imprint>
			<biblScope unit="volume">267</biblScope>
			<biblScope unit="page" from="1" to="38" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Scikit-learn : Machine learning in Python</title>
		<author>
			<persName><forename type="first">F</forename><surname>Pedregosa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Varoquaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gramfort</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Thirion</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Grisel</forename><forename type="middle">O</forename><surname>Blon-Del</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Prettenhofer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Weiss R</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Dubourg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Vanderplas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Passos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Courna-Peau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Brucher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Perrot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Duchesnay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2825" to="2830" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">RISE : randomized input sampling for explanation of black-box models</title>
		<author>
			<persName><forename type="first">V</forename><surname>Petsiuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference 2018</title>
		<meeting><address><addrLine>Newcastle, UK</addrLine></address></meeting>
		<imprint>
			<publisher>BMVA Press</publisher>
			<date type="published" when="2018-09-03">2018. September 3-6, 2018</date>
			<biblScope unit="page">151</biblScope>
		</imprint>
	</monogr>
	<note>BMVC 2018</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Improving language understanding by generative pre-training</title>
		<author>
			<persName><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskeve</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><surname>Child R</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Explaining the predictions of any classifier</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">T</forename><surname>Ribeiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Guestrin</surname></persName>
		</author>
		<idno type="DOI">10.1145/2939672.2939778</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<editor>
			<persName><forename type="first">B</forename><surname>Krishnapuram</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Shah</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Aggarwal</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Shen</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Rastogi</surname></persName>
		</editor>
		<meeting>the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining<address><addrLine>San Francisco, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016-08-13">2016. August 13-17, 2016</date>
			<biblScope unit="page" from="1135" to="1144" />
		</imprint>
	</monogr>
	<note>why should I trust you ?</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Generating realistic natural language counterfactuals</title>
		<author>
			<persName><forename type="first">M</forename><surname>Robeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bex</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Feelders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics : EMNLP 2021, Virtual Event</title>
		<meeting><address><addrLine>Punta Cana, Dominican Republic</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021-11">2021. 16-20 November, 2021</date>
			<biblScope unit="page" from="3611" to="3625" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Multi-armed bandits with episode context</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Rosin</surname></persName>
		</author>
		<idno type="DOI">10.1007/s10472-011-9258-6</idno>
	</analytic>
	<monogr>
		<title level="j">Ann. Math. Artif. Intell</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="203" to="230" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">S</forename><surname>Punla C</surname></persName>
		</author>
		<idno>ORG/ 0000-0002-1094-0018</idno>
		<ptr target="HTTPS://ORCID" />
		<imprint>
			<publisher>CSPUNLA@BPSU.EDU</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Are we there yet ? : An analysis of the competencies of BEED graduates of BPSU-DC</title>
		<author>
			<persName><forename type="first">C</forename><surname>Ph</surname></persName>
		</author>
		<author>
			<persName><surname>Farro R</surname></persName>
		</author>
		<idno>ORG/0000-0002-3571-2716</idno>
	</analytic>
	<monogr>
		<title level="m">HTTPS ://ORCID</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="50" to="59" />
		</imprint>
		<respStmt>
			<orgName>RCFARRO@BPSU.EDU.PH &amp; BA-TAAN PENINSULA STATE UNIVERSITY DINALUPIHAN, BATAAN</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Discriminative adversarial search for abstractive summarization</title>
		<author>
			<persName><forename type="first">T</forename><surname>Scialom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lamprier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Piwowarski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Staiano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th International Conference on Machine Learning, ICML 2020</title>
		<title level="s">Proceedings of Machine Learning Research</title>
		<meeting>the 37th International Conference on Machine Learning, ICML 2020<address><addrLine>Virtual Event</addrLine></address></meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020-07">2020. July 2020</date>
			<biblScope unit="volume">119</biblScope>
			<biblScope unit="page" from="8555" to="8564" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">To beam or not to beam : That is a question of cooperation for language gans</title>
		<author>
			<persName><forename type="first">T</forename><surname>Scialom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lamprier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Piwowarski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Staiano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2021">2021a</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">To beam or not to beam : That is a question of cooperation for language gans</title>
		<author>
			<persName><forename type="first">T</forename><surname>Scialom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Staiano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lamprier</surname></persName>
		</author>
		<author>
			<persName><surname>Piwowarski B. ; M</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName><surname>Beygelzimer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 34 : Annual Conference on Neural Information Processing Systems 2021</title>
		<editor>
			<persName><forename type="first">Y</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Liang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Vaughan</surname></persName>
		</editor>
		<meeting><address><addrLine>NeurIPS</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021-12-06">2021. 2021. December 6-14, 2021</date>
			<biblScope unit="page" from="26585" to="26597" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">ALIME : Autoencoder Based Approach for Local Interpretability</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Shankaranarayana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Runje</surname></persName>
		</author>
		<idno>CoRR, abs/1909.02437</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Mastering the game of go without human knowledge</title>
		<author>
			<persName><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schrittwieser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Antonoglou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Guez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hu-Bert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bolton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">P</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Sifre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Van Den Driessche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Graepel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hassabis</surname></persName>
		</author>
		<idno type="DOI">10.1038/nature24270</idno>
	</analytic>
	<monogr>
		<title level="j">Nat</title>
		<imprint>
			<biblScope unit="volume">550</biblScope>
			<biblScope unit="issue">7676</biblScope>
			<biblScope unit="page" from="354" to="359" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Unsupervised law article mining based on deep pre-trained language representation models with application to the italian civil code</title>
		<author>
			<persName><forename type="first">A</forename><surname>Tagarelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Simeri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence and Law</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="417" to="473" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Optilime : Optimized LIME explanations for diagnostic computer algorithms</title>
		<author>
			<persName><forename type="first">G</forename><surname>Visani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Bagli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Chesani</surname></persName>
		</author>
		<ptr target="CEUR-WS.org" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the CIKM 2020 Workshops co-located with 29th ACM International Conference on Information and Knowledge Management (CIKM 2020)</title>
		<title level="s">CEUR Workshop Proceedings</title>
		<meeting>the CIKM 2020 Workshops co-located with 29th ACM International Conference on Information and Knowledge Management (CIKM 2020)<address><addrLine>Galway, Ireland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-10-19">2020. October 19-23, 2020. 2699</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Polyjuice : Generating counterfactuals for explaining, evaluating, and improving models</title>
		<author>
			<persName><forename type="first">T</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">T</forename><surname>Ribeiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Heer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Weld</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.acl-long.523</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021</title>
		<title level="s">Association for Computational Linguistics</title>
		<editor>
			<persName><forename type="first">C</forename><surname>Zong</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>Xia</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Navigli</surname></persName>
		</editor>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021</meeting>
		<imprint>
			<date type="published" when="2021-08-01">2021. August 1-6, 2021</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="6707" to="6723" />
		</imprint>
	</monogr>
	<note>Virtual Event</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">DLIME : A Deterministic Local Interpretable Model-Agnostic Explanations Approach for Computer-Aided Diagnosis Systems</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Zafar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">M</forename><surname>Khan</surname></persName>
		</author>
		<idno>CoRR, abs/1906.10263</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">OPT : open pre-trained transformer language models</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Roller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Artetxe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Dewan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">T</forename><surname>Diab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">V</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mihaylov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shleifer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Shuster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Simig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Koura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sridhar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2205.01068</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Character-level convolutional networks for text classification</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 28 : Annual Conference on Neural Information Processing Systems 2015</title>
		<editor>
			<persName><forename type="first">C</forename><surname>Cortes</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><forename type="middle">D</forename><surname>Lawrence</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">D</forename><surname>Lee</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Garnett</surname></persName>
		</editor>
		<meeting><address><addrLine>Montreal, Quebec, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-12-07">2015. December 7-12, 2015</date>
			<biblScope unit="page" from="649" to="657" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
