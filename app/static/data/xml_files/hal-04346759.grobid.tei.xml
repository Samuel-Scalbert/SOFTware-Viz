<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Comparative Study of Text Representations for French Real-Estate Classified Advertisements Information Extraction</title>
				<funder ref="#_7wjXW7x">
					<orgName type="full">French government</orgName>
				</funder>
				<funder ref="#_m9CxxMU">
					<orgName type="full">Agence Nationale de la Recherche</orgName>
					<orgName type="abbreviated">ANR</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Lucie</forename><surname>Cadorel</surname></persName>
							<email>author.lucie.cadorel@inria.fr</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Universit√© C√¥te d&apos;Azur</orgName>
								<orgName type="institution" key="instit2">Inria</orgName>
								<orgName type="institution" key="instit3">CNRS</orgName>
								<orgName type="institution" key="instit4">I3S</orgName>
								<address>
									<settlement>Sophia Antipolis</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">KCityLabs</orgName>
								<address>
									<settlement>Sophia Antipolis</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Andrea</forename><forename type="middle">G B</forename><surname>Tettamanzi</surname></persName>
							<email>andrea.tettamanzi@univ-cotedazur.fr</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Universit√© C√¥te d&apos;Azur</orgName>
								<orgName type="institution" key="instit2">Inria</orgName>
								<orgName type="institution" key="instit3">CNRS</orgName>
								<orgName type="institution" key="instit4">I3S</orgName>
								<address>
									<settlement>Sophia Antipolis</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A Comparative Study of Text Representations for French Real-Estate Classified Advertisements Information Extraction</title>
					</analytic>
					<monogr>
						<idno type="ISSN">1613-0073</idno>
					</monogr>
					<idno type="MD5">AECB39C8A57E1FE244C521F3B87D0122</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-04-12T14:43+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Text Representations</term>
					<term>Information Extraction</term>
					<term>Real-Estate Market ENIGMA-23</term>
					<term>September 03-04</term>
					<term>2023</term>
					<term>Rhodes</term>
					<term>Greece</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Text representations are widely used in NLP tasks such as text classification. Very powerful models have emerged and been trained on huge corpora for different languages. However, most of the pre-trained models are domain-agnostic and fail on domain-specific data. We perform a comparison of different text representations applied to French Real Estate classified advertisements through several text classification tasks to retrieve some key attributes of a property. Our results demonstrate the limitations of pre-trained models on domain-specific data and small corpora, but also the strength of text representation, in general, to capture underlying knowledge about language and stylistic specificities.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Real-estate classified advertisements provide great details and relevant information about a property that is valuable for the intelligence of the real-estate market. For example, price predictions are often based on attributes such as the type of property, the number of rooms or even the floor at which it is located. However, those key information are not always clearly specified in the ads and often differ from an ad to another and from an advertiser from another. Thus, the automatic extraction of key information from the text of real-estate classified advertisements is a challenging and promising task to help in some real-estate market applications.</p><p>Given the limited number of different values that economically relevant attributes of a real-estate property may take, this information extraction task can be viewed as a classification problem. For instance, if the number of rooms of a property is sought for, the problem can be stated as assigning the property described by a given advertisement to one of the classes labeled as {1, 2, 3, 4, 5+}, corresponding to single-room, tworoom, three-room, four-room, and five-or-more-room dwellings. Nevertheless, texts of real-estate ads are often short with language and stylistic specificities and variabilities. Also, an additional challenge may be represented by the need to extract information from advertisements written in languages other than English, for which linguistic resources are thus harder to find or less well-developed.</p><p>Text representations models have emerged as very powerful approaches to learn useful features of a text and have been widely used for Machine Learning tasks such as classification. It is thus interesting to carry out a comparative study of the most prominent models found in the literature as they are applied to this specific text classification task, to understand their strengths and weaknesses.</p><p>Our main contributions may be summarized as follows:</p><p>‚Ä¢ we apply different text representations to a classification task to retrieve key attributes of properties found in classified advertisements written in French, that we have collected and annotated manually; ‚Ä¢ we propose a comparison of the strength and limitations of different text representations ; ‚Ä¢ we analyse the vocabulary and register used by French real-estate agents to understand their impact in the classification.</p><p>The rest of the paper is organized as follows: Section 2 positions our contribution with respect to the literature; Section 3 provides a detailed description of the dataset and the method we propose to classify real-estate ads; Section 4 reports the results of the experiment and draws some conclusions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Background and Related Work</head><p>From classical to the state-of-the-art methods, feature extraction models are the process of converting raw data into numeric features.</p><p>One of the most common classical models is Bag-ofwords (BoW), which represents a text by the occurrence of words. This model involves a vocabulary and a measure of the occurrence of words. The vocabulary captures all the words found in the corpus of texts and is fixed.</p><p>Regarding the measure, it can be binary (presence or not of a word in the text) or weighted. For example, Term Frequency -Inverse Document Frequency (TF-IDF) is a weighted BoW that scores each word of a text by its frequency in the text (TF) and across the whole corpus (IDF). This measure penalizes very frequent words and highlights relevant one. The classical methods are easy to compute and customize for any language and text specificities. However, this approach suffers from a curse of dimensionality because of the size of the vocabulary and the sparsity. Also, it does not capture the position and meaning of a word in a text.</p><p>Those limitations led to a feature learning technique in which words are mapped to a vector of ùëÅ dimensions, with ùëÅ smaller than the size of the vocabulary. This approach is called Word Embedding. The most well-known model is Word2Vec, developed by Mikolov et al. <ref type="bibr" target="#b0">[1]</ref> and based on neural networks. Two architectures have been released: continuous bag-of-words (CBOW) and Skipgram. Both methods train a word against its neighboring words in the input corpus. The main difference is CBOW uses neighbors to predict the target word, while Skipgram uses the target to predict its neighbors.</p><p>A variant of Word2Vec is Doc2vec <ref type="bibr" target="#b1">[2]</ref>, which creates a numeric representation for the whole document. As Word2Vec, two architectures have been built Finally, there exist other models similar to Word2Vec, such as Global Vectors (GloVe) <ref type="bibr" target="#b2">[3]</ref> or FastText <ref type="bibr" target="#b3">[4]</ref>. Glove focuses on the global context instead of local one and uses a word-word co-occurrence matrix computed from the entire corpus. FastText is based on the CBOW architecture but using ùëõ-grams as input instead of full words. ùëÅ -grams help to prevent the Out-of-Vocabulary problem that suffer Word2Vec, Glove or classical representations. However, this method requires a huge storage memory.</p><p>All of the feature learning techniques presented above have limitations such as the need for a huge corpus to train and failure to capture contextual information.</p><p>Context-based models try to tackle those limitations. Contextual embeddings help to distinguish a same word with a different meaning according to the semantics and grammar. The first contextual embeddings models are mainly based on the (Bi-)LSTM architecture. For example, Embeddings from Language Models (ELMo) <ref type="bibr" target="#b4">[5]</ref> is a twolayer bidirectional language model using Bi-LSTM. This means that the left and right contexts are taken into account in the predictions. Also, it uses a character-level representation of words. Nevertheless, those kinds of modes do not improve performance significantly and are computationally very expensive.</p><p>The last state-of-the-art context-based model uses a Transformer architecture <ref type="bibr" target="#b5">[6]</ref>. It has been proven that Transformers are faster and more efficient than (Bi)-LSTM (ELMo) or CNN (Word2Vec). For example, Bidirectional Encoder Representations from Transformers (BERT) <ref type="bibr" target="#b6">[7]</ref> and its variants are one of those models. BERT uses parallel attention layers instead of sequential recurrent neural networks as ELMo does. Also, it is trained on a huge corpus with two specific tasks: masked language model (MLM) and next sentence prediction (NSP). For MLM, some tokens are masked and the model has to predict them in a sentence. The other task (NSP) is to try to predict, between two sentences, which one follows the other one. Models like BERT reach high performance compared to the other embedding models. However, some limitations have arisen: the need for a huge training set and a pre-trained model on general domain limits their application to specific domains or tasks. Thus, some specific domain models have been trained such as BERTTweet for Twitter or SciBERT for the biological domain.</p><p>The advantages and limitations of the different existing word embedding models for low-quality data are discussed in a recent survey of word representation models <ref type="bibr" target="#b7">[8]</ref>.</p><p>All the methods described above have been widely used and compared for different tasks and types of text. However, a few of them are focused on small data and French documents, as in our work.</p><p>Dynomant et al. <ref type="bibr" target="#b8">[9]</ref> compare mainly Word2Vec (Skipgram, CBOW), FastText and Glove on a specific dataset (health-related documents) written in French. They also have to come to grips with the issue of the quality of language. According to their evaluation, which is more qualitative (similarity, word clustering, etc.) than quantitative, they find Word2Vec with a Skip-gram architecture to be the most promising model.</p><p>Another work <ref type="bibr" target="#b9">[10]</ref>, comparing Word2Vec to latent semantic analysis (LSA), shows that LSA gives better results for small training corpora, while Word2Vec works best with medium-sized training corpora. This study too carries out a qualitative, rather than quantitative, analysis.</p><p>A recent survey of text classification algorithms <ref type="bibr" target="#b10">[11]</ref> illustrates essentially the same methodology we followed, consisting of pre-processing, feature extraction, and classification, by covering a variety of models of text representations (BoW, TF-IDF, Word2Vec, BERT, etc.) and classification (Naive Bayes, Decision Trees, Random Forests, Gradient Boosting, Logistic Regression, and Deep Learning). It compares the advantages and limitations of each model. Also, the authors mention several works in different domains, including health, business, and social, and their applications.</p><p>Finally, Gupta and Waseem <ref type="bibr" target="#b11">[12]</ref> try different word embedding models to detect hate speech from Twitter and use Logistic Regression to classify tweets. The authors find Word2Vec to perform better than Glove, and FastText to be the worst model in that setting. They also provide compelling evidence that, while domainspecific models outperform domain-agnostic ones, their combination yields the best results. In addition, they find Doc2Vec to perform poorly and Tweet2Vec (specifically designed for Twitter micro-blogs) to perform rather well on imbalanced data, but less so on balanced data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>In this section, we will first describe the dataset, then the preprocessing and feature extraction applied on the ad-vertisments and, finally, the different classification tasks that we have used for the comparison.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Dataset</head><p>We gathered 5,440 real-estate classified advertisements of residential and commercial properties, luxury homes and garages/parkings, all located on the French Riviera, from various French online advertisers. The ads are written in French and composed of a title, description, pictures, and some metadata about the property (e.g type of property, number of rooms, price, etc.). Metadata include the most important and relevant information to summarize a property. However, metadata differ from an ad to another and from an advertiser to another. On the other hand, all ads contain a textual description which is a great source of information to infer missing metadata.</p><p>In our study, we focused on extracting the type of property, the number of rooms, and the apartment's floor. As we have this information in metadata for a sufficient number of ads, it was relatively easy to label the texts and to apply classification for each type of information. Also, we created artificial classes to normalize our targets. For example, for the type of property, we did not distinguish a luxury villa from a small house : both have been classified as 'House'. Regarding the number of rooms, it goes from '1' to '5 and more' rooms which is a popular discretization in the Real Estate market. Finally, the floor of the property is divided in 5 classes : from 'Ground floor' to 'third floor' and then, 'High floor' and 'Last Floor'. The last two classes might be confusing as 'Last Floor' is also a 'High Floor' but the information 'Last Floor' could be important in price predictions.</p><p>Tables <ref type="table" target="#tab_0">1,</ref><ref type="table">2</ref>, and 3 show the class distribution for each target. The type of property is complete (0 missing data) but not really balanced. It is very easy to get the type of property in the metadata since it is essential to sell the product. However, the number of rooms and the floor of a property are not complete. This is the reason why it might be interesting to predict this kind of information from text. Also, we only predict the number of rooms for apartments and houses, and the floor of the property only for apartments.</p><p>Although we plan on making our dataset publicly available, for the time being we could not, for legal reasons, as it is expressly forbidden by the real-estate agencies that own those advertisements to republish them in any form, as long as the relevant properties are still on the market.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Text Preprocessing</head><p>Preprocessing and cleaning texts are a crucial step since French real-estate ads are full of noisy, repetitive words and abbreviations. First, we removed noise such as elongated punctuations ("...... ", "!!!!", etc.) and URLs. We replaced symbols such as "e" by "euros", "m2", "M2", and "M<ref type="foot" target="#foot_0">2</ref> " by "m 2 ". Also, some abbreviations are of common use in the real-estate market, e.g., 'apt.' stands for apartment, 'balc.' for balcony. We tried to remove proper nouns found in phrases such as "contactez Paul Martin" (contact Paul Martin), which refer to the advertiser, thanks to a regex. Then, we lemmatized texts with a French lemmatizer from spaCy. This lemmatizer has been trained on the French Sequoia corpus 1 and WikiNER. The lemmatizer performs pretty well for French, as we can see in Figure <ref type="figure" target="#fig_3">3</ref>. Nevertheless, the syntax of real-estate ads is slightly different from the general French syntax and the lemmatizer fails to assign the correct lemma. For example, the noun "nuit" (night) in the phrase "coin nuit" (sleeping area) is erroneously lemmatized in the verb "nuire" (nuit is also the 3rd person sg. form of "nuire", to harm). The final step was to remove punctuation, numbers and French stopwords. We tuned the stopword list with very frequent words in the real estate vocabulary, e.g., "honoraires" (fees), "agent immobilier" or "agent commercial" (real-estate agent).</p><p>1 https://github.com/UniversalDependencies/UD_French-Sequoia</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Feature Extraction</head><p>After cleaning the texts, we applied different text representations methods presented in Section 2.</p><p>Two classical Bag-of-Words methods have been tested : TF and TF-IDF. We set maximum features to 5,000 and we chose to take a range of ùëõ-grams from 1 to 3, that is to say from 1 to 3 tokens.</p><p>Then, we tried non-contextual embedding with Word2Vec (Skip-Gram), Doc2Vec (PV-DM), and FastText. We trained those 3 models on our corpus of ads. We chose a smaller number of features for FastText because of its need of memory storage. We wanted to compare our own trained non-contextual embeddings with a pretrained model, but only few models have been trained on a French corpora. Thus we only found a pre-trained Word2Vec model <ref type="bibr" target="#b12">[13]</ref>.</p><p>Finally, we wanted to apply contextual embedding. However, although pre-trained (Bi-)LSTM models such as ELMo have recently become available for French, 2 we were not aware of them at the time we planned our experiments. Now, it is very long to train a model and a huge corpus is needed, which we lack. Therefore, we decided to try pre-trained Transformer models for French, like CamemBERT <ref type="bibr" target="#b13">[14]</ref> and FlauBERT <ref type="bibr" target="#b14">[15]</ref>. These French models chiefly differ for their training data. CamemBERT was trained on OSCAR <ref type="bibr" target="#b15">[16]</ref>, whose size is 138 GB after cleaning, while FlauBERT used 24 corpora from different sources (Wikipedia, books, Common Crawl, etc.), whose overall size is only half as the CamemBERT training data (71 GB). They also differ for their tokenizer: FlauBERT uses a basic Byte Pair Encoding <ref type="bibr" target="#b16">[17]</ref>, whereas Camem-BERT prefers its extension, called SentencePiece <ref type="bibr" target="#b17">[18]</ref>. Finally, the models use different strategies for the masking task: FlauBERT masks sub-word, whereas CamemBERT masks the whole word.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Classification Task</head><p>The final step was to classify our real-estate ads according to three labels (Type of property, Number of rooms and Floor of the property) in order to retrieve missing metadata. We used our different feature extractions as input and we applied a classifier. Different classifiers have been tried (e.g. Naive Bayes, Logistic Regression, or Random Forest) for every text representations, but in the following, we will only present results with Logistic Regression as it gave higher score. CamemBERT and FlauBERT have been already trained for classification, so we used their classifier based on neural networks.</p><p>We also compared our models to a simple Regex, which is our baseline. For instance, to find the type of property, we crafted rules that classify a property as an apartment if the words "apartment" or "studio" are found; else if "house" or "villa" are found, it is classified as house, and so on. For the number of rooms, we searched for a number (in words or digits) before the word "pi√®ce" (room) or "chambre" (bedroom); if we find a number before "pi√®ce" then we take it as the label for classification; else, if we find a number before "chambre", we take that number plus one. The same idea is followed for the floor of a property, but the word "√©tage" (floor) is used instead.</p><p>We used the F1-Score as measure of accuracy to handle imbalanced classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments and Results</head><p>In this section, we will present the results of our experiments and the comparison of the different text representations. Afterward, we will discuss the possible explanations of the results and infer knowledge for the real-estate market.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Classification results and Comparison</head><p>The results of the classification show that most of the text representation methods combined with a classifier outperform the Regex baseline. It shows that Real Estate ads are too noisy to retrieve information easily. Nevertheless, we noticed that the predictions of number of rooms are slightly better with a regex for 2, 3 or 4 rooms. The advertisers seem to write more often the number of rooms for those classes than for '1' or '5 and more' rooms in their description.</p><p>Comparing text representations, classical methods (mainly TF-IDF) and CamemBERT achieved the best F1-Score. Non-contextual methods such as Word2Vec, Doc2Vec or FastText lagged behind. FlauBERT fared even worse. FastText and FlauBERT are more based on a character or sub-word level. However, the vocabulary of French real-estate ads is pretty poor and might not be suitable for this level. On the contrary, Doc2Vec embeds the whole paragraph and is less precise. Most paragraphs contain the same kind of vocabulary and syntax, so Doc2Vec fails to discriminate them. In general, classical methods gave better results, except for CamemBERT, which has similar results. Nevertheless, the training dataset was really small and the performance of more complex methods might improve with a larger corpus.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Discussion</head><p>The comparison of the text representations for the classifications tasks highlights that classical representations such as TD-IDF have better results than non-contextual word-embbedings (Word2vec, FastText, etc.) or quite similar with state-of-the-art models (CamemBERT). This result might be explained by the lack of data and a very narrow vocabulary found in the ads. Also, it could be surprising that text representations combined with classification algorithms outperform simple regular expression. Thus, we analysed the odds ratio of the Logistic Regression used for the classification combined with TF-IDF representations since it gave good results. Odds ratio helps to know if a variable (e.g., a word) is increasing or decreasing the probability of the text belonging to one class or another. In Tables <ref type="table" target="#tab_4">7,</ref><ref type="table">8</ref> and 9, we can see the words that increase or decrease the most the probability for each class. For example, the probability is obviously increased by the word that describes the class, such as "apartment", "house", for the type of product, or "two rooms", "three rooms" for the  number of rooms. However, it is interesting to see that unexpected words turn out to have an important impact. Indeed, we can see that real-estate agents target different people according to the number of rooms. They often use words about investment for one-or two-room dwellings, while they target parents with children for three-room properties. Furthermore, the vocabulary used to classify the floor of the property is also different. An agent will describe more the view for a high floor than for the ground floor. For instance, the expression "panoramic view" is associated to "Last Floor".</p><p>In a nutshell, this study points out the specific and stylistic vocabulary used by French real-estate agents. As we have shown, basic information is not always clearly and explicitly written in the ads. However, other words can help to infer such key information for the real-estate market.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion and Further Work</head><p>In this paper, we carried out a comparison of various text representations models with respect to their application to the classification of real-estate classified advertisements in French, having the ultimate goal of extracting key information about the properties they advertise.</p><p>In summary, we found out that, among the models we tested, the classic representation TF-IDF and the most state-of-the-art model CamemBERT are the ones that stand out for the classification task.</p><p>Another interesting finding is that pre-trained models, despite having been trained on a much larger corpus and with impressive computational resources, turn out not to be much useful, due to their being domain-agnostic, whereas models trained on our small domain-specific corpus clearly outperform them, thus confirming similar findings in other domains <ref type="bibr" target="#b11">[12]</ref>.</p><p>Finally, we provided knowledge about the vocabulary and the type of register use by the real-estate agents according to the property they advertise.</p><p>We believe that our results will provide useful guidance to anybody willing to engage in real-estate classified ads classification, in general, and information extraction in particular.</p><p>A promising research direction which is, in our opinion, worth investigating is to combine different text representations models, each capable of capturing different details, in order to obtain a higher overall accuracy. Another quite obvious extension of our investigation would be to gather a bigger corpus of advertisements and study how using it to train the model increases the classification accuracy. Large language models could also be considered, as a possible end-to-end solution to this information extraction task.</p><p>As future work, it could be interesting to capture the syntax of real-estate ads in order to understand even better their language. Also, property pictures, which of- </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>. The first one is based on CBOW with an extra input which is the ID of the document. This model is called Distributed Memory version of Paragraph Vector (PV-DM). The other one is inspired by Skip-gram and is called Distributed Bag of Words version of Paragraph Vector (PV-DBOW).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: CBOW and Skip-gram architectures [1]</figDesc><graphic coords="3,307.56,84.19,198.43,122.32" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: BERT architecture [7]</figDesc><graphic coords="4,151.80,84.19,291.69,127.17" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Example of text pre-processing</figDesc><graphic coords="6,151.80,84.19,291.69,168.59" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1</head><label>1</label><figDesc>Type of property</figDesc><table><row><cell></cell><cell>Table 2</cell></row><row><cell></cell><cell>Number of rooms</cell></row><row><cell>Label Apartment House Commercial property 988 Distribution 2402 1679 Garage/Parking 349 Missing data 0</cell><cell>Label 1 2 3 4 5+ Missing data 63 Distribution 188 694 1104 690 1342</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 3</head><label>3</label><figDesc>Floor of the property</figDesc><table><row><cell>Label</cell><cell>Distribution</cell></row><row><cell cols="2">Ground floor 311</cell></row><row><cell>1</cell><cell>165</cell></row><row><cell>2</cell><cell>131</cell></row><row><cell>3</cell><cell>96</cell></row><row><cell>High floor</cell><cell>97</cell></row><row><cell>Last floor</cell><cell>41</cell></row><row><cell cols="2">Missing data 1560</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4</head><label>4</label><figDesc>Classification Type of product</figDesc><table><row><cell>Model</cell><cell></cell><cell cols="2">F1-Score</cell><cell cols="2">F1-Score</cell><cell cols="2">F1-Score</cell><cell>F1-Score</cell><cell>F1-Score</cell></row><row><cell></cell><cell></cell><cell>(Total)</cell><cell></cell><cell cols="2">(Apartment)</cell><cell cols="2">(House)</cell><cell>(Commercial)</cell><cell>(Parking)</cell></row><row><cell>Regex</cell><cell></cell><cell>71%</cell><cell></cell><cell>74.2%</cell><cell></cell><cell>52.8%</cell><cell>81%</cell><cell>53.1%</cell></row><row><cell>BoW</cell><cell></cell><cell>97.8%</cell><cell></cell><cell>97.8%</cell><cell></cell><cell>98%</cell><cell>98%</cell><cell>97.1%</cell></row><row><cell>TF-IDF</cell><cell></cell><cell>98.3%</cell><cell></cell><cell>98.5%</cell><cell></cell><cell>98.5%</cell><cell>98.2%</cell><cell>97.1%</cell></row><row><cell cols="2">Word2Vec (frWac2Vec)</cell><cell>96.4%</cell><cell></cell><cell>97.7%</cell><cell></cell><cell>96.4%</cell><cell>95.5%</cell><cell>93%</cell></row><row><cell>Word2Vec (corpus)</cell><cell></cell><cell>96.5%</cell><cell></cell><cell>97.5%</cell><cell></cell><cell>96%</cell><cell>95.8%</cell><cell>94.3%</cell></row><row><cell>Doc2Vec</cell><cell></cell><cell>89.8%</cell><cell></cell><cell>93.4%</cell><cell></cell><cell>86.4%</cell><cell>90%</cell><cell>74.3%</cell></row><row><cell>FastText</cell><cell></cell><cell>94.4%</cell><cell></cell><cell>95.3%</cell><cell></cell><cell>94.3%</cell><cell>94%</cell><cell>91.2%</cell></row><row><cell>CamemBERT</cell><cell></cell><cell>98%</cell><cell></cell><cell>98.1%</cell><cell></cell><cell>99%</cell><cell>97.6%</cell><cell>95.8%</cell></row><row><cell>FlauBERT</cell><cell></cell><cell>44.2%</cell><cell></cell><cell>61.2%</cell><cell></cell><cell>0%</cell><cell>0%</cell><cell>0%</cell></row><row><cell>Table 5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Classification Number of Rooms</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Model</cell><cell cols="2">F1-Score</cell><cell cols="2">F1-Score</cell><cell cols="2">F1-Score</cell><cell>F1-Score</cell><cell>F1-Score</cell><cell>F1-Score</cell></row><row><cell></cell><cell cols="2">(Total)</cell><cell cols="2">(1 room)</cell><cell cols="2">(2 rooms)</cell><cell>(3 rooms)</cell><cell>(4 rooms)</cell><cell>(5+ rooms)</cell></row><row><cell>Regex</cell><cell cols="2">59%</cell><cell cols="2">55.6%</cell><cell>76%</cell><cell></cell><cell>75%</cell><cell>61.7%</cell><cell>51%</cell></row><row><cell>BoW</cell><cell cols="2">66%</cell><cell cols="2">90%</cell><cell>67.1%</cell><cell></cell><cell>62%</cell><cell>45%</cell><cell>77.1%</cell></row><row><cell>TF-IDF</cell><cell cols="2">69.3%</cell><cell cols="2">85%</cell><cell>69.7%</cell><cell></cell><cell>62.6%</cell><cell>54%</cell><cell>80%</cell></row><row><cell>Word2Vec (frWac2Vec)</cell><cell cols="2">65.3%</cell><cell cols="2">71.4%</cell><cell>67.1%</cell><cell></cell><cell>57.3%</cell><cell>48.4%</cell><cell>80.6%</cell></row><row><cell>Word2Vec (corpus)</cell><cell cols="2">66.8%</cell><cell cols="2">77%</cell><cell>69.7%</cell><cell></cell><cell>61%</cell><cell>47.8%</cell><cell>80%</cell></row><row><cell>Doc2Vec</cell><cell cols="2">59.1%</cell><cell cols="2">69.8%</cell><cell>57.3%</cell><cell></cell><cell>54.3%</cell><cell>35%</cell><cell>76.7%</cell></row><row><cell>FastText</cell><cell cols="2">61.6%</cell><cell cols="2">66.7%</cell><cell>58%</cell><cell></cell><cell>52.8%</cell><cell>40.2%</cell><cell>81.5%</cell></row><row><cell>CamemBERT</cell><cell cols="2">70.8%</cell><cell cols="2">86.5%</cell><cell>70%</cell><cell></cell><cell>67.2%</cell><cell>43.4%</cell><cell>85.2%</cell></row><row><cell>FlauBERT</cell><cell cols="2">32%</cell><cell>0%</cell><cell></cell><cell>0%</cell><cell></cell><cell>0%</cell><cell>0%</cell><cell>48.4%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 6</head><label>6</label><figDesc></figDesc><table><row><cell>Classification Floor</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Model</cell><cell>F1-Score</cell><cell>F1-Score</cell><cell>F1-Score</cell><cell>F1-Score</cell><cell>F1-Score</cell><cell>F1-Score</cell><cell>F1-Score</cell></row><row><cell></cell><cell>(Total)</cell><cell>(ground floor)</cell><cell>(1 ùë†ùë° floor)</cell><cell>(2 ùëõùëë floor)</cell><cell>(3 ùëüùëë floor)</cell><cell>(high floor)</cell><cell>(last floor)</cell></row><row><cell>Regex</cell><cell>40.7%</cell><cell>49%</cell><cell>67%</cell><cell>63.2%</cell><cell>18.2%</cell><cell>47%</cell><cell>60%</cell></row><row><cell>BoW</cell><cell>61.8%</cell><cell>75%</cell><cell>64%</cell><cell>63.6%</cell><cell>25%</cell><cell>75%</cell><cell>35.3%</cell></row><row><cell>TF-IDF</cell><cell>68.4%</cell><cell>84.8%</cell><cell>76.2%</cell><cell>57.1%</cell><cell>26.7%</cell><cell>83.3%</cell><cell>35.3%</cell></row><row><cell>Word2Vec (frWac2Vec)</cell><cell>56.6%</cell><cell>82%</cell><cell>46.7%</cell><cell>41.7%</cell><cell>14.3%</cell><cell>75%</cell><cell>26.7%</cell></row><row><cell>Word2Vec (corpus)</cell><cell>57.8%</cell><cell>79.4%</cell><cell>38.5%</cell><cell>41.7%</cell><cell>40%</cell><cell>75%</cell><cell>37.5%</cell></row><row><cell>Doc2Vec</cell><cell>50%</cell><cell>68.8%</cell><cell>48%</cell><cell>46.1%</cell><cell>0%</cell><cell>22%</cell><cell>37.5%</cell></row><row><cell>FastText</cell><cell>43.4%</cell><cell>74.6%</cell><cell>36.4%</cell><cell>25%</cell><cell>9.5%</cell><cell>0%</cell><cell>30%</cell></row><row><cell>CamemBERT</cell><cell>72.5%</cell><cell>91.4%</cell><cell>44.4%</cell><cell>0%</cell><cell>0%</cell><cell>0%</cell><cell>0%</cell></row><row><cell>FlauBERT</cell><cell>69.7%</cell><cell>82.1%</cell><cell>0%</cell><cell>0%</cell><cell>0%</cell><cell>0%</cell><cell>0%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 7</head><label>7</label><figDesc>Word importance for the type of property</figDesc><table><row><cell>Label</cell><cell></cell><cell>Odds ratio &gt; 1</cell><cell>Odds ratio &lt; 1</cell></row><row><cell cols="2">Apartment</cell><cell cols="2">apartment, cellar, apartment complex,</cell><cell>land, house, floor, single-storey, location</cell></row><row><cell></cell><cell></cell><cell>room, living-room</cell></row><row><cell>House</cell><cell></cell><cell cols="2">house, villa, bedroom, swimming-pool,</cell><cell>apartment complex, parking, building, lo-</cell></row><row><cell></cell><cell></cell><cell>property</cell><cell>cated, lot</cell></row><row><cell cols="2">Commercial</cell><cell>business, hotel, wall, commercial space</cell><cell>apartment complex, living room, garage,</cell></row><row><cell>property</cell><cell></cell><cell></cell><cell>quiet, view</cell></row><row><cell cols="2">Garage/Parking</cell><cell cols="2">garage, box, apartment complex, parking,</cell><cell>terrace, room, bedroom, sea</cell></row><row><cell></cell><cell></cell><cell>basement</cell></row><row><cell>Table 8</cell><cell></cell><cell></cell></row><row><cell cols="2">Number of rooms</cell><cell></cell></row><row><cell>Label</cell><cell cols="2">Odds ratio &gt; 1</cell><cell>Odds ratio &lt; 1</cell></row><row><cell>1</cell><cell cols="2">studio apartment, investment, lot, living-room,</cell><cell>bedroom, room, house</cell></row><row><cell></cell><cell>rental</cell><cell></cell></row><row><cell>2</cell><cell cols="2">two rooms, bedroom, living-room, kitchen, in-</cell><cell>two bedrooms, parental bedroom, villa, house,</cell></row><row><cell></cell><cell>vestment</cell><cell></cell><cell>studio apartment</cell></row><row><cell>3</cell><cell cols="2">three rooms, two bedrooms, children's bed-</cell><cell>studio apartment, living-room, large room, rare,</cell></row><row><cell></cell><cell cols="2">room, crossing apartment, corner apartment</cell><cell>awesome</cell></row><row><cell>4</cell><cell cols="2">three bedrooms, villa, house, terrace, bourgeois</cell><cell>rental, cellar, furnished, to renovate, garage</cell></row><row><cell></cell><cell>building</cell><cell></cell></row><row><cell>5+</cell><cell cols="2">villa, swimming-pool, land, house, property</cell><cell>apartment complex, shared property, closet,</cell></row><row><cell></cell><cell></cell><cell></cell><cell>parking, small</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 9</head><label>9</label><figDesc>Floor of the propertyten come together with the advertisements, are a major source of information that can contribute to the intelligence of the real-estate market.</figDesc><table><row><cell>Label</cell><cell>Odds ratio &gt; 1</cell><cell>Odds ratio &lt; 1</cell></row><row><cell>Ground floor</cell><cell>ground floor, quiet, living conditions</cell><cell>view, entry, balcony, last floor, shared prop-</cell></row><row><cell></cell><cell></cell><cell>erty</cell></row><row><cell>1</cell><cell>first floor, storage area, living-room, stand-</cell><cell>last floor, view, sea, second floor</cell></row><row><cell></cell><cell>ing, zone</cell><cell></cell></row><row><cell>2</cell><cell>second floor, owner, shared property, lot,</cell><cell></cell></row><row><cell></cell><cell>storage</cell><cell></cell></row><row><cell>3</cell><cell>third floor, cellar, crossing apartment,</cell><cell>space, bathroom, secure</cell></row><row><cell></cell><cell>beautiful view, orientation</cell><cell></cell></row><row><cell>High floor</cell><cell>fourth floor, high, hills, small sea view, last</cell><cell>shared property, gate, children, visit</cell></row><row><cell></cell><cell>floor</cell><cell></cell></row><row><cell>Last floor</cell><cell>last floor, large living-room, residential</cell><cell>shared property, lot</cell></row><row><cell></cell><cell>area, sea, panoramic view</cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>Cf., for example, https://github.com/HIT-SCIR/ ELMoForManyLangs.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>This research was carried out in the Wimmics team, which is a joint research team of <rs type="institution">Universit√© C√¥te d'Azur, Inria</rs>, and I3S. Our research motto: AI in bridging social semantics and formal semantics on the Web. This work has been partially supported by the <rs type="funder">French government</rs>, through the <rs type="programName">3IA C√¥te d'Azur "Investments in the Future</rs>" project managed by the <rs type="funder">National Research Agency (ANR)</rs> with the reference number <rs type="grantNumber">ANR-19-P3IA-0002</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_7wjXW7x">
					<orgName type="program" subtype="full">3IA C√¥te d&apos;Azur &quot;Investments in the Future</orgName>
				</org>
				<org type="funding" xml:id="_m9CxxMU">
					<idno type="grant-number">ANR-19-P3IA-0002</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2013/file/9aa42b31882ec039965f3c4923ce901b-Paper.pdf" />
	</analytic>
	<monogr>
		<title level="s">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">C</forename><forename type="middle">J C</forename><surname>Burges</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<date type="published" when="2013">2013</date>
			<publisher>Curran Associates, Inc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Distributed representations of sentences and documents</title>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<ptr target="http://proceedings.mlr.press/v32/le14.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31th International Conference on Machine Learning, ICML 2014</title>
		<meeting>the 31th International Conference on Machine Learning, ICML 2014<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-06-26">21-26 June 2014. 2014</date>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="1188" to="1196" />
		</imprint>
	</monogr>
	<note>JMLR Workshop and Conference Proceedings</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Manning</surname></persName>
		</author>
		<idno type="DOI">10.3115/v1/D14-1162</idno>
		<ptr target="https://www.aclweb.org/anthology/D14-1162.doi:10.3115/v1/D14-1162" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Em-pirical Methods in Natural Language Processing (EMNLP), Association for Computational Linguistics</title>
		<meeting>the 2014 Conference on Em-pirical Methods in Natural Language Processing (EMNLP), Association for Computational Linguistics<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.04606</idno>
		<title level="m">Enriching word vectors with subword information</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deep contextualized word representations</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NAACL</title>
		<meeting>of NAACL</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">U</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf" />
	</analytic>
	<monogr>
		<title level="s">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">I</forename><surname>Guyon</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">U</forename><forename type="middle">V</forename><surname>Luxburg</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Wallach</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Vishwanathan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Garnett</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<date type="published" when="2017">2017</date>
			<publisher>Curran Associates, Inc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1423</idno>
		<ptr target="https://www.aclweb.org/anthology/N19-1423.doi:10.18653/v1/N19-1423" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Association for Computational Linguistics</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
	<note>Long and Short Papers</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><forename type="first">N</forename><surname>Usman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Imran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">S</forename><surname>Khalid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mukesh</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/2010.15036" />
		<title level="m">A comprehensive survey on word representation models: From classical to state-of-the-art word representation language models</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Word Embedding for the French Natural Language in Health Care: Comparative Study</title>
		<author>
			<persName><forename type="first">E</forename><surname>Dynomant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Lelong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Dahamna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Masson-Naud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Kerdelhu√©</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Grosjean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Canu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Darmoni</surname></persName>
		</author>
		<idno type="DOI">10.2196/12310</idno>
		<ptr target="http://www.ncbi.nlm.nih.gov/pubmed/31359873.doi:10.2196/12310" />
	</analytic>
	<monogr>
		<title level="j">JMIR Med Inform</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Comparative study of LSA vs Word2vec embeddings in small corpora: a case study in dreams database</title>
		<author>
			<persName><forename type="first">E</forename><surname>Altszyler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sigman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ribeiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">F</forename><surname>Slezak</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.concog.2017.09.004</idno>
		<ptr target="http://arxiv.org/abs/1610.01520.doi:10.1016/j.concog.2017.09.004" />
	</analytic>
	<monogr>
		<title level="j">Consciousness and Cognition</title>
		<imprint>
			<biblScope unit="page" from="178" to="187" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">K</forename><surname>Kowsari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">J</forename><surname>Meimandi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Heidarysafa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mendu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">E</forename><surname>Barnes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Brown</surname></persName>
		</author>
		<idno type="DOI">10.3390/info10040150</idno>
		<idno type="arXiv">arXiv:1904.08067[cs,stat]</idno>
		<idno>arXiv: 1904.08067</idno>
		<ptr target="http://arxiv.org/abs/1904.08067.doi:10.3390/info10040150" />
		<title level="m">Text Classification Algorithms: A Survey</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">A Comparative Study of Embeddings Methods for Hate Speech Detection from Tweets</title>
		<author>
			<persName><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Waseem</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName><forename type="first">J.-P</forename><surname>Fauconnier</surname></persName>
		</author>
		<ptr target="http://fauconnier.github.io" />
		<title level="m">French word embeddings</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">CamemBERT: a tasty French language model</title>
		<author>
			<persName><forename type="first">L</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Ortiz Su√°rez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Dupont</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Romary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">√â</forename><surname>De La Clergerie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Seddah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Sagot</surname></persName>
		</author>
		<ptr target="https://www.aclweb.org/anthology/2020.acl-main.645" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics, Association for Computational Linguistics<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="7203" to="7219" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">FlauBERT: Unsupervised language model pre-training for French</title>
		<author>
			<persName><forename type="first">H</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Vial</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Frej</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Segonne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Coavoux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Lecouteux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Allauzen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Crabb√©</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Besacier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Schwab</surname></persName>
		</author>
		<ptr target="https://www.aclweb.org/anthology/2020.lrec-1.302" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th Language Resources and Evaluation Conference, European Language Resources Association</title>
		<meeting>the 12th Language Resources and Evaluation Conference, European Language Resources Association<address><addrLine>Marseille, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="2479" to="2490" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Asynchronous pipelines for processing huge corpora on medium to low resource infrastructures</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Ortiz Su√°rez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Sagot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Romary</surname></persName>
		</author>
		<idno type="DOI">10.14618/ids-pub-9021</idno>
		<ptr target="http://nbn-resolving.de/urn:nbn:de:bsz:mh39-90215.doi:10.14618/ids-pub-9021" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Challenges in the Management of Large Corpora (CMLC-7) 2019</title>
		<meeting>the Workshop on Challenges in the Management of Large Corpora (CMLC-7) 2019<address><addrLine>Cardiff; Mannheim</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-07-22">22nd July 2019. 2019</date>
			<biblScope unit="page" from="9" to="16" />
		</imprint>
		<respStmt>
			<orgName>Leibniz-Institut f√ºr Deutsche Sprache</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Neural machine translation of rare words with subword units</title>
		<author>
			<persName><forename type="first">R</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Birch</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P16-1162</idno>
		<ptr target="https://www.aclweb.org/anthology/P16-1162.doi:10.18653/v1/P16-1162" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1715" to="1725" />
		</imprint>
	</monogr>
	<note>Long Papers</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">SentencePiece: A sim-ple and language independent subword tokenizer and detokenizer for neural text processing</title>
		<author>
			<persName><forename type="first">T</forename><surname>Kudo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Richardson</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D18-2012</idno>
		<ptr target="https://www.aclweb.org/anthology/D18-2012.doi:10.18653/v1/D18-2012" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, Association for Computational Linguistics</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, Association for Computational Linguistics<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="66" to="71" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
