<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning Rich Event Representations and Interactions for Temporal Relation Classification</title>
				<funder ref="#_Qs8HMeg">
					<orgName type="full">CPER Nord-Pas de Calais/FEDER</orgName>
				</funder>
				<funder ref="#_JTGPeYU">
					<orgName type="full">Agence Nationale de la Recherche</orgName>
					<orgName type="abbreviated">ANR</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Onkar</forename><surname>Pandit</surname></persName>
							<email>onkar.pandit@inria.fr</email>
							<affiliation key="aff0">
								<orgName type="department">MAGNET</orgName>
								<orgName type="institution">Inria Lille -Nord Europe</orgName>
								<address>
									<settlement>Villeneuve d&apos;Ascq</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Pascal</forename><surname>Denis</surname></persName>
							<email>pascal.denis@inria.fr</email>
							<affiliation key="aff0">
								<orgName type="department">MAGNET</orgName>
								<orgName type="institution">Inria Lille -Nord Europe</orgName>
								<address>
									<settlement>Villeneuve d&apos;Ascq</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Liva</forename><surname>Ralaivola</surname></persName>
							<email>liva.ralaivola@lif.univ-mrs.fr</email>
							<affiliation key="aff1">
								<orgName type="department">LIS</orgName>
								<orgName type="laboratory">QARMA</orgName>
								<orgName type="institution" key="instit1">IUF</orgName>
								<orgName type="institution" key="instit2">Aix-Marseille University</orgName>
								<orgName type="institution" key="instit3">CNRS</orgName>
								<address>
									<settlement>Marseille</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Criteo AI Labs</orgName>
								<address>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Learning Rich Event Representations and Interactions for Temporal Relation Classification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">CC0773165EC537B19220CB376F38E8CD</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-04-12T14:46+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Most existing systems for identifying temporal relations between events heavily rely on hand-crafted features derived from event words and explicit temporal markers. Besides, less attention has been given to automatically learning contextualized event representations or to finding complex interactions between events. This paper fills this gap in showing that a combination of rich event representations and interaction learning is essential to more accurate temporal relation classification. Specifically, we propose a method in which i) Recurrent Neural Networks (RNN) extract contextual information ii) character embeddings capture morphosemantic features (e.g. tense, mood, aspect), and iii) a deep Convolutional Neural Network (CNN) finds out intricate interactions between events. We show that the proposed approach outperforms most existing systems on the commonly used dataset while using fully automatic feature extraction and simple local inference.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Recovering temporal information from texts is a crucial part of language understanding, and it has applications such as question answering, summarization, etc.</p><p>Temporal relation identification is divided into two main tasks, as identified by Tem-pEval campaigns <ref type="bibr" target="#b0">[1]</ref>: i) the identification of EVENTs and other time expressions (the so-called TIMEX's), and ii) the classification of temporal relations (or TLINKs) among and across events and time expressions <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5]</ref>. Possible relations for this latter task include temporal precedence (i.e., BEFORE or AFTER), inclusion (i.e., INCLUDES or IS INCLUDED), and others inspired from Allen's algebra. In this work, we concentrate on temporal relation classification, specifically EVENT-EVENT relations.</p><p>The problem becomes harder in the absence of explicit temporal connectives (e.g., before, during), determining temporal relations depends on numerous factors, ranging from tense and aspect to lexical semantics and even world knowledge. To address this issue, most state-of-the-art systems for EVENT-EVENT classification <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b4">5]</ref> rely on manually-crafted feature sets directly extracted from human annotations, complemented with syntactic features, and semantic features extracted from static knowledge bases like WordNet or VerbOcean. Such an approach is tedious, error-prone and the semantics of events is poorly modelled due to lack of coverage of existing lexical resources and blindness to event contexts.</p><p>We here propose a radically different approach where we altogether dispense with hand-designed features and instead learn task-specific event representations. These representations include information both from the event words and its surrounding context, thus giving access to the events' arguments and modifiers. Furthermore, character based model capture another type of information captured by morphology such as tense and aspect of event. Plus, we also attempt to learn the potentially rich interactions between events. Concretely, our learning framework, as depicted in Fig. <ref type="figure" target="#fig_0">1</ref>, is based on a neural network architecture, wherein: i) Recurrent Neural Network (RNN) is employed to learn contextualized event representations ii) character embeddings is used to capture morphological information, hence encoding tense, mood and aspect information, and iii) a deep Convolutional Neural Network (CNN) architecture is then used to acquire complex, non-linear interactions between these representations.  This is one important step up from the recent work of Mirza and Tonelli <ref type="bibr" target="#b3">[4]</ref>, which simply uses pretrained word embeddings for event words and still have to resort to additional hand-engineered features to achieve good temporal classification accuracy. We show our system based on fully automatic feature extraction and interaction learning outperforms other local classifier systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Recent temporal classification systems use machine learning techniques due to the availability of annotated datasets. Earlier work <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b5">6]</ref> studied local models (i.e., making pairwise decisions on pairs of events) and used gold-standard features extracted from TimeML annotations. State-of-the-art local models such as ClearTK <ref type="bibr" target="#b6">[7]</ref> relied on an enlarged set of predicted features, and classifiers. These local models may generate globally incoherent temporal relations, in the sense that the symmetry and transitivity holding between relations are not followed at the document level. This has led to development of global models <ref type="bibr" target="#b2">[3]</ref>. The state-of-the-art method <ref type="bibr" target="#b4">[5]</ref> proposes a structured prediction approach.</p><p>These methods all rely on manually engineered features, which fail to model the semantics of events. To address this issue, <ref type="bibr" target="#b3">[4]</ref> have evaluated the effectiveness of pretrained word embeddings of event head-word. They also demonstrated the potency of basic vector combination schemes. However, representing events with word embeddings of only its head-word is not effective and important contextual information is lost. Recently proposed LSTM-based neural network architectures <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9]</ref> learn event representation with use of event head word as well as context. Also, newly proposed method <ref type="bibr" target="#b9">[10]</ref> have shown efficacy of context with use of gated RNN-attention based neural architecture. However, by using only word embeddings they fail to capture inflectional morphology of event head word, which includes crucial linguistic information such as tense, aspect, and mood. Also they lacked in finding complicated interaction between events and relied only on concatenation of event features. Moreover they <ref type="bibr" target="#b8">[9]</ref> used syntactically parsed trees as inputs to the LSTM which adds burden of pre-processing.</p><p>Our proposed neural architecture (Fig. <ref type="figure" target="#fig_0">1</ref>), consists of two main components: Representation Learning and Interaction Learning. In the Representation Learning part, a bag-of-words in a fixed size window centred on each event word is fetched and fed into a RNN to get more expressive and compact representation of events. Output of the RNN is concatenated with character embedding of event head-word to get the final vector for each event. This vector representation is then used at the Interaction Learning stage: the vector representation of each event is fed to a convolution layer and the final pooling layer outputs an interaction vector between the events. A dense layer is used to combine the interaction vector before obtaining a probability score for each relation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Representation Learning</head><p>Context-aware Representation Each word is encoded in the event-word window with word embeddings <ref type="bibr" target="#b10">[11]</ref>. As a result, each word is assigned a fixed d-dimensional vector representation. Let c l be the context length for each event head word w t . Thus we consider a window of 2c l + 1 words as input to the RNN. We represent this as ma-</p><formula xml:id="formula_0">trix W = [w t-c l • • • w t • • • w t+c l ] ∈ R (2c l +1</formula><p>)×d . Note that while considering event context we stop at sentence boundary, also special symbols are padded if context is less than c l . The relation between input and output of RNN at each time t is given as follows,</p><formula xml:id="formula_1">h t = σ h (Q h w t + U h h t-1 + b h )<label>(1)</label></formula><formula xml:id="formula_2">o t = σ o (Q o h t + b o )<label>(2)</label></formula><p>where, w t is the word embedding vector provided at each time step, h t is hidden layer vector and o t is output vector. Q, U , and b are weight matrices and vector; σ h and σ o are activation ReLU functions. For a given event, the o t+c l output vector captures a complete information about the whole sequence. The outputs of the RNN networks give compact representations O A and O B of the events.</p><p>Morphological Representation Semantics and arguments of events are captured with context-aware representation but it doesn't capture morphological information such as tense and aspect. For instance, tense information is captured from context as well with auxiliary verbs (will, is). However in the absence of these words, tense information of event is expressed in inflectional suffixes such as -ed, -ing. To obtain this information, event head word w t is represented as a sequence of character n-grams c 1 , • • • , c m where m is number of n-grams in word. Fixed d c dimension vector is learned for each n-gram. Eventually morphological representation of word is obtained by adding vectors of n-grams present in the word. Standard fasttext <ref type="bibr" target="#b12">[13]</ref> method is used to get this representation. Context-aware vector O A and character embedding vector S A are concatenated to obtain final event representation as E A , similarly E B also obtained for event B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Interaction Learning</head><p>A deep Convolution Neural Network (CNN) is employed to learn nonlinear interactions from E A and E B . It is comprised of three convolution and pooling layers placed alternatively. We feed concatenated learned event representations</p><formula xml:id="formula_3">E AB = E A ⊕ E B<label>(3)</label></formula><p>to the first convolution layer, where ⊕ is the concatenation operation. Each convolution layer i use filters f i , after what we compute a feature map</p><formula xml:id="formula_4">m k i = σ(f i .E AB + b i ) ∀k ∈ {1, 2, 3},<label>(4)</label></formula><p>where f i , b i are filters and bias matrices respectively and σ is the ReLU activation. The output is down-sampled with a max-pooling layer to keep prominent features intact. The output of the last layer gives the interaction between A and B (ρ is max-pooling).</p><formula xml:id="formula_5">E comb = ρ(m 3 i ) (5)</formula><p>The combined E comb vector is fed to a fully connected dense layer, followed by a softmax function to get a probability score for each temporal relation class.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets and Evaluation</head><p>Relations Following recent work <ref type="bibr" target="#b4">[5]</ref>, reduced set of temporal relations: after, before, includes, is included, equal, vague are considered for classification.</p><p>Evaluation Complying with common practice, system's performance is measured over gold event pairs (pairs for which relation is known). Our main evaluation measure is the Temporal Awareness metric <ref type="bibr" target="#b11">[12]</ref>, adopted in recent TempEval campaigns. We also used standard precision, recall, and F1-score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Datasets</head><p>We used TimeBank (TB) and AQUAINT (AQ) dataset for training, TimeBank-Dense(TD) for development and Platinum (TE3-PT) dataset for testing. These are the most popular datasets used for the task which have been provided at TempEval3 <ref type="bibr" target="#b0">[1]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Training Details</head><p>We used pre-trained Word2Vec vectors from Google<ref type="foot" target="#foot_0">1</ref> . Each word in the context window of event is represented with this 300-dimension vector. Character embeddings for event head word are obtained from fasttext <ref type="bibr" target="#b12">[13]</ref> which is also 300-dimension vector. Note that only one RNN is trained with weight sharing to learn representation. Hyperparameters were tuned on the development set using a simple grid search. Considered values are: window size (c l : 3,4,5), number of neurons at RNN (#RNN: 64,128,256,512), number of filters for CNN (#filters: 32,64,128,256), dropout at input (0.1,0.2,0.3,0.4). We also explored a number of optimization algorithms such as AdaDelta, Adam, RMSProp and Stochastic Gradient Descent(SGD). Optimal hyper-parameter values are c l = 4,#RNN =256, #filters = 64, dropout = 0.4 and Adam optimizer. <ref type="foot" target="#foot_1">2</ref> Once we got the optimal parameter values from the validation set, we re-trained multiple models with 50 different seed values on the combined training and development data and report the averaged test performances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Results</head><p>We first compare our RNN-Deep CNN approach to various baseline systems to assess the effectiveness of the learned event representations. We also want to disentangle the respective role of the representation learning and interaction learning.</p><p>Baseline systems As Mirza and Tonelli <ref type="bibr" target="#b3">[4]</ref> reported results only with pairwise fscore on different dataset, we re-implemented their system with scikit-learn Logistic Comparison to Baseline Systems Sections a and b of Table <ref type="table" target="#tab_1">1</ref> summarize the performance of these different systems in terms of pairwise classification accuracy and temporal awareness scores. Looking at the first two rows of the table, we see that, as hypothesized, contextually rich features outperform pre-trained event head word embeddings when combined with simple concatenation, both in pairwise classification and in temporal awareness. A further gain in the performance with character embeddings shows the effectiveness of morphological features. Results from the section establish the effectiveness of our rich representation learning. In section b of the table, we present results of the system with different interaction learning.The system with MLP interaction learning outperforms simple concatenation (E A ⊕ E B ) system, demonstrating importance of interaction learning. Leveraging both rich event representations learning and non-linear interaction learning yield the best scores overall, cf. CN N (E A , E B ) and DCN N (E A , E B ), which shows their complementarity. There, the Deep CNN outperforms the single-layer CNN, with F1 scores of 53.4 and 49.2, respectively. This confirms the importance of non-linear interaction learning in the task.</p><p>Comparison with State-of-the-art Finally, in the section c, we compare the performance of our best system, DCN N (E A , E B ), with recently proposed systems : W A ⊕ W B <ref type="bibr" target="#b3">[4]</ref>, ClearTK <ref type="bibr" target="#b6">[7]</ref>, which was the winner of the TempEval 2013 campaign, the structured prediction (SP) approach <ref type="bibr" target="#b4">[5]</ref>, which is the best system to date and recently proposed LSTM <ref type="bibr" target="#b13">[14]</ref>. Our system delivers substantial improvements over W A ⊕ W B , ClearTK, and LSTM based system. However our system lags in comparison with SP as it relies only on simple local inference opposed to global inference at learning step, also totally dispenses with hand-crafted features. It is important to observe that our system closes the performance gap between SP system and automatic feature learned systems.</p><p>In this work, we proposed RNN based neural architecture to learn event representation and CNN model to get interaction between events. A new perspective towards the combination of events proved to be effective in getting compound interactions. We compared result of our system with multiple baselines and state-of-the art systems and shown effectiveness. We now plan to learn features and interaction while considering global consistency in the relations of event pairs.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Fig. 1: Architecture of our proposed model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Results of baseline and state-of-the-art systems regression module, using l 2 regularization (noted W A ⊕ W B ). Word embeddings W A and W B of events A and B obtained from Word2Vec were simply concatenated (as this is their best performing system). We conducted number of experiments to obtain optimal parameters and reported best performing system's results. We got lower than the originally reported results as the dataset is sparsely annotated compared to the one in the paper (TimebankDense). As additional baselines, we used our Representation Model to learn O A and O B , subsequently E A , E B , but combined these vectors with simple concatenation (O A ⊕ O B and E A ⊕ E B ). In another setting learned representation is combined with multi-layer perceptron (MLP) and single-layer convolution (CNN).</figDesc><table><row><cell></cell><cell>Systems</cell><cell cols="3">Pair Classification</cell><cell cols="2">Temporal Awareness</cell></row><row><cell></cell><cell></cell><cell>P</cell><cell>R</cell><cell>F1</cell><cell>P</cell><cell>R</cell><cell>F1</cell></row><row><cell></cell><cell>W A ⊕ W B</cell><cell cols="3">39.3 34.2 35.5</cell><cell cols="2">27.1 45.8</cell><cell>34.1</cell></row><row><cell>(a)</cell><cell>O A ⊕ O B</cell><cell cols="3">35.7 38.9 37.2</cell><cell cols="2">36.5 35.9</cell><cell>36.2</cell></row><row><cell></cell><cell>E A ⊕ E B</cell><cell cols="3">37.6 44.5 40.8</cell><cell cols="2">41.2 45.9</cell><cell>43.4</cell></row><row><cell></cell><cell>M LP (E A , E B )</cell><cell cols="3">40.2 46.3 43.0</cell><cell cols="2">51.8 42.5</cell><cell>46.7</cell></row><row><cell>(b)</cell><cell>CN N (E A , E B )</cell><cell cols="3">38.2 53.7 44.6</cell><cell cols="2">41.7 60.1</cell><cell>49.2</cell></row><row><cell></cell><cell>DCN N (E A , E B )</cell><cell cols="3">39.4 58.9 47.2</cell><cell cols="2">43.2 70.1</cell><cell>53.4</cell></row><row><cell></cell><cell>ClearTK</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="2">33.1 35.0</cell><cell>34.1</cell></row><row><cell>(c)</cell><cell>LSTM</cell><cell cols="3">38.7 43.1 40.5</cell><cell cols="2">34.6 51.7</cell><cell>41.4</cell></row><row><cell></cell><cell>SP</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="2">69.1 65.5</cell><cell>67.2</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>https://code.google.com/archive/p/word2vec/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>We tried different unidirectional and bidirectional variations of RNN-LSTM and GRU, but RNN gave the best development results.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgement</head><p>This work was supported by <rs type="funder">ANR</rs> Grant <rs type="projectName">GRASP</rs> No. <rs type="grantNumber">ANR-16-CE33-0011-01</rs>, as well as by a grant from <rs type="funder">CPER Nord-Pas de Calais/FEDER</rs> <rs type="programName">DATA Advanced data science</rs> and technologies 2015-2020.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_JTGPeYU">
					<idno type="grant-number">ANR-16-CE33-0011-01</idno>
					<orgName type="project" subtype="full">GRASP</orgName>
				</org>
				<org type="funding" xml:id="_Qs8HMeg">
					<orgName type="program" subtype="full">DATA Advanced data science</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">N</forename><surname>Uzzaman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Llorens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Derczynski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Allen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Verhagen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pustejovsky</surname></persName>
		</author>
		<title level="m">Semeval-2013 task 1: Tempeval-3: Evaluating time expressions, events, and temporal relations</title>
		<imprint>
			<publisher>ACL</publisher>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Machine learning of temporal relations</title>
		<author>
			<persName><forename type="first">I</forename><surname>Mani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Verhagen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wellner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pustejovsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
			<publisher>ACL</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Dense event ordering with a multi-pass architecture</title>
		<author>
			<persName><forename type="first">N</forename><surname>Chambers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Cassidy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mcdowell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bethard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TACL</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">On the contribution of word embeddings to temporal relation classification</title>
		<author>
			<persName><forename type="first">P</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tonelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A structured learning approach to temporal relation extraction</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Ning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Classifying temporal relations between events</title>
		<author>
			<persName><forename type="first">Nathanael</forename><surname>Chambers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACL</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">ClearTK-TimeML: A minimalist approach to tempeval</title>
		<author>
			<persName><forename type="first">S</forename><surname>Bethard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACL</title>
		<imprint>
			<date type="published" when="2013">2013. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Neural temporal relation extraction</title>
		<author>
			<persName><forename type="first">D</forename><surname>Dligach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bethard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Savova</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<publisher>ACL</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Classifying temporal relations by bidirectional lstm over dependency paths</title>
		<author>
			<persName><forename type="first">F</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Miyao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACL</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Context-aware neural model for temporal information extraction</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rumshisky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACL</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Temporal evaluation</title>
		<author>
			<persName><forename type="first">N</forename><surname>Uzzaman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Allen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
			<publisher>ACL</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Advances in pre-training distributed word representations</title>
		<author>
			<persName><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Puhrsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">LREC</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Temporal information extraction for question answering using syntactic dependencies in an lstm-based architecture</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rumshisky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Romanov</surname></persName>
		</author>
		<idno>CoRR, abs/1703.05851</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
