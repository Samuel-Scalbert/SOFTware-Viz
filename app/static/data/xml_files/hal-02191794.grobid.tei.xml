<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main"></title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">C19A02A90018497CBDF664ACF8C03FBA</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-04-12T14:44+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>A data stream is an ordered sequence of data elements that are made available over time, and is potentially unlimited. Very interesting data stream applications can be found in finance, where the life of a company evolves with time and, at each time instant, is characterized by different streams of data reporting its stock price, exchange volumes, balance data, and so on. Companies may even appear and disappear from the market, or change their financial behavior.</p><p>Several techniques for analyzing streaming data have been studied: many rely on the use of a sliding window, i.e. a model of the data based on a window that moves over time, continuously taking into account the newest data. Also, in the financial context, a variety of clustering methods have been proposed to cluster companies that have the same behavior, w.r.t. prices or other measures.</p><p>However, when stock prices are concerned, clusterings done on different windows -even though these might overlap greatly -tend to change a lot, due to the high dynamics of the market and to the scarce permanence of relationships between companies over time. In such a scenario the model tends to fluctuate and remains meaningful only for short amounts of time.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Evolution of Financial Time Series Clusters (Discussion Paper)</head><p>Davide Azzalini 1 , Fabio Azzalini 1 , Mirjana Mazuran 2 , and Letizia Tanca 1 1 Politecnico di Milano, Italy {name.surname}@polimi.it 2 Inria, France mirjana.mazuran@inria.fr</p><p>Abstract. Nowadays, a huge amount of applications exist that natively adopt a data-streaming model to represent highly dynamic phenomena.</p><p>A challenging application is constituted by data from the stock market, where the stock prices are naturally modeled as data streams that fluctuate very much and remain meaningful only for short amounts of time.</p><p>In this paper we present a technique to track evolving clusters of financial time series, with the aim of constructing reliable models for this highly dynamic application. In our technique the clustering over a set of time series is iterated over time through sliding windows and, at each iteration, the differences between the current clustering and the previous one are studied to determine those changes that are "significant" with respect to the application. For example, in the financial domain, if a company that has belonged to the same cluster for a certain amount of time moves to another cluster, this may be a signal of a significant change in its economical or financial situation.</p><p>We present PETRA, a Progressive clustEring TRAcker where the clustering analysis is iterated over time in order to constantly provide a reliable, stable, up-to-date and consistent model. PETRA also focuses on identifying "significant" changes in the clusterings constructed over time by means of parameters that allow the system to adapt to specific application domains. When applying PETRA to the case of stock prices, we can capture the intrinsic dynamics of the stocks' (and companies') relationships, through the recognition of points in time where their co-movements change, that is, we identify changes in the behavior of a company relatively to the behavior of the other companies of the same cluster. A company changing cluster is a signal to present to investors as well as a building block for more accurate prediction tasks. For example, well-known companies communicate more than small ones, however, small companies are often more interesting for investors since they can bring a higher profit. After clustering we can use the rich information available for the well-known companies in a cluster to obtain some guidelines for the small ones in the same cluster.</p><p>PETRA is a part of a broader project: Mercurio <ref type="bibr" target="#b5">[6]</ref>, whose aim is to support financial investors in their decision process. In particular, PETRA helps investors understand how companies influence each other, for a deeper understanding of the market structure. The system has been designed and optimized for the financial application domain; however, it can be applied to any set of time series with rapidly evolving behaviors, provided that the clustering algorithm, the similarity metrics and some other parameters are appropriately tuned. Figure <ref type="figure" target="#fig_0">1</ref> depicts an example of stock-based companies' clustering. Most previous works on clustering stock-market firms showed how custom-made clusters outperform traditional industrial grouping criteria <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14]</ref>. Our work differs from them in that we move from a static to a progressive clustering approach, which allows us to overcome The big problem found when dealing with financial information: stock markets move fast, financial insights expire and get old very quickly. Our progressive clustering provides fresh suggestions daily.</p><p>Cluster Tracking in evolving data streams is partially covered by the scientific community. A lot has been done on visually tracking groups of objects <ref type="bibr" target="#b6">[7]</ref>, e.g., of moving people <ref type="bibr" target="#b7">[8]</ref> or facial features <ref type="bibr" target="#b8">[9]</ref>, and on wireless channel modeling to track multi-path components <ref type="bibr" target="#b9">[10]</ref>. Our work is different because we are not interested in tracking the shift in position of a cluster at consecutive time instants; rather, we focus on tracking the membership of each object.</p><p>General-purpose techniques <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12]</ref> exist that face the cluster-tracking problem in terms of outliers, i.e. the clustering is considered valid until the number of outliers crosses a predefined threshold, then re-clustering is needed. In PETRA instead, we re-cluster at each iteration since we are not just interested in having a consistent model but also in detecting as soon as possible the entities that change cluster, and promptly report it.</p><p>MONIC <ref type="bibr" target="#b14">[15]</ref> also models and monitors cluster transitions. However, PETRA tracks the movements of the companies between clusters in order to generate signals about each company, while MONIC is interested in the transition of the entities between clusters with the scope of drawing conclusion on the lifetime and stability of the clusters. Moreover, MONIC detects the transitions by keeping track of the entity trajectories on spatiotemporal coordinates, while PETRA detects movements by analyzing the contents of the clusters.</p><p>Our objective is to dynamically track the evolution of a set of entities E = (e 1 , e 2 , ..., e j , ..., e N ) by iteratively clustering their data streams over a sliding window W i = [t i-w , t i ] of width w. Figure <ref type="figure" target="#fig_0">1</ref> depicts one such cluster and shows how related companies appear inside the cluster. At each time t i , the sliding window shifts one time-point ahead, and its content is clustered in K clusters</p><formula xml:id="formula_0">C i = (C i 1 , C i 2 , ..., C i K ).</formula><p>Then, C i is compared with C i-1 by associating each new cluster with its "previous version". At the end of the procedure PETRA identifies those entities that have made a "persistent" change of cluster <ref type="foot" target="#foot_0">3</ref> . In particular, our method is aimed at clustering time series that: (i) represent entities belonging to the same domain (e.g. stock-market prices of different companies) and are enough to be grouped into clusters; (ii) are sampled at regular intervals, with the same sampling frequency; (iii) contain a number of time points sufficient to allow a sliding window to move along a significant number of positions<ref type="foot" target="#foot_1">4</ref> . Summarizing, our main contributions are: (i) the identification of an approach to generate clusters that are well-balanced w.r.t. the problem under consideration; (ii) the design of a method to trace clusters' behaviors and spot entities' changes of membership; (iii) the definition of a set of rules needed to identify relevant entity switches. Section 2 provides the details about our approach while in Section 3 we show the experimental results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">The PETRA Method</head><p>We first discuss our design choices and some domain-dependent parameters that need to be set in PETRA, then concentrate on the clustering procedure, and finally show how we track the clusters along time.</p><p>Time series clustering algorithms rely on similarity measures for quantifying how homogeneous two observations are. PETRA uses similarity in time, which is based on comparing different time series at the same time points, as it is the most suitable one to identify sudden changes in the streams' behaviors.</p><p>Davide Azzalini, Fabio Azzalinir, Mirjana Mazuran, and Letizia Tanca finally show how we track the clusters along time. The description follows the steps of Algorithm 1.</p><formula xml:id="formula_1">K desired number of clusters; i 0; repeat CORR = computeCorrelationMatrix(Wi) D = computeDistanceMatrix(CORR) C i =clusterData(D, K) forall change 2 trackClusters(C i , C i 1 ) do if change is relevant then trigger warning; end end i i + 1; until new data keeps coming;</formula><p>Algorithm 1: The PETRA procedure</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Design choices and parametrizations</head><p>Time series clustering algorithms rely on similarity measures for quantifying how homogeneous two observations are. PETRA uses similarity in time, which is based on comparing di↵erent time series at the same time points, as it is the most suitable one to identify sudden changes in the streams' behaviors.</p><p>The similarity measures that can be adopted for clustering time series <ref type="bibr" target="#b0">[1]</ref> range from simple ones such as Euclidean distance to more complex ones like Dynamic Time Warping (DTW). DTW is extremely powerful, as it finds a relationship between two time series S 1 and S 2 not only when S 2 is obtained by shifting S 1 in time, but also when S 2 is a contraction or an expansion of S 1 ; e.g. DTW associates the last week of a company with the last month's behavior of another company. However, this feature is not needed in the financial case, where we need to compare companies on the same time interval and at the same time point, thus, simpler approaches, such as the Euclidean metrics and metrics based on correlation, are su ciently expressive.</p><p>As for the clustering method, PETRA employs hierarchical clustering, since it does not require to set the number of clusters a-priori, and at the same time is well suited for easy visualization. Using a Euclidean distance and hierarchical clustering allows us to use Ward's method for linkage <ref type="bibr" target="#b1">[2]</ref>, which, as we will see, provides a big advantage in the tracking phase. At each iteration, the Pearson Correlation corr ij <ref type="bibr">[18]</ref> between each pair of time series i and j is computed and a global similarity matrix CORR is obtained. Then, to obtain a suitable input for a clustering algorithm, a transformation from correlation coe cients to distances is needed. According to Gower and Legendre <ref type="bibr" target="#b2">[3]</ref>, if CORR is a positive semidefinite similarity matrix, the dissimilarity matrix D where</p><formula xml:id="formula_2">d ij = p (1 corr ij ) is a</formula><p>The similarity measures that can be for clustering time ries <ref type="bibr" target="#b0">[1]</ref> range from simple ones such as Euclidean distance to more complex ones like Dynamic Time Warping (DTW). DTW is extremely powerful, as it finds a relationship between two time series S 1 and S 2 not only when S 2 is obtained by shifting S 1 in time, but also when S 2 is a contraction or an expansion of S 1 ; e.g. DTW associates the last week of a company with the last month's behavior of another company. However, this feature is not needed in the financial case, where we need to compare companies on the same time interval and at the same time point, thus, simpler approaches, such as the Euclidean metrics and metrics based on correlation, are sufficiently expressive.</p><p>As for the clustering method, PETRA employs hierarchical clustering, since it does not require to set the number of clusters a-priori, and at the same time is well suited for easy visualization. Using the Euclidean distance and hierarchical clustering allows us to use Ward's method for linkage <ref type="bibr" target="#b1">[2]</ref>, which, as we will see, provides a big advantage in the tracking phase. At each iteration, the Pearson Correlation corr ij <ref type="bibr" target="#b15">[16]</ref> between each pair of time series i and j is computed and a global similarity matrix CORR is obtained. Then, to obtain a suitable input for a clustering algorithm, a transformation from correlation coefficients to distances is needed. According to Gower and Legendre <ref type="bibr" target="#b2">[3]</ref>, if CORR is a positive semidefinite similarity matrix, the dissimilarity matrix D where d ij = (1corr ij ) is a matrix of euclidean distances. Finally, the width w of the sliding window, and the number of desired clusters K, are left as parameters in PETRA since the best choice strictly depends upon the application goal; in Section 3 we present a possible solution to perform an educated guess; we chose a window of 105 days and a number of clusters equal to 4.</p><p>At each new iteration, the distance matrix D is computed for the portions of time series inside the window. Then, the hierarchical clustering using Ward's linkage method is performed, a hierarchy of clusters -represented by means of a dendrogram like in Figure <ref type="figure">2</ref> -is obtained, and the dendrogram is cut in order to obtain K clusters. Ward's method promotes the merges of small clusters, which is crucial to the tracking phase since well-balanced clusters are easier to track. However, depending on the level of the cut, we obtain different numbers of clusters, and a constant-height cut is not suitable in this case: cutting a dendrogram to obtain a fixed number of clusters irrespectively of its general structure is likely to result in highly unstable clusters. Langfelder at al. <ref type="bibr" target="#b3">[4]</ref> solved the problem of sub-optimal performances on complicated dendrograms presenting the Dynamic Tree Cut (DTC) algorithm, which provides a dynamic branch-cutting method for detecting clusters in a dendrogram, depending on their shape. Like in the standard dendrogram creation procedure, clusters are merged in a bottomup fashion, but this merging depends on a set of parametric criteria such as the minimum size of each cluster and the minimum gap between the joining heights. By using DTC we can obtain much more balanced clusters, both in their size and composition, avoiding the downsides of the standard approach and facilitating the tracking phase. Figure <ref type="figure">2</ref> shows how, although both cuts identify three clusters, those obtained with the fixed-height cut are highly imbalanced (i.e. there are two clusters of just one element), while DTC cut delivers very balanced clusters. The goal of the tracking phase is to observe how clusters evolve and how the clustered streams change their membership. To track cluster dynamics, we have to recognize the same clusters at different times without relying on any external reference. We address this problem by using the Jaccard similarity index <ref type="bibr" target="#b15">[16]</ref> and Linear Programming (LP). Given two clusterings C i and C i-1 obtained from the series over two consecutive windows W i and W i-1 , we the Jaccard score of each pair of clusters in the two clusterings. The pairs with highest Jaccard score are the most similar clusters, likely representing the same cluster at consecutive time points. Of course, once a cluster is ascribed to one predecessor it cannot be ascribed to any other one: each cluster is linked to one and only one predecessor. This can easily be formalized in LP. Given two clusterings</p><formula xml:id="formula_3">C i = (C i 1 , C i 2 , ..., C i h , ..., C i K ) and C i-1 = (C i-1 1 , C i-1 2 , ..., C i-1 j , ..., C i-1</formula><p>K ), we define the square matrix M of dimension K as the matrix of the Jaccard scores of all the possible pairs (C i h , C i-1 j ) of clusters. Then we define the square matrix X as a decision variable matrix of dimension K, such that</p><formula xml:id="formula_4">x hj is 1 if Jac(C i h , C i-1 j</formula><p>) in M is chosen by the optimization process, 0 otherwise. The LP model aims to select from M the K values having highest sum, with the strong constraint that each selected value can share neither the row nor the column with the other selected values. This is equivalent to selecting the K cluster pairs with the highest Jaccard scores, without selecting a cluster more than once. The objective function is defined as maximize M • X (where • represents the element-wise matrix product), subject to the constraints:</p><formula xml:id="formula_5">j X hj = 1 ∀h ∈ [1, K], h X hj = 1 ∀j ∈ [1, K], X hj ∈ N ∀h, j ∈ [1, K].</formula><p>At each new iteration PETRA moves its analysis forward of one time point, performs a new clustering and links each new cluster to its "previous version". By doing so, we detect when an entity moves to another cluster. Three types of change are possible:</p><p>-exit: a company exits the Reference Cluster, moving into a different cluster; -re-entry: an entity re-enters the Reference Cluster; -change outside the reference cluster : an entity moves between two "non-Reference Clusters".</p><p>Note that the Reference Clusters strictly depend on the domain under assessment; in our use case the reference clusters turn out to be the industrial sectors.</p><p>Reference clusters are re-computed every time we move the time window, and are assigned the industrial sector of the majority of the companies inside it. Thus, a cluster that contains mostly banks will be assigned the label "Bank", one that contains mostly automotive companies will be assigned "Automotive", and so on. For example, the industrial sector of Unicredit, a famous Italian bank, is "Bank", thus its reference cluster is the one that contains mostly banks. After an iteration, if most of the companies inside Unicredit's cluster are banks, we say that Unicredit is in its reference cluster, and outside its reference cluster otherwise. This allow us to give more importance to those cluster changes that involve the Reference Cluster. Note that, when using a sliding window, the change in the new configuration might be caused by the new point added or by the one that has been removed. Our goal is to detect changes that are caused by current events, thus, PETRA triggers warnings only when a change is caused by the new point added. To do so we use the intersection window</p><formula xml:id="formula_6">W ∩ i = [t i-w , t i-1 ] = W i ∩ W i-1</formula><p>, and the union window</p><formula xml:id="formula_7">W ∪ i = [t i-w-1 , t i ] = W i ∪ W i-1</formula><p>: by computing the clustering also on the intersection window and comparing it with that of W i we can say, in case they are different, that the new time point t i has actually influenced the clustering results. Moreover, by computing the clustering also on W ∪ i and comparing it with that of W i , if no change has happened, we can also say that the removal of the last point t i-w-1 has not contributed to the change, meaning that it is entirely due to the new point added.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experimental Results</head><p>To test and validate PETRA in a real application, we perform a market simulation consisting in the purchase and sale of companies' stocks. This is because, since cluster switches are a latent feature, there is no general ground truth to test against and prove their actual relevance directly. However, as far as the financial domain is concerned, we can use the market gain as an indicator for the goodness of the clustering performed by PETRA.</p><p>We consider the stock market prices of the 40 most highly capitalized companies in the Italian Stock Exchange<ref type="foot" target="#foot_2">5</ref> between 2008 and 2017. There are 2 values for each trading day: the opening price (open) and the closing price (close); we cluster based on the close-open indicator. Among the 40 companies we randomly select some of them as leaders. Leaders are companies whose behaviour we know in advance (i.e. when their price trend changes): for each of them we identify some companies that are "close to the leader" (friends) for which we don't know the future behavior. The simulation consists in buying and selling stocks of the friends depending on the behavior of the leaders. If the clustering is correct, the friends' behavior is similar to the leader's one, hence the simulation will buy and sell the stocks at the right time, resulting overall in a profit.</p><p>For the validation, since no ground truth for the trend changes of a company's stock price is available, and since the state-of-the-art techniques for segmenting time series revealed to be not suitable as they do not lend themselves to a unique parametrization suitable for all the companies, we designed an ad-hoc procedure for identifying the major changes in the price trend of a leader a-posteriori. The procedure divides the price timeline of a company into intervals of up-trend, down-trend and congestion<ref type="foot" target="#foot_3">6</ref> according to the position of the company's price w.r.t. three moving averages over the price <ref type="bibr" target="#b5">[6]</ref>. For each friend, its closeness to the leader is evaluated using PETRA, thus, we first need to appropriately tune the window length and the number of clusters.</p><p>The width of the sliding window is a crucial parameter: a too narrow window would result in an excessive sensitivity providing poor generalization potential, while with a too wide one we risk to miss important events. To identify the best width we analyze each possible width value, that is: (i) for each value n between 2 (the smallest possible window) and 1260 (half of the 2008-2017 period) we divide the companies' time series into segments of width n; (ii) for each n, for each time segment T of length n, and for each pair of companies, we compute the correlation between their segments, and arrange them all into a matrix A T,n ; (iii) we compute the difference between each correlation matrix and the one at the previous time slot; (iv) we compute their variance and then (v) we average all variances together obtaining a single score for each window width n. We chose the value associated with the smallest score corresponding to 105 days. We estimated the number of clusters through knee/elbow analysis, which suggested an optimal number of 4 clusters. After parameter tuning, the simulation picks 3 leaders and then runs PETRA which, at each iteration, adopts a conservative policy:</p><p>-If a leader enters an up-trend phase, pick the 4 closest companies in the same cluster as friends and buy them. -If a leader enters a down-trend phase, sell all its friends.</p><p>-If a friend changes cluster, sell it.</p><p>To constitute valuable evidence we run the simulation ten thousand times over different portfolios of leaders and then averaged the profits. Comparing the results to a stock market index is the common practice when it comes to market simulation, thus, we use the FTSE MIB index as a baseline; Figure <ref type="figure">3</ref> reports the aggregated results for each year by showing the average profits obtained with PETRA and the FTSE MIB profits for the same year. The profits yielded by our procedure are always positive: even in years like 2008, when the market underwent a severe crisis, our procedure generated profits. Although there are certain years in which FTSE MIB outperforms our approach, our results remains still very comparable, especially when considered on the long term.</p><p>The traditional approach <ref type="bibr" target="#b4">[5]</ref> to validating Stock Market Clustering is to check if the resulting clusters are consistent w.r.t. the business sectors. Being the industrial sectors static entities, we decided not to use this approach for the validation since the aim of our procedure is to dynamically spot anomalies and changes. Nevertheless our clusters have proved to be consistent also under this point of view. The four clusters identified by our procedure coincide with the sectors: banking, energy, consumer products and industrial products. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. A cluster of similarly-behaving companies</figDesc><graphic coords="3,273.09,404.46,207.50,136.10" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>6 Fig. 2 .</head><label>62</label><figDesc>Fig. 2. Fixed-height versus Dynamic Tree Cut</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_0"><p>Note that some changes might be less interesting because being temporary, in the sense that one stream might return to a previous cluster after some time.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_1"><p>Note that the term significant is strongly domain-dependent and should be judged by the designer.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_2"><p>https://en.wikipedia.org/wiki/FTSE MIB</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_3"><p>A period in which the price undergoes oscillations without a clear direction.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>We presented a technique to cluster time series where the clustering is iterated over time and, at each iteration, the differences between the current clustering and the previous one are studied to spot significant changes.</p><p>We are grateful to <rs type="person">U. P. De Vos</rs> and to <rs type="person">L. Raimondi</rs> for taking part in the development of the algorithms and the analysis of the results.</p></div>
			</div>			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Time-series clustering-A decade review</title>
		<author>
			<persName><forename type="first">S</forename><surname>Aghabozorgi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Shirkhorshidi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">Y</forename><surname>Wah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Systems</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Hierarchical grouping to optimize an objective function</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Ward</surname><genName>Jr</genName></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American statistical association</title>
		<imprint>
			<date type="published" when="1963">1963</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Metric and Euclidean properties of dissimilarity coefficients</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Gower</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Legendre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of classification</title>
		<imprint>
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Defining clusters from a hierarchical cluster tree: the Dynamic Tree Cut package for R</title>
		<author>
			<persName><forename type="first">P</forename><surname>Langfelder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Horvath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Mining the stock market: Which measure is best?</title>
		<author>
			<persName><forename type="first">M</forename><surname>Gavrilov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Indyk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Motwani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGKDD</title>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<author>
			<persName><forename type="first">D</forename><surname>Azzalini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Azzalini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Greco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mazuran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Tanca</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Event Recognition Strategies applied in the Mercurio Project</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Extended Object Tracking: Introduction, Overview and Applications</title>
		<author>
			<persName><forename type="first">K</forename><surname>Granstrom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Baum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Reuter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Advances in Information Fusion</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Tracking groups of people</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Mckenna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jabri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Duric</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rosenfeld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wechsler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer vision and image understanding</title>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A robust incremental clustering-based facial feature tracking</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">N</forename><surname>Islam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Seera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">K</forename><surname>Loo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Soft Computing</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Comparison of Automatic Tracking and Clustering Algorithms for Time-Variant Multipath Components</title>
		<author>
			<persName><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Globecom Workshops</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Requirements for clustering data streams</title>
		<author>
			<persName><forename type="first">D</forename><surname>Barbará</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGKDD Explorations Newsletter</title>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Tracking Clusters in Evolving Data Sets</title>
		<author>
			<persName><forename type="first">D</forename><surname>Barbará</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">FLAIRS Conference</title>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Improved forecasting through the design of homogeneous groups</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">J</forename><surname>Edwin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Gruber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Business</title>
		<imprint>
			<date type="published" when="1971">1971</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Stock market co-movement assessment using a threephase clustering method</title>
		<author>
			<persName><forename type="first">S</forename><surname>Aghabozorgi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">Y</forename><surname>Wah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Expert Systems with Applications</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Monic: modeling and monitoring cluster transitions</title>
		<author>
			<persName><forename type="first">M</forename><surname>Spiliopoulou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Ntoutsi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Theodoridis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Schult</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGKDD</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kamber</surname></persName>
		</author>
		<title level="m">Data mining: concepts and techniques</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
