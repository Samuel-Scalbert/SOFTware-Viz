<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Impact of Injecting Ground Truth Explanations on Relational Graph Convolutional Networks and their Explanation Methods for Link Prediction on Knowledge Graphs</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Nicholas</forename><surname>Halliwell</surname></persName>
							<email>nicholas.halliwell@inria.fr</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Inria</orgName>
								<orgName type="institution" key="instit2">UCA</orgName>
								<orgName type="institution" key="instit3">CNRS</orgName>
								<orgName type="institution" key="instit4">I3S</orgName>
								<address>
									<settlement>Sophia Antipolis</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Fabien</forename><surname>Gandon</surname></persName>
							<email>fabien.gandon@inria.fr</email>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">Inria</orgName>
								<orgName type="institution" key="instit2">UCA</orgName>
								<orgName type="institution" key="instit3">CNRS</orgName>
								<orgName type="institution" key="instit4">I3S</orgName>
								<address>
									<settlement>Sophia Antipolis</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Freddy</forename><surname>Lecue</surname></persName>
							<email>freddy.lecue@thalesgroup.com</email>
							<affiliation key="aff2">
								<orgName type="institution" key="instit1">CortAIx</orgName>
								<orgName type="institution" key="instit2">Thales Montreal</orgName>
								<address>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Impact of Injecting Ground Truth Explanations on Relational Graph Convolutional Networks and their Explanation Methods for Link Prediction on Knowledge Graphs</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">3A9ECD7B0D930B38CEE9B06E4CFA8DD1</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-04-12T14:46+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>link prediction</term>
					<term>Explainable AI</term>
					<term>knowledge graphs</term>
					<term>graph neural networks</term>
					<term>explanation evaluation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Relational Graph Convolutional Networks (RGCNs) are commonly applied to Knowledge Graphs (KGs) for black box link prediction. Several algorithms, or explanations methods, have been proposed to explain the predictions of this model. Recently, researchers have constructed datasets with ground truth explanations for quantitative and qualitative evaluation of predicted explanations. Benchmark results showed state-of-theart explanation methods had difficulties predicting explanations. In this work, we leverage prior knowledge to further constrain the loss function of RGCNs, by penalizing node embeddings far away from the node embeddings in their associated ground truth explanation. Empirical results show improved explanation prediction performance of state-of-the-art post hoc explanations methods for RGCNs, at the cost of predictive performance. Additionally, we quantify the different types of errors made both in terms of data and semantics.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Knowledge Graphs <ref type="bibr" target="#b0">[1]</ref> often represent facts as triples in the form (subject, predicate, object), where a subject and object represent an entity, linked by some predicate. Knowledge Graphs are often incomplete. Link prediction is performed on Knowledge Graphs to infer new facts from existing ones. Several researchers have proposed to perform link prediction on Knowledge Graphs using graph embedding algorithms. These algorithms learn a function mapping each subject, object, and predicate to a low dimensional space. A scoring function is defined to quantify if a link (relation) should exist between two nodes (entities). A Relational Graph Convolutional Network (RGCN) <ref type="bibr" target="#b1">[2]</ref> generalizes Graph Convolutional Networks <ref type="bibr" target="#b2">[3]</ref> to Knowledge Graphs, using the scoring function from DistMult <ref type="bibr" target="#b3">[4]</ref> as an output layer to return a probability of the input triple being a fact.</p><p>RGCNs are treated as a black box, that is, the decision function gives no insight, or explanation, as to why the model arrives at a particular decision. As a result, several algorithms for explainable link prediction have been proposed, in particular: ExplaiNE <ref type="bibr" target="#b4">[5]</ref> quantifies how the predicted probability of a link changes when weakening or removing a link with a neighboring node, while GNNExplainer <ref type="bibr" target="#b5">[6]</ref> explains the predictions of any Graph Neural Network, learning a mask over the adjacency matrix to identify the most informative subgraph. ExplaiNE and GNNExplainer return explanations to the user in the form of existing triples in the Knowledge Graph.</p><p>Recently, researchers have proposed several datasets with ground truth explanations for link prediction on Knowledge Graphs, allowing for quantitative comparisons of predicted explanations. The Royalty-20k and Royalty-30k datasets <ref type="bibr" target="#b6">[7]</ref> were constructed such that each observation has one and only one unique explanation. Researchers may want to know how their explanation method performs when there are multiple correct ways to explain why a relation exists between two nodes. The FrenchRoyalty-200k dataset <ref type="bibr" target="#b7">[8]</ref> was constructed to include all possible explanations for each triple in the training and test set. This dataset includes a relevance score between 0 and 1 for each explanation based on how intuitive users found it. For the aforementioned state-of-the-art explanation methods, initial benchmark results showed these methods do not always correctly predict ground truth explanations. Previous approaches to learning Knowledge Graph embeddings did not have access to ground truth explanations, hence do not incorporate information from explanations into the embedding.</p><p>In this work, we adapt RGCNs to incorporate prior knowledge from ground truth explanations into each embedding. This is done by constraining the cross entropy loss functions used by RGCNs. We compare several different explanationconstrained loss functions to an RGCN using the standard binary cross entropy. Results show improved predicted explanation performance of post hoc explanation methods for RGCNs, at the cost of predictive performance. Additionally, we quantify the different types of errors made in terms of data and semantics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED-WORK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Link Prediction</head><p>Knowledge graph embedding algorithms <ref type="bibr" target="#b8">[9]</ref> learn continuous vectors for each subject, predicate and object. The loss functions are often designed to capture specific algebraic properties of predicates (symmetric, reflexive, transitive, etc). A scoring function is defined to assign a value to each triple based on if the subject, predicate, and object form a valid fact. Typically, the scoring function is included in the loss function. This paper focuses on performing link prediction using RGCNs.</p><p>A Relational Graph Convolutional Network (RGCN) can also learn embeddings and perform link prediction on Knowledge Graphs. The RGCN performs embedding updates for a given entity by multiplying the neighboring entities by a weight matrix for each relation in the dataset, and summing across each neighbor and relation. A weight matrix for self connections is also learned, and added to the neighbor embedding summation. The cross entropy loss function optimized is given by</p><formula xml:id="formula_0">L RGCN = - 1 (1 + ω)| Ê| (s,p,o,y)∈T ylog f (s, p, o) +(1 -y)log 1 -f (s, p, o) ,<label>(1)</label></formula><p>where T is the set of all real (positive) and corrupted (negative) triples, ω is the number of negative triples, | Ê| is the number of unique predicates, f is the function learned by the RGCN, and for positive triples, the label y = 1 for positive triples and y = 0 for negative triples. This is not the only approach for link prediction (e.g. rule based, bayesian, etc.), however, this work focuses on the evaluation and explanation of link prediction on Knowledge Graphs using Relational Graph Convolutional Networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Explainable Link Prediction</head><p>Several algorithms have been proposed to explain the predictions of RGCNs. For a model with scoring function g, ExplaiNE <ref type="bibr" target="#b4">[5]</ref> computes the gradient of the scoring function with respect to the adjacency matrix. This measures the change in score due to a small perturbation in the adjacency matrix, that is, how much the score changes if a link is added or removed between two given nodes.</p><p>GNNExplainer <ref type="bibr" target="#b5">[6]</ref> explains the predictions of any Graph Neural Network, learning a mask over the input adjacency matrix to identify the most relevant subgraph. GNNExplainer minimizes the cross entropy between the predicted label using the input adjacency matrix, and the predicted label using the masked adjacency matrix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Explanation Aware Loss Function</head><p>In the context of Image classification, a recent research paper <ref type="bibr" target="#b9">[10]</ref> shows that interpretations are useful and that we can penalize explanations to align neural networks with prior knowledge. To do so, they constrain the loss functions of deep neural networks by introducing an explanation penalty term, which teaches the model to generate correct explanations. This additional constraint was shown to increase classification performance. The explanations generated by this approach however were not empirically evaluated. Without ground truth explanations, this paper relies on assumptions made by either manually annotating explanation labels, or rules to define correct explanations for image data. Indeed manual annotation is difficult with large datasets.</p><p>For link prediction on Knowledge Graphs, the standard RGCN optimizes a cross entropy loss function (Equation <ref type="formula" target="#formula_0">1</ref>) to learn embeddings. Recently, researchers have used a standard RGCN in a benchmark of three datasets to determine the quality of explanations generated post hoc by GNNExplainer and ExplaiNE <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>.</p><p>Until recently, benchmarks did not include ground truth for explanations, and the loss functions used by the standard RGCN do not include any constraints that account for them. This lack of constraints on the standard RGCN loss function causes subject and object embeddings in each triple to be mapped far away in the embedding space from the subject and object embeddings in its associated explanation. The Royalty datasets <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref> gives us the opportunity to train the predictors with the prior knowledge of ground truth explanations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Our contribution: Explanation-constrained Loss Function for Link Prediction</head><p>On the task of link prediction on Knowledge Graphs, the Royalty datasets <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref> provided a rule-based approach to generate a ground truth explanation for each input observation. Quantitative and qualitative results showed that both explanation methods did not always generate correct explanations, and performance across multiple metrics were low. In this work, we propose and evaluate several explanation-constrained loss functions to include prior explanation knowledge on the task of RGCN link prediction on Knowledge Graphs. For each triple in the training set, we penalize node embeddings far away from the node embeddings in the associated ground truth explanation. We reproduce the results of <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref> on three datasets, Royalty-20k, Royalty-30k, and FrenchRoyalty-200k, and use these to evaluate the impact of different explanation constraint strategies. We find our proposed approaches improve performance of post hoc explanation methods compared to a standard RGCN. Lastly, we perform an error analysis on the Royalty datasets, quantifying errors in terms of both data and semantics. Code for this paper is available online. <ref type="foot" target="#foot_0">1</ref></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. INJECTING GROUND TRUTH EXPLANATIONS INTO RGCN EMBEDDINGS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Constraining the Loss Function</head><p>We propose a loss function for RGCNs to improve post hoc explanation method performance on the Royalty datasets. This is achieved by adding an explanation constraint that pushes subject and object embeddings from each input triple closer to subject and object embeddings in the input triple's explanation. This is captured by the penalty expressed in Equation <ref type="formula" target="#formula_1">2</ref>where, for some input triple t p = (s, p, o) and an explanation triple t j = (s j , p j , o j ), we propose an explanation aware constraint that can be added to the binary cross entropy used by RGCNs:</p><formula xml:id="formula_1">P(t p , t j ) = max(||Emb(s) -Emb(s j )|| 2 , ||Emb(s) -Emb(o j )|| 2 ) +max(||Emb(o) -Emb(s j )|| 2 , ||Emb(o) -Emb(o j )|| 2 ).<label>(2)</label></formula><p>This penalty sums the maximum ℓ 2 distances between embedding Emb(.) of the subjects and objects of the input triple t p and an explanation triple t j . Penalizing the maximum allows us to push the subject embedding Emb(s) from the input triple closer to subject and object embeddings from its ground truth explanation.</p><p>The ℓ 2 maximum distance computations accounts for the fact that the direction of the links is an arbitrary modelling decision that should not impact the comparison of the embeddings of subjects and objects. Consider the case when the input triple t p = (John, hasP arent, T om), and its associated ground truth explanation t j = (T om, hasChild, John). Simply summing the distance between subject and objects gives ||Emb(John) -</p><formula xml:id="formula_2">Emb(T om)|| 2 + ||Emb(T om) - Emb(John)|| 2 = 2 * ||Emb(John) -Emb(T om)|| 2 .</formula><p>If however, the direction of the predicate in the explanation changes, for example, if t j = (John, isChildof, T om), the distance summation then becomes ||Emb(John) -Emb(John)|| 2 + ||Emb(T om) -Emb(T om)|| 2 = 0. Certainly the triple pair (John, hasP arent, T om), and (T om, hasChild, John) contains the same amount of information as (John, hasP arent, T om), and (John, isChildof, T om), however, a simple summation over distances results in two times the distance or zero. In order to account for this ambiguous case, we compute the maximum between the subject distances and add this to the maximum between the object distances.</p><p>We can now augment the standard RGCN binary cross entropy loss with the penalty term in Equation 2 in several ways and to account for several types of prior knowledge from explanation ground truth.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Loss Function for Unique Explanations</head><p>The Royalty-20k and Royalty-30k datasets <ref type="bibr" target="#b6">[7]</ref> contain one and only one unique explanation per predicted triple, describe as the case of non-ambiguous explanations. We introduce the first loss function incorporating the penalty term from Equation 2 for non-ambiguous explanations. Formally, let t p ∈ T + be a positive triple in the form (s, p, o), let e p be its explanation that contains a set of explanatory triples t j for the prediction of t p . The equation our proposed approach optimizes is given by</p><formula xml:id="formula_3">L sum = L RGCN + 1 |T + | tp∈T + 1 |e p | tj ∈ep P(t p , t j ),<label>(3)</label></formula><p>where | . | denotes the cardinality, for example |e p | denotes the number of triples in e p . Intuitively, Equation 3 takes a training set triple t p = (s, p, o), and its associated explanation e p , and applies an ℓ 2 penalty for subject and object embeddings in the explanation of t p that are far away from t p 's subject and object in the embedding space. In other words, the subject and object embeddings found in each triple's ground truth explanation should be "similar" in the embedding space to the subject and object embeddings, as they are used to explain why a predicate exists between the triple's subject and object. Using the standard RGCN loss function, this relationship between a triple and its ground truth explanation may not be captures in the embedding space without the additional constraint from Equation 3. We apply this loss function to the Royalty-20k and Royalty-30k datasets, results are reported in the following section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Loss Summing all Possible Explanations</head><p>The FrenchRoyalty-200k dataset <ref type="bibr" target="#b7">[8]</ref> contains multiple explanations for each predicted triples, described as the case of non-unique, or ambiguous explanations. We introduce a loss function including a penalty term for these ambiguous explanations. Formally, let E p = {e 1 , ...e l } be the set of l explanations available for t p and e i = {(s 1 , p 1 , o 1 ), ..., (s k , p k , o k )} be i th explanation for triple t p . Explanation e i contains a set of explanatory triples t j for the prediction of t p . The proposed loss function is given by</p><formula xml:id="formula_4">L sum ′ = L RGCN + 1 |T + | tp∈T + 1 |E p | ei∈Ep 1 |e i | tj ∈ei P(t p , t j )<label>(4)</label></formula><p>Similarly, Equation 4 takes a training set triple t p = (s, p, o), and its associated explanations E p , and applies an ℓ 2 penalty for subject and object embeddings from all explanations of t p that are far away from t p 's subject and object in the embedding space. The subtle difference between this loss function and Equation 3 is that E p = {e p }, that is, there is only one explanation available for Equation <ref type="formula" target="#formula_3">3</ref>, hence the inner summation can be dropped. Equation 4 however must sum across all explanations available to t p , hence is used for the FrenchRoyalty-200k dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Loss Weighting each Possible Explanations</head><p>We also consider a loss function that weights the distance penalty term by the relevance score of each explanation. This approach again pushes the subject and object embeddings of t p closer to the subject and object embeddings from all triples in E p . However, this distance penalty term is weighted by the user score of each explanation in E p , therefore making the embeddings in t p more similar to the embeddings from explanations with high relevance scores. This equation is given by</p><formula xml:id="formula_5">L weight = 1 |T + | tp∈T + 1 |E p | ei∈Ep 1 |e i | tj ∈ei</formula><p>score(e i ) * P(t p , t j )</p><p>(5)</p><formula xml:id="formula_6">L weight = L weight + L RGCN<label>(6)</label></formula><p>where score(e i ) is the relevance score of e i taking values between 0 and 1 of the explanation as provided by the FrenchRoyalty-200k dataset. Intuitively, large distances from embeddings in highly relevant explanations are given a larger penalty than large distance from embeddings in less relevant explanations. This loss function considers all triples in the ground truth explanation set E p , but focuses on intuitive explanations. This loss function relies on the user assigned scores for each explanation included in the FrenchRoyalty-200k, and is thus limited only to applications on this dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Loss Selecting the Highest Score</head><p>Lastly, we consider a loss function that penalizes subject and object embeddings in t p that are far away from the subject and object embeddings of the best available explanation e i , as determined by the given user relevance score. This equation is given by</p><formula xml:id="formula_7">L max = 1 |T + | tp∈T + tj ∈ei;score(ei)= max e∈Ep score(e) P(t p , t j ) |e i |<label>(7)</label></formula><formula xml:id="formula_8">L max = L max + L RGCN<label>(8)</label></formula><p>This loss function pushes the subject and object embeddings from t p close to the subject and object embeddings in the best explanation e i in the embedding space, making these embeddings more similar to each other than the standard RGCN. Embeddings from all other available ground truth explanations are not factored in. Similar to Equation <ref type="formula" target="#formula_6">6</ref>, this loss function relies on the user assigned scores from the FrenchRoyalty-200k, hence this loss function is limited only to applications on this dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. RESULTS AND EVALUATIONS</head><p>In this section, we evaluate the proposed loss functions on three datasets. The Royalty-20k dataset contains 3 types of predicates: hasSpouse, hasSuccessor, and hasPredecessor. The Royalty-30k dataset also contains 3 types of predicates including hasSpouse, hasGrandparent, and hasParent, where hasParent is only used to explain hasGrandparent. These datasets are used to evaluate explanation quality when there is one and only one explanation for each predicted triple.</p><p>The FrenchRoyalty-200k contains 6 types of predicates also based on family relations, hasSpouse hasBrother, hasSister, hasGrandparent, hasChild, and hasParent. Each predicted triple in this dataset includes all possible explanations, and is used to evaluate explanation quality when there are multiple to choose from. We compare all loss functions with a standard RGCN (using the loss function from Equation <ref type="formula" target="#formula_0">1</ref>). We apply two state-of-the-art explanation methods, GNNExplainer <ref type="bibr" target="#b5">[6]</ref> and ExplaiNE <ref type="bibr" target="#b4">[5]</ref> to all RGCNs post hoc, and compare the quality of explanation generated by GNNExplainer and ExplaiNE.</p><p>For all experiments in this work, we fix the number of embedding dimensions to 10 as done in the original benchmark. Across all three datasets, we subset the data by each predicate, and report results on each subset. For example, on the Royalty-20k dataset, the Spouse column from Table <ref type="table" target="#tab_1">I</ref> gives performance results on a subset of data using only hasSpouse triples and their associated explanations. Additionally, we report results on the full dataset, with all predicates included.</p><p>On the Royalty-20k and Royalty-30k datasets, predicted explanation performance is measured using the Jaccard score between each predicted and ground truth explanation. We also report precision, recall and F 1 scores, however, the original authors recommend measuring explanation quality on these datasets using the Jaccard score. On the FrenchRoyalty-200k dataset, predicted explanation performance is measured using the Generalized Precision, Recall, and F 1 scores <ref type="bibr" target="#b10">[11]</ref>, along with the Max-Jaccard score <ref type="bibr" target="#b7">[8]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Results with Non-Ambiguous Explanations</head><p>The top two rows of Table I report the link prediction results for the standard RGCN and the loss function in Equation 3 on the Royalty-20k dataset. We can see the standard RGCN outperformed the proposed approach on the full dataset, along with the hasSpouse subset. The proposed approach outperformed the standard RGCN on the hasSuccessor and hasPredecessor subsets.</p><p>Rows three and four of Table <ref type="table" target="#tab_1">I</ref> report the results of GNNExplainer applied to a standard RGCN, and applied to the proposed RGCN in Equation 3 on the task of explainable link prediction. We observe the GNNExplainer applied to the proposed RGCN outperformed or matched the GNNExplainer applied to the baseline in terms of the Jaccard score on all subsets, and the full dataset.</p><p>Rows five and six of Table <ref type="table" target="#tab_1">I</ref> report the results of ExplaiNE applied to a standard RGCN, and applied to the proposed RGCN in Equation 3. On the hasSpouse, hasSuccessor and hasPredecessor subsets, we find ExplaiNE when applied to the RGCN in Equation 3 improved all performance metrics. On the full dataset, we found using the proposed approach resulted in an improved Jaccard score.</p><p>The three rightmost columns of Table I report the performance metrics for the standard RGCN and the proposed approach on the Royalty-30k dataset. On the task of link prediction, we again find the proposed approach decreased the accuracy on all subsets, including the full dataset.  Rows three and four of Table <ref type="table" target="#tab_1">I</ref> report the results of GNNExplainer applied to the baseline RGCN, and proposed RGCN on the task of explainable link prediction. We find equal or better performance across all metrics. The precision, recall, and F 1 score remain relatively unchanged on the full dataset.</p><p>Rows five and six of Table <ref type="table" target="#tab_1">I</ref> report the results of ExplaiNE applied to the baseline RGCN, and the proposed RGCN. Here we see improved performance on all metrics, and across all data subsets, including the full dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Results with Non-Unique Explanations</head><p>The top four rows of Table II report the link prediction results of the baseline and proposed methods. In general, the baseline RGCN from Equation 1 outperformed the proposed methods in terms of accuracy on the task of link prediction, with the exception of the hasSpouse, hasSister, and hasChild subsets.</p><p>Rows five through eight of Table <ref type="table" target="#tab_3">II</ref> report the results of GNNExplainer applied the baseline RGCN, and the proposed approaches from Equations 4, 6, and 8 on the task of explainable link prediction. Overall, we found all approaches had similar performance metrics, with two proposed approaches having a small increase in Max-Jaccard score on the full dataset.</p><p>Rows nine through twelve of Table <ref type="table" target="#tab_3">II</ref> report the results of ExplaiNE applied to the baseline RGCN, and the proposed approaches on the task of explainable link prediction. We found the proposed approach improved performance on almost all metrics. Most notably, a large increase in Max-Jaccard score on the full dataset and hasSister subset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. ERROR ANALYSIS: QUANTITATIVE EVALUATION OF EXPLANATIONS A. Royalty-20k</head><p>The top row of Table <ref type="table" target="#tab_5">III</ref> gives a breakdown of each explanation method's most frequent error by subset for the Royalty-20k dataset when applied to L RGCN and L sum . Each row reports the most frequent predicate, and the percentage of errors this predicate occured in. For example, under the has-Spouse subset, the most common predicate across ExplaiNE's incorrectly predicted explanations (when applied to the RGCN in Equation <ref type="formula" target="#formula_3">3</ref>) was hasSpouse, and this predicate was observed in 100% of errors. This error occurs when ExplaiNE predicts the wrong subject or object in the explanation. This can occur on the hasSpouse subset, as under this subset, there is only one possible predicate to predict (hasSpouse).</p><p>On the Royalty-20k dataset, we can see on the hasSuccessor subset that the 94% of ExplaiNE with L sum errors contained the hasPredecessor predicate. This type of error occurs when the subject and/or object in the predicted explanation are incorrect. We can deduce this due to the fact that on the hasSuccessor dataset, the RGCNs and explanation methods only observe two predicates, hasSuccessor and hasPredecessor. GNNExplainer when applied to both RGCNs however produce more uniform errors, where 52% of errors occurred by using the wrong subject and/or object, and the remaining errors occurred by identifying the wrong predicate. For GNNExplainer applied to both RGCNs, we observe a similar phenomenon on the hasPredecessor subset as well.</p><p>Note there are three types of explanation errors, one where the predicate in the predicted explanation is incorrect, one where the subject and/or object in the predicted explanation is incorrect, or both. From Tabe III, we can see that ExplaiNE, when applied to the RGCN from L sum , has an increased number of errors using the wrong subject and object on the hasSuccessor subset. Recall each hasSuccessor predicate has an associated hasPredecessor ground truth. Here, the proposed approach produces more errors using the hasPredecessor predicate.  20k dataset. Each row denotes the predicate subset, the ground truth predicates defining the rule, and the percentage of triples not containing the ground truth predicate(s). For example, under the hasSuccessor subset of the Royalty-20k dataset, 6% of ExplaiNE's errors (when applied to L sum ) did not contain hasPredecessor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The first row of</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Royalty-30k</head><p>The second row of Table <ref type="table" target="#tab_5">III</ref> gives a breakdown of each explanation method's most frequent error by subset for the Royalty-30k dataset when applied to L RGCN and L sum . After applying ExplaiNE to L sum , we can see on the has-Grandparent subset, the most frequently predicted predicate was hasParent, accounting for 56% of errors. Conversely, for ExplaiNE with the baseline L RGCN , the most frequently predicted predicate is hasGrandparent. We can conclude from this that the explanation aware loss function L sum changed the most frequent type of error made by ExplaiNE. Rather than predict the wrong predicate, the explanation aware loss instead produces errors using the correct predicate but wrong subject and/or objects.</p><p>The second row of Table IV reports the most frequently missing predicate from each explanation method's errors for the Royalty-30k dataset. On the hasGrandparent subset, we find a decreased number of errors missing the hasParent explanation, consistent with Table <ref type="table" target="#tab_5">III</ref>. In general, we found GNNExplainer when applied to an explanation aware RGCN had minimal changes in errors metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. FrenchRoyalty-200k</head><p>The last row of Table <ref type="table" target="#tab_5">III</ref> gives a breakdown of each explanation method's most frequent error by subset for the FrenchRoyalty-200k dataset when applied to L RGCN and L sum ′ . On the hasBrother subset, we can see the errors produced by ExplaiNE with L sum ′ results in errors using the hasParent predict, instead of hasGrandparent produced by the baseline.</p><p>Figures <ref type="figure" target="#fig_0">1a</ref> and<ref type="figure" target="#fig_0">1b</ref> show frequency counts of the most frequently predicted predicates amongst predictions made by L sum ′ with a Max-Jaccard score less than 1. We can see both ExplaiNE and GNNExplainer's most frequently predicte predicate is hasGrandparent. Additionally, both explanation methods least frequently predicted predicate amongst errors were hasBrother, and hasSister. We found ExplaiNE had difficulty predicting hasSpouse explanations, while GNNExplainer had fewer hasSpouse errors, and more errors with hasChild explanations. The number of errors made by GNNExplainer on the hasParent and hasChild subsets were nearly equal. We   omit this analysis on Royalty-20k and Royalty-30k datasets due to space constraints.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. DISCUSSION</head><p>On all three datasets, we found the proposed approaches matched or increased the Jaccard (or Max-Jaccard) scores on ExplaiNE when training on the full dataset with all predicates included. We found however, the baseline RGCN outperformed the proposed approach on the task of link prediction on the same datasets. From these experiments, we observe a trade off between black box model performance and explainability. Including prior information from ground truth explanations into the embeddings of RGCNs improves the quality of explanations generated by ExplaiNE and GNNExplainer. However, this comes at the cost of predictive power. Our approach allows practitioners and researchers to find a balance between predictive power and model explainability that the standard RGCN is unable to provide.</p><p>Additionally, we found our approach had the biggest impact on ExplaiNE's explanations, and a minimal impact on GN-NExplainer's explanations. Understanding why the proposed approach had a larger impact on ExplaiNE's performance metrics than that of GNNExplainer would require a further understanding of what properties of the graph the embedding has learned. We leave this task for future work.</p><p>We recognize the difficulties in predicting explanations, even after making improvements, Jaccard (and Max-Jaccard) scores were still low. In fact, we found many of the Jaccard scores to be less than 0.5. Applying explanation methods post hoc to a black box model creates difficulties in diagnosing errors in predicted explanations, as there are many possible sources of error. When an explanation method produces an incorrectly predicted explanation, there are no available techniques to our knowledge that can identify if the explanation method is flawed, or if the error is due to a bad embedding that has not capturing the necessary information. Recent research has raised a similar concern, and that explanation methods for black boxes can be misleading <ref type="bibr" target="#b11">[12]</ref>- <ref type="bibr" target="#b14">[15]</ref>. This work contributes to being able to identify where in the pipeline errors are caused. Injecting knowledge into the graph  embedding shows GNNExplainer's errors are likely due to its parameters learned and not the RGCN embeddings, where as ExplaiNE's error are due to the embeddings. We recognize that the proposed approaches do not often have a practical application due to the difficulty of obtaining ground truth explanations. Rather, this work shows the theoretical impact of using prior knowledge during training, in order to identify if errors come from the RGCN or the post hoc explanation methods. For instance, this work showed that injecting the ground truths cause an accuracy decrease in some cases, and this opens some new research directions. The lack of significant changes in performance metrics of GNNExplainer is likely due to the large number of parameters used by the model for each observation. Perturbations to the RGCN embedding are less influential on the predicted explanation, hence we can conclude GNNExplainer is less dependent on the RGCN embeddings for explanation predictions than ExplaiNE.</p><p>We are aware that there are few Knowledge Graphs providing a ground truth for explanations, however we wanted to evaluate the impact of such knowledge on different methods before investing resources in campaigns to manually annotate Knowledge Graphs with explanations. This work focuses on the case of supervised explanation prediction, where ground truth explanations are available. We provide a theoretical study of the behaviour of several explanation methods in the presence of explanation aware embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. CONCLUSION</head><p>In this work, we apply an explanation-constrained loss function <ref type="bibr" target="#b9">[10]</ref> to RGCNs for link prediction on Knowledge Graphs. We add a penalty term for subject and object embeddings far away from the subject and object embeddings found in the ground truth explanation. We compare several different explanation-constrained loss functions to a baseline RGCN, and evaluate performance on three datasets with ground truth explanations. Results show improved performance of post hoc explanation methods. We perform an error analysis on the Royalty datasets, quantifying errors in terms of both data and semantics. This work provides opportunity for future extensions, such as leveraging the proposed RGCNs to distinguish between model error and an explanation method error. That is, determining if an incorrect explanation is caused by the embedding learned by the RGCN, or if the error is caused by the explanation method.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Fig. 1: RGCN with L sum : Predicate Frequency Count on Incorrectly Predicted Explanations on FrenchRoyalty-200k Dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I :</head><label>I</label><figDesc>Results on Royalty-20k, Royalty-30k datasets: Link prediction results for baseline RGCN and proposed loss functions, along with explanation evaluation for GNNExplainer and ExplaiNE. Highest scores per predicate denoted in bold.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Table IV reports the most frequently missing predicate from the explanation method's errors for the Royalty-</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">FrenchRoyalty-200k Results</cell><cell></cell><cell></cell></row><row><cell>Models</cell><cell>Metrics</cell><cell>Spouse</cell><cell>Brother</cell><cell>Sister</cell><cell>Grandparent</cell><cell>Child</cell><cell>Parent</cell><cell>Full data</cell></row><row><cell>L RGCN</cell><cell>Accuracy</cell><cell>0.935</cell><cell>0.909</cell><cell>0.853</cell><cell>0.858</cell><cell>0.792</cell><cell>0.838</cell><cell>0.928</cell></row><row><cell>L sum ′</cell><cell>Accuracy</cell><cell>0.973</cell><cell>0.864</cell><cell>0.999</cell><cell>0.599</cell><cell>0.8</cell><cell>0.639</cell><cell>0.877</cell></row><row><cell>Lmax</cell><cell>Accuracy</cell><cell>0.966</cell><cell>0.75</cell><cell>0.824</cell><cell>0.648</cell><cell>0.793</cell><cell>0.697</cell><cell>0.878</cell></row><row><cell>L weight</cell><cell>Accuracy</cell><cell>0.966</cell><cell>0.909</cell><cell>0.971</cell><cell>0.615</cell><cell>0.801</cell><cell>0.706</cell><cell>0.897</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE II :</head><label>II</label><figDesc>Results on FrenchRoyalty-200k: Link prediction results for baseline RGCN and proposed model, along with explanation evaluation for GNNExplainer and ExplaiNE. Highest scores in bold, and G. being an abbreviation for Generalized.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE III :</head><label>III</label><figDesc>Most frequent predicate across incorrectly predicted explanations, along with the percentage of error by subset. Note L sum ′ is used for FrenchRoyalty-200k.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE IV :</head><label>IV</label><figDesc>Most frequently missing predicate. Each row denotes the predicate subset, the ground truth predicates defining the rule, and the percentage of triples not containing the ground truth predicate(s)</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>https://github.com/halliwelln/penalized-rgcn</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Knowledge graphs</title>
		<author>
			<persName><forename type="first">A</forename><surname>Hogan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Blomqvist</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cochez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Amato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>De Melo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gutierrez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E L</forename><surname>Gayo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kirrane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Neumaier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Polleres</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.02320</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Modeling relational data with graph convolutional networks</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Schlichtkrull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bloem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Van Den Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Semantic Web Conference</title>
		<imprint>
			<publisher>ESWC</publisher>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<publisher>ICLR</publisher>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Embedding entities and relations for learning and inference in knowledge bases</title>
		<author>
			<persName><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<publisher>ICLR</publisher>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Explaine: An approach for explaining network embedding-based link predictions</title>
		<author>
			<persName><forename type="first">B</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lijffijt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">D</forename><surname>Bie</surname></persName>
		</author>
		<idno>abs/1904.12694</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Gnnexplainer: Generating explanations for graph neural networks</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bourgeois</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Linked Data Ground Truth for Quantitative and Qualitative Evaluation of Explanations for Relational Graph Convolutional Network Link Prediction on Knowledge Graphs</title>
		<author>
			<persName><forename type="first">N</forename><surname>Halliwell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Gandon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Lecue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Web Intelligence and Intelligent Agent Technology</title>
		<meeting><address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021-12">Dec. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">User Scored Evaluation of Non-Unique Explanations for Relational Graph Convolutional Network Link Prediction on Knowledge Graphs</title>
		<author>
			<persName><forename type="first">N</forename><surname>Halliwell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Gandon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Lecue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Knowledge Capture</title>
		<meeting><address><addrLine>Virtual Event, United States</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021-12">Dec. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A survey on knowledge graphs: Representation, acquisition and applications</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Marttinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
		<idno>abs/2002.00388</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Interpretations are useful: Penalizing explanations to align neural networks with prior knowledge</title>
		<author>
			<persName><forename type="first">L</forename><surname>Rieger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">J</forename><surname>Murdoch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th International Conference on Machine Learning, ICML 2020</title>
		<meeting>the 37th International Conference on Machine Learning, ICML 2020</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Using graded relevance assessments in IR evaluation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kekäläinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Järvelin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Assoc. Inf. Sci. Technol</title>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Interpretable machine learning: Fundamental principles and 10 grand challenges</title>
		<author>
			<persName><forename type="first">C</forename><surname>Rudin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Semenova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhong</surname></persName>
		</author>
		<idno>abs/2103.11251</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead</title>
		<author>
			<persName><forename type="first">C</forename><surname>Rudin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="206" to="215" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">The dangers of post-hoc interpretability: Unjustified counterfactual explanations</title>
		<author>
			<persName><forename type="first">T</forename><surname>Laugel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lesot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Marsala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Renard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Detyniecki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence, IJCAI</title>
		<meeting>the Twenty-Eighth International Joint Conference on Artificial Intelligence, IJCAI</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">how do I fool you?&quot;: Manipulating user trust via misleading black box explanations</title>
		<author>
			<persName><forename type="first">H</forename><surname>Lakkaraju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Bastani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AIES &apos;20: AAAI/ACM Conference on AI</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Ethics, and Society</publisher>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
