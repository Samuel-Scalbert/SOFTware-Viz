<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Refined Convergence and Topology Learning for Decentralized SGD with Heterogeneous Data</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-10-21">21 Oct 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Batiste</forename><surname>Le Bars</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Aurélien</forename><surname>Bellet</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Marc</forename><surname>Tommasi</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Erick</forename><surname>Lavoie</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Anne-Marie</forename><surname>Kermarrec</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="laboratory">UMR 9189</orgName>
								<orgName type="institution" key="instit1">Univ. Lille</orgName>
								<orgName type="institution" key="instit2">Inria</orgName>
								<orgName type="institution" key="instit3">CNRS</orgName>
								<orgName type="institution" key="instit4">Centrale Lille</orgName>
								<orgName type="institution" key="instit5">CRIStAL</orgName>
								<address>
									<postCode>F-59000</postCode>
									<settlement>Lille</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Université de Bâle</orgName>
								<address>
									<settlement>Bâle</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">EPFL</orgName>
								<address>
									<settlement>Lausanne</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Refined Convergence and Topology Learning for Decentralized SGD with Heterogeneous Data</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-10-21">21 Oct 2022</date>
						</imprint>
					</monogr>
					<idno type="MD5">3D407A7283D21D44437B7AE0CE9139B4</idno>
					<idno type="arXiv">arXiv:2204.04452v3[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-04-12T14:45+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>One of the key challenges in decentralized and federated learning is to design algorithms that efficiently deal with highly heterogeneous data distributions across agents. In this paper, we revisit the analysis of the popular Decentralized Stochastic Gradient Descent algorithm (D-SGD) under data heterogeneity. We exhibit the key role played by a new quantity, called neighborhood heterogeneity, on the convergence rate of D-SGD. By coupling the communication topology and the heterogeneity, our analysis sheds light on the poorly understood interplay between these two concepts. We then argue that neighborhood heterogeneity provides a natural criterion to learn data-dependent topologies that reduce (and can even eliminate) the otherwise detrimental effect of data heterogeneity on the convergence time of D-SGD. For the important case of classification with label skew, we formulate the problem of learning such a good topology as a tractable optimization problem that we solve with a Frank-Wolfe algorithm. As illustrated over a set of simulated and real-world experiments, our approach provides a principled way to design a sparse topology that balances the convergence speed and the per-iteration communication costs of D-SGD under data heterogeneity.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Decentralized and federated learning methods allow training from data stored locally by several agents (nodes) without exchanging raw data, in line with the increasing demand for Preprint. Under review. more privacy-preserving algorithms <ref type="bibr" target="#b20">(Kairouz et al., 2021)</ref>. One of the key challenges in decentralized learning is to deal with data heterogeneity: as each agent collects its own data, local datasets typically exhibit different distributions.</p><p>In this work, we study this challenge in the context of fully decentralized learning algorithms, which provide a scalable and robust alternative to server-based approaches <ref type="bibr" target="#b8">(Colin et al., 2016;</ref><ref type="bibr" target="#b28">Lian et al., 2017;</ref><ref type="bibr" target="#b23">Koloskova et al., 2019</ref><ref type="bibr" target="#b22">Koloskova et al., , 2020))</ref>. Fully decentralized optimization algorithms, such as the celebrated Decentralized SGD (D-SGD) <ref type="bibr" target="#b28">(Lian et al., 2017</ref><ref type="bibr" target="#b29">(Lian et al., , 2018;;</ref><ref type="bibr" target="#b22">Koloskova et al., 2020)</ref>, operate on a graph representing the communication topology, i.e. which pairs of nodes exchange information with each other. The connectivity of the topology then rules a trade-off between the convergence rate and the per-iteration communication complexity of fully decentralized algorithms <ref type="bibr" target="#b45">(Wang et al., 2019)</ref>. Choosing a good topology for fully decentralized machine learning is therefore an important question, and remains a largely open problem in the presence of data heterogeneity.</p><p>Until recently, the impact of the communication topology on the convergence was believed to be mainly characterized by its spectral gap: a large spectral gap indicating good connectivity and thus faster convergence. Focusing solely on the connectivity of the topology has however shown to be insufficient, even when we have identically distributed data <ref type="bibr" target="#b36">(Neglia et al., 2020;</ref><ref type="bibr" target="#b44">Vogels et al., 2022)</ref>. In the heterogeneous setting, <ref type="bibr" target="#b1">Bellet et al. (2022)</ref> notably observe that the choice of topology has a large influence, beyond its spectral gap, on the convergence speed of D-SGD. However, these empirical observations are not supported by any theory.</p><p>In this work, we fill the theoretical gap that currently exists on these questions. We focus on D-SGD <ref type="bibr" target="#b28">(Lian et al., 2017</ref><ref type="bibr" target="#b29">(Lian et al., , 2018;;</ref><ref type="bibr" target="#b22">Koloskova et al., 2020)</ref>, which is arguably the most popular decentralized optimization algorithm in the context of machine learning due to its good properties inherited from centralized SGD. In particular, D-SGD has been praised for its computational scalability <ref type="bibr" target="#b30">(Lin et al., 2021)</ref>, its applicability to training deep neural networks at scale <ref type="bibr" target="#b46">(Ying et al., 2021;</ref><ref type="bibr" target="#b24">Kong et al., 2021)</ref>, and the good generalization guarantees that it provides <ref type="bibr" target="#b39">(Sun et al., 2021;</ref><ref type="bibr" target="#b51">Zhu et al., 2022)</ref>.</p><p>Our first contribution is a refined convergence analysis of D-SGD which introduces a new quantity, called neighborhood heterogeneity, that couples the topology and the local data distributions. Neighborhood heterogeneity essentially measures the expected distance between the global gradient and the aggregated gradients in the neighborhood of nodes. Our results demonstrate that the impact of the topology on the convergence rate of D-SGD, for both convex and nonconvex objectives, does not only depend on its connectivity (i.e., spectral gap): it also depends on its capacity to compensate the heterogeneity of local data distributions at the neighborhood level. This new perspective allows to avoid the restrictive assumption of bounded heterogeneity used in previous work <ref type="bibr" target="#b28">(Lian et al., 2017</ref><ref type="bibr" target="#b29">(Lian et al., , 2018;;</ref><ref type="bibr" target="#b40">Tang et al., 2018;</ref><ref type="bibr" target="#b0">Assran et al., 2019;</ref><ref type="bibr" target="#b22">Koloskova et al., 2020;</ref><ref type="bibr" target="#b46">Ying et al., 2021)</ref>.</p><p>Our second contribution deals with the problem of learning a good data-dependent topology, going beyond prior work which focused mainly on optimizing the spectral gap <ref type="bibr" target="#b3">(Boyd et al., 2004</ref><ref type="bibr" target="#b4">(Boyd et al., , 2006;;</ref><ref type="bibr" target="#b45">Wang et al., 2019)</ref>. We argue that neighborhood heterogeneity provides a natural objective and show that it can be effectively optimized in practice in the important case of classification with label distribution heterogeneity across nodes (label skew) <ref type="bibr" target="#b20">(Kairouz et al., 2021;</ref><ref type="bibr" target="#b17">Hsieh et al., 2020;</ref><ref type="bibr" target="#b1">Bellet et al., 2022)</ref>. We solve the resulting problem using a Frank-Wolfe algorithm <ref type="bibr" target="#b15">(Frank and Wolfe, 1956;</ref><ref type="bibr" target="#b19">Jaggi, 2013)</ref>, allowing us to track the quality of the learned topology as new edges are added in a greedy manner. Our results imply that we can approximately minimize neighborhood heterogeneity up to a fixed additive error with a topology whose maximum degree is constant in the number of nodes. To the best of our knowledge, our work is the first to learn the graph topology for decentralized learning in a way that (i) is data-dependent, (ii) controls communication costs, and (iii) optimizes the convergence rate of D-SGD. We illustrate the usefulness of our approach in simulated and real data experiments with linear and deep models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Consensus vs personalized objectives. In this work, we study the consensus problem which aims to learn a single model that minimizes the average of the local objectives (see Eq. 1). Another line of research tackles the problem of heterogeneity in decentralized learning through personalization <ref type="bibr" target="#b25">(Koppel et al., 2017;</ref><ref type="bibr" target="#b43">Vanhaesebrouck et al., 2017;</ref><ref type="bibr" target="#b50">Zantedeschi et al., 2020;</ref><ref type="bibr" target="#b32">Marfoq et al., 2021;</ref><ref type="bibr" target="#b14">Even et al., 2022)</ref>. In that setting, each agent aims to learn a personalized model that minimizes its own (expected) local objective. It is thus natural and desirable to connect nodes that have similar data distributions. In contrast, our results show that for the consensus problem, the topology should connect nodes that are different so that local neighborhoods are rep-resentative of the global distribution. We emphasize that personalization and consensus are relevant to different use cases and can be considered as orthogonal to each other.</p><p>Algorithmic improvements to decentralized SGD. Significant work has been devoted to extensions of D-SGD. We can mention those based on momentum <ref type="bibr" target="#b0">(Assran et al., 2019;</ref><ref type="bibr" target="#b16">Gao and Huang, 2020;</ref><ref type="bibr" target="#b30">Lin et al., 2021;</ref><ref type="bibr">Yuan et al., 2021)</ref>, cross-gradient aggregations <ref type="bibr" target="#b13">(Esfandiari et al., 2021)</ref>, gradient tracking <ref type="bibr" target="#b21">(Koloskova et al., 2021)</ref> and bias correction (or variance reduction) <ref type="bibr" target="#b40">(Tang et al., 2018;</ref><ref type="bibr" target="#b48">Yuan et al., 2020;</ref><ref type="bibr" target="#b47">Yuan and Alghunaim, 2021;</ref><ref type="bibr" target="#b18">Huang and Pu, 2021)</ref>. Many of these schemes are able to reduce the order of the term that depends on data heterogeneity but remain impacted by strong heterogeneous scenarios. We stress that the above line of research is complementary to ours as it is based on modifications of the D-SGD algorithm (which often requires additional computation and/or communication). In contrast, our work does not modify the algorithm: we provide a refined analysis and a method to learn the topology. We believe that our results can be combined with the above algorithmic improvements, but leave such extensions for future work.</p><p>Good topologies for decentralized learning. There is a long line of research on choosing a good topology (e.g., expanders or exponential graphs) <ref type="bibr" target="#b7">(Chow et al., 2016;</ref><ref type="bibr" target="#b35">Nedić et al., 2018;</ref><ref type="bibr" target="#b46">Ying et al., 2021)</ref>, or learning it to maximize the spectral gap <ref type="bibr" target="#b3">(Boyd et al., 2004</ref><ref type="bibr" target="#b4">(Boyd et al., , 2006;;</ref><ref type="bibr" target="#b45">Wang et al., 2019)</ref> or network throughput <ref type="bibr" target="#b33">(Marfoq et al., 2020)</ref>. Unlike our approach, these methods simply seek to optimize the connectivity of the topology while respecting some communication constraints, but they do not take into account the data distributions across nodes.</p><p>Until recently, <ref type="bibr" target="#b1">Bellet et al. (2022)</ref> was the only approach that leverages the distribution of data in the design of the topology. Focusing on classification under label skew, they propose a heuristic approach that consists of inter-connected cliques, where class proportions in each clique should be as close as possible to the global proportions. Our approach is more flexible: it can learn more general topologies, and provides full control over their sparsity. Furthermore, our topology learning criteria is theoretically justified, while the one in <ref type="bibr" target="#b1">Bellet et al. (2022)</ref> is only supported by empirical experiments. We think however that the ideas of the present paper could pave the way for a theoretical analysis of their work.</p><p>Concurrent to and independently from our work, <ref type="bibr" target="#b10">Dandi et al. (2022)</ref> provide a similar analysis of the convergence rate of D-SGD using a quantity called "relative heterogeneity". However, our approaches differ greatly in how they learn the topology. In fact, <ref type="bibr" target="#b10">Dandi et al. (2022)</ref> do not learn the topology itself (i.e., which nodes are connected) but only the weights of a predefined topology. In other words, the set of edges is fixed in advance. This severely limits the ability to mitigate the effect of data heterogeneity unless the predefined topology is dense. In contrast, our approach learns a sparse topology (both the edges and their associated mixing weights) in order to balance the convergence rate and the communication complexity of D-SGD.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Preliminaries</head><p>Problem setting. In decentralized federated learning, n ∈ N agents (nodes) with their own data distribution seek to collaborate in order to solve a global consensus problem. Formally, the agents aim to learn a single parameter θ ∈ R d so as to optimize the global objective <ref type="bibr" target="#b28">(Lian et al., 2017)</ref>:</p><formula xml:id="formula_0">f * min θ∈R d f (θ) 1 n n i=1 f i (θ) ,<label>(1)</label></formula><p>where</p><formula xml:id="formula_1">f i (θ) E Zi∼Di [F i (θ; Z i )]</formula><p>is the local objective function associated to node i. The random vector Z i is drawn from the data distribution D i of agent i, having support over a space Ω i , and F i : R d × Ω i → R is its pointwise loss function (differentiable in its first argument). Note that the distributions D i can be very different, which is common in real applications <ref type="bibr" target="#b20">(Kairouz et al., 2021)</ref>. From an optimization point of view, this means that a local optimum θ i ∈ arg min θ f i (θ) can be far from a global optimum θ of (1).</p><p>To collaboratively solve (1) in a fully decentralized manner, the agents communicate with each other over a directed graph. The graph topology is represented by a matrix W ∈ [0, 1] n×n , where W ij &gt; 0 gives the weight that agent i gives to messages received from agent j, while W ij = 0 (no edge) means that i does not receive messages from j. The choice of topology W affects the trade-off between the convergence rate of decentralized optimization algorithms and the communication costs. Indeed, more edges imply higher communication costs but often faster convergence. Communication costs, or per-iteration complexity, are often regarded as proportional to the maximum (in or out)degrees of nodes in the topology, representing the maximum (incoming or outcoming) load of a node <ref type="bibr" target="#b28">(Lian et al., 2017)</ref>:</p><formula xml:id="formula_2">d in max (W ) = max i n j=1 I[W ij &gt; 0], d out max (W ) = max i n j=1 I[W ji &gt; 0].<label>(2)</label></formula><p>From this perspective, the complete graph, and the star topology induced by server-based federated learning, yield high communication costs, as the maximum degree is n -1.</p><p>Decentralized SGD. Decentralized Stochastic Gradient Descent (D-SGD) <ref type="bibr" target="#b28">(Lian et al., 2017;</ref><ref type="bibr" target="#b22">Koloskova et al., 2020)</ref> is a popular fully decentralized algorithm for solving problems of the form (1). As mentioned above, such algorithms operate on a graph topology represented by the matrix W ∈ [0, 1] n×n . In particular, D-SGD requires that W is a mixing matrix, i.e. doubly stochastic: W 1 = 1 and 1 T W = 1 T .</p><p>In the rest of the paper, we will use the terms topology and mixing matrix interchangeably. For sake of generality, we consider a setting where the mixing matrix may change at each iteration <ref type="bibr" target="#b22">(Koloskova et al., 2020)</ref>. On the other hand, we assume for simplicity that the mixing matrices are Algorithm 1 Decentralized SGD <ref type="bibr" target="#b28">(Lian et al., 2017)</ref> Require: Initialize ∀i, θ</p><formula xml:id="formula_3">(0) i = θ (0) ∈ R d , iterations T , stepsizes {η t } T -1 t=0 , mixing {W (t) } T -1 t=0 . for t = 0, . . . , T -1 do for each node i = 1, . . . , n (in parallel) do Sample Z (t) i ∼ D i θ (t+ 1 2 ) i ← θ (t) i -η t ∇F i (θ (t) i , Z (t) i ) θ (t+1) i ← n j=1 W (t) ij θ (t+ 1</formula><p>2 ) j end for end for deterministic. All our results can however be extended to random mixing matrices, see Appendix C.1 for details.</p><formula xml:id="formula_4">D-SGD is summarized in Algorithm 1. At iteration t, each node i first updates its local estimate θ (t) i based on ∇F i (θ (t) i , Z (t) i ), the stochastic gradient of F i evaluated at θ (t) i with Z (t)</formula><p>i sampled from D i . Then, each node aggregates its current parameter value with its neighbors according to the mixing matrix W (t) .</p><p>General assumptions. We recall some standard assumptions extensively considered in decentralized learning <ref type="bibr" target="#b5">(Bubeck, 2014;</ref><ref type="bibr" target="#b37">Nguyen et al., 2019;</ref><ref type="bibr" target="#b28">Lian et al., 2017;</ref><ref type="bibr" target="#b40">Tang et al., 2018;</ref><ref type="bibr" target="#b0">Assran et al., 2019;</ref><ref type="bibr" target="#b27">Li et al., 2019;</ref><ref type="bibr" target="#b24">Kong et al., 2021;</ref><ref type="bibr" target="#b46">Ying et al., 2021)</ref>.</p><p>Assumption 1. (L-smoothness) There exists a constant L &gt; 0 such that for any</p><formula xml:id="formula_5">Z ∈ Ω i , θ, θ ∈ R d we have ∇F i (θ, Z) -∇F i ( θ, Z) ≤ L θ -θ .</formula><p>Assumption 2. (Bounded variance) For any node i ∈ 1, . . . , n , there exists a constant σ 2 i &gt; 0 such that for any</p><formula xml:id="formula_6">θ ∈ R d , we have E Z∼Di ∇F i (θ, Z) -∇f i (θ) 2 2 ≤ σ 2 i . Assumption 3. (Mixing parameter) There exists a mixing parameter p ∈ [0, 1] such that for any matrix M ∈ R d×n , we have M W T -M 2 F ≤ (1 -p) M -M 2 F , where • F denotes the Frobenius norm and M = M ( 1 n 11 T ).</formula><p>Assumption 3 measures how well an averaging step using a mixing matrix W brings an arbitrary matrix M closer to M . <ref type="bibr" target="#b4">Boyd et al., 2006)</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>It is always verified for</head><formula xml:id="formula_7">p = 1 -λ 2 (W T W ) with λ 2 (W T W ) the second largest eigenvalue of W T W (</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Joint Effect of Topology and Data Heterogeneity</head><p>In this section, we introduce a new quantity, called neighborhood heterogeneity, and derive new convergence rates for D-SGD that depend on this quantity. These rates have several nice properties: (i) they hold under weaker assumptions than previous work (unbounded local heterogeneity), (ii) they highlight the interplay between the topology and the heterogeneous data distribution across nodes, and (iii) they provide a criterion for choosing topologies not only based on their mixing properties but also based on data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Neighborhood Heterogeneity</head><p>Given a mixing matrix W , our notion of neighborhood heterogeneity measures the expected distance between the aggregated gradients in the neighborhood of a node (as weighted by W ) and the global average of gradients. In our analysis, we will assume this distance to be bounded.</p><p>Assumption 4 (Bounded neighborhood heterogeneity).</p><p>There exists a constant τ 2 &gt; 0 such that ∀θ ∈ R d :</p><formula xml:id="formula_8">1 n n i=1 E n j=1 W ij ∇F j (θ, Z j )- 1 n n j=1 ∇F j (θ, Z j ) 2 2 ≤ τ 2 .</formula><p>(3)</p><p>To better understand Assumption 4, we can upper-bound the left-hand term of the previous equation, denoted H(θ), using a bias-variance decomposition. This leads to the following bound:</p><formula xml:id="formula_9">H(θ) ≤ 1 n n i=1 n j=1 W ij ∇f j (θ) -∇f (θ) 2 2 + σ 2 max n W - 1 n 11 T 2 F ,<label>(4)</label></formula><p>with σ 2 max = max i σ 2 i . This upper bound contains two terms. The first one is a bias term, related to the heterogeneity of the problem. It essentially measures how the gradients of local objectives differ from the gradient of the global objective when they are aggregated at the neighborhood level of the topology through W . The second one is a variance term closely related to the mixing parameter p of Assumption 3: we can show that it is upper bounded by σ 2 max (1 -p) and lower bounded by σ 2 max (1 -p)/n, see Proposition 3 in Appendix C.</p><p>Comparison to classic bounded heterogeneity assumption. In our analysis, we use Assumption 4 in replacement of the bounded local heterogeneity condition used in previous literature <ref type="bibr" target="#b28">(Lian et al., 2017</ref><ref type="bibr" target="#b29">(Lian et al., , 2018;;</ref><ref type="bibr" target="#b0">Assran et al., 2019;</ref><ref type="bibr" target="#b22">Koloskova et al., 2020;</ref><ref type="bibr" target="#b46">Ying et al., 2021)</ref>. We recall it below.</p><p>Assumption 5 (Bounded local heterogeneity). There exists a constant ζ2 &gt; 0 such that</p><formula xml:id="formula_10">1 n n i=1 ∇f i (θ)-∇f (θ) 2 2 ≤ ζ2 , ∀θ ∈ R d .</formula><p>Assumption 5 has the same form as the bias term of in Equation (4) but considers W = I (i.e., it does not depend on the topology). It requires that the local gradients should not be too far from the global gradient: the more heterogeneous the nodes' distribution (and objectives), the bigger ζ2 . In contrast, neighborhood heterogeneity takes into account the mixture of gradients in the neighborhoods defined by W . Crucially, Assumption 4 is more flexible than Assumption 5. More precisely, our set of assumptions (Assumptions 2-4) is less restrictive than those in previous work (Assumptions 2, 3, 5). To see this, we first show that our set of assumptions is implied by the latter (proof in Appendix C).</p><p>Proposition 1. Let Assumptions 2-3 and 5 to be verified. Then Assumption 4 is satisfied with τ 2 = (1 -p) ζ2 + σ2 , where σ2</p><formula xml:id="formula_11">1 n i σ 2 i .</formula><p>We now show that our set of assumptions (2-4) is strictly more general than Assumptions 2, 3, 5 by identifying situations where Assumption 4 is verified while Assumption 5 is not. A trivial example is the complete graph W = 1 n 11 T , for which we have τ 2 = 0, regardless of heterogeneity. More interestingly, some combinations of sparse topologies and data distributions can ensure that τ 2 remains small while ζ2 can be arbitrary large. We give a simple example below (detailed derivations in Appendix A).</p><p>Example 1 (Two clusters and a ring topology). Let n be an even number and assume</p><formula xml:id="formula_12">Z i ∼ D i N (m, σ2 ) if i is odd and Z i ∼ D i N (-m, σ2</formula><p>) if i is even. Let σ2 &lt; +∞ (necessary to have Assumption 2) and m &gt; 0 potentially asymptotically large. We fix F i (θ, Z i ) = (θ -Z i ) 2 (mean estimation). Consider a ring topology that alternates between one odd node and one even node, with the diagonal and off-diagonal entries of W equal to 1/2 and 1/4 respectively. Then we have τ 2 = σ 2 i = 4σ 2 &lt; +∞, while ζ2 = 4m 2 can be arbitrarily large as m grows.</p><p>This illustrates that an appropriate topology, even as sparse as a ring, can control τ 2 and mitigate the underlying heterogeneity of the problem. In Section 5, we will show that we can learn a sparse topology W that (approximately) minimizes the neighborhood heterogeneity bound τ 2 . Before that, we validate the relevance of our new Assumption 4 by deriving a novel convergence result for D-SGD.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Convergence Analysis</head><p>We now present the main theoretical result of this section: two new non-asymptotic convergence results for D-SGD under Assumption 4. The proof of this theorem is given in Appendix B.</p><p>Theorem 1. Consider Algorithm 1 with mixing matrices W (0) , . . . , W (T -1) satisfying Assumptions 3 and 4. Assume further that Assumptions 1-2 are respected, and denote θ(t)</p><formula xml:id="formula_13">1 n n i=1 θ (t)</formula><p>i . For any target accuracy ε &gt; 0, there exists a constant stepsize η ≤ η max = p 8L such that: Convex case:</p><formula xml:id="formula_14">1 T +1 T t=0 E(f ( θ(t) ) -f ) ≤ ε as soon as T ≥ O σ2 nε 2 + √ Lτ pε 3 2 + L pε r 0 ,<label>(5)</label></formula><p>Non-convex case:</p><formula xml:id="formula_15">1 T +1 T t=0 E ∇f ( θ(t) 2 2 ≤ ε as soon as T ≥ O Lσ 2 nε 2 + Lτ pε 3 2 + L pε f 0 , (<label>6</label></formula><formula xml:id="formula_16">)</formula><p>where T is the number of iterations, r 0 = θ (0) -θ 2 2 , f 0 = f (θ (0) ) -f and O(•) hides the numerical constants explicitly provided in the proof.</p><p>Analysis and comparison to prior results. To put the above theorem into perspective, recall that Centralized (Parallel) Stochastic Gradient Descent (C-PSGD) is equivalent to D-SGD with the mixing matrix W = 1 n 11 T (complete graph). For this specific case, it has been shown that in the convex scenario, an accuracy ε is achieved after <ref type="bibr" target="#b11">(Dekel et al., 2012;</ref><ref type="bibr" target="#b2">Bottou et al., 2018;</ref><ref type="bibr" target="#b38">Stich and Karimireddy, 2020)</ref>. On the other hand, existing results for D-SGD (under Assumption 5 instead of Assumption 4) require T ≥ O(</p><formula xml:id="formula_17">T ≥ O( σ2 nε 2 + L ε ) iterations</formula><formula xml:id="formula_18">σ2 nε 2 + √ L(1-p)( ζ+σ √ p) pε 3/2</formula><p>+ L pε ) iterations <ref type="bibr" target="#b22">(Koloskova et al., 2020)</ref>.</p><p>The first thing to note is that rate (5) is consistent with the above rates. When the complete graph topology W = 1 n 11 T is used at each iteration we have τ = 0 and p = 1, which allows us to recover the rate of the communication-inefficient C-PSGD. Furthermore, considering the classical Assumption 5 and using Proposition 1 gives the looser bound</p><formula xml:id="formula_19">O( σ2 nε 2 + √ L(1-p)( ζ+σ) pε 3/2 + L pε )</formula><p>which is equivalent to the rate of D-SGD in <ref type="bibr" target="#b22">Koloskova et al. (2020)</ref>. Similarly, the rate (6) obtained for non-convex objectives is also consistent with <ref type="bibr" target="#b22">Koloskova et al. (2020)</ref>.</p><p>Crucially, recall that in the heterogeneous setting τ can be much smaller than √ 1 -p( ζ + σ) (see Section 4.1), which makes our bounds sharper. This is because the topology now influences the convergence rate in Theorem 1 via both the mixing parameter p and τ . This is of particular significance in situations where communication constraints are strong so that the topology connectivity has to be low (i.e., p close to 0). In that case, prior rates are heavily impacted by data heterogeneity as p can no longer compensate for it. In contrast, we can expect that a well-chosen sparse topology can achieve small τ and thus mitigate the impact of data heterogeneity. To highlight this, we can go back to Example 1. For the chosen ring topology, we have p = Θ( 1 n 2 ), but the specific arrangement of nodes and the weights in W still allow a small bound τ 2 on neighborhood heterogeneity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Learning the Topology</head><p>In the previous rates ( <ref type="formula" target="#formula_14">5</ref>) and ( <ref type="formula" target="#formula_15">6</ref>), the smaller the bound τ 2 on neighborhood heterogeneity, the fewer iterations needed to reach an error ε. This motivates the idea of learning a sparse topology W that approximately minimizes neighborhood heterogeneity (Equation ( <ref type="formula">3</ref>)), in order to control the trade-off between the convergence rate and the per-iteration communication complexity given in Equation (2). However, minimizing neighborhood heterogeneity in the general setting appears to be challenging without further statistical assumptions, as Equation (3) should hold for all θ ∈ R d . Below, we focus on classification with label skew, and show that Equation (3) simplifies to a more tractable quantity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Statistical Learning with Label Skew</head><p>Label skew is an important type of data heterogeneity in federated classification problems <ref type="bibr" target="#b20">(Kairouz et al., 2021;</ref><ref type="bibr" target="#b17">Hsieh et al., 2020;</ref><ref type="bibr" target="#b1">Bellet et al., 2022)</ref>. In this setting, each agent i is associated with a random variable</p><formula xml:id="formula_20">Z i = (X i , Y i ) ∼ D i</formula><p>where X i ∈ R q represents the feature vector and Y i ∈ 1, . . . , K the associated class label. The agents aim to learn a classifier h θ : R q → 1, . . . , K parameterized by</p><formula xml:id="formula_21">θ ∈ R p such that h θ (X i ) is a good predictor of Y i for all i.</formula><p>The heterogeneity of the distributions {D i } n i=1 comes only from a difference in the label distribution P i (Y ) i.e.</p><formula xml:id="formula_22">D i = P i (X, Y ) = P (X|Y )P i (Y ).</formula><p>For simplicity, we assume that all agents use the same pointwise loss function (F i = F for all i), which is typically the cross-entropy.</p><p>Under the above framework, we can derive a neighborhood heterogeneity bound τ 2 that can effectively be minimized with respect to W .</p><p>Proposition 2 (Bounded neighborhood heterogeneity under label skew). Consider the statistical framework defined above and assume there exists B &gt; 0 such that ∀k = 1, . . . , K and ∀θ</p><formula xml:id="formula_23">∈ R d , E X [∇F (θ; X, Y )|Y = k] -1 K K k =1 E X [∇F (θ; X, Y )|Y = k ] 2 2 ≤ B.</formula><p>Then, denoting π jk P j (Y = k), Assumption 4 is satisfied with:</p><formula xml:id="formula_24">τ 2 = KB n K k=1 n i=1 n j=1 W ij π jk - 1 n n j=1 π jk 2 + σ 2 max n W - 1 n 11 T 2 F . (<label>7</label></formula><formula xml:id="formula_25">)</formula><p>The proof is provided in Appendix C.Note that the condition involving B corresponds to a bounded heterogeneity assumption at the class level (rather than at the agent level as in Assumption 5).</p><p>The neighborhood heterogeneity bound τ 2 in ( <ref type="formula" target="#formula_24">7</ref>) is quadratic in W and composed of two terms. The first one is a bias term due to the label skew: it will be minimal if neighborhoodlevel class proportions (weighted by W ) match the global class proportions. This is trivially achieved for any choice of W if the class proportions are the same across nodes.</p><p>The second term is a variance term which is minimal when W = 11 T n , the complete topology with uniform weights. As a matter of fact, this topology is also the unique global minimizer of ( <ref type="formula" target="#formula_24">7</ref>), which is equal to 0 in this case. However, as already discussed, such a dense mixing matrix is impractical as it yields huge communication costs. We will show how Algorithm 2 Sparse Topology Learning with Frank-Wolfe (STL-FW)</p><formula xml:id="formula_26">Require: Initialization W (0) = I n , class proportions Π ∈ [0, 1] n×K and hyperparameter λ &gt; 0.</formula><p>for l = 0, . . . , L do P (l+1) = arg min P ∈A P, ∇g(</p><formula xml:id="formula_27">W (l) ) γ (l+1) = arg min γ∈[0,1] g (1 -γ) W (l) + γP (l+1) W (l+1) = (1 -γ (l+1) ) W (l) + γ (l+1) P (l+1) end for</formula><p>the per-iteration communication complexity of D-SGD can be controlled while approximately minimizing τ 2 in (7).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Optimization with the Frank-Wolfe Algorithm</head><p>In this section, we design an algorithm that finds a sparse approximate minimizer of τ 2 in (7). We focus on learning a single mixing matrix W as a "pre-processing" step (i.e., before running D-SGD), and do so in a centralized manner. Specifically, we assume that a single party (which may be one of the agents, or a third-party) has access to the class proportions π ik = P i (Y = k) for each agent i and each class k. In practice, since each agent has access to its local dataset, it can compute these local proportions locally and share them without sharing the local data itself.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Optimization problem.</head><p>Our objective is to learn a sparse mixing matrix W which approximately minimizes τ 2 in (7).</p><p>Denoting by S W ∈ [0, 1] n×n : W 1 = 1, 1 T W = 1 T the set of doubly stochastic matrices, the optimization problem can be written as follows:</p><formula xml:id="formula_28">min W ∈S g(W ) 1 n W Π -11 T n Π 2 F + λ n W -11 T n 2 F ,<label>(8)</label></formula><p>where Π ∈ [0, 1] n×K contains the class proportions {π ik } and λ &gt; 0 is a hyperparameter. To exactly match (7), λ should be equal to</p><formula xml:id="formula_29">σ 2 max KB , but σ 2</formula><p>max and B are unknown in practice. Instead, we use λ to control the bias-variance trade-off. As discussed in Section 4.1, the variance term is an upper bound of 1 -p with p the mixing parameter of W . Therefore, λ allows to tune a trade-off between the minimization of the bias due to label skew and the maximization of the mixing parameter of W .</p><p>Algorithm. We propose to find sparse approximations of (8) using a Frank-Wolfe (FW) algorithm, which is well-suited to learn a sparse parameter over convex hulls of finite set of atoms <ref type="bibr" target="#b19">(Jaggi, 2013)</ref>. In our case, S corresponds to the convex hull of the set A of all permutation matrices <ref type="bibr" target="#b31">(Lovász and Plummer, 2009;</ref><ref type="bibr" target="#b41">Tewari et al., 2011;</ref><ref type="bibr" target="#b42">Valls et al., 2020)</ref>.</p><p>The algorithm is summarized in Algorithm 2. Starting from the identity matrix W (0) = I n ∈ S, each iteration l ≥ 0 consists of moving towards a feasible point P (l+1) that minimizes a linearization of g at the current iterate W (l) . As finding P (l+1) is a linear problem, solving it over S is equivalent to solving it over A. Although A contains n! elements, the linear program corresponds to the well-known assignment problem <ref type="bibr" target="#b6">(Burkard et al., 2012;</ref><ref type="bibr" target="#b9">Crouse, 2016)</ref> and can be solved tractably with the Hungarian algorithm, which has a worst-case complexity of O(n 3 ) <ref type="bibr" target="#b31">(Lovász and Plummer, 2009)</ref>.<ref type="foot" target="#foot_0">1</ref> Note that the gradient ∇g(W ) needed to solve the assignment problem is given by</p><formula xml:id="formula_30">2 n K k=1 (W Π :,k -Π :,k 1) • Π T :,k + 2 n λ W -11 T n , where Π :,k is the k-th column of Π. The next iterate W (l+1)</formula><p>is then obtained as a convex combination of P (l+1) and W (l) , and is thus guaranteed to be in S. The optimal combining weight is computed by line-search, which has a closedform solution since g is quadratic (see Appendix C.2).</p><p>Crucially, Algorithm 2 allows to control the sparsity of the final solution: since a permutation matrix contains exactly one non-zero entry in each row and each column, at most one new incoming and one new outgoing edge per node are added. As we start from the identity matrix (i.e., only self-edges), this guarantees that at the end of the l-th iteration, each node will have at most l in-neighbors and l outneighbors. The per-iteration communication complexity of D-SGD induced by the learned topology can thus be directly controlled by the number of iterations of our algorithm. The trade-off with the quality of the solution is quantified by the following theorem, which is derived from standard results for FW <ref type="bibr" target="#b19">(Jaggi, 2013)</ref> combined with a tight bound on the smoothness of g in appropriate norm (see Appendix C).</p><p>Theorem 2. Consider the statistical setup presented in Section 5.1 and let { W (l) } L l=1 be the sequence of mixing matrices generated by Algorithm 2. Then, at any iteration l = 1, . . . , L, we have:</p><formula xml:id="formula_31">g( W (l) ) ≤ 16 l + 2 λ+ 1 n K k=1 (Π :,k -Π :,k 1)•Π T :,k 2 , (<label>9</label></formula><formula xml:id="formula_32">)</formula><p>where • 2 stands for the nuclear norm, i.e., the sum of singular values. Furthermore, we have d in max ( W (l) ) ≤ l and d out max ( W (l) ) ≤ l, resulting in a per-iteration complexity bounded by l.</p><p>The above theorem shows that the objective g decreases at a rate of O(1/l) as new connections between nodes are made. In general, we can bound (9) less tightly by g( W (l) ) ≤ 16 l+2 (λ + 1), which is independent of the number of nodes n. Recall that with λ = σ 2 max /KB, the value τ 2 of Proposition 2 is exactly equal to KB •g(W ). Therefore, the bound given in Theorem 2 directly bounds neighborhood heterogeneity and can thus be plugged in the rates of Theorem 1.  (l) ), in green the bias term sup θ</p><formula xml:id="formula_33">1 n n i=1 n j=1 W (l) ij ∇fj(θ) -∇f (θ) 2</formula><p>2 and in yellow the mixing parameter 1 -p = λ2(W (l)T W (l) ). Here, λ = 0.5 and m = 5. (b, c) Error n -1 θ (t) -θ 2 2 (solid line) of D-SGD after 50 iterations, averaged over 10 runs, for increasing levels of heterogeneity (measured by parameter m). The dashed lines show maxi(θ</p><formula xml:id="formula_34">(t) i -θ ) 2 and mini(θ (t) i -θ ) 2</formula><p>, illustrating the variability across nodes.</p><p>To summarize, our approach provides a principled way to learn the topology so as to reduce neighborhood heterogeneity while controlling the per-iteration communication complexity of D-SGD. Remarkably, the fact that g( W (l) ) is independent of n implies that we can find topologies that approximately optimize the convergence rate of D-SGD while keeping the communication load per node constant, thereby guaranteeing scalability to a large number of nodes even in highly heterogeneous scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experiments</head><p>This section shows the practical usefulness of our topology learning method, referred to as Sparse Topology Learning with Frank-Wolfe (STL-FW). We call communication budget d max = max{d in max , d out max } the maximal number of neighbors a node can have in the used topologies, which controls the per-iteration communication complexity incurred by any node.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Simulations on Synthetic Data</head><p>Statistical setup. We generalize the mean estimation objective of Example 1 with K = 10 clusters and n = 100 nodes, with exactly 10 nodes associated to each cluster. Each cluster is associated with a specific Gaussian distribution, which corresponds to a class in the statistical framework described in Section 5.1. The variance of the K distributions is the same (σ 2 = 1) but their means are evenly spread over <ref type="bibr">[-m, m]</ref>. Thus, m ≥ 0 controls the heterogeneity of the problem (the bigger m, the more heterogeneous the setup). We can analytically compute all numerical constants introduced throughout the paper. Unless otherwise noted, λ is set to σ 2 /KB where σ 2 = 4σ 2 and B = 4m 2 .</p><p>Competitor. For a fixed budget d max , we compare the topology learned by STL-FW to a random d max -regular graph with uniform weights 1 dmax+1 . This graph is independent of the data but has good mixing parameter p (every node will have exactly b neighbors, with uniform weights). We use a fixed step-size for D-SGD, which is tuned separately for each topology in the interval [0.001, 1].</p><p>Results. We first study the behavior of our topology learning algorithm. As seen in Figure <ref type="figure">1</ref>(a), the objective function g(W (l) ) decreases quickly in the first iterations with a clear elbow at l = 9 iterations. This is because we have K = 10 "classes", hence 9 neighbors are sufficient to compensate for label skew. We also see that decreasing g successfully decreases the two key quantities that affect the convergence of D-SGD and are upper bounded by g: the bias term in Equation (4) (which does not depend on θ in this setup and can therefore be computed exactly) and the mixing parameter 1 -p (which continues to decrease beyond l = 9).</p><p>Figure <ref type="figure">1(b,</ref><ref type="figure">c</ref>) shows that the topology learned by STL-FW indeed translates into faster convergence for D-SGD than with the random (but well-connected) topology in data heterogeneous settings. This is especially striking when looking at best and worst-case errors across nodes (dashed lines). For a low budget (d max = 3), D-SGD with our topology remains slightly impacted by heterogeneity. But remarkably, for d max = 9, our topology makes D-SGD completely insensitive to increasing data heterogeneity. This observation is consistent with the elbow observed at l = d max = 9 in Figure <ref type="figure">1(a)</ref>. In Appendix D.2, we provide basic statistics on the topologies obtained for the two budgets d max = 3 and d max = 9. We can see in particular that the topologies obtained with STL-FW are d max -regular (like the random graph) but have much lower bias (i.e., the distribution of labels in the neighborhood of nodes is closer to the global distribution). As expected, this bias is equal to 0 for d max = 9.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Experiments on Real Datasets</head><p>Setup. We follow the experimental setup in <ref type="bibr" target="#b1">Bellet et al. (2022)</ref> and consider two classification tasks: a linear model on MNIST <ref type="bibr" target="#b12">(Deng, 2012)</ref>   <ref type="formula">2022</ref>): a learning rate of 0.1 and batch size of 128 for MNIST, and a learning rate of 0.002 and batch size of 20 for CIFAR10. Using D-SGD, we compare the topology learned with our approach STL-FW to other fixed topologies: (1) a fully-connected graph (d max = 99), which exhibits the fastest convergence speed but is impractical, (2) a random graph with the same communication budget as STL-FW, (3) D-Cliques <ref type="bibr" target="#b1">(Bellet et al., 2022)</ref>, also with the same budget, and (4) a deterministic exponential graph promoted in recent work <ref type="bibr" target="#b46">(Ying et al., 2021)</ref> (d max = 14). Note that all competing topologies are dataindependent, except D-Cliques. To have a fair comparison, we use standard D-SGD without algorithmic modifications like "clique-averaging" introduced by <ref type="bibr" target="#b1">Bellet et al. (2022)</ref>. For all experiments with STL-FW, we use λ = 0.1 since, remarkably, its value does not significantly change the results (see Figure <ref type="figure">3</ref> in Appendix D).</p><p>Results. Figure <ref type="figure" target="#fig_1">2</ref> shows our results for varying communication budget d max : small (2), medium (5) and large (10). On MNIST, STL-FW makes convergence faster than all competitors and quickly matches the speed of the fullyconnected topology as the budget d max increases. Remarkably, STL-FW is already showing good performance at d max = 2, which is a very small budget that the other topologies (except the random one) cannot handle. As expected, the two data-dependent topologies (D-Cliques and STL-FW) outperform the random topologies, including the exponential graph which has better connectivity (d max = 14) but does not compensate for the heterogeneity. The fact that STL-FW improves over D-Cliques can be explained by the fact that D-Cliques only compensate the heterogeneity (the bias term in Equation ( <ref type="formula" target="#formula_9">4</ref>)) without consideration for the overall connectivity (the variance term in Equation ( <ref type="formula" target="#formula_9">4</ref>)). This is illustrated in the tables of Appendix D.2.</p><p>On CIFAR10, we see that d max = 2 is not sufficient to reach good performance. This can be explained by the increased complexity of the problem (non-convex objective with a deep model), requiring larger communication budgets. This is in line with empirical results in prior work <ref type="bibr" target="#b24">(Kong et al., 2021)</ref>. However, with slightly larger budgets i.e. d max = 5 and 10 (d max = 3 in Fig. <ref type="figure">5</ref>, App. D), performance improves and the results are consistent with those on MNIST: STL-FW outperforms other sparse topologies and comes close to the performance of the fully connected topology for d max = 10. Overall, STL-FW provides better convergence speed than all tractable alternatives, with the additional ability to operate in low communication regimes (unlike D-Cliques and the exponential graph).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>This paper addressed two important open problems in decentralized learning. First, thanks to our new notion of neighborhood heterogeneity, we characterized the joint effect of the topology and the data heterogeneity in the convergence rate of D-SGD. Our results show that, if chosen appropriately, the topology can compensate for the heterogeneity and speed up convergence. Second, we tackled the problem of learning a good topology under data heterogeneity. To the best of our knowledge, our work is the first to provide a principled and data-dependent approach, with explicit control on the trade-off between the communication costs and the convergence speed of D-SGD. We believe that our work paves the way for the design of other data-dependent topology learning techniques. One may for instance investigate different types of heterogeneity (beyond label skew), different knowledge assumptions (e.g., not knowing the proportions), and dynamic learning of the topology. We can also envision fully decentralized and privacy-preserving versions.</p><p>Let us now find a bound τ 2 on the neighborhood heterogeneity. Using a bias-variance decomposition, we have:</p><formula xml:id="formula_35">H(θ) = 1 n n i=1 E n j=1 W ij ∇F j (θ) - 1 n n j=1 ∇F j (θ) 2 = 1 n n i=1 n j=1 W ij ∇f j (θ) - 1 n n j=1 ∇f j (θ) 2 + 1 n n i=1 E n j=1 (W ij - 1 n )(∇f j (θ) -∇F j (θ)) 2 = 1 n n i=1 (2θ -2θ) 2 + 1 n n i=1 n j=1 (W ij - 1 n ) 2 E(∇f j (θ) -∇F j (θ)) 2 = 0 + 4σ 2 1 n n i=1 n j=1 (W ij - 1 n ) 2 ≤ 4σ 2 .</formula><p>The third equality was obtained thanks to the fact that E[∇f j (θ) -∇F j (θ)] = 0. This result shows that Assumption 4 is verified with τ 2 = 4σ 2 &lt; ∞.</p><p>On the contrary, since m can be arbitrary large, Assumption 5 is not verified. Indeed:</p><formula xml:id="formula_36">1 n n i=1   ∇f i (θ) - 1 n n j=1 ∇f j (θ)   2 = 1 n n i=1 (2m) 2 = 4m 2 n ζ2 -→ m→∞ +∞ .</formula><p>Remark. At first sight, one may wonder why the local variance term σ2 appears in τ 2 but not in ζ2 . This is because we chose to define neighborhood heterogeneity in expectation with respect to the pointwise loss functions F 1 , . . . , F n , resulting in a bias-variance decomposition (see Eq. 4) which is the relevant quantity to optimize when learning the topology in Section 5. In contrast, following the convention used in previous work, local heterogeneity is defined with respect to the local objectives f 1 , . . . , f n and thus only measures a bias term, while the variance term is accounted separately by Assumption 2. Since the variance terms are the same in both settings, the difference is in how the bias term is measured (at the node level or at the neighborhood level): in the example above, it is equal to 4m 2 n for local heterogeneity while it is equal to 0 for neighborhood heterogeneity (see the above calculation of H(θ)).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Proof of Theorem 1 B.1 Notations and Overview</head><p>We start by re-writing the updates of D-SGD (Algorithm 1) in matrix form.</p><p>Let Θ (t)  θ</p><formula xml:id="formula_37">(t) 1 , . . . , θ<label>(t) n</label></formula><p>∈ R d×n be the matrix that contains the parameter vectors of all nodes at time t. Denote by</p><formula xml:id="formula_38">∇F (Θ (t) , Z (t) ) ∇F 1 (θ (t) 1 , Z (t) 1 ), . . . , ∇F n (θ (t) n , Z<label>(t)</label></formula><p>n ) ∈ R d×n the matrix containing all stochastic gradients at time t. The D-SGD update at time t can then be written as:</p><formula xml:id="formula_39">Θ (t+1) = Θ (t) -η t ∇F (Θ (t) , Z (t) ) W (t)T .</formula><p>In the following, we denote Θ  <ref type="formula">2020</ref>)). The main difference resides in how the consensus term</p><formula xml:id="formula_40">Θ (t) -Θ (t) 2</formula><p>F is controlled across iterations (Lemma 3). The proof is organized as follows.</p><p>Convex case. 2. In Lemma 3, the consensus term</p><formula xml:id="formula_41">Θ (t) -Θ (t) 2</formula><p>F , which appears in the result of Lemma 1, is upper-bounded. The resulting upper-bound exhibits our new quantity τ 2 (an upper bound on neighborhood heterogeneity).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Corollary 1 uses the previous lemma to bound</head><formula xml:id="formula_42">1 T +1 T t=0 Θ (t) -Θ (t) 2 F .</formula><p>4. Lemma 4 provides an upper-bound on the error term with the following form:</p><formula xml:id="formula_43">1 T + 1 T t=0 E(f ( θ(t) ) -f ) ≤ 2 br 0 T + 1 1 2 + 2e 1 3 r 0 T + 1 2 3 + dr 0 T + 1 , where b = σ2 n , e = 36Lτ 2 p 2 , d = 8L p and r 0 = θ (0) -θ 2 2 .</formula><p>5. To get the final rate of Theorem 1, it suffices to find T such that each term in the right-hand side of the previous equation in bounded by ε 3 .</p><p>•</p><formula xml:id="formula_44">2 br0 T +1 1 2 ≤ ε 3 ⇐⇒ 36br0 ε 2 ≤ T + 1 ⇐⇒ 36σ 2 r0 nε 2 ≤ T + 1, • 2e 1 3 r0 T +1 2 3 ≤ ε 3 ⇐⇒ e 1 2 6 3 2 r0 ε 3 2 ≤ T + 1 ⇐⇒ 6 5 2 √ Lτ r0 pε 3 2 ≤ T + 1, • dr0 T +1 ≤ ε 3 ⇐⇒ 3dr0 ε ≤ T + 1 ⇐⇒ 24Lr0 pε ≤ T + 1.</formula><p>In particular, it suffices to take</p><formula xml:id="formula_45">T ≥ 36σ 2 r 0 nε 2 + 89 √ Lτ r 0 pε 3 2 + 24Lr 0 pε = O σ2 nε 2 + √ Lτ pε 3 2 + L pε r 0 ,</formula><p>in order to have all three terms bounded by ε 3 , and obtain the final result.</p><p>Non-convex case. The proof is similar to the convex one: it only differs in the descent lemmas that are used.</p><p>1. Lemma 2 provides the descent lemma for the non-convex scenario.</p><p>2. The consensus term is bounded using the same results as in the convex case, i.e., with Lemma 3 and Corollary 1.</p><p>3. Lemma 5 provides an upper-bound on the error term with the following form:</p><formula xml:id="formula_46">1 T + 1 T t=0 E ∇f ( θ(t) ) 2 2 ≤ 2 4bf 0 T + 1 1 2 + 2e 1 3 4f 0 T + 1 2 3 + 4df 0 T + 1 , where b = 2Lσ 2 n , e = 96L 2 τ 2 p 2</formula><p>, d = 8L p and f 0 = f (θ (0) ) -f . 4. We bound in each term of the previous equation by ε 3 and get the sufficient condition:</p><formula xml:id="formula_47">T ≥ 288Lσ 2 f 0 nε 2 + 576Lτ f 0 pε 3 2 + 96Lf 0 pε = O Lσ 2 nε 2 + Lτ pε 3 2 + L pε f 0 .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Preliminaries and Useful Results</head><p>Property 1 (Averaging preservation). Let W ∈ R n×n be a mixing matrix and Θ be any matrix in R d×n . Then, W preserves averaging:</p><formula xml:id="formula_48">(ΘW ) 11 T n = Θ 11 T n = Θ (10)</formula><p>Property 2 (Implications of L-smoothness and convexity).</p><p>• If we assume convexity, we have for all i ∈ 1, . . . , n :</p><formula xml:id="formula_49">∇f i ( θ), θ -θ ≥ f i ( θ) -f i (θ).<label>(11)</label></formula><p>• Under Assumption 1 (L-smoothness), it holds for all i ∈ 1, . . . , n :</p><formula xml:id="formula_50">F i (θ, Z) ≤ F i ( θ, Z) + ∇F i ( θ, Z), θ -θ + L 2 θ -θ 2 2 , ∀θ, θ ∈ R d , Z ∈ θ i . (<label>12</label></formula><formula xml:id="formula_51">)</formula><p>Taking the expectation of the previous equation, we also have:</p><formula xml:id="formula_52">f i (θ) ≤ f i ( θ) + ∇F ( θ), θ -θ + L 2 θ -θ 2 2 , ∀θ, θ ∈ R d .<label>(13)</label></formula><p>• If we further assume that the F i 's are convex, Assumption 1 also implies ∀θ, θ ∈ R d ,Z ∈ θ i :</p><formula xml:id="formula_53">∇f i (θ) -∇f i ( θ) 2 ≤ L θ -θ 2 , (<label>14</label></formula><formula xml:id="formula_54">) ∇f i (θ) -∇f i ( θ) 2 2 ≤ 2L f i (θ) -f i ( θ) -∇f i ( θ), θ -θ , (<label>15</label></formula><formula xml:id="formula_55">) ∇F i (θ, Z) -∇F i ( θ, Z) 2 2 ≤ 2L F i (θ, Z) -F i ( θ, Z) -∇F i ( θ, Z), θ -θ . (<label>16</label></formula><formula xml:id="formula_56">)</formula><p>These results can be found in many convex optimization books and papers, e.g. in <ref type="bibr" target="#b5">Bubeck (2014)</ref>. Property 3 (Norm inequalities).</p><p>• For a set of vectors</p><formula xml:id="formula_57">{a i } n i=1 such that a i ∈ R d , n i=1 a i 2 2 ≤ n n i=1 a i 2 2 . (<label>17</label></formula><formula xml:id="formula_58">) • For two vectors a, b ∈ R d , a + b 2 2 ≤ (1 + α) a 2 2 + (1 + α -1 ) b 2 2 , ∀α &gt; 0. (<label>18</label></formula><formula xml:id="formula_59">) • For two vectors a, b ∈ R d , 2 a, b ≤ α a 2 2 + α -1 b 2 2 , ∀α &gt; 0.<label>(19)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3 Needed Lemmas</head><p>In the following we denote by F t = σ(Z (k) |k ≤ t) the natural filtration with respect to</p><formula xml:id="formula_60">Z (t) = (Z (t) 1 , . . . , Z<label>(t)</label></formula><p>n ). Remark that ∀i = 1, . . . , n the iterates θ (t+1) i and θ(t+1) are in particular F t -measurable.</p><p>Lemma 1 (Descent Lemma -Convex case). Consider the setting of Theorem 1 and let η t ≤ 1 4L , then we almost surely have:</p><formula xml:id="formula_61">E Z (t) |Ft-1 θ(t+1) -θ 2 ≤ θ(t) -θ 2 + η 2 t σ2 n -η t f ( θ(t) ) -f + 3L 2n η t Θ (t) -Θ (t) 2 F ,<label>(20)</label></formula><p>where E Z (t) |Ft-1 stands for the conditional expectation</p><formula xml:id="formula_62">E Z (t) [•|F t-1 ].</formula><p>Proof. The proof closely follows the one in <ref type="bibr" target="#b22">Koloskova et al. (2020)</ref>. Using the recursion of D-SGD and since all mixing matrices are doubly stochastic and preserve the average (Proposition 1) we have:</p><formula xml:id="formula_63">θ(t+1) -θ 2 = θ(t) - η t n n i=1 ∇F i (θ (t) i , Z (t) i ) -θ 2 = θ(t) -θ - η t n n i=1 ∇f i (θ (t) i ) + η t n n i=1 ∇f i (θ (t) i ) - η t n n i=1 ∇F i (θ (t) i , Z (t) i ) 2 = θ(t) -θ - η t n n i=1 ∇f i (θ (t) i ) 2 + η 2 t 1 n n i=1 ∇f i (θ (t) i ) - 1 n n i=1 ∇F i (θ (t) i , Z (t) i ) 2 + 2 θ(t) -θ - η t n n i=1 ∇f i (θ (t) i ), η t n n i=1 ∇f i (θ (t) i ) - η t n n i=1 ∇F i (θ (t) i , Z<label>(t)</label></formula><p>i ) .</p><p>Passing to the conditional expectation, the last term (the inner product) is equal to 0. This comes from the fact that</p><formula xml:id="formula_64">E Z (t) i |Ft-1 [∇F i (θ (t) i , Z (t) i )] = ∇f i (θ (t) i ).</formula><p>We therefore need to bound the first two terms in the conditional expectation. The second one can easily be bounded using Assumption 2:</p><formula xml:id="formula_65">η 2 t E Z (t) |Ft-1 1 n n i=1 ∇f i (θ (t) i ) - 1 n n i=1 ∇F i (θ (t) i , Z (t) i ) 2 = η 2 t n 2 E Z (t) |Ft-1 n i=1 (∇f i (θ (t) i ) -∇F i (θ (t) i , Z (t) i )) 2 = η 2 t n 2 n i=1 E Z (t) i |Ft-1 ∇f i (θ (t) i ) -∇F i (θ (t) i , Z (t) i ) 2 (A.2) ≤ η 2 t σ2 n ,</formula><p>where the second equality was obtained using the identity</p><formula xml:id="formula_66">E i Y i 2 2 = i E Y i 2 2</formula><p>when Y i are independent and EY i = 0. Now that the second term is bounded, we can move to the first one. Because θ (t)</p><p>i and θ(t) are F t-1 -measurable, we have</p><formula xml:id="formula_67">E Z (t) |Ft-1 θ(t) -θ - η t n n i=1 ∇f i (θ (t) i ) 2 = θ(t) -θ - η t n n i=1 ∇f i (θ (t) i ) 2 = θ(t) -θ 2 + η 2 t 1 n n i=1 ∇f i (θ (t) i ) 2 T1 -2η t θ(t) -θ , 1 n n i=1 ∇f i (θ (t) i ) T2 .</formula><p>In order to bound T 1 , recall that by definition 1 n i ∇f i (θ ) = 0, therefore:</p><formula xml:id="formula_68">T 1 = 1 n n i=1 (∇f i (θ (t) i ) -∇f i ( θ(t) ) + ∇f i ( θ(t) ) -∇f i (θ )) 2 (17) ≤ 2 1 n n i=1 (∇f i (θ (t) i ) -∇f i ( θ(t) )) 2 + 2 1 n n i=1 (∇f i ( θ(t) ) -∇f i (θ )) 2 (17) ≤ 2 n n i=1 ∇f i (θ (t) i ) -∇f i ( θ(t) ) 2 + 2 n n i=1 ∇f i ( θ(t) ) -∇f i (θ ) 2 (14)(15) ≤ 2L 2 n n i=1 θ (t) i -θ(t) 2 + 4L n n i=1 f i ( θ(t) ) -f i (θ ) -∇f i (θ ), θ(t) -θ = 2L 2 n n i=1 θ (t) i -θ(t) 2 + 4L n n i=1 f i ( θ(t) ) -f i (θ ) -4L 1 n n i=1 ∇f i (θ ) =0 , θ(t) -θ = 2L 2 n n i=1 θ (t) i -θ(t) 2 + 4L f ( θ(t) ) -f .</formula><p>Finally, we have to bound T 2 :</p><formula xml:id="formula_69">-T 2 = - 2η t n n i=1 θ(t) -θ , ∇f i (θ (t) i ) = - 2η t n n i=1 θ(t) -θ (t) i , ∇f i (θ (t) i ) + θ (t) i -θ , ∇f i (θ (t) i ) (13)(11) ≤ - 2η t n n i=1 f i ( θ(t) ) -f i (θ (t) i ) - L 2 θ(t) -θ (t) i 2 2 + f i (θ (t) i ) -f i (θ ) = -2η t f ( θ(t) ) -f (θ ) + Lη t n n i=1 θ(t) -θ (t) i 2 2 = -2η t f ( θ(t) ) -f + Lη t n Θ (t) -Θ (t) 2 F .</formula><p>Combining all previous results, we get:</p><formula xml:id="formula_70">E Z (t) |Ft-1 θ(t+1) -θ 2 ≤ θ(t) -θ 2 + η 2 t σ2 n + Lη t n (2Lη t + 1) Θ (t) -Θ (t) 2 F + 2η t (2Lη t -1) f ( θ(t) ) -f .</formula><p>Since, by hypothesis, η t ≤ 1 4L , we have 2Lη t + 1 ≤ 3 2 and 2Lη t -1 ≤ -1 2 , which concludes the proof.</p><p>Lemma 2 (Descent Lemma -Non-convex case). Consider the setting of Theorem 1 and let η t ≤ 1 4L , then we almost surely have:</p><formula xml:id="formula_71">E Z (t) |Ft-1 f ( θ(t+1) ) -f ≤ f ( θ(t) ) -f - η t 4 ∇f ( θ(t) ) 2 2 + L 2 n η t Θ (t) -Θ (t) 2 F + Lσ 2 2n η 2 t . (<label>21</label></formula><formula xml:id="formula_72">)</formula><p>Proof. The proof adapts the one of Lemma 10 in Koloskova et al. ( <ref type="formula">2020</ref>) to our setting.</p><formula xml:id="formula_73">E Z (t) |Ft-1 f ( θ(t+1) ) = E Z (t) |Ft-1 f θ(t) - η t n n i=1 ∇F i (θ (t) i , Z (t) i ) (13) ≤ f ( θ(t) ) -E Z (t) |Ft-1 ∇f ( θ(t) ), η t n n i=1 ∇F i (θ (t) i , Z (t) i ) + L 2 E Z (t) |Ft-1 η t n n i=1 ∇F i (θ (t) i , Z (t) i ) 2 2 = f ( θ(t) ) -∇f ( θ(t) ), η t n n i=1 ∇f i (θ (t) i ) T4 + Lη 2 t 2 E Z (t) |Ft-1 1 n n i=1 ∇F i (θ (t) i , Z (t) i ) 2 2 T5</formula><p>Adding and subtracting η t ∇f ( θ(t) ) in T 4 , we have</p><formula xml:id="formula_74">T 4 = -η t ∇f ( θ(t) ) 2 2 + η t n n i=1 ∇f ( θ(t) ), ∇f i ( θ(t) ) -∇f i (θ (t) i ) (19),α=1 ≤ -η t ∇f ( θ(t) ) 2 2 + η t 2 ∇f ( θ(t) ) 2 2 + η t 2n n i=1 ∇f i ( θ(t) ) -∇f i (θ (t) i ) 2 2 (14) ≤ - η t 2 ∇f ( θ(t) ) 2 2 + L 2 η t 2n n i=1 θ(t) -θ (t) i 2 2 .</formula><p>Let us now bound the term T 5 :</p><formula xml:id="formula_75">T 5 = E Z (t) |Ft-1 1 n n i=1 ∇F i (θ (t) i , Z (t) i ) - 1 n n i=1 ∇f i (θ (t) i ) + 1 n n i=1 ∇f i (θ (t) i ) 2 2 = 1 n 2 E Z (t) |Ft-1 n i=1 ∇F i (θ (t) i , Z (t) i ) - n i=1 ∇f i (θ (t) i ) 2 2 + 1 n n i=1 ∇f i (θ (t) i ) 2 2 (A.2) = σ2 n + 1 n n i=1 ∇f i (θ (t) i ) -∇f ( θ(t) ) + ∇f ( θ(t) ) 2 2 (17) ≤ σ2 n + 2 1 n n i=1 ∇f i (θ (t) i ) -∇f ( θ(t) ) 2 2 + 2 ∇f ( θ(t) ) 2 2 (17) ≤ σ2 n + 2 n n i=1 ∇f i (θ (t) i ) -∇f i ( θ(t) ) 2 2 + 2 ∇f ( θ(t) ) 2 2 (14) ≤ σ2 n + 2L 2 n n i=1 θ (t) i -θ(t) 2 2 + 2 ∇f ( θ(t) ) 2 2 .</formula><p>Next, plugging T 4 and T 5 into the first inequality, we have:</p><formula xml:id="formula_76">E Z (t) |Ft-1 f ( θ(t+1) ) ≤ f ( θ(t) ) -η t 1 2 -Lη t ∇f ( θ(t) ) 2 2 + L 2 η t 2n + L 3 η 2 t n Θ (t) -Θ (t) 2 F + Lσ 2 2n η 2 t .</formula><p>Since by hypothesis</p><formula xml:id="formula_77">η t ≤ 1 4L , we have 1 2 -Lη t ≥ 1 4 and L 2 ηt 2n + L 3 η 2 t n ≤ L 2 ηt n , we therefore get E Z (t) |Ft-1 f ( θ(t+1) ) ≤ f ( θ(t) ) - η t 4 ∇f ( θ(t) ) 2 2 + L 2 η t n Θ (t) -Θ (t) 2 F + Lσ 2 2n η 2 t .</formula><p>Subtracting each side of the equation by f , we obtain the final result.</p><p>Lemma 3 (Consensus Control). Consider the setting of Theorem 1 and let η t ≤ p 8L , then:</p><formula xml:id="formula_78">E Θ (t) -Θ (t) 2 F ≤ (1 - p 4 )E Θ (t-1) -Θ (t-1) 2 F + 6nτ 2 p η 2 t-1 . (<label>22</label></formula><formula xml:id="formula_79">)</formula><p>Proof. For any α &gt; 0, we have:</p><formula xml:id="formula_80">E Θ (t) -Θ (t) 2 F = E Θ (t) I - 11 T n 2 F = E Θ (t-1) -η t-1 ∇F (Θ (t-1) , Z (t-1) ) W (t-1)T I - 11 T n 2 F (10) = E Θ (t-1) -η t-1 ∇F (Θ (t-1) , Z (t-1) ) W (t-1)T - 11 T n 2 F (18) ≤ (1 + α)E Θ (t-1) W (t-1)T - 11 T n 2 F +(1 + α -1 )η 2 t-1 E ∇F (Θ (t-1) , Z (t-1) ) W (t-1)T - 11 T n 2 F T3 (A.3) ≤ (1 + α)(1 -p)E Θ (t-1) -Θ (t-1) 2 F + (1 + α -1 )η 2 t-1 T 3 .</formula><p>We now bound T 3 by relying on Assumption 4:</p><formula xml:id="formula_81">T 3 = E ∇F (Θ (t-1) , Z (t-1) ) -∇F (Θ (t-1) , Z (t-1) ) + ∇F (Θ (t-1) , Z (t-1) ) • • W (t-1)T - 11 T n 2 F (17) ≤ 2E ∇F (Θ (t-1) , Z (t-1) ) -∇F (Θ (t-1) , Z (t-1) ) W (t-1)T - 11 T n 2 F +2E ∇F (Θ (t-1) , Z (t-1) ) W (t-1)T - 11 T n 2 F = 2E ∇F (Θ (t-1) , Z (t-1) ) -∇F (Θ (t-1) , Z (t-1) ) W (t-1)T - 11 T n 2 F +2 n i=1 E n j=1 W (t-1) ij ∇F j ( θ(t-1) , Z (t-1) j ) - 1 n n j=1 ∇F j ( θ(t-1) , Z (t-1) j ) 2 2 (3) ≤ 2E ∇F (Θ (t-1) , Z (t-1) ) -∇F (Θ (t-1) , Z (t-1) ) W (t-1)T - 11 T n 2 F + 2nτ 2 .</formula><p>For conciseness, we will denote F i (θ</p><formula xml:id="formula_82">(t-1) i , Z (t-1) j</formula><p>) by F i (θ</p><formula xml:id="formula_83">(t-1) i</formula><p>) and ∇F (Θ, Z (t-1) ) by ∇F (Θ). Using Assumption 3, we can bound the first term of the previous equation by:</p><formula xml:id="formula_84">2(1 -p)E ∇F (Θ (t-1) ) -∇F (Θ (t-1) ) -∇F (Θ (t-1) ) -∇F (Θ (t-1) ) 2 F (17) ≤ 4(1 -p) E ∇F (Θ (t-1) ) -∇F (Θ (t-1) ) 2 F + E ∇F (Θ (t-1) ) -∇F (Θ (t-1) ) 2 F = 4(1 -p)× ×   n i=1 E ∇F i (θ (t-1) i ) -∇F i ( θ(t-1) ) 2 2 + E 1 n n j=1 ∇F j (θ (t-1) j ) -∇F j ( θ(t-1) ) 2 2   (A.1) ≤ 4(1 -p)   L 2 n i=1 E θ (t-1) i -θ(t-1) 2 2 + n n 2 E n j=1 ∇F j (θ (t-1) j ) -∇F j ( θ(t-1) ) 2 2   (17) ≤ 4(1 -p)   L 2 n i=1 E θ (t-1) i -θ(t-1) 2 2 + n j=1 E ∇F j (θ (t-1) j ) -∇F j ( θ(t-1) ) 2 2   (A.1) ≤ 4(1 -p)   L 2 n i=1 E θ (t-1) i -θ(t-1) 2 2 + L 2 n j=1 E θ (t-1) j -θ(t-1) 2 2   = 8(1 -p)L 2 E Θ (t-1) -Θ (t-1) 2 F .</formula><p>Combining all previous results and setting α = p 2 , we get:</p><formula xml:id="formula_85">E Θ (t) -Θ (t) 2 F ≤ (1 + α)(1 -p)E Θ (t-1) -Θ (t-1) 2 F + 8(1 + α -1 )(1 -p)L 2 η 2 t-1 E Θ (t-1) -Θ (t-1) 2 F + 2(1 + α -1 )η 2 t-1 nτ 2 ≤ (1 + p 2 )(1 -p) ≤1-p 2 E Θ (t-1) -Θ (t-1) 2 F + 8(1 + 2 p )(1 -p) ≤ 16 p L 2 η 2 t-1 E Θ (t-1) -Θ (t-1) 2 F + 2(1 + 2 p ) ≤ 6 p η 2 t-1 nτ 2 .</formula><p>Since by hypothesis we have η t-1 ≤ p 8L , we can bound the second term and get:</p><formula xml:id="formula_86">E Θ (t) -Θ (t) 2 F ≤ 1 - p 2 + p 4 E Θ (t-1) -Θ (t-1) 2 F + 6nτ 2 p η 2 t-1 = 1 - p 4 E Θ (t-1) -Θ (t-1) 2 F + 6nτ 2 p η 2 t-1 .</formula><p>Corollary 1 (Consensus recursion). Consider the setting of Theorem 1 and fix η t = η ≤ p 8L , we have:</p><formula xml:id="formula_87">1 T + 1 T t=0 E Θ (t) -Θ (t) 2 F ≤ 24η 2 nτ 2 p 2 . (<label>23</label></formula><formula xml:id="formula_88">)</formula><p>Proof. Unrolling the expression ( <ref type="formula" target="#formula_78">22</ref>) in Lemma 3 up to t = 0, we have for all t &gt; 0:</p><formula xml:id="formula_89">E Θ (t) -Θ (t) 2 F ≤ 1 - p 4 t Θ (0) -Θ (0) 2 F =0 + 6nτ 2 p η 2 t-1 j=0 1 - p 4 j = 6nτ 2 p η 2 × 1 -1 -p 4 t 1 -1 -p 4 ≤ 6η 2 nτ 2 p × 4 p = 24η 2 nτ 2 p 2</formula><p>Summing and dividing by T + 1, we get the final result.</p><p>Lemma 4 (Convergence rate with T -Convex case). Consider the setting of Theorem 1 in the convex case. There exists a constant stepsize η ≤ η max = p 8L such that</p><formula xml:id="formula_90">1 T + 1 T t=0 E(f ( θ(t) ) -f ) ≤ 2 br 0 T + 1 1 2 + 2e 1 3 r 0 T + 1 2 3 + dr 0 T + 1 , (<label>24</label></formula><formula xml:id="formula_91">)</formula><formula xml:id="formula_92">where b = σ2 n , e = 36Lτ 2 p 2 , d = 8L p and r 0 = θ (0) -θ 2 2 .</formula><p>Proof. Thanks to the descent lemma (Lemma 1), we almost surely have:</p><formula xml:id="formula_93">f ( θ(t) ) -f ≤ 1 η θ(t) -θ 2 -E Z (t) |Ft-1 θ(t+1) -θ 2 + η 2 σ2 n + 3L 2n η Θ (t) -Θ<label>(t) 2 F</label></formula><p>, where all terms are F t-1 -measurable. Therefore,</p><formula xml:id="formula_94">E(f ( θ(t) ) -f ) ≤ 1 η E θ(t) -θ 2 -E θ(t+1) -θ 2 + η 2 σ2 n + 3L 2n ηE Θ (t) -Θ (t) 2 F ,</formula><p>and summing up we get:</p><formula xml:id="formula_95">1 T + 1 T t=0 E(f ( θ(t) ) -f ) ≤ 1 η(T + 1) T t=0 E θ(t) -θ 2 -E θ(t+1) -θ 2 + η 2 σ2 n + 3L 2n ηE Θ (t) -Θ (t) 2 F ≤ 1 η(T + 1) θ (0) -θ 2 + ησ 2 n + 3L 2n 1 T + 1 T t=0 E Θ (t) -Θ (t) 2 F (23) ≤ 1 η(T + 1) θ (0) -θ 2 + σ2 n η + 36Lτ 2 p 2 η 2 . Fixing η = min r0 b(T +1) 1 2 , r0 e(T +1) 1 3 , 1 d with b = σ2 n , e = 36Lτ 2 p 2 , d = 8L</formula><p>p and r 0 = θ (0) -θ 2 2 , then applying Lemma 6 that is recalled after, we obtain the final result.</p><p>Lemma 5 (Convergence rate with T -Non convex case). Consider the setting of Theorem 1 in the non-convex case. There exists a constant stepsize η ≤ η max = p 8L such that</p><formula xml:id="formula_96">1 T + 1 T t=0 E ∇f ( θ(t) ) 2 2 ≤ 2 4bf 0 T + 1 1 2 + 2e 1 3 4f 0 T + 1 2 3 + 4df 0 T + 1 , (<label>25</label></formula><formula xml:id="formula_97">)</formula><formula xml:id="formula_98">where b = 2Lσ 2 n , e = 96L 2 τ 2 p 2 , d = 8L p and f 0 = f (θ (0) ) -f .</formula><p>Proof. Similarly to Lemma 4 for the convex case, we can use the descent Lemma 2 and obtain</p><formula xml:id="formula_99">E ∇f ( θ(t) ) 2 2 ≤ 4 η Ef t -Ef t+1 + L 2 η n E Θ (t) -Θ (t) 2 F + Lσ 2 2n η 2 ,</formula><p>where for all t ≥ 0, f t f ( θ(t) ) -f . Then summing up and dividing by T + 1 we get:</p><formula xml:id="formula_100">1 T + 1 T t=0 E ∇f ( θ(t) ) 2 2 ≤ 4f 0 η(T + 1) + 4L 2 n 1 T + 1 T t=0 E Θ (t) -Θ (t) 2 F + 2Lσ 2 n η (23) ≤ 4f 0 η(T + 1) + 4L 2 n 24η 2 nτ 2 p 2 + 2Lσ 2 n η = 4f 0 η(T + 1) + 96L 2 τ 2 p 2 η 2 + 2Lσ 2 n η . Fixing η = min 4f0 b(T +1) 1 2 , 4f0 e(T +1) 1 3 , 1 d with b = 2Lσ 2 n , e = 96L 2 τ 2 p 2 , d = 8L p and f 0 = f (θ (0)</formula><p>) -f , we can apply Lemma 6 and obtain the final result.</p><p>Lemma 6 (Tuning stepsize <ref type="bibr" target="#b22">(Koloskova et al., 2020)</ref>). For any parameter r 0 , b, e, d ≥ 0, T ∈ N, we can fix</p><formula xml:id="formula_101">η = min r 0 b(T + 1) 1 2 , r 0 e(T + 1) 1 3 , 1 d ≤ 1 d ,</formula><p>and get r 0 η(T + 1)</p><formula xml:id="formula_102">+ bη + eη 2 ≤ 2 br 0 T + 1 1 2 + 2e 1 3 r 0 T + 1 2 3 + dr 0 T + 1 .</formula><p>Proof. The proof of this lemma can be found in the supplementary materials of <ref type="bibr" target="#b22">Koloskova et al. (2020)</ref> (Lemma 15).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Additional Results and Proofs</head><p>Proposition 1. Let Assumptions 2-3 and 5 to be verified. Then Assumption 4 is satisfied with τ 2 = (1 -p) ζ2 + σ2 , where σ2</p><formula xml:id="formula_103">1 n i σ 2 i .</formula><p>Proof. Denoting ∇F (θ) = (∇F 1 (θ, Z 1 ), . . . , ∇F n (θ, Z n )) ∈ R d×n , and using the relation</p><formula xml:id="formula_104">E Y 2 2 = EY 2 2 + E Y -EY 2 2 ,<label>(26)</label></formula><p>we have:</p><formula xml:id="formula_105">H (t) = 1 n E ∇F (θ)W (t) -∇F (θ) 2 F (A.3) ≤ 1 -p n E ∇F (θ) -∇F (θ) 2 F = 1 -p n n i=1 E ∇F i (θ, Z i ) - 1 n n j=1 ∇F j (θ, Z j ) 2 2 (26) = 1 -p n n i=1 ∇f i (θ) - 1 n n j=1 ∇f j (θ) 2 2 + E n j=1 1 {j=i} - 1 n ) ∇F j (θ, Z j ) -∇f j (θ)) 2 2 (A.5) ≤ (1 -p) ζ2 + 1 n n i=1 E n j=1 1 {j=i} - 1 n (∇F j (θ, Z j ) -∇f j (θ)) 2 2 .</formula><p>Since all terms j in the norm are independent and with expectation 0, the expectation of the sum is equal to the sum of expectations and</p><formula xml:id="formula_106">H (t) ≤ (1 -p)  ζ 2 + 1 n n i=1 n j=1 1 {j=i} - 1 n 2 E ∇F j (θ, Z j ) -∇f j (θ) 2 2   = (1 -p) ζ2 + 1 n n j=1 E ∇F j (θ, Z j ) -∇f j (θ) 2 2 n i=1 1 {j=i} - 1 n 2 = n-1 n (A.2) ≤ (1 -p) ζ2 + n -1 n σ2 ≤ (1 -p) ζ2 + σ2 ,</formula><p>which concludes the proof.</p><p>Proposition 2.</p><p>(Bounded neighborhood heterogeneity under label skew) Consider the statistical framework defined above and assume there exists B &gt; 0 such that ∀k = 1, . . . , K and ∀θ</p><formula xml:id="formula_107">∈ R d , E X [∇F (θ; X, Y )|Y = k] -1 K K k =1 E X [∇F (θ; X, Y )|Y = k ] 2 2 ≤ B.</formula><p>Then, denoting π jk P j (Y = k), Assumption 4 is satisfied with:</p><formula xml:id="formula_108">τ 2 = KB n K k=1 n i=1 n j=1 W ij π jk - 1 n n j=1 π jk 2 + σ 2 max n W - 1 n 11 T 2 F .</formula><p>Proof. First, observe that the local objective functions can be re-written</p><formula xml:id="formula_109">f j (θ) = E (X,Y )∼Dj [F (θ; X, Y )] = K k=1 P j (Y = k)E X [F (θ; X, Y )|Y = k] = K k=1 π jk E X [F (θ; X, Y )|Y = k] .</formula><p>From (4), we have the bias-variance decomposition</p><formula xml:id="formula_110">H(θ) ≤ 1 n n i=1 n j=1 W ij ∇f j (θ) -∇f (θ) 2 2 + σ 2 max n W - 1 n 11 T 2 F = 1 n n i=1 n j=1 (W ij - 1 n )∇f j (θ) 2 2 + σ 2 max n W - 1 n 11 T 2 F = 1 n n i=1 n j=1 (W ij - 1 n ) K k=1 π jk E X [∇F (θ; X, Y )|Y = k] 2 2 T4 + σ 2 max n W - 1 n 11 T 2 F .</formula><p>Then, observing that</p><formula xml:id="formula_111">n j=1 (W ij -1 n ) = 0 and K k=1 π jk = 1 imply</formula><p>Refined Convergence and Topology Learning for Decentralized SGD with Heterogeneous Data</p><formula xml:id="formula_112">n j=1 (W ij - 1 n ) K k=1 π jk 1 K n k =1 E X [∇F (θ; X, Y )|Y = k ] = 0 ,</formula><p>we can add this in the norm of the term T 4 defined above and get</p><formula xml:id="formula_113">T 4 = n j=1 (W ij - 1 n ) K k=1 π jk E X [∇F (θ; X, Y )|Y = k] - 1 K K k =1 E X [∇F (θ; X, Y )|Y = k ] 2 2 = K k=1 E X [∇F (θ; X, Y )|Y = k] - 1 K K k =1 E X [∇F (θ; X, Y )|Y = k ] n j=1 (W ij - 1 n )π jk 2 2 (17) ≤ K K k=1 E X [∇F (θ; X, Y )|Y = k] - 1 K K k =1 E X [∇F (θ; X, Y )|Y = k ] n j=1 (W ij - 1 n )π jk 2 2 = K K k=1 E X [∇F (θ; X, Y )|Y = k] - 1 K K k =1 E X [∇F (θ; X, Y )|Y = k ] 2 2 ≤B × n j=1 (W ij - 1 n )π jk 2 ≤ KB K k=1 n j=1 W ij π jk - 1 n n j=1 π jk 2 .</formula><p>Finally, plugging this into the upper-bound on H(θ) found above, we get the final result.</p><p>Theorem 2 Consider the statistical setup presented in Section 5.1 and let { W (l) } L l=1 be the sequence of mixing matrices generated by Algorithm 2. Then, at any iteration l = 1, . . . , L, we have:</p><formula xml:id="formula_114">g( W (l) ) ≤ 16 l+2 λ + 1 n K k=1 (Π :,k -Π :,k 1) • Π T :,k 2 ,</formula><p>where • 2 stands for the nuclear norm, i.e., the sum of singular values. Bounding the second term in the parenthesis, we can obtain the looser bound g( W (l) ) ≤ 16 l+2 (λ + 1) .</p><p>Furthermore, we have d in max ( W (l) ) ≤ l and d out max ( W (l) ) ≤ l, resulting in a per-iteration complexity bounded by l.</p><p>Proof. The proof of this theorem is directly derived from Theorem 3 given below, applied with the parameters of our problem. To prove the first inequality, we first need to find a bound on the diameter of the set of doubly stochastic matrices, denoted diam • (S), for a certain (matrix) norm • . We fix this norm to be the operator norm induced by the 2 -norm, denoted • 2 , which is simply the maximum singular value of the matrix.</p><p>For all W, P ∈ S, we have</p><formula xml:id="formula_115">W -P 2 ≤ W 2 + P 2 = 1 + 1 = 2 ,</formula><p>which comes from the fact that W and P are doubly stochastic, i.e., their largest eigenvalue is 1. This shows that diam • 2 (S) ≤ 2.</p><p>Let us now find the Lipschitz constant associated to the gradient of the objective:</p><formula xml:id="formula_116">∇g(W ) = 2 n K k=1 (W Π :,k -Π :,k 1) • Π T :,k + 2 n λ W - 11 T n .</formula><p>Recall that the dual norm • 1 of • 1 is the nuclear norm, i.e., the sum of the singular values.</p><p>For any W, P ∈ S, we have</p><formula xml:id="formula_117">∇g(W ) -∇g(P ) 2 = 2 n (W -P ) λI + K k=1 Π :,k Π T :,k 2 ≤ 2 n λ(W -P )I 2 + 2 n (W -P ) K k=1 Π :,k Π T :,k 2 ≤ 2λ n W -P 2 I 2 + 2 n (W -P ) K k=1 Π :,k Π T :,k 2 ,</formula><p>where the last inequality is obtained using the fact that for any real matrices A and B, AB ≤ A T B .</p><p>Before bounding the second term, we must observe that because W and P are doubly stochastic, (W -P )1 = 0 and therefore, for any matrix A ∈ R n×n , (W -P )A = (W -P )(A -11 T n A). Now, the second term can be re-written and bounded as follows:</p><formula xml:id="formula_118">2 n (W -P ) K k=1 Π :,k Π T :,k 2 = 2 n (W -P ) K k=1 Π :,k Π T :,k - 11 T n K k=1 Π :,k Π T :,k 2 ≤ 2 n W -P 2 K k=1 Π :,k Π T :,k - 11 T n K k=1 Π :,k Π T :,k 2 = 2 n W -P 2 K k=1 (Π :,k -Π :,k 1) • Π T :,k 2 .</formula><p>Plugging the previous result into the bound obtained above, and since I 2 = n, we get</p><formula xml:id="formula_119">∇g(W ) -∇g(P ) 2 ≤ 2 λ + 1 n K k=1 (Π :,k -Π :,k 1) • Π T :,k 2 W -P 2 .</formula><p>We can now apply Theorem 3 with the found Lipschitz constant and diameter, which gives:</p><formula xml:id="formula_120">g( W (l) ) -g(W ) ≤ 16 l + 2 λ + 1 n K k=1 (Π :,k -Π :,k 1) • Π T :,k 2 ,</formula><p>where W is the optimal solution of the problem. Since we known that W = 11 T n with g(W ) = 0, we obtain the first inequality in Theorem 2.</p><p>To prove the second inequality, it suffices to show that</p><formula xml:id="formula_121">K k=1 (Π :,k -Π :,k 1) • Π T :,k 2 ≤ n. We have: K k=1 (Π :,k -Π :,k 1) • Π T :,k 2 = I - 11 T n K k=1 Π :,k Π T :,k 2 ≤ I - 11 T n 2 K k=1 Π :,k Π T :,k 2 = K k=1 Π :,k Π T :,k 2 ≤ K k=1 Π :,k Π T :,k 2 .</formula><p>Because for any k = 1, . . . , K, Π :,k Π T :,k is a rank-1 matrix, its unique eigenvalue is Π T :,k Π :,k and therefore</p><formula xml:id="formula_122">K k=1 (Π :,k -Π :,k 1) • Π T :,k 2 ≤ K k=1 Π :,k Π T :,k 2 = K k=1 Π T :,k Π :,k = K k=1 n i=1 π 2 ik Holder ≤ n i=1 max k {π ik } K k=1 π ik =1 ≤ n i=1 1 = n ,</formula><p>which concludes the proof of the second inequality in Theorem 2.</p><p>The last statement of the theorem follows directly from the structure of permutation matrices and the greedy nature of the algorithm.</p><p>Theorem 3. (Frank-Wolfe Convergence <ref type="bibr" target="#b19">(Jaggi, 2013;</ref><ref type="bibr" target="#b5">Bubeck, 2014)</ref>) Let the gradient of the objective function g : x → g(x) be L-smooth with respect to a norm • and its dual norm • :</p><formula xml:id="formula_123">∇g(x) -∇g(y) ≤ L x -y .</formula><p>If g is minimized over S using Frank-Wolfe algorithm, then for each l ≥ 1, the iterates x (l) satisfy</p><formula xml:id="formula_124">g(x (l) ) -g(x ) ≤ 2Ldiam • (S) 2 l + 2 ,</formula><p>where x ∈ S is an optimal solution of the problem and diam • (S) stands for the diameter of S with respect to the norm • .</p><p>Proof. The proof of this theorem is a direct combination of Theorem 1 and Lemma 7 in Jaggi ( <ref type="formula">2013</ref>), both proved in the paper.</p><p>Proposition 3. (Relation between p and W -11 T n 2 F ) Let W be a mixing matrix satisfying Assumption 3. Then,</p><formula xml:id="formula_125">(1 -p) ≤ W - 11 T n 2 F ≤ (n -1)(1 -p) .</formula><p>Proof. The upper-bound is a direct application of Assumption 3 with M = I, the identity matrix of size n:</p><formula xml:id="formula_126">W T - 11 T n 2 F = IW T -I 11 T n 2 F (A.3) ≤ (1 -p) I - 11 T n 2 F = (1 -p)(n -1) .</formula><p>To show the lower-bound, denote by s 1 (M ), . . . , s n (M ) the (decreasing) singular values of any square matrix M ∈ R n×n . Denote similarly λ 1 (M ), . . . , λ n (M ) the eigenvalues of any symmetric square matrix M ∈ R n×n .</p><p>W -</p><formula xml:id="formula_127">11 T n 2 F = n i=1 s 2 i W - 11 T n ≥ s 2 1 W - 11 T n = λ 1 (W - 11 T n ) T (W - 11 T n ) = λ 1 W T W - 11 T n = λ 2 (W T W ) ≥ 1 -p .</formula><p>The last equality is obtained by noticing that W T W is a symmetric doubly stochastic matrix. It therefore admits an eigenvalue decomposition where the largest eigenvalue 1 is associated with the eigenvector 1 √ n 1. This makes W T W -11 T n having the eigenvalue 0 associated to the vector 1 √ n 1 and the largest eigenvalue of W T W -11 T n becomes the second-largest eigenvalue of W T W . The final inequality comes from the fact that Assumption 3 is always true with p = 1 -λ 2 (W T W ) which implies that the best p satisfying Assumption 3 in necessarily greater or equal to 1 -λ 2 (W T W ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1 Extension to Random Mixing Matrices</head><p>As mentioned in Section 3, all our theoretical results can easily be extended to random mixing matrices. In that framework, at each iteration t of the D-SGD algorithm, the matrix W (t) is sampled from a doubly stochastic matrix distribution denoted W (t) , independent of the iterates of parameters θ (t) , and possibly time-varying.</p><p>To obtain the convergence result, we slightly modify Assumption 3 and Assumption 4 by adding an expectation with respect to W in front of the equations. For instance, Assumption 3 becomes</p><formula xml:id="formula_128">E W ∼W M W T -M 2 F ≤ (1 -p) M -M 2 F .</formula><p>Then, the statement of Theorem 1 is also slightly modified by assuming that it is the distributions W (0) , . . . , W (T -1) that must now respect Assumptions 3 and 4.</p><p>By appropriately conditioning with respect to the random mixing matrices or with respect to the iterates, the proof of the theorem remains the same.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 Closed-Form for the Line-Search</head><p>In this section, we give the closed-form solution of the line-search problem found in the Frank-Wolfe algorithm 2. Recall that we seek to solve:</p><formula xml:id="formula_129">γ = arg min γ∈[0,1] g(γ) g ((1 -γ)W + γP ) , with g(W ) = 1 n W Π - 11 T n Π 2 F + λ n W - 11 T n 2 F .</formula><p>The function g being quadratic, the objective g(γ) is also quadratic with respect to γ. Hence, it suffices to put the derivative g of g equal to 0, and we get the closed-form solution:</p><formula xml:id="formula_130">γ = K k=1 (Π :,k 1 -W Π :,k ) T (P -W ) • Π :,k -λ • tr W -11 T n T (P -W ) (P -W )Π 2 F + λ P -W 2 F .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Additional Experiments</head><p>In this section, we provide additional details on our experimental setup, as well as additional results to complement the main results in the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.1 Detailed Experimental Setup</head><p>Our main goal is to provide a fair comparison of the convergence speed across different topologies in order to show the benefits of the principled approach to topology learning provided by STL-FW. We essentially follow the experimental setup in <ref type="bibr" target="#b1">Bellet et al. (2022)</ref>, which we recall below.</p><p>In our study, we focus our investigation on the convergence speed, rather than the final accuracy after a fixed number of iterations. Indeed, depending on when training is stopped, the relative difference in final accuracy across different algorithms may vary significantly and lead to different conclusions. Instead of relying on somewhat arbitrary stopping points, we show the convergence curves of generalization performance (i.e., the accuracy on the test set throughout training), up to a point where it is clear that the different approaches have converged, will not make significantly more progress, or behave essentially the same.</p><p>Datasets. We experiment with two datasets: MNIST <ref type="bibr" target="#b12">(Deng, 2012)</ref> and CIFAR10 <ref type="bibr" target="#b26">(Krizhevsky et al., 2009)</ref> Models. We use a logistic regression classifier for MNIST, which provides up to 92.5% accuracy in the centralized setting. For CIFAR10, we use a Group-Normalized variant of LeNet <ref type="bibr" target="#b17">(Hsieh et al., 2020)</ref>, a deep convolutional network which achieves an accuracy of 74.15% in the centralized setting. These models are thus reasonably accurate (which is sufficient to study the effect of the topology) while being sufficiently fast to train in a fully decentralized setting and simple enough to configure and analyze. Regarding hyper-parameters, we use the learning rate and mini-batch size found in <ref type="bibr" target="#b1">Bellet et al. (2022)</ref> after cross-validation for n = 100 nodes, respectively 0.1 and 128 for MNIST and 0.002 and 20 for CIFAR10.</p><p>Metrics. We evaluate a network of n = 100 nodes, creating multiple models in memory and simulating the exchange of messages between nodes. To ignore the impact of distributed execution strategies and system optimization techniques, we report the test accuracy of all nodes (min, max, average) as a function of the number of times each example of the dataset has been sampled by a node, i.e. an epoch. This is equivalent to the classic case of a single node sampling the full distribution.</p><p>All our results were obtained on a custom version of the non-iid topology simulator made available online by the authors of <ref type="bibr" target="#b1">Bellet et al. (2022)</ref>,<ref type="foot" target="#foot_1">2</ref> which provides deterministic and fully replicable experiments on top of Pytorch and ensures all topologies were used in the same algorithm implementation and used exactly the same inputs.</p><p>Baselines We compare our results against an ideal baseline: a fully-connected network topology with the same number of nodes. All other things being equal, any other topology using less edges will converge at the same speed or slower: this is therefore the most difficult and general baseline to compare against. This baseline is also essentially equivalent to a centralized (single) IID node using a batch size n times bigger, where n is the number of nodes. Both a fully-connected  network and a single IID node effectively optimize a single model and sample uniformly from the global distribution: both thus remove entirely the effect of label distribution skew and of the network topology on the optimization. In practice, we prefer a fully-connected network because it converges slightly faster and obtains slightly better final accuracy than a single node sampling randomly from the global distribution.</p><p>We also provide comparisons against popular sparse topologies, such as random graphs and exponential graphs <ref type="bibr" target="#b46">(Ying et al., 2021)</ref>. For the random graph, we use a similar number of edges (d max ) per node to determine whether a simple sparse topology could work equally well. For the exponential graph, we follow the deterministic construction of <ref type="bibr" target="#b46">Ying et al. (2021)</ref> and consider edges to be undirected, resulting in d max = 14 for n = 100.</p><p>We finally compare against D-Cliques <ref type="bibr" target="#b1">(Bellet et al., 2022)</ref>, the only competitor which takes into account the data heterogeneity in the choice of topology. D-Cliques constructs a topology around sparsely inter-connected cliques such that the union of local datasets within a clique is representative of the global distribution, i.e. it minimizes the first term in our objective function (Eq. 8) within each clique.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2 Statistics of the Used Topologies</head><p>In this section, we provide tables containing important statistics about the topologies used in the experiments. In each table, a row corresponds to a specific topology having at most d max in and out-neighbors per node. The columns are as follows:</p><p>• In-degree (respectively Out-degree): average and standard deviation of the number of incoming (respectively outgoing) edges per node.</p><p>• Classes in neighborhood: average and standard deviation of the number of different classes in the direct neighborhood of a node. Recall that each node individually observes examples only from a subset of the 10 different classes (1 for the synthetic dataset, 2 for MNIST, 2-4 for CIFAR10).</p><p>• Bias: average and standard deviation of ( n j=1 W ij π jk -1 n n j=1 π jk ) 2 across each node i. In other words, it measures the neighborhood heterogeneity in terms of class proportions, which (up to a constant factor) corresponds to the bias term in (7). According to our theory, the smaller the bias term, the better the topology.</p><p>• 1 -p: the mixing parameter of the topology (see Assumption 3). Recall that for a topology W , 1 -p = λ 2 (W T W ).</p><p>According to our theory (and prior work, see e.g., <ref type="bibr" target="#b22">Koloskova et al., 2020)</ref>, the smaller 1 -p, the better the topology.</p><p>Interestingly, all tables show that our algorithm STL-FW outputs topologies that are d max -regular. Therefore, the communication burden is the same for all nodes. This is a desirable property for scalability, that the star topology induced by server-based federated learning does not satisfy.</p><p>Table <ref type="table" target="#tab_0">1</ref> provides the statistics of the topologies used in the synthetic data experiment. We observe that the mixing parameter p are similar for both our topologies (STL-FW) and the random d-regular graph. However, STL-FW achieves much smaller bias, resulting in more classes being represented in the neighborhood of each node. This explains the faster convergence of D-SGD with our topology, in line with our theoretical results.</p><p>Table <ref type="table" target="#tab_2">2</ref> (MNIST) and Table <ref type="table" target="#tab_3">3</ref> (CIFAR10) provide the statistics of the topologies used in the real data experiments. The same conclusions made regarding the synthetic experiments hold here regarding the comparison of STL-FW with the random d-regular graphs. D-Cliques <ref type="bibr" target="#b1">(Bellet et al., 2022)</ref>, the only topology that is also constructed in a data-dependent fashion, achieves rather low bias (albeit slightly larger than our STL-FW topology) but has rather bad mixing properties (large 1 -p). This confirms our claim that D-Cliques reduces the bias without ensuring good mixing (due to the constrained arrangements of nodes in sparsely interconnected cliques). This explains the superior performance of our topology. Last but not least, looking at d max = 5, we notice that D-Cliques is unable to satisfy the constraints that the maximum degree STL-FW (ours) 10.0 ± 0.0 10.0 ± 0.0 10.0 ± 0.0 0.001 ± 0.001 0.35 Random d-regular 10.0 ± 0.0 10.0 ± 0.0 9.31 ± 0.76 0.03 ± 0.02 0.39 D-cliques 9.9 ± 0.3 9.9 ± 0.3 10.0 ± 0.0 0.005 ± 0.002 0.84 Exponential 14.0 ± 0.0 14.0 ± 0.0 9.72 ± 0.51 0.02 ± 0.01 0.54 </p><formula xml:id="formula_131">d max = 10</formula><p>STL-FW (ours) 10.0 ± 0.0 10.0 ± 0.0 10.0 ± 0.0 0.001 ± 0.001 0.45 Random d-regular 10.0 ± 0.0 10.0 ± 0.0 9.26 ± 0.82 0.033 ± 0.016 0.39 D-cliques 9.9 ± 0.3 9.9 ± 0.3 10.0 ± 0.0 0.004 ± 0.002 0.84 Exponential 14.0 ± 0.0 14.0 ± 0.0 9.68 ± 0.58 0.024 ± 0.012 0.54 should not exceed 5. This also illustrates the greater flexibility of STL-FW when it comes to controlling the per-iteration communication complexity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.3 Impact of λ in STL-FW</head><p>Figure <ref type="figure">3</ref> shows the impact of λ, which rules the bias-variance trade-off in our objective function for learning the topology. We present results for two extreme values, respectively 0.0001 and 1000 as well as middle ground of 0.1. For both datasets, λ has little effect on convergence speed. From a practical perspective, this is an advantage as it removes the need for tuning λ (one can simply set it to a default positive value). This behavior may be explained by the fact that reducing the bias term alone also leads to a reduction of variance. Hence, the variance term becomes useful only when the bias term has been "erased" (or made very small), which can happen only after a certain number of STL-FW iterations, i.e., for a potentially large d max . For all other experiments, we used λ = 0.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.4 Impact of d max on STL-FW</head><p>Figure <ref type="figure">4</ref> shows on a single plot the impact of the communication budget d max of STL-FW on the convergence speed of D-SGD. The communication budget has a strong impact in both cases, with STL-FW providing the same convergence speed as fully-connected when d max = 99, but with some residual variance because some nodes end up wth less than 99 edges. Most of the benefits of STL-FW are obtained with the first 10 edges, with additional edges providing only marginal benefits compared to fully-connected. We thus chose to show all experiments of the main text with three budgets, a small d max = 2, a medium d max = 5, and a large budget d max = 10.</p><p>Finally, for a small budget d max = 2, we had seen in Figure <ref type="figure" target="#fig_1">2</ref> in the main text that STL-FW did not provide significant benefits compared to a random graph on CIFAR10. Figure <ref type="figure">5</ref> shows that as soon as d max = 3, STL-FW starts providing benefits compared to a random topology on CIFAR10.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Figure 1: (a) Evolution of key quantities across the iterations of topology learning: in red the objective function g(W (l) ), in green the bias term sup θ</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure2: Convergence of D-SGD with STL-FW (our approach) and alternative topologies on real datasets under different communication budgets. The fully connected graph induces intractable communication costs but gives a performance upper bound, while the exponential graph is shown for dmax = 10 but exceeds this budget.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>) , . . . , θ(t) = Θ (t) • 1 n 11 T .The proof follows the classical steps found in the literature (see e.g.<ref type="bibr" target="#b22">Koloskova et al. (2020)</ref>;Neglia et al. (</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>1.</head><label></label><figDesc>Lemma 1 provides a descent recursion that allows to control the decreasing of the term θ(t) -θ 2 . The proof closely follows the one of<ref type="bibr" target="#b22">Koloskova et al. (2020)</ref>;<ref type="bibr" target="#b36">Neglia et al. (2020)</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>, which both have K = 10 classes. For MNIST, we use 50k and 10k examples from the original set for training and testing respectively. For CIFAR10, we used 50k images of the original training set for training and 10k examples of the test set for measuring prediction accuracy. For both MNIST and CIFAR10, we use the heterogeneous data partitioning scheme proposed by McMahan et al. (2017) in their seminal FL work: we sort all training examples by class, then split the list into shards of equal size, and randomly assign two shards to each node. When the number of examples of one class does not divide evenly in shards, as is the case for MNIST, some shards may have examples of more than one class and therefore nodes may have examples of up to 4 classes. However, most nodes will have examples of 2 classes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Statistics of the topologies used in the synthetic data experiments of Section 6.1.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Statistics of the topologies used on the MNIST experiments of Section 6.2.</figDesc><table><row><cell></cell><cell>Topology</cell><cell>In-degree</cell><cell cols="2">Out-degree Classes in neighborhood</cell><cell>Bias</cell><cell>1 -p</cell></row><row><cell>d max = 2</cell><cell>STL-FW (ours) Random d-regular</cell><cell>2.0 ± 0.0 2.0 ± 0.0</cell><cell>2.0 ± 0.0 2.0 ± 0.0</cell><cell>5.79 ± 0.45 4.86 ± 0.81</cell><cell>0.08 ± 0.02 0.14 ± 0.06</cell><cell>0.99 0.99</cell></row><row><cell></cell><cell>STL-FW (ours)</cell><cell>5.0 ± 0.0</cell><cell>5.0 ± 0.0</cell><cell>9.98 ± 0.14</cell><cell cols="2">0.008 ± 0.005 0.64</cell></row><row><cell>d max = 5</cell><cell>Random d-regular</cell><cell>5.0 ± 0.0</cell><cell>5.0 ± 0.0</cell><cell>7.4 ± 0.97</cell><cell>0.07 ± 0.03</cell><cell>0.68</cell></row><row><cell></cell><cell>D-cliques</cell><cell cols="2">5.82 ± 0.38 5.82 ± 0.38</cell><cell>9.71 ± 0.55</cell><cell cols="2">0.022 ± 0.012 0.99</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Statistics of the topologies used in the CIFAR10 experiments of Section 6.2.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>The algorithm is quite fast in practice: for instance, the scipy implementation runs in 0.3s on a regular laptop for n = 1000.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>https://gitlab.epfl.ch/sacs/distributed-ml/non-iid-topology-simulator</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix A Details on Example 1</head><p>In this section, we provide more details on Example 1 (Section 4.1) by giving the exact parametrization. Recall that we want to find an example where Assumption 5 is not verified while Assumption 4 is.</p><p>Let us consider n nodes with n an even number. For all i = 1, . . . , n, assume Z i ∼ N (m, σ2 ) if i is odd and Z i ∼ N (-m, σ2 ) if i is even. Assume further that σ2 &lt; +∞ but m &gt; 0 can be asymptotically large. For all i = 1, . . . , n we fix F i (θ, Z i ) = (θ -Z i ) 2 , which corresponds to a simple mean estimation objective.</p><p>Consider a fixed mixing matrix W associated with a ring topology that alternates between the two distributions. Specifically, for i = 2, . . . , n -1 and j = 1, . . . , n, we fix the weights as follows:</p><p>We first verify that Assumptions 2 is satisfied: </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Stochastic gradient push for distributed deep learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Assran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Loizou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rabbat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">D-Cliques: Compensating for Data Heterogeneity with Topology in Decentralized Federated Learning</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bellet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A.-M</forename><surname>Kermarrec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Lavoie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
			<publisher>SRDS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Optimization methods for large-scale machine learning</title>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">E</forename><surname>Curtis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Nocedal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Siam Review</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="223" to="311" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Fastest mixing markov chain on a graph</title>
		<author>
			<persName><forename type="first">S</forename><surname>Boyd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Diaconis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM review</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="667" to="689" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Randomized gossip algorithms</title>
		<author>
			<persName><forename type="first">S</forename><surname>Boyd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Prabhakar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2508" to="2530" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Convex optimization: Algorithms and complexity</title>
		<author>
			<persName><forename type="first">S</forename><surname>Bubeck</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1405.4980</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Assignment problems: revised reprint</title>
		<author>
			<persName><forename type="first">R</forename><surname>Burkard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dell'amico</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Martello</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<publisher>SIAM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Expander graph and communication-efficient decentralized optimization</title>
		<author>
			<persName><forename type="first">Y.-T</forename><surname>Chow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 50th Asilomar Conference on Signals, Systems and Computers</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1715" to="1720" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Gossip dual averaging for decentralized optimization of pairwise functions</title>
		<author>
			<persName><forename type="first">I</forename><surname>Colin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bellet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Salmon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Clémençon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">On implementing 2d rectangular assignment algorithms</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">F</forename><surname>Crouse</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Aerospace and Electronic Systems</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1679" to="1696" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Data-heterogeneity-aware mixing for decentralized learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Dandi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Koloskova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jaggi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">U</forename><surname>Stich</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.06477</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Optimal distributed online prediction using minibatches</title>
		<author>
			<persName><forename type="first">O</forename><surname>Dekel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Gilad-Bachrach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Shamir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">The mnist database of handwritten digit images for machine learning research</title>
		<author>
			<persName><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="141" to="142" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
	<note>MNIST is distributed under Creative Commons Attribution-Share Alike 3.0</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Cross-Gradient Aggregation for Decentralized Learning from Non-IID data</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Esfandiari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Y</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Balu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Herron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hegde</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sarkar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.02051</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Sample Optimality and All-for-all Strategies in Personalized Federated and Collaborative Learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Even</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Massoulié</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Scaman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.13097</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">An algorithm for quadratic programming</title>
		<author>
			<persName><forename type="first">M</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Wolfe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Naval Research Logistics Quarterly</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="95" to="110" />
			<date type="published" when="1956">1956</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Periodic stochastic gradient descent with momentum for decentralized training</title>
		<author>
			<persName><forename type="first">H</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.10435</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">The Non-IID Data Quagmire of Decentralized Machine Learning</title>
		<author>
			<persName><forename type="first">K</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Phanishayee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Mutlu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">B</forename><surname>Gibbons</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Improving the transient times for distributed stochastic gradient methods</title>
		<author>
			<persName><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Pu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.04851</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Revisiting frank-wolfe: Projection-free sparse convex optimization</title>
		<author>
			<persName><forename type="first">M</forename><surname>Jaggi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Advances and open problems in federated learning</title>
		<author>
			<persName><forename type="first">P</forename><surname>Kairouz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">B</forename><surname>Mcmahan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Avent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bellet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foundations and Trends® in Machine Learning</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="1" to="210" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">An improved analysis of gradient tracking for decentralized machine learning</title>
		<author>
			<persName><forename type="first">A</forename><surname>Koloskova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">U</forename><surname>Stich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A unified theory of decentralized sgd with changing topology and local updates</title>
		<author>
			<persName><forename type="first">A</forename><surname>Koloskova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Loizou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Boreiri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jaggi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">U</forename><surname>Stich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Decentralized stochastic optimization and gossip algorithms with compressed communication</title>
		<author>
			<persName><forename type="first">A</forename><surname>Koloskova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Stich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jaggi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Consensus control for decentralized deep learning</title>
		<author>
			<persName><forename type="first">L</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Koloskova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jaggi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Stich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Proximity without consensus in online multiagent optimization</title>
		<author>
			<persName><forename type="first">A</forename><surname>Koppel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">M</forename><surname>Sadler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ribeiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Signal Processing</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="3062" to="3077" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIFAR is distributed under MIT license</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.09126</idno>
		<title level="m">Communication-efficient local decentralized sgd methods</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Can decentralized algorithms outperform centralized algorithms? a case study for decentralized parallel stochastic gradient descent</title>
		<author>
			<persName><forename type="first">X</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-J</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Asynchronous Decentralized Parallel Stochastic Gradient Descent</title>
		<author>
			<persName><forename type="first">X</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Quasi-global momentum: Accelerating decentralized deep learning on heterogeneous data</title>
		<author>
			<persName><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">P</forename><surname>Karimireddy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Stich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jaggi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Matching theory</title>
		<author>
			<persName><forename type="first">L</forename><surname>Lovász</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Plummer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>American Mathematical Soc</publisher>
			<biblScope unit="volume">367</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Federated Multi-Task Learning under a Mixture of Distributions</title>
		<author>
			<persName><forename type="first">O</forename><surname>Marfoq</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Neglia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bellet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Kameni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Vidal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Throughput-optimal topology design for cross-silo federated learning</title>
		<author>
			<persName><forename type="first">O</forename><surname>Marfoq</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Neglia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Vidal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Communication-efficient learning of deep networks from decentralized data</title>
		<author>
			<persName><forename type="first">B</forename><surname>Mcmahan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ramage</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hampson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">A</forename><surname>Arcas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AISTATS</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Network topology and communication-computation tradeoffs in decentralized optimization</title>
		<author>
			<persName><forename type="first">A</forename><surname>Nedić</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Olshevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">G</forename><surname>Rabbat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE</title>
		<meeting>the IEEE</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">106</biblScope>
			<biblScope unit="page" from="953" to="976" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Decentralized gradient methods: does topology matter?</title>
		<author>
			<persName><forename type="first">G</forename><surname>Neglia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Towsley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Calbi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AISTATS</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">New convergence aspects of stochastic gradient algorithms</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">M</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">H</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Richtárik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Scheinberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Takác</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Van Dijk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="176" to="177" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">The errorfeedback framework: Better rates for sgd with delayed gradients and compressed updates</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">U</forename><surname>Stich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">P</forename><surname>Karimireddy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="1" to="36" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Stability and generalization of decentralized stochastic gradient descent</title>
		<author>
			<persName><forename type="first">T</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="9756" to="9764" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">D²: Decentralized training over decentralized data</title>
		<author>
			<persName><forename type="first">H</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Greedy algorithms for structurally constrained high dimensional problems</title>
		<author>
			<persName><forename type="first">A</forename><surname>Tewari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ravikumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Dhillon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Birkhoff&apos;s decomposition revisited: Sparse scheduling for highspeed circuit switches</title>
		<author>
			<persName><forename type="first">V</forename><surname>Valls</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Iosifidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Tassiulas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.02752</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Decentralized collaborative learning of personalized models over networks</title>
		<author>
			<persName><forename type="first">P</forename><surname>Vanhaesebrouck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bellet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tommasi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AISTATS</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Beyond spectral gap: The role of the topology in decentralized learning</title>
		<author>
			<persName><forename type="first">T</forename><surname>Vogels</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hendrikx</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jaggi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2206.03093</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Matcha: Speeding up decentralized sgd via matching decomposition sampling</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Sahu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICC</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Exponential graph is provably efficient for decentralized deep training</title>
		<author>
			<persName><forename type="first">B</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Removing data heterogeneity influence enhances network topology dependence of decentralized sgd</title>
		<author>
			<persName><forename type="first">K</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Alghunaim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.08023</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">On the influence of bias-correction on distributed stochastic optimization</title>
		<author>
			<persName><forename type="first">K</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Alghunaim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">H</forename><surname>Sayed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Signal Processing</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="page" from="4352" to="4367" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Decentlam: Decentralized momentum sgd for large-batch deep training</title>
		<author>
			<persName><forename type="first">K</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Fully decentralized joint learning of personalized models and collaboration graphs</title>
		<author>
			<persName><forename type="first">V</forename><surname>Zantedeschi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bellet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tommasi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AISTATS</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Topology-aware generalization of decentralized sgd</title>
		<author>
			<persName><forename type="first">T</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="27479" to="27503" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
