<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="fr">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Towards Sustainable Dairy Management -A Machine Learning Enhanced Method for Estrus Detection</title>
				<funder ref="#_KpfXkmT #_hdMyR3W">
					<orgName type="full">Agence Nationale de la Recherche</orgName>
					<orgName type="abbreviated">ANR</orgName>
				</funder>
				<funder>
					<orgName type="full">APIS-GENE</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Kevin</forename><surname>Fauvel</surname></persName>
							<email>kevin.fauvel@inria.fr</email>
						</author>
						<author>
							<persName><forename type="first">Véronique</forename><surname>Masson</surname></persName>
							<email>veronique.masson@irisa.fr</email>
						</author>
						<author>
							<persName><forename type="first">Elisa</forename><surname>Fromont</surname></persName>
							<email>elisa.fromont@irisa.fr</email>
						</author>
						<author>
							<persName><forename type="first">Philippe</forename><surname>Faverdin</surname></persName>
							<email>philippe.faverdin@inra.fr</email>
						</author>
						<author>
							<persName><forename type="first">Alexandre</forename><surname>Termier</surname></persName>
							<email>alexandre.termier@irisa.fr</email>
						</author>
						<author>
							<persName><forename type="first">Élisa</forename><surname>Fromont</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Univ Rennes</orgName>
								<orgName type="institution" key="instit2">Inria</orgName>
								<orgName type="institution" key="instit3">CNRS</orgName>
								<orgName type="institution" key="instit4">IRISA</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">Univ Rennes</orgName>
								<orgName type="institution" key="instit2">Inria</orgName>
								<orgName type="institution" key="instit3">CNRS</orgName>
								<orgName type="institution" key="instit4">IRISA</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution" key="instit1">Univ Rennes</orgName>
								<orgName type="institution" key="instit2">Inria</orgName>
								<orgName type="institution" key="instit3">CNRS</orgName>
								<orgName type="institution" key="instit4">IRISA</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="laboratory">PEGASE</orgName>
								<orgName type="institution" key="instit1">INRA</orgName>
								<orgName type="institution" key="instit2">AGROCAMPUS OUEST</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="institution" key="instit1">Univ Rennes</orgName>
								<orgName type="institution" key="instit2">Inria</orgName>
								<orgName type="institution" key="instit3">CNRS</orgName>
								<orgName type="institution" key="instit4">IRISA</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Towards Sustainable Dairy Management -A Machine Learning Enhanced Method for Estrus Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">B4735FCF5AF3C5F40EB98B40A49C2090</idno>
					<idno type="DOI">10.1145/3292500.3330712</idno>
					<note type="submission">Submitted on 8 Jun 2020</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-04-12T14:47+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>CCS CONCEPTS</term>
					<term>Computing methodologies → Machine learning</term>
					<term>• Applied computing → Agriculture Sustainable Dairy Management</term>
					<term>Machine Learning</term>
					<term>Classification</term>
					<term>Interpretability</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>des établissements d'enseignement et de recherche français ou étrangers, des laboratoires publics ou privés.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="fr">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>As underlined in the report Creating A Sustainable Food Future <ref type="bibr" target="#b28">[28]</ref>, ruminant livestock (cattle, sheep, and goats), used for dairy and meat production, occupy two-thirds of global agricultural land and contribute roughly half of agriculture's production-related emissions. Increased efficiency of resource use in farms is one of the most important steps toward meeting both food production and environmental goals. As a response, precision livestock farming (PLF) is a promising way to improve farm performance <ref type="bibr" target="#b30">[30]</ref>. PLF is the use of continuous information to optimize an individualized animal management.</p><p>Nowadays, data (e.g. temperature, activity, body weight, milk production) is collected in dairy farms through different types of sensors to support farmers' decision making in various aspects of management (e.g. reproduction, diseases, feeding, environment). Machine learning methods can help to exploit the value of this ever-growing volume of data.</p><p>Reproduction is a key factor for dairy farm performance. It directly impacts milk production as cows start to produce milk after giving birth to a calf; and milk productivity declines after the first 3 months. The most prevalent reason for cow culling, the act of slaughtering a cow, is reproduction issue (e.g. long interval between 2 calves) <ref type="bibr" target="#b2">[3]</ref>. So, it is crucial to detect estrus, the only period when the cow is susceptible to pregnancy, to timely inseminate cows and therefore increase farm efficiency.</p><p>Traditionally, estrus detection relies on visual observation of animal behaviors. Activity usually increases markedly in cows during estrus <ref type="bibr" target="#b14">[15]</ref> unless the cow is experiencing a silent estrus (estrus without obvious behavioral signs -35% of total estrus). In practice, less than 50% of estruses are detected visually <ref type="bibr" target="#b25">[25]</ref> due to two main reasons: first silent estrus cannot be detected visually and second, sexual behaviors are mostly expressed at night. Different methods have been developed to aid visual detection. The reference method is estrus estimation using automated progesterone analysis in milk <ref type="bibr" target="#b8">[9]</ref>. However, the cost of this solution prohibits its extensive implementation.</p><p>As a result, affordable activity and body temperature sensor data are considered having potential for automatic estrus detection <ref type="bibr" target="#b27">[27]</ref>. Some solutions based on activity data are available. However, their adoption rate remain moderate <ref type="bibr" target="#b30">[30]</ref>. These commercial detection solutions face two major shortcomings. First, solutions based on activity cover only behavioral estruses (estruses associated with obvious behavioral signs -65% of total estrus). Second, false alerts and lack of explanation behind detections generate solutions mistrust from farmers. Therefore, aside from an enhanced performance, key justifications for estrus alerts are also needed to expand automatic detection solution adoption.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Our Contributions</head><p>Our research tackles the challenge of milk production resource use efficiency in dairy farms with machine learning methods. We aim to enhance estrus detection, especially on the currently undetected silent estrus, and allow farmers to rely on automatic estrus detection solutions based on affordable data (e.g. activity, temperature). With our real world data analysis and an exhaustive estrus labeling (behavioral, silent) approach, this study will: </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Multivariate time series (MTS) collected from activity and temperature sensors are labeled as either estrus or anestrus, the period of sexual inactivity between two periods of estrus. Estrus detection can be formulated as a binary classification problem. In this section, we first discuss the classifiers suited to our study. Then, we examine literature on classifier interpretability. Finally, we present existing work on estrus detection through machine learning methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Classification</head><p>Among state-of-the-art binary classifiers for numerical data and in the context of our problem, we can exclude the use of classifiers dedicated to MTS. MTS classifiers do not fit our needs for different reasons. First, current literature in animal science does not provide information to make assumption about a particular physiological model. Second, intervals <ref type="bibr" target="#b4">[5]</ref> and shapelets <ref type="bibr" target="#b16">[17]</ref> classification algorithms are excluded due to the short time windows we consider. Data from sensors are 24hr aggregated (the relevant period for estrus evaluation); and according to animal scientists, data on day of estrus and the day before estrus could be sufficient for estrus detection (time window size of two). Finally, dictionary representation approaches do not allow us to exploit temporal interactions between variables due to the aggregated representation of series on time <ref type="bibr" target="#b3">[4]</ref>. Our dataset has the same frequency among variables, we manage the time aspect by setting the different timestamps as column variables. Accordingly, we explore state-of-the-art classifiers in the following classes: k-nearest neighbors, regularized logistic regressions, support vector machines, neural networks and ensemble methods.</p><p>Firstly, we consider elastic net <ref type="bibr" target="#b31">[31]</ref>, the logistic regression combining L1 and L2 regularization methods, which constitutes the reference in regularized logistic regression.</p><p>Then, given the lower number of features than the number of samples in our dataset, we test a support vector machine with a radial basis function kernel.</p><p>Among categories of neural networks (multilayer perceptron -MLP, convolutional neural network -CNN and recurrent neural network -RNN), we consider small MLPs. Deep MLPs, without convolutional layers, are difficult to train due to the large number of parameters and the vanishing gradient problem <ref type="bibr" target="#b23">[23]</ref>. Moreover, our dataset size (18,000 samples) and the inexistence of a pretrained CNN network on a comparable problem do not allow us to use CNNs. Then, RNNs are not suited to the short time windows we consider.</p><p>Lastly, the explicit (bagging and boosting) and implicit (negative correlation learning and mixture of experts) approaches exhibit respective strengths and limitations therefore a hybrid ensemble method is encouraged <ref type="bibr" target="#b20">[20]</ref>. The strengths and limitations of explicit and implicit approaches concern their ability to generalize beyond the training dataset. Generalization performance depends on the balance found between an algorithm which is not capturing the underlying structure of the training dataset (underfitting -high bias) and an algorithm which is learning too closely the training dataset (overfitting -high variance). This challenge is called the bias-variance tradeoff. Negative Correlation Learning (NCL) attempts to train individual classifiers in an ensemble and combines them in the same learning process. On the entire training set, individual classifiers are trained simultaneously and interactively through the correlation penalty terms of their error functions to adjust the bias-variance tradeoff. The disadvantage is that all individual classifiers are concerned with the whole ensemble error. Mixture of Experts (ME) is an ensemble method based on the divide and conquer principle in which the problem space is divided between few experts (e.g. classifiers), supervised by a dynamic weighted average scheme (gating network). It allows each expert to learn a part of the training data with its corresponding individual error. However, there is no control over the bias-variance tradeoff. Combinations of NCL and ME implicit approaches exist <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b11">12]</ref>. These methods integrate an error function correlation penalty term to encourage different classifiers (NCL), through a divide and conquer approach (ME), to learn using different parts of the training data. However, implicit approaches combinations do not benefit from the improved generalization ability of explicitly creating different training sets by probabilistically changing the distribution of the original training data (bagging, boosting). A method combining the explicit boosting approach with implicit ME divide and conquer approach exists <ref type="bibr" target="#b12">[13]</ref>. Nonetheless, the low bias distribution change of boosting does not ensure a bias-variance tradeoff.</p><p>Therefore, given the lower performance of small MLPs compared to ensemble methods in average (confirmed by our experiments), we propose a new hybrid ensemble method. It combines an explicit bagging-boosting approach to handle the bias-variance tradeoff and an implicit ME divide and conquer approach to learn different parts of the training data.</p><p>As previously mentioned, we cannot separate classifiers detection performance from interpretability. This will be explored in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Interpretability</head><p>There is no mathematical definition of interpretability. A definition proposed by <ref type="bibr" target="#b21">[21]</ref> states that the higher the interpretability of a machine learning algorithm, the easier it is for someone to comprehend why certain decisions or predictions have been made.</p><p>Our problem requires insights into the type of estrus (behavioral versus silent), which suggests local explanations. Morevover, we need a method able to work for the different classifiers identified (model-agnostic). State-of-the-art methods meeting these requirements (local, model-agnostic) are Local Interpretable Model-agnostic Explanations (LIME) <ref type="bibr" target="#b26">[26]</ref> and SHapley Additive exPlanations (SHAP) <ref type="bibr" target="#b18">[19]</ref>. SHAP values come with the black box local estimation advantages of LIME, but also with theoretical guarantees. Therefore, we use SHAP in order to interpret the output of our machine learning algorithm. This technique is inspired by game theory, which is used to determine how much each player in a collaborative game has contributed to its success. In our study, SHAP values measure how much an activity or temperature variable impacts estrus predictions. A higher absolute SHAP value of a variable compared to other variables means that this variable has a higher predictive or discriminative power in detection algorithm. SHAP values are calculated by the average marginal contribution of a feature value towards the prediction over all possible coalitions. SHAP interaction values, an extension of SHAP values based on Shapley interaction index <ref type="bibr" target="#b13">[14]</ref>, capture pairwise interaction effects. In addition, SHAP values are available at local level. We analyze it to compare the impact of variables on algorithm predictions in behavioral and silent estrus.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Automatic Estrus Detection</head><p>There are a couple of studies about the application of machine learning methods on estrus detection <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b22">22]</ref>. None of them uses the currently recognized method for behavioral and silent estrus identification as labels (progesterone profiles), so their estrus labeling methods are not exhaustive. Moreover, two studies use different variables (milk volume, milking order, days since last estrus) rather than the affordable activity or temperature measurements. Finally, none of them gives insights on algorithm predictions based on its interpretability. <ref type="bibr" target="#b22">[22]</ref> bases the study on time series data of milk volume and milking order, using visual detection as the ground truth. Two learning schemes were tested -FOIL and C4.5. Algorithms detected 69% of estruses identified by visual method and a large number of false positives occurred (74%).</p><p>[18] learns a MLP on time series data of activity and the number of days since last estrus, using successful insemination as the ground truth. The model showed a sensitivity, a specificity and an error rate of 77.5, 99.6 and 9.1% on 373 estrus.</p><p>And lastly, <ref type="bibr" target="#b10">[11]</ref> bases the study on time series data of activity, using visual detection as the ground truth (65.6% of all estruses). Three machine learning techniques were tested -random forest, linear discriminant and MLP. Algorithms showed 91%-100% accuracy on a limited dataset of 18 cows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">LCE: LOCAL CASCADE ENSEMBLE</head><p>As mentioned previously, we propose a new hybrid ensemble method which combines an explicit bagging-boosting approach to handle the bias-variance tradeoff and an implicit ME divide and conquer approach to individualize classifier error on different parts of the training data. We have decided to start from an existing combined implicit (NCL and ME) stacking-based approach (cascade generalization <ref type="bibr" target="#b29">[29]</ref>): local cascade <ref type="bibr" target="#b15">[16]</ref>. The bagging/boosting potential of local cascade decision tree divide and conquer method motivates our choice. In this section, we first introduce local cascade, the initial implicit stacking-based approach. Next, we explain LCE, our augmented (explicit and implicit) version of local cascade, and then compare LCE performance to local cascade. Figure <ref type="figure">1</ref> illustrates the presentation of local cascade and LCE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Local Cascade</head><p>First of all, cascade generalization uses a set of classifiers sequentially and at each step adds new attributes to the original dataset <ref type="bibr" target="#b29">[29]</ref>. The new attributes are derived from the class probabilities given by a base classifier (e.g. 𝐻0(𝐷), 𝐻1(𝐷01) in Figure <ref type="figure">1</ref>). The bias-variance tradeoff is obtained by negative correlation learning: at each stage of the sequence, classifiers with different behaviors are selected. It is recommended in cascade generalization to begin with a low variance algorithm to draw stable decision surfaces (𝐻0 in Figure <ref type="figure">1</ref>) and then use a low bias algorithm to fit more complex ones (𝐻1 in Figure <ref type="figure">1</ref>). Local cascade <ref type="bibr" target="#b15">[16]</ref> applies cascade generalization locally following a divide and conquer strategy based on mixture of experts principle. The objective of this approach is to capture new relations that cannot be discovered globally. The local cascade divide and conquer method is a decision tree. When growing the tree, new attributes (class probabilities from a classifier -base classifier) are computed at each decision node and propagated down the tree. In order to be applied as a predictor, local cascade stores, in each node, the model generated by the base classifier. </p><formula xml:id="formula_0">H b D 0 =D 1 +H b (D 1 ) H b H b H b H b H b H b D 11 =D 01 + H b (D 01 ) D 12 =D 02 + H b (D 02 ) H b (D 111 ) H b (D 112 ) H b (D 121 ) H b (D 122 ) D 01 D 02 BOOSTING H b D 0 =D n +H b (D n ) H b H b H b H b H b H b D 11 =D 01 + H b (D 01 ) D 12 =D 02 + H b (D 02 ) H b (D 111 ) H b (D 112 ) H b (D 121 ) H b (D 122 ) D 01 D 02 BOOSTING Figure 1: Local Cascade versus LCE</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">LCE: Local Cascade Ensemble</head><p>Our contribution intervenes in our explicit manner of handling the bias-variance tradeoff whereas local cascade approach is implicit, alternating between base classifiers behaviors (bias, variance) at each level of the tree. LCE reduces bias across decision tree through the use of boosting as base classifier (𝐻 𝑏 in Figure <ref type="figure">1</ref>). Boosting base classifier iterative data distribution change (reweighting) decreases the bias at each tree level. In addition, boosting is propagated down the tree by adding class probabilities of the base classifier to the training dataset (new attributes). Class probabilities contain information about the ability of the base classifier to correctly classify a sample. At the next tree level, class probabilities added to the dataset are exploited by the base classifier as a weighting scheme to focus more on previously misclassified samples.</p><p>Then, the overfit generated by the decision tree divide and conquer bias reduction approach is mitigated by the use of bagging. Bagging provides variance reduction by creating multiple decision trees from different subsamples of the original dataset (random sampling with replacement, see 𝐷 1 . . . 𝐷 𝑛 in Figure <ref type="figure">1</ref>). Trees are aggregated with a simple majority vote.</p><p>LCE new hybrid ensemble method enables to balance the bias-variance tradeoff without the need for an interactive learning between individual classifiers (NCL), while benefiting from the improved generalization ability of explicitly creating different training sets (bagging, boosting). Furthermore, LCE divide and conquer method ensures that classifiers learn on different parts of training data without the need for a supervision scheme (gating network).</p><p>We present LCE pseudocode in Algorithm 1. A function (LCE Tree) builds a tree and the second one (LCE) the forest of trees through bagging.</p><p>There are 2 stopping criteria during a tree building phase: when a node has an unique class or when the tree reaches the maximum depth. We set the range of tree depth from 0 to 3 in for 𝐷 ′(𝑗) ∈ 𝒫(𝐷 ′ ) do 16:</p><p>𝑇 𝑟𝑒𝑒𝑗 = LCE Tree(𝐷 ′(𝑗) , 𝐻, 𝑚𝑎𝑥 𝑑𝑒𝑝𝑡ℎ, 𝑑𝑒𝑝𝑡ℎ)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>17:</head><p>return tree containing a decision node, storing classifier 𝐻 𝑑𝑒𝑝𝑡ℎ (𝐷) and descendant subtrees 𝑇 𝑟𝑒𝑒𝑗 LCE instead of 0 to 5 in local cascade. This hyperparameter is used to control overfitting. Our choice of low bias boosting base classifiers justifies the maximum depth adjustment to 3. In this study, the set of low bias base classifiers is limited to the state-of-the-art boosting algorithm (extreme gradient boosting -XGB <ref type="bibr" target="#b7">[8]</ref>).</p><p>In addition, we removed two rules implemented in local cascade to reduce variance: the maximum base classifier error rate and the minimum class representation in a node. The first rule requires the stopping of propagation down the tree to prevent overfitting if the base classifier, in a node, had an error rate below a certain threshold (0.5). Our approach suggests a variance reduction through bagging, and not during a tree construction; so we did not keep this rule. In order to restrict the attention to well populated classes, the second rule requires considering a class in a node if the number of examples belonging to this class is greater than 𝑁 times (3) the number of attributes. We did not keep the second rule for the same reason.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Performance Comparison: Local</head><p>Cascade versus LCE As expected, results show a higher variability across folds of LCE compared to the local cascade when the number of tree is set to 1 due to its low bias orientation (standard error of 1.6% versus 1.4% on F1 score, performance calculation detailed in section Experiments). However, LCE on 1 tree exhibits a higher detection performance than local cascade (F1 score: 68.1% versus 53.2%).</p><p>Additionally, through bagging, we observe LCE variability reduction to a lower level than local cascade as well as an increase of detection performance (F1 score 95% confidence interval: 68.1 ± 3.2 with 1 tree versus 68.9 ± 2.4 with 70 trees versus 53.2 ± 2.8 with local cascade).</p><p>Therefore, this comparison affirms the superiority of our explicit bias-variance tradeoff approach compared to the implicit NCL approach of local cascade on our dataset. The intrinsic different behavior of LCE and local cascade is confirmed in the results and discussions section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head><p>In this section, we present the composition of our real-world dataset, the preprocessing performed and the experimental setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Dataset</head><p>Our dataset is offline. From 2014 to 2017, an experiment was conducted at the INRA Méjusseaume dairy farm (4806' N, 147' W, Brittany, France). This experiment enrolled 125 Holstein cows housed in free stalls representing 153 lactations.</p><p>Each cow was equipped with a collar-mounted activity meter (HeatPhone -Medria Technologies, Châteaubourg, France) and a temperature sensor in first stomach (Thermobolus -Medria Technologies, Châteaubourg, France). Based on its good performance compared to other solutions <ref type="bibr" target="#b6">[7]</ref> and its international market presence, we hold that Medria estrus detection system is a reasonable basis of comparison. In the following sections, Medria is called the commercial solution (CS). The dataset consists of visual estrus alerts, Medria estrus alerts and Medria numeric variables with a 5-minute frequency (rumination, ingestion, rest, standing up, overactivity, other activity, temperature, and temperature corrected ). Temperature corrected takes into account the cooling effect of water ingestion by the cows. Concerning the visual estrus alerts, visual observation was conducted by farm staffs. Staff also checked the commercial solution alerts before inputting their visual records, thus these visual estrus alerts are shown as Visual&amp;CS in the study. The preprocessing applied on the data collected is a 24hr aggregation (activity: sum, temperature: mean) which corresponds to the relevant window for both estrus detection and, from an alert standpoint, farmers' needs. We assume that the treatment operated by Medria on raw data to generate variables is stable during our experiment.</p><p>Our novel approach addresses both estrus categories detection (behavioral and silent). Therefore, we labeled estrus by measuring the progesterone concentration in whole milk, the current reference for an exhaustive estrus identification. This time-effective and non-invasive method for the cow induces commonly accepted errors (progesterone measurements, profiles analysis <ref type="bibr" target="#b1">[2]</ref>). We mark an estrus as behavioral estrus when either a visual detection or a Medria alert occurred. An estrus is considered silent when neither visual detection or a Medria alert occurred. Our dataset is composed of 671 estruses with 37% of silent estrus which is aligned with the rate of 35% observed in literature <ref type="bibr" target="#b24">[24]</ref>.</p><p>Days preceding estrus are a valuable source of information for estrus detection, we set it as a hyperparameter. Every value in the range from 1 to 21 days, the length of a regular ovarian cycle, are tested. Past days of variables are added as feature columns.</p><p>4.1.1 Feature Selection. We perform feature selection in this study because of the sensitivity of the method chosen to interpret the detection algorithm (SHAP) to high correlations among features. We conduct a subset selection on pairs of collinear features based on the Pearson correlation coefficient (threshold 0.8). One pair of features is above the threshold (0.9: temperature corrected, temperature). Since temperature is affected by the cooling effet of water ingestion, the variable temperature corrected is selected. From this point onwards, temperature corrected is named temperature. After this feature selection, no Pearson pairwise correlation in the case of the 21 past days dataset is above the threshold.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Dataset</head><p>Structure. We make a 5-fold cross validation. Dataset split is presented in Table <ref type="table" target="#tab_3">2</ref>. The split has kept the same number of days in estrus in each fold <ref type="bibr">(1,144 days)</ref>. We made this choice to avoid overfitting on a particular animal. We discuss the impact of a split keeping the same number of animals per fold in the detection performance section. Moreover, we do not observe any structural imbalance on silent estrus percentage across the folds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Experimental Setting</head><p>We present in this section algorithms and methods used in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Algorithms.</head><p>We tested our hybrid ensemble LCE (explicit -implicit approaches) versus the initial implicit approach (local cascade) and the state-of-the-art algorithm for each explicit approach (bagging: random forest, boosting: extreme gradient boosting). K-nearest neighbors, elastic net, support vector machines and small MLPs are also tested.</p><p>• k-nearest neighbors -KNN: we use the implementation neighbors.KNeighborsClassifier in the scikit-learn package for Python   <ref type="bibr" target="#b5">[6]</ref>. Hyperopt chooses the next hyperparameters decision from the previous choices and a tree-based optimization algorithm. Tree of Parzen estimators meet or exceed grid search and random search performance for hyperparameters setting.</p><p>We use the implementation available in the Python package hyperopt <ref type="foot" target="#foot_3">4</ref> and hyperas wrapper for keras. Optimization is undertaken to maximize F1 score. The choice of this metric is driven by 2 reasons. First, we do not make assumption about the dairy management style; farmers can favor a higher estrus detection rate (higher recall) or fewer false alerts (higher precision) according to their needs. Second, we face a class imbalance (33% of estrus days) which renders irrelevant the accuracy metric.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3">Classification Performance.</head><p>Our experiments use progesterone profiles as ground truth for exhaustive estrus identification. The levels of progesterone allow us to identify a time window of 3 days for estrus with a duration of less than 24 hours, in the standard scheme. Adopting a conservative approach, we decided to aggregate by the maximum of our daily predictions on estrus/anestrus period to calculate the classification performance. In addition, we observe that for high thresholds (threshold &gt; 0.95), classifiers performances are unstable with a significant decrease in estrus detection rate (recall below 70%). In addition, for low thresholds (threshold &lt; 0.1), classifiers are equivalent to a random classifier. So, we decided to adopt a F1 score calculation based on the average of F1 score on threshold range 0.1-0.95. This calculation does not modify the classifier selection results or the comparison result with the commercial solution. Nonetheless, it corresponds to the plausible range of calibration for dairy management and shows a detection performance closer to real conditions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.4">Algorithm Selection.</head><p>Based on a 5-fold cross-validation 60/20/20 train/validation/test split, the best classifier is selected based on the highest F1 score on validation sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.5">Statistical Test.</head><p>As recommended by <ref type="bibr" target="#b9">[10]</ref>, we have used a 5 × 2 cross validation t-test for statistical significance of machine learning algorithms on one dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.6">Interpretability.</head><p>As mentioned in the related work section, we use the SHAP implementation available in the Python package shap<ref type="foot" target="#foot_4">5</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">RESULTS AND DISCUSSIONS</head><p>This section is structured into two parts: performance and interpretability. The detection performance part compares LCE to other detection methods (classifiers, commercial solution) and evaluates the relevance of deploying 2 sensors. Then, we identify the key drivers (variables impact, temporal interactions) behind the estrus detection alerts at global and local level (behavioral versus silent) based on algorithm interpretability (SHAP) and propose an approach to reduce the solution mistrust.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Detection Performance</head><p>Classification results on test sets are presented in Figure <ref type="figure" target="#fig_1">2</ref>. The best classifier on validation sets is LCE with the following hyperparameters: 3 past days, depth equals to 1 and 70 trees. We do not observe an overfit of LCE, the performance observed on test sets (F1 score: 68.9) is stable compared to the one of the validation sets (F1 score: 68.1).</p><p>Furthermore, the performance of LCE responds to the objective of an increase in performance in both estrus detection rate and fewer false alerts compared to the commercial solution (CS). At the same precision, LCE recall is constantly higher than commercial solution recall. At a precision of 78%, the precision rate of the commercial solution in this study, our algorithm detects 22% more estrus.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.1">Comparative Analysis.</head><p>We compare the error rate correlation of LCE to those of other detection methods. This comparison allows us to:</p><p>• gain insights into the shortcomings of the commercial solution and LCE detections • identify limitations of our approach for deployment A low correlation indicates that classifiers err in different regions of the instance space. Table <ref type="table" target="#tab_6">3</ref> presents Pearson correlations of LCE prediction errors with other detection methods (classifiers and commercial solution) on test sets. In order to be comparable, we have set the threshold of each classifier with the same precision as the commercial solution (78%). First, the commercial solution shows an intrinsic different behavior from that of LCE (correlation: 0.37). This low correlation is mainly explained by the null performance of the commercial solution on silent estrus detection across the herd. On 67% of the cows, composed of a slighlty higher proportion of silent estrus compared to average (40% versus 37%), predictions correlation of the commercial solution with LCE is 0.21 ± 0.03.</p><p>Next, the low correlation between LCE and local cascade (0.41) confirms the value added by the explicit bias-variance tradeoff of the LCE approach. This low correlation is explained by the low recall (11%) of the local cascade for a precision of 78%. The stable decision surface drawn by naive bayes at the root of the local cascade decision tree substantially limits the range of performance of the algorithm on our dataset (recall drops with a precision higher than 66%). We observe this performance drop for precision above 66% in Figure <ref type="figure" target="#fig_1">2</ref>.</p><p>Finally, the classifier with the closest behavior to LCE is XGB (0.8). However, the correlation difference remains substantial and is explained by some divergence among few cows. The divergence, an error rate correlation below 0.6, concerns 12% of the cows comprising a proportion of silent estrus aligned with average (35%). Therefore, our bias-variance approach enhances XGB performance on standard cases (cows with 35% of silent estrus). Nevertheless, we observe a poor performance of LCE on 11% of the cows exhibiting a high proportion of silent estrus (F1 score &lt; 55%, silent estrus proportion: 54%). Silent estrus are not equally distributed among cows. In our dataset, 16% of the cows represent 40% of the silent estrus. LCE performance per cow is exposed to the animal estrus type proportion. It is confirmed by the LCE performance drop when assessed on the activity and temperature dataset generated by a stratified 5-fold on animals (66.3 ± 3.4). LCE performance per cow variability according to the animal estrus type proportion is a limitation of our solution for deployment; meanwhile it is also a driver for detection improvement. We suggest further investigation to incorporate additional animal individual features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.2">One or Two Sensors?</head><p>In order to answer this question, we compare the detection performance on test sets of LCE on the temperature, the activity and both variables. We also compare LCE detection results to the commercial solution and visual method.</p><p>First, the results confirm the potential of data science techniques for automatic estrus detection versus visual detection as concluded by <ref type="bibr" target="#b10">[11]</ref>. We observe that LCE for both behavioral and silent estrus detection, trained on activity and temperature data, manifests significantly better performance (F1 score and lower variability) than Visual&amp;CS (68.9 ± 2.4 versus 60.4 ± 4.6, 𝑃 &lt; 0.05). Our Visual&amp;CS performance is aligned with the state-of-the-art <ref type="bibr" target="#b25">[25]</ref>; the detection rate is slightly below 50% (47%).</p><p>Second, we observe a better performance (higher F1 score and lower variability) with our algorithm trained on activity and temperature than activity or temperature alone (68.9 ± 2.4 versus 67.0 ± 3.0 versus 55.9 ± 2.3). The performance difference is only significant when compared to the algorithm trained using the temperature. We infer that, in the conditions of our experiment, only activity sensor should be deployed: the performance is not significantly lower than that trained with two sensors (activity and temperature).</p><p>Nonetheless, temperature information cannot be excluded. We observe a markedly lower variability of the algorithm based on temperature across folds which allows the algorithm based on activity and temperature to reduce its variability. It means that the algorithm based on temperature is consistent on different data. It implies a possible higher discriminative and generalizing power. We propose to further study the potential of temperature data for estrus detection with a broader data heterogeneity (cows breed, environment). The next step would consist of a partnership with an automatic detection solution provider to have access to a more diverse dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Interpretability of our Solution</head><p>In this section, we firstly present the relative impact of variables in LCE predictions and their temporal interactions. Then, we propose an approach to give insights on estrus detection to the farmers based on these elements.</p><p>Figure <ref type="figure" target="#fig_2">3</ref> shows the average impact of each variable on algorithm predictions for estrus and anestrus by decreasing order.</p><p>These results confirm the discriminative power of the temperature and its potential for improving estrus detection performance. The variable with the strongest impact to algorithm predictions is the temperature on the day of estrus for both estrus and anestrus classes.</p><p>Next, we observe that the ranking of all activity variables are different with a significant rank change between estrus and anestrus. Therefore, the relative impact of each activity variable in LCE predictions differs between estrus and anestrus. Overactivity on the day of estrus, a typical characteristic of most estrus (65%), appears as the third most impactful variable after temperature estrus and does not appear on the top 20 of variables for anestrus.</p><p>By taking the same impact ranking approach locally for behavioral versus silent estrus, we also observe a significant change on the ranking of activity variables (75% of rank change). Rumination 2 days before estrus is a key variable in silent estrus detection. It is the third most impactful activity variable for silent estrus and appears at the 19th position for behavioral estrus.</p><p>Finally, temporal relations among variables differ between behavioral and silent estrus. SHAP interaction values reveal that algorithm predictions are more impacted by activity variables further to the day of estrus for silent estrus than behavioral estrus. For example, the variable of highest interaction with rumination on the day of estrus is the rest 3 days before estrus for silent estrus versus the rest 2 days before estrus for behavioral estrus. This observation holds true for over activity, standing up and ingestion (two third of activity variables).</p><p>Therefore, in order to support LCE estrus alerts and ease solution adoption, we propose an approach based on LCE interpretability (activity sensor only). First, communicate to the farmer the relatedness of the estrus detection to historical cases through a confidence indicator and the amplitude of differences in the 3 most impactful activity variables (rest 3 days before estrus, over activity 2 days before estrus and over activity on the day of estrus). The confidence indicator corresponds to the weighted average of absolute SHAP values differences by the ranking of impact variables for estrus from our reference presented above. Second, in case of estrus, inform the farmer about the type of estrus (behavioral/silent) with a confidence level and which temporal interactions are satisfied. The information about the type of estrus aims to reassure farmers when they are not able to verify the estrus alert by visual behavioral signs, therefore reduce potential mistrust. Confidence level is calculated like the previous one but using ranking of variables impact of silent estrus as a reference. In addition, temporal interactions are communicated in decreasing order of variable impact.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>Our study confirms the significant performance improvement of LCE on estrus detection compared to commercial solutions, a result driven by silent estrus detection. It also proves the pivotal role of activity sensors deployment in these detections. The interpretability of LCE offered by SHAP, disclosing information about the relatedness of the predictions to historical cases and the possibility of visually verifying the estrus (behavioral versus silent), promises mistrust reduction from farmers. Concerning the deployment of our solution, the homogeneity (cows breed, environment) of our dataset is a limitation. The next step would consist of a partnership with an automatic detection solution provider to have access to a heterogeneous dataset.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Algorithm 1 3 : 8 : 12 :</head><label>13812</label><figDesc>LCE: Local Cascade EnsembleRequire: A dataset 𝐷, a set of classifiers 𝐻, maximum depth of a tree 𝑚𝑎𝑥 𝑑𝑒𝑝𝑡ℎ, number of trees 𝑛 𝑡𝑟𝑒𝑒𝑠 1: function LCE(𝐷, 𝐻, 𝑛 𝑡𝑟𝑒𝑒𝑠, 𝑚𝑎𝑥 𝑑𝑒𝑝𝑡ℎ)for each 𝑖 in [1, 𝑛 𝑡𝑟𝑒𝑒𝑠]do4: 𝑆 ← A bootstrap sample from 𝐷 5: 𝑡 ← LCE Tree(𝑆, 𝐻, 𝑚𝑎𝑥 𝑑𝑒𝑝𝑡ℎ, 0) function LCE Tree(𝐷, 𝐻, 𝑚𝑎𝑥 𝑑𝑒𝑝𝑡ℎ, 𝑑𝑒𝑝𝑡ℎ) 𝐷 ′ ← Concatenate(𝐷, 𝐻 𝑑𝑒𝑝𝑡ℎ (𝐷)) 13: Split 𝐷 ′ on attribute maximizing Gini criterion 14: 𝑑𝑒𝑝𝑡ℎ ← 𝑑𝑒𝑝𝑡ℎ + 1 15:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Precision recall curves on test sets of the classifiers versus the commercial solution</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Average impact of the attributes on algorithm predictions for estrus and anestrus. Abbreviations: DOE -Day Of Estrus; DBE -Day Before Estrus</figDesc><graphic coords="9,53.80,80.12,254.03,192.09" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Our comparison aims to underline the superior performance of LCE compared to a local cascade on our real-world dataset, induced by their different approaches of handling bias-variance tradeoff (explicit versus implicit approach). LCE is implemented according to the description given in the previous section. Local cascade implementation corresponds to the description of the original paper and as recommended, we use naive bayes for low variance base classifier. In order to be comparable, the low bias base classifier is XGB. Depth is set to 1 for LCE and the local cascade. Results are presented in Table1.</figDesc><table><row><cell cols="8">Table 1: F1 score with 95% confidence interval of</cell></row><row><cell cols="7">LCE versus local cascade (LC) on our dataset</cell><cell></cell></row><row><cell>Trees</cell><cell>1</cell><cell>5</cell><cell>10</cell><cell>30</cell><cell>50</cell><cell>70</cell><cell>90</cell></row><row><cell>LCE</cell><cell cols="7">68.1 69.2 68.9 69.1 69.1 68.9 68.9</cell></row><row><cell></cell><cell cols="7">±3.2 ±2.6 ±2.8 ±2.4 ±2.5 ±2.4 ±2.5</cell></row><row><cell>LC</cell><cell></cell><cell></cell><cell></cell><cell>53.2 ± 2.8</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Dataset Split</figDesc><table><row><cell></cell><cell cols="5">Fold1 Fold2 Fold3 Fold4 Fold5</cell><cell>All</cell></row><row><cell>Estrus</cell><cell>126</cell><cell>136</cell><cell>118</cell><cell>141</cell><cell>153</cell><cell>671</cell></row><row><cell>Silent %</cell><cell>33</cell><cell>40</cell><cell>24</cell><cell>40</cell><cell>46</cell><cell>37</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table><row><cell></cell><cell cols="6">Pearson pairwise correlations of LCE pre-</cell></row><row><cell cols="7">diction errors with other detection methods on test</cell></row><row><cell>sets</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="7">KNN EN SVM MLP RF XGB LC CS</cell></row><row><cell>0.61</cell><cell>0.19</cell><cell>0.57</cell><cell>0.69</cell><cell>0.73</cell><cell>0.8</cell><cell>0.41 0.37</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>https://scikit-learn.org/stable/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>https://xgboost.readthedocs.io/en/latest/python/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>https://keras.io/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"><p>https://github.com/hyperopt/hyperopt</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4"><p>https://github.com/slundberg/shap</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>ACKNOWLEDGMENTS</head><p>We thank all technical staff of <rs type="institution">INRA Méjusseaume dairy</rs> farm who helped managing and monitoring this long-term experimentation. We also thank <rs type="institution">Medria</rs> for its collaboration by providing activity and temperature sensor data. This work was supported by the <rs type="funder">French National Research Agency</rs> under the <rs type="programName">Investments for the Future Program</rs> (<rs type="grantNumber">ANR-16-CONV-0004</rs>), <rs type="funder">French national</rs> project <rs type="projectName">Deffilait</rs> (<rs type="grantNumber">ANR-15-CE20-0014</rs>) and <rs type="funder">APIS-GENE</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_KpfXkmT">
					<idno type="grant-number">ANR-16-CONV-0004</idno>
					<orgName type="program" subtype="full">Investments for the Future Program</orgName>
				</org>
				<org type="funded-project" xml:id="_hdMyR3W">
					<idno type="grant-number">ANR-15-CE20-0014</idno>
					<orgName type="project" subtype="full">Deffilait</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Root-Quatric Mixture of Experts for Complex Classification Problems</title>
		<author>
			<persName><forename type="first">E</forename><surname>Abbasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shiri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ghatee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Expert Systems with Applications</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A Novel System for On-Farm Fertility Monitoring Based on Milk Progesterone</title>
		<author>
			<persName><forename type="first">I</forename><surname>Adriaens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Saeys</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Huybrechts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lamberigts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Franois</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Geerinckx</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leroy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">De</forename><surname>Ketelaere</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Aernouts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Dairy Science</title>
		<imprint>
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A Summary of the Reasons Why Farmers Cull Cows</title>
		<author>
			<persName><forename type="first">S</forename><surname>Bascom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Young</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of dairy science</title>
		<imprint>
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="page">8</biblScope>
			<date type="published" when="1998">1998. 1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning a Symbolic Representation for Multivariate Time Series Classification</title>
		<author>
			<persName><forename type="first">M</forename><surname>Baydogan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Runger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Data Mining and Knowledge Discovery</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page">2</biblScope>
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A Bag-of-Features Framework to Classify Time Series</title>
		<author>
			<persName><forename type="first">M</forename><surname>Baydogan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Runger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Tuv</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2013">2013. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Algorithms for Hyper-Parameter Optimization</title>
		<author>
			<persName><forename type="first">J</forename><surname>Bergstra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bardenet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kégl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 24</title>
		<editor>
			<persName><forename type="first">J</forename><surname>Shawe-Taylor</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">L</forename><surname>Bartlett</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>Pereira</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Comparison of Three Devices for the Automated Detection of Estrus in Dairy Cows</title>
		<author>
			<persName><forename type="first">A</forename><surname>Chanvallon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Coyral-Castel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gatien</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lamy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ribaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Allain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Clément</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Salvetti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Theriogenology</title>
		<imprint>
			<biblScope unit="volume">82</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">XGBoost: A Scalable Tree Boosting System</title>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Guestrin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22Nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 22Nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Dairy Cows&apos; Reproductive Response to Feeding Level Differs According to the Reproductive Stage and the Breed</title>
		<author>
			<persName><forename type="first">E</forename><surname>Cutullic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Delaby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gallard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Disenhaus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Animal</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2011">2011. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Approximate Statistical Tests for Comparing Supervised Classification Learning Algorithms</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">G</forename><surname>Dietterich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="1998">1998. 1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Behavioral and Physiological Changes Around Estrus Events Identified Using Multiple Automated Monitoring Technologies</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">A</forename><surname>Dolecheck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">J</forename><surname>Silvia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Heersche</surname><genName>Jr</genName></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">M</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">E</forename><surname>Stone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">A</forename><surname>Wadsworth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Bewley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Dairy Science</title>
		<imprint>
			<biblScope unit="volume">98</biblScope>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Improving Combination Method of NCL Experts Using Gating Network</title>
		<author>
			<persName><forename type="first">R</forename><surname>Ebrahimpour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Arani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Masoudnia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computing and Applications</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2013">2013. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Boost-Wise Pre-Loaded Mixture of Experts for Classification Tasks</title>
		<author>
			<persName><forename type="first">R</forename><surname>Ebrahimpour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Sadeghnejad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Arani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Mohammadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computing and Applications</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2013">2013. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Axiomatic Characterizations of Probabilistic and Cardinal-Probabilistic Interaction Indices</title>
		<author>
			<persName><forename type="first">K</forename><surname>Fujimoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Kojadinovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Games and Economic Behavior</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2006">2006. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Milk Yield and Estrous Behavior During Eight Consecutive Estruses in Holstein Cows Fed Standardized or High Energy Diets and Grouped According to Live Weight Changes in Early Lactation</title>
		<author>
			<persName><forename type="first">C</forename><surname>Gaillard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Barbu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">T</forename><surname>Sørensen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sehested</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Callesen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Vestergaard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Dairy Science</title>
		<imprint>
			<biblScope unit="volume">99</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Cascade Generalization</title>
		<author>
			<persName><forename type="first">J</forename><surname>Gama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Brazdil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2000">2000. 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Generalized Random Shapelet Forests</title>
		<author>
			<persName><forename type="first">I</forename><surname>Karlsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Papapetrou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Boström</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Data Mining and Knowledge Discovery</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Oestrus Detection in Dairy Cows Using Control Charts and Neural Networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Krieter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 56th Annual Meeting of the European Association for Animal Production</title>
		<meeting>56th Annual Meeting of the European Association for Animal Production</meeting>
		<imprint>
			<publisher>Commission on Cattle Production</publisher>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A Unified Approach to Interpreting Model Predictions</title>
		<author>
			<persName><forename type="first">S</forename><surname>Lundberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page">I</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">U</forename><forename type="middle">V</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Luxburg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName><surname>Fergus</surname></persName>
		</author>
		<editor>, S. Vishwanathan, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Mixture of Experts: A Literature Survey</title>
		<author>
			<persName><forename type="first">S</forename><surname>Masoudnia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ebrahimpour</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence Review</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page">2</biblScope>
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Explanation in Artificial Intelligence: Insights from the Social Sciences</title>
		<author>
			<persName><forename type="first">T</forename><surname>Miller</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
	<note type="report_type">ArXiv</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">An Investigation Into the Use of Machine Learning for Determining Oestrus in Cows</title>
		<author>
			<persName><forename type="first">R</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Sherlock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computers and Electronics in Agriculture</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="1996">1996. 1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Neural Networks and Deep Learning</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Nielsen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
			<publisher>Determination Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Estrus Detection and Estrus Characteristics in Housed and Pastured Holstein-Friesian Cows</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Palmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Olmos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Boyle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Mee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Theriogenology</title>
		<imprint>
			<biblScope unit="volume">74</biblScope>
			<biblScope unit="page">2</biblScope>
			<date type="published" when="2010">2010. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Comparison of Three Estrus Detection Systems During Summer in a Large Commercial Dairy Herd</title>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">A</forename><surname>Peralta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Pearson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Nebel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Animal Reproduction Science</title>
		<imprint>
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2005">2005. 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Explaining the Predictions of Any Classifier</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">T</forename><surname>Ribeiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Guestrin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22Nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 22Nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note>Why Should I Trust You?</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Towards an Automated Detection of Oestrus in Dairy Cattle</title>
		<author>
			<persName><forename type="first">M</forename><surname>Saint-Dizier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chastant-Maillard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Reproduction in Domestic Animals</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2012">2012. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Creating a Sustainable Food Future</title>
		<author>
			<persName><forename type="first">T</forename><surname>Searchinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Waite</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hanson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ranganathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dumas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Matthews</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>World Resources Institute</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Generating Ensembles of Heterogeneous Classifiers Using Stacked Generalization</title>
		<author>
			<persName><forename type="first">M</forename><surname>Sesmero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ledezma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sanchis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Characterization of Dutch Dairy Farms Using Sensor Systems for Cow Management</title>
		<author>
			<persName><forename type="first">W</forename><surname>Steeneveld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hogeveen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Dairy Science</title>
		<imprint>
			<biblScope unit="volume">98</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Regularization and Variable Selection via the Elastic Net</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hastie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society. Series B (Statistical Methodology)</title>
		<imprint>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="page">2</biblScope>
			<date type="published" when="2005">2005. 2005</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
