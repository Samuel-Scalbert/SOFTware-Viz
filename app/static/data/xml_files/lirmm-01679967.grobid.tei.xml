<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="fr">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Data reduction in scientific workflows using provenance monitoring and user steering</title>
				<funder ref="#_82X5bAp">
					<orgName type="full">MCTI/RNP-Brazil</orgName>
				</funder>
				<funder>
					<orgName type="full">FAPERJ</orgName>
				</funder>
				<funder>
					<orgName type="full">CNPq</orgName>
				</funder>
				<funder ref="#_skdNKWS">
					<orgName type="full">Inria</orgName>
				</funder>
				<funder ref="#_ZnkSUjA">
					<orgName type="full">EU</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Renan</forename><surname>Souza</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">COPPE/Federal University of Rio de Janeiro</orgName>
								<address>
									<country key="BR">Brazil</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">IBM Research</orgName>
								<address>
									<country key="BR">Brazil</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Vitor</forename><surname>Silva</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Alvaro</forename><surname>Luiz</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Gayoso</forename><surname>De</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Azeredo</forename><surname>Coutinho</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Patrick</forename><surname>Valduriez</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Inria and LIRMM</orgName>
								<address>
									<settlement>Montpellier</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Marta</forename><surname>Mattoso</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">COPPE/Federal University of Rio de Janeiro</orgName>
								<address>
									<country key="BR">Brazil</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Vítor</forename><surname>Silva</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">COPPE/Federal University of Rio de Janeiro</orgName>
								<address>
									<country key="BR">Brazil</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">G</forename><forename type="middle">A</forename><surname>Coutinho</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">COPPE/Federal University of Rio de Janeiro</orgName>
								<address>
									<country key="BR">Brazil</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Data reduction in scientific workflows using provenance monitoring and user steering</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">370B7F0FB92A5CDFB6B171C70F800D6C</idno>
					<idno type="DOI">10.1016/j.future.2017.11.028</idno>
					<note type="submission">Submitted on 10 Jan 2018</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-04-12T14:45+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Scientific Workflows</term>
					<term>Human in the Loop</term>
					<term>Online Data Reduction</term>
					<term>Provenance Data</term>
					<term>Dynamic Workflows</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>à la diffusion de documents scientifiques de niveau recherche, publiés ou non, émanant des établissements d'enseignement et de recherche français ou étrangers, des laboratoires publics ou privés.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="fr">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Scientific Workflow Management Systems (SWMSs) with parallel capabilities have been designed for executing data-intensive scientific workflows, or scientific workflows for short, in High Performance Computing (HPC) environments. A typical execution may involve thousands of parallel tasks with large input datasets <ref type="bibr">[1]</ref>. When it is iterative, the workflow is repeatedly executed for each element of an input dataset. The more the data to process, the longer the workflow may take, which may be days depending on the problem and HPC environment <ref type="bibr">[2]</ref>. Configuring a scientific workflow with parameters and data is hard. Users<ref type="foot" target="#foot_0">1</ref> typically need to try several input data or parameter combinations in different workflow executions. These trials make the scientific experiment take even longer. The obvious solution to improve performance in HPC is through parallel processing. However, reducing the input data to be processed can be also very effective to reduce workflow execution time <ref type="bibr">[3]</ref>.</p><p>In scientific workflows, the total amount of data is large, but not necessarily the entire input dataset has relevant data for achieving the goal of the workflow execution. This is particularly true when a large parameter space needs to be processed in parameter sweep workflows. There may be slices of the parameter space that have no (or little) influence on the results and thus, as with a "branch and bound" optimization strategy, can be bounded before its evaluation. A similar scenario occurs when the workflow involves a large input dataset. When users can actively participate in the computational process, practice frequently referred to as "human-in-the-loop", they may analyze partial result data and tell which part of the data is relevant for the final result <ref type="bibr" target="#b6">[4]</ref>. Then, based on their domain knowledge, users can identify which subset of the data is not interesting and thus should be discarded from the execution by the SWMS, thereby reducing execution time.</p><p>Data reduction can be accomplished in at least three different forms. First, it can be done before the execution starts. However, in most complex scenarios, the high number of possibilities make it impossible to know beforehand the uninteresting subsets, without any prior execution. Furthermore, not only the initial dataset can be reduced, but also the intermediate data generated by the workflow, since the activities composing it continuously produce significant amounts of data that are consumed by other activities. A second form of data reduction is online. When the SWMS allows for partial result data analysis, the user may analyze the partial data, find which slice of the dataset is not interesting, and reduce the dataset online. We use the term online when user is able to inspect the workflow execution, analyze partial and execution data, and dynamically adapt workflow settings while the workflow is running (i.e., at runtime). The third form of data reduction is by stopping the execution, reducing the data offline, and then resume with the reduced dataset. Because of the difficulty in defining the exploratory input dataset and the long execution time of such workflows, users frequently adopt the third form. However, in the offline form, the SWMS is not aware of the changes, and the results with one workflow configuration are not related to the others. Therefore, this is generally more timeconsuming, there is no control or registration of user interactions, and the execution may become inconsistent <ref type="bibr">[2]</ref>.</p><p>Online data reduction has obvious advantages but introduces several challenges related to computational steering in HPC environments <ref type="bibr" target="#b6">[4]</ref>. First, because of the complexity of the scientific scenario and the huge amount of data, users do not know beforehand which data subset should be kept or removed. Identifying these subsets involves relating input data to intermediate data and final results. Also, if users cannot actively follow the result data evolution online, in particular, domain data associated to execution and provenance data (history of data derivation), they can be driven to misleading conclusions when trying to identify the uninteresting data subset. Second, if they can find which subset to remove and try to remove it, the SWMS must allow for such online reduction and guarantee that the operation will be done consistently. Otherwise, it can introduce anomalous data, with no control over data elimination, data redundancy, or even execution failure. Third, in a long run, there may be more than one interaction, each removing more subsets, at different times. If the SWMS does not keep track of the user's actions, it negatively impacts the results' reproducibility and reliability. Although data reduction is not new in SWMSs <ref type="bibr">[3]</ref>, the problem of doing this online, steered by users, while maintaining data provenance has not been addressed before.</p><p>To address these challenges, we propose a data reduction approach. The key idea is to consider input datasets as sets of input data elements that can be manipulated and related to their following data elements along the dataflow generation. Provenance data management is at the core of the approach. In addition to the traditional advantages of managing provenance data in scientific workflows (i.e., reproducibility, reliability, and quality of result data) <ref type="bibr" target="#b7">[5]</ref>, online provenance data management eases interactive domain data analysis <ref type="bibr" target="#b8">[6,</ref><ref type="bibr" target="#b9">7]</ref>. Such analysis helps finding the data subset to be removed. Moreover, the SWMS must guarantee data consistency in the execution before and after the reduction, and keep track of user steering actions to maintain provenance of adaptations.</p><p>Chiron is a SWMS that implements a data-centric approach. It has successfully been used to manage scientific workflow applications in domains such as bioinformatics <ref type="bibr" target="#b8">[6]</ref>, computational fluid dynamics <ref type="bibr">[2]</ref>, and astronomy <ref type="bibr" target="#b9">[7]</ref>. However, Chiron does not control changes in input datasets, including removing a subset. To implement our data reduction approach in Chiron, we add new operators and modules to enable users to reduce sets of input data for workflow activities online, and maintain consistency of the provenance of the removed data. We take advantage of a distributed in-memory database system (MySQL Cluster) in a version called d-Chiron that is significantly more scalable than Chiron <ref type="bibr" target="#b10">[8]</ref>, to address consistency issues with respect to data reduction. We make the following contributions:</p><p>• A mechanism coupled to d-Chiron SWMS for online input data reduction, which allows users to remove data subsets at runtime. It guarantees that both execution and data remain consistent after reduction. • An extension to a provenance data diagram (which is W3C PROV compliant) to maintain the history of user adaptations when users reduce data online. • A module to track provenance of human adaptation when users reduce data online. The mechanism collects provenance of human-adaptation data when data reductions are done in the datasets being consumed by the workflow. The provenance data is inserted online in the wf-Database, which implements the extended data diagram. This paper is a major extension of <ref type="bibr" target="#b11">[9]</ref>, which introduces the initial ideas of online input data reduction in scientific workflows. In this paper, we extend <ref type="bibr" target="#b11">[9]</ref> with: (i) a formalization of the core concepts of the solution; (ii) a formal definition of the user steering operator to reduce input data; (iii) practical examples of how other real-world scientific workflows (other than the one used in the experimental validation), such as SciPhy <ref type="bibr" target="#b12">[10]</ref> and Montage <ref type="bibr" target="#b13">[11]</ref>, could benefit from our solution; (iv) explanation about how consistency is tackled in the approach, and a detailed description about how we implement it; (v) details about how users use the system; (vi) the exploration of different aspects of the benefits of our solution through a broader set of experiments; and (vii) more related work. Paper organization. Section 2 gives our motivating example. Section 3 describes the background for this work. We present our main contribution in Section 4 and its implementation in Section 5. Section 6 shows how users use the system, Section 7 presents an experimental validation, Section 8 shows related work, and Section 9 concludes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Motivating Case-study in the Oil and Gas Industry</head><p>In ultra-deep water oil production systems, a major application is to perform risers' analyses. Risers are fluid conduits between subsea equipment and the offshore oil floating production unit. They are susceptible to a wide variation of environmental conditions (e.g., sea currents, wind speed, ocean waves, temperature), which may damage their structure. The fatigue analysis workflow adopts a cumulative damage approach as part of the riser's risk assessment procedure considering a wide combination of possible conditions. The result is the estimate of riser's fatigue life, which is the length of time that the riser will safely operate. The Design Fatigue Factor (DFF) may range from 3 to 10, meaning that the riser's fatigue life must be at least DFF times higher than the service life <ref type="bibr" target="#b14">[12]</ref>.</p><p>Sensors located at the offshore platform collect external conditions and floating unit data, which are stored in multiple raw files. Offshore engineers use specialized programs (mostly simulation solvers) to consume the files, evaluate the impact of environmental loads on the risers in the near future (e.g., risk of fractures), and estimate the risers' fatigue life. Figure <ref type="figure" target="#fig_0">1</ref> shows a scientific workflow composed of seven piped programs (represented by workflow activities) with a dataset in between, forming a flow of sets of data elements within linked tasks. The &lt;&lt;Stereotypes&gt;&gt; and dataflow concepts are explained in Section 3.1. Each task of Data Gathering (Activity 1) decompresses one large file into many files containing important input data, reads the decompressed files, and gathers specific values (environmental conditions, floating unit movements, and other data), which are used by the following activities. Preprocessing (Activity 2) performs pre-calculations and data cleansing over some other finite element mesh files that will be processed in the following activities. Stress Analysis (Activity 3) runs a computational structural mechanics program to calculate the stress applied to the riser. Each task consumes preprocessed meshes and other important input data values (gathered from first activity) and generates result data files, such as histograms of stresses applied throughout the riser (this is an output file), and stress intensity factors in the riser and principal stress tensor components. It also calculates the current curvature of the riser. Then, Stress Critical Case Selection (Activity 4) and Curvature Critical Case Selection (Activity 5) calculate the fatigue life of the riser based on the stresses and curvature, respectively. These two activities filter out results corresponding to risers that certainly are in a good state (no critical stress or curvature values were identified). Those cases are of no interest to the analysis. Calculate Fatigue Life (Activity 6) uses previously calculated values to execute a standard methodology <ref type="bibr" target="#b14">[12]</ref> and calculate the final fatigue life value of a riser. Compress Results (Activity 7) compresses output files by riser.</p><p>Most of these activities generate result data (both raw data files and some other domainspecific data values), which are consumed by the subsequent activities. These intermediate data need to be analyzed during workflow execution. More importantly, depending on a specific range of data values for an output result data (e.g., fatigue life value), there may be a specific combination of input data (e.g., environmental conditions) that are more or less important during an interval of time within the workflow execution. The specific range is frequently hard to determine and requires a domain expert to analyze partial data during execution. For example, an input data element for Activity 2 is a file that contains a large matrix of data values, composed of thousands of rows and dozens of columns. Each column contains data for an environmental condition and each row has data collected for a given time instant. Each row can be processed in parallel and the domain application needs to consume and produce other data files (on average, about 14 MB consumed and 6 MB produced per processed input data element). After many analyses online, the user finds that, for waves greater than 38 m with frequency less than 1Hz, a riser fatigue will never happen. Thus, within the entire matrix, any input data element that contains this specific uninteresting range does not need to be processed. Therefore, by reducing the input dataset, the overall data processed and generated are reduced and thus the overall execution time. In this paper, we use this workflow as basis for our examples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Background</head><p>It is known that scientific workflows are data-centric. Data management in scientific workflows is critical due to the inherent complexity of the scientific domain data and the HPC requirements, such as efficient exploitation of data parallelism. In this section, we provide the background for this work, which relies on a data-centric algebraic approach for scientific workflows <ref type="bibr" target="#b15">[13,</ref><ref type="bibr" target="#b16">14]</ref>. It provides constructs, mechanisms, and conceptualizations which in essence aim at valorizing fine-grained elements of data flowing throughout the workflow activities, rather than just the chaining of tasks (i.e., chaining of programs or processes). This enables building solutions for dynamic data analyses and even adaptations in the dataflow. This data-centric approach is used by several modern parallel systems, such as Swift <ref type="bibr" target="#b17">[15]</ref>, Apache Spark <ref type="bibr" target="#b18">[16]</ref>, and Panda <ref type="bibr" target="#b19">[17]</ref>, in order to exploit data parallelism. In Section 3.1, we describe the data-centric approach and in Section 3.2, we explain usersteered workflows, which together set the foundation for this work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Data-Centric Algebraic Approach for Scientific Workflows</head><p>A scientific workflow is composed of a set of activities. An activity can be a program, a script or a function that consumes in parallel input datasets, computes, and produces output datasets. The output dataset of a certain activity can become an input dataset of another, defining a data dependency between these two activities. A dataset (either input or output) is a set of data elements. A data element can contain simple primitive data values (e.g., integers, strings) or complex data objects (e.g., matrices, finite element meshes, images). An input data element has the necessary data to be consumed by a workflow activity computation, or task. In a workflow execution, in addition to the data dependency between chained workflow activities, there may be thousands of independent tasks running in parallel within each activity, as in the Many Task-Computing paradigm <ref type="bibr">[1]</ref>. Figure <ref type="figure" target="#fig_1">2</ref> illustrates a generic representation of a scientific workflow, where the dataset 𝑅 " of output data elements of activity 𝐴𝑐𝑡 &amp; is also a set of input data elements for activity 𝐴𝑐𝑡 " . </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Data-centric Algebraic Notation</head><p>A workflow 𝑊 is a chaining of activities 𝐴 = {𝐴𝑐𝑡 &amp; , … , 𝐴𝑐𝑡 -}, 𝑘 = |𝐴|. An activity 𝐴𝑐𝑡 1 ∈ 𝐴 consumes input datasets 𝑅 1 and produces output datasets 𝑅 13&amp; , which can be consumed, as input datasets, by an activity 𝐴𝑐𝑡 13&amp; ∈ 𝐴, for all activities in 𝐴, forming a dataflow. This is presented in <ref type="bibr" target="#b9">[7]</ref>, inspired by concepts proposed by Ikeda et al. <ref type="bibr" target="#b19">[17]</ref>.</p><p>Not necessarily all workflow activities consume the data elements in the same way. For example, an activity 𝐴𝑐𝑡 1 may produce one output data element in 𝑅 13&amp; for each input data element consumed from 𝑅 1 (i.e., 1: 1 ratio between the cardinalities of the input and output datasets); and a chained activity 𝐴𝑐𝑡 13&amp; may consume all 𝑛 elements from 𝑅 13&amp; to produce a dataset 𝑅 13" with a single data element (𝑛: 1 ratio). Workflow engines can highly benefit from this information to anticipate runtime optimizations of data parallel operations <ref type="bibr" target="#b16">[14]</ref>.</p><p>We distinguish between input and output datasets. Input datasets are composed of input data elements and are consumed by activities. Output datasets are composed of output data elements and are produced by activities. Then, let 𝑅 = {𝑅 &amp; , … , 𝑅 7 }, with 𝑟 = |𝑅|, be the set that contains all datasets (either input our output) for the activities in a workflow 𝑊. The data dependencies of the datasets form the dataflow. Considering these concepts, Ogasawara et al. <ref type="bibr" target="#b16">[14]</ref> formalize the general form of the data-centric workflow algebra as: 𝑅 13&amp; ← 𝐷𝑇 𝐴𝑐𝑡 1 , 𝑎𝑑𝑑𝑖𝑡𝑖𝑜𝑛𝑎𝑙 𝑜𝑝𝑒𝑟𝑎𝑛𝑑𝑠 , 𝑅 1 , where 𝐷𝑇 ∈ {𝑀𝑎𝑝, 𝑅𝑒𝑑𝑢𝑐𝑒, 𝐹𝑖𝑙𝑡𝑒𝑟, 𝑀𝑅𝑄𝑢𝑒𝑟𝑦, 𝑆𝑝𝑙𝑖𝑡𝑀𝑎𝑝} is a data transformation and 𝑎𝑑𝑑𝑖𝑡𝑖𝑜𝑛𝑎𝑙 𝑜𝑝𝑒𝑟𝑎𝑛𝑑𝑠 are additional operands that may be needed by some 𝐷𝑇, such as Reduce. 𝐷𝑇 is a dataflow operator that determines how input data elements are transformed into output data elements, in particular, the ratio between number 𝑛 of input data elements consumed and number 𝑚 of produced output data elements in a workflow activity 𝐴𝑐𝑡 1 . For example, Map has 1:1 ratio, Reduce has 𝑛: 1, SplitMap has 1: 𝑚, filter has 1: (1|0), and MRQuery (e.g., a join of two datasets) has 𝑛: 𝑚 ratio. Figure <ref type="figure" target="#fig_2">3</ref> shows a data-centric algebraic representation of the Risers Fatigue Analysis workflow shown in Figure <ref type="figure" target="#fig_0">1</ref>.  All datasets 𝑅 1 ∈ 𝑅 have a predetermined data schema 𝓢 𝑅 1 = (attribute i1 : datatype i1 ,…, attribute iu : datatype iu ), with 𝑢 = number of attributes of 𝓢 𝑅 1 . The data elements of 𝑅 1 follow the schema 𝓢 𝑅 1 . For example, suppose that 𝑅 &amp; ∈ 𝑅 contains input environmental conditions and 𝑅 " ∈ 𝑅 contains output fatigue life values, then possible schemas for these datasets could be: 𝓢 𝑅 1 = (wind_speed: float, wave_frequency: float, air_temperature: float, humidity: float, mesh_path: string) and 𝓢 𝑅 2 = (fatigue_life: integer, histogram_path: string). Thus, the structure of the data flowing between activities is enhanced. The user is also familiar with those domain terms. This not only contributes to the workflow engine for data parallelism, but also enables submitting structured queries on dataflow generation, which includes domain-data values in the flow.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Scientific Domain Data Management using the Algebraic Approach</head><p>In addition to efficiently managing data parallelism, the SWMS needs to manage the scientific domain data. Structured queries ease data analysis and the algebraic approach is useful here <ref type="bibr">[2,</ref><ref type="bibr" target="#b6">4,</ref><ref type="bibr" target="#b9">7]</ref>. Two types of data are stored in those datasets: domain-specific values and other larger or more complex data. The algebraic approach enables expressing the scientific datasets in a structured way following a data schema 𝓢 𝑅 1 with known data types. Often, scientific programs require the processing of other complex large datasets, such as large matrices, binary data, unstructured information, or other domain-specific data representations. We use paths that point to these data files on disk in the attributes of the data elements composing the datasets 𝑅 1 ∈ 𝑅 whenever a file is read or written by an activity <ref type="bibr" target="#b9">[7]</ref>. Some other domain-specific data values within those files can be extracted or generated based on the content of the file and represented as attributes in the data elements, also increasing the expressivity of the file descriptions <ref type="bibr" target="#b15">[13,</ref><ref type="bibr" target="#b16">14]</ref>. These domain-specific quantities can be decisive for the domain so they need to be tracked. Since tracking all finegrained data elements the user wants may be impractical due to the huge amount of data, the domain expert determines which domain-specific values need to be extracted and tracked. All these data management features enable users to query, analyze, and inspect the scientific raw data and the dataflow as the workflow is being processed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">User-steered Workflows</head><p>There are at least six aspects of computational steering in scientific workflows: interactive analysis, monitoring, human adaptation, notification, interface for interaction, and computing model <ref type="bibr" target="#b6">[4]</ref>. Despite the importance of them all, the first three are essential and are the ones this work focuses on. Human adaptation is definitely at the core of computational steering. However, users will only know how to fine-tune parameters or which subset needs further focus if they can explore partial result data during a long-term execution. Thus, interactive analysis and monitoring are important to put the human in the loop. Online provenance data management in SWMSs is an essential asset to support all six aspects of computational steering in scientific workflows. In this section, we explain the three computational steering aspects explored in this paper and how the data-centric algebraic approach supports them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Interactive Analysis</head><p>To allow for interactive analysis, the data-centric approach designs how fine-grained domain-specific data, workflow execution data, performance data, and provenance data are collected by the workflow engine. It also specifies how they are stored in a database (the wf-Database) to be queried by users. We address two aspects of workflow data that can be interactively analyzed: (A) domain dataflow and (B) workflow execution <ref type="bibr" target="#b6">[4]</ref>.</p><p>(A) Domain dataflow. To allow for domain dataflow interactive analysis, dataflow provenance data are stored in the wf-Database. The input and output data elements are continuously collected and stored in the wf-Database at each task execution at runtime. The output elements are linked to the inputs, so that the flow of data elements can be easily retrieved through provenance queries. This approach enables online fine-grained domain dataflow analysis <ref type="bibr" target="#b8">[6]</ref> as well as the analysis of related domain data files through file flow relationships <ref type="bibr" target="#b9">[7]</ref>, as in Figure <ref type="figure" target="#fig_3">4</ref>.</p><p>To exemplify some possible interactive queries, Table <ref type="table">1</ref> has some typical analyses that are executed for the riser fatigue analysis workflow involving domain and provenance dataflow analysis. In an earlier work, we also observed similar query patterns for scientific data analysis in different domains <ref type="bibr" target="#b20">[18]</ref>. For Queries 𝑄1-𝑄4, the SWMS needs to store the history of the data elements generated in Activities 4 and 5 since the beginning of the flow, linking each element-flow in between. For example, environmental conditions (𝑄1) and hull conditions (𝑄2) are obtained in Activity 1, and stress-and curvature-related values are obtained in Activities 4 and 5, respectively. To correlate output elements from Activity 4 or 5 to output elements from Activity 1, provenance data relationships are required.</p><p>Users can analyze the dataflow by running queries in the database query interface at any time during execution or using any application that connects to the database to plot data visualization. Without such structured query support, users need to look for files in their directories, open and analyze them, and try to do this analysis in an ad-hoc way. Frequently, they write scripts to search in these result files. They often interrupt the execution to fine tune input data and save execution time. This user behavior is observed not only in the oil and gas domain, but also in several other domains, such as bioinformatics, computational physics, and astronomy. More examples exploring how this data-centric approach enables querying domain dataflow together with provenance data to enhance online data analysis in scientific workflows can be found in <ref type="bibr" target="#b8">[6,</ref><ref type="bibr" target="#b9">7,</ref><ref type="bibr" target="#b20">18]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 1. Domain dataflow provenance interactive queries.</head><p>𝑸𝟏 What is the average of the 10 environmental conditions that are leading to the largest fatigue life value? 𝑸𝟐 What are the water craft's hull conditions that are leading to risers' curvature lower than 800? 𝑸𝟑 What are the top 5 raw data files that contain original data that are leading to lowest fatigue life value? 𝑸𝟒 What are the histograms and finite element mesh files related when computed fatigue life based on stress analysis is lower than 60? </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>𝑸𝟓</head><p>Determine the average of each environmental conditions (output of Data Gathering -Activity 1) associated to the tasks that are taking more than the double of the average execution time of Curvature Critical Case Selection (Activity 5), grouping the results by the machines (hostnames) where the tasks of Activity 5 were executed.</p><p>𝑸𝟔 Determine the finite element meshes files (output of Preprocessing -Activity 2) associated to the tasks that are finishing with error status.</p><p>𝑸𝟕 List information about the 5 computing nodes with the greatest number of Preprocessing activity tasks that are consuming data elements that contain wind speed values greater than 70 Km/h. B. Workflow execution. Lower level execution engine information, such as physical location (i.e., virtual machine or cluster node) where a task is being executed, can highly benefit data analysis and debugging in large-scale HPC executions. Users may want to interactively investigate how many parallel tasks each node is running. Moreover, this approach eases debugging. Tasks run domain applications that can result in errors. If there are thousands of tasks in a large execution, how to determine which tasks resulted in domain application errors and what the errors were? Furthermore, performance data analysis is very useful. Users are frequently interested in knowing how long tasks are taking, how much computing resources (memory, CPU, disk IO, network throughput, etc.) are being consumed <ref type="bibr" target="#b21">[19]</ref>. All this workflow execution data are important to be analyzed and can deliver interesting insights when linked to domain dataflow data. When execution data is stored separately from domain and provenance data, these steering queries are not possible or require combining different tools and writing specific analysis programs <ref type="bibr" target="#b9">[7]</ref>. To support all this, the data-centric approach allows for recording parallel workflow execution data in a way that they can be linked to domain and provenance data. Table <ref type="table">1</ref> shows some provenance queries for the Risers Analysis workflow that link workflow execution data to domain dataflow. Figure <ref type="figure" target="#fig_3">4</ref> shows how the domain-data elements flowing within the activities of Risers Fatigue Analysis workflow are managed as datasets in the wf-Database, linked to workflow execution data and dataflow provenance. Also, the datasets contain paths to raw data files on disk <ref type="bibr" target="#b9">[7]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Human Adaptation</head><p>After users have analyzed partial data and gained insights, they may decide to adapt the workflow execution. It brings powerful abilities to users, putting the human in control of a scientific workflow execution. Many aspects can be adapted by humans, but very few systems support human-in-the-loop actions <ref type="bibr" target="#b6">[4]</ref>. The human-adaptable aspects range from computing resources involved in the execution (e.g., adding or removing nodes), to checkpointing and rolling-back (debugging), loop break conditions, reducing datasets, modification of filter conditions, and parameter fine-tuning.</p><p>Populating the wf-Database during workflow execution helps all these aspects. For example, in <ref type="bibr" target="#b22">[20]</ref>, it is shown that it is possible to change filter conditions during execution. Also, in <ref type="bibr">[2]</ref>, a data-centric algebraic approach is proposed to adapt loop conditions of iterative workflows (e.g., modify number of iterations or loop stop conditions). These works show that adaptations can significantly reduce overall execution time, since users are able to identify a satisfactory result before the programmed number of iterations. Prior to this work, no work has been developed to tackle user-steered data reduction online taking advantage of a data-centric approach.</p><p>Since provenance data is so beneficial, we consider that when a user interacts with the workflow execution, new data (user steering data) are generated, and thus their provenance must be registered as well. In a long-running execution, many interactive data analysis and adaptations may occur. If the SWMS does not adequately register the provenance of interaction data, users can easily lose track of what they have changed in the past. This is critical if the entire computational experiment takes days and many adaptations occurred, since it may be impossible to remember in the last day of execution what they have steered in the first days. Furthermore, adding human-adaptation data to the wf-Database enriches its content and enables future interaction analysis. One example of how such data can be exploited is that the registered adaptation data could be used by artificial intelligence algorithms to understand interaction patterns and recommend future adaptations. Thus, the SWMS that enables computational steering should collect provenance of user interaction data. To the best of our knowledge, this has not been done before.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">The Role of Scientists in SWMSs</head><p>This work aims at supporting one type of scientist, i.e., computational scientists, who are the typical users of a steered workflow. However, running complex data-centric workflows in HPC usually involves several scientists with different levels of expertise on each of the aspects involved in the process. We consider three types of scientists: (a) domain scientist, (b) computational scientist, and (c) computer scientist. All these scientists collaborate to scientific discovery.</p><p>(a) Domain scientists. Examples are geologists, biologists, and experimental physicists. They are very familiar with concepts, nomenclature, and semantics of the domain. They are commonly very good at understanding the scientific hypothesis, results and data interpretation. They may not have programming or computational skills. The resulting data of a complex computational simulation are often delivered to them as well organized, cured, and with some aggregations, visualizations, plots, and dashboards. Their main work is typically to give sense to these cured data.</p><p>(b) Computational scientists. Examples are engineers, bioinformaticians, and computational physicists. They are not domain specialists, but have knowledge on the domain. However, they are more focused on the computational aspects. They typically have programming and computational skills, and they are familiar with command line interfaces. They are more prone to learning new computing technologies and use new systems that support their computational simulations. They know how to analyze domain-specific data and metadata and organize large raw data files into analyzed data so they can work together with domain scientists to deeply interpret the data. They know how to chain the different simulation programs and design a scientific workflow to attend the main goal of a computer simulation. They are able to operate a SWMS or dispatch jobs in an HPC cluster.</p><p>(c) Computer scientists. They are experts in developing tools, methods, or systems that support large-scale simulations. Examples are HPC, data management, workflow solution specialists. They do not necessarily have domain knowledge. Often, computer scientists work closely with computational scientists to obtain the best performance for an HPC simulation and achieve the final goal. They can analyze performance, linked with domain and provenance data to help adjusting the system, debugging, and fixing errors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">User-steered Online Data Reduction and Adaptive Monitoring</head><p>In this section, we show our main contributions. In the data-centric approach, removing a subset of the entire dataset to be processed means removing a set of input data elements from a dataset to be consumed by a workflow activity. As a consequence of this removal, the tasks that would process the elements within the removed subset will not be executed, hence, reducing both workflow execution time and data processing. Data processing reduction becomes more evident if the removed data elements contain paths to large raw data files that would be consumed by tasks if the elements were not removed. Furthermore, if an input data element of a given activity is removed, the following elements forming the element-flow of the next linked activities will not be processed too, reducing data and, more importantly, execution time in cascade. In Section 4.1, we introduce a new datacentric algebraic operator for user-steered data reduction. In Section 4.2, we discuss the importance of maintaining data consistent after reduction. In Section 4.3, propose an adaptive monitoring approach. In Section 4.4, we explain our approach in the context of existing workflow execution models. In Section 4.5, we exemplify how real-world scientific workflows can highly benefit from our approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">An Operator for Data Reduction</head><p>Once users analyze data elements that have already been processed, they might identify data elements that will be processed, following the pre-specified dataflow, but will not contribute to the final results. To tackle this, we extend the data-centric algebraic operators (Section 3.1) with a new user-steered operator, 𝐶𝑢𝑡, to enable users to cut off a slice containing input data elements. Definition (Cut): 𝐶𝑢𝑡 is a user-steered data-centric operator that removes a subset of a dataset 𝑅 1 based on activity 𝐴𝑐𝑡 that evaluates the criteria 𝐶. Its general form is: 𝑅 1 a ← 𝐶𝑢𝑡 𝐴𝑐𝑡, 𝐶, 𝑅 1 It transforms an input dataset 𝑅 1 ∈ 𝑅 in the dataflow into the output dataset 𝑅 1 a and the ratio between 𝑛 input data elements and 𝑚 output data elements is 𝑛: 𝑚, with 𝑛 ≥ 𝑚. 𝑅 1 and 𝑅 1 a follow the same data schema 𝓢 𝑅 1 . 𝐶 is the criteria that addresses the slice of input data elements that are removed. 𝐶 may be either a simple predicate (e.g., 𝑎𝑡𝑡𝑟 &amp;c = ′𝐹𝐴𝑇𝐼𝐺𝑈𝐸′) or a minterm predicate (e.g.,</p><formula xml:id="formula_0">𝑎𝑡𝑡𝑟 &amp;&amp; &gt; 38 ∧ 𝑎𝑡𝑡𝑟 &amp;" &gt; 0.1 ∧ 𝑎𝑡𝑡𝑟 &amp;m &lt; 1.0).</formula><p>As any other dataflow operator (e.g., Map, Reduce, Filter, SplitMap, etc.), 𝐶𝑢𝑡 performs data transformation. However, it is not part of the initially designed workflow composition. Differently than the other operators, 𝐶𝑢𝑡 is not initially defined in the workflow composition. Rather, it is dynamically inserted in the dataflow as a result of a user steering action. Then, after the transformation of 𝑅 1 into 𝑅 1 a , it is naturally consumed by the subsequent activities that would consume 𝑅 1 if no reduction happened. 𝐶𝑢𝑡 allows the SWMS to keep track of user adaptations during a dataset reduction, closely relating the metadata about the human action (e.g., data about the user who performed the action, when the action was performed, how the action occurred, relating with the 𝐶 criteria used in the reduction, etc.) to the actual data that have been reduced in a 𝐶𝑢𝑡. In Section 5.5, we propose extensions to a W3C-PROV compliant data diagram to represent provenance data collected. Figure <ref type="figure" target="#fig_4">5</ref> illustrates how 𝐶𝑢𝑡 would be dynamically inserted by a user in the Risers Fatigue Analysis workflow, using the algebraic representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Consistency Issues in a User-steered Data Reduction</head><p>𝐶𝑢𝑡 can only operate on input data elements that are waiting to be processed in the workflow. When a 𝐶𝑢𝑡 happens, the dataset 𝑅 1 that will be reduced is a shared resource between the SWMS engine that is normally processing the workflow in a batch job and the user who wants to remove a slice from 𝑅 1 . Thus, race conditions can occur. Suppose, for example, that at a given instant 𝜏 in time, the SWMS finishes processing a set of data elements and then needs to get new data elements that were waiting to be processed. If at the same time 𝜏, the user decides to remove some of those input data elements that were waiting to be processed, the SWMS may go to an inconsistent state because it could try to process elements that were removed. Or, the user may try to remove a slice that the SWMS already considered to process, thus generating errors. These inconsistencies are even more likely to occur in a highly concurrent execution, such as executions on large HPC clusters with thousands of computing cores.</p><p>To address this problem, we specify a safe subset of an input dataset 𝑅 1 to be applied the reduction. We split 𝑅 1 into two subsets 𝑃 1 and 𝑆 1 , where 𝑃 1 is the subset of 𝑅 1 with input data elements that have already been processed and 𝑆 1 is the subset of 𝑅 1 with elements waiting to be processed. Thus, using set theory, 𝑅 1 ← 𝑃 1 ∪ 𝑆 1 | 𝑃 1 ∩ 𝑆 1 = ∅. 𝑆 1 is the subset of 𝑅 1 that is safe to remove a data slice from. To guarantee this, the SWMS must provide lock controls so that only the subset 𝑆 1 will be reduced. In Section 5.2, we give details about how we implement this in a SWMS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Adaptive Monitoring</head><p>In this section, we present an adaptive monitoring approach that combines monitoring and human adaptation. It helps users following the evolution of interesting parameters and result data to find which subsets of the dataset can be removed during execution. Also, since what users find interesting may change over time, this approach allows the user to adapt the monitoring definitions, such as which data should be monitored and how. The adaptive monitoring relies on online queries to the continuously populated wf-Database. Users can set up monitoring queries (as in Table <ref type="table">1</ref> and Table <ref type="table" target="#tab_1">2</ref>), analyze monitoring results, and adapt monitoring settings.</p><p>Monitoring works as follows. There is a set {𝑄} composed of monitoring queries 𝑚𝑞 1 , 0 ≤ 𝑖 ≤ | 𝑄 |, each one to be executed at each 𝑑 1 &gt; 0 time intervals. Users do not need to specify queries at the beginning of execution, since they do not know everything they want to monitor. This is why {𝑄} starts empty. After users gain insights from the data, after interactive provenance data analyses, they can add monitoring queries to {𝑄} in an ad-hoc manner. Each 𝑑 1 can be adapted, meaning that users have control of the time frame of each 𝑚𝑞 1 during execution. The monitoring queries and settings are stored in the wf-Database.</p><p>Each 𝑚𝑞 1 execution generates a monitoring query result set 𝑚𝑞𝑟 1u , 𝑡 = 𝑘𝑑 1 | 𝑘 ∈ ℕ wx , at each time interval 𝑑 1 . This result set is also stored in the wf-Database. The users have the flexibility to adapt monitoring during workflow execution. To do so, at each time instant 𝑡 after each monitoring query result 𝑚𝑞𝑟 1u has been generated, the values for 𝑑 1 and 𝑚𝑞 1 are reloaded from the wf-Database. If any change has happened, it will be considered in the next iteration 𝑡 + 𝑑 1 . Moreover, at each certain time during execution (also configured by the user), the system checks if the user has added new monitoring queries in {𝑄}. Our approach takes full advantage of the data stored online in the wf-Database to enable users to steer monitoring settings, including which data will be monitored and how. We show how an example of a user adapting monitoring settings in Section 7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Data Reduction in Workflow Execution Models</head><p>Among the different workflow execution models presented in <ref type="bibr" target="#b23">[21]</ref>, our data reduction can be used in both acyclic (sequential and concurrent) and cyclic models. To exemplify a data reduction in these models, let us consider any two activities 𝐴𝑐𝑡 1 and 𝐴𝑐𝑡 13&amp; in a given workflow 𝑊, where one depends on the other. Using the data-centric algebraic representation, we have:</p><formula xml:id="formula_1">𝑅 13&amp; ← 𝐷𝑇 1 𝐴𝑐𝑡 1 , 𝑅 1 ; 𝑅 13" ← 𝐷𝑇 13&amp; 𝐴𝑐𝑡 13&amp; , 𝑅 13&amp; .</formula><p>In a sequential execution between 𝐴𝑐𝑡 1 and 𝐴𝑐𝑡 13&amp; , 𝐴𝑐𝑡 13&amp; only starts to operate when 𝐴𝑐𝑡 1 completely finishes all its tasks <ref type="bibr" target="#b23">[21]</ref>. Suppose that the user wants to reduce input data for 𝐴𝑐𝑡 1 . The safe subset 𝑆 1 contains data elements that are ready to be processed but are only waiting for a free processor, considering that all other processors are still processing tasks from 𝐴𝑐𝑡 1 . This happens when there are more tasks than available processors, which frequently occurs in large-scale workflows. Then, after some tasks for 𝐴𝑐𝑡 1 have been processed, the user can submit analytical queries using the output of 𝐴𝑐𝑡 1 , make a decision, and reduce input data from 𝑆 1 . In this case, the reduction favors 𝐴𝑐𝑡 1 directly, and 𝐴𝑐𝑡 13&amp; indirectly because 𝐴𝑐𝑡 13&amp; has less data to consume. The same occurs when the user wants to reduce input data for 𝐴𝑐𝑡 13&amp; .</p><p>In a concurrent execution between 𝐴𝑐𝑡 1 and 𝐴𝑐𝑡 13&amp; , there is a pipeline of data elements, i.e., when 𝐴𝑐𝑡 1 finishes processing one task that generates output elements, 𝐴𝑐𝑡 13&amp; can process those input elements. In this case, the safe subset 𝑆 1 contains elements that will be processed by 𝐴𝑐𝑡 1 . When they are removed, the tasks from 𝐴𝑐𝑡 13&amp; that would consume the outputs from 𝐴𝑐𝑡 1 are not executed. In this case, the reduction favors both 𝐴𝑐𝑡 1 and 𝐴𝑐𝑡 13&amp; directly, since a reduction in 𝐴𝑐𝑡 1 removes a pipeline between 𝐴𝑐𝑡 1 and 𝐴𝑐𝑡 13&amp; .</p><p>Both sequential and concurrent execution models can be iterated in a cyclic model. Thus, at runtime, when the workflow is running in a specific cycle (iteration), online usersteered data reduction can occur. There are four types of cyclic models <ref type="bibr">[2]</ref>: (1) counting loops without dependencies between iterations (also known as parameter sweep), (2) counting loops with dependencies (iteration 𝑘 + 1 depends on iteration 𝑘), (3) conditional loops (𝑤ℎ𝑖𝑙𝑒 … 𝑑𝑜), and (4) dynamic loops (user adapts loop stop condition). In addition to reducing data inside an iteration, as described for the acyclic models, one can reduce iterations using a dynamic loop model <ref type="bibr">[2]</ref>. Therefore, our user-steered data reduction approach can be used in almost all the execution models presented in <ref type="bibr" target="#b23">[21]</ref>, and is a complementary approach to user-steered iteration reduction in cyclic execution models, as done in <ref type="bibr">[2]</ref>. We carry out our experimental validation using the case study of Section 2, which combines acyclic (with both concurrent and sequential activities) and cyclic (more specifically, parameter sweep) models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Data Reduction in Real-world Scientific Workflows</head><p>Several real-world scientific workflows have this same behavior: data elements organized in datasets flowing in a dataflow. To exemplify data reduction in other different workflow execution models, we use (i) SciPhy <ref type="bibr" target="#b12">[10]</ref>, which iterates over a time consuming input dataset from the bioinformatics domain; and (ii) Montage <ref type="bibr" target="#b13">[11]</ref>, which has been used to benchmark scientific workflow solutions <ref type="bibr" target="#b24">[22]</ref> and represents data-intensive workflows.</p><p>(i) SciPhy <ref type="bibr" target="#b12">[10]</ref> is a bioinformatics workflow composed of eight activities for phylogenetic analysis. In Figure <ref type="figure" target="#fig_6">6</ref>(a) and Figure <ref type="figure" target="#fig_6">6</ref>(c), we show SciPhy with its dataflow and algebraic representation, respectively. SciPhy aims at producing phylogenetic trees that represent evolutionary relationships. These trees are analyzed to identify or discover drug targets. Given an input set of new genomic sequences or genes, specific programs, which are both compute-and data-intensive, are used in a workflow to infer similarity and homology. These sequences are transformed through the dataflow until they arrive at the Model Generator activity, which is mostly compute-intensive, and takes a long time to calculate the similarity. Based on previous execution information stored in a provenance database, combined with domain-specific knowledge, the user can tell that a specific combination of genomic sequences will likely take an undesirable amount of time to complete. This is critical for executions in cloud environments, because it will significantly increase the costs. The constraint in this case is cost and not whether the input will lead to interesting results. The user simply cannot pay for the time the program will take to complete a specific slice of the input or the user prefers to spend more time on sequences that will take shorter time and will return results faster, contributing for the overall analysis. Without our solution, the user needs to stop execution, remove this undesired set of genomic sequences by hand, and restart. This is error-prone, the interactions are not integrated with the workflow execution, and it is time consuming.</p><p>(ii) Montage <ref type="bibr" target="#b13">[11]</ref> is a well-known toolkit for assembling astronomical images into custom mosaics of the sky. The workflow has been modeled using the data-centric algebra <ref type="bibr" target="#b9">[7]</ref>, with nine activities. In Figure <ref type="figure" target="#fig_6">6</ref>(b), we illustrate with a visual representation of the workflow, showing the datasets between each workflow activity. In Figure <ref type="figure" target="#fig_6">6</ref>(d), we show how we model Montage using the data-centric algebra <ref type="bibr" target="#b9">[7]</ref>. The first activity extracts many Flexible Image Transport System (FITS) files. The contents of these files are data about common astronomy coordinate systems, arbitrary image sizes, rotations, and world coordinate system map projects. Each file has twenty different types of data, modeled as attributes of a dataset in the data-centric approach. The activity Create Mosaic builds a mosaic of a delimited region in the outer space and generates an image. Analyzing the generated mosaic, users can infer the presence of an interesting celestial object. Identifying them is hard, subjective, and requires domain expertise.</p><p>By correlating specific combinations of input data values captured in those FITS files with the generated mosaics, the domain specialist can tell whether a certain region of the outer space is more or less likely to contain an interesting celestial object. Identifying this correlation is tricky and it may dynamically change during execution, depending on how the data values are evolving. In particular, users investigate the color intensity to detect potential regions of interest that may represent a celestial object. Thus, a pixel with high color intensity may represent an object emitting light or reflecting it. For this reason, the user needs to monitor the results looking for those color intensity changes. Based on the results, the user can identify regions of the space that are very unlikely to lead to significant color change. These regions, which are delimited in data values in the input FITS files, could be eliminated at runtime, thus reducing processed data and execution time.</p><p>To show a specific case of data reduction, Figure <ref type="figure" target="#fig_7">7</ref> shows an excerpt of the Montage workflow. List FITS activity consumes a list of compressed files and each produces a list of many FITS files (i.e., it has a SplitMap behavior). The list of FITS files is represented as 𝑅 7¡¢£¤u1¡¥ dataset, which contains paths to the actual files stored on disk. The modeled workflow extracts data values from those files and store in 𝑅 7¡¢£¤u1¡¥ dataset. Among these data values extracted, there are CRVAL1 and CRVAL2 that represent two coordinate values to determine a position in the native image coordinate system. The files in 𝑅 7¡¢£¤u1¡¥ are then processed by Projection activity, generating 𝑅 ¦£ §£¤u1¡¥¨7¡¢£¤u1¡¥© dataset, following the remainder of the dataflow. After some time has elapsed and files have been processed, a user runs several data analyses in the dataflow and determines that certain values for CRVAL1 and CRVAL2 will very unlikely lead to an interesting celestial object identification and these files containing the values can be cut off. Thus, the user does a reduction command using criteria C to cut a slice from 𝑅 7¡¢£¤u1¡¥ dataset, transforming it into a reduced 𝑅′ 7¡¢£¤u1¡¥ to be consumed by Projection activity in the remainder of the dataflow.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Implementation in d-Chiron</head><p>d-Chiron <ref type="bibr" target="#b10">[8]</ref> is a SWMS that implements the data-centric approach described in Section 3.1. It collects and stores data at a fine-grain level in the wf-Database during workflow execution. d-Chiron takes advantage of a distributed in-memory database system (MySQL Cluster) to manage the wf-Database. Documentation on how to run d-Chiron and the data schema used to implement the wf-Database are publicly available on GitHub <ref type="bibr">[23]</ref>. In this section, we explain how we implement user-steered data reduction and adaptive monitoring in the data-centric approach in d-Chiron.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Using a Relational DBMS to Implement the Data-centric Approach</head><p>Distributed and parallel relational database technology has been successful at managing very large datasets <ref type="bibr" target="#b25">[24]</ref>. d-Chiron exploits this technology to support many user steering aspects described in Section 3.2. In this section, we give three reasons to explain why relational DBMS is a good choice to implement data reduction in a data-centric SWMS.</p><p>(i) The first motivation is related to the need to find the slice to be removed. A relational DBMS has efficient querying capabilities to enable analysis of sets of data with a query language (SQL) and a query interface. Also, an integrated data modeling using a PROV-compliant data diagram enables complex data queries that analyze scientific domain, provenance, and workflow execution data. This highly contributes to the online analytical capabilities of the SWMS. In Section 6, we show how d-Chiron takes advantage of a query interface to enable users to query the data online to support data reduction.</p><p>(ii) The second motivation is related to the fact that scientific workflows are datacentric. There is a flow of data elements organized in sets (datasets). The relational model is set-oriented and there are multiple native constructs that significantly ease managing the flow of data elements. Thus, not only the user can benefit from querying the data, but also the SWMS engine itself. d-Chiron engine uses SQL to access and modify the data in the wf-Database and uses these data in its internals functioning, such as task scheduling. Also, a relational data model eases data reduction. The user does not need to know about the task scheduling details in a data reduction action. Rather, only the slice criteria (domain data only) can be specified and d-Chiron will use SQL to join domain data with scheduling data to select the affected tasks by a reduction. We explain this in details in this section.</p><p>(iii) The third motivation is related to consistency. It is essential that the data remains consistent within a user-steered data reduction. It is quite complex to guarantee a consistent execution when a user decides to reduce data, in particular in a large HPC execution and without stopping the workflow execution. On the other hand, all parallel and distributed relational DBMS natively provide atomicity, consistency, isolation, and durability (ACID) transactions <ref type="bibr" target="#b25">[24]</ref>. We can take advantage of this capability and implement a user-steered data reduction in a way to outsource to the DBMS complex transaction control that guarantees consistency. We also explain this in this section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Supporting Consistent Data Reduction with a Relational DBMS</head><p>Before actually reducing the data, we explain how a slice is defined in d-Chiron, using its relational DBMS, so it can later be safely removed while guarantying consistency after reduction. Input data elements are consumed by the many parallel tasks (usually thousands in Many Task Computing workflows <ref type="bibr">[1]</ref>) that need to be scheduled by the SWMS engine. Since there is this strong relationship between tasks and domain data elements, which contain scientific domain-specific data values, we represent it in a relational data schema (using crow's foot database notation) where each task is related to one or more data elements of a domain-data dataset: Task Domain Dataset . All datasets in set 𝑅 are specializations of 𝐷𝑜𝑚𝑎𝑖𝑛 𝐷𝑎𝑡𝑎𝑠𝑒𝑡 and the scheduling of input data elements to parallel tasks is represented as instances in 𝑇𝑎𝑠𝑘 table, which stores all tasks in the workflow execution. Thus, for a certain input dataset 𝑅 1 ∈ 𝑅, the join (𝑅 1 ⋈ 𝑇𝑎𝑠𝑘) returns a set containing tasks with their input data elements in 𝑅 1 . Moreover, among other attributes, each task has an important 𝑠𝑡𝑎𝑡𝑒 attribute that determines if a task is READY to be executed (already knows its input data to start, but is waiting for a free CPU so it can be scheduled), RUNNING, COMPLETED (already been successfully executed), BLOCKED (even though there may be free CPUs, the task does not have the input to start yet), or any other state a task may assume. Depending on the data transformation performed by an activity, each task may consume one (e.g., Map, SplitMap, Filter) or more (e.g., Reduce, MRQuery) input data elements. Therefore, we distinguish between (i) activities in which each task consumes one data element (we denote such tasks as 𝑡𝑎𝑠𝑘𝑠 &amp;:&amp; ), and (ii) activities in which each task consumes more than one data element (we denote them as 𝑡𝑎𝑠𝑘𝑠 &amp;:¥ ).</p><p>(i) In activities with 𝑡𝑎𝑠𝑘𝑠 &amp;:&amp; , removing input data element means "informing" the SWMS not to execute the tasks that would consume them, hence reducing overall execution time. To implement the set 𝑆 1 (Section 4.2), a semi-join relational operation <ref type="bibr" target="#b25">[24]</ref> is used to join input data elements from the input dataset 𝑅 1 with tasks in READY state in order to only select the domain data elements that still need to be processed. Then, after having 𝑆 1 , the SWMS can obtain the elements in 𝑆 1 that follow the criteria 𝐶. Since a set containing both the input data elements together with the related tasks that will consume them is important for the implementation of data reduction, we denote this set as § &amp;:&amp; , so that: § &amp;:&amp; ← 𝜎 Á 𝑅 1 ⋉ 𝜎 ©uÃu£ÄÅAEÇ¯È 𝑇𝑎𝑠𝑘 , where the ratio 1: 1 means that the tasks in this set are 𝑡𝑎𝑠𝑘𝑠 &amp;:&amp; (as tasks for Map, Filter, and SplitMap) and the criteria 𝐶 is defined in the 𝐶𝑢𝑡 operator. Finally, the SWMS will know that tasks in § &amp;:&amp; should not be processed.</p><p>(ii) In activities with 𝑡𝑎𝑠𝑘𝑠 &amp;:¥ , a data reduction in an input dataset 𝑅 1 can only occur if the task that will consume them is in a BLOCKED state. The task has not started yet because the needed input data for it to start is still being generated by a running task in a previous activity. When this running task finishes, it signals that the BLOCKED task can start. While it is still blocked and the input data elements are being generated, the user can analyze them and identify data values that can be removed. In this case, we denote the set § &amp;:¥ similarly to the previous one, but it rather returns the input data elements in 𝑅 1 that are being consumed by the tasks in BLOCKED state: § &amp;:¥ ← 𝜎 Á 𝑅 1 ⋉ 𝜎 ©uÃu£ÄÉÊËÁÌAE¯𝑇 𝑎𝑠𝑘 , where the ratio 1: 𝑛 means that the tasks in this set are 𝑡𝑎𝑠𝑘𝑠 &amp;:¥ (as tasks for Reduce and MRQuery data transformations). Finally, the SWMS will know that tasks in § &amp;:¥ should not be processed. In other words, d-Chiron will normally execute the tasks, but with a reduced dataset instead. This is different for 𝑡𝑎𝑠𝑘𝑠 &amp;:&amp; because they cannot be executed if their input datasets are removed. The SWMS will know how to handle the tasks in sets § &amp;:&amp; or § &amp;:¥ as long as it knows the type of data transformation of the activity that would consume the elements defined by the criteria 𝐶. Such verifications are important to guarantee consistency during reduction, which is better explained next.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Steer Module: User-steered Data Reduction Implementation</head><p>To ease slice removal in d-Chiron, we developed the Steer module. With the Steer module, users can issue command lines to inform the name of the input dataset 𝑅 1 and the criteria 𝐶 (see 𝐶𝑢𝑡 definition). More specifically, the slice delimited by 𝐶 is added to the where clause in the SQL query that will form the select expressions. As an implementation decision, instead of physically removing the input data elements (either in § &amp;:&amp; or § &amp;:¥ ) from the wf-Database, we move them to a Modified_Elements table, maintaining the relationships. Likewise, the tasks in § &amp;:&amp; , which cannot be executed, are not physically removed, but they have their state marked as REMOVED_BY_USER. By doing so, we enable these tasks and data elements to be later analyzed with provenance queries, similar to the ones shown in Table <ref type="table">1</ref> and<ref type="table" target="#tab_1">Table 2</ref>.</p><p>To guarantee consistency, we take advantage of d-Chiron's DBMS with ACID transactions. In a user-steered data reduction, both d-Chiron's engine and the Steer module need to concurrently update shared resources: Task and Domain Dataset tables in the wf-Database. The Steer module knows if it is about to reduce data elements within a slice of the type § &amp;:&amp; or type § &amp;:¥ , since it depends on the dataset being reduced, which is a parameter to the module. With respect to the input data elements (either in § &amp;:&amp; or in § &amp;:¥ ), while d-Chiron's engine gets the input data elements to execute, the Steer module needs to concurrently move the cut off input data elements to the Modified_Elements table. With respect to the tasks in § &amp;:&amp; , the Task table is a shared resource because while d-Chiron's engine updates the runnable tasks (select them, update their status to RUNNING, execute them, and mark them as completed), Steer needs to update the Task table to mark the tasks as removed by user, so that the engine will not get them for execution. These concurrent actions make concurrency control critical. Figure <ref type="figure" target="#fig_8">8</ref> illustrates these steps with a sequence diagram. The Steer module acts concurrently with the SWMS engine on the shared resources, which are in red in the figure. The steps 1-3 in Steer module are put together in a single DBMS transaction, which is atomic.</p><p>The wf-Database' tables are distributed, thus making concurrency control of the tables' partitions even more complex. In d-Chiron engine, distributed concurrency control in these tables is outsourced to the DBMS that guarantees the ACID properties <ref type="bibr" target="#b25">[24]</ref>. We developed the Steer module to also exploit the DBMS in a way that the concurrency caused by the aforementioned updates is controlled by the DBMS. Therefore, we implement our approach such that both d-Chiron engine and the Steer module rely on the DBMS to outsource those complex distributed locks and releases of shared resources to guarantee that both execution and data remain consistent before and after a user-steered reduction.</p><p>To store provenance of removed data elements, we extend the wf-Database schema with the table User_Query to store the queries that select the slice of the dataset to be removed. The description for each User_Query column is described in Table <ref type="table" target="#tab_2">3</ref>. We keep track of the removed data elements in table Modified_Elements, which is a table that represents a many-to-many relationship between User_Query and Domain Dataset. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Column name Description query_id</head><p>Auto increment identifier slice_query Query that selects the slice of the dataset to be removed. tasks_query Query generated by the SWMS to retrieve the ready tasks associated. issued_time Timestamp of the user interaction query_type Field that determines how the user interacted. It could be "Removal", "Addition", and others. user_id</p><p>Relationship with the user who issued the interaction query wkfid To maintain relationship with the rest of workflow execution data. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Monitor Module: Adaptive Monitoring Implementation</head><p>To implement our approach to adaptive monitoring, we extend the wf-Database schema. To store {𝑄}, we add the table Monitoring_Query, shown in Table <ref type="table" target="#tab_3">4</ref>. The main advantage of storing monitoring results in the wf-Database (and adequately linking the results with the remainder of the data already stored in this database) whenever a monitoring query result is executed is that users are able to query the results immediately after their generation. The wf-Database can also serve as data source for data visualization applications. We add another table: Monitoring_Query_Result, shown in Table <ref type="table" target="#tab_4">5</ref>, to store monitoring results in the wf-Database. In Section 5.5, we show the extensions of PROV-Wf for these adaptive monitoring concepts.</p><p>A command line starts the Monitor module that runs in background. It establishes a connection with the distributed DBMS. Connection settings are provided in a configuration file. d-Chiron makes use of this configuration file to define the workflow design, workflow general settings, and other user-defined variables. Then, the Monitor program keeps querying the Monitoring_Query table at each 𝑠 time units to check if a new monitoring query was added. The default value for 𝑠 is 30 seconds, as the time interval to check if monitoring queries were added or removed. Users can customize this value. After the Monitor has started, users can add (or remove) monitoring queries to (or from) the Monitoring_Query table. Currently, users can add monitoring queries using a command line to inform which SQL query will be executed at each time interval and the time interval. Whenever the Monitor module identifies that the user added a new monitoring query, it launches a new thread. Each thread is responsible for executing each monitoring query in monitoring_query Raw SQL query to be executed.</p><p>wkfid Relationship between the monitoring queries and the current execution of the workflow. In d-Chiron's wf-Database, there may be data from past executions for a same workflow. Store query results in the wf-Database 3.</p><p>Reload all information for 𝒎𝒒 𝒊 from the wf-Database for the next time iteration. The user could have adapted any of this information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4.</head><p>Wait for 𝒅 𝒊 seconds Monitoring_Query at each defined time interval. A thread is finished when a monitoring query is removed or when the workflow stops executing (in that case, all threads are finished). Figure <ref type="figure" target="#fig_9">9</ref> shows the steps executed at each time interval.</p><p>To enable all these steering capabilities, three of these steps represent queries to the wf-Database, including reads and writes. The stored results can be further analyzed aposteriori or, more interestingly, used as input for runtime data visualization tools, since results are immediately made available after they are generated. In Section 6, we show how users can interact with d-Chiron to add monitoring queries and use the Steer module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Extending PROV-Wf for Data Reduction and Adaptive Monitoring</head><p>The data schema that governs the data organization in the wf-Database follows PROV-Wf <ref type="bibr" target="#b8">[6]</ref>. It adheres the W3C PROV <ref type="bibr" target="#b26">[25]</ref> recommendations to help query specification, to maintain compatibility between different SWMSs, and facilitate interoperability between different databases. PROV-Wf specializes PROV concepts for scientific workflows. It is called PROV-Wf, which allows for domain, execution, and workflow provenance data representation at a finer grain than PROV.</p><p>We propose extensions to PROV-Wf to accommodate the concepts presented in this work. These concepts extended are: UserQuery, MonitoringQuery, and MonitoringResult, as in Figure <ref type="figure" target="#fig_10">10</ref>. Using PROV nomenclature, UserQuery is a PROV Activity that stores the slice that represents sets of data elements that will be removed. MonitoringQuery is a PROV Activity that contains the monitoring queries submitted by the user in specific time intervals. The monitoring queries generate PROV Entity MonitoringResult that stores the query results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Using d-Chiron</head><p>The ultimate goal of this work is to contribute with user-steered workflows in HPC. In Section 3.2, we explained that there are at least six aspects that need to be considered for this: interactive analysis, monitoring, human adaptation, notification, interface for interaction, and computing model <ref type="bibr" target="#b6">[4]</ref>. In this work, we mostly focused on the first three, considering human adaptation as the core of computational steering and the one we mostly contributed with. As most related contributions to putting the human in the loop of HPC workflows <ref type="bibr">[2,</ref><ref type="bibr" target="#b27">26,</ref><ref type="bibr" target="#b28">27]</ref>, we focused on the efforts for engineering the backend enabling technology for user steering in an HPC workflow. More specifically, we contributed with allowing users to steer data reductions in scientific workflows online, focusing on providing a consistent execution within a data reduction, managing provenance data of user steering actions, and minimizing performance overheads in the HPC system (discussed in Section 7.4). Enabling such features without jeopardizing performance in an HPC environment is sufficiently complex. However, besides engineering the backend enabling technology, the interface for interaction is another aspect to be considered.</p><p>Designing good interfaces requires usability studies to determine whether the interfaces are in fact good for the target user profile (i.e., computational scientists in our case). This would need a comprehensive user experience test to understand user behavior while interacting with their workflows, then we would develop interfaces based on the gathered design insights, and evaluate the usability. For a valid and comprehensive usability evaluation, we would need to ask multiple users to use the system and the modules developed, observe how they use, and interview them. However, the general scenario (HPC workflows) is very complex. Our user profile is quite rare (compared with general business applications) and the results depend on the domain and on the application in the domain. For example, if we want to measure the time a user takes to identify that a certain slice will not contribute for the final results and then remove it, a valid evaluation would require analyzing multiple users of a same application, in a same domain. Finding users of a same specific application is so rare that makes a comprehensive usability test extremely hard. Also, many other questions need to be addressed. For instance, "does the user expertise in the domain-application interfere in the results? -perhaps the more experienced the user is, the faster she will find which slice to remove and the better she understands the consequences of a reduction"; or "what if the tests were carried out on a different application for a same domain?"; or "what about a different domain?". Additionally, an extra raises because using an HPC cluster requires scheduling. For a best usability test, the analyzer needs to observe the user while she is interacting with the HPC workflow, and thus the analyzer's and the user's scheduling must match the HPC job scheduling time, for each user. In other words, a valid and comprehensive usability test would require observing users of a same application, of different domains, of different expertise levels, and matching scheduling times with the HPC cluster. Combining these requirements makes it very hard and out of the scope of this work, which focuses on the enabling backend technology for steering an HPC workflow.</p><p>Therefore, instead of usability tests, in this section, we show how users can use our modules Steer and Monitor in d-Chiron. Before developing, we interviewed few computational scientists. We found that they are very used to command line interfaces and they frequently have to learn new computational tools. They often browse logs in terminals and follow execution status of their simulations. To reduce data, they need to stop their workflow process, modify the input datasets by hand, and restart execution. For some users, this means resubmitting a job to an HPC cluster subject to scheduling. This may be really long (even weeks). Therefore, developing a technology that allows them to reduce data online, based on provenance data analysis through structured queries (rather than Unix-like shell commands to filter multiple logs in the file system -which is what most of them do) is a very desirable feature and we are not aware of any other SWMS that provides it. Thus, we developed simple command line interfaces to enable them to steer monitoring queries and reduce data online. Since developing the best interface and analyzing its usability is out of the scope, the command line interfaces are our current best effort to make the technology usable. As we show in Section 7, for validation purposes, a real user is able to use the system, after a d-Chiron specialist trained her and provided support.</p><p>In d-Chiron, computer scientists who are experts in operating d-Chiron work closely with computational scientists, the target-user of this work. However, computational scientists are able to operate d-Chiron and steer the running workflow. Before steering a workflow, the workflow has to be modeled. The user identifies the input and output data elements of each of those activities and gives a name to the dataset that contains those elements (following the data-centric algebra presented in Section 3.1). When this is done, d-Chiron modules creates tables corresponding to those datasets, where each table column is an element produced or consumed by a workflow activity. If needed, application-specific extractor scripts are built to collect output data to be stored in the wf-Database <ref type="bibr" target="#b9">[7]</ref>.</p><p>The aspects of computational steering workflows tackled in this work are strongly related to how d-Chiron manages the data and the dataflow. It is all about the wf-Database being populated online by the SWMS. The workflow execution plan depends on the data in this database (hence can be adapted at runtime) and the wf-Database is available for user queries immediately after the workflow has started to run and data elements in the domaindataflow are stored while they are generated. Then, they are linked to execution and provenance data in the wf-Database to enable queries that integrate all these data.</p><p>Therefore, the main way users can interact with a workflow execution in d-Chiron is through query interfaces, generally provided by the DBMS, to query the wf-Database. To be able to run queries, the user must understand the database schema [23] that logically organizes data in d-Chiron. Computational scientists can work with computer scientists (d-Chiron experts in this case) so they can build complex analytical SQL queries to interactively analyze the dataflow. From our experience, computational scientists do not take much time to learn how to write simple queries to a relational database and they later learn how to write complex analytical queries on their own. d-Chiron uses MySQL Cluster to manage its wf-Database. MySQL users are accustomed to using MySQL Workbench as a visual interface to the DBMS. They can see the relational database schema, build and run SQL queries, and get their tabular results in the interface as the workflow runs. Figure <ref type="figure" target="#fig_13">11</ref> shows how MySQL Workbench can be used to write Q5 (from Table <ref type="table" target="#tab_1">2</ref> described as natural language in Section 3.2.1), which integrates domain, provenance and execution data in a same query. More queries with their natural langue descriptions are on GitHub <ref type="bibr">[23]</ref>.</p><p>However, such interactivity does not need to use SQL queries only. Some users prefer graphical user interfaces, so there are many other ways to interact with a DBMS: graphical interfaces with drag and drop boxes to help building queries, Natural Language to Database solutions to translate regular English sentences into SQL queries, dashboards that plot results from a query, etc. d-Chiron developers have been working on new tools to facilitate such interactions, including dashboards that plot monitoring charts or command line tools that does not require users to type raw SQL but simpler commands. In this work, discussing the usability of these interfaces is not the focus. However, we show how users currently use command line interfaces for the modules Steer and Monitor.</p><p>For the Steer module (Figure <ref type="figure" target="#fig_11">12</ref>), a user informs who is going to interact (this information is stored in the wf-Database for provenance) and passes a configuration file that contains information about the workflow, the HPC cluster, and the DBMS connection settings. After that, a user can run as many dataflow steering commands as necessary informing the input dataset 𝑅 1 in the workflow that will be reduced and the C criteria to select the slice (operands from 𝐶𝑢𝑡 definition in Section 4.1). These actions are stored in the wf-Database for provenance. The response messages (in green) allow the user to understand what is happening after a command line is issued. In particular, after a 𝐶𝑢𝑡 action, the response message informs the user of the number data elements that were removed from the dataset to be processed. For more complex analyses on the consequences of those reductions, users can query the wf-Database using the tables introduced in this work to verify, for example, if there were files and their sizes to quantify the number of bytes that were not processed. We do those analyses in the next section.</p><p>For the Monitor module (Figure <ref type="figure" target="#fig_12">13</ref>), a user runs a command to start the monitoring module as a background process on any cluster node that has access to the DBMS, usually the same node from which the SWMS execution was launched. Then, users can add monitoring queries at any time. The monitoring query results are also properly stored in the 1. $&gt; ./Monitor --start --conf=SC.xml System is ready to accept new monitoring queries. 2. $&gt; ./Monitor --add --mq="`cat q1.sql`" --label="query 1" --interval=30 Monitoring query "query 1" will be executed at each 30 seconds. 3. $&gt; ./Monitor --add --mq="`cat q2.sql`" --label="query 2" --interval=20 Monitoring query "query 2" will be executed at each 20 seconds. 4. $&gt; ./Monitor --update --label="query 2" --interval=5 Monitoring query "query 2" was updated. It will be executed at each 5 seconds. 5. $&gt; vi q1.sql 6. $&gt; ./Monitor --update --label="query 1" --mq="`cat q1.sql`" Monitoring query "query 1" was updated.  wf-Database, as they are generated. Dashboard graphic visualization applications can query these results to deliver better data visualization for the user.</p><p>In this example, after the user starts the monitoring module (line 1), two monitoring queries are added with intervals 30 and 20 seconds, respectively 2 and 3). The user wrote the queries in text files (q1.sql and q2.sql), which are loaded in the Monitor --add commands, using cat Unix command. Those query files are only to facilitate the command lines and they are not a requirement. A user could write the query string directly in the command line. After some time, user decides to decrease the time interval in the monitoring of query with label "query 2" by issuing line 4. In line 5, user decides to modify a specific query aspect (e.g., increase the result limit) by editing the query text file and in line 6 he modifies the monitoring query. All these interactions are properly stored in the wf-Database for provenance, following the data schema extensions provided in Section 5.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Experimental Validation</head><p>In this section, we validate our solution for online data reduction based on a real case study. Section 7.1 shows the experimental setup. Section 7.2 presents a use case where the user deals with the Steer and Monitor modules. Section 7.3 provides broader analyses of reduction and Section 7.4 analyzes the added overheads.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Experimental Setup</head><p>Scientific workflow. We use the Riser Fatigue Analysis workflow (see Figure <ref type="figure" target="#fig_0">1</ref>), which is based on a real-world case study. The workflow processes over 350 GB of raw data. In all executions, we use the same dataset, which spans over 60,000 data elements to be processed in parallel. Depending on the workflow activity, tasks may take few seconds (e.g., Activity 1) or up to one minute on average (e.g., Activity 3). The execution model is an iterative workflow (parameter sweep) with concurrent activities, except for the last activity (Activity 7), which is a 𝑅𝑒𝑑𝑢𝑐𝑒 that requires that Activity 6 completely finishes before it can start.</p><p>Software. In all executions, we use d-Chiron <ref type="bibr" target="#b10">[8]</ref> with MySQL Cluster 7.4.9 as its inmemory distributed database system to manage the wf-Database. The code to run d-Chiron and setup files are available on GitHub <ref type="bibr">[23]</ref>.</p><p>Hardware. The experiments were conducted in Grid5000 using a cluster with 39 nodes, containing 24 cores each (summing 936 cores). Every node has two AMD Opteron 1.7 GHz 12-core processors, 48GB RAM, and 250GB of local disk. All nodes are connected via Gigabit Ethernet and access a shared storage of 10TB.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Test Case</head><p>Let us consider the following scenario. Peter is an offshore engineer, expert in riser analysis and learned how to set up monitoring, analyze d-Chiron's wf-Database, and use the Steer module developed in this work. In Peter's project, the Design Fatigue Factor is set to 3 and service life is set to 20 years, meaning that fatigue life must be at least 60 years (Section 2). Peter is only interested in analyzing risers with low fatigue life values, because they are critical and might need repair or replacement. During workflow execution, it would be interesting if Peter could inform the SWMS, which input values would lead to low risk of fatigue, so they could be removed. However, this is not simple because it is hard to determine the specific range of values (i.e., the slice to be removed). For this, Peter first needs to understand the pattern of input values associated to low risk of fatigue life values. In the workflow (Figure <ref type="figure" target="#fig_0">1</ref>), the final value of fatigue life is calculated in Activity 6, but input values are obtained as output of Activity gathered from raw input files. Keeping provenance is essential to associate data from Activity 1 with data from Activity 6.</p><p>To understand which input values are leading to high fatigue life values, Peter monitors the generated data online. For simplicity, we consider wind speed, which is only one out of the many environmental condition parameter values captured by Activity 1 to serve as input for Activity 2. Peter knows that wind speed has a strong correlation with fatigue life in risers. He expects that with low speed winds, there is a lower risk of accident.</p><p>When workflow execution starts, the Monitor module is initialized. Then, Peter adds two monitoring queries: 𝑚𝑞 &amp; shows the average of the 10 greatest values of fatigue life calculated in the last 30s of workflow execution, setting 𝑑 &amp; = 30 s; and 𝑚𝑞 " shows the average wind speed associated to the 10 greatest values of fatigue life calculated in the last 30s, also setting the query interval 𝑑 " = 30s. We recall from Table <ref type="table">1</ref> that 𝑚𝑞 &amp; is similar to 𝑄1, but only considering data processed in the last 30 s. 𝑚𝑞 &amp; and 𝑚𝑞 " queries are added to the Monitoring_Query table.</p><p>Peter monitors the results using the Monitoring_Result table. These results can be a data source for a visualization tool that plots dashboards dynamically, refreshed according to the query intervals. After gaining insights from the results and understanding patterns, he can start removing the undesired values for wind speed. The monitoring query results 𝑚𝑞𝑟 &amp;u and 𝑚𝑞𝑟 "u for the two previously listed queries, as well as when the user reduced the data, are plotted along the workflow elapsed time, as shown in Figure <ref type="figure" target="#fig_14">14</ref>. It shows 𝑚𝑞𝑟 &amp;u (Fatigue life) in green line with square markers and 𝑚𝑞𝑟 "u (Wind speed) in blue line with triangle markers. These markers determine when the monitoring occurred.</p><p>The workflow execution starts at 𝑡 = 0, but only after approximately 150 s, the first output results from Activity 6 start to be generated. From the first results, at t=150 and t=180, Peter checks that when wind speed is less than 16 km/h (see horizontal dashed line in 𝑤𝑖𝑛𝑑 𝑠𝑝𝑒𝑒𝑑 = 16 in Figure <ref type="figure" target="#fig_14">14</ref>, the results lead to the largest fatigue life values. Since risers with large fatigue life values are not interesting in this analysis, he decides, at t =190, to remove all input data elements that contain wind speed less than 16 km/h. For this, the first user query 𝑞 &amp; is issued with a command line to the Steer module. User queries are represented with red circles in the horizontal axis (Elapsed time). The time a user issued an interaction query is stored in User_Query table.</p><p>The next marker after 𝑞 &amp; happens at 𝑡 = 210. Comparing with the previous monitoring mark, at 𝑡 = 180, we can observe that this Peter's steering (𝑞 &amp; ) increases the minimum speed values to be considered from 14.2 km/h to 24.1 km/h. Also, we observe a significant decrease in the slope of the largest values for fatigue life <ref type="bibr">(10.6% lower)</ref>. This means that the removal of these input data containing wind speed less than 16 km/h made the SWMS not process data containing low wind speed values, which would lead to larger fatigue life results.</p><p>Then, monitoring continues, but that slope decrease calls Peter's attention. To obtain a finer detail of what is happening, he decides to adjust monitoring settings, the monitoring interval times (𝑑 &amp; and 𝑑 " ) in this case, at runtime. He reduces them to 10 s to get monitoring feedbacks more frequently. We can observe that for both lines 𝑚𝑞𝑟 &amp;u and 𝑚𝑞𝑟 "u , the markers become more frequent during 𝑡 = <ref type="bibr">[220,</ref><ref type="bibr">270]</ref>. This is because a monitoring is registered at every 10 s. Although we show monitoring correlations between wind speed and fatigue life, other monitoring correlations could also be analyzed and users can add, remove or adjust monitoring queries at any time during execution. After verifying that the results are reasonable, he decides to adjust the monitoring setting to increase back the monitoring query intervals for both queries to 30s after 𝑡 = 270. Then he observes that since 𝑞 &amp; , wind speed less than 25 km/h are leading to large fatigue life values. Then, at 𝑡 = 310, he calls Steer again to issue 𝑞 " that removes input data for wind speed &lt; 25 km/h. The next markers after 𝑞 " shows that this steering made the wind speed value associated to large fatigue life be at least 30.5 Km/h and a decrease of 6.5% in large fatigue life values between 𝑡 = 300 and 𝑡 = 330.</p><p>Similarly, Peter continues to monitor and steer the execution. He issues 𝑞 m at 𝑡 = 370 to remove input data with wind speed &lt; 30.5 km/h, making a decrease of 4.9% in large fatigue life (comparing fatigue life in 𝑡 = 360 and 𝑡 = 390). Then, he issues 𝑞 Ó at 𝑡 = 430 to remove input data with wind speed &lt; 34.5, attaining a decrease of 1.7% in large fatigue life (comparing fatigue life in 𝑡 = 420 and 𝑡 = 450). Despite this small decrease, he decides at t = 520 to further remove data, but with wind speed &lt; 35.5 km/h. However, no decrease greater than 1% in the large fatigue life values was registered after this last Peter's steering. Thus, he keeps analyzing the monitoring results, but does not remove input data anymore until the end of execution.</p><p>We store each interaction in the User_Query table and map (in table Modified_Elements) its rows with rows in Domain Dataset and Task tables, to consistently keep provenance of which data elements were modified (in this case, removed) by each specific user steering. Thus, keeping provenance of user steering helps analyzing how specific interactions impacted the results. Figure <ref type="figure" target="#fig_14">14</ref> shows that some specific interactions imply significant changes in lines' slopes (key output values for the user).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">Analyzing User-steered Data Reduction</head><p>In this section, we analyze how those previous user interactions impact the amount of resources saved during the workflow execution. More specifically, we analyze three aspects: (i) the number of data elements reduced, (ii) the time that was saved due to the input data not processed, and (iii) the number of bytes of the raw data files that were not processed. For validation purposes, we can count the resources saved as consequences of a data reduction. For this, we compare the executions with and without user-steering. We run the exact same workflow and input datasets for both scenarios. The workflow execution with no steering processes all input data, including those containing wind speed values that lead to risers with risk of fatigue, which are not valuable for Peter's analyses.</p><p>In Figure <ref type="figure" target="#fig_15">15</ref>, we depict the three analyzed aspects per activity in the workflow. In other words, we count the total input data elements each activity consumes; the total number of gigabytes of data files processed in each activity; and the total time each activity took to complete. In total, considering all activities, the workflow with no steering processed 60,939 input data elements in parallel, 356GB of domain data files, and the overall execution time was 16.3 min running on the 936-cores cluster.</p><p>Then, we can compare these numbers with analogous numbers in the scenario with user-steered data reductions. Table <ref type="table" target="#tab_5">6</ref> summarizes the user interactions (i.e., user-steered reductions) performed as described in the previous section. Figure <ref type="figure" target="#fig_6">16</ref> illustrates how each interaction 𝑞 1 affected the three analyzed aspects in each workflow activity: Figure <ref type="figure" target="#fig_6">16(a)</ref> shows the number of input data elements reduced, Figure <ref type="figure" target="#fig_6">16(b)</ref> shows the time saved, and Figure <ref type="figure" target="#fig_6">16</ref>(c) shows the amount of gigabytes not processed due to data reduction. In the three charts, although the reductions happen in dataset 𝑅 " consumed by 𝐴𝑐𝑡 " , we can see that they impact all subsequent activities (𝐴𝑐𝑡 &amp; , which is a preceding activity, is not affected by the reductions). In particular, we can see that the first interaction 𝑞 &amp; alone causes a time reduction of 15% (𝑞 &amp; makes 𝐴𝑐𝑡 m complete 33s faster, whereas without reductions 𝐴𝑐𝑡 m would take 221 s).</p><p>Figure <ref type="figure" target="#fig_7">17</ref> shows the summary of the impacts in the entire workflow by each interaction 𝑞 1 . Overall, the steering reductions in this experimental validation yield a reduction of 7,854 out of 60,939 data elements (12.89%), including elements in 𝑅 " and elements in subsequent datasets as consequences of the reduction in 𝑅 " . Also, the interactions make the SWMS not process 51GB out of 356GB (14.3% of data files processing reduction) and the activities run faster, reducing in total 5.3 min out of 16.3 min (32% of total workflow execution time reduction) in the 936-cores cluster. In particular, we see that the first usersteered reduction 𝑞 &amp; represents 45% of the total amount of time saved, meaning that at the beginning, the user can identify a large slice of the input data that would not lead to interesting results, and we see that the last interaction 𝑞 c did not considerably affect execution. These results were obtained by querying the wf-Database at the end of execution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.4">Analyzing the Monitoring Overhead</head><p>The SWMS we used to implement our solution implements the data-centric algebraic approach and captures and manages domain dataflow, provenance, and execution data in a fine-grain level during execution, enabling users to query these data online. These functionalities add some overhead. Measuring this overhead is out of the scope of this work, but some measurements are provided in <ref type="bibr" target="#b10">[8,</ref><ref type="bibr" target="#b16">14,</ref><ref type="bibr" target="#b29">28]</ref>.</p><p>However, in this section, we discuss the overhead caused by the solutions proposed in this work. First, when a user-steered data reduction happens, there are data movements in the wf-Database, i.e., some tasks and input data elements are updated or transferred from a table to another (see Section 5.3). Time spent doing these updates in the database is significantly lower than the overall workflow execution time. In fact, each data reduction   𝑞 1 (Table <ref type="table" target="#tab_5">6</ref>) takes less than 1 second to finish, whereas the overall execution time of the workflow, after the reductions, is 661 s. Thus, we consider those data movements' overhead negligible. Second, our adaptive monitoring solution adds overheads and need to be measured. Recall that every monitoring query 𝑚𝑞 1 in {𝑄} is run by a thread at each 𝑑 1 seconds. Depending on the number of threads (|{𝑄}|) and on the interval 𝑑 1 there may be too many concurrent accesses to the wf-Database, which may add overhead.</p><p>To measure this, we set up the Monitor module to run queries, which are variations of the queries 𝑄1-𝑄7 (Table <ref type="table">1</ref> and<ref type="table" target="#tab_1">Table 2</ref>). For example, in 𝑄2, we vary the curvature value. We also modify them to calculate only the results over the last 𝑑 seconds, at each 𝑑 seconds. To evaluate the overheads, we measure execution time without monitoring and then with monitoring, but varying the number of queries |{𝑄}| and the interval 𝑑, which is considered the same for all queries in {𝑄} in this experiment. The experiments were repeated until the standard deviation of workflow elapsed times was less than 1%. The results are the average of these times within the 1% margin. Figure <ref type="figure" target="#fig_17">18</ref> shows the results, where the blue portion represents the workflow execution time when no monitoring is used and the red portion represents the difference between the workflow execution time with and without monitoring (i.e., the monitoring overhead). From these results, we observe that when the interval 𝑑 is equal to 30s, the overhead is negligible. For 1s interval, the overhead is higher when the number of monitoring threads is also higher. This happens because three queries are executed in each time interval (see Figure <ref type="figure" target="#fig_9">9</ref>), for each thread. In the scenarios with 30 threads, there will be 120 queries in a single time interval 𝑑. In that case, if 𝑑 is small (e.g., 𝑑 = 1), there are 120 queries being executed per second, just for the monitoring. The database that is queried by the monitors is also concurrently queried by the SWMS engine, thus adding higher overhead. However, even in this specific scenario that shows higher overhead (|{𝑄}| = 30 and 𝑑 = 1), it is only 33 s or 3.19% higher than when no monitoring is used. Most of the real monitoring cases do not need such frequent (every second) updates. If 30s is frequent enough for the user, there might be no added overhead, like in this test case. We also evaluated the same scenarios without storing monitoring results in the wf-Database, but rather appending in CSV files, which is simpler. As Figure <ref type="figure" target="#fig_17">18</ref> shows, the results are nearly the same as in either cases (saving in the wf-Database or saving in CSV files). This suggests storing all monitoring results in the wf-Database at runtime, which enables users to submit powerful queries as the monitoring results are generated, with all other provenance data. This would not be possible with a solution that appends data to CSV.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Related Work</head><p>Considering our contributions, we discuss the SWMSs with respect to human adaptation (especially data reduction), online provenance support, and monitoring features.</p><p>We proposed a data-centric data reduction approach, which requires modifications in the workflow scheduling, since tasks associated to the removed input data should not be executed. Consequently, they should not be a part of the workflow execution plan. To be able to support online human adaptation, the SWMS needs to employ a data-centric execution model. Although online human adaptation is the core of computational steering, there are few SWMSs <ref type="bibr" target="#b30">[29]</ref><ref type="bibr" target="#b31">[30]</ref><ref type="bibr" target="#b32">[31]</ref> that support it. These solutions have monitoring services and are highly scalable, but do not allow for online data reduction as a mean to reduce execution time. WorkWays <ref type="bibr" target="#b28">[27]</ref> is a powerful science gateway that enables users to steer and dynamically reduce data being processed online by dimension reduction or by reducing the range of some parameters, sharing similar motivations to our work. It uses Nimrod/K as its underlying parallel workflow engine, which is an extension of the Kepler workflow system <ref type="bibr" target="#b33">[32]</ref>. WorkWays presents several tools for user interaction contributing to humanin-the-loop workflows, such as graphic user interfaces, data visualization, and interoperability among others. However, WorkWays does not provide for provenance representation and users may not define an online query involving simulation data, execution data, metadata, and provenance, all related in a database, which limits the power of online computational steering. For example, it prevents ad-hoc data analysis using both domain and workflow execution data, such as those presented in Table <ref type="table">1</ref> and Table <ref type="table" target="#tab_1">2</ref>, which support the user in defining which slice of the dataset should be removed. In contrast, d-Chiron uses an in-memory distributed database system to manage and relate analytical data involved in the workflow execution. Moreover, the lack of provenance data support in WorkWays, either online or post-mortem, does not support reproducibility and prevents from registering user adaptations, missing opportunities to determine how specific user interactions influenced workflow results. Another SWMS example is WINGS/Pegasus <ref type="bibr" target="#b34">[33]</ref>, which focuses on assisting users in automatic data discovery. It helps generating and executing multiple combinations of workflows based on user contraints, selecting appropriate input data, and eliminating workflows that are not viable. However, it differs from our solution in that it tries to explore multiple workflows until finding the most suitable one, whereas we often model our experiments as one single scientific workflow to be fine tuned as the results come out. Also, it does not employ a data-centric execution model and does not aim at providing online computational steering support to actively eliminate subsets of an input dataset, especially based on extensive ad-hoc intermediate data analysis online. It has a static execution model, in the sense that the execution is predefined, submitted to the HPC environment, and no online human adaptation is enabled. Additionally, as WorkWays, provenance data is not collected online, nor is it integrated with domain-specific and execution data for enhanced analysis. Likewise, solutions like VisTrails and Kepler do not support data-centric online human adaptations.</p><p>Different from those SWMSs, Swift <ref type="bibr" target="#b17">[15]</ref> has a data-centric execution model. It has a parallel dataflow programming language that allows to write scripts for distributing tasks across distributed computing resources. It runs multiple programs in parallel as soon as their input data are available. It generates tasks at runtime and, consequently, it has potential to enable online human-adaptation in the execution plan. However, to the best of our knowledge, it does not support user-steered online input data reduction, nor does it store provenance data related to domain data during workflow execution.</p><p>Moreover, while our data reduction techniques aim at avoiding data to be generated, there is an intense area of data reduction research focused on reducing data already generated by the simulation. For example, initiatives like CODAR <ref type="bibr" target="#b35">[34]</ref> propose data reduction strategies such as dimension reduction, outlier detection and compression also based on online data analyses, which are complementary to our approach.</p><p>Although human adaptation is a desired feature that remains an open problem in SWMSs, monitoring is widely supported in several existing SWMSs <ref type="bibr" target="#b6">[4,</ref><ref type="bibr" target="#b36">35]</ref>. For example, Pegasus <ref type="bibr" target="#b37">[36]</ref> provides a framework to monitor workflow executions and has rich capabilities for online performance monitoring, troubleshooting, and debugging. However, in such solutions, it is not possible to monitor workflow execution data associating to provenance and domain data, or run ad-hoc online data queries, as we do using data in the wf-Database. To the best of our knowledge, no related work allows for online data reduction based on a rich analytical support with adaptive monitoring and provenance registration of human adaptations in HPC workflows. These features allow for performance improvements of scientific workflows, while keeping data reduction consistency and provenance queries that can show the history of user-steering actions and results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.">Conclusion</head><p>This work contributes to putting the human in the loop of online scientific workflow executions, especially when users can actively steer and reduce data to improve performance. As a solution to the input data reduction problem, we make use of a datacentric algebraic approach that organizes workflow data to be processed as sets of data elements stored in a wf-Database, managed by an in-memory distributed database system at runtime. We introduced 𝐶𝑢𝑡 as part of a new class of algebraic operators that only exist because of dynamic human adaptation actions. This is the first work that introduces this representation for dynamic workflow adaptations. 𝐶𝑢𝑡 is just one among many other user adaptation possibilities that are yet to be explored. We developed a mechanism coupled to d-Chiron, a distributed version of Chiron SWMS, to implement 𝐶𝑢𝑡 and maintain both data and execution consistency after a reduction, and track provenance of user adaptations. A major challenge to the problem of data reduction is to identify which subset of the data should be removed. To address it, we proposed an adaptive monitoring approach that aids users in analyzing partial result data at runtime. Based on the evaluation of input data elements and its corresponding results, the user may find which subset of the input data is not interesting for a particular execution, hence can be removed. The adaptive monitoring allows users not only to follow the evolution of the workflow, but also to dynamically adjust monitoring aspects during execution. We extended our previous workflow provenance data diagram to be able to represent provenance of the online data reduction actions by users and the monitoring results. Although we implemented our solution in d-Chiron, other SWMS could be used if provenance, execution, and domain dataflow data are managed in a database at runtime.</p><p>To validate our solution, we executed a data-intensive parameter sweep workflow based on a real case study from oil and gas industry, running on a 936-cores cluster. A test case demonstrated how the user can monitor the execution, dynamically adapt monitoring settings, and remove uninteresting data to be processed, all during execution. Results for this test case show that the user interactions reduced the execution time by 32% and total amount of data processed by 14%. Although the test case was from the oil and gas domain, other workflow applications could have been used, like the ones discussed in bioinformatics and astronomy domains (Section 4.5). In SciPhy workflow, users frequently run in cloud environments where the longer the workflow takes to execute, the more expensive the final costs will be. Being able to dynamically identify certain combinations of genomic sequences that will make the execution take undesirably longer and remove such combinations online are very beneficial features to SciPhy's users. Whereas in Montage workflow, users can identify certain regions of the outer space that are unlikely to contain celestial objects and remove delimited regions from the input dataset during execution. Thus, as long as users can tell which slice is not interesting, our solution supports dynamic reduction from the input data with no harm to the final results.</p><p>To the best of our knowledge, this is the first work that explores user-steered online data reduction in scientific workflows steered by ad-hoc queries and adaptive monitoring, while maintaining provenance of user interactions. The results motivate us to extend our solution and explore different aspects that can be adapted based on dataflow online data analysis. Our solution is currently dependent on the users' knowledge to identify correlations between input and output data to determine which subsets are uninteresting. We plan to address in-situ data visualization using adaptive monitoring and interactive queries results and develop recommendation models to suggest correlations based on the history stored in the wf-Database to help identifying such correlations. Other future works include: enabling users to set priorities to different slices of the data in a way that the SWMS system will process critic slices before; exploring the potential of the solution in a higher extent; and improving usability of the system by improving the system's interfaces.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Risers Fatigue Analysis Workflow.</figDesc><graphic coords="5,131.04,216.96,349.92,88.56" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Data dependency between activities.</figDesc><graphic coords="6,106.08,568.56,418.32,81.12" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Risers Fatigue Analysis workflow using the algebraic dataflow representation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Scientific domain data management showing how the data elements flowing between activities are stored as datasets linked with workflow execution data and dataflow provenance in the wf-Database.</figDesc><graphic coords="9,87.60,71.52,436.80,198.96" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. Workflow representation of a user-steered Cut operation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>(𝑚𝐴𝑑𝑑, 𝑀𝑂𝑆𝐴𝐼𝐶 ®¯, 𝑅𝑆𝑒𝑙𝑒𝑐𝑡𝑃𝑟𝑜𝑗𝑒𝑐𝑡𝑖𝑜𝑛𝑠) 𝑅𝑈𝑛𝑐𝑜𝑟𝑟𝑒𝑐𝑡𝑒𝑑𝐹𝐼𝑇𝑆 ← 𝑀𝑎𝑝(𝑚𝐽𝑃𝐸𝐺, 𝑅𝐶𝑈𝑛𝑐𝑜𝑟𝑟𝑒𝑐𝑡𝑒𝑑𝑀𝑜𝑠𝑎𝑖𝑐) 𝑅𝐸𝑥𝑡𝐷𝑖𝑓𝑓𝑒𝑟𝑒𝑛𝑐𝑒𝑠 ← 𝑅𝑒𝑑𝑢𝑐𝑒(𝑚𝑂𝑣𝑒𝑟𝑙𝑎𝑝𝑠, 𝑀𝑂𝑆𝐴𝐼𝐶 ®¯, 𝑅𝑃𝑟𝑜𝑗𝑒𝑐𝑡𝑖𝑜𝑛)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 .</head><label>6</label><figDesc>Figure 6. Real-world scientific workflows modeled using the data-centric workflow algebra.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 .</head><label>7</label><figDesc>Figure 7. User-steered Cut in an excerpt of Montage workflow. The dataset 𝑹 𝒑𝒓𝒐𝒋𝒆𝒄𝒕𝒊𝒐𝒏 is divided into subsets 𝑷 𝒑𝒓𝒐𝒋𝒆𝒄𝒕𝒊𝒐𝒏 and 𝑺 𝒑𝒓𝒐𝒋𝒆𝒄𝒕𝒊𝒐𝒏 . A slice is cut off from 𝑹 𝒑𝒓𝒐𝒋𝒆𝒄𝒕𝒊𝒐𝒏 transforming it into 𝑹′ 𝒑𝒓𝒐𝒋𝒆𝒄𝒕𝒊𝒐𝒏 using criteria C.</figDesc><graphic coords="17,80.64,71.52,436.08,228.24" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 8 .</head><label>8</label><figDesc>Figure 8. Sequence diagram showing what happens in a user-steered data reduction.</figDesc><graphic coords="20,97.44,71.52,417.12,223.92" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 9 .</head><label>9</label><figDesc>Figure 9. Steps executed by each thread within a time interval.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 10 .</head><label>10</label><figDesc>Figure 10. Extended PROV-Wf entity-relationship diagram to accommodate modified tasks and monitoring.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 12 .</head><label>12</label><figDesc>Figure 12. Steer module command line interface.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 13 .</head><label>13</label><figDesc>Figure 13. Monitor module command line interface.</figDesc><graphic coords="25,84.48,453.12,441.60,207.84" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 11 .</head><label>11</label><figDesc>Figure 11. Using MySQL Workbench to query wf-Database at workflow runtime</figDesc><graphic coords="25,157.92,71.28,295.68,190.08" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 14 .</head><label>14</label><figDesc>Figure 14. Use case plot to analyze impact of user steering comparing Wind Speed (input) with Fatigue life.</figDesc><graphic coords="27,85.20,547.68,441.12,160.56" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 15 .</head><label>15</label><figDesc>Figure 15. Total data elements, gigabytes, and time consumed by workflow activity running with no user steering.</figDesc><graphic coords="30,150.24,71.52,311.04,150.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Figure 16 .Figure 17 .</head><label>1617</label><figDesc>Figure 16. Reduced resources by activity caused by each user-steered reduction 𝒒 𝒊 .</figDesc><graphic coords="30,138.72,330.96,334.08,196.08" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Figure 18 .</head><label>18</label><figDesc>Figure 18. Results of adaptive monitoring overhead.</figDesc><graphic coords="31,84.96,316.08,441.84,178.08" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 . Provenance and domain data linked to execution data.</head><label>2</label><figDesc></figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>User_Query table description.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 . Monitoring_Query table description. Column name Description monitoring_id</head><label>4</label><figDesc></figDesc><table /><note><p><p>Auto increment identifier interval</p>Interval time (in seconds) between each monitoring query (𝑑 1 )</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 . Monitoring_Query_Result table description. Column name Description</head><label>5</label><figDesc></figDesc><table /><note><p><p>monitoring_result_id Auto increment identifier monitoring_id Relationship with the monitoring query that generated this result monitoring_values Results of the monitoring_query result_type Data type of the result values of both queries. Currently, "Integer", "Double", "Array[Integer]", and "Array[Double]" 1.</p>Execute the monitoring query 𝒎𝒒 𝒊 2.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 . Summary of the user-steered reductions (𝒒 𝟏 -𝒒 𝟓 ) with their user-defined slices.</head><label>6</label><figDesc></figDesc><table><row><cell>Interaction</cell><cell>Issued time (s)</cell><cell>Slice query</cell></row><row><cell>𝑞 &amp;</cell><cell>190</cell><cell>wind_speed &lt; 16</cell></row><row><cell>𝑞 "</cell><cell>310</cell><cell>wind_speed &lt; 25</cell></row><row><cell>𝑞 m</cell><cell>370</cell><cell>wind_speed &lt; 30</cell></row><row><cell>𝑞 Ó</cell><cell>430</cell><cell>wind_speed &lt; 34.5</cell></row><row><cell>𝑞 c</cell><cell>520</cell><cell>wind_speed &lt; 35.5</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>Our target-user profile is a computational scientist, who is expert in domain-specific systems (e.g., bioinformatician, computational physicist, or engineer).</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>This work was partially funded by <rs type="funder">CNPq</rs>, <rs type="funder">FAPERJ</rs> and <rs type="funder">Inria</rs> (<rs type="projectName">SciDISC</rs> project), <rs type="funder">EU</rs> <rs type="programName">H2020 Programme</rs> and <rs type="funder">MCTI/RNP-Brazil</rs> (<rs type="grantName">HPC4E</rs> grant no. <rs type="grantNumber">689772</rs>), and performed (for <rs type="person">P. Valduriez</rs>) in the context of the Computational Biology Institute (www.ibc-montpellier.fr). The experiments were carried out using Grid'5000 (https://www.grid5000.fr). The authors would like to thank <rs type="person">Andres Codas</rs>, <rs type="person">Juliana Jansen</rs>, and <rs type="person">Heloisa Candello</rs> from <rs type="affiliation">IBM Research</rs> for their help on how the system is being used by scientists for data reduction.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_skdNKWS">
					<orgName type="project" subtype="full">SciDISC</orgName>
				</org>
				<org type="funding" xml:id="_ZnkSUjA">
					<orgName type="program" subtype="full">H2020 Programme</orgName>
				</org>
				<org type="funding" xml:id="_82X5bAp">
					<idno type="grant-number">689772</idno>
					<orgName type="grant-name">HPC4E</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m">Peter&quot; --conf=SC</title>
		<imprint/>
	</monogr>
	<note>Steer --user=. Next workflow interactions will be issued by user Peter</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m">Steer --cut --dataset=&quot;opreprocessing&quot; --criteria=&quot;wind_speed &lt; 12.0 and wave_freq &gt; 2.0</title>
		<imprint/>
	</monogr>
	<note>&quot; 177 data elements were cut off from OPREPROCESSING dataset</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m">Steer --cut --dataset=&quot;opreprocessing&quot; --criteria=&quot;wind_speed &lt; 11.3 and wave_freq &gt; 1.8</title>
		<imprint/>
	</monogr>
	<note>&quot; 55 data elements were cut off from OPREPROCESSING dataset</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Many-task computing for grids and supercomputers</title>
		<author>
			<persName><forename type="first">I</forename><surname>Raicu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">T</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">MTAGS</title>
		<imprint>
			<biblScope unit="page" from="1" to="11" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Data-centric iteration in dynamic workflows</title>
		<author>
			<persName><forename type="first">J</forename><surname>Dias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Guerra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Rochinha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L G A</forename><surname>Coutinho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Valduriez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mattoso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">FGCS</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">C</biblScope>
			<biblScope unit="page" from="114" to="126" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Workflows and e-Science: an overview of workflow system features and capabilities</title>
		<author>
			<persName><forename type="first">E</forename><surname>Deelman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Gannon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shields</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">FGCS</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="528" to="540" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Dynamic steering of HPC scientific workflows: A survey</title>
		<author>
			<persName><forename type="first">M</forename><surname>Mattoso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">A C S</forename><surname>Ocaña</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Ogasawara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Costa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Horta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>De Oliveira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">FGCS</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="page" from="100" to="113" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Provenance and scientific workflows: challenges and opportunities</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">B</forename><surname>Davidson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Freire</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIGMOD</title>
		<imprint>
			<biblScope unit="page" from="1345" to="1350" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Capturing and querying workflow runtime provenance with PROV: a practical approach</title>
		<author>
			<persName><forename type="first">F</forename><surname>Costa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Oliveira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ocaña</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Ogasawara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mattoso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EDBT/ICDT Workshops</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="282" to="289" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Analyzing related raw data files through dataflows</title>
		<author>
			<persName><forename type="first">V</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Oliveira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Valduriez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mattoso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CCPE</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2528" to="2545" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Parallel execution of workflows driven by a distributed database management system</title>
		<author>
			<persName><forename type="first">R</forename><surname>Souza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName><surname>Oliveira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A B</forename><surname>Valduriez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lima</surname></persName>
		</author>
		<author>
			<persName><surname>Mattoso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Poster in IEEE/ACM Supercomputing</title>
		<imprint>
			<biblScope unit="page" from="1" to="3" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Online input data reduction in scientific workflows</title>
		<author>
			<persName><forename type="first">R</forename><surname>Souza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L G A</forename><surname>Coutinho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Valduriez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mattoso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">WORKS</title>
		<imprint>
			<biblScope unit="page" from="44" to="53" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">SciPhy: A Cloud-Based Workflow for Phylogenetic Analysis of Drug Targets in Protozoan Genomes</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">A C S</forename><surname>Ocaña</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Oliveira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Ogasawara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M R</forename><surname>Dávila</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A B</forename><surname>Lima</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mattoso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Bioinformatics and Computational Biology</title>
		<imprint>
			<biblScope unit="page" from="66" to="70" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Montage: a grid portal and software toolkit for science-grade astronomical image mosaicking</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Jacob</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Katz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">B</forename><surname>Berriman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Good</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Laity</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Deelman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kesselman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-H</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computational Science and Engineering (IJCSE)</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="73" to="87" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Recommended practice: riser fatigue</title>
		<idno>DNV-RP-F204</idno>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
		<respStmt>
			<orgName>Det Norse Veritas</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Raw data queries during data-intensive parallel workflow execution</title>
		<author>
			<persName><forename type="first">V</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leite</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Camata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Oliveira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L G A</forename><surname>Coutinho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Valduriez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mattoso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">FGCS</title>
		<imprint>
			<biblScope unit="volume">75</biblScope>
			<biblScope unit="page" from="402" to="422" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">An algebraic approach for data-centric scientific workflows</title>
		<author>
			<persName><forename type="first">E</forename><surname>Ogasawara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Oliveira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Porto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Valduriez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mattoso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PVLDB</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1328" to="1339" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Swift/T: large-scale application composition via distributed-memory dataflow processing</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Wozniak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">G</forename><surname>Armstrong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wilde</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Katz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Lusk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">T</forename><surname>Foster</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/ACM Int. Symp. Cluster, Cloud and Grid Computing</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="95" to="102" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Spark: cluster computing with working sets</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zaharia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chowdhury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Franklin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shenker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Stoica</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd USENIX conference on Hot topics in cloud computing</title>
		<meeting>the 2nd USENIX conference on Hot topics in cloud computing</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="10" to="17" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Logical provenance in data-oriented workflows?</title>
		<author>
			<persName><forename type="first">R</forename><surname>Ikeda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Das</forename><surname>Sarma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Widom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 IEEE International Conference on Data Engineering (ICDE 2013)</title>
		<meeting>the 2013 IEEE International Conference on Data Engineering (ICDE 2013)</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="877" to="888" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">How Much Domain Data Should Be in Provenance Databases?</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">De</forename><surname>Oliveira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mattoso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceeding of the 7th USENIX Workshop on the Theory and Practice of Provenance (TaPP 15)</title>
		<meeting>eeding of the 7th USENIX Workshop on the Theory and Practice of Provenance (TaPP 15)</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Integrating domain-data steering with code-profiling tools to debug data-intensive workflows</title>
		<author>
			<persName><forename type="first">V</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Neves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Souza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Coutinho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Oliveira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mattoso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">WORKS</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Supporting dynamic parameter sweep in adaptive and user-steered workflow</title>
		<author>
			<persName><forename type="first">J</forename><surname>Dias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Ogasawara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Oliveira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Porto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L G A</forename><surname>Coutinho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mattoso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">WORKS</title>
		<imprint>
			<biblScope unit="page" from="31" to="36" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A characterization of workflow management systems for extreme-scale applications</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">F</forename><surname>Da Silva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Filgueira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Pietri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sakellariou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Deelman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">FGCS</title>
		<imprint>
			<biblScope unit="volume">75</biblScope>
			<biblScope unit="page" from="228" to="238" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Characterizing and profiling scientific workflows</title>
		<author>
			<persName><forename type="first">G</forename><surname>Juve</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Chervenak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Deelman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bharathi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Vahi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">FGCS</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="682" to="692" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Principles of distributed database systems</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">T</forename><surname>Özsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Valduriez</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
			<publisher>Springer</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
	<note>3 ed</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<author>
			<persName><forename type="first">L</forename><surname>Moreau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Missier</surname></persName>
		</author>
		<ptr target="http://www.w3.org/TR/prov-dmAccessed:1" />
		<title level="m">PROV-DM: the PROV data model</title>
		<imprint>
			<date type="published" when="2013">Aug 2016. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Usersteering of HPC workflows: state-of-the-art and future directions</title>
		<author>
			<persName><forename type="first">M</forename><surname>Mattoso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ocaña</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Horta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Ogasawara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Oliveira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Costa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Araújo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd ACM SIGMOD Workshop on Scalable Workflow Execution Engines and Technologies (SWEET)</title>
		<meeting>the 2nd ACM SIGMOD Workshop on Scalable Workflow Execution Engines and Technologies (SWEET)</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">WorkWays: interacting with scientific workflows</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">A</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Abramson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kiporous</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Janke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Galloway</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Gateway Computing Environments Workshop</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="21" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Tavaxy: Integrating Taverna and Galaxy workflows with cloud computing support</title>
		<author>
			<persName><forename type="first">M</forename><surname>Abouelhoda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Issa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">77</biblScope>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">OpenMOLE, a workflow engine specifically tailored for the distributed exploration of simulation models</title>
		<author>
			<persName><forename type="first">R</forename><surname>Reuillon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Leclaire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Rey-Coyrehourcq</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">FGCS</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1981" to="1990" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">FireWorks: a dynamic workflow system designed for high-throughput applications</title>
		<author>
			<persName><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">P</forename><surname>Ong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Medasani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kocher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Brafman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Petretto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G.-M</forename><surname>Rignanese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CCPE</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">17</biblScope>
			<biblScope unit="page" from="5037" to="5059" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Achieving Self-Management via Utility Functions</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kephart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Das</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Internet Computing</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="40" to="48" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Nimrod/K: Towards massively parallel dynamic grid workflows</title>
		<author>
			<persName><forename type="first">D</forename><surname>Abramson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Enticott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Altinas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Supercomputing</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="1" to="24" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Wings: intelligent workflowbased design of computational experiments</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Gil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Ratnakar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Gonzalez-Calero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Groth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Moody</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Deelman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Intellig. Sys</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="62" to="72" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Computing just what you need: online data analysis and reduction at extreme scales</title>
		<author>
			<persName><forename type="first">I</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ainsworth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Allen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bessac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Cappello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Y</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Constantinescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">E</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Di</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Euro-Par</title>
		<imprint>
			<biblScope unit="page" from="3" to="19" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Toward an end-to-end framework for modeling, monitoring and anomaly detection for scientific workflows</title>
		<author>
			<persName><forename type="first">A</forename><surname>Mandal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ruth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Baldin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Krol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Juve</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mayani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">Ferreira</forename><surname>Da Silva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Deelman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Meredith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IPDPSW</title>
		<imprint>
			<biblScope unit="page" from="1370" to="1379" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Pegasus, a workflow management system for science automation</title>
		<author>
			<persName><forename type="first">E</forename><surname>Deelman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Vahi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Juve</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rynge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Callaghan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Maechling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mayani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ferreira Da Silva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">FGCS</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">C</biblScope>
			<biblScope unit="page" from="17" to="35" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
