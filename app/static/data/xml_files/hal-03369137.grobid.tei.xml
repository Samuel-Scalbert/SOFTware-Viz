<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Enhancing Speech Privacy with Slicing</title>
				<funder ref="#_wzDewmw">
					<orgName type="full">French National Research Agency</orgName>
				</funder>
				<funder ref="#_KdaCKTG">
					<orgName type="full">European Union</orgName>
				</funder>
				<funder ref="#_gKSARzN">
					<orgName type="full">Agence Nationale de la Recherche</orgName>
					<orgName type="abbreviated">ANR</orgName>
				</funder>
				<funder>
					<orgName type="full">CNRS</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Mohamed</forename><surname>Maouche</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">UMR 9189</orgName>
								<orgName type="institution" key="instit1">Univ. Lille</orgName>
								<orgName type="institution" key="instit2">Inria</orgName>
								<orgName type="institution" key="instit3">CNRS</orgName>
								<orgName type="institution" key="instit4">Centrale Lille</orgName>
								<address>
									<addrLine>-CRIStAL</addrLine>
									<postCode>F-59000</postCode>
									<settlement>Lille</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Brij</forename><surname>Mohan</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Lal</forename><surname>Srivastava</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">UMR 9189</orgName>
								<orgName type="institution" key="instit1">Univ. Lille</orgName>
								<orgName type="institution" key="instit2">Inria</orgName>
								<orgName type="institution" key="instit3">CNRS</orgName>
								<orgName type="institution" key="instit4">Centrale Lille</orgName>
								<address>
									<addrLine>-CRIStAL</addrLine>
									<postCode>F-59000</postCode>
									<settlement>Lille</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Nathalie</forename><surname>Vauquier</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">UMR 9189</orgName>
								<orgName type="institution" key="instit1">Univ. Lille</orgName>
								<orgName type="institution" key="instit2">Inria</orgName>
								<orgName type="institution" key="instit3">CNRS</orgName>
								<orgName type="institution" key="instit4">Centrale Lille</orgName>
								<address>
									<addrLine>-CRIStAL</addrLine>
									<postCode>F-59000</postCode>
									<settlement>Lille</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Aurélien</forename><surname>Bellet</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">UMR 9189</orgName>
								<orgName type="institution" key="instit1">Univ. Lille</orgName>
								<orgName type="institution" key="instit2">Inria</orgName>
								<orgName type="institution" key="instit3">CNRS</orgName>
								<orgName type="institution" key="instit4">Centrale Lille</orgName>
								<address>
									<addrLine>-CRIStAL</addrLine>
									<postCode>F-59000</postCode>
									<settlement>Lille</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Marc</forename><surname>Tommasi</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">UMR 9189</orgName>
								<orgName type="institution" key="instit1">Univ. Lille</orgName>
								<orgName type="institution" key="instit2">Inria</orgName>
								<orgName type="institution" key="instit3">CNRS</orgName>
								<orgName type="institution" key="instit4">Centrale Lille</orgName>
								<address>
									<addrLine>-CRIStAL</addrLine>
									<postCode>F-59000</postCode>
									<settlement>Lille</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Emmanuel</forename><surname>Vincent</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">Université de Lorraine</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
								<orgName type="institution" key="instit3">Inria</orgName>
								<orgName type="institution" key="instit4">LORIA</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Enhancing Speech Privacy with Slicing</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">0BE440665387A4FEE94A1D08AA94FD1D</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-04-12T14:50+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>anonymization</term>
					<term>speaker verification</term>
					<term>automatic speech recognition</term>
					<term>segmentation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Privacy preservation calls for anonymization methods which hide the speaker's identity in speech signals while minimizing the impact on downstream tasks such as automatic speech recognition (ASR) training or decoding. In the VoicePrivacy 2020 Challenge, voice anonymization methods have been proposed to transform speech utterances in a way that preserves their verbal and prosodic contents while reducing the accuracy of a speaker verification system. In this paper, we propose to further increase the privacy achieved by such methods by segmenting the utterances into shorter slices. We show that our approach has two major impacts on privacy. First, it reduces the accuracy of speaker verification with respect to unsegmented utterances. Second, it also reduces the amount of personal information that can be extracted from the verbal content, in a way that cannot easily be reversed by an attacker. We also show that it is possible to train an ASR system from anonymized speech slices with negligible impact on the word error rate.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>With the increasing popularity of smart devices, more users have access to voice-based interfaces. The underlying technologies, especially automatic speech recognition (ASR), are often trained on speech data collected from the users to improve performance and adapt to new domains. The collection and exploitation of this data raises privacy threats. Indeed, speech carries personal or sensitive information about the speaker (e.g., gender, age, emotion) <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref> and it is a biometric characteristic that can be used to recognize the speaker through, e.g., ivector <ref type="bibr" target="#b2">[3]</ref> or x-vector <ref type="bibr" target="#b3">[4]</ref> based speaker verification. To address this privacy issue, various voice anonymization 1 methods have been proposed in the literature. These methods, which rely on simple feature transformation <ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref>, feature perburbation <ref type="bibr" target="#b8">[9]</ref>, Gaussian mixture model based voice conversion <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11]</ref>, or neural network based voice conversion <ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref>, aim to transform speech signals in a way that preserves all content except features related to the speaker identity, thereby making it hard for an attacker to re-identify the speaker.</p><p>Besides speech signals, ASR system training also requires the corresponding text transcripts, irrespective of whether the speech signals have been subject to voice anonymization or not. These transcripts can contain personal information about the speaker too. Text sanitization methods, which redact or replace sensitive words in the text, can mitigate this issue for textonly data <ref type="bibr" target="#b14">[15]</ref><ref type="bibr" target="#b15">[16]</ref><ref type="bibr" target="#b16">[17]</ref><ref type="bibr" target="#b17">[18]</ref>. Unfortunately, word replacement is unusable 1 In the legal community, the term "anonymization" means that this goal has been achieved. Following the VoicePrivacy 2020 Challenge <ref type="bibr" target="#b4">[5]</ref>, we use it to refer to the task, even when the method has failed.</p><p>for ASR system training since it breaks the correspondence between the transcripts and the verbal contents of speech, while word redaction often fails to detect and remove some sensitive words. As a result, the transcripts (or the verbal content of speech) could be exploited by an attacker to break the protection offered by voice anonymization.</p><p>The method we introduce in this paper follows a different path. We propose to segment every speech utterance into shorter slices after it has been anonymized. In this way, we reduce the amount of speech available to the attacker in each slice, which is expected to lower the risk of speaker re-identification with respect to unsegmented utterances. On top of that, the amount of personal information that can be extracted from the transcript of each slice is also reduced, since it becomes isolated from its context. We quantify the risk of speaker re-identification, and the risk that an attacker could reverse the slicing procedure by reassembling successive speech signals or text transcripts together. Most importantly, we also evaluate the impact of slicing on the utility of the data for ASR acoustic model training. Our experiments are conducted on LibriSpeech <ref type="bibr" target="#b18">[19]</ref> and follow the VoicePrivacy 2020 Challenge setup <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b19">20]</ref> with a stronger attacker. In particular, we use the x-vector based voice conversion baseline <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b21">22]</ref> of the Challenge for anonymization, not only for reproducibility purposes but also because it is representative of modern neural network based voice anonymization methods and it still offers one of the best privacy/utility trade-offs today.</p><p>While slicing may be seen as a simple method, it has not been studied in the context of privacy so far. Previous studies on the impact of utterance duration on speaker verification performance focused on clear (non-anonymized) utterances and did not evaluate the utility for ASR <ref type="bibr" target="#b22">[23]</ref><ref type="bibr" target="#b23">[24]</ref><ref type="bibr" target="#b24">[25]</ref>. Our study is the first one that tackles privacy, utility and reassembling together. Our results highlight a sweet spot in the choice of slice length for which our method provides a large increase in privacy (larger than the one provided by voice anonymization itself) with no loss of utility. In addition, we show the difficulty for an attacker to reassemble the utterance from short slices of speech or text.</p><p>The structure of the paper is as follows. We describe the threat model in Section 2 and introduce the slicing method in Section 3. Section 4 reports the evaluation on real data in terms of both privacy and utility. We present our study on the reversibility of the slicing in Section 5. We conclude in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Threat Model</head><p>The attack scenario is depicted in Fig. <ref type="figure" target="#fig_0">1</ref>. Speakers process their voice through an anonymization method. This anonymization step takes as input one or more private speech utterances along with some configuration parameters, and outputs a new speech signal. The transformed utterances from one or more speakers form a public speech dataset that is processed by a third-party Given unprocessed or anonymized utterances from a known speaker, an attacker attempts to find which anonymized utterances in the public dataset are spoken by this speaker <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b26">27]</ref>. Formally, an attacker has access to two sets of utterances: A (enrollment/found data) and B (trial/public speech), but knows the corresponding speakers in A only. The attacker designs a linkage function LF (a, b) that outputs a score for any a ∈ A and b ∈ B. Typically, this score is a similarity obtained through a speaker verification system. The attacker then makes a binary decision (same vs. different speaker) based on this score. We also consider that the attacker knows the anonymization method used and can leverage it to enhance the attack.</p><p>Anonymization techniques must achieve a suitable privacy/utility trade-off. On the one hand, privacy is measured by the attacker's ability to re-identify the speaker using metrics such as equal error rate (EER) or linkability <ref type="bibr" target="#b27">[28]</ref>. On the other hand, utility is measured by the performance of the desired downstream task(s), e.g., the word error rate (WER) of an ASR system or the intelligibility for a human listener. In the following, we are interested in the utility of the data for training an ASR acoustic model, assuming that the other components of the ASR system (lexicon, language model) are available or have been trained on text-only data (see Fig. <ref type="figure" target="#fig_1">2</ref>). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Word-Level Slicing of Speech</head><p>To increase privacy beyond the level achieved by the voice anonymization methods mentioned in Section 1, we propose to cut the anonymized utterances into multiple, shorter slices. To ensure that the sliced transcripts match the sliced spoken content, we constrain these cuts to happen between successive words rather than in the middle of a word. At the same time, we wish the duration of the slices to be close to a target duration δ in order to control the trade-off between privacy and utility. This is achieved by force-aligning each original (unsegmented) transcript with the corresponding utterance. For a given utterance u and transcript w = w1 . . . wn, alignment yields two series of timestamps (t s k ) 1≤k≤n and (t e k ) 1≤k≤n where [t s k , t e k ] is the time interval when word w k has been uttered in u. To create the first slice, we start from the first word and include the following words one by one until we reach a number k such that the duration becomes at least δ. Besides the words, we keep the silence between them, as well as the silence before the first word and after the last word. We then start again from the (k + 1)-th word to create the second slice, and so on. The final segment (if any) whose duration is shorter than δ is discarded. See Algorithm 1 for details.</p><p>Algorithm 1 Word-level slicing method. A ← Align(u, w)</p><formula xml:id="formula_0">4:</formula><p>tprv ← 0 5:</p><formula xml:id="formula_1">kprv ← 0 6:</formula><p>for k in range(|w|) do 7: return slices 20: end function</p><formula xml:id="formula_2">t s k , t e k ← A[k] # w k is uttered in [t s k ,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Utility and Privacy Evaluation</head><p>In this section, we evaluate how the duration of the slices impacts the privacy/utility trade-off.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experimental Setup</head><p>Voice anonymization: We use the first baseline of the VoicePrivacy 2020 Challenge <ref type="bibr" target="#b4">[5]</ref> as the voice anonymization method. This method extracts pitch, bottleneck, and (source) x-vector features from the input speech. It then re-synthesizes a speech signal using the original pitch and bottleneck features and a new target x-vector generated from a public pool of x-vectors using one of several possible strategies. In the following, we do not use the default strategy reported in <ref type="bibr" target="#b4">[5]</ref>. Instead, we choose the so-called dense strategy with random gender, which was reported to be the most successful in <ref type="bibr" target="#b21">[22]</ref>. Data which have not been anonymized are referred to as clear data.</p><p>Slicing: Slicing is performed using forced-alignments obtained using the pretrained model Gentle (https:// lowerquality.com/gentle/).</p><p>Privacy metric: The attacker assesses the speaker similarity between an enrollment and a trial utterance using the probabilistic linear discriminant analysis (PLDA) score between their x-vectors. Privacy is evaluated via the linkability D sys ↔ , which measures the non-overlap between the distributions of sameand different-speaker scores <ref type="bibr" target="#b27">[28]</ref>. Lower linkability means higher privacy. We assume a semi-informed attacker who knows the anonymization method (but not the mapping from source to target x-vectors) and uses that knowledge to anonymize the enrollment data and the training data for the x-vector and PLDA models <ref type="bibr" target="#b26">[27]</ref>. In contrast to the VoicePrivacy 2020 Challenge where all training utterances of a given speaker are mapped to the same target x-vector, our attacker maps each training utterance to a different target. <ref type="foot" target="#foot_0">2</ref> This greatly increases the attacker's strength and highlights the limited privacy offered by voice anonymization alone, with linkability jumping from 0.18 in [22, Fig. <ref type="figure" target="#fig_2">11</ref> right] to 0.63 here. The x-vector and PLDA models are trained and tested using the Kaldi <ref type="bibr" target="#b29">[30]</ref> recipe in <ref type="bibr" target="#b4">[5]</ref>, except that the enrollment and trial data are sliced.</p><p>ASR system and utility metric: To evaluate the utility of sliced utterances for ASR acoustic model training, we use the state-of-the-art Kaldi <ref type="bibr" target="#b29">[30]</ref> ASR recipe for LibriSpeech involving a factorized time delay neural network (TDNN-F) acoustic model and a 3-gram language model. The recipe is identical to <ref type="bibr" target="#b4">[5]</ref>, except that we train it on sliced utterances and test it on unsegmented utterances. We report the resulting WER.</p><p>Datasets: The experiments are conducted on LibriSpeech <ref type="bibr" target="#b18">[19]</ref>. The x-vector and PLDA models and the ASR system are trained on the train-clean-360 set (∼1k speakers, ∼100k utterances and 360 h of speech). Part of the test-clean set (40 speakers, 1,496 utterances) forms the trial/public data. The remaining part (29 speakers, 438 utterances) forms the enrollment/found data. This is the established VoicePrivacy 2020 setup <ref type="bibr" target="#b4">[5]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Effect of Slicing on Utility</head><p>We first explore which utterance durations are suitable for training an ASR acoustic model. Table <ref type="table" target="#tab_1">1</ref> reports the WERs achieved in four cases, depending on whether the training data has been anonymized or not before slicing and whether the test data has been anonymized or not. In each case, we report the results achieved with the original training utterances and with different slicing durations. We notice an increase in the WER when decoding anonymized data with an ASR acoustic model trained on clear data and vice-versa, which can be attributed to a training/test mismatch. Nevertheless, the WER obtained when training and testing on unsegmented anonymized data (4.86%) is similar to training and testing on unsegmented clear data (4.26%). As for the effect of slicing itself, we notice that, for clear training data, 1.5 s is the shortest possible duration below which the WER degrades a lot. With anonymized training data, the duration can be shortened to 1 s only when decoding anonymized speech. The resulting WER (4.92%) is statistically equivalent to training on unsegmented anonymized data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Effect of Slicing on Privacy</head><p>In terms of privacy, we present in Fig. <ref type="figure" target="#fig_3">3</ref> the linkability achieved with utterances of different durations. We consider the setting where the attacker aims to re-identify speakers in the trial/public data, hence the focus is now on test data (instead of training data). The purple and orange curves are obtained by shortening the utterances to a fixed duration (irrespective of word boundaries). The results on clear data illustrate the positive impact of shorter utterances on linkability, especially for durations shorter than 1 s. Unfortunately, our utility experiment demonstrated that utterances shorter than 1 s are too short to train an ASR system. Also, for durations longer than 1 s the level of privacy offered by shortening alone is insufficient. For this reason, shortening must be used together with anonymization: this combination yields a high level of privacy for δ = 1 s.</p><p>In addition, we show the results obtained with word-level slicing (Algorithm 1) and with unsegmented anonymized utterances. Word-level slicing achieves consistent results with shortening to a fixed duration of 1 s or 1.5 s. We observe that the word-level constraint, which is desirable for ASR training, does not come at the cost of a privacy loss. The linkability achieved when slicing anonymized utterances with δ = 1 s decreases to 0.14, compared to 0.63 before slicing. <ref type="foot" target="#foot_1">3</ref>To sum up, the results of Sections 4.2-4.3 show that slicing anonymized data with δ = 1 s greatly decreases the linkability while maintaining the utility for ASR training. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Reversibility of the Slicing</head><p>We assess the risk that an attacker manages to reverse the slicing and reassemble speech slices or text transcripts together. We focus on the task of linking two successive slices, since an attacker who performs poorly on this task is unlikely to reassemble entire utterances. Due to the novelty of slicing for privacy, there exists no reassembling method which we can compare to.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Text Successiveness</head><p>Regarding text, we design an attacker that leverages a language model (LM) to construct a "text successiveness score" to be used as linkage function, similarly to the speaker verification attack described in Section 2. The LM estimates the probability P (w) of any sentence w. The attacker uses P to compute a score function SF (w, v) between two transcripts w and v. The higher the score, the higher the chance that the transcripts are successive. In our experiments, we used a 3-gram LM P (w) where wi:j = wiwi+1 . . . wj and wpwq denotes concatenation between words/sentences. To restrict our attention to the terms involving both w and v, we define the score function as SF (w, v) = P (wn-1wnv1v2).</p><p>(</p><formula xml:id="formula_3">)<label>2</label></formula><p>To retrieve the successor of a given slice w in the public dataset, the attacker computes the scores SF (w, v) for all other slices v in the dataset and sorts them in decreasing order. The success of the attack can be quantified via the rank r(w) of the correct successor. We consider the following ranking metrics: (1) Average normalized rank: mean of r(w) over all w, divided by the maximum possible rank (that is the number of slices minus one); (2) Median normalized rank: median of r(w) divided by the maximum possible rank; (3) Precision at top-1: how often the slice with top score is the successor; (4) Precision at top-10%: how often the successor belongs to the top-10% scores. In addition to the LibriSpeech test set, which may be more easily attacked due to speakers reading text from distinct books including specific words like character names, we also consider the Mozilla Common Voice test set. In the latter case, we slice the transcripts into 3-word slices (the average number of words per second is 2.7) and we retrain the LM.</p><p>Table <ref type="table" target="#tab_2">2</ref> presents the results. We notice that the correct successive slice usually has large rank (27%-32% normalized rank in average and 16%-23% median rank) meaning that thousands of wrong successive slices have a better score. We also see that the correct slice almost never ranks first (less than 3% of the cases), and rarely in the top-10% (only one third of the cases).  non-successive different-speaker pairs. For δ = 1.5, we obtain on average 232 successive pairs, 5, 378 non-successive samespeaker pairs, and 125, 796 non-successive different-speaker pairs. We observe that, even though the overall accuracy of the classifiers is 80%, the vast majority of correct classifications are for the easier, non-successive different-speaker class. We consider the same ranking metrics as in Section 5.1. The results given in Table <ref type="table" target="#tab_3">3</ref> show that for δ = 1, the average rank of the correct slice is 268 (top-43%), with a median of 158 (top-25%). Furthermore, the top-1 precision is again lower than 2%. Overall, these results show that it is very difficult for an attacker to consistently find the correct successive slice, and thus it is even harder to reassemble entire utterances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>We provided the first study on slicing speech utterances into shorter segments in the context of speech anonymization. Combining our slicing approach with state-of-the-art x-vector based anonymization methods, we showed that slices of 1 s of speech can be used to train an ASR system with 4.92% WER (compared to 4.86% without slicing) while reducing speaker verification performance to 0.14 linkability (compared to 0.63 without slicing). In addition, our approach naturally helps to obfuscate sensitive information contained in the verbal content as each slice contains few words that become isolated from their context. Finally, we showed that reversing the slicing to reconstruct the original utterances is a very difficult task.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Anonymization procedure and attack model.</figDesc><graphic coords="2,57.60,73.99,226.77,104.17" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: ASR system architecture.</figDesc><graphic coords="2,68.94,485.11,204.09,79.03" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>1 :</head><label>1</label><figDesc>function SLICE(u: speech signal, w: transcript, δ: target (minimum) duration)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Linkability achieved by shortening utterances to a fixed duration (purple and yellow curves), by word-level slicing (short horizontal bars) and without slicing (long horizontal bar). The horizontal bar of a given setup is positioned by the set's overall linkability and spans the mean and standard deviation of the utterance durations.</figDesc><graphic coords="3,317.25,391.31,217.70,163.28" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>≈</head><label></label><figDesc>P (w1)P (w2 | w1) len(w) k=3 P (w k | w k-2:k-1 ),(1)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>WER (%) achieved on (unsegmented) test data when training the ASR acoustic model on sliced data.</figDesc><table><row><cell>Training Data</cell><cell>Slicing</cell><cell cols="2">Test Data Clear Anonymized</cell></row><row><cell></cell><cell>None</cell><cell>4.26</cell><cell>7.56</cell></row><row><cell>Clear</cell><cell>δ = 3 s δ = 2 s</cell><cell>4.31 4.44</cell><cell>7.36 7.58</cell></row><row><cell></cell><cell>δ = 1.5 s</cell><cell>4.66</cell><cell>8.00</cell></row><row><cell></cell><cell>δ = 1 s</cell><cell>6.11</cell><cell>11.4</cell></row><row><cell></cell><cell cols="2">δ = 0.5 s 26.59</cell><cell>35.67</cell></row><row><cell></cell><cell>None</cell><cell>10.93</cell><cell>4.86</cell></row><row><cell></cell><cell>δ = 3 s</cell><cell>13.38</cell><cell>4.90</cell></row><row><cell>Anonymized</cell><cell cols="2">δ = 2 s δ = 1.5 s 21.46 15.94</cell><cell>4.95 4.90</cell></row><row><cell></cell><cell>δ = 1 s</cell><cell>30.13</cell><cell>4.92</cell></row><row><cell></cell><cell cols="2">δ = 0.5 s 71.28</cell><cell>8.77</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Text-based successor identification performance.</figDesc><table><row><cell>Test set</cell><cell></cell><cell cols="2">LibriSpeech</cell><cell></cell><cell>Common Voice</cell></row><row><cell>Target slice duration δ (s)</cell><cell>1</cell><cell>1.5</cell><cell>3</cell><cell>4</cell><cell>/</cell></row><row><cell>Number of slices</cell><cell cols="4">14,931 11,330 5,407 5,487</cell><cell>5,292</cell></row><row><cell>Average normalized rank (%)</cell><cell>27.25</cell><cell cols="3">28.31 29.37 30.24</cell><cell>32.38</cell></row><row><cell>Median normalized rank (%)</cell><cell>16.11</cell><cell cols="3">17.87 18.81 20.92</cell><cell>23.14</cell></row><row><cell>Precision at top-1 (%)</cell><cell>1.39</cell><cell>1.41</cell><cell>2.18</cell><cell>2.56</cell><cell>0.75</cell></row><row><cell>Precision at top-10% (%)</cell><cell>40.48</cell><cell cols="3">37.8 38.36 37.84</cell><cell>33.89</cell></row><row><cell cols="2">5.2. Speech Successiveness</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note><p>We now consider the problem of linking two successive speech signals. Our approach is to concatenate the two signals and score them by the softmax score of a binary classifier trained</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Speech-based successor identification performance. We report the average results of 5 repeated experiments (the standard deviation is not reported as it is lower than 0.01) To prevent the model from learning to classify most examples as non-successive, we train it on a balanced dataset: half of the training examples are successive, one fourth are non-successive from the same speakers and one fourth are non-successive from different speakers. The training data are taken from LibriSpeech train-clean-360 sliced with δ = 1 s or (resp., δ = 1.5 s) and anonymized. To evaluate the attacker's performance, we sample five times 100 utterances from Librispeech test-clean sliced with δ = 1 s (resp., δ = 1.5 s) and anonymized. This allows us to construct on average for δ = 1 s, 528 successive pairs, 15, 142 non-successive same-speaker pairs, and 378, 821</figDesc><table><row><cell>Anonymized Librispeech Test Set</cell><cell>Number of slices</cell><cell>Average normalized rank (%)</cell><cell>Median normalized rank (%)</cell><cell>Precision at top-1 (%)</cell><cell>Precision at top-10% (%)</cell></row><row><cell>Sliced δ = 1.5 s</cell><cell>364</cell><cell>43.48</cell><cell>19.83</cell><cell>2.48</cell><cell>38.29</cell></row><row><cell>Sliced δ = 1 s</cell><cell>627</cell><cell>42.77</cell><cell>25.28</cell><cell>1.34</cell><cell>29.52</cell></row><row><cell cols="6">to distinguish successive vs. non-successive pairs. We use the</cell></row><row><cell cols="6">TDNN architecture proposed in [4] for speaker classification.</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>This newly proposed attacker has recently been selected for evaluation in the VoicePrivacy 2022 Challenge<ref type="bibr" target="#b28">[29]</ref>.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1"><p>Surprisingly, the linkability achieved on unsegmented utterances (0.63) is lower than on utterances shortened to 7 or 10 s (up to 0.79). We attribute this to the wide range of utterance durations (8.6 s average with ±5.2 s standard deviation), which increases x-vector variability.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="7.">Acknowledgements</head><p>This work was supported in part by the <rs type="funder">European Union</rs>'s <rs type="programName">Horizon 2020 Research and Innovation Program</rs> under Grant Agreements No. <rs type="grantNumber">825081</rs> <rs type="projectName">COMPRISE</rs> and No. <rs type="grantNumber">952215 TAILOR</rs> and by the <rs type="funder">French National Research Agency</rs> under project <rs type="projectName">DEEP-PRIVACY</rs> (<rs type="grantNumber">ANR-18-CE23-0018</rs>). Experiments presented in this paper were carried out using the Grid'5000 testbed, supported by a scientific interest group hosted by <rs type="funder">Inria</rs> and including <rs type="funder">CNRS</rs>, RENATER and several Universities as well as other organizations (see https://www.grid5000.fr).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_KdaCKTG">
					<idno type="grant-number">825081</idno>
					<orgName type="project" subtype="full">COMPRISE</orgName>
					<orgName type="program" subtype="full">Horizon 2020 Research and Innovation Program</orgName>
				</org>
				<org type="funded-project" xml:id="_wzDewmw">
					<idno type="grant-number">952215 TAILOR</idno>
					<orgName type="project" subtype="full">DEEP-PRIVACY</orgName>
				</org>
				<org type="funding" xml:id="_gKSARzN">
					<idno type="grant-number">ANR-18-CE23-0018</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">De-identification for privacy protection in multimedia content: A survey</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ribaric</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ariyaeeinia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Pavesic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Signal Processing: Image Communication</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="page" from="131" to="151" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Preserving privacy in speaker and speech characterisation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Nautsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jiménez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Treiber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kolberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Jasserand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Kindt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Delgado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Todisco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Hmani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mtibaa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Abdelraheem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Abad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Teixeira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Matrouf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Speech and Language</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="page" from="441" to="480" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Front-end factor analysis for speaker verification</title>
		<author>
			<persName><forename type="first">N</forename><surname>Dehak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Kenny</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Dehak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dumouchel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ouellet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="788" to="798" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">X-vectors: Robust DNN embeddings for speaker recognition</title>
		<author>
			<persName><forename type="first">D</forename><surname>Snyder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Garcia-Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="5329" to="5333" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Introducing the VoicePrivacy initiative</title>
		<author>
			<persName><forename type="first">N</forename><surname>Tomashenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">M L</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Nautsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yamagishi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Patino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-F</forename><surname>Bonastre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-G</forename><surname>Noé</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Todisco</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Interspeech</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1693" to="1697" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Reversible speaker de-identification using pre-trained transformation functions</title>
		<author>
			<persName><forename type="first">C</forename><surname>Magariños</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lopez-Otero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Docio-Fernandez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Rodriguez-Banga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Erro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Garcia-Mateo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Speech and Language</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="page" from="36" to="52" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Voicemask: Anonymize and sanitize voice input on mobile devices</title>
		<author>
			<persName><forename type="first">J</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X.-Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Deng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.11460</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Speaker anonymisation using the McAdams coefficient</title>
		<author>
			<persName><forename type="first">J</forename><surname>Patino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Tomashenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Todisco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Nautsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Evans</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1099" to="1103" />
		</imprint>
	</monogr>
	<note>Interspeech</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Voiceindistinguishability: Protecting voiceprint in privacy-preserving speech data release</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yoshikawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE International Conference on Multimedia and Expo (ICME)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Speaker deidentification via voice transformation</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Toth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Schultz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">W</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU)</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="529" to="533" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Online speaker de-identification using voice transformation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Pobar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Ipšić</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">37th International Convention on Information and Communication Technology, Electronics and Microelectronics</title>
		<meeting><address><addrLine>MIPRO</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1264" to="1267" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Convolutional neural network based speaker de-identification</title>
		<author>
			<persName><forename type="first">F</forename><surname>Bahmaninezhad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H L</forename><surname>Hansen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Odyssey</title>
		<imprint>
			<biblScope unit="page" from="255" to="260" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Speaker anonymization using x-vector and neural waveform models</title>
		<author>
			<persName><forename type="first">F</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yamagishi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Echizen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Todisco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-F</forename><surname>Bonastre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">10th ISCA Speech Synthesis Workshop (SSW)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="155" to="160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Voice privacy through x-vector and CycleGAN-based anonymization</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">P</forename><surname>Prajapati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">K</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">P</forename><surname>Amin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">A</forename><surname>Patil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Interspeech</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1684" to="1688" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Preserving privacy in spoken language databases</title>
		<author>
			<persName><forename type="first">M</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hakkani-Tür</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Tur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECML/PKDD International Workshop on Privacy and Security Issues in Data Mining</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Evaluating the state-of-theart in automatic de-identification</title>
		<author>
			<persName><forename type="first">Ö</forename><surname>Uzuner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Szolovits</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Medical Informatics Association</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="550" to="563" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Privacy guarantees for de-identifying text transformations</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">I</forename><surname>Adelani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Davody</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kleinbauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Klakow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Interspeech</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="4666" to="4670" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Sensitive data detection and classification in Spanish clinical text: Experiments with BERT</title>
		<author>
			<persName><forename type="first">A</forename><surname>García-Pablos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cuadros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">12th Language Resources and Evaluation Conference (LREC)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="4486" to="4494" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Librispeech: An ASR corpus based on public domain audio books</title>
		<author>
			<persName><forename type="first">V</forename><surname>Panayotov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="5206" to="5210" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">The VoicePrivacy 2020 Challenge: Results and findings</title>
		<author>
			<persName><forename type="first">N</forename><surname>Tomashenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Patino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">M L</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-G</forename><surname>Noé</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Nautsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yamagishi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>O'brien</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Chanclu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-F</forename><surname>Bonastre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Todisco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Maouche</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Speech and Language</title>
		<imprint>
			<biblScope unit="volume">74</biblScope>
			<biblScope unit="page">101362</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Design choices for x-vector based speaker anonymization</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">M L</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Tomashenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yamagishi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Maouche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bellet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tommasi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1713" to="1717" />
		</imprint>
	</monogr>
	<note>Interspeech</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Privacy and utility of x-vector based speaker anonymization</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">M L</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Maouche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sahidullah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bellet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tommasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Tomashenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yamagishi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Audio, Speech, and Language Processing</title>
		<imprint/>
	</monogr>
	<note>to appear</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Speaker verification with short utterances: a review of challenges, trends and opportunities</title>
		<author>
			<persName><forename type="first">A</forename><surname>Poddar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sahidullah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Saha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IET Biometrics</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="91" to="101" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Quality measures for speaker verification with short utterances</title>
	</analytic>
	<monogr>
		<title level="j">Digital Signal Processing</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="page" from="66" to="79" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">An analysis of the short utterance problem for speaker characterization</title>
		<author>
			<persName><forename type="first">I</forename><surname>Viñals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ortega</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Miguel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Lleida</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Sciences</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">18</biblScope>
			<biblScope unit="page">3697</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Towards privacy-preserving speech data publishing</title>
		<author>
			<persName><forename type="first">J</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X.-Y</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE Conference on Computer Communications (INFOCOM)</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1079" to="1087" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Evaluating voice conversion-based privacy protection against informed attackers</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">M L</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Vauquier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sahidullah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bellet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tommasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Vincent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="2802" to="2806" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A comparative study of speech anonymization metrics</title>
		<author>
			<persName><forename type="first">M</forename><surname>Maouche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">M L</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Vauquier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bellet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tommasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Vincent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Interspeech</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1708" to="1712" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<author>
			<persName><forename type="first">N</forename><surname>Tomashenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Nourtel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Champion</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Todisco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yamagishi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-F</forename><surname>Bonastre</surname></persName>
		</author>
		<ptr target="https://www.voiceprivacychallenge.org/vp2020/docs/VoicePrivacy2022EvalPlanv1.0.pdf" />
		<title level="m">The VoicePrivacy 2022 Challenge evaluation plan</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">The Kaldi speech recognition toolkit</title>
		<author>
			<persName><forename type="first">D</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ghoshal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Boulianne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Burget</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Glembek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hannemann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Motlíček</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Schwarz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Silovský</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Stemmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Veselý</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2011 IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU)</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
