<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Statistical Claim Checking: StatCheck in Action</title>
				<funder ref="#_ZqWydB5">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Oana</forename><surname>Balalau</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Inria</orgName>
								<orgName type="institution" key="instit2">Institut Polytechnique de Paris</orgName>
								<address>
									<settlement>Palaiseau</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Simon</forename><surname>Ebel</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Inria</orgName>
								<orgName type="institution" key="instit2">Institut Polytechnique de Paris</orgName>
								<address>
									<settlement>Palaiseau</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Théo</forename><surname>Galizzi</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Inria</orgName>
								<orgName type="institution" key="instit2">Institut Polytechnique de Paris</orgName>
								<address>
									<settlement>Palaiseau</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ioana</forename><surname>Manolescu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Inria</orgName>
								<orgName type="institution" key="instit2">Institut Polytechnique de Paris</orgName>
								<address>
									<settlement>Palaiseau</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Quentin</forename><surname>Massonnat</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Inria</orgName>
								<orgName type="institution" key="instit2">Institut Polytechnique de Paris</orgName>
								<address>
									<settlement>Palaiseau</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Statistical Claim Checking: StatCheck in Action</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">1AE69F1A6025CF33B3710137A0B7CD0F</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-04-12T14:49+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Fact-checking is a staple of journalists' work. As more and more important data is available in electronic format, computational factchecking, leveraging digital data sources, has been gaining interest from the journalists as well as the computer science community.</p><p>A particular class of interesting data sources are statistics, that is, numerical data compiled mostly by governments, administrations, and international organizations.</p><p>We propose to demonstrate StatCheck, a fact-checking system specialized in the French media arena. StatCheck builds upon a prior pipeline [CMT17, CMT18, DCMT19] for fact-checking statistical claims against the INSEE national statistic institute's data and reports. In collaboration with factchecking journalists from France Info (Radio France), we have revisited and improved this pipelined, and enlarged its database by an order of magnitude by adding all Eurostat statistics. StatCheck also includes two novel statistic claim extraction modules, generalizing and improving over [DCMT19]. Based on the journalists' feedback, we built a new user interface, suiting better their needs. We will showcase StatCheck on a variety of scenarios, where statistical claims made in social media are checked against our statistic data.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Professional journalism work has always involved verifying information with the help of trusted sources. In recent years, the proliferation of media in which public figures make statements, including traditional media available online, as well as social media, has lead to an explosion in the amount of content that may need to be verified in order to distinguish accurate from inaccurate, and even potentially dangerous, information.</p><p>Computational fact-checking is a growing and multidisciplinary field [CLL + 18, NCH + 21] which has lead to the creation of meeting venues such as the Conference for Truth and Trust Online<ref type="foot" target="#foot_0">1</ref> . The main tasks of a fact-checking system are: identifying the claims made in an input document, finding the relevant evidence from a reference dataset, and optionally producing an automated verdict or if not, letting an end user decide on the truthfulness of the claim. Recent systems proposed in this area include [HZA + 17, <ref type="bibr" target="#b16">KSPT20,</ref><ref type="bibr" target="#b18">PMYW18]</ref>. In a recent survey <ref type="bibr" target="#b22">[SP21]</ref>, the authors compare the performance of four statistical fact-checking systems [CWC + 20, HNM + 20, JTY + 19, KSPT20]. A statistical claim is defined as any textual claim that can be verified over a trustworthy database by performing a mathematical expression on the database cell value <ref type="bibr" target="#b22">[SP21]</ref>. The four systems take in input a claim and a table, and should output the truth value of the claim.</p><p>We propose to demonstrate StatCheck, a fact-checking system specialized in the French media arena. Differently from the abovementioned systems, StatCheck also includes a claim detection step, while assuming that an end user will decide if a claim is true or not based on the evidence retrieved. Another specificity is our focus on checking statistical claims, with the help of large public statistic databases (specifically, INSEE and Eurostat); the nature and organization of these databases raises specific challenges when searching for the statistic most appropriate to check a given claim.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Architecture and outline</head><p>The overall architecture of our platform is presented in Figure <ref type="figure" target="#fig_0">1</ref>, based on which we present the outline of this paper. We first acquire statistic data from reference sources, currently INSEE and Eurostat; we describe how data is stored in Section 2. In Section 3, we show how users's keyword search queries can be answered against the stored data. To help journalists even more in their work to fact-check claims made in the public space, also ingest them in our system (by subscribing to media sources, or allowing users to upload their own content), and apply Natural Language Processing to identify, from their textual content, statistical claims. This is described in Section 4. We then describe the proposed demonstration scenarios (Section 5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">STATISTIC DATA ACQUISITION AND STORAGE</head><p>INSEE publishes each statistic report as an HTML page that links to statistic tables, which may be in Excel (the most frequent case) or in HTML. The tables are not relational. On one hand, they have human-understandable header cells not only for each column (as is the case for a relational table), but also for each line. From this perspective, a statistic file resembles more a multidimensional aggregate query result. On the other hand, many tables feature hierarchical (nested) headers: for instance, a header cell corresponding to "Paris (75)" may appear as a child of another header cell corresponding to "Île-de-France". To capture such structure, the data is modeled as an RDF graph, as illustrated in Figure <ref type="figure" target="#fig_1">2</ref>; each cell becomes an RDF URI, connected to its closest line header (blue arrows), and closest column header (red arrows). The black arrows are triples relating column header cells to their parent header cell. While revisiting the platform, we re-crawled the INSEE Web site up to May 2022, leading to 60,002 Excel files and 58,849 HTML files. We then extract statistic tables from those files and convert them into RDF graphs, accounting for 7,362,538,629 RDF triples, including 22,366,376 row or column header cells. To store this large amount of data, we use JENA TDB2<ref type="foot" target="#foot_2">2</ref> ; loading the complete set of INSEE published statistics took around 29 hours. Subsequently, we incrementally re-crawl the Web site each night to retrieve and ingest the possible studies published every day.  As part of our collaboration with RadioFrance, we recently added a new corpus of reference statistics, namely Eurostat: (𝑖) 6,803 data tables; these are two-dimensional tables with line headers, row headers, and no nesting in the headers. Each header is a concatenation of a set of dimension values, e.g., EU15_NO as a line header corresponds to the value of metric "Union européenne -15 pays (1995-2004) et Norvège" (𝑖𝑖) 580 dictionaries that map 243,083 statistical concepts codes, such as BLA in the above, into naturallanguage descriptions. The data files range from 2 lines, to 37 million lines (this largest file holds information about farmers and their agricultural properties across Europe). Together, the Eurostat data files total 414.908.786 lines. As the tables of Eurostat are normalized and flat, they do not require a semi-structured (graph) storage format. Due to their relatively high number, instead of storing them in a relational database, we keep them as plain files, and developed specialized search techniques for them, as we explain below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">STATISTIC SEARCH</head><p>Given a keyword query 𝑄 = {𝑘 1 , 𝑘 2 , . . . , 𝑘 𝑛 }, our purpose is to return a ranked list of the most pertinent datasets w.r.t the query. Further, if in a dataset we can identify a row, column, or a cell that seems to answer exactly the query, we return that result at the finest level of granularity.</p><p>It is important to note that query results are virtually always numbers, because this is what statistic institutes publish. Such numbers can only be interpreted through the metadata (elements of natural language) associated to them. We use L = {𝑇 , 𝐶𝐻, 𝑅𝐻, 𝐶} to denote the set of possible locations in which a term can appear in the metadata of a table, respectively: the dataset title, a column header, a row header, or a comment. The locations are important: (𝑖) since a term appearing in a title is more important than one appearing in a header (Section 3.1); (𝑖𝑖) to determine whether a given dataset matches some terms in a row header and others in a column header -in which case the cell at the intersection of the row with the column likely has a very pertinent result (Section 3.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Dataset Indexing and Search</head><p>For each dataset 𝑑, we call metadata of 𝑑 its title together with any comment published by INSEE next to the dataset. Further, the statistic tables within 𝑑 have row headers and column headers which may be INSEE internal codes, not directly readable by non-experts. In such cases, 𝑑 also includes (in a separate sheet) the naturallanguage descriptions of the dimensions involved in all these headers. These are also part the dataset's metadata.</p><p>We split the metadata of 𝑑 into a set of tokens 𝑇 = {𝑡 1 , . . . , 𝑡 𝑁 }. For each token 𝑡, we identify, based on a Word2Vec model<ref type="foot" target="#foot_3">3</ref> , the 50 tokens 𝑡 ′ closest semantically to 𝑡. Next, for each appearance of a token 𝑡 in a location 𝑙 within 𝑑, our term-location index 𝐼 𝑇 𝐿 stores: (𝑖) the index entry (𝑡, 𝑑, 𝑙) corresponding to the token actually found in 𝑑; and (𝑖𝑖) 50 entries of the form (𝑡 ′ , 𝑑, 𝑙, 𝑑𝑖𝑠𝑡), corresponding to the 50 tokens closest to 𝑡. These extra entries enable answering queries on terms close to (but disjoint from) the dataset content. For instance, when 𝑡 is "Enseignement", 𝑡 ′ could be "Ecole", "Enseignant", "Elève", etc. To determine the datasets pertinent for the query 𝑄, we look up the query keywords in 𝐼 𝑇 𝐿 ; a dataset containing at least one keyword is potentially interesting.</p><p>To rank datasets, a relevance score was introduced in [CMT18], based on word distances between the query keywords and the datasets' metadata, and also reflecting the locations where the keywords were found for each dataset. To this custom score, we have added the classic BM25 <ref type="bibr" target="#b19">[RZ09]</ref> computed over all the datasetes' metadata, and are currently experimenting with the two.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Improving the INSEE pipeline.</head><p>We brought several improvements over <ref type="bibr" target="#b7">[CMT18]</ref>. First, we collected and processed metadata during data acquisition (as opposed to retrieving it from Fuseki after loading). Tokening and lemmatization, based on Spacy, were sped up by accumulating all the metadata of a table in a single string for which we call Spacy only once per dataset.</p><p>Next, we improved the understanding and interpretation of geographical query terms. Users may query for a city, department, such as "Essonne", or region, while statistic data may be available at different levels of aggregation, e.g., a dataset may be about "Île-de-France" (the region). The system needs to be aware of the relationship between the two, in order to also consider returning data about the region. To that purpose, we used a list 4 containing the names of all cities, departments and regions of France (35.984 names in all). To quickly identify them in user queries, we used an implementation<ref type="foot" target="#foot_5">5</ref> of the FlashText algorithm <ref type="bibr" target="#b21">[Sin17]</ref>, capable of finding, in a document of size 𝑁 , one of 𝑀 fixed keywords, in 𝑂 (𝑁 ) time complexity. This is much faster than the typical 𝑂 (𝑁 𝑀) cost of regular expression pattern matching, a sizable advantage for us. The optimizations introduced above allow us to reduce the overall INSEE loading and indexing from 39 hours to about 3h.</p><p>3.1.2 Eurostat indexing pipeline. As detailed in Section 2, Eurostat datasets (𝑖) have a simpler tabular structure, (𝑖𝑖) together, are much larger than the INSEE corpus, leading to metadata 100× larger, and (𝑖𝑖𝑖) contain a significant number of very large files (hundreds of millions of rows). This discourages row-or cell-level indexing of the Eurostat metadata, as the index would be much too large.</p><p>Instead, we decide to use Eurostat statistical concept codes as a basis for indexing, as follows. Let 𝑐 be a Eurostat concept, e.g., ED1, appearing in dataset 𝑑 at a location 𝑙 ∈ L. We insert in a conceptdataset index 𝐼 𝐶 the entry (𝑐, 𝑑, 𝑙). For instance, if 𝑐 appears in 1 million row headers in 𝑑, only one entry with 𝑙 = 𝑅𝐻 is generated.</p><p>Next, let 𝑑 𝑐 be the natural-language description that Eurostat associates to 𝑐, e.g., "Enseignement primaire" for ED1. Let 𝑇 = {𝑡 1 , 𝑡 2 , . . . , 𝑡 𝑁 } be the tokens in 𝑑 𝑐 , and for 1 ≤ 𝑖 ≤ 𝑁 , let 𝑡 𝑗 𝑖 , for 1 ≤ 𝑗 ≤ 50, be the closest tokens to 𝑡 𝑖 . For each 𝑡 𝑖 ∈ 𝑇 , we look up all the (𝑐, 𝑑, 𝑙) pairs in 𝐼 𝐶 , and insert in the term-dataset index 𝐼 𝑇 , a (𝑡 𝑖 , 𝑑, 𝑙) entry. Next, for every 𝑡 𝑗 𝑖 similar to 𝑡 𝑖 , we insert in 𝐼 𝑇 an entry (𝑡 𝑗 𝑖 , 𝑑, 𝑙, 𝑑𝑖𝑠𝑡, 𝑡 𝑖 ), where 𝑑𝑖𝑠𝑡 is the distance between 𝑡 𝑖 and 𝑡 𝑗 𝑖 . 𝐼 𝐶 and 𝐼 𝑇 are stored in Redis; 𝐼 𝐶 only serves to accelerate the construction of 𝐼 𝑇 , which was prohibitively slow without it. Indexing the complete Eurostat data in this way took around 4 minutes.</p><p>When a query {𝑘 1 , . . . , 𝑘 𝑚 } is asked, we search 𝐼 𝑇 for entries of the form (𝑘 𝑖 , 𝑑, 𝑙) or (𝑘 𝑖 , 𝑑, 𝑙, 𝑑𝑖𝑠𝑡, 𝑘 ′ 𝑖 ). Any dataset having an entry for at least one 𝑘 𝑖 is potentially interesting; then, we score them either using either BM25 or the score introduced in [CMT18].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Data Cell Indexing and Search</head><p>From the 20 highest-scores datasets, we try to extract results at the cell, row, or column level. Given the query {𝑘 1 , . . . , 𝑘 𝑚 }, the index 𝐼 𝐶𝐿 (INSEE) or 𝐼 𝑇 (Eurostat) returns, for each 1 ≤ 𝑖 ≤ 𝑚, a set of entries, each containing: a keyword 𝑘 𝑖 or a close term 𝑘 𝑗 𝑖 , a dataset 𝑑, and a location 𝑙. Let 𝑑 be one of the most interesting datasets, and 𝐼 (𝑑) be the set of all index entries for this query and 𝑑.</p><p>If 𝐼 (𝑑) only features title (𝑇 ) or comment (𝐶) locations, then 𝑑 is pertinent as a whole, and no cell search is needed.</p><p>On the contrary, if 𝐼 (𝑑) has some entries with 𝑙 = 𝑅𝐻 , say, for keyword 𝑘 1 (or a close term 𝑘 𝑗 1 ), and other entries with 𝑙 = 𝐶𝐻 , say, for 𝑘 2 (or some 𝑘 𝑗 2 ), then at the intersection of the row whose header matches 𝑘 1 , with the column whose header marched 𝑘 2 , is a cell that appears to be a very fine-granularity answer to 𝑄.</p><p>• If 𝑑 is an INSEE dataset, 𝐼 (𝑑) specifies exactly which rows and columns are concerned. Then, the cell is identified by asking a SPARQL query <ref type="bibr" target="#b7">[CMT18]</ref>, evaluated by Fuseki. • On the contrary, if 𝑑 is an Eurostat dataset, 𝐼 (𝑑) only specifies that "some row (column) headers match". Identifying the relevant cells require more effort, as we explain below.</p><p>Identifying the right columns An Eurostat file has at most a few dozen columns. To find the column referred to by an 𝐼 (𝑑) entry whose key is 𝑘, we read the first (header) line of 𝑑, and identify the column(s) whose header contains 𝑘. This step is very fast, because we only read the beginning of the file.</p><p>Identifying the right rows This required an extra index to be efficient even on files with millions of rows. Specifically, when indexing the data, we also create a row location index 𝐼 𝑅 which, for every row header entry (𝑘, 𝑑, 𝑅𝐻 ) ∈ 𝐼 𝑇 , stores (𝑘, 𝑑, 𝑅 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">CLAIM DETECTION</head><p>A claim is a statement to be validated, that is we aim to establish if it is true of false. The validation is achieved by finding related statements, called evidence, which back up or disprove the claim. In our work, the claims are detected in an input text, while the evidence is retrieved from a set of trusted sources, our reference datasets. Our platform detects claims from text stored in .txt, .odt, .docx or .pdf files, and from the Twitter and Facebook posts of public figures. For posts, our platform retrieves regularly the most recent updates of a predefined group of users.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Statistical Claim Detection</head><p>In <ref type="bibr" target="#b10">[DCMT19]</ref>, the authors introduced a statistical claim detection method that given an input set of statistical entities (e.g. chômage, coefficient budgétaire) and a sentence, it retrieves all the statistical statements of the form ⟨statistical entity, numerical value and unit, date⟩ present in the sentence. The statistical statement, if present, represents the statistical claim to be verified. The statistical entities and units are retrieved using exact string matching, while the date is extracted using HeidelTime [SG10], a time expression parser. If no date is found by the parser, the posting timestamp is used. More context about the claim to be verified is found using a Named Entity Recognition (NER) model, which returns organizations and locations. We note, however, that the organization and location are optional, while a statistical statement is not complete without one of its three elements. The initial statistical entity list is constructed from the reference datasets by taking groups of tokens from the headers of tabels, we refer to <ref type="bibr" target="#b10">[DCMT19]</ref> for more details.</p><p>We improved the method presented in <ref type="bibr" target="#b10">[DCMT19]</ref> to optimize both speed and quality of extractions. We refer to the two methods as OriginalStatClaim <ref type="bibr" target="#b10">[DCMT19]</ref> and StatClaim. We first performed a more careful match between the tokens of a sentence and our input statistical entities. Using the syntactic tree of the sentence and a lemmatizer, statistical entities are matched on using their lemma, and are extended to contain the entire nominal group of the matched token. Numerical values are associated with units using both lemma matching from our set of units and a syntactic analysis. Units can be a noun following a numerical value, or a nominal group containing one or more units. (e.g. "millions d'euros"). As in the original approach, if we retrieve a statistical statement of the form ⟨statistical entity, numerical value and unit, date⟩, we have found a claim to verify. In the default setting of our algorithm, a claim should contain all the three element. In addition, we filter claims coming from sentences in which the verb is in the future tense or in the first person. We have incorporated feedback from the journalists and we allow through our interface a relaxation of the approach: the numerical value can be missing, and the filtering on the verbs can be removed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Check-worthy Claim Detection</head><p>To complement the statistical claim detection model, we developed a model that is not conditioned on a set of initial statistical entities. The model classifies a sentence as check-worthy or not, where check-worthiness is defined as sentences containing factual claims that the general public will be interested in learning about their veracity <ref type="bibr" target="#b1">[AHLT20]</ref>. We leveraged the ClaimBuster dataset [AHLT20], a English dataset containing check worthy claims from the U.S. Presidential debates, to train a cross-lingual language model, XLM-R [CKG + 19], which is able to perform zero-shot classification on French sentences after having been trained on English data.</p><p>The ClaimBuster dataset ClaimBuster is a crowd-sourced dataset where the sentences from the 15 U.S. presidential elections debates from 1960 to 2016 have been annotated. The labels are Non-Factual Sentences (NFS), Unimportant Factual Sentences (UFS) or Check-Worthy Factual Sentences (CFS). The dataset contains 23𝐾 sentences, and a subset of higher quality of 11𝐾 sentences was produced by the authors for training models on classification tasks. In this smaller dataset, the NFS and UFS labels are grouped together as negative labels, and the CFS labels are considered positive. We chose this higher quality dataset to fine-tune the XLM-R model.  while remaining competitive on monolingual tasks. We used a pretrained model with a vocabulary size of 250𝐾, 12 hidden layers of size 768 and 12 attention heads. To account for the unbalanced ratio of labels, we used a weighted cross-entropy loss. The dataset was split into train, dev and test datasets with a ratio of 80%/%10%/10%.</p><p>Evaluation To ptimize the performance, we trained the model with different hyperparameters. The best results were obtained with a learning rate of 5 • 10 -5 , a batch size of 64, and using the AdamW optimizer. To evaluate the performance of the different models on French data, we annotated 200 randomly sampled French tweets and labeled them as check-worthy or not following the definition in <ref type="bibr" target="#b1">[AHLT20]</ref>. Two annotators labeled each tweet; in the golden standard, a tweet is deemed check-worthy if both annotators agree on it, and not check-worthy otherwise. The Cohen Kappa score for inter-annotator agreement is 0.6, which is considered moderate to substantial agreement. The results can be found in Table <ref type="table" target="#tab_1">1</ref>. The drop in precision on French data could be because we are evaluating on a small test dataset (only 200 tweets with 39 positive examples), or the fact that the tweets' format and vocabulary might be different than the ones in the Presidential debate sentences used for training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Integration and Evaluation of the Claim Detection Models</head><p>We evaluate the claim detection models, (OriginalStatClaim Evaluation procedure For StatClaim and OriginalStatClaim, a tweet is considered positive if models return at least one extracted statistical statement. Our StatClaim was used in its default configuration: extractions with numerical values, and without verbs conjugated in the future or at the first person. For CheckWorthy-Claim, a tweet is considered positive if the model return a checkworthy score &gt; 0.9. We report the results in Table <ref type="table" target="#tab_2">2</ref> and Table <ref type="table" target="#tab_3">3</ref>. StatClaim performs better than the original at detecting INSEE verifiable claims, and CheckWorthyClaim vastly outperforms both models on the detection of numerical claims, as they are a subset of check-worthy sentences that the model was trained to detect.</p><p>Default claim detection strategy. By default, StatCheck uses StatClaim for statistical claim detection. However, given the good performance of CheckWorthyClaim on numerical claims, we allow users to switch to it, even if we might not be able to verify them against the reference datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">DEMONSTRATION SCENARIOS</head><p>Our system is developed in Python and deployed on a Unix server. Its GUI is accessible via a Web server; Figure <ref type="figure" target="#fig_2">3</ref> shows two screen captures. Demonstration attendees will be able to:</p><p>• Ask queries in the statistic search interface, and inspect the results, at the level of cell, line, or column, together with their metadata from the original statistic site (INSEE or Eurostat); • Visualize incoming social media messages (as they arrive in real-time and are stored and analyzed by our platform), in order to see (𝑖) the statistical mentions and (𝑖𝑖) claims deemed potentially check-worthy, identified in these messages, as illustrated in Figure <ref type="figure" target="#fig_2">3</ref>. Note that the system also proposes candidate search queries for the statistic search interface. • Users will be able to select various options (restrict to numerical claims or not, include statements about the future or not, include first-person texts or not, etc.) and see their impact on the claim extraction output. • Write their own text and/or suggest other content to be processed by our analysis pipeline (Section 4).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Statistical fact-checking architecture overview.</figDesc><graphic coords="4,128.34,83.68,353.08,120.62" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: RDF graph modeling of INSEE table data [CMT17].</figDesc><graphic coords="4,53.80,226.57,240.22,68.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Screen captures of our tool. Top: sample candidate data cells with the corresponding header row (blue) and header column (red); bottom: tweet analysis interface.</figDesc><graphic coords="6,53.80,192.17,504.41,70.47" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fine-tuning</head><label></label><figDesc>the XLM-R model The XLM-R model is a Transformerbased masked language model trained on one hundred languages with 2.5TB of Common Crawl data. It achieves state-of-the-art results on multilingual tasks such as the XNLI benchmark [CRL + 18],</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>𝑘,𝑑 ), where 𝑅 𝑘,𝑑 is the list of the indexes of data rows in 𝑑 which contain 𝑘 in their header. For efficiency, 𝐼 𝑅 is saved directly as a file on disk, and supports direct access by 𝑘 and 𝑑, following the Adaptive Positional Map of [ABB + 15]. Cell extraction Once the right rows and column indexes are known, we read the relevant rows from 𝑑, and carve out of them the relevant data cell(s). With the help of a line cache library 6 , this process is quite efficient.Overall, on INSEE datasets, using Fuseki, data cell search takes 35ms up to 2.86s. On Eurostat, using 𝐼 𝑅 , we record 4.76𝜇s up to 2.66s. The lower bound is higher for INSEE because we have to pass SPARQL queries across a connection to the Fuseki server.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Evaluation of the fine-tuned XLM-R model.</figDesc><table><row><cell>Dataset</cell><cell cols="3">Precision Recall F1 score Accuracy</cell></row><row><cell>ClaimBuster</cell><cell>0.883 0.848</cell><cell>0.865</cell><cell>0.934</cell></row><row><cell>French tweets</cell><cell>0.612 0.769</cell><cell>0.682</cell><cell>0.856</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Model evaluation on verifiable numerical claims.</figDesc><table><row><cell>Models</cell><cell cols="2">Precision Recall F1 score</cell></row><row><cell>OriginalStatClaim</cell><cell>0.692 0.466</cell><cell>0.557</cell></row><row><cell>StatClaim</cell><cell>0.833 0.517</cell><cell>0.638</cell></row><row><cell>CheckWorthyClaim</cell><cell>0.701 0.915</cell><cell>0.794</cell></row><row><cell>Models</cell><cell cols="2">Precision Recall F1 score</cell></row><row><cell>OriginalStatClaim</cell><cell>0.282 0.688</cell><cell>0.400</cell></row><row><cell>StatClaim</cell><cell>0.333 0.750</cell><cell>0.462</cell></row><row><cell>CheckWorthyClaim</cell><cell>0.195 0.938</cell><cell>0.323</cell></row></table><note><p><p><ref type="bibr" target="#b10">[DCMT19]</ref></p>, StatClaim and CheckWorthyClaim), on a set of 1595 tweets. Each tweet was labeled with two classes: "Verifiable numerical claim" (True if the tweet contains at least one numerical and verifiable claim") and "INSEE statistical claim" (True if the tweet contains at</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Model evaluation on INSEE statistical claims.least one numerical, statistical claim verifiable against the INSEE dataset"). We chose these two labels as the first one gives us an indication of the tweets that can be verified if we had an unlimited access to resources, while the second class identifies the tweets verifiable in the setting in which we have access to only one resource. To construct our set, we gathered 1595 random tweets from our scraped dataset. Then, we automatically detected if a tweet contained a numerical value, if not, the tweet was labeled as negative for both classes. After that first step, we manually labeled the remaining 101 tweets. Two annotators labeled each tweet, and the gold standard was chose as True if both annotators agreed. For the class "verifiable numerical claim", we obtained a Kappa inter-Annotator Agreement score of 0.917 (almost perfect agreement) and 59 tweets were labeled as positive. For the class "INSEE statistical claim" we obtained an inter-annotator Agreement score of 0.807 (subtantial agreement) and 16 tweets were labeled as positive.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>https://truthandtrustonline.com/ BDA '22, October</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_1"><p>24-27, 2022, Clermont-Ferrand, France 2018. ACM ISBN 978-1-4503-XXXX-X/18/06. . . $15.00 https://doi.org/XXXXXXX.XXXXXXX</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_2"><p>https://jena.apache.org/documentation/tdb2/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_3"><p>We used the model frWac_non_lem_no_postag_no_phrase_200_skip_cut100.bin from https://fauconnier.github.io</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_4"><p>https://www.data.gouv.fr/fr/datasets/regions-departements-villes-et-villages-defrance-et-doutre-mer/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_5"><p>https://github.com/vi3k6i5/flashtext</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_6"><p>https://docs.python.org/fr/3/library/linecache.html</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>ACKNOWLEDGEMENTS</head><p>We thank <rs type="institution">Radio France, France Info</rs>, and in particular <rs type="person">Antoine Krempf</rs> and his Le vrai du faux team, for their valuable feedback. This work is partially funded by <rs type="projectName">AI Chair SourcesSay</rs> project (<rs type="grantNumber">ANR-20-CHIA-0015-01</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_ZqWydB5">
					<idno type="grant-number">ANR-20-CHIA-0015-01</idno>
					<orgName type="project" subtype="full">AI Chair SourcesSay</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Nodb: efficient query execution on raw data files</title>
		<author>
			<persName><forename type="first">Ioannis</forename><surname>Alagiannis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Renata</forename><surname>Borovica-Gajic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miguel</forename><surname>Branco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stratos</forename><surname>Idreos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anastasia</forename><surname>Ailamaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="112" to="121" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A Benchmark Dataset of Check-worthy Factual Claims</title>
		<author>
			<persName><forename type="first">Naeemul</forename><surname>Fatma Arslan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengkai</forename><surname>Hassan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><surname>Tremayne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">14th International AAAI Conference on Web and Social Media</title>
		<imprint>
			<publisher>AAAI</publisher>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName><surname>Ckg +</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Unsupervised cross-lingual representation learning at scale</title>
		<author>
			<persName><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kartikay</forename><surname>Khandelwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vishrav</forename><surname>Chaudhary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Wenzek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francisco</forename><surname>Guzmán</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.02116</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title/>
		<author>
			<orgName type="collaboration">CLL</orgName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A content management perspective on fact-checking</title>
		<author>
			<persName><forename type="first">Sylvie</forename><surname>Cazalens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philippe</forename><surname>Lamarre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Leblay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ioana</forename><surname>Manolescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Tannier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW (Companion</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="565" to="574" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Extracting linked data from statistic spreadsheets</title>
		<author>
			<persName><forename type="first">Tien</forename><surname>Duc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cao</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Ioana</forename><surname>Manolescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Tannier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Workshop on Semantic Big Data, International Workshop on Semantic Big Data</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Searching for Truth in a Database of Statistics</title>
		<author>
			<persName><forename type="first">Tien-Duc</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ioana</forename><surname>Manolescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Tannier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WebDB</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Xnli: Evaluating cross-lingual sentence representations</title>
		<author>
			<persName><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruty</forename><surname>Rinott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adina</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><forename type="middle">R</forename><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Tabfact : A large-scale dataset for table-based fact verification</title>
		<author>
			<persName><forename type="first">Wenhu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongmin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianshu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunkai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shiyang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiyou</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wang</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Extracting statistical mentions from textual claims to provide trusted content</title>
		<author>
			<persName><forename type="first">Tien</forename><surname>Duc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cao</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Ioana</forename><surname>Manolescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Tannier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NLDB</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">TaPas: Weakly supervised table parsing via pre-training</title>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Herzig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Krzysztof</forename><surname>Pawel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Nowak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francesco</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Piccinno</surname></persName>
		</author>
		<author>
			<persName><surname>Eisenschlos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="4320" to="4333" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title/>
		<author>
			<persName><surname>Hza</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Claimbuster: The first-ever end-to-end fact-checking system</title>
		<author>
			<persName><forename type="first">Naeemul</forename><surname>Hassan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gensheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fatma</forename><surname>Arslan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Josue</forename><surname>Caraballo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Damian</forename><surname>Jimenez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siddhant</forename><surname>Gawsane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shohedul</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minumol</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaditya</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anil</forename><surname>Kumar Nayak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vikas</forename><surname>Sable</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengkai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Tremayne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PVLDB</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1945" to="1948" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title/>
		<author>
			<persName><surname>Jty</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Verifying text summaries of relational data sets</title>
		<author>
			<persName><forename type="first">Saehan</forename><surname>Jo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Immanuel</forename><surname>Trummer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weicheng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuezhi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niyati</forename><surname>Mehta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGMOD, SIGMOD &apos;19</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="299" to="316" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Scrutinizer: Fact checking statistical claims</title>
		<author>
			<persName><forename type="first">Georgios</forename><surname>Karagiannis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammed</forename><surname>Saeed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paolo</forename><surname>Papotti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Immanuel</forename><surname>Trummer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. VLDB Endow</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2965" to="2968" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Automated fact-checking for assisting human fact-checkers</title>
		<author>
			<persName><forename type="first">Preslav</forename><surname>Nakov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Corney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maram</forename><surname>Hasanain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Firoj</forename><surname>Alam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tamer</forename><surname>Elsayed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alberto</forename><surname>Barrón-Cedeño</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paolo</forename><surname>Papotti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaden</forename><surname>Shaar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Giovanni</forename><surname>Da</surname></persName>
		</author>
		<author>
			<persName><forename type="first">San</forename><surname>Martino</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="4551" to="4558" />
		</imprint>
	</monogr>
	<note type="report_type">Survey Track</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">DeClarE: Debunking fake news and false claims using evidenceaware deep learning</title>
		<author>
			<persName><forename type="first">Kashyap</forename><surname>Popat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Subhabrata</forename><surname>Mukherjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Yates</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gerhard</forename><surname>Weikum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="22" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">The probabilistic relevance framework: BM25 and beyond</title>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Zaragoza</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>Now Publishers Inc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Heideltime: High quality rule-based extraction and normalization of temporal expressions</title>
		<author>
			<persName><forename type="first">Jannik</forename><surname>Strötgen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Gertz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int&apos;l. Workshop on Semantic Evaluation</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="321" to="324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Replace or retrieve keywords in documents at scale</title>
		<author>
			<persName><forename type="first">Vikash</forename><surname>Singh</surname></persName>
		</author>
		<idno>CoRR, abs/1711.00046</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Fact-checking statistical claims with tables</title>
		<author>
			<persName><forename type="first">Mohammed</forename><surname>Saeed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paolo</forename><surname>Papotti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Data Engineering Bulletin</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
