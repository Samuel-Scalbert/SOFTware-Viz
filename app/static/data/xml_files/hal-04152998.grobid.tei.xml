<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">RULK NE : Representing User Knowledge State in Search-as-Learning with Named Entities</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Celia</forename><surname>Da-Costa-Pereira</surname></persName>
							<idno type="ORCID">0000-0001-6278-7740</idno>
							<affiliation key="aff0">
								<orgName type="institution">d &apos;</orgName>
							</affiliation>
							<affiliation key="aff1">
								<address>
									<country>The</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">RULK NE : Representing User Knowledge State in Search-as-Learning with Named Entities</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">E0191A67220B43C272D3CFA9A96D86F9</idno>
					<idno type="DOI">10.1145/3576840.3578330</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-04-12T14:50+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Search-As-Learning</term>
					<term>User Knowledge</term>
					<term>Named Entities</term>
					<term>Interactive IR</term>
					<term>Retrieval system</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>A reliable representation of the user's knowledge state during a learning search session is crucial to understand their real information needs. When a search system is aware of such a state, it can adapt the search results and provide greater support for the user's learning objectives. A common practice to track the user's knowledge state is to consider the content of the documents they read during their search session(s). However, most current work ignores entity mentions in the documents, which, when linked to knowledge graphs, can be a source of valuable information regarding the user's knowledge. To fill this gap, we extend RULK-Representing User Knowledge in Search-as-Learning-with entity linking capabilities. The extended framework RULK NE represents and tracks user knowledge as a collection of such entities. It eventually estimates the user knowledge gain-learning outcome-by measuring the similarity between the represented knowledge and the learning objective. We show that our methods allow for up to 10% improvements when estimating user knowledge gains.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>systems can provide the user with content that is relevant to their query and adapted to their actual knowledge level.</p><p>A common practice is to represent that state of user knowledge using the content of the documents they read during the search session. We recently proposed in <ref type="bibr" target="#b1">[2]</ref> RULK-A framework for Representing User Knowledge in Search-as-Learning-two different types of representation: keyword-based and language-model-based, exploiting large pre-trained language models. However, neither of these two types exploits an explicit annotation of named entities (NE) and their links to resources of extensive knowledge graphs, which can be much more precise and specific in identifying what a given document is about, and, consequently, what kind of knowledge it entails.</p><p>A knowledge graph (KG) is a knowledge base where knowledge is represented as a set of entities (or resources), which are the vertices of a graph, connected by binary relationships (or roles, properties, attributes), which constitute the labeled edges of the graph. In modern approaches to information access, knowledge graphs are ubiquitous <ref type="bibr" target="#b3">[4]</ref>. Specifically, they can be used in information retrieval to support semantic search <ref type="bibr" target="#b11">[12]</ref>.</p><p>The resources described by a knowledge graph not only allow documents to be semantically annotated, but also represent the epistemic state of a user and provide a background knowledge enriching queries and refining results.</p><p>The use of KGs to represent information about the user is not new; personal knowledge graphs organised user's personal information <ref type="bibr" target="#b0">[1]</ref>, life events <ref type="bibr" target="#b17">[18]</ref> or profiling interests <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6]</ref>. The main distinctive feature of our work is what we aim to make of the information described by a KG, especially the links between entities, as a way to represent and measure users' knowledge during their search sessions (i.e., their epistemic state). What we propose here is to take a further step and use links to resources described by knowledge graphs not only to represent information about entities personally related to a user, but also what an information retrieval system may presume its user already knows (i.e., what we might call the user's epistemic state), based, for instance, on past interaction, documents that the user has read, or direct feedback from the user.</p><p>More specifically, we propose another instantiation of RULK that leverages the coverage of a large KG as a measure of user knowledge. Such entity-based representation, which we name RULK NE , is a highly structured and rich representation that promises to be more accurate and detailed than a representation based simply on keywords or latent semantic components. Therefore, in this paper, we answer the following research questions: RQ1 Are the estimated knowledge gains produced by RULK NE correlated to actual users' knowledge? RQ2 How does RULK NE compare to other existing RULK implementations?</p><p>Our framework has two main requirements: (1) a way to recognize named entities in a text and link them to a knowledge graph, and</p><p>(2) a similarity measure among collections of linked named entities allowing one to estimate the knowledge gain provided by a set of documents to a user. We empirically compare our proposal to a learning task's keyword-and language model-based representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">BACKGROUND</head><p>In this section, we first give a brief overview of our previously proposed RULK framework <ref type="bibr" target="#b1">[2]</ref>. Secondly, we quickly introduce knowledge graphs (KGs), and how they connect to our new proposal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">The RULK Framework</head><p>The framework proposed in <ref type="bibr" target="#b1">[2]</ref> is composed of three interacting components: Feature Extractor (ğœ¸ ), Updater (ğˆ) and Estimator (ğœ½ ).</p><p>As users interact with the search results,<ref type="foot" target="#foot_0">1</ref> they learn about a given topic of interest. RULK tracks the user's knowledge through an internal state represented by a vector current knowledge state Ã¬ ğ‘ ğ‘˜ğ‘  . The search system that instantiates RULK is assumed to have access to a target knowledge, or reference document, covering the "ideal" knowledge to be acquired regarding a specific topic ğ‘‡ . We consider a user's search need-or learning objective-is represented in this document. For the sake of simplicity, we also make the assumption that the user has no previous knowledge about ğ‘‡ . Figure <ref type="figure">1</ref> shows an overview of the framework RULK as presented in <ref type="bibr" target="#b1">[2]</ref>.</p><p>Feature Extractor (ğœ¸ ). A Feature Extractor is a component that, given a document ğ‘‘,<ref type="foot" target="#foot_1">2</ref> encodes it into a fixed-length representation, using a method like TF-IDF, Word2Vec or any other encoding method. ğœ¸ encodes a document read by the user into Ã¬ ğ‘£ ğ‘‘ , and the reference document into Ã¬ ğ‘¡ ğ‘˜ğ‘  .</p><p>Updater (ğˆ). When the user reads a new document, they acquire new knowledge that is added to the previous ones. ğˆ updates the current state of the knowledge Ã¬ ğ‘ ğ‘˜ğ‘  with the new information Ã¬ ğ‘£ ğ‘‘ . The updated knowledge state Ã¬ ğ‘â€² ğ‘˜ğ‘  is the result of combining the user's current state Ã¬ ğ‘ ğ‘˜ğ‘  to the Ã¬</p><formula xml:id="formula_0">ğ‘£ ğ‘‘ : Ã¬ ğ‘â€² ğ‘˜ğ‘  = ğœ ( Ã¬ ğ‘ ğ‘˜ğ‘  , Ã¬ ğ‘£ ğ‘‘ ).</formula><p>Estimator (ğœ½ ). To estimate the user's knowledge gain-or learning outcome-on a specific topic during a session, the Estimator ğœ½ compares the user's current knowledge state Ã¬ ğ‘ ğ‘˜ğ‘  to the target knowledge state Ã¬ ğ‘¡ ğ‘˜ğ‘  : ğº = ğœƒ ( Ã¬ ğ‘ ğ‘˜ğ‘  , Ã¬ ğ‘¡ ğ‘˜ğ‘  ), where ğº is an estimation of the user's knowledge gain in the session and ğœ½ is a similarity function (e.g., cosine similarity). The intuition behind ğœ½ is that the user, by progressing in their session, "moves" their knowledge state toward the target. As both vectors are in the same embedding space, the similarity between them provides an estimation of how close the user is to acquiring the "ideal" knowledge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Keyword and Language Model Implementations of RULK</head><p>We briefly present the Keyword (KW) and Language Model (LM) implementations previously proposed in <ref type="bibr" target="#b1">[2]</ref>.</p><p>RULK KW . The target knowledge in this implementation is represented as a set of ğ‘› keywords that the user must read to fulfill their search need. The ğœ¸ component encodes the reference document into Ã¬ ğ‘¡ ğ‘˜ğ‘  containing the occurrence count number of the ğ‘› keywords. Those numbers represent the number of times the user has to read a specific keyword to fulfill the need. Similarly, the clicked documents are encoded into a vector containing the frequency of ğ‘› keywords. To update the user knowledge, ğˆ adds the keywords' count in the documents to the one in the knowledge state. The user knowledge is assumed to increase monotonically. The ğœ½ component estimates the user knowledge gain by calculating the cosine similarity between Ã¬ ğ‘ ğ‘˜ğ‘  and Ã¬ ğ‘¡ ğ‘˜ğ‘  .</p><p>RULK LM . In this implementation, both the Ã¬ ğ‘¡ ğ‘˜ğ‘  and Ã¬ ğ‘£ ğ‘‘ are a BERTembedding of fixed length ğ‘š. Given a document ğ‘‘ (or, conversely, a reference document) with ğ‘˜ sentences {ğ‘  1 , ğ‘  2 . . . ğ‘  ğ‘˜ }, ğœ¸ generates, for each sentence ğ‘  ğ‘– , an embedding of size ğ‘š given by:</p><formula xml:id="formula_1">Ã¬ ğ‘£ ğ‘  ğ‘– = ğµğ¸ğ‘…ğ‘‡ ([ğ¶ğ¿ğ‘†]; ğ‘  ğ‘– :ğ‘™ ; [ğ‘†ğ¸ğ‘ƒ]),<label>(1)</label></formula><p>where ; is a concatenation, ğ‘™ the maximum input size of the model and [ğ¶ğ¿ğ‘†] and [ğ‘†ğ¸ğ‘ƒ] are special BERT tokens. Ã¬ ğ‘£ ğ‘‘ (conversely, Ã¬ ğ‘¡ ğ‘˜ğ‘  ) is then given by an element-wise sum over all Ã¬ ğ‘£ ğ‘  ğ‘– . The Updater ğˆ is then a simple element-wise sum over all elements of Ã¬ ğ‘£ ğ‘‘ and Ã¬ ğ‘ ğ‘˜ğ‘  . The cosine similarity is also used in ğœ½ to estimate the knowledge gain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Knowledge Graphs</head><p>A knowledge graph is a knowledge base that uses a graph-structured data model to integrate data. Knowledge graphs are often used to store interlinked descriptions of entities-objects, events, situations or abstract concepts-while also encoding the semantics underlying the used terminology.</p><p>In this paper, we will assume that knowledge graphs follow a standard and technical infrastructure like the ones provided by the W3C for the Semantic Web, i.e.: OWL (the Web ontology language, based on description logics) based on the underlying data model RDF (the resource definition standard). This would allow a practical implementation of our proposal using state-of-the-art knowledge engineering technologies.</p><p>The basic statement of RDF is a triple âŸ¨ğ‘ , ğ‘, ğ‘œâŸ©, where ğ‘  is called the subject, ğ‘ the predicate, and ğ‘œ the object. The subject of a triple is a resource (or, in other words, an entity), represented by an internationalized resource identifier (IRI ), like &lt;http://example.org/ resource/LHR&gt;; the (binary) predicate represents a property of the subject, denoted by an IRI, like &lt;http://example.org/ontology#city&gt; or &lt;http://example.org/ontology#iataCode&gt;; the object, which represents a value of that property, may be a resource, denoted by an IRI, like &lt;http://example.org/resource/London&gt;, or a data value, such as a string, a number, or a date, denoted by a literal, like "LHR", 2, or 07:30. In addition, the subject and object can be so-called blank nodes, which correspond to anonymous resources and can be understood as a kind of existentially quantified variables.</p><p>For the sake of readability and conciseness, when several IRIs share the same base, a prefix may be defined, for instance @prefix : &lt;http://example.org/resource/&gt; . @prefix o: &lt;http://example.org/ontology#&gt; . and the IRIs may then be abbreviated as :LHR instead of the full &lt;http://example.org/resource/LHR&gt; or o:iataCode instead of the full &lt;http://example.org/ontology#iataCode&gt;.</p><p>An important thing to observe is that, behind an IRI, which is essentially an identifier, many different notions can hide, like an instance (i.e., a constant, what is called an individual name in description logics), a binary predicate (i.e., a binary relation, what is called a role in description logics and a property in OWL), or a concept (i.e., a unary predicate, called a class in OWL). It is exactly this uniform naming convention that makes RDF so flexible and versatile. Thus, for instance, an assertion of the form ğ¶ (ğ‘), where ğ¶ is a unary predicate (a concept) and ğ‘ is the name of an individual, may be represented as an RDF triple âŸ¨ğ‘, rdf:type, ğ¶âŸ©, which may be paraphrased as "ğ‘ is an instance of ğ¶", thanks to the rdf:type relation, and an assertion of the form ğ‘…(ğ‘, ğ‘), where ğ‘… is a binary predicate and ğ‘ and ğ‘ are entity names, may be represented as an RDF triple âŸ¨ğ‘, ğ‘…, ğ‘âŸ©, for example :LHR o:city :London . which may be paraphrased as "the city of the Heathrow Airport is London".</p><p>A collection of RDF triples, which may represent assertions and other OWL axioms with a uniform syntax, may be regarded as a knowledge base under the open-world assumption, from which other triples can be deduced using an inference engine called an OWL reasoner. <ref type="foot" target="#foot_2">3</ref> Furthermore, a collection of RDF triples intrinsically represents a directed multi-graph (an RDF graph), whose vertices are resources; every triple âŸ¨ğ‘ , ğ‘, ğ‘œâŸ© then represents an arc of type ğ‘ from vertex ğ‘  to vertex ğ‘œ.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">A NAMED-ENTITY-BASED IMPLEMENTATION OF RULK</head><p>We propose a novel instantiation of RULK using named entities (NE) to represent both the internal and target knowledge states. We will call RULK NE this Named-Entity-based variant of the framework. The Feature Extractor ğœ¸ here produces a collection of links to knowledge graph resources, corresponding to named entities. A collection ğ¾ ğ‘¡ ğ‘˜ğ‘  is extracted from the reference document and another ğ¾ ğ‘‘ for every visited document ğ‘‘. Producing a collection of such links from a document requires a reference knowledge graph, to be used as background knowledge, as it were, and two NLP tasks to be carried out, namely (i) named entity recognition (i.e., spotting chunks of text that are likely to refer to specific entities such as people, places, organizations, etc.), and (ii) entity linking (i.e., establishing a link between a recognized named entity and a resource defined in the reference knowledge graph). These two tasks can be challenging, but in recent years, a wide range of emerging tools can accomplish them with acceptable performance, albeit not always perfectly. This enables one to visualize what we are suggesting. One can only foresee that these tools will be improved and new, even better tools will become available in a near future, thus contributing to making RULK NE more and more accurate. The reference knowledge graph can be any of the large general-purpose RDF datasets available in the Linked Open Data cloud, like DBpedia, Yago, or Wikidata. We encode the target knowledge as a vector Ã¬ ğ‘¡ ğ‘˜ğ‘  of the counts of the occurrences of the 10 most common entities ğ¾ ğ‘¡ ğ‘˜ğ‘  . Each document is then encoded as a vector Ã¬ ğ‘£ ğ‘‘ containing the counts of the top-10 entities ğ¾ ğ‘¡ ğ‘˜ğ‘  in ğ‘‘. The Updater ğˆ is then a simple addition of the two count vectors Ã¬ ğ‘ ğ‘˜ğ‘  and Ã¬ ğ‘£ ğ‘‘ . Finally, the Estimator ğœ½ should compute the similarity between the encoded current knowledge state and the encoded target knowledge state. That is done using the cosine similarity:</p><formula xml:id="formula_2">ğº = Ã¬ ğ‘ ğ‘˜ğ‘  â€¢ Ã¬ ğ‘¡ ğ‘˜ğ‘  | Ã¬ ğ‘¡ ğ‘˜ğ‘  | | Ã¬ ğ‘ ğ‘˜ğ‘  | . (<label>2</label></formula><formula xml:id="formula_3">)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENT 4.1 Dataset</head><p>We test our proposed framework RULK NE on the same dataset previously used in <ref type="bibr" target="#b1">[2]</ref>. The dataset <ref type="foot" target="#foot_3">4</ref> originates from the study by Camara et al. <ref type="bibr" target="#b2">[3]</ref> and logs search-as-learning sessions. The dataset was collected with a search system implemented on top of SearchX <ref type="bibr" target="#b10">[11]</ref>, a framework for Interactive Information Retrieval research. We also show some statistics about the dataset in Table <ref type="table" target="#tab_1">1</ref>.</p><p>The dataset contains the interaction logs of 126 crowd-workers. At the start of the study, the system measured the user's previous knowledge ğ‘£ğ‘˜ğ‘  ğ‘ğ‘Ÿğ‘’ on a specific topic, then users were given 45 minutes to perform their search about the topic. Finally, at the end of the search session, the user's knowledge was measured again ğ‘£ğ‘˜ğ‘  ğ‘ğ‘œğ‘ ğ‘¡ . The logged interactions included behavioural features, issued queries, and clicked documents.</p><p>The dataset contained 1107 unique clicked documents. We were retrieved their related texts using a digital archive-The Wayback Machine<ref type="foot" target="#foot_4">5</ref> -of the World Wide Web at the time the study was conducted (August 2020).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Actual Knowledge Gain Measurement</head><p>The self-reported knowledge, ğ‘£ğ‘˜ğ‘  ğ‘ğ‘Ÿğ‘’ and ğ‘£ğ‘˜ğ‘  ğ‘ğ‘œğ‘ ğ‘¡ , were measured with a Vocabulary Knowledge Scale (VKS) test <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b16">17]</ref>, a commonly used method to measure user knowledge <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b15">16]</ref>. For that, users were presented with a 4-point scale questionnaire, asking about their familiarity with ten topic-related terms.</p><p>The user's learning during their session could therefore be measured by computing the difference between ğ‘£ğ‘˜ğ‘  ğ‘ğ‘Ÿğ‘’ and ğ‘£ğ‘˜ğ‘  ğ‘ğ‘œğ‘ ğ‘¡ . The learning measures is defined as follows:</p><formula xml:id="formula_4">ğ´ğ¿ğº = 1 10 10 âˆ‘ï¸ ğ‘–=1 ğ‘šğ‘ğ‘¥ (0, ğ‘£ğ‘˜ğ‘  ğ‘ğ‘œğ‘ ğ‘¡ (ğ‘£ ğ‘– ) -ğ‘£ğ‘˜ğ‘  ğ‘ğ‘Ÿğ‘’ (ğ‘£ ğ‘– )) ğ‘€ğ¿ğº = 1 10 10 âˆ‘ï¸ ğ‘–=1 2 -ğ‘£ğ‘˜ğ‘  ğ‘ğ‘Ÿğ‘’ (ğ‘£ ğ‘– ) ğ‘…ğ‘ƒğ¿ = ğ´ğ¿ğº ğ‘€ğ¿ğº<label>(3)</label></formula><p>where ğ‘£ğ‘˜ğ‘  (ğ‘£ ğ‘– ) the score of the user for the i-th term. ğ´ğ¿ğº is the Absolute Learning Gain and ğ‘€ğ¿ğº is the Maximum Learning Gain (i.e., the maximum amount of new knowledge a user can acquire, given what they already know). As for RPL, it represents the fraction of knowledge the user acquired from the total knowledge they could obtain in their session. In this paper, we use actual knowledge gain interchangeably to refer to RPL and ALG.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Target knowledge</head><p>The topics used in the original user study came from the list of topics used in the CAR track from TREC 2018 <ref type="bibr" target="#b6">[7]</ref>. In that track, each topic is the title of a Wikipedia article from a 2018 dump. Our experiment uses these Wikipedia texts, from the same 2018 dump as the original paper, as "reference documents" for generating the target knowledge state Ã¬ ğ‘¡ ğ‘˜ğ‘  . It is also worth mentioning that, in the user study which originated this dataset, the mentioned work filtered Wikipedia and clones of Wikipedia from the search results during the user study. Therefore, no document proposed to the user in the page results of the experiment came from Wikipedia or a similar page.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Implementation Details</head><p>Entity Recognition. We detect the named entities using the Spacy <ref type="bibr" target="#b7">[8]</ref> Python library. We chose the English pipeline en_core_web_sm, which is trained on written web text (blogs, news, comments).</p><p>Entity Linking. We automatically annotate the texts with DBpedia resources using the dbpedia_spotlight tool <ref type="bibr" target="#b8">[9]</ref>.</p><p>Similarity Calculation and Comparison of results. As discussed in Section 3, the user's estimated knowledge gain ğº is calculated as the cosine similarity between the tracked knowledge Ã¬ ğ‘ ğ‘˜ğ‘  and the target knowledge state Ã¬ ğ‘¡ ğ‘˜ğ‘  . To assess the validity of the framework (RQ1), we measure the correlation between the estimated gain ğº and the actual user's knowledge (ALG and RPL).</p><p>Baseline. We compare our approach against previous work <ref type="bibr" target="#b1">[2]</ref>, in which knowledge states were represented by keywords RULK KW and large language models RULK LM . These two representations are briefly described above, in Section 2. achieved the best performance when using a combination of keywords and language models, RULK KW+LM . The best correlation between the previously estimated gain and the actual gain was 0.3164 against ALG and 0.3192 against RPL.</p><p>RULK mixed approaches. We also test a combination of our proposed instantiation RULK NE with the previous approaches RULK LM , RULK KW , and RULK KW+LM . In such combinations, each instantiation has the potential to contribute to the overall estimation of the knowledge gain by capturing some specific characteristics of the text documents. The mixed approach is defined by an interpolated estimator ğœ½ , parameterized by ğ›¼ and ğ›½, defined as follows:</p><formula xml:id="formula_5">ğœƒ RULK LM+KW+NE = ğ›¼ ğº RULK LM + ğ›½ ğº RULK KW + (1 -ğ›¼ -ğ›½) ğº RULK NE ,<label>(4)</label></formula><p>where ğº RULK is the estimated knowledge gain according to the respective RULK implementation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">RESULTS AND DISCUSSION</head><p>To answer our first research question (RQ1), we test the validity of the RULK NE by computing the Pearson correlation between the actual knowledge gain of a user and its estimated knowledge gain ğº, as measured by the estimator ğœ½ . We then answer the second research question (RQ2) by comparing the reported correlation to the ones of the other implementations. Table <ref type="table" target="#tab_2">2</ref> shows a comparison between implementations involving named entity NE representations with the baseline.</p><p>The estimated knowledge gain resulting from the proposed approach RULK NE reported a correlation of 0.0931 with the ALG and 0.118 with RPL. While the results of NE alone may look disappointing, it is interesting to notice that all the combinations that include NE outperform all the ones that do not include it. This is clear evidence that the proposed representation is complementary to the others, i.e., capable of capturing something that the others miss. Indeed, the difference between keywords and entities such as those that are stored in a knowledge graph is that keywords in general correspond to individual words, whereas entities correspond to concepts or instances of concepts whose lexicalization may involve phrases. The word embedding produced by the language model too, like keywords, operates at the level of individual words, although, by taking context into account, it can be able to distinguish different meanings of the same word or merge different words in the  Table <ref type="table" target="#tab_4">3</ref> shows the parameters of the mixed models, which are optimized according to the ALG and RPL metrics, respectively. It is intriguing that, while the combinations that involve NE are those that perform best, the weight of NE in those combinations is lower than the weight of the other two implementations, varying between 14% and 20%, meaning a lower contribution to the estimation of the user's knowledge gain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>We have proposed an implementation of the RULK framework using named entities to represent the user's knowledge state and the contents of the documents. The knowledge gain -also said learning outcome -was estimated as the similarity between the tracked user's knowledge state and a target knowledge state expressing the learning objective. The estimated knowledge gain resulting from this framework was compared to the actual user's knowledge gain reported in a search-as-learning study. Our experiments suggest that such representation alone does not lead to an accurate estimate of the knowledge gains. However, by combining this approach to previously proposed ones (namely keyword and language model based), the results outperformed the state-of-the-art baseline by 10%. We proved that named entities representations are complementary to the other representations.</p><p>A promising research direction is to extend this proposal to include relational knowledge in addition to entities, in an attempt to fully exploit the power of knowledge graphs.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Figure 1: The RULK framework and its main components, extracted from [2]. At first, a clicked document ğ‘‘ is transformed into Ã¬ ğ‘£ ğ‘‘ by ğœ¸ . The target knowledge document is also encoded into Ã¬ ğ‘¡ ğ‘˜ğ‘  . Next, ğˆ updates the current state Ã¬ ğ‘ ğ‘˜ğ‘  with Ã¬ ğ‘£ ğ‘‘ . Finally, ğœ½ compares Ã¬ ğ‘ ğ‘˜ğ‘  to a target knowledge Ã¬ ğ‘¡ ğ‘˜ğ‘  to get an estimation of the user's knowledge gain ( ğº) in the session.</figDesc><table><row><cell></cell><cell>ğ‘£ ğ‘‘</cell><cell></cell><cell>ğ‘ ğ‘˜ğ‘ </cell></row><row><cell>Clicked Document</cell><cell>Updater Ïƒ</cell><cell></cell><cell></cell></row><row><cell>Feature</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Extractor ğ›¾</cell><cell>ğ‘¡ ğ‘˜ğ‘ </cell><cell>Estimator</cell><cell>à·¨ ğº</cell></row><row><cell></cell><cell></cell><cell>ğœƒ</cell><cell></cell></row><row><cell>Target Knowledge</cell><cell cols="3">The RULK Framework</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Statistics, per user, extracted from the dataset used by Camara et al.<ref type="bibr" target="#b2">[3]</ref>.</figDesc><table><row><cell></cell><cell>Total</cell><cell>Mean</cell><cell>Median</cell></row><row><cell cols="2">Number of users per topic 126</cell><cell>18.14 Â± 2.79</cell><cell>19.0</cell></row><row><cell>Number of topics</cell><cell>7</cell><cell>-</cell><cell>-</cell></row><row><cell>Number of queries</cell><cell>1095</cell><cell>8.62 Â± 6.47</cell><cell>7.0</cell></row><row><cell>Number of documents clicked</cell><cell>2116</cell><cell>16.66 Â± 8.85</cell><cell>16.0</cell></row><row><cell>Number of snippets seen</cell><cell cols="3">15184 119.56Â±72.43 105.0</cell></row><row><cell>Documents Clicked per query</cell><cell>-</cell><cell>2.78 Â± 2.50</cell><cell>2.11</cell></row><row><cell cols="2">Session duration (minutes) -</cell><cell cols="2">56.18 Â± 14.58 54.05</cell></row><row><cell>Document dwell time (sec-onds)</cell><cell>-</cell><cell cols="2">79.94 Â± 69.77 60.0</cell></row><row><cell>Pre-test scores (ğ‘£ğ‘˜ğ‘  ğ‘ğ‘Ÿğ‘’ )</cell><cell>-</cell><cell>1.07 Â± 1.60</cell><cell>0.00</cell></row><row><cell>Post-test scores (ğ‘£ğ‘˜ğ‘  ğ‘ğ‘œğ‘ ğ‘¡ )</cell><cell>-</cell><cell>6.21 Â± 4.09</cell><cell>6.00</cell></row><row><cell>Actual Learning Gain (ALG)</cell><cell>-</cell><cell>0.53 Â± 0.38</cell><cell>0.50</cell></row><row><cell>Realised Potential Learn-ing (RPL)</cell><cell>-</cell><cell>0.28 Â± 0.20</cell><cell>0.25</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Pearson correlation between the estimated knowledge gain of a given RULK implementation and the actual knowledge gain. bold values indicate the best correlation against a learning metric.</figDesc><table><row><cell>2. The previous approaches</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>RULK KW+LM RULK NE+LM RULK NE+KW RULK NE+LM+KW</figDesc><table><row><cell></cell><cell>ğ›¼</cell><cell>0.44</cell><cell>0.82</cell><cell>-</cell><cell>0.44</cell></row><row><cell>ALG</cell><cell>ğ›½</cell><cell>0.66</cell><cell>-</cell><cell>0.86</cell><cell>0.41</cell></row><row><cell></cell><cell>1 -ğ›¼ -ğ›½</cell><cell>-</cell><cell>0.18</cell><cell>0.14</cell><cell>0.150</cell></row><row><cell></cell><cell>ğ›¼</cell><cell>0.38</cell><cell>0.80</cell><cell>-</cell><cell>0.39</cell></row><row><cell>RPL</cell><cell>ğ›½</cell><cell>0.62</cell><cell>-</cell><cell>0.84</cell><cell>0.44</cell></row><row><cell></cell><cell>1 -ğ›¼ -ğ›½</cell><cell>-</cell><cell>0.20</cell><cell>0.16</cell><cell>0.169</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Comparing parameters with the different mixture models.same meaning. Entities, however, when they are successfully recognized and linked to background knowledge, are much more precise because they are capable of designating very specific concepts. Consider for example two texts like "the President of the United States was in Manchester" and "the President of Manchester United is in the States": after eliminating the stop words, the two texts contain the same keywords (Manchester, President, States, United); in terms of entities, however, they differ: [President of the United States] and [United States] are in the former, while [Manchester United (football team)] is in the latter.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>A search result can be a web pages, videos, online courses, etc. In this paper, a "document" refers to a search result.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>For the sake of simplicity, we assume that ğ‘‘ is always a text-only page.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>Examples of popular OWL reasoners are Fact++, HermiT, and Pellet</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"><p>The data is available at https://github.com/ArthurCamara/CHIIR21-SAL-Scaffolding</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4"><p>https://archive.org/web</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Personal knowledge graphs: A research agenda</title>
		<author>
			<persName><forename type="first">Krisztian</forename><surname>Balog</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Kenter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 ACM SIGIR International Conference on Theory of Information Retrieval</title>
		<meeting>the 2019 ACM SIGIR International Conference on Theory of Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="217" to="220" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">RULK: A Framework for Representing User Knowledge in Search-As-Learning</title>
		<author>
			<persName><forename type="first">Arthur</forename><surname>CÃ¢mara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dima</forename><surname>El Zein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">CÃ©lia</forename><surname>Da</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Costa</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Third International Conference on Design of Experimental Search &amp; Information REtrieval Systems (DESIRES&apos;22)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Searching to Learn with Instructional Scaffolding</title>
		<author>
			<persName><forename type="first">A</forename><surname>CÃ¢mara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nirmal</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Maxwell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Claudia</forename><surname>Hauff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference on Human Information Interaction and Retrieval</title>
		<meeting>the 2021 Conference on Human Information Interaction and Retrieval</meeting>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Constructing Query-Specific Knowledge Bases</title>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Dalton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laura</forename><surname>Dietz</surname></persName>
		</author>
		<idno type="DOI">10.1145/2509558.2509568</idno>
		<ptr target="https://doi.org/10.1145/2509558.2509568" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Workshop on Automated Knowledge Base Construction</title>
		<meeting>the 2013 Workshop on Automated Knowledge Base Construction<address><addrLine>San Francisco, California, USA; New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="55" to="60" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Towards a graph-based user profile modeling for a session-based personalized search</title>
		<author>
			<persName><forename type="first">Mariam</forename><surname>Daoud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lynda-Tamine</forename><surname>Lechani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohand</forename><surname>Boughanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowledge and Information Systems</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="365" to="398" />
			<date type="published" when="2009">2009. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A personalized graph-based document ranking model using a semantic user profile</title>
		<author>
			<persName><forename type="first">Mariam</forename><surname>Daoud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lynda</forename><surname>Tamine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohand</forename><surname>Boughanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on User Modeling, Adaptation, and Personalization</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="171" to="182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">TREC Complex Answer Retrieval Overview</title>
		<author>
			<persName><forename type="first">Laura</forename><surname>Dietz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Gamari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Dalton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Craswell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Trec (NIST Special Publication</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="500" to="331" />
		</imprint>
		<respStmt>
			<orgName>National Institute of Standards and Technology (NIST)</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">spaCy 2: Natural language understanding with Bloom embeddings, convolutional neural networks and incremental parsing</title>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Honnibal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ines</forename><surname>Montani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
	<note>To appear</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">DBpedia spotlight: shedding light on the web of documents</title>
		<author>
			<persName><forename type="first">Pablo</forename><forename type="middle">N</forename><surname>Mendes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Jakob</surname></persName>
		</author>
		<author>
			<persName><forename type="first">AndrÃ©s</forename><surname>GarcÃ­a-Silva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Bizer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th international conference on semantic systems</title>
		<meeting>the 7th international conference on semantic systems</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The Role of Domain Knowledge in Search as Learning</title>
		<author>
			<persName><forename type="first">L</forename><surname>Heather</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>O'brien</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amelia</forename><forename type="middle">W</forename><surname>Kampen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kathleen</forename><surname>Cole</surname></persName>
		</author>
		<author>
			<persName><surname>Brennan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Chiir. Acm</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="313" to="317" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">SearchX: Empowering Collaborative Search Research</title>
		<author>
			<persName><forename type="first">Rikarno</forename><surname>Sindunuraga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felipe</forename><surname>Putra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Claudia</forename><surname>Moraes</surname></persName>
		</author>
		<author>
			<persName><surname>Hauff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Sigir. Acm</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1265" to="1268" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Knowledge Graphs: An Information Retrieval Perspective</title>
		<author>
			<persName><forename type="first">Ridho</forename><surname>Reinanda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edgar</forename><surname>Meij</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maarten</forename><surname>De Rijke</surname></persName>
		</author>
		<ptr target="https://staff.fnwi.uva.nl/m.derijke/wp-content/papercite-data/pdf/reinanda-2020-knowledge.pdf" />
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">153</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">Preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Exploring Users&apos; Learning Gains within Search Sessions</title>
		<author>
			<persName><forename type="first">Nirmal</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felipe</forename><surname>Moraes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Claudia</forename><surname>Hauff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Human Information Interaction and Retrieval</title>
		<meeting>the 2020 Conference on Human Information Interaction and Retrieval</meeting>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Exploring the Feasibility of Crowd-Powered Decomposition of Complex User Questions in Text-to-SQL Tasks</title>
		<author>
			<persName><forename type="first">Sara</forename><surname>Salimzadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ujwal</forename><surname>Gadiraju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Claudia</forename><surname>Hauff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arie</forename><surname>Van Deursen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Ht. Acm</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="154" to="165" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Contemporary Classroom / Vocabulary Assessment / for Content Areas</title>
		<author>
			<persName><forename type="first">Katherine</forename><surname>Anne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dougherty</forename><surname>Stahl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><forename type="middle">A</forename><surname>Bravo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Reading Teacher</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="page" from="566" to="578" />
			<date type="published" when="2010">2010. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Optimizing search results for human learning goals</title>
		<author>
			<persName><forename type="first">Rohail</forename><surname>Syed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevyn</forename><surname>Collins-Thompson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Retrieval Journal</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="506" to="523" />
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Assessing Second Language Vocabulary Knowledge: Depth Versus Breadth</title>
		<author>
			<persName><forename type="first">Marjorie</forename><surname>Bingham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wesche</forename></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Sima</forename><surname>Paribakht</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Canadian Modern Language Review-revue Canadienne Des Langues Vivantes</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="page" from="13" to="40" />
			<date type="published" when="1996">1996. 1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Personal knowledge base construction from text-based lifelogs</title>
		<author>
			<persName><forename type="first">An-Zi</forename><surname>Yen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hen-Hsen</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hsin-Hsi</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="185" to="194" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
