<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">An Integrated Framework for Understanding Multimodal Embodied Experiences in Interactive Virtual Reality</title>
				<funder ref="#_C6phPbC">
					<orgName type="full">French National Research Agency</orgName>
				</funder>
				<funder ref="#_cmVEXGX #_UtBWP3n #_WRSQaYC">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Centre Inria d&apos;</orgName>
								<orgName type="institution" key="instit2">Université Côte d&apos;Azur Sophia-Antipolis</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Lucile Sassatelli</orgName>
								<address>
									<settlement>&apos; d&apos;</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">An Integrated Framework for Understanding Multimodal Embodied Experiences in Interactive Virtual Reality</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">548BDBAC9643B75D5043813075C9A870</idno>
					<idno type="DOI">10.1145/3573381.3596150</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-04-12T14:51+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Human-centered computing → Systems Computing methodologies → Virtual reality Embodied experiences</term>
					<term>3D environments</term>
					<term>immersion</term>
					<term>scene ontology</term>
					<term>task modeling</term>
					<term>interaction</term>
					<term>navigation</term>
					<term>user experience analysis</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Figure 1: Our virtual reality framework based on Dourish's [15] theory of embodiment. Dourish describes three elements to address when bridging gaps of perception between user, system, and designer during user experience analysis, including (1) an environment ontology , (2) the intersubjectivity communication and (3) intentionality communication.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Virtual Reality (VR) offers exciting opportunities for designing embodied experiences -experiences that offer users a high-level of immersion, with life-like scene events, rich interactions, freedom of movement, and the feeling of living and breathing in the 3D world. Inspiring works have shown how embodiment impacts racial <ref type="bibr" target="#b3">[4]</ref> and gender <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b37">38]</ref> bias and how immersive journalism invokes empathy <ref type="bibr" target="#b10">[11]</ref>. These perspectives open exciting avenues of research in domains of education, training, and rehabilitation, where user behaviour can be observed in realistic situations in order to provide personalised content and protocols.</p><p>However, building a VR experience enabling complex interactions while allowing a proper embodied experience analysis includes multiple challenges and limitations. Analysing embodied interactions to interpret and understand the user's experience and intentions is complicated: on the system level, there can be a mismatch between the intentions of the user and the designers' expectations when the experience is first created <ref type="bibr" target="#b15">[16]</ref>; on the individual level, there are non-visible changes in our visual, emotional, motion processing and related sensations that reflect one's intentions <ref type="bibr" target="#b30">[31]</ref>. Creating an environment allowing the fluid and clear communication of intentions between the designer and the user is very important when we want to design personalised applications, and thus a grand challenge for VR media.</p><p>In order to reduce the gaps in perception between the designers, the user, and the system, previous work <ref type="bibr" target="#b45">[46]</ref> identified three major components to develop a framework for embodied experiences, shown in Figure <ref type="figure">1</ref>, that was inspired by Dourish's work on embodiment theory <ref type="bibr" target="#b14">[15]</ref> :</p><p>• Ontology: building semantically understandable 3D environments and scenarios in which the user's interactions (e.g., navigating to a location, picking up an object) can be understood without being precisely explained. • Intersubjectivity: designing real-time visualisation and control systems such that designers can communicate goals and constraints of the scenario to the users (e.g., for the task "open the door", indicate that they need to first find a key). • Intentionality: designing computational methods for the analysis and identification of users' actions and their purpose of enacting an effect on the world (e.g., taking a key in order to open a door enables navigation to a previously inaccessible space).</p><p>Based on Dourish's theory, we used the open GUsT-3D framework <ref type="bibr" target="#b45">[46]</ref> to develop an experimental paradigm in a joint effort with researchers in computer and cognitive sciences, and designed tools that can give better insight into user embodied interactions in VR environments. The chosen study investigates the impact of low-vision conditions on life-sized road crossing scenes. Users were given <ref type="bibr" target="#b0">(1)</ref> various tasks in succession (process visual indices, obtain objects, navigate to points of interest, interact with entities), <ref type="bibr" target="#b1">(2)</ref> under either normal or simulated low-vision conditions (i.e. with a virtual scotoma, a region in the centre of the visual field where visual information is blocked out), <ref type="bibr" target="#b2">(3)</ref> and with real walking or simulated walking using a headset gamepad. The ultimate goal of the study is to investigate the potential of VR for rehabilitation and training for patients with low-vision and other motor-sensory impairments.</p><p>This work focuses not on the results of the study itself, but the implementation of the open framework that enabled the study to be realised: from the design of the interactive task model, to setup and running of the study, to preliminary analyses that investigate the complexity of embodied experiences. The framework presents three main contributions:</p><p>• the definition and encoding of a task model at levels of ontology, intersubjectivity, and intentionality, • realisation of a technical platform with multimodal sensors and freedom of navigation, and • a study and preliminary analysis of 16 participants under this framework to show its potential to support global and fine-grained analyses.</p><p>In the following sections, we first present in section 2 the related work on user experience analysis in VR, and how this work is positioned in respect to the existing. Then we describe in section 3 more precisely the technical setup for embodied experience analysis, the study design, and how our methodology allows us to respond to the three senses of embodiment. Then in section 4 we present the study design and early results in section 5 showing both aggregated results and a fine-grained analysis of user behaviour. Finally, we discuss in section 6 the limitations and present the next steps of this work and the future analyses we are aiming for.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Virtual Reality triggers stronger emotions and sense of presence as compared to traditional 2D media, as the user conduct embodied interaction to exchange with the system, stimulating the brain in a way comparable to real life interactions as shown in Alcañiz et al. <ref type="bibr" target="#b0">[1]</ref>. Multimodal behavioural indices (e.g., motion, attention, emotion) can thus enrich and better characterise the multimodal embodied user experience that VR systems offer, as compared to traditional media.</p><p>In order to address these elements, we base our study of existing work on Dourish's theory <ref type="bibr" target="#b14">[15]</ref>, the most dominant and influential theory in the domain of embodied interactions applied to research in HCI, explaining how the gap of comprehension between user and designer can be created on three levels: the understanding of the environment (ontology), the understanding of the task to perform (intersubjectivity), and the communication of the experience to the designer (intentionality). These three elements are needed for a complete embodied experience analysis. The model proposed by Dourish has the strong advantage of being simple but comprehensive in characterising all the interactions and gaps of perception that take place between the 3 actors (user, designer, environment), which is well aligned with common software architectures in human-computer interactions.</p><p>In this section we review the way VR technologies have been adopted for multimodal user understanding in interactive scenarios. With regards to the three axes of embodied experiences, we review: (1) systems for the study of user behaviour in VR with annotated contexts, (2) a taxonomy of task modelling and interaction for VR, and (3) behaviour understanding in cognitive sciences using VR setups.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Annotated contexts for user understanding</head><p>These past years, with the popularisation of VR technologies, there has been an explosion of research into user understanding in specific contexts. Its potential for applications in contexts that can be considered as highly dangerous or expertise intensive are most notable, such as for understanding driving habits <ref type="bibr" target="#b27">[28]</ref>, sports learning <ref type="bibr" target="#b44">[45]</ref>, and firefighter training <ref type="bibr" target="#b8">[9]</ref>. With rich representations and annotations of the 3D environments, we can envision much more personalised training and feedback protocols.</p><p>There are few works that propose ontology-based methods for context representation in interactive 3D applications. One notable example is that of Bouville et al. <ref type="bibr" target="#b6">[7]</ref> who built the system #FIVE on modelling interactive VR environments through the annotation of elements composing the environment with properties, though with a pre-defined ontology. Kim et al. <ref type="bibr" target="#b24">[25]</ref> used deep learning techniques to construct 3D scene graphs from images to create a simulated environment for robot interactions with real environments. The vocabulary in the scene graph is primarily composed of objects and their visual properties, without knowledge about their interactive capabilities. The recent workflow proposed by Wu et al. <ref type="bibr" target="#b45">[46]</ref> integrates a customizable ontology component in order to define a set of vocabulary representing objects, a limited number of interactive possibilities, and navigation regions.</p><p>The design of VR embodied experiences can vary due to the design goals, target user experience, as well as social, cultural and language contexts. A shift from a universal vocabulary to dynamic vocabulary, an ontology, that can adapt to the situation of usage is thus key to creating personalised experiences for a larger variety of contexts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Interaction modalities and task models</head><p>The Unity game engine is one of the most popular applications used to help researchers in the creation of experiences in VR, with works such as Ugwitz et al. <ref type="bibr" target="#b39">[40]</ref> allowing the trigger and the record of various interactive events in the environment, or Villenave et al. <ref type="bibr" target="#b42">[43]</ref>, allowing the record of multimodal data on the user experience, allowing a visualisation of the data combined for a more comprehensive view of the user experience.</p><p>Few works involve long range (greater than in a 4m x 3m space) navigation tasks with real walking, mainly due to the limits of current VR technologies. Boldt et al. <ref type="bibr" target="#b5">[6]</ref> mention room-scale navigation but cite the HTC Vive Pro's 12m 2 space limitation and do not precise the scale of their study design. They designed a task involving walking to a target and interacting with it, with the goal to investigate the effect of haptic and auditory feedback on wall perception. Mousas et al. <ref type="bibr" target="#b33">[34]</ref> conducted a path following task of 150 meters with a VR backpack, and analysed physiological and pose data to investigate how mismatching virtual and real environments can strongly affect user behaviour. This paper puts to light the fact that both virtual and real environment matter in VR experiences, adding a layer of complexity on the design of a proper VR experiences in which real walking is included. VRoamer <ref type="bibr" target="#b7">[8]</ref> also chooses a VR backpack to allow navigation within a 40m x 40m space, though the work focuses on enabling free navigation and is not task oriented. Finally, studies on perception during navigation in VR with EEG <ref type="bibr" target="#b11">[12]</ref> also used a VR backpack.</p><p>In our work, we present a setup allowing real walking in a contextual environment in a 10m x 4m space by using a wireless module for VR headset. While technical requirements, documentation, and error management of the device was experimental, this turned out to be an efficient solution for natural navigation and sense of presence. To our knowledge, the usage of this technology in a 6DoF movement study has never been done before.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">User cognition understanding</head><p>The question of the relations between VR content, user interactions, and user perception is gaining interest in the area of cognitive neuroscience. We see this from works using cognitive modelling techniques to understand the general relation between emotion and agency <ref type="bibr" target="#b21">[22]</ref>, or properties of the design of content such as visual complexity <ref type="bibr" target="#b17">[18]</ref> and information placement <ref type="bibr" target="#b38">[39]</ref>.</p><p>In parallel, we also see a general rise in the use of methodologies of perception, emotion, and behaviour analysis from neurocognition for the study of the user experience in VR. Alcañiz et al. <ref type="bibr" target="#b1">[2]</ref> apply transcranial Doppler sonography to investigate how VR experiences stimulate the brain, finding them comparable to real life experiences. Dickinson et al. <ref type="bibr" target="#b13">[14]</ref> investigated diegetic interfaces (interface integrated and adapted to the virtual environment) in VR explain that while these type of interfaces show good efficiency in traditional 2D media, in VR, such interfaces did not bring immediate benefits, but did however increase the workload and completion time of users. These works show how the modalities of interactions in VR differ greatly from 2D media, and may require a whole new set of interactive metaphors, and ways to observe and analyse the user experience. Many works such as Dewez et al. <ref type="bibr" target="#b12">[13]</ref>, Aseeri et al. <ref type="bibr" target="#b2">[3]</ref>, Peck et al. <ref type="bibr" target="#b34">[35]</ref> and Ricca et al. <ref type="bibr" target="#b36">[37]</ref> show how embodiment, through body or hands visualisation, is an important aspect of VR interaction, affecting how users will interact with the environment and interact with other users.</p><p>Many studies focus on experience analysis of users in VR, such as the of works Xue et al. <ref type="bibr" target="#b46">[47]</ref> and Guimard et al. <ref type="bibr" target="#b16">[17]</ref>, showing how multiple videos triggering different levels of arousal and valence in the user in order to observe and analyse their physiological responses. These works allowed only 3DoF (3 Degrees of Freedom) head movement, such that users had to stay in-place, reducing the amount of constraints to take into account for the proper analysis of the experience.</p><p>Physiological analysis of VR experiences in 3D environments are often used for multiple purposes such as Keighrey et al. <ref type="bibr" target="#b22">[23]</ref> measuring the QoE (Quality of experience) based on physiological data and interactions, or Bender et al. <ref type="bibr" target="#b4">[5]</ref> quantifying with physiological data the gratification people feel while carrying out violent actions in virtual scenes, such as killing zombies in a VR game. In real-life applications, physiological information have been used in clinical settings to analyse the physiological and mental state of the user for medical purposes for anxiety prediction <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b32">33]</ref> as well as stress and cognitive load assessment <ref type="bibr" target="#b41">[42]</ref>. Other applications uses physiological data for driver stress detection <ref type="bibr" target="#b31">[32]</ref>, and emotion evaluation with smart clothing <ref type="bibr" target="#b35">[36]</ref>.</p><p>The aforementioned review highlights the need of a framework for the comprehensive understanding of the embodied experience in VR in various stages of an application scenario for behavioural studies: from the design of the virtual scenario, to the data recording, and finally in the analyses in order to address the complex interactions between scene context, task design, and user's experience. Most systems and usages we surveyed focus on a subset of these aspects in a well-defined situation of usage. In our work, based on Dourish's <ref type="bibr" target="#b14">[15]</ref> three senses of embodiment, we built a methodology aiming to address a wide range of aspects that could create a gap of perception between a designer and a user, in the environment creation and descriptive ontology, in providing correct user guidance in the scenario, and in the collection of multimodal metrics and subsequent analysis of the user's experience. With a well-controlled environment naturally offered by VR, there is the challenge of providing sufficient control of the experience and the elements affecting it for content designers from non-technical backgrounds, and provide cognitive science researchers with the tools to understand the system and cater it to their research hypotheses, as well as avoid unwanted external factors that may influence the final results. This is the challenge we must address in this work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">METHODS AND MATERIALS</head><p>In a joint effort of computer science, neuroscience, and clinical practitioners, our aim for this study was to investigate the impact that a simulated low vision condition had on user navigation in complex environments, from the analysis of attention, emotion, and behaviour. The complex environment of choice was road crossing scenes: a common daily situation where the difficulty to access and process visual information (e.g., traffic lights, approaching cars) in a timely fashion can lead to serious consequences on a person's safety and well-being. Analysing the impact of low vision and navigation in such situations imposed a number of technical constraints that needed to be fulfilled:</p><p>• Viably capture gaze, physiology, and motion data without the sensors interfering with the tasks. • Free natural walking in a large space -at least the length of a standard two lane road pedestrian crossing. • Logging of user interactions within a scene context -scene annotated with an ontology representing object, interaction, and navigation properties.</p><p>The rest of the section, we present and discuss the design of such a study and the rationale for our design choices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Multimodal sensors</head><p>The metrics chosen shown in Figure <ref type="figure" target="#fig_0">2</ref> were selected in order to be able to rebuild the whole experience by synchronising and correlating the data relevant to the embodied experience (i.e., where was the user, what the user was interacting with, in what state was the environment, in what emotional state was the user, ...). Table <ref type="table" target="#tab_0">1</ref> summarises the modalities of data recorded : system logs, physiological data, motion capture data, and gaze and head motion.</p><p>The framework was made in order to incorporate a large variety of metrics, and be as flexible as possible in order to adapt to specific other potential study conditions. We surveyed a large range of equipment and their usage in existing work. On the technical requirements side, an important motivation for the choice of metrics was the presence of (or possibility to add) a Unix Timestamp, and the minimisation of latency, in order to facilitate the synchronisation of all the data together at any time of the experiment, necessary to the analysis and viability of the results we wish to present.</p><p>Eye tracking headset. Eye tracking is a strong prerequisite for this study, in order to place a virtual scotoma in real time based on the gaze position for the simulated low-vision condition. However, eye tracking data itself is also insightful in an embodied experience analysis, allowing the observation of the user's attention during a scenario, as done in Xue et al. <ref type="bibr" target="#b46">[47]</ref> and Guimard et al. <ref type="bibr" target="#b16">[17]</ref>. We surveyed the HTC Vive Pro Eye<ref type="foot" target="#foot_0">1</ref> , the HP Omnicept Reverb 2 <ref type="foot" target="#foot_1">2</ref> , and the Varjo VR2<ref type="foot" target="#foot_2">3</ref> headset, all of which included more or less equal eye tracking capabilities. The weight of the Varjo VR2 excluded it from our choices. In the end we chose the HTC Vive Pro Eye, which includes by default eye and head tracking functionality. With this headset, the SteamVR application was used for the VR environment configuration. The main factor influencing headset choice was the navigation constraints, which will be detailed in Section 3.2. One issue that is present for most eye tracking devices in VR headsets is latency in the recorded data. The head data on the HTC Vive Pro Eye does not have this delay, which we use as a baseline to align with the eye tracking data before synchronising with the data captured with other equipment.</p><p>Motion capture. Motion capture provides very rich information about embodied experiences, as shown in Mousas et al. <ref type="bibr" target="#b33">[34]</ref> work, using metrics such as step length of participants to evaluate how confident they feel walking in a VR environment. We surveyed two inertia-based motion capture systems: the Rokoko mocap costume <ref type="foot" target="#foot_3">4</ref> with 19 sensors and the MVN Awinda (jacket + 17 sensors) <ref type="foot" target="#foot_4">5</ref>for body motion tracking. The MVN Awinda system was chosen in the end due to its resilience towards magnetic interference as well as the precision of the captured data. The Xsens MVN 2022 software was used to calibrate and record the data, and the Xsens MVN motion cloud was used to convert the records in formats .mvnx and .bvh for later analysis, the .mvnx format being the only one containing Unix Timestamp. Physiology sensors. We decided to use sensors including skin conductance and heart rate, metrics popularly used by Memar et al. <ref type="bibr" target="#b31">[32]</ref>, and Pidgeon et al. <ref type="bibr" target="#b35">[36]</ref> for the evaluation of the level of user emotion arousal. We surveyed the Empatica E4 6 and the GSR Shim-mer3+ 7 which were at similar price ranges and included sensors for skin conductance and heart rate. The GSR Shimmer solution was chosen in the end for its higher data rate (15Hz compared to 4Hz). The Consensys software was used for the configuration and the data record done on the Shimmer's SD card.</p><p>Sensors not interfering with the VR embodied interaction was another important criterion in the design of this setup, as it would affect both user behaviour, and introduce noise into the data. We did multiple test to find the ideal setup and how the user would use it, notably for physiological sensors and controllers, both relying on hands for interaction and data record. Using the non-dominant hand for physiological sensors and the dominant hand for simulated walk on controller was the most efficient solution we found. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Navigating in a large space</head><p>In order to analyse the impact of low vision on navigation in large spaces, such as for our road crossing task, we first have to define the size of the environment. In standard road crossings, the minimum required width of a car lane is 3.5m, and the minimum width of the pedestrian crosswalk was 2.5m with the standard being 4 -6m. We included one meter of pedestrian crossing on each side of a two lane road, and some margin on the sides of the crosswalk to ensure safety. In total, the required navigation space for this study is 10m x 4m, delimited in Figure <ref type="figure" target="#fig_1">3</ref>.</p><p>To find a lightweight solution to navigate in a large space represented the most challenging part of the setup. Existing headsets that have eye tracking capabilities which we were able to survey were all tethered headsets. We therefore had only three options: buy a very long headset cord, use a VR backpack along with other sensors, or find ways to untether the headsets. The first two were discarded after multiple rounds of discussion: the longest available cord for all three headsets would only just allow the user to reach the borders of the space, resulting in tension at the back of the head. With all the required sensors for the study, the VR backpack would add a significant load on the user and impact analysis on the pose.</p><p>In the end, we went with the wireless module <ref type="foot" target="#foot_5">8</ref> for the HTC Vive Pro Eye. However, the module was extremely experimental, mostly relying on community forums for information. There was, for example, no information on the range of the module sensor, nor whether eye tracking data could be collected with the module. We ran multiple tests and were able to verify a number of capabilities of the module shown in Table <ref type="table" target="#tab_1">2</ref>, including the range of navigation we desired, as well as compatibility with the eye tracking data.</p><p>To our knowledge, we are the first to use the wireless module in a study requiring 6DoF movement, with few very recent and notable instances of its usage to facilitate placement of EEG electrodes <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b25">26]</ref>. Nevertheless, it ticked all of the boxes for weight, data collection, and freedom of movement. By removing the cable, pilot testers felt that the movement was much more natural, not restrained by the cable, increasing their feeling of presence. Compared to other existing solutions, we thus decided that we would work around the limitations. We tested two computers, one with a 2080 GTX and i9 CPU, and the second with a 3080 RTX and Xeon CPU, both with the required PCIe slot, but only the latter was able to smoothly lanch the scenes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Navigation space</head><p>The camera needs to be able to visibly "see" the headset at all times. We used four base stations, one at each corner of the space, and were able to calibrate the space of 4m x 10m.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data</head><p>Compatible with Steam VR to collect eye tracking data and 6DoF headset position and rotation data. Battery</p><p>The battery allows roughly two hours of usage Comfort</p><p>The module was relatively light, but continuous usage for longer than an hour could result in overheating and transmission errors, causing significant lag and screen freezes </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">System logging</head><p>We conceived a study with different types of tasks, in two movement conditions and two vision conditions. We then needed a way to annotate the scene with object types and interaction possibilities, then record fine-grained interactions in the scene context. Our task management and logging system is built on the open source workflow 9 of GUsT-3D <ref type="bibr" target="#b45">[46]</ref>, allowing the conception of a vocabulary to describe 3D environments and their interactive properties, as well as the definition, with this vocabulary, of tasks for the user to carry out. The vocabulary ultimately becomes an ontology in the form of a scene graph. This ontology is then used for the creation of scenarios and the guidance of users through it. The system will automatically record all the states and interactions about the environment (i.e., object the user is currently holding, current state of the traffic light, position of the cars) and can be used post-study to provide visualisations to the designer. This workflow appears to be very effective to offer a proper control of the complete experience, from the environment design to the experience analysis, in coherence with Dourish's theory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">STUDY</head><p>We conceived a framework for the analysis of embodied experiences in VR with multimodal data. Using this framework, we design a study to analyse user experience in VR, with different types of 9 https://project.inria.fr/creattive3d/gust-3d/ tasks (observation, movement, grasping) in two different movement conditions (walking physically, walking with a joystick) and two different vision conditions (normal vision, simulated low-vision), for a total of four conditions. The study was reviewed and approved by the Universtié Côte d'Azur's ethics review board (CER).</p><p>This section details the framework, using a task model for subsequent design of the conditions and tasks in the study, and elaborates on the questionnaires that complement the physiological and motion data recorded.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Task model</head><p>Figure <ref type="figure">4</ref>: Modelling tasks supported by the system for the study in a task model. The task model defines the precise the order of tasks, and the subdivision of the tasks in various levels of granularity from abstract to low-level motor tasks. In the study, instructions are given at the level of "Guided User Tasks", and the lower granular motor tasks act as a cursor for intentionality analysis.</p><p>The framework is based on a task model shown in Figure <ref type="figure">4</ref>, created to define all scenario tasks, and visualise all objects and interactions the system should be able to support in the study. To allows fine-grained analysis, knowing precisely what task are done and in what order allows us to synchronise the recorded data, to match the physiological state of the user with the task they did. The tasks were described from the high abstract level of the task such as "go outside" to the motor level of the task such as "approach the key" or "press the game pad trigger to grab the key".</p><p>Choosing at which level to give to the user instructions depends on the type of study and desired subsequent analysis. In our case, this study has the purpose to analyse user behaviour and intention, establishing intersubjectivity with the user, while still leaving a certain amount of freedom such that they may carry out the tasks somewhat differently, allowing a richer analysis of intentionality later on. The intermediate "Guided user task" level in Figure <ref type="figure">4</ref> was therefore selected and transcribed as audio instructions played to the user. Tasks below this level are therefore considered as information, the "intentions" that we want to deduct and analyse from our multimodal data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Task design</head><p>Based on the task model, scenarios were created with the suitable interactions. The task design presented two main constraints:</p><p>• The tasks themselves needed to involve both navigation and interaction according to the task model, in somewhat realistic situations, and with a slight amount of changes between each scene, to avoid learning effects from repeating the exact same scene. • To limit fatigue, the study should fit in two hours, including the time taken to setup, and no more than one hour in the headset. In the end, we considered that for each condition, we could perform six scenarios of one to two minutes long, each with precise properties to trigger certain types of user behaviour. In order to build a dataset composed of a large range of user behaviours, six scenarios were designed around two axis of metrics we wanted to observe on the user experience shown in Figure <ref type="figure" target="#fig_2">5 :</ref> • Cognitive load axis affected by changing the amount of road lanes and cars driving in the VR environment, ranging from 2 lanes with a car on each, to 1 lane with no car at all, as shown in Figure <ref type="figure" target="#fig_3">6</ref>. • Interaction complexity axis affected by changing the amount and the type of interactions asked to the player during the scenario, ranging from only one task asking to pick up one object, to multiple tasks of object interaction, object pick up, object placement, traffic light observation. Figure <ref type="figure" target="#fig_4">7</ref> show a scenario with high interaction complexity. The six scenarios were performed in four conditions: two movement conditions and two visual conditions, for a total of 24 scenarios, as shown in Figure <ref type="figure" target="#fig_5">8:</ref> • Simulated walking the user moves forward or backward using the touchpad of the controller. The direction of movement is determined by the direction of the gamepad, leaving the user's head free to explore the environment. The user could turn on the spot, but physical walking wasn't allowed.    • Real walking the user walks physically and naturally in the 10 meters by 4 meters tracked space. • Normal vision no vision change.</p><p>• Simulated low-vision using eye tracking, a black circle with a diameter of 10°of the foveal field is placed in the centre of the vision of the user, as shown in Figure <ref type="figure" target="#fig_6">9</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Questionnaire</head><p>The questionnaire was split into three parts: pre-and post-study questionnaires, and post-condition questionnaire that were identical following each condition. The questions came globally from existing studies of presence (Witmer &amp; Singer <ref type="bibr" target="#b43">[44]</ref>, emotions (SAM <ref type="bibr" target="#b26">[27]</ref>), simulator sickness (SSQ <ref type="bibr" target="#b23">[24]</ref>, and technology acceptability (UTAUT2 <ref type="bibr" target="#b40">[41]</ref> with the validated French version <ref type="bibr" target="#b19">[20]</ref>). The full list of questions included in the questionnaires is listed in Table <ref type="table">3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Study protocol</head><p>With all the above elements, the experimental protocol is presented in Figure <ref type="figure" target="#fig_5">8</ref>. The participants upon recruitment were sent a message with their booked time slot, guidelines to wear fitting or light attire, as well as the informed consent to permit ample reading time. The study lasted approximately two hours long, and was conducted in either English or French at the preference of the participant. 20 euros compensation was given in the form of a check at the end of the study. At the study time, the participants were first invited to sign the informed consent, answer the pre-study questionnaire, and fitted with the equipment. Participants were briefed on the risks of nausea, fatigue, and motion sickness, and were encouraged to ask for a pause or request ending the study if they felt discomfort, which would not impact their compensation. Snacks and drinks were made available to the participant and offered by the experimenters between conditions and at the end.</p><p>During the study, two experimenters were always present to help arrange the equipment, answer questions, and provide the participant with guidance in using the equipment. When the participant is walking with the headset on, one of the experimenters is always focused on the participants to notice any loose equipment, check for risk of collision or falling. Inflatable mattresses surround the navigation zone (Figure <ref type="figure" target="#fig_1">3</ref>) to prevent any collision with walls or equipment.</p><p>At the beginning of each condition, a pilot scenario (numbered 0) was presented to the participant to help them discover the environment, familiarise with the interactive and navigational modalities in order to lower the learning curve. This scenario is also used to calibrate the headset height, as shown in Figure <ref type="figure" target="#fig_6">9</ref>. The calibration was designed after numerous pilot tests that showed a miscalculation of headset height using the Vive's integrated sensors, and also encouraged users to maintain a more upright pose to avoid instability.</p><p>Following the pilot scenario, each participant then completed six scenarios under the four conditions, for a total of 24 scenarios. The sequence of the conditions and the sequence of scenarios under each condition were pseudo-randomised using the Latin Square attribution to avoid the effect of repetitive learning and fatigue on specific conditions.</p><p>At the end of every three scenarios, participants also performed a spatial perspective taking test <ref type="bibr" target="#b9">[10]</ref>, to quantify the level of presence the user experiences in the environment. During this test, the virtual environment is hidden, and the participant is asked to point with their arm in the direction of a target designated by the audio instruction, usually a salient object with which they interacted during the scenario such as a trash can or the initial location of the key or garbage bag. A strong deviation between the participant's arm and the correct direction would indicate a higher level of spatial disorientation, and likely a reduced level of presence.</p><p>The post-condition questionnaire was presented to the participant after each condition, which also served as a pause period, to re-calibrate motion tracking, eye tracking, physiological sensors, and give the participants the opportunity to ask any questions and declare sensations of fatigue or nausea. At the end of all conditions, the equipment was removed and the final post-study questionnaire was presented to the participant.</p><p>Table <ref type="table">3</ref>: The questionnaire we used, split into pre-experience, post-experience, and post-condition (following each condition). The participants were allowed to skip questions if they did not wish to respond. Set of questions on investment Scale of 1 (not at all) to 10 (very much) on: physical load, mental load, time pressure, success in carrying out tasks, required level of attention to maintain performance, and level of anxiety/discouragement/irritation/annoyance/stress Intensity of emotions Scale of 1 (not at all intense / positive) to 5 (very intense / positive) Positivity of emotions Set of questions concerning mode of navigation Scale of 1 (not at all) to 5 (very strongly): eye strain, physical strain on shoulders/back/legs, nausea, general discomfort, headache, difficulty concentrating, feeling of really walking in the 3D environment Set of questions concerning task design Scale of 1 (not at all) to 5 (very strongly): the task was interesting, the task was repetitive, the mocap equipment interfered with the task, the physiological sensors interfered with the task, the scenes were realistic, feeling of really interacting with the objects Did you have difficulties understanding or accomplishing the tasks? open question</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Did you have any difficulties in general?</head><p>open question Post-experience Set of questions on presence Scale of 1 (not at all) to 7 (very strongly): feeling of really being present in the virtual environment, thoughts of being present in the virtual environment, considered the virtual environment to be closer to somewhere you visited before (as compared to an image seen), dominant feeling of being in the virtual scene (as compared to elsewhere), recollections are similar in structure to an actual memory Set of questions on emotion Scale of 1 (not at all) to 47 (very strongly): interested, anxious, excited, annoyed, irritated, enthusiastic, alert, inspired, attentive UTAUT2 same as in pre-study questionnaire Any additional comments or suggestions open question</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">RESULTS</head><p>To achieve this study and result analysis, we follow the methodology in Figure <ref type="figure" target="#fig_7">10</ref>. The conception of this study was based on a task model which allows us to efficiently represent the generated scenes, list grids for configuration and scene parameters. Following a pilot study, we converged with partners on a merged grid of final parameters, designing observations forms for the formal study.</p><p>After the study, behavioural data was then mapped to contextual data for analysis using the scene annotations and user interaction logs generated based on the task model. We exemplify the potential of this methodology with the collected data in this section by presenting a selection of correlational analysis for UX understanding. We then elaborate the potential for fine-grained analysis of user intentions in scene context and discuss the limitations so far.</p><p>We use a number of abbreviations as follows: The two walking conditions are abbreviated as real walking (RW) and simulated walking (SW). The two visual conditions are abbreviated as normal vision (NV) and simulated low vision (LV). The six scenarios are abbreviated S1-S6 in the order presented in Fig. <ref type="figure" target="#fig_2">5</ref>. The EDA (electrodermal activity) signal is the raw measurement of skin conductance in micro-Siemens (uS).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Demographics and preliminary questionnaire results</head><p>In this first stage of studies, we published an open call to the local university and laboratories. Participants needed to have normal or corrected to normal binocular vision, no motor sensory difficulties, but no other restrictions. As a result, a total of 16 people were recruited (14 men and 2 women) between ages 18 and 34. Three participants used VR for the first time during this study, five only used it once or twice before. In the end, all recruited participants were able to complete the entire study.</p><p>From the questionnaires results, we made two observations on the study design:</p><p>Relation between fatigue and study duration. Despite the study length, participants did not report an increase in the level of fatigue over time, nor any levels of nausea where they required a pause or skipping of conditions. This could be mainly a result of our Relation between condition and cognitive load. On the selfreported scale of 1 (low) to 5 (high) on cognitive load, we observed that the RW+NV condition was clearly the easiest with an average of 2.0, followed by 2.5 for real walking+low vision and 2.8 for SW+NV. Naturally, the SW+LV condition was ranked with the highest cognitive load, with an average of 4.0 on the score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Preliminary analysis of embodied interactions</head><p>Our large variety of data was synchronised by their Unix Timestamp and processed in a Jupyter notebook. The Python toolkit Neurokit <ref type="bibr" target="#b29">[30]</ref> was used to process EDA data, allowing the extraction of the phasic and tonic components. Every person have a very different EDA baseline level, for this reason, we used the normalised EDA, calculated for every person individually by deducting to their EDA data the average EDA of their complete experience. Well known Python libraries were used for data processing, such as Pandas to organise dataframes, NumPy for mathematical functions, and Matplotlib for data visualisation. Combining multimodal data (physiological, motion, gaze, questionnaire, and scenario interaction data) though a notebook allows a clear perception and analysis of the user embodied experience.</p><p>Learning curve. As we can see from Figure <ref type="figure" target="#fig_8">11</ref>, participants are not faster in carrying out the tasks in the third and fourth conditions they encountered, as compared to the first and second one. Participants are however are slower on the last scenarios of the last conditions, which is unexpected. A potential explanation is that, unlike the answers given in the questionnaire, at this moment of the study, participants experienced some fatigue. It shows that the participants usually spent more time on the first scenario of each condition than the scenarios that immediately followed, except for the last condition.</p><p>Relation between scenario and skin conductance. In the top half of Figure <ref type="figure" target="#fig_0">12</ref>, we can see results on the EDA in relation to the scenarios. Based on the original design, shown in Figure <ref type="figure" target="#fig_2">5</ref>, the three most emotionally intensive scenarios are supposed to be the third, fifth and sixth. As displayed by the global data, the sixth scenario indeed resulted in higher arousal (0.064 uS), but the second and third highest arousal was observed in S1 (0.106 uS) and S4 (0.232 uS), both scenario with no cars at all. This echoes a comment two participants had, explaining the fact that when no cars are visible, they were not sure if a car would suddenly appear. In a scenario with a car, they always see where is the car, and at what speed it is moving, making it easier for them to plan their action.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Impact of low vision -first view.</head><p>As shown in the top half of Figure <ref type="figure" target="#fig_0">12</ref>, the LV condition had an impact on both the time to do the conditions, and the EDA measured. The impact on speed is small, taking on average less than 1 second more to finish regardless of walking condition -RW (43.3 to 44.1 seconds) and SW (46.8 to 47.2 Figure <ref type="figure" target="#fig_0">12:</ref> (Top) Average time participants takes to complete scenarios for every condition and the electrodermal activity (EDA) measured during these tasks. (Bottom Left) Average time participants take to cross a road for every condition and the EDA measured during these tasks, (Bottom Right) compared to the average EDA and time to take an object. seconds). The impact is however more noticeable on EDA, rising from -0.145 to -0.058 uS in RW, and from 0.052 to 0.146 uS in SW. In accordance with this results, participants in the questionnaire said that SW+LV was the most mentally demanding task, and often commented that they had difficulty to handle the joystick while monitoring the road traffic, explaining why this condition is the one resulting in the highest arousal.</p><p>Fine-grained task analysis. In the bottom left side of Figure <ref type="figure" target="#fig_0">12</ref>, we can see the time to perform crossing a road task. As shown in the global results, the condition generating the highest EDA is the condition SW+LV. It is interesting to note that between the least arousal-generating condition (RW+NV), and the most arousal generating one (SW+LV) in Figure <ref type="figure" target="#fig_0">12</ref> top half (complete scenario) compared to bottom left side (road crossing task), the difference in EDA rises from 0.291 uS to 0.377 uS. This shows that the SW+LV condition has a stronger impact on participant arousal when they have to cross the road than in the rest of the scenario.</p><p>In the bottom half of Figure <ref type="figure" target="#fig_0">12</ref>, we can see in more detail the impact between a task asking the user to take an object, and the task to cross the road. We can notice that while the road crossing scenario generate much more EDA overall, both have a big increase when going from SW+NV to SW+LV, reported by most participants as the complicated condition, as they have to handle both the walk with the controller and the black dot in the center of the vision, asking for more effort.</p><p>Fine-grained emotion analysis. In Figure <ref type="figure" target="#fig_9">13</ref> we can see the evolution of the EDA of one participant across the different tasks of S2, including one car and simple interactions. Most notably, we observed a momentary sharp rising of EDA when they were honked at by a car while jaywalking during a red light. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Potential in-depth analysis on embodied experiences</head><p>The comprehensiveness of the metrics that were captured in relation to our formalised task model affords many rich directions of analysis from a neuroscience and cognitive science point of view. We noted in particular multiple types of analyses that are yet to be explored:</p><p>Contextual mapping. With the use of granular user interaction logs and the task model, we can correlate behavioural metrics including gaze, emotion, and motion to the actual tasks and interactions the user is carrying out, or the stimuli the user is reacting to, such as the example shown in Figure <ref type="figure" target="#fig_9">13</ref>. This figure illustrates the workflow's capacity to allow designers to observe the evolution of a single participant's emotional intensity over the time of one scenario (5 tasks). It enables a multimodal visualisation of the user experience, combining information regarding the physiological state of the user, and their interactions in the scene.</p><p>Cross modality analysis. Previous analysis of user behaviour has often focused on single modalities, with an exception of few recent works, notably Guimard et al. <ref type="bibr" target="#b16">[17]</ref> who investigate the correlation between emotion, gaze, and content. Multimodal analysis will allow us to both better understand the interactions between modalities of user experiences, while appreciating its complexity.</p><p>Impact of low vision. Thanks to the possibility of conducting real walking in large spaces, as well as the improvement of eye tracking techniques deployed in VR headsets, we can simulate accessibility constraints such as low vision in order to better understand the impact on patients' everyday lives.</p><p>Characterising presence. The question of how to create engaging media content for each individual can help improve the way VR can be used both for entertainment as well as applications in education, rehabilitation, and training. Through correlating questionnaires to emotional responses, we hope to be able to identify factors that can influence the feeling of presence in interactive VR environments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Limitations</head><p>In terms of experimental setup, we noted difficulties correctly capturing the heart rate of participants due to the sensitivity of the equipment. The sensors were placed at the base of the finger, but would easily be disrupted from slight motions or pressing. Another limiting factor were the demographics of the study participants, mostly being from the similar age and gender groups. Diversifying the recruitment of participants will be a top priority for the follow-up studies.</p><p>The focus of the paper is on the framework and its deployment in an actual study, and how the technical platform was established incarnating Dourish's theory. The preliminary analyses we present here are therefore limited, and will be the focus of the next phase of work to investigate the hypotheses posed on the impacts of low vision, the sense of presence, and the potential of VR as a viable tool for training in a clinical setting. Questions on how we can analyse granular data collected from sensors and self-reported metrics in questionnaires will be key to characterising a comprehensive embodied experience, and moving towards a better understanding of user intentions in VR.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>In this work we have established a framework for the design, capture, and analysis for embodied interactions. We validated the feasibility of the framework in a study with the goal to understand the impact of low vision conditions in complex real walking situations, and in the long term to investigate the usage of VR for training and rehabilitation in clinical settings. Our main contribution is the design and encoding of the task model, the implementation of the technical platform to capture of multimodal behavioural indices, and a preliminary presentation of the results to show potential future analyses.</p><p>In the continuation of this work, we will evolve our technical platform based on the limitations we observed and conduct a larger follow-up study. The most important next step of work involves the statistical analysis and modelling of the data, in order to investigate hypotheses in cognitive science on the impact of low vision conditions, and from the perspective of human-computer interactions, establish a model to interpret the data in relation to user intentions. It is our hope and belief that this framework will bring us to yet-unexplored dimensions of embodied experiences in VR.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure2: The recorded data includes sensors in the headset and system -gaze and head tracking and system logs -, motion capture data from the XSens MVN Awinda starter, and skin conductance and heart rate from Shmmer3 GRS+ sensors.</figDesc><graphic coords="6,122.40,83.69,367.19,130.81" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: The environment used for the experience measures 4 by 10 meters in navigation zone is delimited by four base stations, one at each corner, and aligned with the virtual environment. Mattresses surround the zone for security.</figDesc><graphic coords="7,53.80,282.45,240.23,68.42" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: (Right) Our study was comprised of four conditions as a combination of real or simulated walking, and with or without a simulated scotoma as a low vision condition. (Left) Each condition involved six scenarios with varying levels of interaction complexity and cognitive load (number of cars)</figDesc><graphic coords="8,53.80,298.09,240.24,98.45" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Top-down view of the two different type of environments included in the scenarios. One lane road is used for scenario with 1 or less cars, Two lane road is used for 2 cars scenarios.</figDesc><graphic coords="8,317.96,226.58,240.25,76.58" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Participant point of view of the scenario #4 from the study featuring multiple interactions (picking up the trash bag and pushing the traffic button) and crossing a single lane street with no cars.</figDesc><graphic coords="8,317.96,363.74,240.25,177.84" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: The study protocol consists of three main stages: (1) pre-experience preparation including signing the informed consent, a questionnaire and equipping the headset and sensors (2) the study involving seven scenarios per condition (the first of which is a calibration scenario), two perspective taking tasks in the middle and at the end of a condition, and a post-condition questionnaire after each condition, and (3) a post-study questionnaire and removal of equipment. The condition and scenario sequence were pseudo randomised. The entire study lasted roughly two hours.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: (Left) Top-down view of the calibration scenario environment, with no cars nor objects to interact with. In this scene, the participant tests the current walking and visual condition. They are then asked to stand up straight in front of the meter (right side of the figure) to calibrate their height in order to avoid feelings of loss of balance. (Right) Participant view of the simulated scotoma -a region at the centre of the visual field with no visual information -following the gaze of the participant. The scotoma represents 10°in diameter of the foveal field of view, roughly equivalent to the max distance between the index and middle finger with the arm fully stretched.</figDesc><graphic coords="9,122.40,83.69,367.19,107.34" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: Our methodology to configure and analyse the study involving multiple steps that are first and foremost based on a detailed task model. From the model, we then created annotated scenes, and compiled all the study configuration and parameters.Following pilot studies, we then converged with partners on a final merged parameter grid. After the study, we are then able to generate a mapping between the behavioural data, interaction logs, and the scene context based on the task model, facilitating fine-grained analysis on the embodied experience.</figDesc><graphic coords="11,107.10,83.68,397.80,183.28" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 11 :</head><label>11</label><figDesc>Figure 11: The evolution of the average time spent by all participants over the study across conditions and scenarios in the order executed following the Latin Square assignment.It shows that the participants usually spent more time on the first scenario of each condition than the scenarios that immediately followed, except for the last condition.</figDesc><graphic coords="11,317.96,348.66,240.23,70.97" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 13 :</head><label>13</label><figDesc>Figure 13: The EDA of participant 1 during the scenario 2 in RW+NV condition, honked by a car while jaywalking during a red traffic light.</figDesc><graphic coords="12,317.96,359.01,240.24,70.73" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="2,104.24,286.10,403.51,192.49" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>The collected data modalities, the logging frequencies, and a brief description of the data modalities.</figDesc><table><row><cell>Type</cell><cell>Freq.</cell><cell>Description</cell></row><row><cell cols="2">System scene log 10 Hz</cell><cell>objects position, objects state, ob-</cell></row><row><cell></cell><cell></cell><cell>jects interactive properties</cell></row><row><cell>System user log</cell><cell>10 Hz</cell><cell>position, interactions, current task,</cell></row><row><cell></cell><cell></cell><cell>object visibility to the user</cell></row><row><cell>Physiological sen-</cell><cell>15 Hz</cell><cell>Heart Rate (HR), electrodermal ac-</cell></row><row><cell>sors</cell><cell></cell><cell>tivity (EDA, skin conductance)</cell></row><row><cell>Motion capture</cell><cell>60 Hz</cell><cell>17 sensors for head (1), torso (4:</cell></row><row><cell></cell><cell></cell><cell>shoulders, hip, and stern), arms and</cell></row><row><cell></cell><cell></cell><cell>legs (8: upper and lower limb), and</cell></row><row><cell></cell><cell></cell><cell>feet and hands (4)</cell></row><row><cell>Gaze and head</cell><cell>120 Hz</cell><cell>For left, right, and cyclopean eye</cell></row><row><cell>tracking</cell><cell></cell><cell>(combined gaze vector of both eyes):</cell></row><row><cell></cell><cell></cell><cell>gaze vector (x,y,z) , pupil size, eye</cell></row><row><cell></cell><cell></cell><cell>openness percentage, and data va-</cell></row><row><cell></cell><cell></cell><cell>lidity mask</cell></row></table><note><p><p>6 </p>https://www.empatica.com/en-eu/research/e4/ 7 https://shimmersensing.com/product/shimmer3-gsr-unit/</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Technical capacity of the HTC Vive Pro wireless module based on our homemade tests</figDesc><table><row><cell>Technical re-</cell></row><row><cell>quirements</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>https://www.vive.com/eu/product/vive-pro-eye/overview/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>https://www.hp.com/us-en/vr/reverb-g2-vr-headset-omnicept-edition.html</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>https://varjo.com/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"><p>https://www.rokoko.com/products/smartsuit-pro</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4"><p>https://www.movella.com/products/motion-capture/xsens-mvn-awinda</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_5"><p>https://www.vive.com/eu/accessory/wireless-adapter/</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>ACKNOWLEDGMENTS</head><p>This work has been partially supported by the <rs type="funder">French National Research Agency</rs> though the <rs type="projectName">ANR</rs> <rs type="projectName">CREATTIVE3D</rs> project <rs type="grantNumber">ANR-21-CE33-0001</rs> and <rs type="projectName">UCA</rs> <rs type="programName">J EDI Investissements d'Avenir</rs> <rs type="grantNumber">ANR-15-IDEX-0001</rs> (IDEX reference center for extended reality <rs type="grantNumber">XR 2 C 2</rs> ).</p><p>We would also like to thank <rs type="person">Johanna Delachambre</rs>, <rs type="person">Clément Quéré</rs>, <rs type="person">Franz Franco Gallo</rs>, and <rs type="person">Kateryna Pirkovets</rs> for aiding with the user studies, and <rs type="person">Marie-Cécile Lafont</rs> and <rs type="person">Nadia Belfegas</rs> for their administrative support in recruitment, equipment purchases, and compensating user studies.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_C6phPbC">
					<orgName type="project" subtype="full">ANR</orgName>
				</org>
				<org type="funded-project" xml:id="_cmVEXGX">
					<idno type="grant-number">ANR-21-CE33-0001</idno>
					<orgName type="project" subtype="full">CREATTIVE3D</orgName>
				</org>
				<org type="funded-project" xml:id="_UtBWP3n">
					<idno type="grant-number">ANR-15-IDEX-0001</idno>
					<orgName type="project" subtype="full">UCA</orgName>
					<orgName type="program" subtype="full">J EDI Investissements d&apos;Avenir</orgName>
				</org>
				<org type="funding" xml:id="_WRSQaYC">
					<idno type="grant-number">XR 2 C 2</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A neuroscience approach to virtual reality experience using transcranial Doppler monitoring</title>
		<author>
			<persName><forename type="first">Mariano</forename><surname>Alcañiz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Beatriz</forename><surname>Rey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jose</forename><surname>Tembl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vera</forename><surname>Parkhutik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Presence: Teleoperators and Virtual Environments</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="97" to="111" />
			<date type="published" when="2009">2009. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A neuroscience approach to virtual reality experience using transcranial Doppler monitoring</title>
		<author>
			<persName><forename type="first">Mariano</forename><surname>Alcañiz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Beatriz</forename><surname>Rey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jose</forename><surname>Tembl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vera</forename><surname>Parkhutik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Presence: Teleoperators and Virtual Environments</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="97" to="111" />
			<date type="published" when="2009">2009. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The Influence of Avatar Representation on Interpersonal Communication in Virtual Social Environments</title>
		<author>
			<persName><forename type="first">Sahar</forename><surname>Aseeri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victoria</forename><surname>Interrante</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2021.3067783</idno>
		<ptr target="https://doi.org/10.1109/TVCG.2021.3067783" />
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="2608" to="2617" />
			<date type="published" when="2021-05">2021. May 2021</date>
		</imprint>
	</monogr>
	<note>IEEE Transactions on Visualization and Computer Graphics</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Virtual embodiment of white people in a black virtual body leads to a sustained reduction in their implicit racial bias</title>
		<author>
			<persName><forename type="first">Domna</forename><surname>Banakou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Parasuram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mel</forename><surname>Hanumanthu</surname></persName>
		</author>
		<author>
			<persName><surname>Slater</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers in human neuroscience</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">601</biblScope>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Fright, attention, and joy while killing zombies in Virtual Reality: A psychophysiological analysis of VR user experience</title>
		<author>
			<persName><forename type="first">M</forename><surname>Stuart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Billy</forename><surname>Bender</surname></persName>
		</author>
		<author>
			<persName><surname>Sung</surname></persName>
		</author>
		<idno type="DOI">10.1002/mar.21444</idno>
		<ptr target="https://onlinelibrary.wiley.com/doi/pdf/10.1002/mar.21444" />
	</analytic>
	<monogr>
		<title level="j">Psychology &amp; Marketing</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="937" to="947" />
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">You Shall Not Pass: Non-Intrusive Feedback for Virtual Walls in VR Environments with Room-Scale Mapping</title>
		<author>
			<persName><forename type="first">Mette</forename><surname>Boldt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Bonfert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Inga</forename><surname>Lehne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melina</forename><surname>Cahnbley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kim</forename><surname>Korschinq</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Loannis</forename><surname>Bikas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Finke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Hanci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Valentin</forename><surname>Kraft</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Boxuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tram</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alina</forename><surname>Panova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ramneek</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Steenbergen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rainer</forename><surname>Malaka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Smeddinck</surname></persName>
		</author>
		<idno type="DOI">10.1109/VR.2018.8446177</idno>
		<ptr target="https://doi.org/10.1109/VR.2018.8446177" />
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Virtual Reality and 3D User Interfaces (VR)</title>
		<meeting><address><addrLine>Tuebingen/Reutlingen, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018. 2018</date>
			<biblScope unit="page" from="143" to="150" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">#FIVE : High-level components for developing collaborative and interactive virtual environments</title>
		<author>
			<persName><forename type="first">Rozenn</forename><surname>Bouville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Valerie</forename><surname>Gouranton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Boggini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Florian</forename><surname>Nouviale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bruno</forename><surname>Arnaldi</surname></persName>
		</author>
		<idno type="DOI">10.1109/SEARIS.2015.7854099</idno>
		<ptr target="https://doi.org/10.1109/SEARIS.2015.7854099" />
	</analytic>
	<monogr>
		<title level="m">2015 IEEE 8th Workshop on Software Engineering and Architectures for Realtime Interactive Systems (SEARIS)</title>
		<meeting><address><addrLine>Arles, France</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="33" to="40" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">VRoamer: Generating On-The-Fly VR Experiences While Walking inside Large, Unknown Real-World Building Environments</title>
		<author>
			<persName><forename type="first">Lung-Pan</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eyal</forename><surname>Ofek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Holz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">D</forename><surname>Wilson</surname></persName>
		</author>
		<idno type="DOI">10.1109/VR.2019.8798074</idno>
		<ptr target="https://doi.org/10.1109/VR.2019.8798074" />
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Virtual Reality and 3D User Interfaces (VR)</title>
		<meeting><address><addrLine>Osaka, Japan</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019">2019. 2019</date>
			<biblScope unit="page" from="359" to="366" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Creating a Stressful Decision Making Environment for Aerial Firefighter Training in Virtual Reality</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Rory</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sungchul</forename><surname>Clifford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Hoermann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><forename type="middle">W</forename><surname>Billinghurst</surname></persName>
		</author>
		<author>
			<persName><surname>Lindeman</surname></persName>
		</author>
		<idno type="DOI">10.1109/VR.2019.8797889</idno>
		<ptr target="https://doi.org/10.1109/VR.2019.8797889ISSN:2642-5254" />
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Virtual Reality and 3D User Interfaces (VR)</title>
		<meeting><address><addrLine>Osaka, Japan</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019">2019. 2019</date>
			<biblScope unit="page" from="181" to="189" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The role of mental rotation and age in spatial perspective-taking tasks: when age does not impair perspective-taking performance</title>
		<author>
			<persName><forename type="first">Rossana</forename><surname>De Beni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francesca</forename><surname>Pazzaglia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simona</forename><surname>Gardini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Cognitive Psychology: The Official Journal of the Society for Applied Research in Memory and Cognition</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="807" to="821" />
			<date type="published" when="2006">2006. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Immersive journalism: Immersive virtual reality for the first-person experience of news</title>
		<author>
			<persName><forename type="first">Nonny</forename><surname>De La Peña</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peggy</forename><surname>Weil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joan</forename><surname>Llobera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Spanlang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Doron</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maria</forename><forename type="middle">V</forename><surname>Sanchez-Vives</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mel</forename><surname>Slater</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Presence</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="291" to="301" />
			<date type="published" when="2010">2010. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Mobile brain/body imaging of landmark-based navigation with high-density EEG</title>
		<author>
			<persName><forename type="first">Alexandre</forename><surname>Delaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean-Baptiste</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saint</forename><surname>Aubert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Ramanoël</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marcia</forename><surname>Bécu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukas</forename><surname>Gehrke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marius</forename><surname>Klug</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ricardo</forename><surname>Chavarriaga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">José-Alain</forename><surname>Sahel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Klaus</forename><surname>Gramann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angelo</forename><surname>Arleo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">European Journal of Neuroscience</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="8256" to="8282" />
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Towards &amp;#x201c;Avatar-Friendly&amp;#x201d; 3D Manipulation Techniques: Bridging the Gap Between Sense of Embodiment and Interaction in Virtual Reality</title>
		<author>
			<persName><forename type="first">Diane</forename><surname>Dewez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ludovic</forename><surname>Hoyet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anatole</forename><surname>Lécuyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ferran Argelaguet</forename><surname>Sanz</surname></persName>
		</author>
		<idno type="DOI">10.1145/3411764.3445379</idno>
		<ptr target="https://doi.org/10.1145/3411764.3445379" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems (CHI &apos;21)</title>
		<meeting>the 2021 CHI Conference on Human Factors in Computing Systems (CHI &apos;21)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Diegetic Tool Management in a Virtual Reality Training Simulation</title>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Dickinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Cardwell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adrian</forename><surname>Parke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kathrin</forename><surname>Gerling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Murray</surname></persName>
		</author>
		<idno type="DOI">10.1109/VR50410.2021.00034</idno>
		<ptr target="https://doi.org/10.1109/VR50410.2021.00034ISSN:2642-5254" />
	</analytic>
	<monogr>
		<title level="m">IEEE Virtual Reality and 3D User Interfaces (VR)</title>
		<meeting><address><addrLine>Lisboa, Portugal</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021">2021. 2021</date>
			<biblScope unit="page" from="131" to="139" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><forename type="first">Paul</forename><surname>Dourish</surname></persName>
		</author>
		<title level="m">Where the action is: the foundations of embodied interaction</title>
		<meeting><address><addrLine>USA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT press</publisher>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Perception of Affordances and Experience of Presence in Virtual Reality</title>
		<author>
			<persName><forename type="first">Paweł</forename><surname>Grabarczyk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marek</forename><surname>Pokropski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Avant. The Journal of the Philosophical-Interdisciplinary Vanguard</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">0</biblScope>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">PEM360: A dataset of 360 videos with continuous Physiological measurements, subjective Emotional ratings and Motion traces</title>
		<author>
			<persName><forename type="first">Quentin</forename><surname>Guimard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Florent</forename><surname>Robert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Camille</forename><surname>Bauce</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aldric</forename><surname>Ducreux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucile</forename><surname>Sassatelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hui-Yin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Winckler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Auriane</forename><surname>Gros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th ACM Multimedia Systems Conference</title>
		<meeting>the 13th ACM Multimedia Systems Conference<address><addrLine>Athlone, Ireland</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="252" to="258" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Visual Complexity and Scene Recognition: How Low Can You Go?</title>
		<author>
			<persName><forename type="first">Joshua</forename><surname>Peter Handali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Gau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Valentin</forename><surname>Holzwarth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Vom Brocke</surname></persName>
		</author>
		<idno type="DOI">10.1109/VR50410.2021.00051</idno>
		<ptr target="https://doi.org/10.1109/VR50410.2021.00051ISSN:2642-5254" />
	</analytic>
	<monogr>
		<title level="m">2021 IEEE Virtual Reality and 3D User Interfaces (VR)</title>
		<meeting><address><addrLine>Lisboa, Portugal</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="286" to="295" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">VR-based Student Priming to Reduce Anxiety and Increase Cognitive Bandwidth</title>
		<author>
			<persName><forename type="first">Dan</forename><surname>Hawes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Arya</surname></persName>
		</author>
		<idno type="DOI">10.1109/VR50410.2021.00046</idno>
		<ptr target="https://doi.org/10.1109/VR50410.2021.00046ISSN:2642-5254" />
	</analytic>
	<monogr>
		<title level="m">IEEE Virtual Reality and 3D User Interfaces (VR)</title>
		<meeting><address><addrLine>Lisboa, Portugal</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021">2021. 2021</date>
			<biblScope unit="page" from="245" to="254" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">The French eHealth acceptability scale using the unified theory of acceptance and use of technology 2 model: instrument validation study</title>
		<author>
			<persName><forename type="first">Meggy</forename><surname>Hayotte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Thérouanne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laura</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karine</forename><surname>Corrion</surname></persName>
		</author>
		<author>
			<persName><surname>Fabienne D'arripe Longueville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of medical Internet research</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">16520</biblScope>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Virtual reality safety training using deep EEG-net and physiology data</title>
		<author>
			<persName><forename type="first">Dongjin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xianglong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinhua</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinyao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The visual computer</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1195" to="1207" />
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Effects of Emotion and Agency on Presence in Virtual Reality</title>
		<author>
			<persName><forename type="first">Crescent</forename><surname>Jicol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chun</forename><surname>Hin Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Doling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caitlin</forename><forename type="middle">H</forename><surname>Illingworth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinha</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charlotte</forename><surname>Headey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christof</forename><surname>Lutteroth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karin</forename><surname>Proulx</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eamonn O'</forename><surname>Petrini</surname></persName>
		</author>
		<author>
			<persName><surname>Neill</surname></persName>
		</author>
		<idno type="DOI">10.1145/3411764.3445588</idno>
		<ptr target="https://doi.org/10.1145/3411764.3445588" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems</title>
		<meeting>the 2021 CHI Conference on Human Factors in Computing Systems<address><addrLine>Yokohama Japan</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Comparing User QoE via Physiological and Interaction Measurements of Immersive AR and VR Speech and Language Therapy Applications</title>
		<author>
			<persName><forename type="first">Conor</forename><surname>Keighrey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ronan</forename><surname>Flynn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siobhan</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sean</forename><surname>Brennan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niall</forename><surname>Murray</surname></persName>
		</author>
		<idno type="DOI">10.1145/3126686.3126747</idno>
		<ptr target="https://doi.org/10.1145/3126686.3126747" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the on Thematic Workshops of ACM Multimedia 2017 -Thematic Workshops &apos;17</title>
		<meeting>the on Thematic Workshops of ACM Multimedia 2017 -Thematic Workshops &apos;17<address><addrLine>Mountain View, California, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="485" to="492" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Simulator sickness questionnaire: An enhanced method for quantifying simulator sickness</title>
		<author>
			<persName><forename type="first">Norman</forename><forename type="middle">E</forename><surname>Robert S Kennedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><forename type="middle">S</forename><surname>Lane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">G</forename><surname>Berbaum</surname></persName>
		</author>
		<author>
			<persName><surname>Lilienthal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The international journal of aviation psychology</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="203" to="220" />
			<date type="published" when="1993">1993. 1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">3-D Scene Graph: A Sparse and Semantic Representation of Physical Environments for Intelligent Agents</title>
		<author>
			<persName><forename type="first">Ue-Hwan</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jin-Man</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taek-Jin</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jong-Hwan</forename><surname>Kim</surname></persName>
		</author>
		<idno type="DOI">10.1109/TCYB.2019.2931042</idno>
		<ptr target="https://doi.org/10.1109/TCYB.2019.2931042" />
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Cybernetics</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="4921" to="4933" />
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Estimation of Affective States in Virtual Reality Environments using EEG</title>
		<author>
			<persName><forename type="first">Meghan</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Connor</forename><surname>Delaney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dean</forename><surname>Krusienski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th International Conference on PErvasive Technologies Related to Assistive Environments</title>
		<meeting>the 15th International Conference on PErvasive Technologies Related to Assistive Environments<address><addrLine>New York, NY, United States</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="396" to="401" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Behavioral treatment and bio-behavioral assessment: Computer applications</title>
		<author>
			<persName><forename type="first">Peter</forename><surname>Lang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Technology in mental health care delivery systems</title>
		<imprint>
			<biblScope unit="volume">0</biblScope>
			<biblScope unit="issue">0</biblScope>
			<biblScope unit="page" from="119" to="137" />
			<date type="published" when="1980">1980. 1980</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Synthesizing Personalized Training Programs for Improving Driving Habits via Virtual Reality</title>
		<author>
			<persName><forename type="first">Yining</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yibiao</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lap-Fai</forename><surname>Yu</surname></persName>
		</author>
		<idno type="DOI">10.1109/VR.2018.8448290</idno>
		<ptr target="https://doi.org/10.1109/VR.2018.8448290" />
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Virtual Reality and 3D User Interfaces (VR)</title>
		<meeting><address><addrLine>Reutlingen</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018. 2018</date>
			<biblScope unit="page" from="297" to="304" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Investigating implicit gender bias and embodiment of white males in virtual reality with full body visuomotor synchrony</title>
		<author>
			<persName><forename type="first">Sarah</forename><surname>Lopez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Beltran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soo</forename><forename type="middle">Jung</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jennifer</forename><forename type="middle">Cruz</forename><surname>Hernandez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chelsy</forename><surname>Simran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bingkun</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Beste</forename><forename type="middle">F</forename><surname>Yuksel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 CHI Conference on human factors in computing systems</title>
		<meeting>the 2019 CHI Conference on human factors in computing systems<address><addrLine>New York, NY, United States</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">NeuroKit2: A Python toolbox for neurophysiological signal processing</title>
		<author>
			<persName><forename type="first">Dominique</forename><surname>Makowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tam</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><forename type="middle">C</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">François</forename><surname>Brammer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hung</forename><surname>Lespinasse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Schölzel</surname></persName>
		</author>
		<author>
			<persName><surname>Annabel Chen</surname></persName>
		</author>
		<idno type="DOI">10.3758/s13428-020-01516-y</idno>
		<ptr target="https://doi.org/10.3758/s13428-020-01516-y" />
	</analytic>
	<monogr>
		<title level="j">Behavior Research Methods</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1689" to="1696" />
			<date type="published" when="2021-02">2021. feb 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">It feels real: physiological responses to a stressful virtual reality environment and its impact on working memory</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Marieke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angus</forename><surname>Martens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Antley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mel</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><forename type="middle">J</forename><surname>Slater</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elizabeth</forename><forename type="middle">M</forename><surname>Harrison</surname></persName>
		</author>
		<author>
			<persName><surname>Tunbridge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Psychopharmacology</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1264" to="1273" />
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Stress level classification using statistical analysis of skin conductance signal while driving</title>
		<author>
			<persName><forename type="first">Maryam</forename><surname>Memar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amin</forename><surname>Mokaribolhassan</surname></persName>
		</author>
		<idno type="DOI">10.1007/s42452-020-04134-7</idno>
		<ptr target="https://doi.org/10.1007/s42452-020-04134-7" />
	</analytic>
	<monogr>
		<title level="j">SN Applied Sciences</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">64</biblScope>
			<date type="published" when="2021-01">2021. Jan. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Real-time Anxiety Prediction in Virtual Reality Exposure Therapy</title>
		<author>
			<persName><forename type="first">Deniz</forename><surname>Mevlevioğlu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sabin</forename><surname>Tabirca</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">New York City</title>
		<imprint>
			<biblScope unit="volume">0</biblScope>
			<biblScope unit="issue">0</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Real and virtual environment mismatching induces arousal and alters movement behavior</title>
		<author>
			<persName><forename type="first">Christos</forename><surname>Mousas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dominic</forename><surname>Kao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandros</forename><surname>Koilias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Banafsheh</forename><surname>Rekabdar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)</title>
		<meeting><address><addrLine>Atlanta, GA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="626" to="635" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">The Effect of Gender Body-Swap Illusions on Working Memory and Stereotype Threat</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">C</forename><surname>Peck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Doan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">A</forename><surname>Bourne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Good</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2018.2793598</idno>
		<ptr target="https://doi.org/10.1109/TVCG.2018.2793598" />
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization &amp; Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="1604" to="1612" />
			<date type="published" when="2018-04">2018. apr 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Federated Learning to Understand Human Emotions via Smart Clothing: Research Proposal</title>
		<author>
			<persName><forename type="first">Mary</forename><surname>Pidgeon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nadia</forename><surname>Kanwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niall</forename><surname>Murray</surname></persName>
		</author>
		<idno type="DOI">10.1145/3524273.3533936</idno>
		<ptr target="https://doi.org/10.1145/3524273.3533936" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th ACM Multimedia Systems Conference</title>
		<meeting>the 13th ACM Multimedia Systems Conference<address><addrLine>Athlone, Ireland; New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="408" to="411" />
		</imprint>
	</monogr>
	<note>) (MM-Sys &apos;22)</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">The influence of hand visualization in tool-based motor-skills training, a longitudinal study</title>
		<author>
			<persName><forename type="first">Aylen</forename><surname>Ricca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amine</forename><surname>Chellali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samir</forename><surname>Otrnane</surname></persName>
		</author>
		<idno type="DOI">10.1109/VR50410.2021.00031</idno>
		<ptr target="https://doi.org/10.1109/VR50410.2021.00031ISSN:2642-5254" />
	</analytic>
	<monogr>
		<title level="m">IEEE Virtual Reality and 3D User Interfaces (VR)</title>
		<meeting><address><addrLine>Lisboa, Portugal</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021">2021. 2021</date>
			<biblScope unit="page" from="103" to="112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">The effects of embodiment in virtual reality on implicit gender bias</title>
		<author>
			<persName><forename type="first">Stephanie</forename><surname>Schulze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Toni</forename><surname>Pence</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ned</forename><surname>Irvine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Curry</forename><surname>Guinn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Multimodal Interaction: 11th International Conference, VAMR 2019, Held as Part of the 21st HCI International Conference, HCII 2019</title>
		<meeting><address><addrLine>Orlando, FL, USA; USA</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019-07-26">2019. July 26-31, 2019</date>
			<biblScope unit="page" from="361" to="374" />
		</imprint>
	</monogr>
	<note>Virtual, Augmented and Mixed Reality. Part I 21</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Impact of Information Placement and User Representations in VR on Performance and Embodiment</title>
		<author>
			<persName><forename type="first">Sofia</forename><surname>Seinfeld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tiare</forename><surname>Feuchtner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Pinzek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jörg</forename><surname>Müller</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2020.3021342</idno>
		<ptr target="https://doi.org/10.1109/TVCG.2020.3021342" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note>IEEE transactions on visualization and computer graphics Publication Title: IEEE transactions on visualization and computer graphics</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Toggle toolkit: A tool for conducting experiments in unity virtual environments</title>
		<author>
			<persName><forename type="first">Pavel</forename><surname>Ugwitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alžběta</forename><surname>Šašinková</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Čeněk</forename><surname>Šašinka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zdeněk</forename><surname>Stachoň</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vojtěch</forename><surname>Juřík</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Behavior research methods</title>
		<imprint>
			<biblScope unit="volume">0</biblScope>
			<biblScope unit="issue">0</biblScope>
			<biblScope unit="page" from="1" to="11" />
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Consumer acceptance and use of information technology: extending the unified theory of acceptance and use of technology</title>
		<author>
			<persName><forename type="first">Viswanath</forename><surname>Venkatesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">Yl</forename><surname>Thong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">MIS quarterly</title>
		<imprint>
			<biblScope unit="volume">0</biblScope>
			<biblScope unit="issue">0</biblScope>
			<biblScope unit="page" from="157" to="178" />
			<date type="published" when="2012">2012. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">The Effects of Cognitive Load on Engagement in a Virtual Reality Learning Environment</title>
		<author>
			<persName><forename type="first">Jhon</forename><surname>Bueno Vesga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinhao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>He</surname></persName>
		</author>
		<idno type="DOI">10.1109/VR50410.2021.00090</idno>
		<ptr target="https://doi.org/10.1109/VR50410.2021.00090ISSN:2642-5254" />
	</analytic>
	<monogr>
		<title level="m">IEEE Virtual Reality and 3D User Interfaces (VR)</title>
		<meeting><address><addrLine>Lisboa, Portugal</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021">2021. 2021</date>
			<biblScope unit="page" from="645" to="652" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">XREcho: A Unity plug-in to record and visualize user behavior during XR sessions</title>
		<author>
			<persName><forename type="first">Sophie</forename><surname>Villenave</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Cabezas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Baert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Florent</forename><surname>Dupont</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Lavoué</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th ACM Multimedia Systems Conference</title>
		<meeting>the 13th ACM Multimedia Systems Conference<address><addrLine>Athlone, Ireland</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="341" to="346" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Measuring presence in virtual environments: A presence questionnaire</title>
		<author>
			<persName><forename type="first">G</forename><surname>Bob</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Witmer</surname></persName>
		</author>
		<author>
			<persName><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Presence</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="225" to="240" />
			<date type="published" when="1998">1998. 1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">SPin-Pong -Virtual Reality Table Tennis Skill Acquisition using Visual, Haptic and Temporal Cues</title>
		<author>
			<persName><forename type="first">Erwin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mitski</forename><surname>Piekenbrock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Takuto</forename><surname>Nakumura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hideki</forename><surname>Koike</surname></persName>
		</author>
		<idno type="DOI">10.1109/TVCG.2021.3067761</idno>
		<ptr target="https://doi.org/10.1109/TVCG.2021.3067761" />
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="2566" to="2576" />
			<date type="published" when="2021-05">2021. May 2021</date>
		</imprint>
	</monogr>
	<note>IEEE Transactions on Visualization and Computer Graphics</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Designing Guided User Tasks in VR Embodied Experiences</title>
		<author>
			<persName><forename type="first">Hui-Yin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Florent</forename><surname>Robert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Théo</forename><surname>Fafet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brice</forename><surname>Graulier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barthélemy</forename><surname>Passin-Cauneau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucile</forename><surname>Sassatelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Winckler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM on Human-Computer Interaction</title>
		<meeting>the ACM on Human-Computer Interaction</meeting>
		<imprint>
			<publisher>EICS</publisher>
			<date type="published" when="2022">2022. 2022</date>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="1" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">CEAP-360VR: A Continuous Physiological and Behavioral Emotion Annotation Dataset for 360 VR Videos</title>
		<author>
			<persName><forename type="first">Abdallah</forename><forename type="middle">El</forename><surname>Tong Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyi</forename><surname>Ali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gangyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pablo</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><surname>Cesar</surname></persName>
		</author>
		<idno type="DOI">10.1109/TMM.2021.3124080</idno>
		<ptr target="https://doi.org/10.1109/TMM.2021.3124080" />
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">0</biblScope>
			<biblScope unit="issue">0</biblScope>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
