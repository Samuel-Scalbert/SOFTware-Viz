<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Low-cost Multispectral Scene Analysis with Modality Distillation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Heng</forename><surname>Zhang</surname></persName>
							<email>heng.zhang@irisa.fr</email>
						</author>
						<author>
							<persName><forename type="first">Elisa</forename><surname>Fromont</surname></persName>
							<email>elisa.fromont@irisa.fr</email>
						</author>
						<author>
							<persName><forename type="first">Sébastien</forename><surname>Lefevre</surname></persName>
							<email>sebastien.lefevre@irisa.fr</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Univ Rennes</orgName>
								<orgName type="institution" key="instit2">Inria</orgName>
								<orgName type="institution" key="instit3">CNRS</orgName>
								<orgName type="institution" key="instit4">IRISA</orgName>
								<address>
									<postCode>F-35000</postCode>
									<settlement>Rennes</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">Univ Rennes</orgName>
								<orgName type="institution" key="instit2">IUF</orgName>
								<orgName type="institution" key="instit3">Inria</orgName>
								<orgName type="institution" key="instit4">CNRS</orgName>
								<orgName type="institution" key="instit5">IRISA</orgName>
								<address>
									<postCode>F-35000</postCode>
									<settlement>Rennes</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution" key="instit1">Univ Bretagne Sud</orgName>
								<orgName type="institution" key="instit2">IRISA</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">Bruno Avignon ATERMES company</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Low-cost Multispectral Scene Analysis with Modality Distillation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">355D6BB5D49DDEE8D2DEB1603EC01467</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-04-12T14:48+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Despite its robust performance under various illumination conditions, multispectral scene analysis has not been widely deployed due to two strong practical limitations: 1) thermal cameras, especially high-resolution ones are much more expensive than conventional visible cameras; 2) the most commonly adopted multispectral architectures, twostream neural networks, nearly double the inference time of a regular mono-spectral model which makes them impractical in embedded environments. In this work, we aim to tackle these two limitations by proposing a novel knowledge distillation framework named Modality Distillation (MD). The proposed framework distils the knowledge from a high thermal resolution two-stream network with featurelevel fusion to a low thermal resolution one-stream network with image-level fusion. We show on different multispectral scene analysis benchmarks that our method can effectively allow the use of low-resolution thermal sensors with more compact one-stream networks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Vision applications such as autonomous driving or remote surveillance need to maintain high reliability under various conditions, such as insufficient illumination or adverse weather. These situations are challenging for systems using only visible cameras, which is why multispectral systems introduce additional thermal cameras to provide supplementary information. In particular, visible cameras provide visual details of colour and texture, while thermal cameras are sensitive to temperature changes, thus their contributions are complementary and their combination can en-sure reliable recognition performance round-the-clock.</p><p>Under the conventional settings of multispectral scene analysis, thermal cameras and visible ones must provide image pairs with identical perception fields and identical spatial resolution. The former requirement can be achieved through camera calibration. However, due to the extreme price gap between high-resolution visible and thermal cameras <ref type="foot" target="#foot_0">1</ref> , the requirement of identical spatial resolution usually leads to either 1) visible image downsampling that may cause information loss or 2) high manufacturing costs for thermal cameras that prevent massive production. From a practical point of view, using a high-resolution visible camera and a low-resolution thermal one would be the best compromise in performance/price.</p><p>Another constraint from the current multispectral systems lies in the software part. Nowadays, deep learningbased methods dominate the field of (multispectral) scene analysis. Multispectral information fusion methods can be categorized into: image-level fusion, feature-level fusion and decision-level fusion. Architectures that implement a feature-level fusion, usually within a two-stream network architecture (one dedicated to each source), have been proven to outperform the other strategies, and are currently the most studied in the literature <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b7">8]</ref>. However, the computational overhead provided by two-stream networks is huge, which is particularly undesirable for software deployment on embedded devices.</p><p>In this paper, we propose a novel knowledge distillation framework named Modality Distillation (MD) to tackle the aforementioned hardware and software constraints. This framework follows two steps: Firstly, a multispectral system with high-resolution visible and thermal cameras is used to collect training data and to learn a precise but complex two-stream neural network for scene analysis. This model will be used as a teacher model with fixed weights. Secondly, a more efficient image-level fusion student model is trained with high-resolution visible images and downsampled thermal images to simulate production systems that are equipped with more economical low-resolution thermal cameras. The knowledge from the teacher model is transferred to the student model to mimic the more accurate feature-level fusion architecture and to reconstruct highresolution thermal details. We performed extensive experiments for multispectral pedestrian detection <ref type="bibr" target="#b10">[11]</ref> and semantic segmentation <ref type="bibr" target="#b6">[7]</ref>. In both tasks, our model strongly reduces thermal camera requirements (resolution divided by 16) and achieves substantial inference acceleration (runtime divided by, at least, 1.8), with minor precision drop compared to full-resolution two-stream teacher networks which makes the deployment of multiple low-cost student networks on embedded devices much more viable.</p><p>The rest of this paper is organized as follows: In Section 2, we provide some representative work on multispectral scene analysis and knowledge distillation. Section 3 details the proposed method. In section 4, we conduct various experiments to study the effects of MD under different thermal resolutions and compare our results to state-of-the-art methods. Section 5 concludes the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>Our review of the related work mainly focuses on multispectral scene analysis and knowledge distillation, the two critical techniques to build the proposed framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Multispectral scene analysis</head><p>The first dataset for pedestrian detection from visiblethermal image pairs was introduced in <ref type="bibr" target="#b10">[11]</ref> and then various deep learning-based methods have been proposed to tackle this problem. In <ref type="bibr" target="#b21">[22]</ref>, the authors compared, on this dataset, two different fusion strategies: image-level fusion (called early fusion) and feature-level fusion (there called late fusion because it is at the last possible feature level). Early fusion combines the information from the two modalities by directly concatenating visible and thermal images. Late fusion methods usually apply a two-stream architecture which employs two separate feature extraction networks (for visible and thermal images respectively) and combine the multispectral information by feature concatenation. <ref type="bibr" target="#b21">[22]</ref> concluded that feature-level fusion methods produce superior performance, whereas image-level fusion ones cannot even surpass traditional methods (such as Aggregated Channel Features (ACF) <ref type="bibr" target="#b3">[4]</ref>). To the best of our knowledge, worldwide research on image-level fusion for multispectral image analysis has been mostly interrupted since these findings. The research focus has then shifted to feature-level fusion: <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b12">13]</ref> studied the optimal fusion "timing" in the detection network and came to the same conclusion that halfway feature fusion produces better results; <ref type="bibr" target="#b13">[14]</ref> introduced an auxiliary segmentation task on the basis of halfway feature fusion for further performance improvements. <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b26">27]</ref> applied attention mechanisms to adaptively weigh the visible and thermal features in the feature fusion stage; <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b31">32]</ref> alleviated the inconsistency between visible and thermal features to facilitate the optimization process of a dualmodality network. Apart from these studies on featurelevel fusion, multiple decision-level fusion methods were suggested: <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b14">15]</ref> used illumination information to guide the fusion of predictions (decisions) from visible/thermal images or from day/night sub-networks; <ref type="bibr" target="#b29">[30]</ref> discussed a confidence-aware fusion mechanism, where the disagreement between visible and thermal predictions is used to reweigh visible contributions, which could also be regarded as a decision-level fusion approach.</p><p>Compared to multispectral pedestrian detection, multispectral semantic segmentation is a younger research topic, and most related methods are based on feature-level fusion. Some relevant datasets and baseline methods were introduced in <ref type="bibr" target="#b6">[7]</ref>. Similarly to other feature fusion methods, the baseline method also adopts two separate feature extractors for visible and thermal images respectively. Moreover, short-cut blocks were designed to concatenate the extracted multispectral feature maps. Based on this, <ref type="bibr" target="#b20">[21]</ref> adopted stronger feature extraction networks to further boost segmentation accuracy. However, because of this complexity, the inference speed of their models was much slower. On a different type of data, <ref type="bibr" target="#b7">[8]</ref> tackled the RGB-D semantic segmentation task, the architecture of which is also applicable for visible-thermal inputs. Therefore, we also include this model for comparison in our experiments.</p><p>Previous studies on multispectral scene analysis aimed to improve detection/segmentation accuracy, neglecting the computational cost and the effectiveness of operational deployment. In this paper, we rather exploit the (forgotten) potentialities of image-level fusion architectures, which have a similar complexity as mono-spectral networks. Moreover, we also deal with the issue of reducing thermal camera resolution, which has never been discussed in the literature despite the great interest in practical multispectral applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Knowledge distillation</head><p>Knowledge distillation is a method for inheriting the knowledge learnt from one or multiple pre-trained teacher models to a student model. This concept was firstly introduced in <ref type="bibr" target="#b9">[10]</ref>, where the training objective of a student model is the prediction produced by the teacher models. The use of this method strongly improved the accuracy of the student model. <ref type="bibr" target="#b19">[20]</ref> proposed to distil features instead of predictions. Concretely, features from internal layers of a student model should mimic that of a teacher model. <ref type="bibr" target="#b11">[12]</ref> suggested that directly parsing features as guidance might be difficult, therefore, they transferred attention maps (generated in an unsupervised manner) that function as a summary of the whole feature maps.</p><p>Apart from image classification, knowledge distillation is also widely used in other scene analysis tasks. <ref type="bibr" target="#b0">[1]</ref> proposed the first knowledge distillation framework for object detection, which includes a distillation loss on both the feature network and the detection head. Instead of distilling all feature maps, <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b22">23]</ref> applied knowledge distillation only on ROI-sampled features or features near object bounding boxes to alleviate the extreme imbalance between foreground and background. In terms of semantic segmentation, <ref type="bibr" target="#b23">[24]</ref> distilled both the fine annotated images and unlabelled auxiliary data to regularize the training of a student segmentation model; <ref type="bibr" target="#b18">[19]</ref> proposed to distil the structured knowledge for a semantic segmentation task.</p><p>We take inspiration from previous work and propose two specific knowledge distillation methods for multispectral scene analysis: Attention transfer and Semantic transfer. The former generates teacher attention maps (masks) via Guided Attentive Feature Fusion (GAFF) <ref type="bibr" target="#b26">[27]</ref> from a two-stream teacher network. These masks are then transferred to a one-stream student network (thus performing an image-level fusion). The latter tackles the imbalance problem in feature distillation through a novel "Focal Mean Square Error" loss. Moreover, contrary to previous works where the objective is to transfer knowledge from a "larger" teacher network into a "smaller" student network (e.g., from ResNet-101 to ResNet-18), our objective is instead to transfer from a high thermal resolution two-stream multispectral network into a low thermal resolution one-stream multispectral network, while the "base" network remains unchanged (e.g., we use ResNet-18 for all experiments).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Modality Distillation</head><p>This section starts with an overview of the proposed Modality Distillation (MD) framework. We then briefly provide the basic concepts of <ref type="bibr" target="#b26">[27]</ref> that are used in our framework. Finally, the two proposed knowledge distillation modules are described in details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Overview</head><p>As illustrated in Fig. <ref type="figure" target="#fig_0">1(A</ref> works, a GAFF <ref type="bibr" target="#b26">[27]</ref> module for multispectral fusion and a task-specific network for pedestrian detection/semantic segmentation. Contrarily to the teacher model, the student model uses a low-resolution thermal input and a onestream feature extraction network that takes as input the image-level fusion of both modalities (through different input channels). We also conduct distillation experiments for a student model without the thermal modality, i.e., in this particular case, we attempt to use a multispectral teacher to improve the performance of a visible-only student.</p><p>The proposed MD framework includes two training stages. In the first stage, we train the teacher model and fix its weights, such that the fused features from GAFF module contain the rich semantics of high-resolution thermalvisible image pairs. These features are used to guide the training of the student model; In the second stage, the optimization of the student model is supervised by a taskspecific loss (e.g., pedestrian detection or semantic segmentation loss) as well as the knowledge transfer loss. The objective of the knowledge transfer loss is two-fold: using a more efficient one-stream network to mimic a more precise two-stream network and using the more available lowresolution thermal images to reconstruct high-resolution thermal details. Finally, we obtain a student model that takes low-resolution thermal images as input, and the required parameters and calculations are greatly reduced. Meanwhile, the precision of the low thermal resolution onestream student model is supposed to be close to the high thermal resolution two-stream teacher model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Guided attentive feature fusion</head><p>We carefully follow the implementations of GAFF module from <ref type="bibr" target="#b26">[27]</ref> for multispectral feature fusion in the teacher model. GAFF learns the intra-and inter-modality attention masks to adaptively enhance important areas and to identify reliable modalities, respectively. Intra-modality attention is used to distinguish the foreground and background for each individual modality, and the inter-modality attention is used to select features among visible and thermal modalities according to the dynamic comparison of their prediction quality of the former mask. 2 . The weighted visible and thermal features are obtained via:</p><formula xml:id="formula_0">f visible weighted = f visible ⊗ (1 + m visible intra ) ⊗ (1 + m visible inter ) f thermal weighted = f thermal ⊗ (1 + m thermal intra ) ⊗ (1 + m thermal inter ) (1)</formula><p>where f visible and f thermal denote features from visible and thermal feature extraction branches. m intra and m inter are predicted intra-and inter-modality attention masks from GAFF. Their superscript indicates the modality.</p><p>The fusion of the weighted visible and thermal features are assigned as the teacher features:</p><formula xml:id="formula_1">f teacher = f visible weighted + f thermal weighted 2<label>(2)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Knowledge transfer modules</head><p>To preserve the knowledge learnt from the teacher model to the maximum extent, we apply two knowledge transfer strategies: Attention transfer that guides the one-stream model to mimic the two-stream attentive fusion, and Semantic transfer that rebuilds high-resolution visual details from low-resolution thermal images.</p><p>Attention transfer. GAFF significantly improves the scene analysis performance in a two-stream teacher model. However, such a multispectral feature fusion module does not exist in a one-stream student model. Thus, as illustrated in the left part of Fig. <ref type="figure" target="#fig_0">1</ref>(B), we design the Attention transfer module to simulate this attentive fusion in a one-stream model. The teacher attention mask is the combination of intra-and inter-modality attention masks. To keep the architecture simple, the student attention mask is generated by a 1 × 1 convolution followed by a Sigmoid activation, and is supervised by minimizing the Dice loss <ref type="bibr" target="#b2">[3]</ref> between 2 Due to space constraint, we refer the reader to <ref type="bibr" target="#b26">[27]</ref> for more details.</p><p>the student and teacher attention masks:</p><formula xml:id="formula_2">m teacher = m visible intra ⊗ m visible inter + m thermal intra ⊗ m thermal inter m student = F(f student ) L attention = 1 - 2|m student ⊗ m teacher | |m student | + |m teacher |<label>(3</label></formula><p>) where L attention denotes the Attention transfer loss; m teacher and m student represent the teacher and student attention masks respectively; f student denotes the student features acquired from the joint feature branch; F represents a 1 × 1 convolution followed by a Sigmoid activation; ⊗ and || represent respectively the pixel-wise multiplication and summation operation.</p><p>Semantic transfer. To compensate for the resolution reduction of thermal input images, the Semantic transfer module performs an implicit super-resolution of student feature maps. As shown in the right part of Fig. <ref type="figure" target="#fig_0">1(B)</ref>, we use a basic residual block <ref type="bibr" target="#b8">[9]</ref> to increase the details in the joint features. Semantic transfer aims to minimize the distance between the student (joint) and teacher (fused) feature maps. However, optimizing this distance has proven to be difficult. At first glance, this is due to the extreme imbalance between the foreground and background areas <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b22">23]</ref>. Inspired by the Focal loss <ref type="bibr" target="#b16">[17]</ref>, we argue that the true problem lies in the extreme imbalance between easily-mimic and hardly-mimic areas. Therefore, we propose the Focal Mean Square Error (F-MSE) loss defined as:</p><formula xml:id="formula_3">d = (f student -f teacher ) 2 L semantic = w h 1 n (δ( n d) × n d)<label>(4)</label></formula><p>where L semantic denotes the Semantic transfer loss; d is the squared L2 distance between student and teacher feature maps. δ signifies the Softmax function; w, h, n represent the width, height and depth of feature maps, respectively. The major difference between the proposed F-MSE loss and the standard MSE loss (used in <ref type="bibr" target="#b19">[20]</ref>) is the spatial reweighting based on feature-mimicking errors. Concretely, the Softmax function generates a 2-D re-weighting mask, where each value reflects the difficulty of feature mimicking on a specific area, and the summation of all values on the mask is equal to 1. In such a manner, the optimization adaptively "focuses" on mis-predicted areas, and the imbalance problem is therefore solved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section, we conduct experiments on two multispectral benchmarks <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b6">7]</ref> and two scene analysis tasks (pedestrian detection and semantic segmentation) to evaluate the effectiveness of the proposed Modality Distillation (MD) framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets</head><p>KAIST. The KAIST multispectral pedestrian detection dataset <ref type="bibr" target="#b10">[11]</ref> (denoted as KAIST Dataset) focuses on the pedestrian detection task based on aligned multispectral image pairs. These image pairs are collected during daytime and nighttime. This dataset contains 21,622 annotated image pairs for training, and 2,252 image pairs for testing. Due to the problematic annotations from the original dataset, we adopt the improved annotations proposed by <ref type="bibr" target="#b29">[30]</ref> and <ref type="bibr" target="#b17">[18]</ref> for training and evaluation, respectively. Following previous works, we adopt the log-average Miss Rate (computed by averaging the miss rate on false positive per-image points sampled within the range of 10 -2 , 10 0 , lower is better) under a "reasonable" setting <ref type="bibr" target="#b4">[5]</ref> (pedestrians that are occluded or shorter than 55 pixels are eliminated) as the evaluation metric.</p><p>MFNet. The Multispectral semantic segmentation dataset <ref type="bibr" target="#b6">[7]</ref> (denoted as MFNet Dataset) targets the semantic segmentation of street scenes for autonomous vehicles. The segmentation labels consist of eight classes: car, person, bike, curve, car stop, guardrail, colour cone and bump. It provides 1,568 aligned multispectral image pairs in the training set, 392 pairs in the validation set and 393 pairs in the test set. Among each subset, half of the image pairs are taken during daytime, and the other half during nighttime. To evaluate the segmentation accuracy, we report the class-wise Mean Accuracy, calculated by averaging the ratio between the number of true positive pixels and the sum of true positive and false negative pixels for each class.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Implementation details</head><p>Network architecture. For all experiments, we apply ResNet-18 <ref type="bibr" target="#b8">[9]</ref> as the feature extraction network, RetinaNet <ref type="bibr" target="#b16">[17]</ref> (its label assignment is optimized via Mutual Guidance strategy <ref type="bibr" target="#b24">[25]</ref>) as the pedestrian detection network and PSPNet <ref type="bibr" target="#b30">[31]</ref> as the semantic segmentation network. For the teacher model (Fig. <ref type="figure" target="#fig_0">1</ref>(A) upper model), GAFF <ref type="bibr" target="#b26">[27]</ref> is adopted for the attentive fusion of visible and thermal features. For the student model (Fig. <ref type="figure" target="#fig_0">1</ref>(A) lower model), visible and thermal images are concatenated to generate 6-channel multispectral inputs, i.e., 3 channels from each modality. The first convolution layer is modified to suit the 6-channel input <ref type="foot" target="#foot_2">3</ref> . Note that in lieu of generating 4-channel input as done in <ref type="bibr" target="#b21">[22]</ref>, we duplicate the single-channel ther- Training details. All models (including teacher and student models) are trained on a single GPU with 16 multispectral image pairs per mini-batch, and with an initial learning rate of 1e-2. To stabilize the training at the beginning, we adopt the warm-up strategy, where the learning rate is linearly increased from 1e-6 to 1e-2 within the first 500 iterations, then decayed with a cosine annealing. Pedestrian detection models are trained for 3,500 iterations, and semantic segmentation models are trained for 14,000 iterations. The whole project is coded in PyTorch 1.20. For fair runtime comparisons, all models' runtimes are measured on an Nvidia GTX 1080Ti GPU. We repeat each training 3 times with different random seed values and report the average performance as well as the standard error (only for the "all" setting because of the space restriction). The best results are shown in bold.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Experimental results</head><p>Baseline results. Image-level, feature-level and decisionlevel are the three major fusion methods for multispectral scene analysis. We list in Tab. 1 and 2 their prediction accuracy and inference time on KAIST Dataset <ref type="bibr" target="#b10">[11]</ref> and on MFNet Dataset <ref type="bibr" target="#b6">[7]</ref>, respectively. The visible-only  results are also listed for reference. In order to fairly compare these fusion methods, we use the same feature network (ResNet-18 <ref type="bibr" target="#b8">[9]</ref>) and detection/segmentation network (RetinaNet <ref type="bibr" target="#b16">[17]</ref>/PSPNet <ref type="bibr" target="#b30">[31]</ref>). Specifically, we adopt GAFF <ref type="bibr" target="#b26">[27]</ref> as the feature-level fusion method. For simplicity, we average the prediction from visible and thermal images for decision-level fusion. The tables show that, regardless of the information fusion stage (imagelevel, feature-level or decision-level), multispectral methods greatly improve the detection/segmentation accuracy compared to the visible-only model, especially for nighttime detection/segmentation. Feature-level and decisionlevel fusion methods almost double the execution runtimes (as well as the number of parameters, not reported here because of the space restriction) of a visible-only model. In contrast, the computational overhead for the image-level fusion is negligible, which shows the relevance of this fusion method when fewer computational resources are available.</p><p>Distillation results. We list in Tab. 3 and 4 the comparisons between native image-level fusion models and distilled image-level fusion models (i.e., student models) on KAIST Dataset <ref type="bibr" target="#b10">[11]</ref> and MFNet Dataset <ref type="bibr" target="#b6">[7]</ref>, respectively. It can be observed that MD strategy brings important improvements for all thermal resolutions for both datasets.  Specifically, on the multispectral pedestrian detection task (Tab. 3), our full thermal resolution result with MD is already close to that of the feature-level fusion model (i.e., teacher model) shown in Table 1 (7.78% versus 7.77%), while the inference time is almost halved (10.97ms versus 6.55ms). When it comes to the most practical case where thermal resolution is 16 times lower than visible resolution, MD strategy brings 2.14% of Miss Rate improvement, and the performance difference compared to the teacher model is only 0.26% (8.03% versus 7.77%). We show some detection results from native model and distilled model for this practical case in Fig. <ref type="figure" target="#fig_1">2</ref>, and it can be observed that our distilled model provides more precise detection results. In- terestingly, the nighttime detection precision is boosted by 27.06% (7.07% versus 34.13%) even if the thermal resolution is reduced to 80 × 64 (i.e., 64 times downsampled), proving the necessity of the thermal modality in nighttime detections. Moreover, our strategy remains helpful when the thermal image is completely removed (e.g., the Miss Rate for visible-only model is reduced from 22.84% to 21.08%). Here the multispectral knowledge from the teacher model allows the visible-only student to perform pseudo-multispectral detection, which is the main reason of improvements.</p><p>On the multispectral semantic segmentation task, the improvements are more important (around 5% for all thermal resolutions using MD). It is noteworthy that the performance of the distilled visible-only model is even better than that of the native full-resolution image-level fusion model (60.62% versus 59.42%). Here, our assumption is that the multispectral semantic segmentation task is more critical for the choice of fusion architecture, e.g., according to Tab. 2, native image-level fusion performs 4.03% worse than feature-level fusion. This may be the reason why rare previous work use image-level fusion for multispectral semantic segmentation. However, our MD strategy makes the student model mimic a feature-level fusion teacher model, which compensates its established disadvantage of imagelevel fusion architecture, and thus brings tremendous accuracy improvements. For the practical case (16 times thermal</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Miss Rate (%) Runtime Day Night All ACF <ref type="bibr" target="#b10">[11]</ref> 42.57 56.17 47.32 2730ms Halfway Fusion <ref type="bibr" target="#b17">[18]</ref>   resolution downsampling), the mean accuracy difference with the teacher model is minor (63.52% versus 63.45%).</p><p>We visualize in Fig. <ref type="figure" target="#fig_2">3</ref> the segmentation results from the native model and our distilled model for the practical case, and it could be noted that the improvement from MD strategy on segmentation quality is obvious.</p><p>Comparing with state-of-the-art. We compare the results of our distilled models (which adopt the more efficient image-level fusion) with state-of-the-art methods (all adopting the more cumbersome feature-level fusion) on KAIST Dataset (Tab. 5) and MFNet Dataset (Tab. 6). Note that our teacher models use <ref type="bibr" target="#b26">[27]</ref> and this method has already been shown to give better results than its competitors. However, our goal here is to show how our student models (which are supposed to be less good than their teachers) perform compared to their competitors. Specifically, we provide our onestream student models' results using full thermal resolution (same condition as our competitors, denoted as "full") and using 16 times downsampled thermal resolution (denoted as "practical"). We also list our two-stream teacher models' results (denoted as "teacher") for reference. We consider "practical" the most interesting setting for actual multispectral applications.</p><p>On the multispectral pedestrian detection task, the achieved Miss Rate from the distilled "practical" model is already better than that of the best feature-level fusion methods in the literature <ref type="bibr" target="#b31">[32]</ref> (8.03% versus 8.13%). It should be noted that our "practical" model takes downsampled thermal images as input and adopts a much simpler architecture (one-stream networks for "full"/"practical" and two-stream networks for others). It is also worth noting that our nighttime detection performance surpasses all previous methods, which proves that the thermal information has been wellpreserved in the student model.</p><p>On the multispectral semantic segmentation task, thanks to the substantial accuracy improvements from MD (about 5%), our distilled "practical" model also surpasses the best previous result <ref type="bibr" target="#b20">[21]</ref> (63.5% versus 63.1%). For this dataset as well, all our trained models (including the "practical" model with downsampled thermal input) have obvious advantage in nighttime prediction. It should be pointed out that both our distilled models with one-stream ResNet-18 feature network even outperform RTFNet-152 <ref type="bibr" target="#b20">[21]</ref> with two-stream ResNet-152 feature network, demonstrating the high efficiency of our distilled models (ours are about 6 times faster than RTFNet-152). More surprisingly, we can see in Tab. 6 that the full student model gives slightly better results than the teacher model. The student model's feature extraction network is the same as the teacher's one, so the student could theoretically achieve similar performance, and the student model has more sources of supervision, i.e., the ground truth and the knowledge learnt from the teacher model, which we believe is the reason for the higher performance shown by the student model.</p><p>Ablation experiments. To explore the effects of the proposed Attention transfer and Semantic transfer modules, we conduct ablation experiments on KAIST Dataset (Tab. 7) and MFNet Dataset (Tab. 8), under the most practical case (thermal images are 16 times downsampled). The "S" and  According to our experimental results, the latter provides better performance for both tasks. Moreover, we visualize some examples of the 2-D spatial re-weighting mask from F-MSE loss (Eq.4) in Fig. <ref type="figure" target="#fig_3">4</ref>, and it can be observed that: 1) the imbalance between easily-mimic and hardly-mimic area is grave, where the former occupies most of a given image;</p><p>2) with F-MSE loss, the optimization on the feature mimicking is automatically "focused" on more important areas, e.g., pedestrians, vehicles and colour cones. This specific loss tackles the imbalance problem in the Semantic transfer module. In conclusion, according to our ablation experiments on two datasets, both the proposed Semantic transfer and Attention transfer modules bring notable improvements and their combination leads to the best performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we identify the hardware and software constraints in today's multispectral scene analysis systems and propose a novel Modality Distillation framework to tackle these constraints. Specifically, this framework distils the knowledge from a high thermal resolution two-stream network with feature-level fusion to a low thermal resolution one-stream network with image-level fusion. The distilled model could perform prediction on widely available lowresolution thermal cameras and shows similar complexity with the mono-spectral models. In this framework, we present two knowledge transfer modules named Attention transfer and Semantic transfer specifically for multispectral learning. Extensive experiments for multispectral pedestrian detection and semantic segmentation demonstrate the efficiency of the proposed framework. In the future, we plan to extend our method for scene analysis with even more modalities such as depth sensor, Doppler radar, LiDAR, etc.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure1. Overview of the proposed method (A) and details on knowledge transfer modules (B). Blue and orange blocks represent components from teacher and student models, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Visual improvements on KAIST Dataset.</figDesc><graphic coords="6,323.53,288.25,72.19,231.02" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Visual improvements on MFNet Dataset.</figDesc><graphic coords="7,61.77,72.23,73.18,274.41" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Visualization of the visible-thermal image pairs and the 2-D spatial re-weighting masks from the proposed F-MSE loss. The first two lines of multispectral images pairs come from KAIST Dataset, and the last two lines come from MFNet Dataset.</figDesc><graphic coords="8,318.99,376.57,215.98,54.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Different fusion methods on MFNet Dataset. For fair comparisons, all listed methods use the same feature network (ResNet-18) and segmentation network (PSPNet).</figDesc><table><row><cell>Fusion stage</cell><cell cols="3">Mean Accuracy (%) Day Night All</cell><cell>Runtime</cell></row><row><cell>Visible-only</cell><cell cols="3">50.83 52.26 55.06±0.21</cell><cell>4.57ms</cell></row><row><cell>Image-level</cell><cell cols="3">54.97 56.07 59.42±0.22</cell><cell>4.68ms</cell></row><row><cell>Feature-level</cell><cell cols="3">57.21 62.18 63.45±0.24</cell><cell>8.94ms</cell></row><row><cell cols="4">Decision-level 51.72 53.37 56.21±0.14</cell><cell>9.14ms</cell></row><row><cell cols="2">Thermal resolution MD</cell><cell cols="3">Miss Rate (%) Day Night All</cell></row><row><cell>Full resolution</cell><cell>✓</cell><cell>10.73 9.45</cell><cell cols="2">6.61 9.40±0.39 4.61 7.78±0.28</cell></row><row><cell>4x downsample</cell><cell>✓</cell><cell>11.57 9.39</cell><cell cols="2">6.51 9.84±0.72 5.07 7.91±0.11</cell></row><row><cell>16x downsample</cell><cell>✓</cell><cell>12.09 9.85</cell><cell cols="2">6.73 10.17±0.42 4.84 8.03±0.19</cell></row><row><cell>64x downsample</cell><cell cols="4">14.92 10.66 13.37±0.30 ✓ 10.75 7.07 9.50±0.06</cell></row></table><note><p>Visible-only 16.95 35.15 22.84±0.77 ✓ 14.74 34.13 21.08±0.21</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Comparison between native models and distilled models on KAIST Dataset under different thermal resolution settings (from full thermal resolution to no thermal scenario). All listed methods use an image-level fusion architecture.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>Comparison between native models and distilled models on MFNet Dataset under different thermal resolution settings (from full thermal resolution to no thermal scenario). All listed methods use an image-level fusion architecture.</figDesc><table><row><cell cols="2">Thermal resolution MD</cell><cell>Mean Accuracy (%) Day Night All</cell></row><row><cell>Full resolution</cell><cell cols="2">54.97 56.07 59.42±0.22 ✓ 59.71 62.78 64.93±0.11</cell></row><row><cell>4x downsample</cell><cell cols="2">53.89 55.18 57.93±0.27 ✓ 58.32 61.88 64.25±0.11</cell></row><row><cell>16x downsample</cell><cell cols="2">53.85 55.43 58.21±0.46 ✓ 58.46 61.37 63.52±0.87</cell></row><row><cell>64x downsample</cell><cell cols="2">53.68 53.68 57.06±0.18 ✓ 57.58 59.67 62.62±0.81</cell></row><row><cell>Visible-only</cell><cell cols="2">50.83 52.26 55.06±0.21 ✓ 57.74 56.67 60.62±0.42</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 .</head><label>5</label><figDesc>Comparison between state-of-the-art multispectral pedestrian detection methods and ours on KAIST Dataset. Our competitors' results are taken from<ref type="bibr" target="#b31">[32]</ref>.</figDesc><table><row><cell></cell><cell cols="3">24.88 26.59 25.75</cell><cell>430ms</cell></row><row><cell cols="4">FusionRPN+BF [13] 19.57 16.27 18.29</cell><cell>800ms</cell></row><row><cell>IAF R-CNN [15]</cell><cell cols="3">14.55 18.26 15.73</cell><cell>210ms</cell></row><row><cell>IATDNN+IASS [6]</cell><cell cols="3">14.67 15.72 14.95</cell><cell>250ms</cell></row><row><cell>RFA [28]</cell><cell cols="3">16.78 10.21 14.61</cell><cell>80ms</cell></row><row><cell>CIAN [29]</cell><cell cols="3">14.77 11.13 14.12</cell><cell>70ms</cell></row><row><cell>MSDS-RCNN [14]</cell><cell cols="3">10.53 12.94 11.34</cell><cell>220ms</cell></row><row><cell>AR-CNN [30]</cell><cell>9.94</cell><cell cols="2">8.38 9.34</cell><cell>120ms</cell></row><row><cell>MBNet [32]</cell><cell>8.28</cell><cell cols="2">7.86 8.13</cell><cell>70ms</cell></row><row><cell>Ours (teacher)</cell><cell>9.37</cell><cell>4.71</cell><cell>7.77</cell><cell>11ms</cell></row><row><cell>Ours (full)</cell><cell>9.45</cell><cell cols="2">4.61 7.78</cell><cell>7ms</cell></row><row><cell>Ours (practical)</cell><cell>9.85</cell><cell cols="2">4.84 8.03</cell><cell>7ms</cell></row><row><cell>Method</cell><cell cols="4">Mean Accuracy (%) Runtime Day Night All</cell></row><row><cell>MFNet [7]</cell><cell>42.6</cell><cell cols="2">41.4 45.1</cell><cell>4.35ms</cell></row><row><cell>FuseNet [8]</cell><cell>49.5</cell><cell cols="2">48.9 52.4</cell><cell>3.92ms</cell></row><row><cell>RTFNet-50 [21]</cell><cell>57.3</cell><cell cols="3">59.4 62.2 11.25ms</cell></row><row><cell cols="2">RTFNet-152 [21] 60.0</cell><cell cols="3">60.7 63.1 29.35ms</cell></row><row><cell>Ours (teacher)</cell><cell>57.2</cell><cell cols="2">62.2 63.5</cell><cell>8.94ms</cell></row><row><cell>Ours (full)</cell><cell>59.7</cell><cell cols="2">62.8 64.9</cell><cell>4.92ms</cell></row><row><cell>Ours (practical)</cell><cell>57.6</cell><cell cols="2">61.4 63.5</cell><cell>4.92ms</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 .</head><label>6</label><figDesc>Comparison between state-of-the-art multispectral semantic segmentation methods and ours on MFNet Dataset. Our competitors' results are taken from<ref type="bibr" target="#b20">[21]</ref>.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 .</head><label>7</label><figDesc>Ablation experiments on KAIST Dataset. We study the effects of Semantic transfer (with MSE or F-MSE loss) and Attention transfer modules in the proposed MD framework.</figDesc><table><row><cell cols="2">S(M) S(F) A</cell><cell cols="3">Miss Rate (%) Day Night All</cell></row><row><cell></cell><cell></cell><cell>12.09</cell><cell cols="2">6.73 10.17±0.42</cell></row><row><cell>✓</cell><cell></cell><cell>11.92</cell><cell cols="2">6.69 10.09±0.06</cell></row><row><cell>✓</cell><cell></cell><cell>10.24</cell><cell>6.93</cell><cell>9.11±0.15</cell></row><row><cell></cell><cell cols="2">✓ 10.61</cell><cell>5.83</cell><cell>8.99±0.23</cell></row><row><cell>✓</cell><cell>✓</cell><cell>9.85</cell><cell>4.84</cell><cell>8.03±0.19</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 8 .</head><label>8</label><figDesc>S(M) S(F) AMean Accuracy (%) Day Night All 53.85 55.43 58.21±0.46 ✓ 56.04 57.87 60.41±0.28 ✓ 57.49 58.47 61.16±0.13 ✓ 57.55 58.46 61.51±0.30 ✓ ✓ 57.58 61.37 63.52±0.87 Ablation experiments on MFNet Dataset. We study the effects of Semantic transfer (with MSE or F-MSE loss) and Attention transfer modules in the proposed MD framework.</figDesc><table><row><cell>Visible image</cell><cell>Thermal image</cell><cell>Re-weighting mask</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>A typical thermal camera of resolution 640 × 480 could cost more than 8,000 USD. When the resolution is reduced to 80 × 60, the price becomes much more affordable (around</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="200" xml:id="foot_1"><p>USD).</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>Concretely, the pretrained ImageNet<ref type="bibr" target="#b1">[2]</ref> weights for the first convolution layer are duplicated along the input channel dimension, and the values are halved.</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning efficient object detection models with knowledge distillation</title>
		<author>
			<persName><forename type="first">Guobin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wongun</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tony</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manmohan</forename><surname>Chandraker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on Neural Information Processing Systems</title>
		<meeting>the 31st International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="742" to="751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Measures of the amount of ecologic association between species</title>
		<author>
			<persName><surname>Lee R Dice</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ecology</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="297" to="302" />
			<date type="published" when="1945">1945</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Fast feature pyramids for object detection</title>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ron</forename><surname>Appel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1532" to="1545" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Pedestrian detection: An evaluation of the state of the art</title>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Wojek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="743" to="761" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Fusion of multispectral data through illumination-aware deep neural networks for pedestrian detection</title>
		<author>
			<persName><forename type="first">Dayan</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanpeng</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiangxin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanlong</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Fusion</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="page" from="148" to="157" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Mfnet: Towards real-time semantic segmentation for autonomous vehicles with multispectral scenes</title>
		<author>
			<persName><forename type="first">Qishen</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kohei</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Takumi</forename><surname>Karasawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshitaka</forename><surname>Ushiku</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tatsuya</forename><surname>Harada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017">2017. 2017</date>
			<biblScope unit="page" from="5108" to="5115" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Fusenet: Incorporating depth into semantic seg-mentation via fusion-based cnn architecture</title>
		<author>
			<persName><forename type="first">Caner</forename><surname>Hazirbas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingni</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Csaba</forename><surname>Domokos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Cremers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="213" to="228" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Distilling the knowledge in a neural network</title>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.02531</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Multispectral pedestrian detection: Benchmark dataset and baseline</title>
		<author>
			<persName><forename type="first">Soonmin</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaesik</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Namil</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yukyung</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">In</forename><surname>So Kweon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1037" to="1045" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Paying more attention to attention: improving the performance of convolutional neural networks via attention transfer</title>
		<author>
			<persName><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Fully convolutional region proposal networks for multispectral person detection</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Konig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Jarvers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georg</forename><surname>Layher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heiko</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Teutsch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="49" to="56" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Multispectral pedestrian detection via simultaneous detection and segmentation</title>
		<author>
			<persName><forename type="first">Chengyang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruofeng</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference (BMVC)</title>
		<imprint>
			<date type="published" when="2018-09">September 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Illumination-aware faster r-cnn for robust multispectral pedestrian detection</title>
		<author>
			<persName><forename type="first">Chengyang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruofeng</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">85</biblScope>
			<biblScope unit="page" from="161" to="171" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Mimicking very efficient network for object detection</title>
		<author>
			<persName><forename type="first">Quanquan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shengying</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junjie</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ieee conference on computer vision and pattern recognition</title>
		<meeting>the ieee conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="6356" to="6364" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Focal loss for dense object detection</title>
		<author>
			<persName><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Multispectral deep neural networks for pedestrian detection</title>
		<author>
			<persName><forename type="first">Jingjing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoting</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dimitris</forename><forename type="middle">N</forename><surname>Metaxas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.02644</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Structured knowledge distillation for semantic segmentation</title>
		<author>
			<persName><forename type="first">Yifan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ke</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zengchang</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenbo</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2604" to="2613" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Fitnets: Hints for thin deep nets</title>
		<author>
			<persName><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samira</forename><forename type="middle">Ebrahimi</forename><surname>Kahou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Polytechnique</forename><surname>Montréal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Université</forename><surname>De Montréal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samira</forename><forename type="middle">Ebrahimi</forename><surname>Kahou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Chassang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlo</forename><surname>Gatta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<publisher>ICLR</publisher>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Rtfnet: Rgbthermal fusion network for semantic segmentation of urban scenes</title>
		<author>
			<persName><forename type="first">Yuxiang</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weixun</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Robotics and Automation Letters</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="2576" to="2583" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Multispectral pedestrian detection using deep fusion convolutional neural networks</title>
		<author>
			<persName><forename type="first">Jörg</forename><surname>Wagner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Volker</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Herman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sven</forename><surname>Behnke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ESANN</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">587</biblScope>
			<biblScope unit="page" from="509" to="514" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Distilling object detectors with fine-grained feature imitation</title>
		<author>
			<persName><forename type="first">Tao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaopeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4933" to="4942" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Improving fast segmentation with teacherstudent learning</title>
		<author>
			<persName><forename type="first">Jiafeng</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Shuai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian-Fang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingyang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei-Shi</forename><surname>Zheng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.08476</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Localize to classify and classify to localize: Mutual guidance in object detection</title>
		<author>
			<persName><forename type="first">Heng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elisa</forename><surname>Fromont</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sébastien</forename><surname>Lefèvre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bruno</forename><surname>Avignon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Asian Conference on Computer Vision</title>
		<meeting>the Asian Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Multispectral fusion for object detection with cyclic fuse-and-refine blocks</title>
		<author>
			<persName><forename type="first">Heng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elisa</forename><surname>Fromont</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sébastien</forename><surname>Lefèvre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bruno</forename><surname>Avignon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Image Processing (ICIP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020">2020. 2020</date>
			<biblScope unit="page" from="276" to="280" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Guided attentive feature fusion for multispectral pedestrian detection</title>
		<author>
			<persName><forename type="first">Heng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elisa</forename><surname>Fromont</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sébastien</forename><surname>Lefèvre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bruno</forename><surname>Avignon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="72" to="80" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">The cross-modality disparity problem in multispectral pedestrian detection</title>
		<author>
			<persName><forename type="first">Lu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.02645</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Cross-modality interactive attention network for multispectral pedestrian detection</title>
		<author>
			<persName><forename type="first">Lu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shifeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hong</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaizhu</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amir</forename><surname>Hussain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Fusion</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="page" from="20" to="29" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Weakly aligned cross-modal learning for multispectral pedestrian detection</title>
		<author>
			<persName><forename type="first">Lu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhen</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyong</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="5127" to="5137" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaojuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2881" to="2890" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Improving multispectral pedestrian detection by addressing modality imbalance problems</title>
		<author>
			<persName><forename type="first">Kailai</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linsen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xun</forename><surname>Cao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.03043</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
