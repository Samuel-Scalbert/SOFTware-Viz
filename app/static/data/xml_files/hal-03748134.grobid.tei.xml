<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="fr">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Incremental and multimodal visualization of discographies: exploring the WASABI music knowledge base</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Aline</forename><surname>Menin</surname></persName>
							<email>aline.menin@inria.fr</email>
						</author>
						<author>
							<persName><forename type="first">Michel</forename><surname>Buffa</surname></persName>
							<email>michel.buffa@inria.fr</email>
						</author>
						<author>
							<persName><forename type="first">Maroua</forename><surname>Tikat</surname></persName>
							<email>maroua.tikat@inria.fr</email>
						</author>
						<author>
							<persName><forename type="first">Benjamin</forename><surname>Molinet</surname></persName>
							<email>benjamin.molinet@inria.fr</email>
						</author>
						<author>
							<persName><forename type="first">Guillaume</forename><surname>Pelerin</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Laurent</forename><surname>Pottier</surname></persName>
							<email>laurent.pottier@univ-st-etienne.fr</email>
						</author>
						<author>
							<persName><forename type="first">Franck</forename><surname>Michel</surname></persName>
							<email>franck.michel@inria.fr</email>
						</author>
						<author>
							<persName><forename type="first">Marco</forename><surname>Winckler</surname></persName>
							<email>marco.winckler@inria.fr</email>
						</author>
						<author>
							<persName><forename type="first">Guillaume</forename><surname>Pellerin</surname></persName>
							<email>guillaume.pellerin@ircam.fr</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Université Côte d&apos;Azur</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Inria Sophia Antipolis</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution" key="instit1">Université Côte d&apos;Azur</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">Inria Sophia Antipolis</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="institution" key="instit1">Université Côte d&apos;Azur</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="institution">Inria Sophia Antipolis</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff6">
								<orgName type="institution" key="instit1">Université Côte d&apos;Azur</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff7">
								<orgName type="institution">Inria Sophia Antipolis</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff8">
								<orgName type="institution">IRCAM</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff9">
								<orgName type="institution">Université Jean Monnet Saint-Etienne</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff10">
								<orgName type="institution" key="instit1">Université Côte d&apos;Azur</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff11">
								<orgName type="institution">Inria Sophia Antipolis</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff12">
								<orgName type="institution" key="instit1">Université Côte d&apos;Azur</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff13">
								<orgName type="institution">Inria Sophia Antipolis</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Incremental and multimodal visualization of discographies: exploring the WASABI music knowledge base</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">4585798A59E5686FA79773097BFE1FBD</idno>
					<idno type="DOI">10.5281/zenodo.6767530</idno>
					<note type="submission">Submitted on 9 Aug 2022</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-04-12T14:46+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>come from teaching and research institutions in France or abroad, or from public or private research centers. L'archive ouverte pluridisciplinaire HAL, est destinée au dépôt et à la diffusion de documents scientifiques de niveau recherche, publiés ou non, émanant des établissements d'enseignement et de recherche français ou étrangers, des laboratoires publics ou privés.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="fr">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Music data is increasingly available in digital form. Currently, there are several datasets available on the Web that describe musical content in various multimedia dimensions, such as lyrics, chords, sounds, metadata, etc. <ref type="bibr" target="#b7">[8]</ref>.Therefore, digital methods are especially important in musicology (the study of music as a branch of knowledge or field of research distinct from composition or performance <ref type="bibr" target="#b20">[21]</ref>) for storing, structuring, and analyzing large amounts of music data available in digital form <ref type="bibr" target="#b29">[29]</ref>. In this context, visualization techniques are suitable tools to facilitate the access to the data, while being capable of highlighting relationships between structural elements of music. A recent survey <ref type="bibr" target="#b17">[18]</ref> illustrates the importance of visualization in musicology through an overview of the visualization techniques used to represent music-related data, including musical collections (albums, playlists, music archives), musical works, artists, or data issued from audio analysis (song structure, musical dimensions, etc.).</p><p>In this paper, we support the exploration of artists' discographies and collaborations. Artist collaborations in music tend to result in successful songs <ref type="bibr" target="#b9">[10]</ref>. Thus, analysis of artists' collaboration data can help to understand the impact of these collaborative projects on artists' careers and their music style. Despite the importance and essential aspect of cultural context (e.g., release dates, collaborations, locations), the analysis of the timbre and annotations of the audio signal, as well as careful listening to the songs, is essential when studying the acoustic characteristics of songs <ref type="bibr" target="#b24">[25]</ref>. Therefore, in this paper, we also explore the potential of the audio dimension to further support musicologists on their analysis, as well as to improve user perception through an auditory data exploration approach.</p><p>As a case of study, we explore the data from the WASABI RDF knowledge graph (KG) <ref type="bibr" target="#b7">[8]</ref>, which gathers metadata describing over two million commercial songs (200K albums and 77K artists -mainly from pop/rock culture). It includes metadata about songs, albums, and artists (e.g., artists, discography, producers, dates, etc.) retrieved from multiple data sources on the Web. This data is linked to metadata extracted from NLP (Natural Language Processing) analysis of song lyrics and MIR (Music Information Retrieval) analysis of song audio content.</p><p>The WASABI dataset contains data on commercial music recordings from 1922 to 2022, but it does not provide explicit information about collaborations between artists and how they intersect over time. In this paper, we present a visualization tool to support the exploration of artists' discographies and collaborations: a social network of artists and bands derived from their recordings and mapped to the dimensions of association (type of contribution) and time (progression of the artist's career). The contributions in this article are as follows:</p><p>• a web-based interactive tool to visualize the discography of artists / groups and collaborations between musical artists and groups across time;</p><p>• an auditory exploration approach based on audio thumbnailing to support user perception throughout the visual exploration process;</p><p>• a direct link with external services that support further exploration of songs through MIR analysis.</p><p>The remainder of this document is organized as follows. Section 2 summarizes previous approaches to visualizing artist discography and collaborations, multimodal visualization, and audio thumbnailing techniques. Section 3 describes the WASABI dataset and the data model of our visualization tool. Section 4 describes our approach to a multimodal visualization that combines graphics, audio, and MIR analysis to support music artist data exploration. Section 5 describes a short evaluation conducted with an expert in musicology to assess the effectiveness and usefulness of our approach. Sections 6 and 7 conclude this work while discussing its limitations and potential future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">RELATED WORK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Music datasets</head><p>The Million Song Dataset <ref type="bibr" target="#b5">[6]</ref> is one of the outstanding music datasets describing one million contemporary popular songs through metadata extracted primarily from audio content analysis. It provides in addition metadata describing albums, artists, labels, tracks and relationships (among artists, albums, tracks, etc.). A recent addition in this domain is the WASABI dataset 1 , which consists of more than 2 million commercial songs and holds a rich set of cultural metadata, about songs, albums and artists, which were retrieved from multiple sources on the web that it draws on by reusing their identifiers, or references via links. It also contains metadata extracted from the NLP analysis of song lyrics and from the MIR (Music Information Retrieval) analysis of song audio content, done during the project. The RDF representation of the data set is publicly available. We explore the data from this dataset in our case study. 1 https://github.com/micbuffa/WasabiDataset</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Visualization of artist discography and collaboration network</head><p>As shown in <ref type="bibr" target="#b17">[18]</ref>, various visualization techniques exist to explore music-related data, including song and instrument structure, performance analysis, emotions, similarity and listening statistics, and music alignment. Nonetheless, a few solutions have been proposed to explore the social network of musical artists, at least not in terms of collaborative composition and recordings. For instance, Miller and al. <ref type="bibr" target="#b22">[23]</ref> propose a network graph to represent the relationships between musicians described in the Linked Jazz database, where typical relations are familial, academic, or work related. In addition to exploring collaborative productions, we are also interested in visualizing the progress of those collaborations together within the whole discography of artists.</p><p>MusicianMap <ref type="bibr" target="#b30">[30]</ref> is an interactive web-based tool that enables the visualization of relationships between artists, including band membership, musical collaborations, and similarity of musical genre. This tool superposes two visualization techniques: a node-link diagram where nodes represent artists and links represent a relationship between them; and a timeline to show the evolution of those relationships over time, while providing time-based information (i.e., quantity of sales and recordings issued). A similar approach has been adopted to explore the recording sessions between jazz musicians over time <ref type="bibr" target="#b12">[13]</ref>, with the possibility of highlighting the strength of the collaborations at various points of time.</p><p>MusicLynx <ref type="bibr" target="#b0">[1]</ref> is a music web application that supports the exploration of a graph of artist based on their similarities. Built from the linking of several free public data sources (including the "Million Song Dataset", DBPedia, MusicBrainz, and AcousticBrainz), it provides a multifaceted navigation platform, offering a graph-based representation of the similarity links (modeled via audio descriptors of rhythm, tonality, and timbre) between artists. The web application also features a music player that streams short previews of the artist's top tracks (if available in the database of the streaming service).</p><p>As MusicianMap represents artist collaboration over time, we consider this tool the closets with respect to the approach presented in this paper. Nonetheless, our approach differs from the MusicianMap and other previous works by the fact that we support the exploration of artist discography based on different production types (composition, production, and performance) and through an auditory dimension, which allows the user to listen to what they see on the visualization. Furthermore, our multimodal and incremental visualization approach supports a better understanding of the collaborations between artists or between members of the same group on a recording as it represents collaboration links according to the different roles played by the artist on the collaborative recording (composer, producer, performer). The visualization approach leverage the focus+context interaction technique to provide data exploration in different granularity levels (at artist and song levels). The audio dimension has been incorporated, allowing, at the macro level, to perceive in an audible way the musical evolution of an artist over time or the influences of the different collaborators on the musical style of each one, while at the micro level it is possible to explore the audio signal of a song by performing MIR analysis (on demand) of the sound and by adding very precise annotations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Timbre descriptors processing and access</head><p>The notion of timbre has been deeply modified by the evolution of recording studio techniques. For instance, there is an increasing use of DSP (Digital Signal Processing) effects by musicians. Ever more, electronic music uses sounds produced by synthesis or electroacoustic music, where composition is based on multi-category sounds. Thus, timbre can no longer be tied solely to the instrumental origin of sounds, but rather to integrated acoustic qualities <ref type="bibr" target="#b8">[9]</ref>. Until recently, the only possible descriptions of sound were related to its spectrum and to sonograms and were mainly qualitative <ref type="bibr" target="#b26">[26,</ref><ref type="bibr" target="#b27">27,</ref><ref type="bibr" target="#b6">7]</ref>.</p><p>The acoustic characteristics of a sound are complex and it is difficult to relate them to the perception one may have of them. The quality and nature of the sound that we perceive depends on many interdependent factors that can be measured using a number of acoustic descriptors <ref type="bibr" target="#b18">[19]</ref> each of which represents a certain quality of a sound. What we perceive is a combination of these different parameters, but studies to analyze and understand the link between the measured values of these parameters and the perception by a listener are still recent and very partial. The field is huge and still remains to be explored to better understand how the sounds of different musics resemble each other, to be able to establish classifications, or to finely analyze the music.</p><p>Among the descriptors that are used by many authors, the spectral centroid is the most widespread and is generally associated with the brilliance of the sound (energy distributed in the upper midrange or treble). Other statistical data extracted from the spectrum, such as the spectral slope, is also commonly used. The dynamic aspects of sound, at a time scale of a second, are still rather poorly described, even if there are some descriptors on this topic, such as the spectral flux <ref type="bibr" target="#b10">[11]</ref>. Many studies are currently focused on the detection and recognition of musical instruments <ref type="bibr" target="#b19">[20]</ref>, on the separation of different audio tracks <ref type="bibr" target="#b14">[15]</ref>, on the isolation and erasure of the voice <ref type="bibr" target="#b15">[16]</ref>, and automatic detection of words <ref type="bibr" target="#b21">[22]</ref>. These studies are usually based on machine learning methods. Few studies focus on the quantification and correlation of perceived sound parameters, especially in the case of electric or electronic music study. By studying the acoustic characteristics of music, we want to be able to make classifications and establish similarities between various pieces of music, based on the perception of timbre. The objective is to determine a kind of sound signature of the sound <ref type="bibr" target="#b23">[24]</ref>, on the scale of a few seconds, a duration that allows us, for example, to instantly recognize a group or a song that is familiar to us. By adding the visualization of the sound signature and audio descriptors associated with the presentation of songs of various artists, we aim to improve the perception of the sound quality of those songs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">DATA AND DATASET</head><p>Our visualization technique has been applied to data retrieved from the WASABI dataset, accessible through a REST API<ref type="foot" target="#foot_0">2</ref> and a public SPARQL endpoint <ref type="foot" target="#foot_1">3</ref> . Hereafter, we describe the processes of retrieving metadata about artist discography and collaborations and the process of extracting audio thumbnails from songs to support our multimodal exploration of data via visual and auditory approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Artist discography and collaborations</head><p>The query presented in the Listing 1 is used to obtain the discography of a particular artist from the WASABI KG. The results contain the list of published songs (described by a title, an identifier, a release date), the album to which each song belongs, and the list of performers, authors (writers) and producers. Each artist is described by a name, a type (person or group), and an identifier. An artist can be associated with one or more songs, and a song can be associated with one or more artists. This association is defined by one or more contribution types (i.e. performer, producer, writer).</p><p>The data obtained from the data set is transformed to fit the data model used to create the visualizations, which corresponds to a multidimensional dynamic collaboration network. In the first layer of the network, the nodes represent the artists and the links between them are defined by the songs they have worked on together (produced, performed, or composed together). The second layer of the network describes these collaborations at a finer level: nodes represent songs that are linked to each other by multiple links (one per type of contribution of the involved artists). The data model describes an artist's collaboration network, as well as their entire discography (whether or not there was a collaboration) over time. The resulting output data is a JSON document containing a list of albums and songs per artist, a list of nodes, a list of links, and a dictionary of artists each containing attributes such as id, name, type, lifespan (birth/death for a person and begin/end for a group), and the list of members when artist is a group. Listing 1: SPARQL query used to retrieve an artist discography and collaboration data from the WASABI KG. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Audio thumbnails</head><p>To support auditory exploration of artist discography, we developed several scripts that take as input a hierarchy of audio files (discography/albums/songs) and produce as output audio summaries composed of selected excerpts of songs and metadata files. In our case, if a discography is composed of about ten music albums, each containing about ten songs, the final audio file will be made of about 100 song excerpts. If each excerpt lasts 3 seconds, i.e. 300 seconds or 5 minutes. The excerpts, ideally, should make the songs easily identifiable, especially for classic songs that have marked the history of music (by having been a number one hit for example). Several methods exist for extracting the most representative short part of a song in an unsupervised way, such as music summarization, audio thumbnailing, chorus detection, repeated pattern discovery, music structure analysis <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4]</ref>, and hybrid techniques combining song lyrics and audio content in one whole to get better results <ref type="bibr" target="#b11">[12]</ref>. In its current proof-of-concept version, our solution simply select song extracts in the middle of the song, and shift a window backward in time to ensure that the full interval is not composed of silence.</p><p>A difficulty in associating audio albums with the WASABI dataset concerns first of all the notion of discography. In our case, after discussion with the musicologists we only considered studio albums in the original version of their release in their country of origin. For example, from the multiple versions of David Bowie's album "The Rise and Fall of Ziggy Stardust and the Spiders from Mars", the WASABI KG only describes the original version released in the UK in 1972, which included 11 songs. Thus, For a particular artist, we start with the audio albums in their original version, and we organize the audio discography as a hierarchy of audio files with normalized file names. Through the exploration of these folders/files, for each album, we query the WASABI dataset via its REST API, and retrieve the WASABI id of the artist, albums, and songs. As we are comparing album/song names, the scripts include interactive hints when a proper match can not be achieved allowing to manually correct the information (e.g., it proposes suggestions when the exact name could not be found, or proposes to enter manually a new name). The metadata files also includes information such as the length of the excerpts used to build discography or album audio thumbnails. Both the mp3 files containing these audio summaries and the metadata files are accessible via a single REST API.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">INCREMENTAL AND MULTIMODAL VISUALIZATION APPROACH</head><p>In this visualization, we want to support the visual exploration of the following dimensions of the data: the two-layer network that describes the collaborations between artists (see the description above) and the artist discography that describes the albums and songs released over time. Furthermore, we seek to improve user perception by associating audio to the visualization, in a way that the users can listen to the songs visually represented in their screen. Hereafter, we describe how we combine graphics, audio, and MIR analyses into a multimodal visualization for artist discography and collaboration exploration.</p><p>The proposed visualization tool consists of three main elements: a tool bar (Fig. <ref type="figure" target="#fig_0">1a</ref>), a visualization technique (Fig. <ref type="figure" target="#fig_0">1b</ref>), and an audio player (Fig. <ref type="figure" target="#fig_0">1c</ref>). In the tool bar, the user can select an artist to explore discography and collaboration data. Once the visualization is displayed, the user may also search for a particular song using the search bar. The visualization technique combines a timeline and a stream-graph to represent artist discography over time. The complete reasoning and features of the visualization technique are discussed in the following. Finally, the tool contains an audio player that allows users to listen to the songs displayed on the screen, as well as to a thumbnail of the artist discography or a particular album.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Visual exploration of discography</head><p>Typically, the time dimension can be mapped in an animation to a simulated time (time-to-time mapping) or to a space dimension of the generated visualization representing a timeline (time-to-space mapping) <ref type="bibr" target="#b4">[5]</ref>. Timeline-based approaches provide a better overview of time, since they show the complete sequence of graphs in a static image <ref type="bibr" target="#b28">[28]</ref>, while animation-based approaches show one time slice at a time and, therefore, suffer with perception problems, as the user must rely on their own memory to understand the data and grasp the timely changes <ref type="bibr" target="#b1">[2]</ref>. To support users to grasp at a glance the complete artist discography and to reduce cognitive effort, we chose a timeline-based visualization approach.</p><p>The proposed visualization follows the structure presented in Figure <ref type="figure" target="#fig_0">1b</ref>. The name of the artist is placed on the left axis, whereas the top and bottom axes represent time in years, ranging from the release year of the first album to the last album. Albums are represented through a circle packing technique, where dark red circles represent the songs belonging to that album. Singles or songs from other artists in which the referred artist collaborated are represented as dark red circles placed on the side of albums. Notice that the label of the artist contains a player control, which serve to explore the artist discography by listening to a thumbnail of it (see more details below).</p><p>Furthermore, we represent the music production profile of an artist over time using a stream-graph technique, where each stream area represents the amount of contribution the artist had in the associated songs. We consider three types of contribution in a song (i.e., performance, production, or composition), which we color-coded through a blue gradient ranging from dark to light blue, respectively. The amount of production per year corresponds to the sum of songs released by the artist or to which the artist contributed. Through this visualization, the user can grasp rapidly when an artist was the most productive and what type of musical production they did.</p><p>We provide different levels of interaction to explore the discography of an artist (see Figure <ref type="figure" target="#fig_1">2</ref>). The tool includes a slider that can be dragged over the x -axis to focus on the productions of different years, while progressively hiding the items that do not correspond to the selection. In a second level, the user can focus and zoom in on the productions of a particular year by selecting that year on the x axis (top or bottom), resulting in a distortion of these axes and expansion of items within the affected area (Figure <ref type="figure" target="#fig_1">2c</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Auditory exploration of discography</head><p>At a third level of interaction, we allow the user to listen to the songs being represented. For that, an audio player using the WebAudio API has been specially developed to interact bidirectionally with the audio thumbnails. This player offers playback modes to explore the entire discography, an album, or a particular song. Written as a Web Component, it is integrated to the visualization tool and offers a complete JavaScript API. Thus, from the graphical visualization, it is possible to trigger the playback, to jump from one extract to another in audio summaries, to adjust the playback speed, etc. Particularly, the user can use the audio controls placed under the name of the artist or directly click on a circle representing either an album or a song to listen to the associated audio. Figure <ref type="figure" target="#fig_2">3</ref> shows the process to play the audio thumbnail of a particular album. When hovering over a particular album, the system displays a "play" symbol indicating that the user can listen to that album (Figure <ref type="figure" target="#fig_2">3a</ref>). By clicking on it, the system launches the audio player for that particular album and goes into a state of "pause", as illustrated by the icon on the album (Figure <ref type="figure" target="#fig_2">3b</ref>). The visualization shows the user which song thumbnail is currently playing by highlighting the circle that represents it. The audio player also shows this information by displaying the title of the song, while also showing the variations in balance and volume through the audio monitors (Figure <ref type="figure" target="#fig_2">3c</ref>). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Visual exploration of artist collaborations</head><p>We adopted an incremental approach to support the exploration of artist collaboration. Let us assume that we are interested in exploring the collaborations of Queen over time. When right-clicking on the artist's name, the system displays a context menu that provides a set of options for the user, including the list of artists with whom they collaborated throughout their career (Figure <ref type="figure" target="#fig_3">4a</ref>). Upon selecting an artist from the list, let us say Freddie Mercury, the system queries the RDF graph to retrieve the data (albums, songs, and collaborators) corresponding to the selected artist and displays it as shown in Figure <ref type="figure" target="#fig_3">4b</ref>. At this point, we are exploring data from two related artists, which means that our data correspond to the two-layer network mentioned above. Freddie Mercury, in this particular case, had a solo career, which solo albums can be seen on his timeline. However, we can also identify songs in which he participated, even if they are not in his solo albums. This is the case for all Queen songs he composed as a member of the group, represented by the dark red circles placed next to the albums (white circles).</p><p>Network data is typically visualized through node-link diagrams, where nodes are connected by links, or adjacency matrices, where vertices are mapped to rows and columns of the matrix, and a colored intersection cell encoding an edge <ref type="bibr" target="#b4">[5]</ref>. As we are representing the time dimension using a timeline, we chose a node-link diagram to represent the collaborations between artists. In particular, we chose a juxtaposed layout to present the network and discography side by side. As mentioned above, our network consists of two layers: collaborations at the artist and song levels. Collaborations at the artist level are represented through single black lines that connect artists in each year where a collaboration has been detected. At this level, the user can grasp the information that a collaboration exists between those artists, without information regarding the type of collaboration. By hovering over an artist's label, the system highlights all the collaborative songs and albums of this artist, as well as the artists to whom those collaborations happened.</p><p>Exploring the second layer of the network is supported through a focus+context interaction technique. When clicking on a year of interest, the timeline is stretched horizontally, revealing the collaboration links between artists at the song level (Figure <ref type="figure" target="#fig_3">4c</ref>). The visualization displays a link between each two nodes (per artist pair) representing the songs where both artists worked together. The links are colored to represent the different types of collaboration (performance, production, or composition) following the same color-code used to color the artist's temporal profile. The user can easily change the focus by clicking on another year on the top/bottom time axes or by dragging the slider over the axes to browse across the different years.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Visual exploration of timbre descriptors</head><p>In order to access to the timbre descriptors of each track described in 2.3, we support the analysis of audio signal by linking the WASABI dataset songs to a remote TimeSide audio processing service.</p><p>TimeSide is an open, scalable, audio processing framework implemented in Python that enables low-and high-level audio analysis, visualization, transcoding, streaming, and labeling. Initially designed for MIR and computational musicology usecases <ref type="bibr" target="#b13">[14]</ref>, it supports reproducible and extensible processing on large datasets of any audio or video format through state-of-the-art audio libraries (i.e. VAMP, Aubio, Essentia, Numpy/Scipy). To support the exploration of the WASABI songs, we have created or extended some parts of the framework such as a secured and documented REST API with JSON Web Token access capabilities, a Provider module to handle automatic extraction of YouTube and Deezer tracks or 30-second extracts, a JavaScript SDK<ref type="foot" target="#foot_2">4</ref> for the development of client applications, and a new web front-end prototype <ref type="foot" target="#foot_3">5</ref> . The Timeside remote service based at IRCAM is then used to run several timbre analyzers based on VAMP plugins<ref type="foot" target="#foot_4">6</ref> on demand. The WASABI server sends some metadata, the YouTube ID and the list of the needed processors in order to compute and store the result data of each track. As a result, the user of the WASABI main interface can explore the audio related data for a particular song from a contextmenu embedded in each dark red circle that represents a song. The timbre descriptors curves are then displayed in an augmented multi-track audio player which provides a dynamic and zoomable interface to facilitate time based fine analyzing and labeling (Figure <ref type="figure" target="#fig_4">5</ref>).  In the same way, for the purpose of data check and to provide more information about the items to the user, our interface also directs the user to the source of data by rightclicking on the element of interest (i.e., the artist name, an album, or a song). As we can see in Figure <ref type="figure" target="#fig_3">4a</ref>, the user can choose the option "Display in the Wasabi Explorer", which will redirect them to the artist's page in the WASABI Explorer 7 .</p><p>7 https://wasabi.i3s.unice.fr/</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">FORMATIVE EVALUATION</head><p>We conducted a semi-structured interview of about one hour via a video conference call with an expert on musicology (over 15 years of experience), specialized in studying music through computer technology (e.g., computer-assisted composition, sound spatialization, timbre analysis, etc.). The goals of this interview were two-fold: (i) to better understand the needs of domain experts in terms of exploring artist discography and collaboration data and (ii) to assess the extent to which our visualization approach can support those needs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Protocol and Questions</head><p>The interview followed a three-part protocol. It began with a set of questions regarding the user needs in terms of exploring artist discography and collaboration data. In particular, the interviewer asked them the following questions:</p><p>• Is exploring artist discography important? Why?</p><p>• When exploring discography, how important is to explore artist collaboration? Please provide examples.</p><p>• What is difficult when analyzing artist collaboration data?</p><p>• What attributes do you consider important when defining the discography/collaboration of two or more music artists? Rank them by importance</p><p>• In your opinion, how important is it to access audio information throughout the exploration of artist discography/collaboration?</p><p>In the second part of the interview, the interviewer showed the tool through a set of use case scenarios designed to guide the expert user during the semi-directed interview. These scenarios demonstrate the different aspects of the tool (described in Section 4), namely: (i) exploring the discography of a particular artist (Queen), (ii) using audio to explore the artist discography, (iii) exploring the collaborations of a particular artist, (iv) investigating the collaboration between two artists in a certain period of time, and (v) further exploring songs through external services (Wasabi Explorer and TimeSide). After each scenario, the interviewer asked them to identify (i) three things they like about the scenario, (ii) three things they dislike, and (iii) suggestions on how to improve it.</p><p>Finally, in the debriefing part, a set of general questions was asked about the usefulness of the tool to support their exploration needs. In particular, the interviewer asked them the following questions:</p><p>• In your opinion, what is the usefulness of a visualization technique [in general and this specific visualization] to explore artist discography?</p><p>• In your opinion, what is the usefulness of a visualization technique to explore artist collaboration?</p><p>• In your opinion, what is the usefulness of multimedia to explore artist discography and collaboration?</p><p>• Would you be willing to use this visualization? If yes, in what situation?</p><p>Throughout the interview, the questions could include follow-up questions based on the responses of the interviewee, which for the sake of simplicity is called domain expert.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">Exploration needs</head><p>According to our domain expert, exploring artist discography is important as it allows one to observe how the music of an artist evolves over time, as well as to identify the time periods when the artist were the most productive. In terms of artist collaboration, the domain expert had already studied the collaborations of an artist with sound engineers and compositors. When studying the collaborative projects between groups, it was considered important to identify the influences of a group on another in terms of style, sound and music diversity. However, collaboration information is not relevant for every artist, as not every collaborative project is interesting, i.e. not every collaborative project impacts the music style of an artist or results in successful songs.</p><p>When asked about the relevant attributes in artist discography, the domain expert mentioned music style of the artist/group, the variation of music styles over multiple pieces of the song (there is no taxonomy to describe this variation), tempo, and instrumentation. Regarding the relevant attributes in artist collaboration, the domain expert find important to provide elements that describe the influence of an artist over another; in particular, whether (or not) there were an influence over another artist and if that influence remains over the next albums. Another relevant aspect is to analyze the impacts the collaborative project had on the career of an artist and how that collaboration was received by the public. Finally, the domain expert believes that the usage of audio signal is fundamental when exploring artist discography and collaboration, as it allows one to illustrate the data (e.g., identify the influence of style by comparing audios).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">Appraisal of the visualization tool</head><p>Overall, the domain expert was satisfied with the tool in all the different use case scenarios. The way we represented the discography of an artist and the easy and direct access to different information and tools was greatly appreciated.</p><p>In terms of the auditory exploration approach, the domain expert enjoyed the ability to click on a visual variable that represents a song and immediately listen to the audio extract while seeing the spectrum, volume, and balance variations directly on the augmented audio player (a suggestion was made to make it larger to allow better perception of the visual variations). The domain expert also appreciated the ability to control the volume and speed of the audio and to play the audio summary of the previous or next album, which supports a dynamic browsing of the discography of artists.</p><p>When exploring collaborations between two artists over a certain period of time, the domain expert enjoyed the ability to zoom in on the productions and collaborations over a certain year while displaying the links between particular songs. It was mentioned that the number of links displayed could clutter the visualization, hindering the identification of the different colors encoding the different collaboration types. Nonetheless, the domain expert acknowledged that hovering an item or link highlights the important information, thus mitigating the cluttering effect.</p><p>In general, tool-tips displaying more information about an item was praised, but the domain expert would rather like to have more information (such as the history of the artist/group, the name of albums, etc.) displayed directly on the interface, reducing the mouse movements necessary to display more information about an item (e.g., a song). It was also suggested to have another visual representation when clicking on an album, which would enable the exploration of different types of songs.</p><p>The domain expert appreciated the link between the visualization tool and the external services; in particular, the link to the WASABI Explorer as it provides detailed information about the album, the song, or the artist to deepen the analysis. The domain expert was happy with the possibility of directly exploring the results of the MIR analysis on the audio signal of a particular song. However, the domain expert would prefer to have the TimeSide interface embedded in the visualization tool, instead of being redirected to a different window. The domain expert also proposed to include multiple ways of accessing those external services, such as through a button that opens up the WASABI Explorer or TimeSide without a particular song associated (which could be chosen directly in the respective interfaces).</p><p>A more general concern of the domain expert was related to the accuracy of the data, as the discography of an artist is sometimes represented with more albums then the actual discography (the original albums). The data on the Web, although powerful, is known to have many quality issues, which is one of the matters addressed by the participant. In the presented scenario (i.e. exploration of Queen discography), they observed that sometimes Queen does not appear as the composer of their own songs, particularly at the early years of their career. This particular case could be explained by the fact that, at the beginning, the songs were signed by whichever member of the group had composed them, it was only after 1995 that Queen started signing some of their songs as a band (i.e. Queen (band)). This is, however, a type of issue that requires knowledge about the data context to be fixed (or understood).</p><p>Finally, the domain expert said that they would use this visualization in their daily work for tasks such as performing research on artists, identifying important time periods of their productions, and exploring the discography of different artists.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">DISCUSSION</head><p>The visualization approach proposed in this paper to assist in the exploration of artist discography and collaborations, raises some points for discussion, as follows:</p><p>Analytical tasks. We propose a four-fold approach to support the resolution of domain tasks. First, we propose a visual stream-graph representation that shows the variation of artist production over time together with their discography. Second, we combine multiple stream-graphs side-by-side to support (i) comparison of artist production over time and (ii) exploration of their interactions at different time periods, defined by different products (albums/songs). Third, we propose an auditory exploration of discographies, which allow users to listen to what they see, recognizing the data (successful songs) while increasing user perception. Finally, the fourth approach refers to the integration of external services such as TimeSide which allows to deepen the exploration of songs through MIR analysis.</p><p>Visual design and interactions. Our approach represents multiple and complex dimensions of the data: time, network, attributes. As we deal with musical data, we combine visual and auditory senses to support multiple domain-based tasks while improving user perception. We leverage a juxtaposed layout to provide clear representation of artist discography and their collaboration network side-by-side, while providing focus+context interaction to support a deeper exploration of collaboration data. To reduce visual clutter, our exploration approach also allows users to incrementally include or remove artist data, as well as to hide/show the products (albums/songs) or the production profile (stream-graph) according to the ongoing analysis and the space available. By clicking on artist, album, or song, the user can listen to an extract of the artist discography, album, or song, respectively. Furthermore, each artist, album, and song are linked to their respective page in the WASABI Explorer, and each song can be further explored on TimeSide through a simple selection on the visualization.</p><p>Auditory data exploration. To support auditory exploration of data, we extracted a set of audio thumbnails that represent the artist discography or a particular album and integrated them to the visualization. Currently, these audio thumbnails are generated through extracts of about 3 seconds from the middle of each song, using a shift function to avoid extracting silent audio. In the future work, as we have access to 1.7M of song lyrics in the WASABI dataset 8 , we intend to use an hybrid technique based on song lyrics and audio content <ref type="bibr" target="#b11">[12]</ref> to extract meaningful pieces of audio.</p><p>Usability and Suitability. We assessed our approach through a semi-structured interview with a musicologist. Although we have only interviewed one expert user, the results suggest that we have a promising approach to support musicologists in the resolution of domain-related tasks. Future work includes the design and implementation of user-based evaluations to investigate the usability of our approach for exploring artist discography and collaboration networks of different artists and its suitability for assisting the resolution of domain-related tasks.</p><p>Generalization. Although the use case study of this paper is the WASABI dataset, the visualization technique has been implemented in a way that it can be supplied with data describing the discography and collaboration of any artist regardless of the source. Moreover, the technique can be ex- 8 Privately available (contact for potential academic collaboration) tended to explore other types of collaboration networks (e.g., scientific co-authorship). However, the auditory exploration feature relies on an audio thumbnailing procedure that is not currently automatized, as it requires a manual retrieval of songs that will feed the audio thumbnailing scripts. Future work includes to fully automatize this procedure to support a complete visual and auditory exploration of all music data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">CONCLUSIONS AND FUTURE WORK</head><p>We presented a web-based interactive tool that supports an incremental and multimodal exploration of music data through visual and auditory stimulus. In particular, our approach supports the analysis of artist discography and collaboration network. We have also linked our visualization with the TimeSide tool to support MIR analysis of the songs. We have demonstrated the feasibility and usefulness of our tool through a semi-structured interview with a domain (musicology) expert user, who gave us valuable feedback on the utility of our tool to solve the domain-related tasks and on the aspects that need improving. Future work includes the design and implementation of user-based evaluations to assess the usability and effectiveness of our approach to support domain-related tasks, as well as to support the exploration of data from different artists and sources. We provide auditory exploration of data through audio thumbnails played on-demand by the user while exploring the data in the visualization. Our proof-of-concept support auditory exploration of only a limited set of songs, which should be extended in future work. We also intend to improve the audio thumbnailing process by using the method proposed by Fell et al. <ref type="bibr" target="#b11">[12]</ref> to enhance the exploration process by providing more meaningful audio extracts. The tool is available at http://dataviz.i3s.unice.fr/muvin.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">ACKNOWLEDGMENTS</head><p>We thank the domain expert for their time and valuable feedback that allowed us to strength the results of this research. This work is also partially funded by University of Côte d'Azur through its IDEX JEDI program (CC : C870A06232 EOTP : LINKED OPEN DATA DF : D103).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Overview of the visualization tool, including (a) a tool bar, where users can choose an artist to explore and search for particular songs in the visualization, (b) the visualization technique, displaying the discography of Queen band over time (from 1973 to 1995) as well as the songs of other artists in which they contributed, (c) an audio player, allowing the user to listen to a thumbnail of the artist discography or a particular album or yet, to listen to a whole song.</figDesc><graphic coords="5,53.80,53.79,502.13,252.77" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Focus+context exploration of artist discography. (a) Default presentation of albums and songs. (b) Focus on production of a particular year by dragging the slider over it. (c) Zoom into the productions of a particular year by clicking on the year of interest.</figDesc><graphic coords="6,53.80,53.80,502.11,169.52" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure3: The interactive process that support listening to an album thumbnail. (a) By hovering an item, the "play" icon indicates that the user can listen to that item, while the tooltip shows information about the item. (b) By clicking on the item, the system launches the audio player for that item; the current played song is then highlighted in the visualization. (c) the audio player displays the name of the current song thumbnail and displays the changes in balance and volume accordingly.</figDesc><graphic coords="7,53.80,86.16,239.10,244.21" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Exploration of artist collaboration. (a) Incremental integration of artist data by selecting a collaborator of the current artist. (b) Collaborations at the artist level represented by line segments between them. (c) Collaborations at the song level represented through colored arcs between them, where color encodes the type of contribution of each involved artist.</figDesc><graphic coords="8,53.80,53.80,502.09,111.98" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Overview of the Timeside-Player interface picturing timbre and rhythm analyses for the Queen song "Under Pressure". It shows (a) the audio player and the descriptors of (b) Spectral centroid, (c) Spectral kurtosis, (d) Spectral slope, (e) Onsets, (f ) Linear Spectrogram and (g) Waveform with spectral centroid color from the time window selected on the first waveform. The selected segment on the (c) Spectral Kurtosis curve is used to annotate a sound event in this part.</figDesc><graphic coords="8,53.80,226.79,239.09,270.16" type="bitmap" /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>https://wasabi.i3s.unice.fr/apidoc/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1"><p>http://wasabi.inria.fr/sparql</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2"><p>https://github.com/Ircam-Web/timeside-sdk-js</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_3"><p>https://github.com/Ircam-Web/timeside-player</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_4"><p>https://www.vamp-plugins.org/</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Musiclynx: Exploring music through artist similarity graphs</title>
		<author>
			<persName><forename type="first">A</forename><surname>Allik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Thalmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sandler</surname></persName>
		</author>
		<idno type="DOI">10.1145/3184558.3186970</idno>
	</analytic>
	<monogr>
		<title level="m">Companion Proceedings of the The Web Conference 2018, WWW &apos;18</title>
		<meeting><address><addrLine>Republic and Canton of Geneva, CHE</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="167" to="170" />
		</imprint>
	</monogr>
	<note>International World Wide Web Conferences Steering Committee</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A Descriptive Framework for Temporal Data Visualizations Based on Generalized Space-Time Cubes</title>
		<author>
			<persName><forename type="first">B</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dragicevic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Archambault</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hurter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Carpendale</surname></persName>
		</author>
		<idno type="DOI">10.1111/cgf.12804</idno>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">An overview of audio thumbnailing techniques and their relevance to the cogitch project</title>
		<author>
			<persName><forename type="first">J</forename><surname>Balen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Tech. Rep</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Audio thumbnailing of popular music using chroma-based representations</title>
		<author>
			<persName><forename type="first">M</forename><surname>Bartsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wakefield</surname></persName>
		</author>
		<idno type="DOI">10.1109/TMM.2004.840597</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="96" to="104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The State of the Art in Visualizing Dynamic Graphs</title>
		<author>
			<persName><forename type="first">F</forename><surname>Beck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Burch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Diehl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Weiskopf</surname></persName>
		</author>
		<idno type="DOI">10.2312/eurovisstar.20141174</idno>
	</analytic>
	<monogr>
		<title level="m">EuroVis -STARs. The Eurographics Association</title>
		<editor>
			<persName><forename type="first">R</forename><surname>Borgo</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Maciejewski</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">I</forename><surname>Viola</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The million song dataset</title>
		<author>
			<persName><forename type="first">T</forename><surname>Bertin-Mahieux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Ellis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Whitman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lamere</surname></persName>
		</author>
		<idno type="DOI">10.7916/D8NZ8J07</idno>
		<idno>10.7916</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. of the ISMIR Conf</title>
		<meeting>of the ISMIR Conf</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
	<note>/D8NZ8J07</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Les unités sémiotiques temporelles</title>
		<author>
			<persName><forename type="first">P</forename><surname>Bootz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Hautbois</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Séminaire MaMuX</title>
		<imprint>
			<publisher>IRCAM</publisher>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The wasabi dataset: Cultural, lyrics and audio analysis metadata about 2 million popular commercially released songs</title>
		<author>
			<persName><forename type="first">M</forename><surname>Buffa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Cabrio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Gandon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Giboin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hennequin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pauwels</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Pellerin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tikat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Semantic Web Conference</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="515" to="531" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Dissolution of the Notion of Timbre</title>
		<author>
			<persName><forename type="first">M</forename><surname>Chion</surname></persName>
		</author>
		<idno type="DOI">10.1215/10407391-1428906</idno>
	</analytic>
	<monogr>
		<title level="j">differences</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">2-3</biblScope>
			<biblScope unit="page" from="235" to="239" />
			<date type="published" when="2011">12 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Come Together, Right Now: An Empirical Study of Collaborations in the Music Industry</title>
		<author>
			<persName><forename type="first">A</forename><surname>Deshmane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Martinez-De Albeniz</surname></persName>
		</author>
		<idno type="DOI">10.2139/ssrn.3743462</idno>
	</analytic>
	<monogr>
		<title level="j">SSRN Electronic Journal</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Onset detection revisited</title>
		<author>
			<persName><forename type="first">S</forename><surname>Dixon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 9th International Conference on Digital Audio Effects</title>
		<meeting>the 9th International Conference on Digital Audio Effects</meeting>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">120</biblScope>
			<biblScope unit="page" from="133" to="137" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Lyrics segmentation via bimodal text-audio representation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Fell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Nechaev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Meseguer-Brocal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Cabrio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Gandon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Peeters</surname></persName>
		</author>
		<idno type="DOI">10.1017/S1351324921000024</idno>
	</analytic>
	<monogr>
		<title level="j">Natural Language Engineering</title>
		<imprint>
			<biblScope unit="page" from="1" to="20" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Dynamic exploration of recording sessions between jazz musicians over time</title>
		<author>
			<persName><forename type="first">D</forename><surname>Filippova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fitzgerald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kingsford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Benadon</surname></persName>
		</author>
		<idno type="DOI">10.1109/SocialCom-PASSAT.2012.78</idno>
	</analytic>
	<monogr>
		<title level="m">2012 International Conference on Privacy, Security, Risk and Trust and 2012 International Confernece on Social Computing</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="368" to="376" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Telemeta: An open-source web framework for ethnomusicological audio archives management and automatic analysis</title>
		<author>
			<persName><forename type="first">T</forename><surname>Fillon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Simonnot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-F</forename><surname>Mifune</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Khoury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Pellerin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">Amy</forename><surname>De La Bretèque</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Doukhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Fourer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-L</forename><surname>Rouas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pinquier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Barras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">1st International Digital Libraries for Musicology workshop</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Spleeter: a fast and efficient music source separation tool with pre-trained models</title>
		<author>
			<persName><forename type="first">R</forename><surname>Hennequin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Khlif</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Voituret</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Moussallam</surname></persName>
		</author>
		<idno type="DOI">10.21105/joss.02154</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Open Source Software</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">50</biblScope>
			<biblScope unit="page">2154</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Singing voice separation with deep u-net convolutional networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Jansson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Humphrey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Montecchio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bittner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Weyde</surname></persName>
		</author>
		<ptr target="https://openaccess.city.ac.uk/id/eprint/19289/" />
		<imprint>
			<date type="published" when="2017-10">March 2022. October 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Towards efficient audio thumbnailing</title>
		<author>
			<persName><forename type="first">N</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Müller</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICASSP.2014.6854593</idno>
	</analytic>
	<monogr>
		<title level="m">2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<biblScope unit="page" from="5192" to="5196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A survey on visualizations for musical data</title>
		<author>
			<persName><forename type="first">R</forename><surname>Khulusi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kusnick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Meinecke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gillmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Focht</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jänicke</surname></persName>
		</author>
		<idno type="DOI">10.1111/cgf.13905</idno>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="82" to="110" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName><forename type="first">O</forename><surname>Lartillot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Toiviainen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Eerola</surname></persName>
		</author>
		<title level="m">Mirtoolbox 1.6. 1 user&apos;s manual</title>
		<meeting><address><addrLine>Denmark</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
		<respStmt>
			<orgName>Aalborg University</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Automatic Musical Instrument Recognition and Related Topics</title>
		<author>
			<persName><forename type="first">A</forename><surname>Livshin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
		<respStmt>
			<orgName>Université Pierre et Marie Curie-Paris VI</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<author>
			<persName><surname>Merrian-Webster</surname></persName>
		</author>
		<ptr target="https://www.merriam-webster.com/dictionary/musicology" />
		<title level="m">Definition of musicology</title>
		<imprint>
			<date type="published" when="2018-03">2018. March 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Automatic recognition of lyrics in singing</title>
		<author>
			<persName><forename type="first">A</forename><surname>Mesaros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Virtanen</surname></persName>
		</author>
		<idno type="DOI">10.1155/2010/546047</idno>
	</analytic>
	<monogr>
		<title level="j">EURASIP Journal on Audio, Speech, and Music Processing</title>
		<imprint>
			<biblScope unit="page" from="1" to="11" />
			<date type="published" when="2010">2010. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Visualizing linked Jazz: A web-based tool for social network analysis and exploration</title>
		<author>
			<persName><forename type="first">M</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Walloch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">C</forename><surname>Pattuelli</surname></persName>
		</author>
		<idno type="DOI">10.1002/meet.14504901295</idno>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the American Society for Information Science and Technology</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="3" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Local sound signatures for music recommendations</title>
		<author>
			<persName><forename type="first">L</forename><surname>Pottier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Timbre 2018: Timbre Is a Many-Splendored Thing</title>
		<meeting>Timbre 2018: Timbre Is a Many-Splendored Thing<address><addrLine>Montréal, Québec, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="65" to="66" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Étude des caractéristiques acoustiques de quatre pièces du groupe led zeppelin et comparaison avec d&apos;autres répertoires</title>
		<author>
			<persName><forename type="first">L</forename><surname>Pottier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Led Zeppelin -Contexte, analyse, réception</title>
		<imprint>
			<biblScope unit="page" from="109" to="132" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title/>
		<author>
			<persName><surname>Ph</surname></persName>
		</author>
		<author>
			<persName><surname>Gonin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
			<publisher>EUD</publisher>
			<pubPlace>Dijon</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Schaeffer</surname></persName>
		</author>
		<title level="m">Traité des objets musicaux</title>
		<imprint>
			<publisher>Média Diffusion</publisher>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Spectro-morphology and structuring processes</title>
		<author>
			<persName><forename type="first">D</forename><surname>Smalley</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-1-349-18492-7_5</idno>
	</analytic>
	<monogr>
		<title level="m">The language of electroacoustic music</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1986">1986</date>
			<biblScope unit="page" from="61" to="93" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Animation: can it facilitate? International journal of human-computer studies</title>
		<author>
			<persName><forename type="first">B</forename><surname>Tversky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Morrison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Betrancourt</surname></persName>
		</author>
		<idno type="DOI">10.1006/ijhc.2002.1017</idno>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="page" from="247" to="262" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Pasts and futures of digital humanities in musicology: Moving towards a &quot;bigger tent</title>
		<author>
			<persName><forename type="first">M</forename><surname>Urberg</surname></persName>
		</author>
		<idno type="DOI">10.1080/10588167.2017.1404301</idno>
	</analytic>
	<monogr>
		<title level="j">Music Reference Services Quarterly</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3-4</biblScope>
			<biblScope unit="page" from="134" to="150" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Musician map: Visualizing music collaborations over time</title>
		<author>
			<persName><forename type="first">J.-D</forename><surname>Yim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shaw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bartram</surname></persName>
		</author>
		<idno type="DOI">10.1117/12.812377</idno>
	</analytic>
	<monogr>
		<title level="m">Visualization and Data Analysis</title>
		<meeting><address><addrLine>VDA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009">2009. 2009</date>
			<biblScope unit="page">72430</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
