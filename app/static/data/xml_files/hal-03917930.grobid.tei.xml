<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Black-box language model explanation by context length probing</title>
				<funder ref="#_GY8Qc2Y #_KyTp7a4">
					<orgName type="full">Agence Nationale de la Recherche</orgName>
					<orgName type="abbreviated">ANR</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Ondřej</forename><surname>Cífka</surname></persName>
							<email>cifka@matfyz.cz</email>
							<affiliation key="aff0">
								<orgName type="laboratory" key="lab1">Zenith Team</orgName>
								<orgName type="laboratory" key="lab2">LIRMM</orgName>
								<orgName type="laboratory" key="lab3">CNRS UMR 5506 Inria</orgName>
								<orgName type="institution">Université de Montpellier</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Antoine</forename><surname>Liutkus</surname></persName>
							<email>antoine.liutkus@inria.fr</email>
							<affiliation key="aff0">
								<orgName type="laboratory" key="lab1">Zenith Team</orgName>
								<orgName type="laboratory" key="lab2">LIRMM</orgName>
								<orgName type="laboratory" key="lab3">CNRS UMR 5506 Inria</orgName>
								<orgName type="institution">Université de Montpellier</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Black-box language model explanation by context length probing</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">C22B5D20BC67AE23298320314EB1EEA6</idno>
					<idno type="DOI">10.18653/v1/2023.acl-short.92</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-04-12T14:52+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The increasingly widespread adoption of large language models has highlighted the need for improving their explainability. We present context length probing, a novel explanation technique for causal language models, based on tracking the predictions of a model as a function of the length of available context, and allowing to assign differential importance scores to different contexts. The technique is modelagnostic and does not rely on access to model internals beyond computing token-level probabilities. We apply context length probing to large pre-trained language models and offer some initial analyses and insights, including the potential for studying long-range dependencies. The source code 1 and an interactive demo 2 of the method are available.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Large language models (LMs), typically based on the Transformer architecture <ref type="bibr" target="#b29">(Vaswani et al., 2017)</ref>, have recently seen increasingly widespread adoption, yet understanding their behaviour remains a difficult challenge and an active research topic.</p><p>Notably, as the length of the context that can be accessed by LMs has grown, a question that has attracted some attention is how this influences their predictions. Some recent studies in this line of research suggest that even "long-range" LMs focus heavily on local context and largely fail to exploit distant ones <ref type="bibr" target="#b20">(O'Connor and Andreas, 2021;</ref><ref type="bibr" target="#b26">Sun et al., 2021;</ref><ref type="bibr" target="#b23">Press et al., 2021;</ref><ref type="bibr" target="#b27">Sun et al., 2022)</ref>. A more nuanced understanding of how contexts of different lengths influence LMs' predictions may hence be valuable for further improving their performance, especially on tasks like long-form text generation where long-range dependencies are of critical importance.</p><p>Figure <ref type="figure">1</ref>: A screenshot of a demo<ref type="foot" target="#foot_1">2</ref> of the proposed method. After selecting a target token (here "birds"), the preceding tokens are highlighted according to their (normalized) differential importance scores (green = positive, red = negative), obtained using our method. The user can also explore the top predictions for contexts of different lengths (here the context "house, shouting about lunatics. [. . .] mortally afraid of").</p><p>In this work, we propose context length probing, a simple explanation technique for causal (autoregressive) language models, based on tracking the predictions of the model as a function of the number of tokens available as context. Our proposal has the following advantages:</p><p>• It is conceptually simple, providing a straightforward answer to a natural question: How does the length of available context impact the prediction?</p><p>• It can be applied to a pre-trained model without retraining or fine-tuning and without training any auxiliary models.</p><p>• It does not require access to model weights, internal representations or gradients.</p><p>• It is model-agnostic, as it can be applied to any causal LM, including attentionless architectures like RNN <ref type="bibr" target="#b18">(Mikolov et al., 2010)</ref> and <ref type="bibr">CNN (Dauphin et al., 2017)</ref>. The only requirement for the model is to accept arbitrary input segments (i.e. not be limited to document prefixes).</p><p>Furthemore, we propose a way to use this technique to assign what we call differential importance scores to contexts of different lengths. This can be seen as complementary to other techniques like attention or saliency map visualization. Interestingly, contrary to those techniques, ours appears promising as a tool for studying long-range dependencies, since it can be expected to highlight important information not already covered by shorter contexts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>A popular way to dissect Transformers is by visualizing their attention weights (e.g. <ref type="bibr" target="#b30">Vig, 2019;</ref><ref type="bibr" target="#b10">Hoover et al., 2020)</ref>. However, it has been argued that this does not provide reliable explanations and can be misleading <ref type="bibr" target="#b11">(Jain and Wallace, 2019;</ref><ref type="bibr" target="#b25">Serrano and Smith, 2019)</ref>. A more recent line of work <ref type="bibr" target="#b6">(Elhage et al., 2021;</ref><ref type="bibr" target="#b21">Olsson et al., 2022)</ref> explores "mechanistic explanations", based on reverse-engineering the computations performed by Transformers. These techniques are tied to concrete architectures, which are often "toy" versions of those used in real-world applications, e.g. attention-only Transformers in Elhage et al.</p><p>Other options include general-purpose methods like neuron/activation interpretation (e.g. <ref type="bibr" target="#b8">Geva et al., 2021;</ref><ref type="bibr" target="#b9">Goh et al., 2021;</ref><ref type="bibr" target="#b3">Dai et al., 2022)</ref>, saliency maps (e.g. <ref type="bibr" target="#b7">Fong and Vedaldi, 2017;</ref><ref type="bibr" target="#b0">Ancona et al., 2019)</ref> and influence functions <ref type="bibr" target="#b15">(Koh and Liang, 2017)</ref>. These require access to internal representations and/or the ability to backpropagate gradients, and have some caveats of their own <ref type="bibr" target="#b13">(Kindermans et al., 2019;</ref><ref type="bibr" target="#b17">Kokhlikyan et al., 2021)</ref>.</p><p>More closely related to our work are studies that perform ablation (e.g. by shuffling, truncation or masking) on different contexts to understand their influence on predictions <ref type="bibr" target="#b20">(O'Connor and Andreas, 2021;</ref><ref type="bibr" target="#b26">Sun et al., 2021;</ref><ref type="bibr" target="#b23">Press et al., 2021;</ref><ref type="bibr" target="#b28">Vafa et al., 2021)</ref>. To our knowledge, all such existing works only test a few select contexts or greedily search for the most informative one; in contrast, we show that it is feasible to consider all context lengths in the range from 1 to a maximum c max , which permits us to obtain fine-grained insights on the example level, e.g. in the form of the proposed differential importance scores. Moreover, many existing analyses (e.g. <ref type="bibr" target="#b28">Vafa et al., 2021;</ref><ref type="bibr" target="#b20">O'Connor and Andreas, 2021)</ref> rely on specific training or finetuning, which is not the case with our proposal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Context length probing</head><p>A causal LM estimates the conditional probability distribution of a token given its left-hand context in a document:</p><formula xml:id="formula_0">p(x n+1 | x 1 , . . . , x n ).</formula><p>(1)</p><p>We are interested here in computing the probabilities conditioned on a reduced context of length c ∈ {1, . . . , n}:</p><formula xml:id="formula_1">p(x n+1 | x n-c+1 , . . . , x n ),<label>(2)</label></formula><p>so that we may then study the behavior of this distribution as a function of c. An apparent obstacle in doing so is that applying the model to an arbitrary subsequence x n-c+1 , . . . , x n , instead of the full document x 1 , . . . , x N , may lead to inaccurate estimates of the probabilities in Eq. ( <ref type="formula" target="#formula_1">2</ref>). However, we note that large LMs are not usually trained on entire documents. Instead, the training data is pre-processed by shuffling all the documents, concatenating them (with a special token as a separator), and splitting the resulting sequence into chunks of a fixed length (usually 1024 or 2048 tokens) with no particular relation to the document length. Thus, the models are effectively trained to accept sequences of tokens starting at arbitrary positions in a document and it is therefore correct to employ them as such to compute estimates of Eq. (2). <ref type="foot" target="#foot_2">3</ref>It now remains to be detailed how to efficiently evaluate the above probabilities for all positions n and context lengths c. Specifically, for a given document x 1 , . . . , x N and some maximum context length c max , we are interested in an (N -1) × c max × |V| tensor P , where V = w 1 , . . . , w |V| is the vocabulary, such that: <ref type="foot" target="#foot_3">4</ref> Observe that by running the model on any segment x m , . . . , x n , we obtain all the values P m+c-1,c, * for c ∈ {1, . . . , n -m + 1}. Therefore, we can fill in the tensor P by applying the model along a sliding window of size c max , i.e. running it on N (overlapping) segments of length at most c max . See Appendix A for an illustration and additional remarks.</p><formula xml:id="formula_2">P n,c,i = p(x n+1 = w i | x n-c+1 , . . . , x n ), (3) with P n,c, * = P n,n-1, * for n ≤ c.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Metrics</head><p>Having obtained the tensor P as we have just described, we use it to study how the predictions evolve as the context length is increased from 1 to c max . Specifically, our goal is to define a suitable metric that we can compute from P n,c, * and follow it as a function of c (for a specific n or on average).</p><p>One possibility would be to use the negative loglikelihood (NLL) loss values:</p><formula xml:id="formula_3">-log p(x n+1 | x n-c+1 , . . . , x n ).</formula><p>(4) However, this may not be a particularly suitable metric for explainability purposes, as it depends (only) on the probability assigned to the ground truth x n+1 , while the LM outputs a probability distribution P n,c, * over the entire vocabulary, which may in fact contain many other plausible continuations. For this reason, we propose to exploit a metric defined on whole distributions, e.g. the Kullback-Leibler (KL) divergence. To achieve this, we choose the maximum-context predictions P n,cmax, * as a reference and get: (5)</p><p>The rationale for ( <ref type="formula">5</ref>) is to quantify the amount of information that is lost by using a shorter context c ≤ c max . Interestingly, this metric is not related to the absolute performance of the model with maximal context, but rather to how the output changes if a shorter context is used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Differential importance scores</head><p>We are also interested in studying how individual increments in context length affect the predictions.</p><p>We propose to quantify this as the change in the KL divergence metric (5) when a new token is introduced into the context. Specifically, for a pair of tokens x n+1 (the target token) and x m (the context token), we define a differential importance score (∆-score for short)</p><formula xml:id="formula_4">∆D n,m = D n,n-m-1 -D n,n-m .<label>(6)</label></formula><p>We may visualize these scores as a way to explain the LM predictions, much like is often done with attention weights, with two important differences. First, a high ∆D n,m should not be interpreted as meaning that x m in isolation is important for predicting x n+1 , but rather that it is salient given the context that follows it (which might mean that it brings information not contained in the following context). Second, unlike attention weights, our scores need not sum up to one, and can be negative; in this regard, the proposed representation is more conceptually similar to a saliency map than to an attention map.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results</head><p>We apply the proposed technique to publicly available pre-trained large Transformer language models, namely GPT-J (Wang and Komatsuzaki, 2021) and two GPT-2 <ref type="bibr" target="#b24">(Radford et al., 2019)</ref> variantssee Table <ref type="table" target="#tab_0">1</ref> for an overview. We use the validation set of the English LinES treebank<ref type="foot" target="#foot_4">5</ref> from Universal Dependencies (UD; <ref type="bibr" target="#b19">Nivre et al., 2020)</ref>, containing 8 documents with a total length of 20 672 tokens<ref type="foot" target="#foot_5">6</ref> and covering fiction, an online manual, and Europarl data. We set c max = 1023. We use the Transformers library<ref type="foot" target="#foot_6">7</ref>  <ref type="bibr" target="#b32">(Wolf et al., 2020)</ref> to load the pre-trained models and run inference. Further technical details are included in Appendix B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">LM loss by context length</head><p>Fig. <ref type="figure">2</ref> shows the cross entropy losses (NLL means) across the whole validation dataset as a function of context length c. As expected, larger models perform better than smaller ones, which is traditionally explained by their larger capacity. A less common observation we can make thanks to this detailed representation is that the gains in performance come mostly from relatively short contexts (8-256 tokens); this is consistent with prior works <ref type="bibr" target="#b26">(Sun et al., 2021;</ref><ref type="bibr" target="#b23">Press et al., 2021)</ref>   that very long contexts bring only minimal improvement (though these focused on specific long-range architectures and on contexts beyond the range we investigate here).</p><p>In Fig. <ref type="figure" target="#fig_1">3</ref>, we display the same information (loss by context length) broken down by part-of-speech (POS) tags, for GPT-J only. For most POS tags, the behavior is similar to what we observed in Fig. <ref type="figure">2</ref> and the loss appears to stabilize around context lengths 16-64. However, we see a distinct behaviour for proper nouns (PROPN), which are the hardest-to-predict category for short contexts, but whose loss improves steadily with increasing c, surpassing that of regular nouns (NOUN) at c = 162 and continuing to improve beyond that point.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Per-token losses by context length</head><p>We have also examined token-level losses, as well as the KL divergence metric (see Section 3.2); an example plot is shown in Fig. <ref type="figure">4</ref> and more are found in Appendix C.1. In general, we observe that the values tend to change gradually with c; large differences are sparse, especially for large c, and can often be attributed to important pieces of information appearing in the context (e.g. "owl" and "swoop" in the context of "birds" in Fig. <ref type="figure">4</ref>). This justifies our use of these differences as importance scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Differential importance scores</head><p>To facilitate the exploration of ∆-scores from Section 3.3, we have created an interactive web demo, 2 which allows visualizing the scores for any of the 3 models on the validation set as shown in Fig. <ref type="figure">1</ref>.</p><p>In Fig. <ref type="figure">5</ref>, we display the magnitudes of the ∆scores -normalized for each position to sum up to 1 across all context lengths -as a function of context length. The plot suggests a power-law-like inverse relationship where increasing context length proportionally reduces the ∆-score magnitude on average. We interpret this as far-away tokens being less likely to carry information not already covered by shorter contexts. Long contexts (see inset in Fig. <ref type="figure">5</ref>) bear less importance for larger models than for smaller ones, perhaps because the additional capacity allows relying more on shorter contexts.</p><p>In Fig. <ref type="figure">6</ref>, we also display the mean importance score received by each POS category, by model. We can see that proper nouns (PROPN) are substantially more informative than other categories (which is in line with the observations in the previous section), but less so for the smallest model. This could mean e.g. that larger models are better at memorizing named entities from training data and using them to identify the topic of the document, or simply at copying them from distant context as observed in <ref type="bibr" target="#b26">(Sun et al., 2021)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Limitations and future directions</head><p>Experiments. We acknowledge the limited scope of our experiments, including only 8 (closeddomain) documents, 3 models and a single language. This is largely due to the limited availability of suitable large LMs and their high computational cost. Still, we believe that our experiments are valuable as a case study that already clearly showcases some interesting features of our methodology.</p><p>Computational cost. While we have demonstrated an efficient strategy to obtain predictions for all tokens at all possible context lengths, it still requires running the model N times for a document of length N .</p><p>For a k-fold reduction in computational cost, the technique may be modified to use a sliding window with stride k &gt; 1 (instead of k = 1 as proposed above). See Appendix A.1 for details. Choice of metrics. The proposed methodology allows investigating how any given metric is impacted by context, yet our study is limited to NLL loss and the proposed KL divergence metric (the latter for defining importance scores). These may not be optimal for every purpose, and other choices should be explored depending on the application. For example, to study sequences generated (sampled) from a LM, one might want to define importance scores using a metric that does depend on the generated token, e.g. its NLL loss or its ranking among all candidates. (Indeed, our web demo also supports ∆-scores defined using NLL loss values.)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion and future directions</head><p>We have presented context length probing, a novel causal LM explanation technique based on tracking the predictions of the LM as a function of context length, and enabling the assignment of differential importance scores (∆-scores). While it has some advantages over existing techniques, it answers different questions, and should thus be thought of as complementary rather than a substitute.</p><p>A particularly interesting feature of our ∆-scores is their apparent potential for discovering longrange dependencies (LRDs) (as they are expected to highlight information not already covered by shorter contexts, unlike e.g. attention maps).</p><p>Remarkably, our analysis suggests a power-lawlike inverse relationship between context length and importance score, seemingly questioning the importance of LRDs in language modeling. While LRDs clearly appear crucial for applications such as longform text generation, their importance may not be strongly reflected by LM performance metrics like cross entropy or perplexity. We thus believe that there is an opportunity for more specialized benchmarks of LRD modeling capabilities of different models, such as that of <ref type="bibr" target="#b27">Sun et al. (2022)</ref>, for example. These should further elucidate questions like to what extent improvements in LM performance are due to better LRD modeling, how LRDs are handled by various Transformer variants (e.g. <ref type="bibr" target="#b14">Kitaev et al., 2020;</ref><ref type="bibr" target="#b12">Katharopoulos et al., 2020;</ref><ref type="bibr" target="#b2">Choromanski et al., 2021;</ref><ref type="bibr" target="#b22">Press et al., 2022)</ref>, or what their importance is for different tasks. When the LM is run on a segment of the document, the effective context length for each target token is equal to its offset from the beginning of the segment, e.g. the context for predicting " D" is " the" (c = 1), the context for "urs" is " the D" (c = 2), etc.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Context length probing</head><p>Fig. <ref type="figure" target="#fig_3">7</ref> illustrates a step of context length probing. We wish to obtain the tensor P from Eq. ( <ref type="formula">3</ref>), understood as a table where each cell contains the predictions (next-token logits) for a given position in the text and a given context length. By running our LM on a segment of the text, we get predictions such that for the n-th token in the segment, the effective context length is equal to n, which corresponds to a diagonal in the table. We can thus fill in the whole table by running the LM on all segments of length c max (plus trailing segments of lengths c max -1, . . . , 1).</p><p>Notice that this process is somewhat similar to (naïvely) running the LM in generation mode, except that at each step, the leading token is removed, preventing the use of caching to speed up the computation.</p><p>In practice, it is not necessary to explicitly construct the tensor P . Indeed, we find it more efficient to instead store the raw logits obtained by running the model on all the segments, then do the necessary index arithmetics when computing the metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Strided context length probing</head><p>For a k-fold reduction in computational cost, we may instead use a sliding window with a stride k &gt; 1, i.e. run the model only on segments starting at positions k (n -1) + 1 for all n ∈ {1, . . . , ⌈N/k⌉}, rather than all positions. This way, for a target token x n+1 , we obtain the predictions p(x n+1 | x n-c+1 , . . . , x n ) only for such context lengths c that c mod k = n. In other words, predictions with context length c are only available for tokens x c+1 , x c+k+1 , x c+2k+1 , . . .. Consequently:</p><p>• Overall, we still cover all context lengths 1, . . . , c max , allowing us to perform aggregate analyses like the ones in Section 4.1.</p><p>• When analyzing the predictions for a specific target token in a document (e.g. to compute ∆-scores), context tokens come in blocks of length k. Visualizations like the ones in Figs. 1 and 4 are still possible for all target tokens, but become less detailed, grouping every k context tokens together.</p><p>• Computation time, as well as the space needed to store the predictions, is reduced by a factor of k.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Technical details</head><p>Data. The LinES treebank is licensed under Creative Commons BY-NC-SA 4.0. We concatenated all tokens from each of the documents from the treebank, then re-tokenized them using the GPT-2 tokenizer.</p><p>We mapped the original (UD) POS tags to the GPT-tokenized dataset in such a way that every GPT token is assigned the POS tag of the first UD token it overlaps with.</p><p>Models. We used the models EleutherAI/gpt-j-6B (Apache 2.0 license), and gpt2-xl and gpt2 (MIT license), all from huggingface.co.</p><p>Computation. We parallelized the inference over 500 jobs on a compute cluster,<ref type="foot" target="#foot_7">8</ref> each running on 8 CPU cores with at least 8 GB of RAM per core, with a batch size of 16. Each job took about 10-20 min for GPT-2 and 30-60 min for GPT-J. Additionally, computing the metrics from the logits (which take up 2 TB of disk space in float16) took between 2 and 4 h per model on a single machine with 32 CPU cores. The total computing time was 318 core-days, including debugging and discarded runs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Additional plots</head><p>C.1 Token-wise metrics as a function of context length</p><p>Figs. 8 and 9 show NLL and KL divergence (5), respectively, as a function of context length, for selected target tokens (proper nouns) from the validation set.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>D</head><label></label><figDesc>n,c = D KL [P n,cmax, * ∥ P n,c, * ] = |V| i=1 P n,cmax,i log P n,cmax,i P n,c,i .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Mean GPT-J loss by context length and partof-speech (POS) tag of the target token. Only POS tags with at least 100 occurrences in the dataset are included. The tags are grouped (arbitrarily) for clarity.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :Figure 5 :Figure 6 :</head><label>456</label><figDesc>Figure4: NLL (left) and KL divergence (right) as a function of context length for a selected example: "[. . .] mortally afraid of birds" (same as in Fig.1). The x axis is reversed for visual correspondence with the left-hand context. The 5 context tokens causing the largest drops in each metric for GPT-J are marked by red dots.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 7 :</head><label>7</label><figDesc>Figure7: A step of context length probing with c max = 10. The input tokens are shown at the top, the target tokens at the bottom. When the LM is run on a segment of the document, the effective context length for each target token is equal to its offset from the beginning of the segment, e.g. the context for predicting " D" is " the" (c = 1), the context for "urs" is " the D" (c = 2), etc.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Hyperparameters of the 3 models used.</figDesc><table><row><cell cols="5">name #param #layer #head d model max len</cell></row><row><cell>gpt2</cell><cell>117 M</cell><cell>12</cell><cell>12 768</cell><cell>1024</cell></row><row><cell cols="2">gpt2-xl 1.5 B</cell><cell>48</cell><cell>25 1600</cell><cell>1024</cell></row><row><cell cols="2">gpt-j-6B 6.1 B</cell><cell>28</cell><cell>16 4096</cell><cell>2048</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>https://github.com/cifkao/ context-probing/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>https://cifkao.github.io/ context-probing/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>For models trained on data that is pre-processed differently, (re)training or fine-tuning with data augmentation such as random shifts may be needed in order to apply our method, analogously to<ref type="bibr" target="#b28">Vafa et al. (2021)</ref>, who use word dropout to ensure compatibility with their method.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"><p>  4  Pn,c, * is a |V|-dimensional slice of P along the last axis.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4"><p>https://universaldependencies.org/ treebanks/en_lines/index.html</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5"><p>After concatenating all sentences and applying the GPT-2 tokenizer, which is used by both GPT-2 and GPT-J.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_6"><p>https://github.com/huggingface/ transformers</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_7"><p>Nef, the cluster computing infrastructure of Inria Sophia Antipolis Méditerranée; see https://wiki.inria.fr/ ClustersSophia</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>This work was supported by the <rs type="funder">LabEx NUMEV</rs> (<rs type="grantNumber">ANR-10-LABX-0020</rs>) within the <rs type="projectName">I-Site MUSE</rs> (<rs type="grantNumber">ANR-16-IDEX-0006</rs>). The authors are grateful to the OPAL infrastructure from <rs type="institution">Université Côte d'Azur</rs> for providing resources and support.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_GY8Qc2Y">
					<idno type="grant-number">ANR-10-LABX-0020</idno>
					<orgName type="project" subtype="full">I-Site MUSE</orgName>
				</org>
				<org type="funding" xml:id="_KyTp7a4">
					<idno type="grant-number">ANR-16-IDEX-0006</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>(a) . . . and attribute means (and thus how the data between them will look in a browser), XML uses the tags only to delimit pieces of data, and leaves the interpretation of the data completely to the application that reads it. Additional information about XML can be found on the web site. About importing XML data Access   Below each plot, the target token is displayed in bold, along with a context of 60 tokens. The x axis is reversed to correspond visually to left-hand context. The red dots show the 10 tokens that cause the largest drops in the metric (for GPT-J) when added to the context.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Explaining deep neural networks with a polynomial time algorithm for Shapley value approximation</title>
		<author>
			<persName><forename type="first">Marco</forename><surname>Ancona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cengiz</forename><surname>Oztireli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Markus</forename><surname>Gross</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International Conference on Machine Learning</title>
		<title level="s">Proceedings of Machine Learning Research</title>
		<meeting>the 36th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="272" to="281" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName><surname>Pmlr</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Rethinking attention with Performers</title>
		<author>
			<persName><forename type="first">Valerii</forename><surname>Krzysztof Marcin Choromanski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Likhosherstov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xingyou</forename><surname>Dohan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreea</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tamás</forename><surname>Gane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Sarlós</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><forename type="middle">Quincy</forename><surname>Hawkins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Afroz</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Mohiuddin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">Benjamin</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucy</forename><forename type="middle">J</forename><surname>Belanger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adrian</forename><surname>Colwell</surname></persName>
		</author>
		<author>
			<persName><surname>Weller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">9th International Conference on Learning Representations (ICLR 2021)</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note>OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Knowledge neurons in pretrained transformers</title>
		<author>
			<persName><forename type="first">Damai</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaru</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhifang</forename><surname>Sui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baobao</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2022.acl-long.581</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 60th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="8493" to="8502" />
		</imprint>
	</monogr>
	<note>: Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Language modeling with gated convolutional networks</title>
		<author>
			<persName><forename type="first">N</forename><surname>Yann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angela</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName><surname>Grangier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="933" to="941" />
		</imprint>
	</monogr>
	<note>Proceedings of Machine Learning Research</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title/>
		<author>
			<persName><surname>Pmlr</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">A mathematical framework for Transformer circuits</title>
		<author>
			<persName><forename type="first">Nelson</forename><surname>Elhage</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neel</forename><surname>Nanda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Catherine</forename><surname>Olsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuntao</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anna</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Conerly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nova</forename><surname>Dassarma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawn</forename><surname>Drain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deep</forename><surname>Ganguli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zac</forename><surname>Hatfield-Dodds</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danny</forename><surname>Hernandez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jackson</forename><surname>Kernion</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liane</forename><surname>Lovitt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kamal</forename><surname>Ndousse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Mccandlish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Olah</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note>Transformer Circuits Thread</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Interpretable explanations of black boxes by meaningful perturbation</title>
		<author>
			<persName><forename type="first">Ruth</forename><forename type="middle">C</forename><surname>Fong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICCV.2017.371</idno>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<meeting><address><addrLine>Venice, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="3449" to="3457" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Transformer feed-forward layers are keyvalue memories</title>
		<author>
			<persName><forename type="first">Mor</forename><surname>Geva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roei</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.emnlp-main.446</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2021 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Punta Cana, Dominican Republic</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="5484" to="5495" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Multimodal neurons in artificial neural networks</title>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Cammarata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chelsea</forename><surname>Voss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shan</forename><surname>Carter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ludwig</forename><surname>Schubert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Olah</surname></persName>
		</author>
		<idno type="DOI">10.23915/distill.00030</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
			<publisher>Distill</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">exBERT: A Visual Analysis Tool to Explore Learned Representations in Transformer Models</title>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Hoover</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hendrik</forename><surname>Strobelt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Gehrmann</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-demos.22</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: System Demonstrations</title>
		<title level="s">Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics: System Demonstrations</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="187" to="196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Attention is not Explanation</title>
		<author>
			<persName><forename type="first">Sarthak</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Byron</forename><forename type="middle">C</forename><surname>Wallace</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1357</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="3543" to="3556" />
		</imprint>
	</monogr>
	<note>Long and Short Papers</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Transformers are RNNs: Fast autoregressive Transformers with linear attention</title>
		<author>
			<persName><forename type="first">Angelos</forename><surname>Katharopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Apoorv</forename><surname>Vyas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikolaos</forename><surname>Pappas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">François</forename><surname>Fleuret</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th International Conference on Machine Learning</title>
		<meeting>the 37th International Conference on Machine Learning</meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">The (un)reliability of saliency methods</title>
		<author>
			<persName><forename type="first">Pieter-Jan</forename><surname>Kindermans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sara</forename><surname>Hooker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julius</forename><surname>Adebayo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maximilian</forename><surname>Alber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kristof</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sven</forename><surname>Schütt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dumitru</forename><surname>Dähne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Been</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><surname>Kim</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-28954-6_14</idno>
	</analytic>
	<monogr>
		<title level="m">Explainable AI: Interpreting, Explaining and Visualizing Deep Learning</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">Wojciech</forename><surname>Samek</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Grégoire</forename><surname>Montavon</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Lars</forename><forename type="middle">Kai</forename><surname>Hansen</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Klaus-Robert</forename><surname>Müller</surname></persName>
		</editor>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">11700</biblScope>
			<biblScope unit="page" from="267" to="280" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Reformer: The efficient Transformer</title>
		<author>
			<persName><forename type="first">Nikita</forename><surname>Kitaev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anselm</forename><surname>Levskaya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">8th International Conference on Learning Representations (ICLR 2020)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note>OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Understanding black-box predictions via influence functions</title>
		<author>
			<persName><forename type="first">Pang</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Koh</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="1885" to="1894" />
		</imprint>
	</monogr>
	<note>Proceedings of Machine Learning Research</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title/>
		<author>
			<persName><surname>Pmlr</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">Narine</forename><surname>Kokhlikyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vivek</forename><surname>Miglani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bilal</forename><surname>Alsallakh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miguel</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Orion</forename><surname>Reblitz-Richardson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.07475</idno>
		<title level="m">Investigating sanity checks for saliency maps with image and text classification</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Recurrent neural network based language model</title>
		<author>
			<persName><forename type="first">Tomáš</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Karafiát</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukáš</forename><surname>Burget</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Černocký</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjeev</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INTER-SPEECH 2010, 11th Annual Conference of the International Speech Communication Association</title>
		<meeting><address><addrLine>Makuhari, Chiba, Japan</addrLine></address></meeting>
		<imprint>
			<publisher>ISCA</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="1045" to="1048" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Universal Dependencies v2: An evergrowing multilingual treebank collection</title>
		<author>
			<persName><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marie-Catherine</forename><surname>De Marneffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Filip</forename><surname>Ginter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Hajič</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sampo</forename><surname>Pyysalo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francis</forename><surname>Tyers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Zeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th Language Resources and Evaluation Conference</title>
		<meeting>the 12th Language Resources and Evaluation Conference<address><addrLine>Marseille, France</addrLine></address></meeting>
		<imprint>
			<publisher>European Language Resources Association</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="4034" to="4043" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">What context features can transformer language models use</title>
		<author>
			<persName><forename type="first">O'</forename><surname>Joe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Connor</surname></persName>
		</author>
		<author>
			<persName><surname>Andreas</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.acl-long.70</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="851" to="864" />
		</imprint>
	</monogr>
	<note>Long Papers</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">In-context learning and induction heads</title>
		<author>
			<persName><forename type="first">Catherine</forename><surname>Olsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nelson</forename><surname>Elhage</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neel</forename><surname>Nanda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nova</forename><surname>Dassarma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuntao</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anna</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Conerly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawn</forename><surname>Drain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deep</forename><surname>Ganguli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zac</forename><surname>Hatfield-Dodds</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danny</forename><surname>Hernandez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Johnston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jackson</forename><surname>Kernion</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liane</forename><surname>Lovitt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kamal</forename><surname>Ndousse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Mccandlish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Olah</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note>Transformer Circuits Thread</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Train short, test long: Attention with linear biases enables input length extrapolation</title>
		<author>
			<persName><forename type="first">Ofir</forename><surname>Press</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Tenth International Conference on Learning Representations, Virtual Event</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note>OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Shortformer: Better language modeling using shorter inputs</title>
		<author>
			<persName><forename type="first">Ofir</forename><surname>Press</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.acl-long.427</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="5493" to="5505" />
		</imprint>
	</monogr>
	<note>Long Papers</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Is attention interpretable</title>
		<author>
			<persName><forename type="first">Sofia</forename><surname>Serrano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1282</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2931" to="2951" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Do long-range language models actually use long-range context</title>
		<author>
			<persName><forename type="first">Simeng</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kalpesh</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Mattarella-Micke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.emnlp-main.62</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2021 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Punta Cana, Dominican Republic</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="807" to="822" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">ChapterBreak: A challenge dataset for long-range language models</title>
		<author>
			<persName><forename type="first">Simeng</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katherine</forename><surname>Thai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2022.naacl-main.271</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Seattle, United States</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="3704" to="3714" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Rationales for sequential predictions</title>
		<author>
			<persName><forename type="first">Keyon</forename><surname>Vafa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuntian</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Rush</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.emnlp-main.807</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2021 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Punta Cana, Dominican Republic</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="10314" to="10332" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A multiscale visualization of attention in the transformer model</title>
		<author>
			<persName><forename type="first">Jesse</forename><surname>Vig</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-3007</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: System Demonstrations</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics: System Demonstrations<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="37" to="42" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">GPT-J-6B: A 6 billion parameter autoregressive language model</title>
		<author>
			<persName><forename type="first">Ben</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aran</forename><surname>Komatsuzaki</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Transformers: State-of-the-art natural language processing</title>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lysandre</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clement</forename><surname>Delangue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anthony</forename><surname>Moi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierric</forename><surname>Cistac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Rault</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Remi</forename><surname>Louf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Morgan</forename><surname>Funtowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joe</forename><surname>Davison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Shleifer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clara</forename><surname>Patrick Von Platen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yacine</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Canwen</forename><surname>Plu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Teven</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sylvain</forename><surname>Le Scao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mariama</forename><surname>Gugger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quentin</forename><surname>Drame</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Lhoest</surname></persName>
		</author>
		<author>
			<persName><surname>Rush</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-demos.6</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="38" to="45" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
