<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Knowledge Enhanced Graph Neural Networks 1 st</title>
				<funder ref="#_2U2Nh6d">
					<orgName type="full">Agence Nationale de la Recherche</orgName>
					<orgName type="abbreviated">ANR</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Luisa</forename><surname>Werner</surname></persName>
							<email>luisa.werner@inria.fr</email>
						</author>
						<author>
							<persName><forename type="first">Nabil</forename><surname>Layaïda</surname></persName>
							<email>nabil.layaida@inria.fr</email>
						</author>
						<author>
							<persName><forename type="first">Pierre</forename><surname>Genevès</surname></persName>
							<email>pierre.geneves@inria.fr</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Université Grenoble Alpes</orgName>
								<orgName type="institution" key="instit2">INRIA Grenoble</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">INRIA Grenoble</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<address>
									<addrLine>rd</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution" key="instit1">CNRS</orgName>
								<orgName type="institution" key="instit2">INRIA Grenoble</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="institution">Sarah Chlyah INRIA Grenoble</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Knowledge Enhanced Graph Neural Networks 1 st</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">4A300DB0F7F44337C5F9B02D7999AD2E</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-04-12T14:49+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>neuro-symbolic integration</term>
					<term>graph neural networks</term>
					<term>relational learning</term>
					<term>knowledge graphs</term>
					<term>fuzzy logic</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Graph data is omnipresent and has a wide variety of applications, such as in natural science, social networks, or the semantic web. However, while being rich in information, graphs are often noisy and incomplete. As a result, graph completion tasks, such as node classification or link prediction, have gained attention. On one hand, neural methods, such as graph neural networks, have proven to be robust tools for learning rich representations of noisy graphs. On the other hand, symbolic methods enable exact reasoning on graphs. We propose Knowledge Enhanced Graph Neural Networks (KeGNN), a neurosymbolic framework for graph completion that combines both paradigms as it allows for the integration of prior knowledge into a graph neural network model. Essentially, KeGNN consists of a graph neural network as a base upon which knowledge enhancement layers are stacked with the goal of refining predictions with respect to prior knowledge. We instantiate KeGNN in conjunction with two well-known graph neural networks, Graph Convolutional Networks and Graph Attention Networks, and evaluate KeGNN on multiple benchmark datasets for node classification.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Graphs are ubiquitous across diverse real-world applications such as e-commerce <ref type="bibr" target="#b0">[1]</ref> , natural science <ref type="bibr" target="#b1">[2]</ref> or social networks <ref type="bibr" target="#b2">[3]</ref>. Graphs connect nodes by edges and allow to enrich them with features. This makes them a versatile and powerful data structure that encodes relational information. As graphs are often derived from noisy data, incompleteness and errors are common issues. Consequently, graph completion tasks such as node classification or link prediction have become increasingly important. These tasks are approached from different directions. In the field of deep learning, research on graph neural networks (GNNs) has gained momentum. Numerous models have been proposed for various graph topologies and applications <ref type="bibr" target="#b3">[4]</ref>- <ref type="bibr" target="#b6">[7]</ref>. The key strength of GNNs is to find meaningful representations of noisy graph data, that can be used to improve prediction tasks <ref type="bibr" target="#b7">[8]</ref>. Despite this advantage, as a subcategory of deep learning methods, GNNs are criticized for their limited interpretability and large data consumption <ref type="bibr" target="#b8">[9]</ref>. Alongside, the research field of symbolic AI addresses the above-mentioned tasks. In symbolic AI, solutions are found by performing logic-like reasoning steps that are exact, interpretable and data-efficient <ref type="bibr" target="#b9">[10]</ref>. For large graphs, however, symbolic methods are often computationally expensive or even infeasible. Since techniques from deep learning and from symbolic AI have complementary pros and cons, the field of neuro-symbolic AI aims to combine both paradigms. Neurosymbolic AI not only paves the way towards the application of AI to learning with limited data, but also allows for jointly using symbolic information (in the form of logical rules) and sub-symbolic information (in the form of real-valued data). This helps to overcome the black-box nature of deep learning methods and to improve interpretability through symbolic representations <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>.</p><p>In this work, we present the neuro-symbolic approach Knowledge enhanced Graph Neural Networks (KeGNN) to conduct node classification given graph data and a set of prior knowledge. In KeGNN, knowledge enhancement layers <ref type="bibr" target="#b12">[13]</ref> are stacked on top of a GNN and adjust its predictions in order to increase the satisfaction of some prior knowledge. In addition to the parameters of the GNN, the knowledge enhancement layers contain learnable clause weights that reflect the impact of the prior knowledge on the predictions. Both components form an end-to-end differentiable model. KeGNN can be seen as a variant of knowledge enhanced neural networks (KENN), which stack knowledge enhancement layers onto a multi-layer perceptron (MLP) and have been proven successful in semantic point cloud segmentation, image segmentation and multi-label classification <ref type="bibr" target="#b13">[14]</ref>- <ref type="bibr" target="#b15">[16]</ref>. However, relational information in sparse graphs can only be introduced through the logical clauses with binary predicates in the knowledge enhancement layer and not at base neural network level. In contrast, KeGNN is based on GNNs that process the graph structure, which makes both the neural and symbolic components sufficiently powerful to exploit the graph structure. In this work, we instantiate KeGNN in conjunction with two well-known GNNs: Graph Attention Networks <ref type="bibr" target="#b16">[17]</ref> and Graph Convolutional Networks <ref type="bibr" target="#b17">[18]</ref>. We apply KeGNN to the benchmark datasets for node classification Cora, Citeseer, PubMed <ref type="bibr" target="#b18">[19]</ref> and Flickr <ref type="bibr" target="#b19">[20]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. METHOD: KEGNN</head><p>KeGNN is a neuro-symbolic approach that can be applied to node classification tasks with the capacity of handling graph structure at the base neural network level. The model takes two types of input: (1) real-valued graph data and (2) prior knowledge expressed in first-order logic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Graph-structured Data</head><p>A Graph G = (N, E) consists of a set of n nodes N and a set of k edges E where each edge of the form (v i , v j ) connects two nodes v i ∈ N and v j ∈ N. The neighborhood N (v i ) describes the set of first-order neighbors of v i . For an attributed and labelled graph, nodes are enriched with features and labels. Each node has a feature vector x ∈ R d of dimension d and a label vector y ∈ R m . The label vector y contains one-hot encoded ground truth labels for m classes. In matrix notation, the features and labels of the entire graph are described as X ∈ R n×d and Y ∈ R n×m . A graph is typed if type functions f E and f N assign edge types and node types to the edges and nodes, respectively. A graph with constant type functions (that assign the same edge and node type to all edges and nodes) is called homogeneous, whereas for heterogeneous graphs, nodes and edges may have different types <ref type="bibr" target="#b4">[5]</ref>.</p><p>Example 2.1: A Citation Graph G Cit consists of documents and citations. Figure <ref type="figure" target="#fig_0">1</ref> shows an extract of the Citeseer citation graph that is used as example to guide through this paper. The documents are represented by a set of nodes N Cit and citations by a set of edges E Cit . Documents can be attributed with features X Cit that describe their content as Word2Vec <ref type="bibr" target="#b20">[21]</ref> vectors. Each node is labelled with one of the six topic categories {AI, DB, HCI, IR, ML, AG}<ref type="foot" target="#foot_0">1</ref> that are encoded in Y Cit . Since all nodes (documents) and edges (citations) have the same type, G Cit is homogeneous.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Prior Knowledge</head><p>Some prior knowledge K is provided to KeGNN. It can be described as a set of logical clauses expressed in the logical language L that is defined as sets of constants C, variables X and predicates P. Predicates have an arity r of one (unary) or two (binary): P = P U ∪ P B . Predicates of arity r &gt; 2 are not considered in this work. Unary predicates express properties, whereas binary predicates express relations. L supports negation (¬) and disjunction (∨). Each clause ϕ ∈ K = {ϕ 1 , . . . , ϕ } can be formulated as a disjunction of (possibly negated) atoms q j=1 o j with q atoms {o 1 , . . . , o q }. Since the prior knowledge is general, all clauses are assumed to be universally quantified. Clauses can be grounded by assigning constants to the free variables. A grounded clause is denoted as ϕ[x 1 , x 2 , ...|c 1 , c 2 , ...] with variables x i ∈ X and constants c i ∈ C. The set of all grounded clauses in a graph is G(K, C).</p><p>Example 2.2: The graph G Cit in Figur 1 can be expressed in L. Nodes are represented by a set of constants C = {a, b, . . . , f }. Node labels are expressed as a set of unary predicates P U = {AI, DB, . . . , AG} and edges as a set of binary predicates P B = {Cite}. L has a set of variables X = {x, y}. The atom AI(x), for example, expresses the membership of x to the class AI and Cite(x, y) expresses the existence of a citation between x and y. Some prior knowledge K can be written as a set of = 6 disjunctive clauses in L.</p><p>Here, the assumption is denoted that two papers that cite each other have the same document class: ϕ AI : ∀xy¬AI(x) ∨ ¬Cite(x, y) ∨ AI(y) ϕ DB : ∀xy¬DB(x) ∨ ¬Cite(x, y) ∨ DB(y) . . . The atoms are grounded by replacing the variables x and y with the constants {a, b, . . . f } to obtain the sets of unary groundings {AI(a), ML(b), . . . , IR(f)} and binary groundings {Cite(a, d), Cite(a, e), . . . , Cite(a, f)}. Assuming a closed world and exclusive classes, other facts could be derived, such as {¬DB(a), ¬IR(a), . . . , ¬Cite(a, b)}. For the sake of simplicity, these are omitted here.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Node Classification</head><p>Node classification is a subtask of knowledge graph completion on a graph G with the objective to assign classes to nodes where they are unknown. This task is accomplished given node features X, edges E and some prior knowledge K encoded as a set of clauses in L. A predictive model is trained on a subset of the graph G train with ground truth labels Y train and validated on a test set G test for which the ground truth labels are compared to the predictions in order to assess the predictive performance. Node classification can be studied in a transductive or inductive setting. In a transductive setting, the entire graph is available for training, but the true labels of the test nodes are masked. In an inductive setting, only the nodes in the training set and the edges connecting them are available, making it more challenging to classify unseen nodes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Fuzzy Semantics</head><p>Let us consider an attributed and labelled graph G and the prior knowledge K. While K can be defined in the logic language L, the neural component in KeGNN relies on continuous and differentiable representations. To interpret Boolean logic in the real-valued domain, KeGNN uses fuzzy logic <ref type="bibr" target="#b21">[22]</ref>, which maps Boolean truth values to the continuous interval [0, 1] ⊂ R. A constant in C is interpreted as a realvalued feature vector x ∈ R d . A predicate P ∈ P with arity r is interpreted as a function f P : R r×d → [0, 1] that takes r feature vectors as input and returns a truth value.</p><p>Example 2.3: In the example, a unary predicate P U ∈ P U = {AI, DB, . . .} is interpreted as a function f P U : R d → [0, 1] that takes a feature vector x and returns a truth value indicating whether the node belongs to the class encoded as P U . The binary predicate Cite ∈ P B is interpreted as the function</p><formula xml:id="formula_0">f Cite (v i , v j ) = 1, if (v i , v j ) ∈ E Cit 0, else.</formula><p>f Cite returns the truth value 1 if there is an edge between two nodes v i and v j in G Cit and 0 otherwise. T-conorm functions ⊥ : [0, 1] × [0, 1] → [0, 1] [23] take real-valued truth values of two literals<ref type="foot" target="#foot_1">2</ref> and define the truth value of their disjunction. The Gödel t-conorm function for two truth values t i , t j is defined as</p><formula xml:id="formula_1">⊥(t i , t j ) → max(t i , t j ).</formula><p>To obtain the truth value of a clause ϕ : o 1 ∨...∨o q , the function ⊥ is extended to a vector t of q truth values: ⊥(t 1 , t 2 , ..., t q ) = ⊥(t 1 , ⊥(t 2 ...⊥(t q-1 , t q ))). Fuzzy negation over truth values is defined as t → 1 -t <ref type="bibr" target="#b21">[22]</ref>. </p><formula xml:id="formula_2">(a) = t 1 , AI(b) = t 2 and Cite(a, b) = t 3 , the truth value of ϕ AI [x, y|a, b] is max{max{(1 -t 1 ), (1 -t 3 )}, t 2 }.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Model Architecture</head><p>The way KeGNN computes the final predictions can be divided in two stages. First, a GNN predicts the node classes given the features and the edges. Subsequently, the knowledge enhancement layers use the predictions as truth values for the grounded unary predicates and update them with respect to the knowledge. An overview of KeGNN is given in Figure <ref type="figure" target="#fig_2">2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1) Neural Component:</head><p>The role of the GNN in the neural component is to exploit feature information in the graph structure. The key strength of a GNN is to enrich node representations with graph structure by nesting k message passing layers <ref type="bibr" target="#b7">[8]</ref>. Per layer, the representations of neighboring nodes are aggregated and combined to obtain updated representations. The node representation v k+1 i in the k-th message passing layer is</p><formula xml:id="formula_3">v k+1 i = combine v k i , aggregate {v k j |v k j ∈ N (v i )} .</formula><p>The layers contain learnable parameters that are optimized with backpropagation. In this work, we consider two wellknown GNNs as components for KeGNN: Graph Convolutional Networks (GCN) <ref type="bibr" target="#b17">[18]</ref> and Graph Attention Networks (GAT) <ref type="bibr" target="#b16">[17]</ref>. While GCN considers the graph structure as given, GAT allows for assessing the importance of the neighbors with attention weights α ij between node v i and node v j . In case of multi-head attention, the attention weights are calculated multiple times and concatenated which allows for capturing different aspects of the input data. In KeGNN, the GNN implements the functions f P U (see Section II-D). In other words, the predictions are used as truth values for the grounded unary predicates in the symbolic component.</p><p>2) Symbolic Component: To refine the predictions of the GNN, one or more knowledge enhancement layers are stacked onto the GNN to update its predictions Y to Y . The goal is to increase the satisfaction of the prior knowledge. The predictions Y of the GNN serve as input to the symbolic component where they are interpreted as fuzzy truth values for the unary grounded predicates U := Y with U ∈ R n×m . Fuzzy truth values for the groundings of binary predicates are encoded as a matrix B where each row represents an edge (v i , v j ) and each column represents an edge type e. In the context of node classification, the GNN returns only predictions for the node classes, while the edges are assumed to be given. A binary grounded predicate is therefore set to truth value 1 (true) if an edge between two nodes v i and v j exists:</p><formula xml:id="formula_4">B [(vi,vj ),e] = 1, if (v i , v j ) of type e ∈ E 0, else.</formula><p>Example 2.5: In case of the beforementioned citation graph of Figure <ref type="figure" target="#fig_0">1</ref>, U and B are defined as:</p><formula xml:id="formula_5">U :=      AI(a) . . . AG(a) AI(b) . . . AG(b) . . . . . . AI(f) . . . AG(f)      B :=          Cite(a, d) Cite(a, e)</formula><p>Cite(a, c) . . .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cite(c, e)</head><p>Cite(e, f)</p><formula xml:id="formula_6">        </formula><p>To enhance the satisfaction of clauses that contain both unary and binary predicates, their groundings are joined into one matrix (v i , v j ). As a result, M contains all required grounded unary predicates for v i and v j .</p><formula xml:id="formula_7">M ∈ R k×p with p = 2 • |P U | + |P B |. M is</formula><p>Example 2.6: For the example citation graph, we obtain M as follows:</p><p>A knowledge enhancement layer consists of multiple clause enhancers. A clause enhancer is instantiated for each clause ϕ ∈ K. Its aim is to compute updates δM ϕ for the groundings in M that increase the satisfaction of ϕ.</p><p>First, fuzzy negation is applied to the columns of M that correspond to negated atoms in ϕ. Then δM ϕ is computed by a t-conorm boost function φ <ref type="bibr" target="#b12">[13]</ref>. This function φ : [0, 1] q → [0, 1] q takes q truth values and returns changes to those truth values such that the satisfaction is increased: ⊥(t) ≤ ⊥(t + φ(t)). <ref type="bibr" target="#b12">[13]</ref> propose the following differentiable t-conorm boost function</p><formula xml:id="formula_8">φ wϕ (t) i = w ϕ • e ti q j=1 e tj .</formula><p>The boost function φ wϕ employs a clause weight w ϕ that is initialized in the beginning of the training and optimized during training as a learnable parameter. The updates for the groundings calculated by φ wϕ are proportional to w ϕ . Therefore, w ϕ determines the magnitude of the update and thus reflects the impact of a clause. The changes to the atoms that do not appear in a clause are set to zero. The boost function is applied row-wise to M as illustrated in the following example. </p><formula xml:id="formula_9">      </formula><p>The values of δM ϕ AI are calculated by φ wAI , for example:</p><formula xml:id="formula_10">δ ¬AI x (a) = φ wAI (z) a = - e -z AI(a) e -z AI(a) + e -z Cit(a,c) + e z AI(c)</formula><p>Each clause enhancer computes updates δM ϕ to increase the satisfaction of a clause independently. The updates of all clause enhancers are finally added, resulting in a matrix δM = ϕ∈K δM ϕ . To apply the updates to the initial predictions, δM has to be added to Y. The updates in δM can not directly be applied to the predictions Y of the GNN. Since the unary groundings U were joined with the binary groundings B, multiple changes may be proposed for the same grounded unary atom. For example, for the grounded atom AI(c) the changes δ ¬AI y (c) and δ ¬AI x (c) are proposed, since c appears in the grounded clauses ϕ AI [x, y|a, c] and ϕ AI [x, y|c, e]. In G Cit the node c appears in first place of edge (a, c) and in second place of edge (c, e). Therefore, all updates for the same grounded atom are summed, which reduces the size of M to the size of U.</p><p>To ensure that the updated predictions remain truth values in the range of [0, 1], the knowledge enhancement layer updates at first the preactivations Z of the GNN and then applies the activation function σ to the updated preactivations Z in order to obtain the final predictions: Y = σ(Z ). Therefore, a knowledge enhancement layer transforms Z to Z (with Z, Z ∈ R n×m ). In the last step, the updates by the knowledge enhancer are added to the preactivations Z of the GNN and passed to σ to obtain the updated predictions</p><formula xml:id="formula_11">Y = σ Z + ϕ∈K δU ϕ</formula><p>where δU ϕ is the matrix obtained by extracting the changes to the unary predicates from δM ϕ . Regarding the binary groundings, the values in B are set to a high positive value that results in one when σ is applied.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. RELATED WORK</head><p>The field of knowledge graph completion is addressed from several research directions. Symbolic methods exist that conduct link prediction given a set of prior knowledge <ref type="bibr" target="#b23">[24]</ref> [25]. Embedding-based methods <ref type="bibr" target="#b25">[26]</ref> are mostly sub-symbolic methods to obtain node embeddings that are used for knowledge graph completion tasks. Usually, their common objective is to find similar embeddings for nodes that are located closely in the graph. The majority of these methods only encodes the graph structure, but does not consider node-specific feature information <ref type="bibr" target="#b26">[27]</ref>. However, KeGNN is based on GNNs that are suited for learning representations of graphs attributed with node features. It stacks additional layers that interpret the outputs of the GNN in fuzzy logic and modify them to increase the satisfiability. Therefore, it is considered a neuro-symbolic method. In the multifaceted neuro-symbolic field, KeGNN can be placed in the category of knowledge-guided learning <ref type="bibr" target="#b12">[13]</ref>, where the focus lies on learning in the presence of additional supervision introduced as prior knowledge. Within this category, KeGNN belongs to the model-based approaches, where prior knowledge in the form of knowledge enhancement layers is an integral part of the model <ref type="bibr" target="#b13">[14]</ref>. Beyond, lossbased methods such as logic tensor networks <ref type="bibr" target="#b27">[28]</ref> exist that encode the satisfiability of prior knowledge as an optimization objective.</p><p>Further, in <ref type="bibr" target="#b28">[29]</ref> neuro-symbolic approaches dealing with graph structures are classified into three categories. First, logically informed embedding approaches <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b30">[31]</ref> use predefined logical rules that provide knowledge to a neural system, while both components are mostly distinct. Second, approaches for knowledge graph embedding with logical constraints <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b32">[33]</ref> use prior knowledge as constraints on the neural knowledge graph embedding method in order to modify predictions or embeddings. Thirdly, neuro-symbolic methods are used for learning rules for graph reasoning tasks <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b34">[35]</ref>. This allows for rule generation or confidence scores for prior knowledge and makes the models robust to exceptions or soft knowledge. KeGNN best falls into the second category, since the prior knowledge is interpreted in fuzzy logic to be integrated with the neural model and update the GNN's predictions. The idea of confidence values in category three shares the common property of weighting knowledge as with KeGNN's clause weights. However, even though KeGNN's clause weights introduce a notion of impact of a clause when predictions are made, they cannot directly be interpreted as the confidence in a rule.</p><p>In the well-known Kautz Taxonomy <ref type="bibr" target="#b35">[36]</ref> that classifies neuro-symbolic approaches according to the integration of neural and symbolic modules, KeGNN falls best into the category Neuro[Symbolic] (Type 6) of fully-integrated neuro-symbolic systems that embed symbolic reasoning in a neural architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENS</head><p>To evaluate the performance of KeGNN, we apply it to the datasets Citeseer, Cora, PubMed and Flickr that are common benchmarks for node classification in a transductive setting. In the following, KeGNN is called KeGCN and KeGAT when instantiated to a GCN or a GAT, respectively. As additional baseline, we consider KeMLP, that stacks knowledge enhancement layers onto an MLP, as proposed in <ref type="bibr" target="#b13">[14]</ref>. Further, the standalone neural models MLP, GCN and GAT are used as baselines. While Citeseer, Cora and PubMed are citation graphs that encode citations between scientific papers (as in Example 2.2), Flickr contains images and shared properties between them. All datasets can be modelled as homogeneous, labelled and attributed graphs as defined in Section II-A. Table <ref type="table">I</ref> gives an overview of the named datasets in this work. The datasets are publicly available on the dataset collection<ref type="foot" target="#foot_2">3</ref> of PyTorch Geometric <ref type="bibr" target="#b36">[37]</ref>. For the split into train, valid and test set, we take the predefined splits in <ref type="bibr" target="#b37">[38]</ref> for the citation graphs and in <ref type="bibr" target="#b19">[20]</ref> for Flickr. Word2Vec vectors <ref type="bibr" target="#b20">[21]</ref> are used as node features for the citation graphs and image data for Flickr. Figure <ref type="figure" target="#fig_0">1</ref> visualizes the graph structure of the underlying datasets in this work as a homogeneous, attributed and labelled graph on the example of Citeseer.</p><p>The set of prior logic for the knowledge enhancement layers is manually defined. In this work, we encode the assumption that the existence of an edge for a node pair points to their membership to the same class and hence provides added value to the node classification task. In the context of citation graphs, this implies that two documents that cite each other refer to the same topic, while for Flickr, linked images share the same properties. Following this pattern for all datasets, a clause ϕ: ∀xy : ¬Cls i (x)∨ ¬Link(x, y) ∨ Cls i (y) is instantiated for each node class Cls i , i ∈ {1, . . . , m}. More details on the experiments are given in Section IV-B. The source code of the experiments are publicly available<ref type="foot" target="#foot_3">4</ref> . .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Results</head><p>To compare the performance of all models, we examine the average test accuracy over 50 runs (10 for Flickr) for the knowledge enhanced models KeMLP, KeGCN, KeGAT and the standalone base models MLP, GCN, GAT on the named datasets. The results are given in Table <ref type="table">II</ref>. For Cora and Citeseer, KeMLP leads to a significant improvement over MLP (p-value of one-sided t-test 0.05). In contrast, no significant advantage of KeGCN or KeGAT in comparison to the standalone base model is observed. Nevertheless, all GNN-based models are significantly superior to KeMLP for Cora. This includes not only KeGCN and KeGAT, but also the GNN baselines. For Citeseer, KeGAT and GAT both outperform KeMLP. In the case of PubMed, only a significant improvement of KeMLP over MLP can be observed, while the    GNN-based models and their enhanced versions do not provide any positive effect. For Flickr, no significant improvement between the base model and the respective knowledge enhanced model can be observed. Nevertheless, all GNN-based models outperform KeMLP, reporting significantly higher mean test accuracies for KeGAT, GAT, GCN and KeGCN. 1) Exploitation of the Graph Structure: It turns out that the performance gap between MLP and KeMLP is larger than for KeGNN in comparison to the standalone GNN. To explain this observation, we examine how the graph structure affects the prediction performance. Therefore, in Figure <ref type="figure" target="#fig_4">3</ref> we analyze the accuracy grouped by the node degree for the entire graph for MLP vs. KeMLP and GCN vs. KeGCN. The findings for KeGAT are in line with those for KeGCN. It is observed that KeMLP performs better compared to MLP as the node degree increases. By contrast, when comparing GCN and KeGCN, for both models, the accuracy is observed superior for nodes with a higher degree.</p><p>This shows that rich graph structure is helpful for the node classification in general. Indeed, the MLP is a simple model that misses information on the graph structure and thus benefits from graph structure contributed by KeMLP in the form of binary predicates. On the contrary, standalone GNNs can process graph structure by using message passing techniques to transmit learned node representations between neighbors. The prior knowledge introduced in the knowledge enhancer is simple. It encodes that two neighbors are likely to be of the same class. An explanation for the small difference in performance is that GNNs may be able to capture and propagate this simple knowledge across neighbors implicitly, using its message passing technique. In other words we observe that, in this particular case, the introduced knowledge happens to be redundant for GNNs. However, the introduced knowledge significantly improves the accuracy of MLPs. In this context, we discuss perspectives for future work in Section V.</p><p>2) Robustness to wrong knowledge: Furthermore, a question of interest is how the knowledge enhanced model find a balance between knowledge and graph data in case of knowledge that is not consistent with the graph data. In other words, can the KeGNN successfully deal with nodes having mainly neighbors that belong to a different ground truth class and thus contribute misleading information to the node classification?</p><p>To analyze this question, we categorize the accuracy by the proportion of misleading nodes in the neighborhood, see Figure <ref type="figure" target="#fig_5">4</ref>. Misleading nodes are node neighbors that have a different ground truth class than the node to be classified. It turns out that KeMLP is particularly helpful over MLP when the neighborhood provides the right information. However, if the neighborhood is misleading (if most or even all of the  neighbors belong to a different class), an MLP that ignores the graph structure can lead to even better results. When comparing KeGCN and GCN, there is no clear difference. This is expected, since both models are equally affected by misleading nodes as they utilize the graph structure. Just as a GCN, the KeGCN is not necessarily robust to wrong prior knowledge since the GCN component uses the entire neighborhood, including the misleading nodes. When comparing GCN to KeMLP, see plot below in Figure <ref type="figure" target="#fig_5">4</ref>, KeMLP is more robust to misleading neighbors. While GCN takes the graph structure as given and includes all neighbors equally in the embeddings by graph convolution, the clause weights in the knowledge enhancement layers provide a way to devalue knowledge. If the data frequently contradicts a clause, the model has the capacity to reduce the respective clause weight in the learning process and reduce its impact.</p><p>3) Clause Weight Learning: Further, we want to examine whether the clause weights learned during training are aligned with the knowledge in the ground truth data. The clause weights provide insights on the magnitude of the updates made by a clause. The clause compliance <ref type="bibr" target="#b12">[13]</ref> measures how well </p><formula xml:id="formula_12">V i = {v i |v i ∈ V ∧ Cls(v i ) == i},</formula><p>and the neighborhood N (v i ) of v i , the clause compliance of clause ϕ on graph G is defined as follows:</p><formula xml:id="formula_13">Compliance(G, ϕ) = vi∈V i vj ∈N (v) 1[ if v j ∈ V i ] vi∈V i |N (v i )|</formula><p>(1) In other words, the clause compliance counts how often among nodes of a class Cls i the neighboring nodes have the same class <ref type="bibr" target="#b12">[13]</ref>. The clause compliance can be calculated on the ground truth classes of the training set or the predicted classes. As a reference, we measure the clause compliance based on the ground truth labels in the training set. Figure <ref type="figure" target="#fig_6">5</ref> displays the learned clause weights for KeGCN and KeMLP versus the clause compliance on the ground truth labels of the training set. For KeMLP, a positive correlation between the learned clause weights and the clause compliance on the training set is observed. This indicates that higher clause weights are learned for clauses that are satisfied in the training set. Consequently, these clauses have a higher impact on the updates of the predictions. In addition, the clause weights corresponding to clauses with low compliance values make smaller updates to the initial predictions. Accordingly, clauses that are rarely satisfied learn lower clause weights during the training process. In the case of KeGCN, the clause weights are predominantly set to values close to zero. This is in accordance with the absence of a significant performance gap between GCN and KeGCN. Since the GCN itself already leads to valid classifications, smaller updates are required by the clause enhancers.</p><p>Furthermore, we analyze how the compliance evolves during training to investigate whether the models learn predictions that increase the satisfaction of the prior knowledge. Figure <ref type="figure">6</ref> plots the evolution of the clause compliance for the six clauses for GCN vs. KeGCN and MLP vs. KeMLP. It is observed that GCN and KeGCN yield similar results as the evolution of the compliance during training for both models is mostly aligned. For MLP vs. KeMLP the clause compliance of the prediction of the MLP converges to lower values for all classes than the clause compliance obtained with the KeMLP. This gives evidence that the knowledge enhancement layer actually improves the satisfiability of the prior knowledge. As already observed, this gives evidence that the standalone GCN is able to implicitly satisfy the prior knowledge even though it is not explicitly defined.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Additional Experiment Details</head><p>1) Implementation: The code 4 is based on PyTorch <ref type="bibr" target="#b38">[39]</ref> and the graph learning library PyTorch Geometric <ref type="bibr" target="#b36">[37]</ref>. The Weights &amp; Biases tracking tool <ref type="bibr" target="#b39">[40]</ref> is used to monitor the experiments. All experiments are conducted on a machine running an Ubuntu 20.4 equipped with an Intel(R) Xeon(R) Silver 4114 CPU 2.20GHz processor, 192G of RAM and one GPU Nvidia Quadro P5000.</p><p>2) Model Parameters and Hyperparameter Tuning: KeGNN contains a set of hyperparameters. Batch normalization <ref type="bibr" target="#b40">[41]</ref> is applied after each hidden layer of the GNN. The Adam optimizer <ref type="bibr" target="#b41">[42]</ref> is used as optimizer for all models. Concerning the hyperparameters specific to the knowledge enhancement layers, the initialization of the preactivations of the binary predicates (which are assumed to be known) is taken as a hyperparameter. They are set to a high positive value for edges that are known to exist and correspond to the grounding of the binary predicate. Furthermore, different initializations of clause weights and constraints on them are tested. Moreover, the number of stacked knowledge enhancement layers is a hyperparameter. We further allow the model to randomly neglect a proportion of edges by setting an edges drop rate parameter. Further, we test whether the normalization of the edges with the diagonal matrix D = j Ãi,j (with Ã = A+I) is helpful.</p><p>To find a suitable set hyperparameters for each dataset and model, we perform a random search with up to 800 runs and 48h time limit and choose the parameter combination which leads to the highest accuracy on the validation set. The hyperparameter tuning is executed in Weights and Biases <ref type="bibr" target="#b39">[40]</ref>.</p><p>The following hyperparameter values are tested:</p><p>• Adam optimizer parameters: β 1 : 0.9, β 2 : 0.99, : 1e-07  <ref type="table">III</ref>. We set the random seed for all experiments to 1234.</p><p>The reference models MLP, GCN and GAT are trained with the same parameter set as the respective knowledge enhanced models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. LIMITATIONS AND PERSPECTIVES</head><p>The method of KeGNN is limited in some aspects, which we present in this section. In this work, we focus on homogeneous graphs. In reality, however, graphs are often heterogeneous with multiple node and edge types <ref type="bibr" target="#b3">[4]</ref>. Adaptations are necessary on both the neural and the symbolic side to apply KeGNN to heterogeneous graphs. The restriction to homogeneous graphs also limits the scope of formulating complex prior knowledge. Eventually, the datasets used in this work and the set of prior knowledge are too simple for KeGNN to exploit its potential and lead to a significant improvement over the GNN. The experimental results show that the knowledge encoded in the symbolic component leads to significant improvement over an MLP that is not capable to capture and learn that knowledge. This indicates that for more complex knowledge that is harder for a GNN to learn, KeGNN has the potential to bring higher improvements. A perspective for further work is the extension of KeGNN to more generic data structures such as incomplete and heterogeneous knowledge graphs in conjunction with more complex prior knowledge.</p><p>Another limitation of KeGNN is scalability. With an increasing number of stacked knowledge enhancement layers, the affected node neighborhood grows exponentially, which can lead to significant memory overhead. This problem is referred as neighborhood explosion <ref type="bibr" target="#b6">[7]</ref> and is particularly problematic in the context of training on memory-constrained GPUs. This affects both the GNN and the knowledge enhancement layers that encode binary knowledge. Methods from scalable graph learning <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b42">[43]</ref>, <ref type="bibr" target="#b43">[44]</ref> represent potential solutions for the neighborhood explosion problem in KeGNN.</p><p>Furthermore, limitations appear in the context of link prediction with KeGNN. For link prediction, a neural component is required that predicts fuzzy truth values for binary predicates. At present, KeGNN can handle clauses containing binary predicates, but their truth values are initialized with artificial predictions, where a high value encodes the presence of an edge. This limits the application of KeGNN to datasets for which the graph structure is complete and known a priori.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSION</head><p>In this work, we introduced KeGNN, a neuro-symbolic model that integrates GNNs with symbolic knowledge enhancement layers to create an end-to-end differentiable model. This allows the use of prior knowledge to improve node classification while exploiting the strength of a GNN to learn expressive representations. Experimental studies show that the inclusion of prior knowledge has the potential to improve simple neural models (as observed in the case of MLP). However, the knowledge enhancement of GNNs is harder to achieve on the underlying and limited benchmarks for  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. ( 1 )</head><label>1</label><figDesc>Fig. (1) Example extract of the Citeseer citation graph.</figDesc><graphic coords="3,135.92,50.54,340.16,178.52" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Example 2 . 4 :</head><label>24</label><figDesc>Given the clause ϕ AI : ∀xy ¬AI(x) ∨ ¬Cite(x, y) ∨ AI(y) and its grounding ϕ AI [x, y|a, b] : AI(a) ∨ ¬Cite(a, b) ∨ AI(b) to the constants a and b and truth values for the grounded predicates AI</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. ( 2 )</head><label>2</label><figDesc>Fig. (2) Overview of KeGNN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Example 2 . 7 :δ</head><label>27</label><figDesc>Given the clause ϕ AI : ∀xy¬AI(x) ∨ ¬Cite(x, y) ∨ AI(y) and the clause weight w AI , the changes for this clause are δM ϕ AI = w AI • ¬AI x (a) 0 . . . δ AI y (c) 0 . . . δ ¬Cit(a,c) δ ¬AI x (a) 0 . . . δ AI y (e) 0 . . . δ ¬Cit(e,a) δ ¬AI x (a) 0 . . . δ AI y (d) 0 . . . δ ¬Cit(c,d) . . . . . . . . . δ ¬AI x (e) 0 . . . δ AI y (f) 0 . . . δ ¬Cit(e,f)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. ( 3 )</head><label>3</label><figDesc>Fig. (3) The accuracy grouped by the node degree for MLP vs. KeMLP (above) and GCN vs. KeGCN (middle) and GAT vs. KeGAT(below) on Citeseer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. ( 4 )</head><label>4</label><figDesc>Fig.<ref type="bibr" target="#b3">(4)</ref> The accuracy grouped by the ratio of misleading first-order neighbors for GCN vs. KeGCN (left), MLP vs. KeMLP (right), GCN vs. KeMLP (below) on Citeseer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. ( 5 )</head><label>5</label><figDesc>Fig. (5) Learned clause weights vs. clause compliance for KeMLP (left) and KeGCN (right) on Citeseer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>•</head><label></label><figDesc>Attention heads: {1, 2, 3, 4, 6, 8, 10} • Batch size: {128, 512, 1024, 2048, full batch} • Binary preactivation: {0.5, 1.0, 10.0, 100.0, 500.0} • Clause weights initialization: {0.001, 0.1, 0.25, 0.5, random uniform distribution on [0,1)} • Dropout rate: 0.5 • Edges drop rate: random uniform distribution [0.0, 0.9] • Edge normalization: {true, false} • Early stopping: δ min : 0.001, patience: {1, 10, 100} • Hidden layer dimension: {32, 64, 128, 256} • Learning rate: random uniform distribution [0.0001, 0.1] • Clause weight clipping: w min : 0.0, w max : random uniform distribution: [0.8, 500.0] • Number of knowledge enhancement layers: {1, 2, 3, 4, 5, 6} • Number of hidden layers: {2, 3, 4, 5, 6} • Number of epochs 200 (unless training stopped early) The obtained parameter combinations for the models KeMLP, KeGCN and KeGAT for Cora, Citeseer, PubMed and Flickr are displayed in Table</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE ( I</head><label>(</label><figDesc>) Overview of the datasets Citeseer, Cora, PubMed and Flickr</figDesc><table><row><cell></cell><cell>MLP</cell><cell>KeMLP</cell><cell>GCN</cell><cell>KeGCN</cell><cell>GAT</cell><cell>KeGAT</cell></row><row><cell>Cora</cell><cell>0.7098 (0.0080)</cell><cell>0.8072 (0.0193)</cell><cell>0.8538 (0.0057)</cell><cell>0.8587 (0.0057)</cell><cell>0.8517 (0.0068)</cell><cell>0.8498 (0.0066)</cell></row><row><cell>CiteSeer</cell><cell>0.7278 (0.0081)</cell><cell>0.7529 (0.0067)</cell><cell>0.748 (0.0102)</cell><cell>0.7506 (0.0096)</cell><cell>0.7718 (0.0072)</cell><cell>0.7734 (0.0073)</cell></row><row><cell>PubMed</cell><cell>0.8844 (0.0057)</cell><cell>0.8931 (0.0048)</cell><cell>0.8855 (0.0062)</cell><cell>0.8840 (0.0087)</cell><cell>0.8769 (0.0040)</cell><cell>0.8686 (0.0081)</cell></row><row><cell>Flickr</cell><cell>0.4656 (0.0018)</cell><cell>0.4659 (0.0012)</cell><cell>0.5007 (0.0063)</cell><cell>0.4974 (0.0180)</cell><cell>0.4970 (0.0124)</cell><cell>0.4920 (0.0189)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE (</head><label>(</label><figDesc>II) Average test accuracy of 50 runs (10 for Flickr). The standard deviations are reported in brackets.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE (</head><label>(</label><figDesc>III) Hyperparameters and experiment configuration for PubMed and Flickr which the injection of simple knowledge concerning local neighborhood is redundant with the representations that GNNs are able to learn. Nevertheless, KeGNN has not only the potential to improve graph completion tasks from a performance perspective, but also to increase interpretability through clause weights. This work is a step towards a holistic neurosymbolic method on incomplete and noisy semantic data, such as knowledge graphs.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>The classes are abbreviations for the categories Artificial Intelligence, Databases, Human-Computer Interaction, Information Retrieval, Machine Learning and Agents.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>A literal is a (possibly negated) grounded atom, e.g. AI(a)</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>https://pytorch-geometric.readthedocs.io/en/latest/modules/datasets.html</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"><p>https://gitlab.inria.fr/tyrex/kegnn</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>ACKNOWLEDGMENTS</head><p>This work has been partially supported by the <rs type="funder">MIAI Knowledge communication and evolution chair</rs> (<rs type="grantNumber">ANR-19-P3IA-0003</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_2U2Nh6d">
					<idno type="grant-number">ANR-19-P3IA-0003</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Item relationship graph neural networks for e-commerce</title>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Caverlee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-A</forename><surname>Heng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">03</biblScope>
			<biblScope unit="page" from="1" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Graph networks as learnable physics engines for inference and control</title>
		<author>
			<persName><forename type="first">A</forename><surname>Sanchez-Gonzalez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Merel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Riedmiller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Battaglia</surname></persName>
		</author>
		<ptr target="https://proceedings.mlr.press/v80/sanchez-gonzalez18a.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International Conference on Machine Learning, ser. Proceedings of Machine Learning Research</title>
		<editor>
			<persName><forename type="first">J</forename><surname>Dy</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Krause</surname></persName>
		</editor>
		<meeting>the 35th International Conference on Machine Learning, ser. Machine Learning Research</meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2018-07-15">10-15 Jul 2018</date>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="4470" to="4479" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Graph convolutional networks with markov random field reasoning for social spammer detection</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Chen</surname></persName>
		</author>
		<ptr target="https://ojs.aaai.org/index.php/AAAI/article/view/5455" />
		<imprint>
			<date type="published" when="2020-04">Apr. 2020</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="1054" to="1061" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Simple and efficient heterogeneous graph neural network</title>
		<author>
			<persName><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Fan</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2207.02547" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<title level="m">Deep Learning on Graphs</title>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">A comprehensive survey on graph neural networks</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
		<ptr target="https://doi.org/10.1109%2Ftnnls.2020.2978386" />
		<imprint>
			<date type="published" when="2021-01">jan 2021</date>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="4" to="24" />
		</imprint>
		<respStmt>
			<orgName>Institute of Electrical and Electronics Engineers</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A comprehensive study on large-scale graph training: Benchmarking and rethinking</title>
		<author>
			<persName><forename type="first">K</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><forename type="first">L</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
		<title level="m">Graph Neural Networks: Foundations, Frontiers, and Applications</title>
		<meeting><address><addrLine>Singapore; Singapore</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Neuro-symbolic AI: an emerging class of AI workloads and their characterization</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Susskind</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Arden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">K</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Stockton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">B</forename><surname>John</surname></persName>
		</author>
		<idno>abs/2109.06133</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">From statistical relational to neural-symbolic artificial intelligence</title>
		<author>
			<persName><forename type="first">L</forename><surname>De Raedt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Dumančić</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Manhaeve</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Marra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence, ser. IJCAI&apos;20</title>
		<meeting>the Twenty-Ninth International Joint Conference on Artificial Intelligence, ser. IJCAI&apos;20</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Reconciling deep learning with symbolic artificial intelligence: representing objects and relations</title>
		<author>
			<persName><forename type="first">M</forename><surname>Garnelo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shanahan</surname></persName>
		</author>
		<ptr target="https://www.sciencedirect.com/science/article/pii/S2352154618301943" />
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="17" to="23" />
		</imprint>
	</monogr>
	<note>artificial Intelligence</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Relational neural machines</title>
		<author>
			<persName><forename type="first">G</forename><surname>Marra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Diligenti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Giannini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Maggini</surname></persName>
		</author>
		<idno type="DOI">10.3233/FAIA200237</idno>
		<ptr target="https://doi.org/10.3233/FAIA200237" />
	</analytic>
	<monogr>
		<title level="m">Conference on Prestigious Applications of Artificial Intelligence (PAIS 2020), ser. Frontiers in Artificial Intelligence and Applications</title>
		<editor>
			<persName><forename type="first">G</forename><forename type="middle">D</forename><surname>Giacomo</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Catalá</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><surname>Dilkina</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Milano</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Barro</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Bugarín</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Lang</surname></persName>
		</editor>
		<meeting><address><addrLine>Santiago de Compostela, Spain</addrLine></address></meeting>
		<imprint>
			<publisher>IOS Press</publisher>
			<date type="published" when="2020-09-08">29 August-8 September 2020. August 29 -September 8, 2020 -Including 10th. 2020</date>
			<biblScope unit="volume">325</biblScope>
			<biblScope unit="page" from="1340" to="1347" />
		</imprint>
	</monogr>
	<note>ECAI 2020 -24th European Conference on Artificial Intelligence</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Neural networks enhancement with logical knowledge</title>
		<author>
			<persName><forename type="first">A</forename><surname>Daniele</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Serafini</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2009.06087" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note>unpublished</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Knowledge enhanced neural networks for relational domains</title>
	</analytic>
	<monogr>
		<title level="m">AIxIA 2022 -Advances in Artificial Intelligence</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Dovier</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Montanari</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Orlandini</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="91" to="109" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Knowledge enhanced neural networks for point cloud semantic segmentation</title>
		<author>
			<persName><forename type="first">E</forename><surname>Grilli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Daniele</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bassier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Remondino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Serafini</surname></persName>
		</author>
		<ptr target="https://www.mdpi.com/2072-4292/15/10/2590" />
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Knowledge enhanced neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Daniele</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Serafini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">PRICAI 2019: Trends in Artificial Intelligence</title>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Nayak</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Sharma</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="542" to="554" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Graph attention networks</title>
		<author>
			<persName><forename type="first">P</forename><surname>Veličković</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Liò</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Semi-Supervised Classification with Graph Convolutional Networks</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=SJU4ayYgl" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th International Conference on Learning Representations, ser. ICLR &apos;17</title>
		<meeting>the 5th International Conference on Learning Representations, ser. ICLR &apos;17</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Revisiting semisupervised learning with graph embeddings</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33rd International Conference on International Conference on Machine Learning</title>
		<meeting>the 33rd International Conference on International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="40" to="48" />
		</imprint>
	</monogr>
	<note>ICML&apos;16. JMLR.org</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Graphsaint: Graph sampling based inductive learning method</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Prasanna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Word2vec applied to recommendation: Hyperparameters matter</title>
		<author>
			<persName><forename type="first">H</forename><surname>Caselles-Dupré</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Lesaint</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Royo-Letelier</surname></persName>
		</author>
		<idno type="DOI">10.1145/3240323.3240377</idno>
		<ptr target="https://doi.org/10.1145/3240323.3240377" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th ACM Conference on Recommender Systems, ser. RecSys &apos;18</title>
		<meeting>the 12th ACM Conference on Recommender Systems, ser. RecSys &apos;18<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="352" to="356" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Fuzzy logic</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zadeh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="83" to="93" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<author>
			<persName><forename type="first">E</forename><surname>Klement</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mesiar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Pap</surname></persName>
		</author>
		<ptr target="https://books.google.fr/books?id=HXzvCAAAQBAJ" />
		<title level="m">Triangular Norms, ser. Trends in Logic</title>
		<meeting><address><addrLine>Netherlands</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Semantic data mining: A survey of ontology-based approaches</title>
		<author>
			<persName><forename type="first">D</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 IEEE 9th International Conference on Semantic Computing (IEEE ICSC 2015</title>
		<meeting>the 2015 IEEE 9th International Conference on Semantic Computing (IEEE ICSC 2015</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="244" to="251" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Anytime bottom-up rule learning for knowledge graph completion</title>
		<author>
			<persName><forename type="first">C</forename><surname>Meilicke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">W</forename><surname>Chekol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ruffinelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Stuckenschmidt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Joint Conference on Artificial Intelligence, ser. IJCAI&apos;19</title>
		<meeting>the 28th International Joint Conference on Artificial Intelligence, ser. IJCAI&apos;19</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3137" to="3143" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A survey on knowledge graph embedding: Approaches, applications and benchmarks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">N</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Guo</surname></persName>
		</author>
		<ptr target="https://www.mdpi.com/2079-9292/9/5/750" />
	</analytic>
	<monogr>
		<title level="j">Electronics</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Node classification meets link prediction on knowledge graphs</title>
		<author>
			<persName><forename type="first">R</forename><surname>Abboud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">I</forename><surname>Ceylan</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2106.07297" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Logic tensor networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Badreddine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Garcez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Serafini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Spranger</surname></persName>
		</author>
		<ptr target="https://doi.org/10.1016%2Fj.artint.2021.103649" />
		<imprint>
			<date type="published" when="2022-02">feb 2022</date>
			<publisher>Elsevier BV</publisher>
			<biblScope unit="volume">303</biblScope>
			<biblScope unit="page">103649</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Neurosymbolic ai for reasoning on graph structures: A survey</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">N</forename><surname>Delong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">F</forename><surname>Mir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Whyte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Fleuriot</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2302.07200" />
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Knowledge graph completion by jointly learning structural features and soft logical rules</title>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="2724" to="2735" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Improving knowledge graph embeddings with ontological reasoning</title>
		<author>
			<persName><forename type="first">N</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-K</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Gad-Elrab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Stepanova</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-88361-4_24</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-88361-424" />
	</analytic>
	<monogr>
		<title level="m">The Semantic Web -ISWC 2021: 20th International Semantic Web Conference, ISWC 2021, Virtual Event</title>
		<meeting><address><addrLine>Berlin, Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2021">October 24-28, 2021. 2021</date>
			<biblScope unit="page" from="410" to="426" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Improved knowledge graph embedding using background taxonomic information</title>
		<author>
			<persName><forename type="first">B</forename><surname>Fatemi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ravanbakhsh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Poole</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="3526" to="3533" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Jointly embedding knowledge graphs and logical rules</title>
		<author>
			<persName><forename type="first">S</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Guo</surname></persName>
		</author>
		<ptr target="https://aclanthology.org/D16-1019" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Austin, Texas</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016-11">Nov. 2016</date>
			<biblScope unit="page" from="192" to="202" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Featgraph: A flexible and efficient backend for graph neural network systems</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SC20: International Conference for High Performance Computing, Networking, Storage and Analysis</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">{RNNL}ogic: Learning logic rules for reasoning on knowledge graphs</title>
		<author>
			<persName><forename type="first">M</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-P</forename><surname>Xhonneux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">A</forename><surname>Kautz</surname></persName>
		</author>
		<idno type="DOI">10.1002/aaai.12036</idno>
		<ptr target="https://onlinelibrary.wiley.com/doi/10.1002/aaai.12036" />
		<title level="m">The third ai summer: Aaai robert s. engelmore memorial lecture</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Fast graph representation learning with PyTorch Geometric</title>
		<author>
			<persName><forename type="first">M</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Lenssen</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1903.02428" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Fastgcn: Fast learning with graph convolutional networks via importance sampling</title>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Xiao</surname></persName>
		</author>
		<ptr target="http://dblp.uni-trier.de/db/conf/iclr/iclr2018.html#ChenMX18" />
	</analytic>
	<monogr>
		<title level="m">ICLR (Poster). OpenReview.net</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Köpf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Raison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chilamkurthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1912.01703" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Experiment tracking with weights and biases</title>
		<author>
			<persName><forename type="first">L</forename><surname>Biewald</surname></persName>
		</author>
		<ptr target="https://www.wandb.com/" />
	</analytic>
	<monogr>
		<title level="m">software available from wandb</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<ptr target="https://proceedings.mlr.press/v37/ioffe15.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Machine Learning, ser. Proceedings of Machine Learning Research</title>
		<editor>
			<persName><forename type="first">F</forename><surname>Bach</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Blei</surname></persName>
		</editor>
		<meeting>the 32nd International Conference on Machine Learning, ser. Machine Learning Research<address><addrLine>Lille, France</addrLine></address></meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2015-07-09">07-09 Jul 2015</date>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1412.6980" />
	</analytic>
	<monogr>
		<title level="m">3rd International Conference on Learning Representations, ICLR 2015</title>
		<editor>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</editor>
		<meeting><address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015">May 7-9, 2015. 2015</date>
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Gnnautoscale: Scalable and expressive graph neural networks via historical embeddings</title>
		<author>
			<persName><forename type="first">M</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Lenssen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Weichert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<ptr target="https://proceedings.mlr.press/v139/fey21a.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th International Conference on Machine Learning, ser. Proceedings of Machine Learning Research</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Meila</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</editor>
		<meeting>the 38th International Conference on Machine Learning, ser. Machine Learning Research</meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021-07">Jul 2021</date>
			<biblScope unit="volume">139</biblScope>
			<biblScope unit="page" from="3294" to="3304" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on Neural Information Processing Systems, ser. NIPS&apos;17</title>
		<meeting>the 31st International Conference on Neural Information Processing Systems, ser. NIPS&apos;17</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Red</forename><surname>Hook</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<publisher>Curran Associates Inc</publisher>
			<biblScope unit="page" from="1025" to="1035" />
			<pubPlace>NY, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
