<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Open Information Extraction with Entity Focused Constraints</title>
				<funder ref="#_yhHZcJk">
					<orgName type="full">Agence Nationale de la Recherche</orgName>
					<orgName type="abbreviated">ANR</orgName>
				</funder>
				<funder ref="#_RXDEa5y">
					<orgName type="full">GENCI-IDRIS</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Prajna</forename><surname>Upadhyay</surname></persName>
							<email>prajna.u@hyderabad.bits-pilani.ac.in</email>
						</author>
						<author>
							<persName><forename type="first">Oana</forename><surname>Balalau</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Ioana</forename><surname>Manolescu</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">BITS Pilani Hyderabad Campus</orgName>
								<address>
									<settlement>Secunderabad</settlement>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">Inria</orgName>
								<orgName type="institution" key="instit2">Institut Polytechnique de Paris</orgName>
								<address>
									<settlement>Palaiseau</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Open Information Extraction with Entity Focused Constraints</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">B9E221D3F69C067E5064625A980C1D48</idno>
					<note type="submission">Submitted on 9 Feb 2023</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-04-12T14:46+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>HAL is</head></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Open Information Extraction (OIE) is the task of extracting triples from unstructured corpora in a domain-independent manner. A triple consists of a subject, a relation, and an object. OIE has important applications, such as question answering <ref type="bibr" target="#b18">(Lu et al., 2019)</ref>, or automatically creating or extending knowledge bases <ref type="bibr" target="#b3">(Bhutani et al., 2019)</ref>. OIE is a challenging task, with the performance of state-ofthe-art models varying from 88.5% F1 score <ref type="bibr">(Wang</ref> The work was done when the first author was at Inria and Institut Polytechnique de Paris. <ref type="bibr">et al., 2021)</ref> to 34% <ref type="bibr" target="#b9">(Gashteovski et al., 2021)</ref>, depending on the difficulty of the benchmark.</p><p>When the named entities in a domain are known to be the subject/object of extractions, OIE should also identify relations between these entities. An important use case is automatically creating a knowledge base of relations between scientists and companies, i.e. identifying conflict-of-interest between the scientists and funding bodies, where the named entities are the names of scientists and companies, and the relation describes the conflict of interest between them. Clustering these relation phrases, such as received a research gift from, received speaker fees or consults for helps analyze the relationships that companies engage with the scientists. These relations are crucial to understanding scientists' positions on health issues <ref type="bibr" target="#b22">(Oreskes and Conway, 2010)</ref> in investigative journalism. However state-of-the-art OIE models do not always retain named entities in the extractions, for example, given the sentence "Shahrad Taheri received funding for research through a grant from Cambridge Weight Plan", an OIE tool <ref type="bibr">(Kolluru et al., 2020a)</ref> returns ⟨Shahrad Taheri, received, funding for research⟩.</p><p>While this extraction correctly identifies the subject of the triple, the quality of the predicate and object could be improved as follows: the extraction ⟨Shahrad Taheri, received funding for research through a grant from, Cambridge Weight Plan⟩ retains the second important entity ( Cambridge Weight Plan) and is precise about the relation. Such sentences are frequent in the declarations of conflict of interest that authors add to articles in PubMed, a dataset of scientific articles on life sciences and biomedical topics.</p><p>In this work, we focus on relation extraction, when the subject and object are named entities. In particular, we would like to significantly improve the performance of OIE tools, such that triples as ⟨first entity, predicate, second entity⟩ are not missed or poorly extracted. To achieve this, we leverage deep learning with constraints, i.e. techniques that enforce constraints on the classifier's predictions. Constrained learning is very common in sequence-to-sequence tasks, such as relation or entity extraction, where the output should have a specific form. Constraint learning has also been successfully used in OIE. In our case, we enforce constraints on the subject, object and predicate forms, and we investigate several techniques to achieve the best result, such as constraint-aware training <ref type="bibr" target="#b20">(Nandwani et al., 2019)</ref> and constraint inference <ref type="bibr">(Lee et al., 2019)</ref>. We deployed our technique within OpenIE6 <ref type="bibr">(Kolluru et al., 2020a)</ref>, a state-of-the-art tool for OIE.</p><p>Our salient contributions are: i) We extend the OpenIE6 model with entity-centric constraints; ii) We implement the constraints as penalties in the loss function, and as hard constraints during inference. iii) We show through an extensive evaluation that our method improves over the state-ofthe-art; iv) We perform a large scale evaluation of the system, on conflict of interest declarations from PubMed bibliographical data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>In the literature, the extraction of triples of the form ⟨subject, relation, object⟩ has been studied in several settings. A relation can be expressed using a surface form, i.e., the tokens present in a sentence, or a canonical form, usually introduced in a knowledge base. In the most general setting, we do not enforce any constraints on the types of the three elements, and the task is referred to as open information extraction (OIE). In the most restricted setting, the subject and object are entities, and the relation comes from a predefined set of relations. This task is known as relation extraction.</p><p>Finally, open relation extraction, also referred to as relation discovery, refers to approaches that use little training (such as distant supervision, fewshot learning, or semi-supervision) or no training (unsupervised) to classify relations between entities. Some inconsistencies arise in the use of the terminology in the literature, e.g., "open relation extraction" has been also used to designate open information extraction, in <ref type="bibr" target="#b19">(Mesquita et al., 2013)</ref>.</p><p>Open Information Extraction. Open information extraction <ref type="bibr">(Kolluru et al., 2020a;</ref><ref type="bibr" target="#b7">Etzioni et al., 2008)</ref> extracts triples from unstructured corpora in a domain-independent way. More precisely, the relations are not known beforehand and the subject and object are not required to be named entities. The state-of-the-art techniques are based on neural networks, which model the problem as a sequence labeling task <ref type="bibr">(Kolluru et al., 2020a;</ref><ref type="bibr" target="#b25">Stanovsky et al., 2018;</ref><ref type="bibr" target="#b4">Cui et al., 2018)</ref>. OpenIE6 <ref type="bibr">(Kolluru et al., 2020a</ref>) is a neural model that achieves state-of-theart results when compared with several other models <ref type="bibr" target="#b5">(Del Corro and Gemulla, 2013;</ref><ref type="bibr" target="#b8">Gashteovski et al., 2017;</ref><ref type="bibr" target="#b4">Cui et al., 2018;</ref><ref type="bibr" target="#b25">Stanovsky et al., 2018;</ref><ref type="bibr" target="#b23">Roy et al., 2019;</ref><ref type="bibr" target="#b32">Zhan and Zhao, 2020;</ref><ref type="bibr">Kolluru et al., 2020b)</ref>. Since these tools work without any domain knowledge, they might miss or extract poorly triples containing named entities. We aim to solve this problem, and our technique is trained to improve relation extraction when entities are present in the corpus.</p><p>Relation Extraction. In relation extraction <ref type="bibr" target="#b10">(Han et al., 2020)</ref>, given a sentence containing two entities, the task is to select the relation between the entities from a fixed set of relations. This is achieved via a classifier, and the challenge is in identifying relevant features for classification. Traditionally this has been achieved via hand-crafted features, such as lexical, syntactic, or semantic <ref type="bibr" target="#b13">(Jiang and Zhai, 2007;</ref><ref type="bibr" target="#b21">Nguyen et al., 2007)</ref>. More recently, neural models such as BERT <ref type="bibr" target="#b6">(Devlin et al., 2018)</ref> have been very successful in relation classification <ref type="bibr" target="#b1">(Baldini Soares et al., 2019)</ref>.</p><p>Open Relation Extraction/Relation Discovery. In <ref type="bibr" target="#b30">(Yao et al., 2011)</ref>, the authors first discover relations between entities using the dependency paths between two tagged entities, and they propose an unsupervised probabilistic generative model for inducing clusters from the surface forms. In <ref type="bibr" target="#b31">(Yu et al., 2017)</ref>, surface forms of relations are first extracted by taking into account the dependency path between entities, and finally, they are mapped to canonical forms present in a KB. In <ref type="bibr" target="#b12">(Hu et al., 2020)</ref>, the authors propose a relation encoder based on BERT <ref type="bibr" target="#b6">(Devlin et al., 2018)</ref> that computes an embedding representation of the relation based on the sentence where named entities appear, together with an adaptive clustering technique that does not require prior knowledge of the number of clusters. While some approaches <ref type="bibr" target="#b30">(Yao et al., 2011;</ref><ref type="bibr" target="#b31">Yu et al., 2017)</ref> extract surface forms of relations when the arguments are entities, similar to our goal in this work, they use for this only dependency path information and do not deal with conjunctive sentences as OpenIE <ref type="bibr">(Kolluru et al., 2020a)</ref>. In addition, Ope-nIE6 has shown better performance than models using dependency parsing such as ClausIE <ref type="bibr">(Kolluru et al., 2020a;</ref><ref type="bibr" target="#b5">Del Corro and Gemulla, 2013)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Problem Definition</head><p>Our goal is to extract triples from sentences that respect the guidelines detailed by the CaRB metric <ref type="bibr" target="#b2">(Bhardwaj et al., 2019)</ref>, i.e., they should be i) complete: all triples should be extracted from a sentence, ii) asserted: the triple should be implied from the sentence iii) informative: the triple should contain maximum relevant information from the sentence and iv) atomic: extraction cannot be split into multiple extractions.</p><p>Given a sentence S containing entities E = {e 1 , ..., e i , ..., e n }, we denote by ⟨S, R, O⟩ a triple that is extracted from S. The CaRB rules can be customized to fit our setting as follows:</p><p>• Complete: For every e i , there exists at least a triple ⟨S, R, O⟩ where e i is S or O.</p><p>• Asserted: Each tuple must be implied by the original sentence.</p><p>• Informative: The extraction should contain the maximum possible information from S.</p><p>For instance, from Joe Biden is the president of the US, an uninformative extraction is ⟨Joe Biden, is, the president⟩ while the informative extraction is ⟨Joe Biden, is the president of, US⟩.</p><p>• Atomic: If S or O contains e i , then it contains only that entity and no additional tokens. If S or O contain e i and e j , it is always possible to create two triples ⟨S1, R, O⟩ and ⟨S2, R, O⟩, S1 = e i and S2 = e j , similarly for O.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Entity Focused Constraints</head><p>OpenIE6 <ref type="bibr">(Kolluru et al., 2020a)</ref> receives in input a sentence and outputs a list of extractions of the form ⟨subject, predicate, object⟩. The architecture of the model is a deep neural network that first encodes tokens using BERT <ref type="bibr" target="#b6">(Devlin et al., 2018)</ref>, and then iteratively identifies at most M extractions, i.e., calls the same architecture for each extraction for M times (Figure <ref type="figure" target="#fig_0">1</ref>). The embeddings of the labels generated at the end of the 1st iteration are added to the embeddings of the tokens in the second iteration, and so on. This adds context so Head Verb Coverage (HVC). Verbs that are not light verbs (e.g., do, give, have, make, etc.), referred to as head verbs, should be present in the relation span of a few but not too many extractions.</p><p>Head Verb Exclusivity (HVE). The relation span of one extraction should contain at most one head verb.</p><p>Extraction Count (EC). The extractions having head verbs in the relation should be at least equal to the number of head verbs in the sentence.</p><p>These are entity independent constraints. Their full equations can be found in the OpenIE6 paper <ref type="bibr">(Kolluru et al., 2020a)</ref>.</p><p>Adding entity-specific constraints. We enforce additional constraints to obtain extractions satisfying our problem statement. Let x ent n ∈ {0, 1} denote whether the nth token w n belongs to some entity tagged in the sentence, and E be the set of entities. At each extraction level m, the model com-putes Y mn (k), the probability of assigning to the nth token the label k ∈ {S, R, O, N } (subject, relation, object or none). We introduce the following entity-specific constraints:</p><p>1. Entities as subject or object (ENT-ARG).</p><p>Each entity in the sentence should be present in at least a subject or object of an extraction:</p><formula xml:id="formula_0">Jent_so = N n=1 x ent n • 1-max m∈[1,M ] max k∈{S,O} Ymn(k) (1)</formula><p>The penalty is 0 when for each token belonging to an entity (x ent n = 1) we have Y mn (k) = 1, that is maximum probability of being in the subject or object, for at least one extraction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Entity exclusivity (ENT-EXCL).</head><p>The subject and object should contain at most one entity each. Let p e (k), with k ∈ {S, R, O, N } be the average token probability of label k in entity e, where e consists of one or more tokens. Then, we express the penalty as follows:</p><formula xml:id="formula_1">Jent_exs = M m=1 max 0, e∈E pe(S) -1 (2) Jent_exo = M m=1 max 0, e∈E pe(O) -1 (3)</formula><p>The penalty is 0 when no entity is labeled as subject/object or when only one entity is labeled as such ( e∈E p e (O/S) is 0 or 1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Entity in relation penalty (ENT-REL).</head><p>A penalty is introduced if an entity appears as a part of a relation of some extraction. This loss is directly proportional to the probability of tokens that are part of some entities and which have been labeled as part of a relation:</p><formula xml:id="formula_2">J ent_rel = N n=1 x ent n • M m=1 Ymn(R) (4)</formula><p>The penalty is 0 when Y mn (R) is 0 for every token of an entity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Entity segmentation penalty (ENT-TOG).</head><p>A penalty is introduced if tokens describing the same entity are not labeled in the same way, for example, the first token of the entity is part of the predicate, while the rest of the tokens are part of the object. Let w(e) be the set of tokens in a given entity e. Let l m p (w) be the predicted label of a token (the label with the highest probability) at extraction m. As we are concerned with entities described by two or more tokens, the predicted label l m e of the entity e is the majority label of its tokens, or the label with the highest total sum of probabilities in case of a tie. For each w ∈ w(e), we introduce a loss equivalent to</p><formula xml:id="formula_3">Y mw(lp) if l m p (w) ̸ = l m e : Jent_seg = M m=1 e∈E w∈w(e) Y mw(lp) (1 -δ l m p (w),l m e ) (<label>5</label></formula><formula xml:id="formula_4">)</formula><p>where δ is the Kronecker delta function.</p><p>Finally, the total loss can be written as:</p><formula xml:id="formula_5">J ent = J + λ 1 J ent_so + λ 2 (J ent_exs + J ent_exo ) + λ 3 J ent_rel + λ 4 J ent_seg<label>(6)</label></formula><p>where λ * are hyperparameters, while J is the original OpenIE6 loss.</p><p>Constraints at inference. We investigate a second type of constrained learning called constraint inference. The constraints applied in this setting are hard constraints, which the model is forced to apply. The constraints are applied in the decoding phase and modify the tokens' labels (S, P , O, N ). We propose three constraints inspired by the entity constraints introduced in the constraint-aware training.</p><p>1. Entity exclusivity. Once we have encountered one entity labeled as a subject or object in the sentence, the following entities are not allowed to receive the same label.</p><p>5 Experimental Evaluation</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Datasets</head><p>We use the OpenIE6 data for training and validation and Pubmed data for testing. The OpenIE6 dataset consists of Wikipedia sentences, while the Pubmed data is a set of conflict of interest statements between authors and various organizations, such as those illustrated in Section 1.</p><p>Given that our focus is on improving performance when entities are present in a sentence (Section 3), and in particular, enforcing that entities are the subject or object, we need appropriate training data for the task. We are unaware of a dataset of extractions where arguments are entities, while the extraction also has the surface forms of relation. For example, FewRel <ref type="bibr" target="#b11">(Han et al., 2018)</ref> and TA-CRED <ref type="bibr" target="#b33">(Zhang et al., 2017)</ref>, two standard datasets used in relation extraction, do not contain the surface form of the relation; they only label the entire sentence as containing a particular relation.</p><p>Training data. The OpenIE6 training dataset consists of 91K sentences and 190K extractions of the form ⟨subject, predicate, object⟩. We tag entities in each sentence using the state-ofnamed entity recognition tool Flair <ref type="bibr" target="#b0">(Akbik et al., 2019)</ref>. We focus on extractions of the following form: i) The subject of the extraction is exactly one entity; and ii) The object ends with an entity . We discard the extractions that do not match these constraints. In each extraction, we keep only the entity in the object and move the preceding tokens to the relationship part of the extraction. For example, one of the sentences in the original training set is "Parmenides had a large influence on Plato, who not only named a dialogue, Parmenides, after Parmenides, but always spoke of Parmenides with veneration." and one of the extractions is ⟨Parmenides, had, a large influence on Plato⟩. The extraction satisfies both the above conditions, hence we transform it to ⟨Parmenides, had a large influence on, Plato⟩. If the object contains only an entity, we apply the identify transformation. We refer to a sentence with at least one transformed extraction as a clean sentence.</p><p>We create 3 training datasets: ORIGINAL: The original training set containing 91K sentences.</p><p>CLEAN: 7K clean sentences with their modified extractions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MIXED:</head><p>We add the remaining sentences and their extractions from the original training set to CLEAN.</p><p>Gold data. We created a gold standard dataset from Pubmed conflict-of-interest statements to be used as test data. We tagged and counted the entities with NER Flair and selected 282 sentences with a minimum of 2 entities. The maximum number of entities found in a sentence was 14.</p><p>We asked the annotators to find all the triples ⟨S, P, O⟩ containing those entities as arguments (in S or O). In addition, the extractions should follow the guidelines explained in Section 3 on completeness, assertion, informativeness, and atomicity. The total number of extractions obtained after annotations were 1113. One annotator annotated each sentence.</p><p>Quality of gold data. To evaluate the dataset's quality, we sampled 50 sentences from our gold sentences, and one of the authors annotated them so that we had two annotations for this set. We found the agreement by considering one annotation as gold and computing WiRE57 F1. The agreement F1 score obtained was 83, which is a high agreement.</p><p>Table <ref type="table" target="#tab_0">1</ref> shows example annotations of triples for the sentence Menno Huisman reports grants from and personal fees from Boehringer Ingelheim and Bayer Health Care. For each CaRB property, we show the correct and incorrect extractions. An extraction of the form ⟨Menno Huisman, reports grants from, Bayer Health Care, Germany⟩ violates the assertion property because it adds extra information to the sentence. ⟨Menno Huisman, reports, grants⟩ violates the informativeness property even if it is a valid extraction because it lacks the complete second argument, i.e., Boehringer Ingelheim. The extraction ⟨Menno Huisman, reports grants from, Boehringer Ingelheim and Bayer Health Care⟩ is not atomic because the two entities in the second argument should be part of 2 extractions. If any of the four correct extractions adhering to the completeness property are missing, this property is violated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Models</head><p>We experimented with the following models:</p><p>OpenIE6. This is the default OpenIE6 model. OpenIE6(ECIN). To the trained model OpenIE6, we add constraints at inference in the evaluation of the test data.</p><p>We note that the models use a different coordinate boundary model than the one in the OpenIE6 paper. We retrained the coordinate boundary model using a newer Huggingface Transformers library version <ref type="bibr" target="#b29">(Wolf et al., 2020)</ref> for compatibility with our code. However, we could not reproduce the accuracy, obtaining 83.3 instead of 85.4. A better coordinate boundary model would positively impact performance, both with and without constraints.</p><p>Parameters. The model's training consists of two phases, a warm-up phase, where the training is done without constraints, and a constrained training part. The warm-up training was done for 30 epochs, and the constrained training was done for 15 epochs. During constrained training, all constraints had equal weights. The learning rate was set to 5e-06. BERT-base-cased model was used with two iterative layers. We repeat the experiments with 6 different random seeds for the network initialization, and we average the results. We run our code on a 32GB GPU.</p><p>Baselines We implement four baselines.</p><p>ConnectingPhrase. This simple technique returns the phrase connecting the two entities in a sentence as the relation between them. It comprises the following steps:</p><p>1. We first use the coordinate boundary detection model (available with OpenIE6 code). Coordinate boundary detection models <ref type="bibr" target="#b24">(Saha and Mausam, 2018;</ref><ref type="bibr">Kolluru et al., 2020a)</ref> split a conjunctive sentence into smaller parts. For example, the sentence "Adrian Brown and Shahrad Taheri received funding for research through a grant from Cambridge Weight Plan." is split into:</p><p>(a) "Adrian Brown received funding for research through a grant from Cambridge Weight Plan." (b) "Shahrad Taheri received funding for research through a grant from Cambridge Weight Plan."</p><p>This is crucial to improve the recall.</p><p>2. Next, we label the entities in sentences obtained using Flair <ref type="bibr" target="#b0">(Akbik et al., 2019)</ref>.</p><p>3. For each consecutive pair of entities e i , e i+1 in the sentence, we return an extraction containing as subject e i , as predicate the phrase connecting the entities, and as object e i+1 .</p><p>4. We filter the extractions by removing the ones whose predicates do not contain a token labeled as a verb by a part-of-speech parser. The final set of extractions is obtained at the end of this step.</p><p>DependencyPath. We follow the same steps as in ConnectingPhrase, except that in 3. above, we return as the predicate the tokens on the dependency path between entities e i and e i+1 .</p><p>PostprocessedOpenIE6. We run the original OpenIE6 tool and post-process its output as follows: we tag entities in subject and object of the extractions, and then we modify extractions, in the same manner as when we created the CLEAN dataset (Section 5.1), and leave unchanged the ones not satisfying our conditions.</p><p>FilteredOpenIE6. We remove the extractions from PostprocessedOpenIE6 that were not modified according to the procedure used for generating the CLEAN dataset.</p><p>Evaluation metrics. Several evaluation metrics have been proposed to evaluate the performance of an OpenIE system. WiRe57 <ref type="bibr" target="#b16">(Lechelle et al., 2019)</ref> is a one-to-one matching metric, in which each system extraction is matched to exactly one gold extraction. Given a sentence, a system extraction matches a gold extraction if they share at least one word from each of the relation, subject, and object. Two extractions are compared by computing the token level recall and precision between the gold subject and system subject, respectively, the predicates and objects. Precision is the percentage of system words found in the gold extraction. The recall is the percentage of gold words in the systems' predictions. The system extractions are matched one-to-one to gold extraction in decreasing order of F 1-score.</p><p>CaRB <ref type="bibr" target="#b2">(Bhardwaj et al., 2019)</ref> is a many-to-one matching metric in which several gold extractions can be matched to one system extraction when computing the recall. This avoids penalizing a system if one extraction would better correspond to two or more golden extractions, as is the case, for instance, in ⟨Adrian Brown; has received travel grants from; Cambridge Weight Plan and Oxford University⟩ (note that there should have been two triples extracted here, each with a different object). Precision is computed by matching system extractions one-to-one to gold extractions, decreasing order of precision score. Hence, we will penalize the extraction above when computing precision, as one gold extraction will not be matched. We report both metrics, however, WiRe57 is more in line with our task as it respects the atom-icity constraint in Section 3, given that it does not reward system triples with several entities in one argument.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Results and Discussion</head><p>Evaluation. In Table <ref type="table" target="#tab_1">2</ref> we show the results on the test data, measuring both CaRB and WiRe57. We use the different training datasets that we introduced and the different training constraints. When training without any entity constraints, the training dataset can make a significant difference, as we observe OpenIE6 trained on CLEAN has a more than 26% increase in CaRB F1 than OpenIE6 trained on the ORIGINAL dataset. In addition, adding entity constraints further improves the results as shown by the models OpenIE6(ECTR) which has the best WiRe57 score for all CLEAN, MIXED and ORIG-INAL models. The smallest improvement is for the model trained with the ORIGINAL dataset, as in this case the training data may be in conflict with the constraints, having for example several entities in one argument. OpenIE6(ECIN) improves upon OpenIE6, with a significant increase in the precision of the WiRe57 metric, which is expected given the hard constraints are being forced on the triples. However,OpenIE6(ECTR) has a more significant improvement than OpenIE6(ECIN) according to WiRe57 (the metric aligned with our problem statement, as explained in Section 5.2), showing that it is more important to have soft constraints, which are rewarding good extractions during training and hence obtaining a better extraction model. Combining soft and hard constraints gives the best model, OpenIE6(ECTR, ECIN). Regarding the baselines, PostprocessedOpenIE6 and FilteredOpenIE6 have good precision but lower recall than our top-performing models, showing the importance of the constraint learning and adapted training datasets.</p><p>Ablation study. We perform an ablation study to evaluate the importance of the entity constraints added during the training. We take our best performing model, OpenIE6(ECTR) trained on the CLEAN dataset, and we train it with 1, 2, or 3 constraints at a time. Table <ref type="table" target="#tab_2">3</ref> shows the results obtained on our test set. When we add just one constraint, as expected, the constraint ENT-ARG enforces the highest WiRe57 recall, as it has learned to penalize extractions where entities may be missing from the arguments. However, this model has the lowest precision, due to the fact it allows more than one  For a complete analysis, we also compute the percentage of violations in the extractions (Table <ref type="table" target="#tab_2">3</ref>). For ENT-ARG, we count as a violation every entity that is not found in at least one extraction, and we divide by the total number of entities in the test set. For ENT-EXCL, we count a violation for each subject or object with more than one entity and normalize by twice the number of extractions. For ENT-REL, a violation is a relation containing an entity, normalized by the number of extractions. Finally, for ENT-TOG, a violation is an entity in extraction with more than one tag (S,O,R,N), normalized by the number of extractions containing an entity. We observe that ENT-ARG is violated the most, followed by ENT-REL. When enforcing ENT-ARG, we obtain the best results for 3 out of 4 constraints. This does not result, however, in the best F 1 score, showing the importance of minimizing violations of type ENT-EXCL. ENT-ARG and ENT-EXCL have competing goals: ENT-ARG enforces the occurrence of entities in arguments, but ENT-EXCL does not allow more than one entity in an argument. So, whenever ENT-ARG is enforced with ENT-EXCL, we see an increase in the number of ENT-ARG or ENT-EXCL violations. Finally, when comparing the model with no entity constraints, OpenIE6, with the model enforcing all 4 constraints, OpenIE6(ECTR), we observe a more significant difference in the violations ENT-ARG and ENT-REL, the constraints that are more frequently violated.</p><p>Quality of Named Entity Recognition on Pubmed. We sampled and annotated 50 test set sentences, taking care to keep the words together in long named entities, such as "Oregon Health and Science University Center for Embryonic Cell and Gene Therapy". We obtained 88% F1 score for the NER model Flair <ref type="bibr" target="#b0">(Akbik et al., 2019)</ref> , in line with the performance of the model on Ontonotes <ref type="bibr" target="#b28">(Weischedel et al., 2017)</ref> and CONLL <ref type="bibr" target="#b26">(Tjong Kim Sang and De Meulder, 2003)</ref>.</p><p>Evaluation on the CaRB dataset. OpenIE6 has been evaluated on the CaRB dataset <ref type="bibr" target="#b2">(Bhardwaj et al., 2019)</ref>. We evaluate our constrained models to investigate their performance on this standard benchmark, see Table <ref type="table" target="#tab_3">4</ref>. Note that annotating guidelines for CaRB were not the same as for our Pubmed test data: there might be more than one entity in the arguments of a relation. This inherently limits the quality of our results. However, we show that the constrained models trained on the MIXED and ORIGINAL datasets have competitive performance with the original OpenIE6 model while performing much better on our test data, as shown in Table <ref type="table" target="#tab_1">2</ref>. As expected, the models trained on the CLEAN dataset perform the worst, as they have seen only extractions with entities in the arguments; to achieve the best results, the user should choose a model considering the nature of the dataset. We note that the results for the OpenIE6 model are slightly lower than those reported in the original paper because of the coordinate boundary model, as mentioned in Section 5. The conjunctive model is a core OpenIE6 component; gains in its precision would likely improve performance, both with and without constraints.</p><p>Conflicts of interest in PubMed. We analyse the extractions by OpenIE6(ECTR) and the original OpenIE6 model on a larger PubMed dataset consisting of 170K sentences. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion.</head><p>We presented an approach that significantly improves OIE when the input sentence contains entities while being competitive on a standard OIE benchmark. Finally, we showed that our method is much better suited for a real use case, as it extracts high-quality triples from PubMed.</p><p>We identify the following limitations affecting our proposed methods:</p><p>• The performance of our models is impacted by the quality of the named entity recognition tool, as well as the performance of the conjunctive model.</p><p>• Training OpenIE6 with more constraints requires around 3h/epoch, while the model with the original constraints requires half this time.</p><p>• Users trying our tool, but also the original OpenIE model, should have the computational possibility of using the BERT-based model, the main component of OpenIE6. We plan to release trained models based of smaller language models.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: OpenIE6 uses the same architecture to generate embeddings for the words in M extractions, with the output of the previous extraction given as input for the next extraction</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Examples of correct and incorrect annotations for the 4 CaRB propertiesOpenIE6(ECTR, ECIN). To the trained model OpenIE6(ECTR), we add constraints at inference in the evaluation of the test data.</figDesc><table><row><cell></cell><cell>Correct</cell><cell>Incorrect</cell></row><row><cell>Completeness</cell><cell>⟨Menno Huisman, reports grants</cell><cell>If any of the extractions is missing</cell></row><row><cell></cell><cell>from, Boehringer Ingelheim⟩,</cell><cell></cell></row><row><cell></cell><cell>⟨Menno Huisman, reports grants</cell><cell></cell></row><row><cell></cell><cell>from, Bayer Health Care⟩, ⟨Menno</cell><cell></cell></row><row><cell></cell><cell>Huisman, reports personal fees</cell><cell></cell></row><row><cell></cell><cell>from, Boehringer Ingelheim⟩,</cell><cell></cell></row><row><cell></cell><cell>⟨Menno Huisman, reports</cell><cell></cell></row><row><cell></cell><cell>personal fees from, Bayer</cell><cell></cell></row><row><cell></cell><cell>Health Care⟩</cell><cell></cell></row><row><cell>Assertion</cell><cell>⟨Menno Huisman, reports grants</cell><cell cols="2">⟨Menno Huisman, reports grants</cell></row><row><cell></cell><cell>from, Bayer Health Care⟩</cell><cell>from, Bayer Health Care,</cell></row><row><cell></cell><cell></cell><cell>Germany⟩</cell></row><row><cell cols="2">Informativeness ⟨Menno Huisman, reports grants</cell><cell cols="2">⟨Menno Huisman, reports, grants⟩</cell></row><row><cell></cell><cell>from, Boehringer Ingelheim⟩</cell><cell></cell></row><row><cell>Atomic</cell><cell>⟨Menno Huisman, reports grants</cell><cell cols="2">⟨Menno Huisman, reports grants</cell></row><row><cell></cell><cell>from, Boehringer Ingelheim⟩</cell><cell cols="2">from, Boehringer Ingelheim</cell></row><row><cell></cell><cell></cell><cell>and Bayer Health Care⟩,</cell><cell>⟨Menno</cell></row><row><cell></cell><cell></cell><cell>Huisman, reports grants</cell></row><row><cell></cell><cell></cell><cell cols="2">from and personal fees from,</cell></row><row><cell></cell><cell></cell><cell>Boehringer Ingelheim⟩</cell></row></table><note><p>OpenIE6(ECTR). OpenIE6 model with entity constraint training (ECTR), as in Section 4.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Model comparison on the test dataset. Best values are in bold and second best are underlined.</figDesc><table><row><cell>CaRB</cell><cell>WiRe57</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Ablation study with models trained on the CLEAN dataset. We report CaRB, WiRe57, and the percentage of entity constraints violations on the test set.</figDesc><table><row><cell>entity in one argument. Removing the constraint</cell></row><row><cell>from the set, EC \ ENT-ARG, gives us the highest</cell></row><row><cell>precision. A combination of ENT-EXCL and ENT-</cell></row><row><cell>REL performs the best among the models that were</cell></row><row><cell>trained with 2 constraints, which is expected since</cell></row><row><cell>the models trained with ENT-EXCL and ENT-REL</cell></row><row><cell>were the top-2 performing models when trained</cell></row><row><cell>individually. Enforcing only ENT-TOG does not</cell></row><row><cell>bring important improvements, and training with</cell></row><row><cell>the whole EC is slightly better than when training</cell></row><row><cell>with EC \ ENT-TOG. Hence, ENT-TOG could be</cell></row><row><cell>removed without a significant drop in quality.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Table 5 shows the Model comparison on the CaRB dataset. Best values are in bold and second best are underlined.number of extractions (#ext), extractions containing one entity in the subject and object (#ext1), containing a "Person" entity in subject and "Organization" entity in the object (#ext2), and the number of sentences processed by the model per second (speed). OpenIE6(ECTR) finds more interesting triples where a conflict of interest relation is expressed between a person and an organization entity, compared to the original OpenIE6. Also, our model processes more sentences per second compared to the original OpenIE6. This is because OpenIE6 generates more extractions per sentence, however, even with more extractions, the model retrieves fewer conflicts of interest relations between a person and an organization.</figDesc><table><row><cell>CaRB WiRe57</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Comparison of extractions from a larger dataset of PubMed conflict of interest statements.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>Entity in relation. We enforce that an entity appearing in the predicate is classified according to its second-best class probability.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1"><p>3. Entity segmentation penalty. We enforce that all the tokens belonging to an entity be labeled with the same label.We do not transform the constraint entities as subject or object in an inference constraint as it cannot be applied at the level of one existing extraction. This constraint can only be a penalty in the loss, such that it rewards sets of extractions in which all the entities are part of the arguments.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgments We thank <rs type="person">Rémi Goujot</rs> for manually annotating the PubMed dataset used to test our model, during his high school internship.</p><p>This work was performed using HPC resources from <rs type="funder">GENCI-IDRIS</rs> (Grant <rs type="grantNumber">2022-AD011011614R2</rs>). The authors were partially funded by the <rs type="grantNumber">ANR-20-CHIA-0015</rs> project and by the <rs type="funder">Hi!PARIS Center</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_RXDEa5y">
					<idno type="grant-number">2022-AD011011614R2</idno>
				</org>
				<org type="funding" xml:id="_yhHZcJk">
					<idno type="grant-number">ANR-20-CHIA-0015</idno>
				</org>
			</listOrg>

			<div type="availability">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>data are available at https: //github.com/prajnaupadhyay/</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">FLAIR: An easy-to-use framework for state-of-theart NLP</title>
		<author>
			<persName><forename type="first">Alan</forename><surname>Akbik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tanja</forename><surname>Bergmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Duncan</forename><surname>Blythe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kashif</forename><surname>Rasul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Schweter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roland</forename><surname>Vollgraf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Conference of the North American Chapter of the Association for Computational Linguistics (Demonstrations)</title>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
			<biblScope unit="page" from="54" to="59" />
		</imprint>
	</monogr>
	<note>NAACL 2019</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Matching the blanks: Distributional similarity for relation learning</title>
		<author>
			<persName><forename type="first">Baldini</forename><surname>Livio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Soares</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Fitzgerald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName><surname>Kwiatkowski</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1279</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2895" to="2905" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">CaRB: A crowdsourced benchmark for open IE</title>
		<author>
			<persName><forename type="first">Sangnie</forename><surname>Bhardwaj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samarth</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mausam</forename><surname>Mausam</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1651</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6262" to="6267" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Open information extraction from question-answer pairs</title>
		<author>
			<persName><forename type="first">Nikita</forename><surname>Bhutani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshihiko</forename><surname>Suhara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wang-Chiew</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alon</forename><surname>Halevy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">V</forename><surname>Jagadish</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1239</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2294" to="2305" />
		</imprint>
	</monogr>
	<note>Long and Short Papers</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Neural open information extraction</title>
		<author>
			<persName><forename type="first">Lei</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.1805.04270</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Clausie: clause-based open information extraction</title>
		<author>
			<persName><forename type="first">Luciano</forename><surname>Del</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Corro</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Rainer</forename><surname>Gemulla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd international conference on World Wide Web</title>
		<meeting>the 22nd international conference on World Wide Web</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="355" to="366" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Open information extraction from the web</title>
		<author>
			<persName><forename type="first">Oren</forename><surname>Etzioni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michele</forename><surname>Banko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Soderland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">S</forename><surname>Weld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="68" to="74" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">MinIE: Minimizing facts in open information extraction</title>
		<author>
			<persName><forename type="first">Kiril</forename><surname>Gashteovski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rainer</forename><surname>Gemulla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luciano</forename><surname>Del</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Corro</forename></persName>
		</author>
		<idno type="DOI">10.18653/v1/D17-1278</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2630" to="2640" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">Kiril</forename><surname>Gashteovski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingying</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bhushan</forename><surname>Kotnis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carolin</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Goran</forename><surname>Glavas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mathias</forename><surname>Niepert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.06850</idno>
		<title level="m">Benchie: Open information extraction evaluation based on facts, not tokens</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">More data, more relations, more context and more openness: A review and outlook for relation extraction</title>
		<author>
			<persName><forename type="first">Xu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyu</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yankai</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaoliang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chaojun</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing</title>
		<meeting>the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing<address><addrLine>Suzhou, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="745" to="758" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">FewRel: A large-scale supervised few-shot relation classification dataset with state-of-the-art evaluation</title>
		<author>
			<persName><forename type="first">Xu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengfei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziyun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D18-1514</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4803" to="4809" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">SelfORE: Self-supervised relational feature learning for open relation extraction</title>
		<author>
			<persName><forename type="first">Xuming</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lijie</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yusong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><surname>Yu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.299</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="3673" to="3682" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A systematic exploration of the feature space for relation extraction</title>
		<author>
			<persName><forename type="first">Jing</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengxiang</forename><surname>Zhai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Conference of the North American Chapter of the Association for Computational Linguistics; Proceedings of the Main Conference</title>
		<meeting><address><addrLine>Rochester, New York</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2007">2007. 2007</date>
			<biblScope unit="page" from="113" to="120" />
		</imprint>
	</monogr>
	<note>Human Language Technologies</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">a. Openie6: Iterative grid labeling and coordination analysis for open information extraction</title>
		<author>
			<persName><forename type="first">Keshav</forename><surname>Kolluru</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vaibhav</forename><surname>Adlakha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samarth</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mausam</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Soumen</forename><surname>Chakrabarti</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.306</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP 2020</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP 2020<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020-11-16">2020. November 16-20, 2020</date>
			<biblScope unit="page" from="3748" to="3761" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">IMo-JIE: Iterative memory-based joint open information extraction</title>
		<author>
			<persName><forename type="first">Keshav</forename><surname>Kolluru</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samarth</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName><surname>Vipul Rathore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soumen</forename><surname>Mausam</surname></persName>
		</author>
		<author>
			<persName><surname>Chakrabarti</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.521</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="5871" to="5886" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">WiRe57 : A fine-grained benchmark for open information extraction</title>
		<author>
			<persName><forename type="first">William</forename><surname>Lechelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabrizio</forename><surname>Gotti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phillippe</forename><surname>Langlais</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W19-4002</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th Linguistic Annotation Workshop</title>
		<meeting>the 13th Linguistic Annotation Workshop<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Gradient-based inference for networks with output constraints</title>
		<author>
			<persName><forename type="first">Jay</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lee</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Sanket</forename><surname>Vaibhav Mehta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Wick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean-Baptiste</forename><surname>Tristan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaime</forename><surname>Carbonell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="4147" to="4154" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Answering complex questions by joining multi-document evidence with quasi knowledge graphs</title>
		<author>
			<persName><forename type="first">Xiaolu</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soumajit</forename><surname>Pramanik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rishiraj</forename><surname>Saha Roy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abdalghani</forename><surname>Abujabal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yafang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gerhard</forename><surname>Weikum</surname></persName>
		</author>
		<idno type="DOI">10.1145/3331184.3331252</idno>
	</analytic>
	<monogr>
		<title level="m">SIGIR&apos;19</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="105" to="114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Effectiveness and efficiency of open relation extraction</title>
		<author>
			<persName><forename type="first">Filipe</forename><surname>Mesquita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jordan</forename><surname>Schmidek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denilson</forename><surname>Barbosa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Seattle, Washington, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="447" to="457" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A primal dual formulation for deep learning with constraints</title>
		<author>
			<persName><forename type="first">Yatin</forename><surname>Nandwani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhishek</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Parag</forename><surname>Singla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="12157" to="12168" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Relation extraction from wikipedia using subtree mining</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Dat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yutaka</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mitsuru</forename><surname>Matsuo</surname></persName>
		</author>
		<author>
			<persName><surname>Ishizuka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd National Conference on Artificial Intelligence</title>
		<meeting>the 22nd National Conference on Artificial Intelligence</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1414" to="1420" />
		</imprint>
	</monogr>
	<note>AAAI&apos;07</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<author>
			<persName><forename type="first">N</forename><surname>Oreskes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Conway</surname></persName>
		</author>
		<title level="m">Merchants of Doubt: How a Handful of Scientists Obscured the Truth on Issues from Tobacco Smoke to Global Warming</title>
		<imprint>
			<publisher>Bloomsbury Publishing</publisher>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Supervising unsupervised open information extraction models</title>
		<author>
			<persName><forename type="first">Arpita</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Youngja</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taesung</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shimei</forename><surname>Pan</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1067</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="728" to="737" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Open information extraction from conjunctive sentences</title>
		<author>
			<persName><forename type="first">Swarnadeep</forename><surname>Saha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mausam</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Computational Linguistics</title>
		<meeting>the 27th International Conference on Computational Linguistics<address><addrLine>Santa Fe, New Mexico, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2288" to="2299" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Supervised open information extraction</title>
		<author>
			<persName><forename type="first">Gabriel</forename><surname>Stanovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N18-1081</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>New Orleans, Louisiana</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="885" to="895" />
		</imprint>
	</monogr>
	<note>Long Papers</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Introduction to the CoNLL-2003 shared task: Language-independent named entity recognition</title>
		<author>
			<persName><forename type="first">Erik</forename><forename type="middle">F</forename><surname>Tjong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kim</forename><surname>Sang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fien</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meulder</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventh Conference on Natural Language Learning at HLT-NAACL 2003</title>
		<meeting>the Seventh Conference on Natural Language Learning at HLT-NAACL 2003</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="142" to="147" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Zero-shot information extraction as a unified text-to-triple translation</title>
		<author>
			<persName><forename type="first">Chenguang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zui</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoyun</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawn</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2021 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1225" to="1238" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Ontonotes : A large training corpus for enhanced processing</title>
		<author>
			<persName><forename type="first">Ralph</forename><forename type="middle">M</forename><surname>Weischedel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eduard</forename><forename type="middle">H</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mitchell</forename><forename type="middle">P</forename><surname>Marcus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martha</forename><surname>Palmer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Transformers: State-of-the-art natural language processing</title>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lysandre</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clement</forename><surname>Delangue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anthony</forename><surname>Moi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierric</forename><surname>Cistac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Rault</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rémi</forename><surname>Louf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Morgan</forename><surname>Funtowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joe</forename><surname>Davison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Shleifer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clara</forename><surname>Patrick Von Platen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yacine</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Canwen</forename><surname>Plu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Teven</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sylvain</forename><surname>Le Scao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mariama</forename><surname>Gugger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quentin</forename><surname>Drame</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Lhoest</surname></persName>
		</author>
		<author>
			<persName><surname>Rush</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="38" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Structured relation discovery using generative models</title>
		<author>
			<persName><forename type="first">Limin</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aria</forename><surname>Haghighi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2011 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Edinburgh, Scotland, UK</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1456" to="1466" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Open relation extraction and grounding</title>
		<author>
			<persName><forename type="first">Dian</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lifu</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heng</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighth International Joint Conference on Natural Language Processing</title>
		<meeting>the Eighth International Joint Conference on Natural Language Processing<address><addrLine>Taipei, Taiwan</addrLine></address></meeting>
		<imprint>
			<publisher>Asian Federation of Natural Language Processing</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="854" to="864" />
		</imprint>
	</monogr>
	<note>Long Papers</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Span model for open information extraction on accurate corpus</title>
		<author>
			<persName><forename type="first">Junlang</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hai</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="9523" to="9530" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Position-aware attention and supervised data improve slot filling</title>
		<author>
			<persName><forename type="first">Yuhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabor</forename><surname>Angeli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D17-1004</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="35" to="45" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
