<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Online Deep Learning Hyperparameter Tuning based on Provenance Analysis</title>
				<funder>
					<orgName type="full">Inria (HPDaSc associated team)</orgName>
				</funder>
				<funder>
					<orgName type="full">FAPERJ</orgName>
				</funder>
				<funder>
					<orgName type="full">CNPq</orgName>
				</funder>
				<funder ref="#_7Vq2Vaf">
					<orgName type="full">Coordenação de Aperfeiçoamento de Pessoal de Nível Superior -Brasil (CAPES) -Finance</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Liliane</forename><surname>Kunstmann</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">COPPE/Federal University of Rio de Janeiro</orgName>
								<address>
									<settlement>Rio de Janeiro</settlement>
									<country key="BR">Brazil</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Débora</forename><surname>Pina</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">COPPE/Federal University of Rio de Janeiro</orgName>
								<address>
									<settlement>Rio de Janeiro</settlement>
									<country key="BR">Brazil</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Filipe</forename><surname>Silva</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">COPPE/Federal University of Rio de Janeiro</orgName>
								<address>
									<settlement>Rio de Janeiro</settlement>
									<country key="BR">Brazil</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Aline</forename><surname>Paes</surname></persName>
							<email>alinepaes@ic.uff.br</email>
							<affiliation key="aff1">
								<orgName type="institution">Fluminense Federal University</orgName>
								<address>
									<settlement>Niterói, Rio de Janeiro</settlement>
									<country key="BR">Brazil</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Patrick</forename><surname>Valduriez</surname></persName>
							<email>patrick.valduriez@inria.fr</email>
							<affiliation key="aff2">
								<orgName type="laboratory">LIRMM</orgName>
								<orgName type="institution" key="instit1">Inria</orgName>
								<orgName type="institution" key="instit2">University of Montpellier</orgName>
								<orgName type="institution" key="instit3">CNRS</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Daniel</forename><surname>De Oliveira</surname></persName>
							<email>danielcmo@ic.uff.br</email>
							<affiliation key="aff1">
								<orgName type="institution">Fluminense Federal University</orgName>
								<address>
									<settlement>Niterói, Rio de Janeiro</settlement>
									<country key="BR">Brazil</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Marta</forename><surname>Mattoso</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">COPPE/Federal University of Rio de Janeiro</orgName>
								<address>
									<settlement>Rio de Janeiro</settlement>
									<country key="BR">Brazil</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Online Deep Learning Hyperparameter Tuning based on Provenance Analysis</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">23FFD9CECDE2E2744074C2CE06CB2D7D</idno>
					<idno type="DOI">10.5753/jidm.2021.1924</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-04-12T14:50+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Descriptors: H.2.1 [Database Management]: Logical Design; H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval Deep Learning</term>
					<term>Provenance</term>
					<term>Hyperparameter tuning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Training Deep Learning (DL) models require adjusting a series of hyperparameters. Although there are several tools to automatically choose the best hyperparameter configuration, the user is still the main actor to take the final decision. To decide whether the training should continue or try different configurations, the user needs to analyze online the hyperparameters most adequate to the training dataset, observing metrics such as accuracy and loss values. Provenance naturally represents data derivation relationships (i.e., transformations, parameter values, etc.), which provide important support in this data analysis. Most of the existing provenance solutions define their own and proprietary data representations to support DL users in choosing the best hyperparameter configuration, which makes data analysis and interoperability difficult. We present Keras-Prov and its extension, named Keras-Prov++, which provides an analytical dashboard to support online hyperparameter fine-tuning. Different from the current mainstream solutions, Keras-Prov automatically captures the provenance data of DL applications using the W3C PROV recommendation, allowing for hyperparameter online analysis to help the user deciding on changing hyperparameters' values after observing the performance of the models on a validation set. We provide an experimental evaluation of Keras-Prov++ using AlexNet and a real case study, named DenseED, that acts as a surrogate model for solving equations. During the online analysis, the users identify scenarios that suggest reducing the number of epochs to avoid unnecessary executions and fine-tuning the learning rate to improve the model accuracy.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Over the last few years, in the Deep Learning (DL) domain, Convolutional Neural Networks <ref type="bibr" target="#b25">[Matsugu et al. 2003</ref>] (henceforth named only as CNNs) have presented an outstanding performance for several applications and scenarios (e.g., visual computing <ref type="bibr" target="#b24">[Liu et al. 2021;</ref><ref type="bibr" target="#b15">Guérin et al. 2021]</ref>, natural language processing <ref type="bibr" target="#b37">[Ren et al. 2020;</ref><ref type="bibr" target="#b0">Agrawal and Urolagin 2020]</ref>, finances <ref type="bibr">[Özbayoglu et al. 2020]</ref>, bioinformatics <ref type="bibr" target="#b6">[de Oliveira et al. 2020]</ref>, game development <ref type="bibr" target="#b14">[Goulart et al. 2019]</ref>, engineering <ref type="bibr" target="#b47">[Yang et al. 2021]</ref>, etc.). This performance is due to new architectures, new training procedures of neural networks, and advances in hardware, especially the general-purpose graphics processing units (GPG-PUs) <ref type="bibr" target="#b45">[Valero 2016</ref>], which demonstrated to be a suitable environment for training CNNs. To develop applications based on CNNs, users commonly gather data, annotate these data, train a model with the data, and evaluate this model in a cycle as represented in Fig. <ref type="figure">1</ref>. Depending on the results of the evaluation, the model is retrained (revised, by varying hyperparameters), or the data has to be revised. To move from one cycle step to another, the user evaluates intermediate data and log data generated by each step. At a large scale, after many cycles, it becomes difficult to browse and analyze data from all the steps with different file formats and representations, as shown in Fig. <ref type="figure">1a</ref>. To analyze data, programs must be written to relate these different files. Recent approaches use a provenance database to integrate data from these cycle steps and improve data analyses and decisions (Fig. <ref type="figure">1b</ref>).</p><p>In fact, according to <ref type="bibr" target="#b10">[Fekry et al. 2020</ref>], this cycle is being adopted as a new development paradigm in many domains.</p><p>The performance of a CNN depends on the input dataset and the choice of a set of hyperparameters (a type of parameter that one can use to influence the learning process). However, the user cannot adjust the input dataset in most cases, so the only "control" the user has to improve the quality of the trained model is the proper choice of hyperparameter values. Several hyperparameters need to be set when a user develops a CNN-based application. Examples of commonly effective hyperparameters include the learning rate (α -scales the magnitude of the weights update), batch size (defines the number of subsamples given to the network at each iteration), number of epochs (defines the number of times the training data is provided to the network during the training process), momentum (βdefines the direction of the next step considering previous knowledge), and dropout rate (defines the number of units that are temporarily removed during a training step to avoid overfitting). Choosing the proper values for these hyperparameters is a top priority, yet far from trivial. The reason CNNs are difficult to configure is that several hyperparameters need to be set with a large range of possible values for each one of them and CNNs are particularly sensitive to hyperparameter settings <ref type="bibr" target="#b16">[Hoos and Leyton-Brown 2014]</ref>. Besides, several models require a non-negligible amount of time to be trained and a poor hyperparameter configuration can lead to undesired results and time wasted.</p><p>In the worst case, when the user does not have any preference over a subset of the hyperparameters, the model should be trained for a large combination of hyperparameter values. Each combination must be registered for future analysis. Experiment tracking tools such as Weights &amp; Biases <ref type="bibr" target="#b3">[Biewald 2020</ref>] may help in recording and visualizing the experiment tries. However, exploring and choosing the hyperparameters' values remain a non-trivial exploration process, due to the large search space. There are a plethora of available methods to set hyperparameters for a CNN, e.g., manual search, Grid Search, Cloud Auto ML 1 , and Bayesian Optimization <ref type="bibr" target="#b1">[Badan and Sekanina 2019]</ref>. Such methods are implemented in well-known frameworks such as scikit-learn 2 . Let us take as an example the Grid Search method <ref type="bibr" target="#b23">[Liashchynskyi and Liashchynskyi 2019]</ref>. Grid Search generates the model and evaluates it for a specific combination of hyperparameters. It repeats the process for a pre-defined set of hyperparameter combinations. In the end, the combination of hyperparameters values that • 3 produced the best score (e.g., accuracy) is chosen.</p><p>These methods represent a step forward, but they lack support for user participation <ref type="bibr" target="#b5">[Chevalier-Boisvert et al. 2019;</ref><ref type="bibr" target="#b46">Wang et al. 2019]</ref>. Often, based on data analysis, users gain insights that allow reducing the hyperparameter search space. Solutions that present some user support to evaluate which hyperparameter values led to which results adopt a proprietary data representation <ref type="bibr" target="#b39">[Schelter et al. 2017;</ref><ref type="bibr" target="#b44">Tsay et al. 2018</ref>]. These proprietary representations of the data derivation path make exploratory data analysis difficult. Thus, the results of the training process on different tools require additional analysis and implementation to be further compared, as there is no uniform way to represent the choice of hyperparameters that leads to the best model. Furthermore, it is not trivial to verify if two models have been trained with the same hyperparameter values.</p><p>One possible alternative to this problem is to represent the hyperparameters' configuration derivations using provenance data. In this case, one can use recommendations such as W3C PROV <ref type="bibr" target="#b29">[Moreau and Groth 2013]</ref>, which represent provenance data and aim at fostering interoperability. Representing the metadata associated with the training process of a CNN in a provenance repository simplifies associating hyperparameter values and input data with the trained models. Additionally, by leveraging the provenance data, one can improve both hyperparameter tuning and online data analysis. Also, the evaluation of the various hyperparameters requires the relationship between several types of data (e.g., domain-specific data, metrics such as accuracy, metadata from the computational environment used for CNN training <ref type="bibr" target="#b48">[Zaharia et al. 2018]</ref>). Correlating hyperparameters and the results is an analysisintensive task, that requires user expertise and data from several training runs. In addition, queries for evaluating hyperparameter values consumed during the training of a CNN resemble the typical provenance queries already found in different systems <ref type="bibr" target="#b41">[Silva et al. 2018;</ref><ref type="bibr" target="#b40">Silva et al. 2020;</ref><ref type="bibr" target="#b13">Godoy et al. 2020]</ref>.</p><p>In a previous work <ref type="bibr" target="#b36">[Pina et al. 2019]</ref>, we presented CNNProv, an approach for automatic capture of provenance from CNN applications, based on DfAnalyzer <ref type="bibr" target="#b40">[Silva et al. 2020]</ref>. We specialized CNNProv into Keras-Prov <ref type="bibr" target="#b35">[Pina et al. 2020</ref>] using the Python DL API Keras<ref type="foot" target="#foot_0">3</ref> . Keras-Prov avoids code annotations from the user, as required in CNNProv, and automatically captures typical hyperparameter values from Keras. Like CNNProv, Keras-Prov keeps compliance to W3C PROV, which eases interoperability. Keras-Prov tracks data transformations and datasets related to CNN training hyperparameters and metrics automatically. Provenance data and user adaptations in configurations are stored in a database that allows for submitting queries during the training. Although Keras-Prov represents a step forward, it lacks relevant features for supporting online provenance analysis.</p><p>In this paper, we extend Keras-Prov into Keras-Prov++, to improve online analysis and finetuning. Keras-Prov++ provides an analytical dashboard to support users in examining the CNN training online. This visual representation complements database queries by showing the behavior of hyperparameters with the corresponding metrics evolving with epochs. It helps to identify the relationship between a specifically chosen hyperparameter with accuracy and loss, improving the online fine-tuning. This paper goes deeper in presenting Keras-Prov <ref type="bibr" target="#b35">[Pina et al. 2020]</ref>, including a broader discussion on related work with background details, and introducing Keras-Prov++ with its added visual support for hyperparameter analysis.</p><p>In addition to using the well-known CNN AlexNet <ref type="bibr" target="#b20">[Krizhevsky et al. 2017]</ref> case study with Keras-Prov++, this paper explores a dense CNN surrogate model in a real scientific application called DenseED <ref type="bibr" target="#b12">[Freitas et al. 2021</ref>]. These case studies show how Keras-Prov++ provides new insights into hyperparameter tuning and evidences the relationship of hyperparameter values with metrics such as accuracy. The experiments show how user adaptivity can reduce the number of epochs previously set and consequently the overall training time. This work contributes to a W3C PROV provenance-based architecture that can benefit from several visual data analysis tools. The architecture is designed to work with the DL framework chosen by the user, in this case, Keras.</p><p>The remainder of this paper is structured as follows. Section 2 presents background on hyperparameters, Section 3 introduces the proposed approach. Section 4 details the case studies, and the experimental evaluation. Section 5 discusses related work. Finally, Section 6 concludes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">THE ROLE OF HYPERPARAMETERS IN DEEP LEARNING</head><p>The popularity of DL and CNNs has increased at a fast pace over the last ten years <ref type="bibr" target="#b22">[Li et al. 2020</ref>].</p><p>According to <ref type="bibr" target="#b21">[LeCun et al. 2015]</ref>, DL methods aim at learning representations from input raw data and using such representations to automatically learn how to solve a task. The representations are obtained through the composition of non-linear functions that transform the representation at each level, starting with the raw input at the lower level to a higher and slightly more abstract level.</p><p>Hyperparameters are key components in DL since choosing their values properly may strongly influence the quality of the outcome of an algorithm. According to <ref type="bibr" target="#b2">[Bengio 2012</ref>], a hyperparameter is defined as "a variable to be defined before the actual application of a given learning algorithm to the data, being that variable not directly selected by the learning algorithm itself ". Tuning hyperparameters in DL is particularly challenging since the usual trial-and-error process of experimenting on them is much more expensive in complex and highly dependent models such as deep CNNs <ref type="bibr" target="#b16">[Hoos and Leyton-Brown 2014]</ref>.</p><p>Algorithms for training deep models include a plethora of hyperparameters, such as the number of epochs, optimizers, momentum, learning rate, etc. Following, we briefly explain some of the most common hyperparameters. The number of epochs hyperparameter defines how many times an entire dataset is passed forward and backward through the network. When this hyperparameter value increases, the number of times the weights in a network are adjusted also increases and the tendency is that the curve goes from underfitting to optimal. One has to be careful to not pass through the optimal point and reach overfitting when the number of epochs is larger than required. The batch size is the number of examples that are presented to the network during the training. Properly defining the batch size may be a hardware requirement due to the size of the GPU's memory but can also lead to better generalization. The learning rate hyperparameter controls how quickly or slowly the neural network weights are adjusted. The optimizer hyperparameter defines the algorithm or method used to adjust the weights of the network, for example, by computing adaptive learning rates for each parameter <ref type="bibr" target="#b17">[Ioffe and Szegedy 2015]</ref>. The output of a loss function works as a guide to optimizers for them to update the parameters of the model. There are several available optimizers. Following, we provide some details about the optimizers available in Keras:</p><p>-SGD (Stochastic Gradient Descent) is an extension of the gradient descent algorithm that is less expensive computationally. Thus, while gradient descent needs to load an entire dataset of n-points to compute the derivative, SGD computes derivatives for each point; -RMSprop (Root Mean Square Propagation) maintains a moving (discounted) average of the square of gradients and divides the gradient by the root of this average. This optimizer uses plain momentum; -Adagrad adaptively scales the learning rate for each parameter during the training, those updates are relative to the frequency of updates a parameter receives <ref type="bibr" target="#b9">[Duchi et al. 2011</ref>]; -Adadelta is an extension of Adagrad that allows for a per-dimension learning rate method for SGD. Its main advantage is that it does not require setting a default learning rate method <ref type="bibr" target="#b49">[Zeiler 2012</ref>]; -Adam <ref type="bibr" target="#b18">[Kingma and Ba 2014]</ref> is a stochastic gradient descent method that is based on the adaptive estimation of first-order and second-order moments. Adam requires less memory and is well suited for problems that are large in terms of data and/or parameters;</p><p>• 5</p><p>-Adamax is a variant of Adam, based on infinity norm <ref type="bibr" target="#b18">[Kingma and Ba 2014]</ref>; -NAdam is the optimizer Adam with Nesterov momentum <ref type="bibr" target="#b8">[Dozat 2016</ref>]. NAG (Nesterov adaptive gradient) is superior to traditional momentum. So, NAdam is the NAG incorporated to Adam.</p><p>A hyperparameter that is often used with some optimizers is Momentum. Momentum is an optimization technique that instead of using only the gradient of the current step to guide the search, like in SGD, it also accumulates the gradient of the past steps to determine the direction to go <ref type="bibr" target="#b38">[Ruder 2016</ref>], i.e., it replaces the gradient with a momentum which is the aggregation of gradients. This hyperparameter aims at speeding up learning and avoiding getting stuck in local minima.In addition to the common hyperparameters, the user might set custom hyperparameters according to the network architecture like the number of layers of a specific block, or custom metrics such as loss function with multiple coefficients.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">AN INTRODUCTION TO KERAS-PROV++</head><p>This section presents Keras-Prov++ showing its architecture in section 3.1, and how to use it in section 3.2.</p><p>Keras is an API for the well-known and popular TensorFlow<ref type="foot" target="#foot_1">4</ref> library. Keras-Prov++ is a library with a Python interface to provide provenance data to Keras DL applications. The idea behind Keras-Prov++ architecture is to maintain the original Keras core (with its multiple layers) while registering online the values of hyperparameters and their relationships as provenance data. Keras-Prov++ components' architecture acts as provenance plugins to the software that executes the DL, in this case, Keras API. In this approach, the user can continue using Keras without having to run the neural network training under a portal or tool. Keras-Prov++ has a lightweight provenance capture and storage with a negligible computational overhead to the DL execution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Keras-Prov++ architecture</head><p>The architecture of Keras-Prov++ is shown in Fig. <ref type="figure" target="#fig_1">2</ref> with its three main layers: (i) Training Layer, (ii) Data Layer, and (iii) Analysis Layer.</p><p>The Training Layer is where Keras core executes and interacts with libraries such as TensorFlow. It is important to note that the original Keras functionalities are not modified. Keras-Prov encapsulates Keras Core so that a component named Provenance Extractor can have access to the hyperparameter values to capture them. To provide extensibility, a class called Provenance is added to Keras, containing methods that capture data transformations of the DL application. Keras has a class called Model and in this class, a method to capture provenance data was added. This method expects a variable named dataflow_tag to identify the dataflow (data transformations), and the list of hyperparameters to be captured. If there is an adaptation of the hyperparameters during the training (e.g., update in the learning rate), Keras-Prov uses Keras methods such as LearningRateScheduler and ReduceLROnPlateau to register the tuning in the provenance database. Then, as Keras executes, Keras-Prov automatically identifies the data transformations in the DL application, the hyperparameters of interest, and the values used in each training to be captured. Provenance Extractor interacts with Keras methods to capture these data and to send them asynchronously to the Data Layer to manage provenance data. Data Layer is prepared to be executed in a different computer node to manage provenance data without competing with the DL execution resources. The Analysis Layer of Keras-Prov aims at producing provenance graphs in different representations.</p><p>The Data Layer is the main component of Keras-Prov. It receives provenance data and uses a DBMS to store it. We chose to use MonetDB, a columnar DBMS, to manage provenance data due to its efficiency in analytical queries and data ingestion. MonetDB has also scientific data support with plugins for the R library. By choosing a relational DBMS like MonetDB, visualization tools and Python plugins are available to be incorporated by the Analysis Layer. The database schema follows DfAnalyzer's PROV-Df representation <ref type="bibr" target="#b42">[Silva et al. 2016]</ref>, with extensions to model CNN metadata and hyperparameters <ref type="bibr" target="#b34">[Pina et al. 2021</ref>]. This relational database schema corresponds to the W3C PROV prospective provenance representation, as shown in Fig. <ref type="figure">3</ref>. Provenance data received at the Data Layer is structured according to this database schema and ingested in the provenance database to provide the corresponding retrospective provenance. Fig. <ref type="figure">3</ref> follows W3C PROV-N notation that is based on three core concepts of Entity, Activity, and Agent. An entity is a physical, digital, or conceptual representation of a thing. In the context of this paper, an example of an entity may be data in a file, entities are represented as yellow ellipses. Activities are actions (processes) that occur in a specific period and act upon entities. The training process of a CNN may be considered an activity, an activity is represented by a blue rectangle. An agent is a person or software agent that is responsible for performing activities, own an entity, etc. Agents are represented as orange pentagons. This representation follows a graphic notation familiar to W3C PROV users, which facilitates comparison and interoperability.</p><p>In There are several advantages in choosing W3C PROV. The provenance data captured online can be exported using a W3C PROV-compliant representation, e.g., JSON, as presented in Fig. <ref type="figure" target="#fig_4">4</ref>. Keras- Prov++ generates the representation in Fig. <ref type="figure">3</ref> using Prov Python<ref type="foot" target="#foot_2">5</ref> and Graphviz<ref type="foot" target="#foot_3">6</ref> , and still, validates the W3C PROV compliance using ProvValidator<ref type="foot" target="#foot_4">7</ref> .</p><p>To empower the Analysis Layer, Keras-Prov++ has a dashboard for analyzing captured provenance data. Four new components were added to Keras-Prov to become the Keras-Prov++ architecture: (i) Provenance Exporter, (ii) pymonet, (iii) ElasticSearch, and (iv) Kibana, as presented in Fig. <ref type="figure" target="#fig_1">2</ref>. The Provenance Exporter aims at extracting provenance data from the provenance database and sends it to the pymonet<ref type="foot" target="#foot_5">8</ref> . Pymonet ingests data into ElasticSearch periodically during execution. ElasticSearch is a search engine built on top of the Lucene library, it can search several kinds of documents and provides scalable search. Due to the volume of captured provenance data, ElasticSearch is a natural choice for searching data. Then, ElasticSearch is accessed by Kibana to produce the Keras-Prov++ Dashboard. Kibana aims at providing rich visualization capabilities on top of the content indexed on ElasticSearch (i.e., provenance data). Data analysis is not real-time, since ElasticSearch only updates its information within a certain interval, but is fast enough for runtime tuning. Finally, the Provenance Viewer component generates a visual representation of the provenance graph and accesses the provenance database, to simplify the user analysis.</p><p>Although similar data is stored in Keras logs, this comparative and visual analysis would become much more labor-intensive requiring preprocessing of logs for ElasticSearch ingestion. Adopting W3C PROV representation, documents like PROV-N can be generated and post-processed by libraries such as Prov Python libraries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Keras-Prov++ in action</head><p>To use Keras-Prov++ the user needs to download, install and start the library from https://github. com/hpcdb/keras-prov, DfAnalyzer, MonetDB, ElasticSearch, and Kibana. Once all those compo-Continues.... nents are installed, the user can execute the DL application with Keras. One only has to change the code by adding the provenance method, as shown in Fig. <ref type="figure" target="#fig_6">5</ref>, to set the hyperparameters to be captured. When this code is not added, the execution proceeds with Keras and without provenance capture. In the instantiation of the method provenance, the attributes dataflow_tag and hyps are mandatory. If all the attributes inside the variable hyps are set as False, Keras-Prov++ will register the CNN architecture and the adaptation values if this variable is set as True. To access the captured data the user interacts with the Provenance Viewer or, the Keras-Prov++ dashboard, both through a browser.</p><p>Behind this hyperparameters setting, Keras-Prov uses the predefined provenance representation, so that provenance data is captured and stored accordingly. Relationships between hyperparameters (e.g., optimizer and learning rate) can be queried as they are ingested in the database. In case the user needs to analyze additional metadata or domain data, one can instrument the code for this purpose. For more information about the instrumentation in Keras-Prov please visit https: //github.com/hpcdb/keras-prov.</p><p>The user can access the Keras-Prov++ database and make analyses through the Keras-Prov++ Dashboard and the Provenance Viewer, during and after the training execution. The Keras-Prov++ Dashboard is customizable and the user can set which available data to use to generate the charts and also a time interval for ElasticSearch to refresh its information. In addition to monitoring with Kibana, the Provenance Viewer allows the user to query the specifications of dataflows that have already been registered in the database, without having to write SQL. For example, the user can obtain directly the execution time of each epoch, learning rate, with the current accuracy for each epoch, the adaptations applied to the training configuration, and at which point they happened. The Provenance Viewer component uses a graphical interface that presents tables and views (predefined joined tables) as datasets with attributes for the users to define filters and aggregates. It is necessary  to specify the datasets that will be used in the analysis, the attributes for the projection clause, and the attributes for the selection clause. Then, this component uses these arguments to generate the corresponding SQL query, which is sent to the Data Layer to be executed by MonetDB on the provenance database. In sections 4.2 and 4.3 we show examples of Keras-Prov++ in action.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">EVALUATING HYPERPARAMETERS USING KERAS-PROV++</head><p>This section presents KerasProv++ in action with experiments that show the potential of provenance data in hyperparameter analyses in two DL applications. It is worth mentioning that these analyses were performed online with the user submitting queries to the Keras-Prov++ Dashboard and the Provenance Viewer. During the execution, the provenance database is ingested with provenance data from the training, test, and validation phases, as they evolve.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experiment Setup</head><p>Our experiments were performed on the Lobo Carneiro computer cluster, also known as LoboC, from the High-Performance Computing Center (NACAD) at COPPE/UFRJ. LoboC is an SGI ICE-X Linux cluster with 504 Intel Xeon E5-2670v3 (Haswell) CPUs, totaling 6,048 processors. The processors feature Hyper-Threading (HT) technology, offering 48 processing threads per node, with 64GB of RAM. Compute nodes are interconnected with InfiniBand FDR-56 Gbs (Hypercube) technology. The cluster runs under a shared disk architecture, with an Intel Luster parallel file system with 500TB storage capacity.</p><p>The experiments are done with MonetDB, instantiated in a dedicated computational node to manage the provenance database. Keras-Prov++ receives HTTP requests with data to be stored and then establishes a connection with MonetDB through the JDBC driver to ingest data. The first case study, Alexnet, is executed with two nodes and the second case study, DenseED, used four nodes at LoboC.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Case Study: Alexnet</head><p>AlexNet <ref type="bibr" target="#b19">[Krizhevsky et al. 2012</ref>] is a CNN for image classification, with a focus on high-resolution images. AlexNet classifies images into more than 1,000 categories. The network has an image input size of 227 X 227. The architecture of AlexNet consists of eight layers: five convolutional layers and three fully connected layers. In addition, as an activation function, AlexNet uses Rectified Linear Units (ReLU) instead of the Hyperbolic tangent (tanh), which was standard at the time, and this reduced the training time. In its seminal paper is discussed that in the lowest layers of the network, the model learns feature extractors that resembled traditional image processing filters. The layers in the middle can represent larger structures such as eyes or noses and the higher layers can represent entire objects like flowers, people, airplanes, etc. The final hidden layers learn a compact summarized representation of the image content in a way to differ diverse categories. Due to this, AlexNet won the 2012 annual challenge of ImageNet, which is a challenge focused on creating methods and learning architectures for the classification of the ImageNet database <ref type="bibr" target="#b7">[Deng et al. 2009</ref>]. Thus, AlexNet is widely recognized as the architecture that popularized CNNs for computer vision.</p><p>In the experiments presented in this section, we trained Alexnet using the Oxford flowers <ref type="bibr" target="#b30">[Nilsback and Zisserman 2006]</ref> dataset. This dataset contains images of flowers belonging to 17 different categories. The images are acquired by searching the Web. There are 80 images available for each category. All executions were performed with 100 epochs, the optimizers used were SGD, RMSprop, Adam, Adadelta, Adagrad, Adamax, and NAdam, and to evaluate and score each experiment we use accuracy and loss as reference metrics. All other hyperparameters were set with default values.</p><p>Several experiments have been conducted with the Oxford dataset to analyze the behavior of different optimizers and learning rates. To analyze each combination, we split the training dataset into 80%/20% to train and validate the dataset. In all experiments, we used the default settings of each optimizer and the size of all images was constant. All data presented in this subsection was captured by Keras-Prov++ and shown with its dashboard.</p><p>Figures <ref type="figure" target="#fig_7">6a</ref> and<ref type="figure" target="#fig_7">6b</ref> show the training accuracy of different executions with Alexnet. In the first execution (Fig. <ref type="figure" target="#fig_7">6a</ref>) the user chooses Adam, Adamax, and NAdam optimizers with a learning rate of 0.1. In Fig. <ref type="figure" target="#fig_7">6a</ref>, around epoch 30, the accuracy values of the network when using ADAM optimizer are much lower than expected, which suggests further investigation. The user queries the current learning rate value and decides to drop it from 0.1 to 0.05. After this action, the user keeps following the execution through the dashboard. After epoch 40, a comparison between the running optimizers suggests that NAdam's execution is not worth continuing. At epoch 50, Fig. <ref type="figure" target="#fig_7">6a</ref> shows that accuracy is still low for Adam and Adamax, so a new learning rate update is applied by the user, who decides to drop it again from 0.05 to 0.025, without any improvement. Since, by epoch 70, Fig. <ref type="figure" target="#fig_7">6a</ref> does not suggest much difference between the training accuracy of Adamax and Adam, the training of the Adamax optimizer is also interrupted by the user. Those actions have an impact on energy and resource consumption. When the execution of Adam ends, the user decides to try again with other optimizers with a learning rate of 0.001, using StepDecay as a learning rate adaptation function, this case is shown in Fig. <ref type="figure" target="#fig_7">6b</ref>. StepDecay is a function that drops the learning rate value by a factor every n epochs, in this case, the factor is defined as 0.5 and n = 10. The execution of Alexnet with a smaller learning rate and different optimizers are shown in Fig. <ref type="figure" target="#fig_7">6b</ref>. Through the charts shown in the dashboard, the user might have a clear perspective of each run. Even though a visual tool is useful when there is a need to analyze a significant number of values, query analysis can be more effective to investigate specific values. Using the Provenance Viewer, the user can submit queries such as Q1: "What is the loss value of epoch 30 of each optimizer in Alexnet with learning rate 0.001?", Q2: "List the layers of the model.", Q3: "What are the average time and loss for training each epoch?". The result of these queries is presented in Tables I, II, III and can be retrieved through the Provenance Viewer. <ref type="table" target="#tab_0">I</ref>) is a query that helps decision-making, such as changing a learning rate or dropout values. Q2 (Table <ref type="table" target="#tab_0">II</ref>) shows the activation and dropout layers of AlexNet. This is important because, in some trials, the user may change the dropout value or the activation. To be able to present these differences between trials, Keras-Prov++ captures and stores the name that identifies the layer (e.g., activation_1, dropout_1), the type of the layer (activation or dropout), and the value of this layer (e.g., the value for activation is relu, the value for dropout is 0.4). Q3 (Table <ref type="table" target="#tab_0">III</ref>) can be used to detect execution anomalies, such as a NN training that takes longer than expected with a specific optimizer</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Q1 (Table</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Case Study: DenseED</head><p>Our second case study uses the neural network DenseED and its datasets, as proposed by <ref type="bibr" target="#b12">[Freitas et al. 2021]</ref>. DenseED uses a Physics-guided CNN as a surrogate model to enable the quantification of uncertainties <ref type="bibr" target="#b50">[Zhu and Zabaras 2018]</ref>. The network architecture is a dense CNN as shown in Fig. <ref type="figure" target="#fig_8">7</ref>. DenseED aims to replace the calculation of the Reverse Time Migration (RTM) equations with a trained model. In this way, the RTM calculation that would have to be performed for each probability distribution can be reduced. DenseED's architecture adopts a fully convolutional Bayesian encoder-decoder network. The number of dense layers in DenseED is variable and its evaluation metric is the Mean Squared Error (MSE). This case study requires more data to be analyzed than the Keras-Prov++ default hyperparameter values. Therefore, it required additional modeling and instrumentation to track the model's architecture. The provenance graph representation of the dense blocks and their transformations can be seen in Fig. <ref type="figure">8</ref>. As reported in <ref type="bibr" target="#b12">[Freitas et al. 2021</ref>], the user conducted several hyperparameter fine tunings. Without provenance, it is time consuming to gather the different training executions to compare and register adjustments. In this case study we reexecuted some of these trainings with the provenance support from Keras-Prov++ to show its analytical support. The input dataset of DenseED contains 100,000 velocity fields. From the input dataset 12000 samples were randomly selected for the training and 1000 for the testing set. In the validation of this NN, 50% of the training set was used. This neural network generates a surrogate model for RTM, thus, the expected output is a seismic image. The loss in this model is the Mean Squared Error (MSE) and it is monitored during the training and the testing phases, where the Coefficient of Determination (R 2 ) is the main metric in the test phase. The training explores alternatives for the Growth Rate (k = (16, 24, 32)) and the number of layers in the dense block (l = (4, 6, 8, 9)). All these data are stored in the provenance database.</p><p>In Figures 9a, 10a and 10b, we show the DenseED training results with different k and l values. In the cases explored, after finishing the training of combinations where k = 16, the user observes in Fig. <ref type="figure" target="#fig_9">9a</ref> that at epoch 140 and beyond, there is a tendency that MSE values stay within a range of 0 and 0.001, zooming the chart (Fig. <ref type="figure" target="#fig_9">9b</ref>) and querying the database, this tendency is confirmed. Assuming the same behavior for the combinations of k = 24, he decides to decrease the number of epochs from 200 to 170 epochs. As expected, in Fig. <ref type="figure">10a</ref>, the MSE from epoch 140 stabilizes within  values 0 and 0.001, and then the user trains the k = 32 combinations with 150 epochs. These changes in the number of epochs to be trained decrease the resource consumption and the execution time of combinations of k = 24 and k = 32, with no significant change in MSE. Looking at these results through queries is also possible, but the amount of information presented in a table would take longer to generate insight and decision.</p><p>The user can submit analytical queries to fine-tune DenseED parameters like Q4: "What are the initial learning rate, optimizer and the number of layers when the training for DenseED achieved the highest R 2 ?", Q5: "Retrieve the combinations of k and l that achieved the top three MSE values and their R 2 .", Q6: "What are the top five training MSE values and their elapsed time for each epoch when k = 32 and l = 9?", Q7: "What are the combinations of k and l that consume less time during the training?". Tables IV,V, VI, and VII show query results. Q4 (Table <ref type="table" target="#tab_1">IV</ref>) shows the characteristics of the model that had the best value for R 2 and Q5 (Table <ref type="table" target="#tab_2">V</ref>) might suggest that greater values of l lead to smaller MSE values. The results of Q6 (Table <ref type="table" target="#tab_2">VI</ref>) might be a case where, if more values were required, a graphical representation would be more helpful. Q7 (Table <ref type="table" target="#tab_2">VII</ref>) shows that the training time increases with the number of layers (l), this is important for the user to evaluate the tradeoff between time cost and the value of R 2 . For DenseED, the user considers that R 2 ≥ 0.95 as a satisfactory R 2 . The combination k = 16 and l = 4 was chosen since it already reached the user's criteria and costs less time. Queries that filter and order results to show highest and lowest values, such as Q4-Q7, assist in evaluating extreme values and outliers as the training evolves. Without this data, keeping track of this progress associated with execution data and the hyperparameter set would be costly and error-prone. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Threats to Validity</head><p>The case study of Alexnet aims at using a well-known neural network to show how outcomes may change varying one hyperparameter, the amount o data generated by each attempt, and how an online analysis (through charts and queries) may benefit the life cycle. DenseED case study is a real case and the variations explored were used in the process of designing this model.</p><p>We are yet to explore the heterogeneity of the execution environment, Alexnet and DenseED case studies were executed at Lobo Carneiro Supercomputer only. Studies to analyze the time and storage overhead of the functionalities associated with Keras-Prov++ Dashboard are yet to be performed. The current version of Keras-Prov++ is limited to Tensorflow 2.2. Also, information is limited by what can be provided by Keras, such as how to identify the NN architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">RELATED WORK</head><p>To decide whether the DL training should continue or try different configurations, the user needs to analyze online the hyperparameters most adequate to the training dataset, observing metrics such as accuracy and loss values. Three approaches provide provenance data to support DL hyperparameter analysis. The first is having provenance data as part of the DL system, like Keras logs, which is difficult to analyze. The second is to add a provenance system <ref type="bibr" target="#b26">[McPhillips et al. 2015;</ref><ref type="bibr" target="#b32">Pimentel et al. 2016;</ref><ref type="bibr" target="#b33">Pimentel et al. 2017;</ref><ref type="bibr" target="#b40">Silva et al. 2020;</ref><ref type="bibr" target="#b41">Silva et al. 2018]</ref> to the DL application. However, these generic provenance approaches require that the user defines data representations and functions to explicit and analyze the relationships between hyperparameters and provenance data, for each DL application. The work of Souza et al. <ref type="bibr" target="#b43">[Souza et al. 2021]</ref> supports the whole ML life cycle, including provenance support from several steps before the training. Souza et al. <ref type="bibr" target="#b43">[Souza et al. 2021</ref>] present a rich provenance data representation, W3C PROV compliant, already associated with ML data, but the approach needs the user participation to instrument the DL application with the corresponding provenance capture library calls.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>• 15</head><p>The third approach encompasses provenance-based hyperparameter analysis solutions that are coupled to DL systems. However, related work in this approach, like <ref type="bibr" target="#b27">[Miao et al. 2015;</ref><ref type="bibr" target="#b39">Schelter et al. 2017;</ref><ref type="bibr" target="#b44">Tsay et al. 2018]</ref>, adopt a specific provenance data representation, disregarding open standards and recommendations. Some of these tools require running the DL application under a portal or framework, limiting the choice of DL environment by the user. The approach of Keras-Prov is to provide a provenance library to be plugged into DL libraries to access hyperparameters and support online analysis. In addition, Keras-Prov is W3C PROV compliant. Next, we discuss the closest approaches to Keras-Prov and present a comparative table between them.</p><p>The HParams panel of TensorFlow<ref type="foot" target="#foot_6">9</ref> provides several tools to assist in defining the hyperparameters. The HParams panel allows for the user to list all executions and associate hyperparameters values and their metrics and generate charts (e.g., parallel coordinates that show each execution as a line passing through an axis for each hyperparameter and metric) to evaluate the results. Although HParams panel represents a step forward, it does not allow for online analysis neither can import data from other approaches to perform comparisons (which is possible by using open standards such as PROV).</p><p>Neptune<ref type="foot" target="#foot_7">10</ref> is a lightweight analysis tool for DL experiments. It is similar to the HParams tool. It logs hyperparameters and metrics values from multiple executions and generates charts to compare results. One advantage of Neptune is that it can be integrated with notebooks, which eases the analysis process. It does not allow for online analysis (the data is also provided at the end of the training process, for example) neither can import data from other approaches to perform comparisons. <ref type="bibr" target="#b4">[Chatzimparmpas et al. 2020]</ref> propose VisEvol, a visual analytics tool that aims at supporting interactive analysis of hyperparameters in DL experiments. One of the advantages of VisEvol is that it allows for interventions during the training. Through exploration of the parameter value space, the user can create new models within the tool. VisEvol allows for online analysis, but only if the model is generated within it. If the users choose to use an external DL framework, it is not possible to capture the metadata for analysis. ModelHub <ref type="bibr" target="#b28">[Miao et al. 2017</ref>] is a life cycle management system for DL which aims at storing models in different versions. ModelHub has a proprietary model for representing the training metadata of the neural network, which makes interoperability difficult. Runway <ref type="bibr" target="#b44">[Tsay et al. 2018</ref>] aims at managing DL artifacts, such as models, data, and experiments. Runway allows for the user to track the model and data used during the training process. However, in addition to being a proprietary solution, it is restricted to the Python programming language. <ref type="bibr" target="#b39">[Schelter et al. 2017]</ref> propose an automated tool to extract the metadata from the DL model and present it with interactive visualization to assist the comparison of experiments. However, the approach proposed by <ref type="bibr" target="#b39">[Schelter et al. 2017]</ref> does not use standards, such as W3C PROV, affecting interoperability.</p><p>Table VIII presents a comparison between provenance-based hyperparameter analysis solutions and Keras-Prov. The column Hyperparameter selection refers to whether the tool assists in defining which hyperparameters are to be analyzed. The column Analysis interface refers to how the user queries and monitors DL data, through a graphical interface or a specific language. The column Data storage is related to how the captured data is being stored and represented, which in the case of a DBMS, can ease online querying. The column Provenance refers to whether the tool captures prospective provenance (p-prov) in addition to retrospective provenance (r-prov), and if it follows any known recommendation for provenance data representation. The column Online analysis refers to the possibility of online DL data analysis during the training execution.</p><p>Despite the importance and approaches for provenance support in DL applications, to the best of our knowledge, no solution provides online W3C PROV provenance data analysis using an independent provenance capture component with the DL system. The provenance capture component of Keras-Prov can be compiled to other DL systems, rather than having to execute under a specific platform.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig</head><label></label><figDesc>Fig. 1: (a) Life cycle of a CNN application. (b) Life cycle of a CNN application using a provenance database.</figDesc><graphic coords="3,86.40,110.32,185.20,184.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>Fig. 2: Architecture of Keras-Prov++</figDesc><graphic coords="7,141.30,110.31,329.41,209.86" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Fig. 3 Training, Adaptation and Testing are activities. Training consumes (arrow used ) a series of AttributeValues such as the name of the optimizer, the hyperparameters learning rate, number of epochs, and number of layers in the network, and produces (arrow wasGeneratedBy) a set of metrics, such as the accuracy, the value of the loss function, the elapsed time and the date and time of the end of the execution of each epoch. Adaptation consumes (arrow used ) the data produced by the previous transformation (Training), and hyperparameter values such as new learning rate and produces (arrow wasGeneratedBy) the value of the epoch and the date and time when the adaptation occurred, in addition to identification for the adaptation. Finally, Testing provides data on the evaluation of the model according to the training dataset and outputs the accuracy and loss function values. Entities represent data related to the training, divided into the classes ds_itrainingmodel, ds_otrainingmodel, ds_iadaptation, ds_oadaptation, and ds_otestingmodel, which follow the description of the activities previously presented (Training, Adaptation, and Testing).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>• 7 Fig. 3 :</head><label>73</label><figDesc>Fig. 3: Keras-Prov provenance representation with W3C PROV notation.</figDesc><graphic coords="8,86.40,110.31,439.20,238.14" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 :</head><label>4</label><figDesc>Fig. 4: A fragment of the PROV-compliant JSON file.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>a f l o w _ t a g="k e r a s -a l e x n e t -d f " , a d a p t a t i o n=True , hyps = hyps ) . . .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 5 :</head><label>5</label><figDesc>Fig. 5: A Fragment of Keras-Prov Code</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 6 :</head><label>6</label><figDesc>Fig. 6: (a) Accuracy of Alexnet with initial learning rate of 0.1 and optimizers Adam, Adamax and NAdam and (b) Accuracy of Alexnet with initial learning rate of 0.001 and optimizers available in Keras, and adaptation with StepDecay function.</figDesc><graphic coords="11,86.40,110.31,207.00,121.20" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 7 :</head><label>7</label><figDesc>Fig. 7: DenseED: Deep Convolutional Encoder-Decoder network architecture from<ref type="bibr" target="#b11">[Freitas et al. 2020]</ref> </figDesc><graphic coords="13,178.63,110.31,254.74,116.26" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 9 :</head><label>9</label><figDesc>Fig. 9: (a) Graphical view of the training results of DenseED with k = 16 and l = (4, 6, 8, 9) (b) Zoomed view of the training results for DenseED with k = 16 and l = (4, 6, 8, 9) from epoch 140 to epoch 200.</figDesc><graphic coords="14,86.40,110.31,212.10,125.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>Fig. 10: (a) Graphical view of the training results of DenseED with k = 24 and l = (4, 6, 8, 9) (b) Graphical view of the training results of DenseED with k = 32 and l = (4, 6, 8, 9).</figDesc><graphic coords="14,86.40,289.86,213.30,126.30" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table I :</head><label>I</label><figDesc>Q1: What is the loss value of epoch 30 of each optimizer in Alexnet with learning rate 0.001?</figDesc><table><row><cell></cell><cell></cell><cell>Optimizer</cell><cell>Loss</cell><cell>Validation loss</cell></row><row><cell></cell><cell></cell><cell>Adadelta</cell><cell>3.3</cell><cell>2.75</cell></row><row><cell></cell><cell></cell><cell>Adagrad</cell><cell>0.29</cell><cell>2.64</cell></row><row><cell></cell><cell></cell><cell>Adam</cell><cell>0.09</cell><cell>2.07</cell></row><row><cell></cell><cell></cell><cell>Adamax</cell><cell>0.14</cell><cell>2.16</cell></row><row><cell></cell><cell></cell><cell>NAdam</cell><cell>0.07</cell><cell>2.08</cell></row><row><cell></cell><cell></cell><cell>RMSprop</cell><cell>0.11</cell><cell>1.76</cell></row><row><cell></cell><cell></cell><cell>SGD</cell><cell>2.3</cell><cell>2.27</cell></row><row><cell cols="2">Table II: Q2: List the layers of the</cell><cell></cell><cell></cell></row><row><cell>model.</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Name activation_1 activation_2 activation_3 activation_4 activation_5 activation_6 dropout_1 activation_7 dropout_2 activation_8</cell><cell>Attribute type activation activation activation activation activation activation dropout activation dropout activation</cell><cell>Value relu relu relu relu relu relu 0.5 relu 0.5 relu</cell><cell cols="2">Table III: Q3: What are the average time and average loss for the NN's training with each optimizer? Average time Average Loss Optimizer 23.46 2.68 Adadelta 22.48 1.41 Adagrad 23.27 2.24 Adam 23.11 2.84 Adamax 23.55 2.07 NAdam 22.83 2.15 RMSprop 22.41 1.95 SGD</cell></row><row><cell>dropout_3</cell><cell>dropout</cell><cell>0.5</cell><cell></cell></row><row><cell>activation_9</cell><cell>activation</cell><cell>softmax</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table IV :</head><label>IV</label><figDesc>Q4: What are the initial learning rate, optimizer, and the number of layers when the training for the combination of k and l that achieved the highest R 2 ?</figDesc><table><row><cell>Initial Learning Rate</cell><cell>Optimizer Name</cell><cell>Number of Layers</cell><cell>k</cell><cell>l</cell><cell>R 2</cell></row><row><cell>0.01</cell><cell>Adam</cell><cell>118</cell><cell>32</cell><cell>9</cell><cell>0.9979</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table V :</head><label>V</label><figDesc>Q5: Retrieve the combinations of k and l that achieved the top three MSE values and their R 2</figDesc><table><row><cell>MSE</cell><cell>R 2</cell><cell>k</cell><cell>l</cell></row><row><cell>0.00093</cell><cell>0.9979</cell><cell>32</cell><cell>9</cell></row><row><cell>0.00103</cell><cell>0.9976</cell><cell>16</cell><cell>9</cell></row><row><cell>0.00111</cell><cell>0.9975</cell><cell>24</cell><cell>8</cell></row><row><cell></cell><cell></cell><cell cols="3">Table VII: Q7: What are the combinations of k and l</cell></row><row><cell></cell><cell></cell><cell cols="3">that consume less time during the training?</cell></row><row><cell>Table VI: Q6: What are the top five train-ing MSE values and their elapsed time for each epoch when k = 32 and l = 9 ? Epoch Elapsed time MSE 144 41.7 0.000589 138 41.93 0.000609 142 41.71 0.000612 141 41.74 0.000617 145 41.8 0.000619</cell><cell></cell><cell></cell><cell>Average elapsed time 7.3 9.2 11 13.2 17.1 20.3 21.5 25 28.1</cell><cell>k 16 24 32 16 24 16 32 16 24</cell><cell>l 4 4 4 6 6 8 6 9 8</cell></row><row><cell></cell><cell></cell><cell></cell><cell>34.1</cell><cell>24</cell><cell>9</cell></row><row><cell></cell><cell></cell><cell></cell><cell>35.6</cell><cell>32</cell><cell>8</cell></row><row><cell></cell><cell></cell><cell></cell><cell>43.8</cell><cell>32</cell><cell>9</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_0"><p>https://keras.io/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_1"><p>www.tensorflow.org   </p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_2"><p>https://prov.readthedocs.io/en/latest/prov.html</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_3"><p>http://www.graphviz.org/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_4"><p>https://openprovenance.org/services/view/validator</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_5"><p>https://pypi.org/project/pymonet/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_6"><p>https://www.tensorflow.org/tensorboard/hyperparameter_tuning_with_hparams</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10" xml:id="foot_7"><p>https://neptune.ai/</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>ACKNOWLEDGEMENTS</head><p>This work was funded by <rs type="funder">CNPq</rs>, <rs type="funder">FAPERJ</rs>, <rs type="funder">Inria (HPDaSc associated team)</rs> and <rs type="funder">Coordenação de Aperfeiçoamento de Pessoal de Nível Superior -Brasil (CAPES) -Finance</rs> <rs type="grantNumber">Code 001</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_7Vq2Vaf">
					<idno type="grant-number">Code 001</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Keras-Prov, through Keras-Prov++, presents visual graphic support to monitoring training data in addition to a query interface that generates SQL automatically for online analysis based on a W3C PROV compliant database.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">CONCLUSION</head><p>In this paper, we presented Keras-Prov and its extension, Keras-Prov++, to improve online hyperparameter fine-tuning based on provenance analysis. Keras-Prov aims at reducing the effort to adapt and instrument the DL application code to capture provenance data during the training of CNNs. Keras-Prov++ builds on Keras-Prov using powerful online analytical features. By automatically capturing provenance data, it is possible to analyze the chosen hyperparameter values related to the training stages and adjust them to achieve results with more quality. Case studies have been conducted with the AlexNet CNN and the real CNN application DenseED surrogate model. We showed experiments where the analysis of hyperparameters during the training of CNNs has led to fine-tunings that improved the overall training. Our experiments showed how using the Keras-Prov++ approach for online provenance query analysis and monitoring can support decision-making.</p><p>As future work, we intend to use GPUs in the evaluations of Keras-Prov. We also plan to extend the solution to better assist the training of Physics Guided Neural Networks, where loss function specification and analysis are much different from typical CNNs. Also, we look forward to making this solution portable (even for HPC environments) using a hierarchy of containers to improve provenance management deployment.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">2-way arabic sign language translator using CNNLSTM architecture and NLP</title>
		<author>
			<persName><forename type="first">T</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Urolagin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BDET 2020: 2nd International Conference on Big Data Engineering and Technology</title>
		<meeting><address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2020">January 3-5, 2020. 2020</date>
			<biblScope unit="page" from="96" to="101" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Optimizing convolutional neural networks for embedded systems by means of neuroevolution</title>
		<author>
			<persName><forename type="first">F</forename><surname>Badan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Sekanina</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPNC</title>
		<imprint>
			<biblScope unit="volume">11934</biblScope>
			<biblScope unit="page" from="109" to="121" />
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Practical recommendations for gradient-based training of deep architectures</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural networks: Tricks of the trade</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="437" to="478" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Experiment tracking with weights and biases</title>
		<author>
			<persName><forename type="first">L</forename><surname>Biewald</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Software available from wandb</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Visual analytics to support hyperparameter search through evolutionary optimization</title>
		<author>
			<persName><forename type="first">A</forename><surname>Chatzimparmpas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Martins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kucher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kerren</surname></persName>
		</author>
		<author>
			<persName><surname>Visevol</surname></persName>
		</author>
		<idno>CoRR vol. abs/2012.01205</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">First steps towards grounded language learning with a human in the loop</title>
		<author>
			<persName><forename type="first">M</forename><surname>Chevalier-Boisvert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lahlou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Willems</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Saharia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">H</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><surname>Babyai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">COVID-19 x-ray image diagnostic with deep neural networks</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">B</forename><surname>De Oliveira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Padilha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Dorte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Cereda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Miyazaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lopes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Dias</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Bioinformatics and Computational Biology -13th Brazilian Symposium on Bioinformatics</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Proceedings</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">W</forename><forename type="middle">M C</forename><surname>Setubal</surname></persName>
		</editor>
		<editor>
			<persName><surname>Silva</surname></persName>
		</editor>
		<meeting><address><addrLine>BSB; São Paulo, Brazil</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020-11-23">2020. November 23-27, 2020. 2020</date>
			<biblScope unit="volume">12558</biblScope>
			<biblScope unit="page" from="57" to="68" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009">2009. 2009</date>
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Incorporating nesterov momentum into adam</title>
		<author>
			<persName><forename type="first">T</forename><surname>Dozat</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Adaptive subgradient methods for online learning and stochastic optimization</title>
		<author>
			<persName><forename type="first">J</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">7</biblScope>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">To tune or not to tune? in search of optimal configurations for data analytics</title>
		<author>
			<persName><forename type="first">A</forename><surname>Fekry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Carata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Pasquier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rice</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hopper</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="2494" to="2504" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">An encoder-decoder deep surrogate for reverse time migration in seismic imaging under uncertainty</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Freitas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>Barbosa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">M</forename><surname>Guerra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Coutinho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">A</forename><surname>Rochinha</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.09550</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">An encoder-decoder deep surrogate for reverse time migration in seismic imaging under uncertainty</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Freitas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>Barbosa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">M</forename><surname>Guerra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Coutinho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">A</forename><surname>Rochinha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Geosciences</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1229" to="1250" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">ADIOS 2: The adaptable input output system. A framework for high-performance data management</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">F</forename><surname>Godoy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Podhorszki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Atkins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Eisenhauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">E</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Germaschewski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">A</forename><surname>Huck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Huebl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kress</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Kurç</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Logan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Ostrouchov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Parashar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Poeschel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Pugmire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Suchyta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Takahashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Thompson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tsutsumi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Klasky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SoftwareX</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">100561</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning how to play bomberman with deep reinforcement and imitation learning</title>
		<author>
			<persName><forename type="first">Í</forename><surname>Goulart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Paes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Clua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Entertainment Computing and Serious Games -First IFIP TC 14 Joint International Conference, ICEC-JCSG 2019</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">E</forename><forename type="middle">D V</forename><surname>Proceedings</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Der Spek</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><forename type="middle">Y</forename><surname>Göbel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><surname>Do</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Clua</surname></persName>
		</editor>
		<editor>
			<persName><surname>Hauge</surname></persName>
		</editor>
		<meeting><address><addrLine>Arequipa, Peru</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">November 11-15, 2019. 2019</date>
			<biblScope unit="volume">11863</biblScope>
			<biblScope unit="page" from="121" to="133" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Combining pretrained CNN feature extractors to enhance clustering of complex natural images</title>
		<author>
			<persName><forename type="first">J</forename><surname>Guérin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Thiery</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Nyiri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Gibaru</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Boots</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">423</biblScope>
			<biblScope unit="page" from="551" to="571" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">An efficient approach for assessing hyperparameter importance</title>
		<author>
			<persName><forename type="first">H</forename><surname>Hoos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Leyton-Brown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="754" to="762" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">A method for stochastic optimization</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="1097" to="1105" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="84" to="90" />
			<date type="published" when="2017-05">May, 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deep learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">nature</title>
		<imprint>
			<biblScope unit="volume">521</biblScope>
			<biblScope unit="issue">7553</biblScope>
			<biblScope unit="page">436</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Deep learning for EEG data analytics: A survey</title>
		<author>
			<persName><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">C</forename><surname>Youn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Camacho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Concurr. Comput. Pract. Exp</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">18</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Grid search, random search, genetic algorithm: A big comparison for NAS</title>
		<author>
			<persName><forename type="first">P</forename><surname>Liashchynskyi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Liashchynskyi</surname></persName>
		</author>
		<idno>CoRR vol. abs/1912.06059</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Deep learning framework based on integration of s-mask R-CNN and inception-v3 for ultrasound image-aided diagnosis of prostate cancer</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Future Gener. Comput. Syst</title>
		<imprint>
			<biblScope unit="volume">114</biblScope>
			<biblScope unit="page" from="358" to="367" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Subject independent facial expression recognition with robust face detection using a convolutional neural network</title>
		<author>
			<persName><forename type="first">M</forename><surname>Matsugu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Mitari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Kaneda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Networks Research: IJCNN &apos;03</title>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="555" to="559" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Yesworkflow: A user-oriented, language-independent tool for recovering workflow information from scripts</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Mcphillips</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kolisnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Aulenbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Belhajjame</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Bocinsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Chirigati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>Dey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Freire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">N</forename><surname>Huntzinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Koop</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Missier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Schildhauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">R</forename><surname>Schwalm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cheney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bieda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ludäscher</surname></persName>
		</author>
		<idno>CoRR vol. abs/1502.02403</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Lifecycle management for deep learning</title>
		<author>
			<persName><forename type="first">H</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Deshpande</surname></persName>
		</author>
		<author>
			<persName><surname>Modelhub</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
			<publisher>Univ. of Maryland</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Towards unified data and lifecycle management for deep learning</title>
		<author>
			<persName><forename type="first">H</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Deshpande</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 33rd ICDE. IEEE</title>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
			<biblScope unit="page" from="571" to="582" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Provenance: an introduction to prov</title>
		<author>
			<persName><forename type="first">L</forename><surname>Moreau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Groth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Synthesis Lectures on the Semantic Web: Theory and Technology</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="129" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A visual vocabulary for flower classification</title>
		<author>
			<persName><forename type="first">M.-E</forename><surname>Nilsback</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR&apos;06)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2006">2006. 2006</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1447" to="1454" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Deep learning for financial applications : A survey</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Özbayoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">U</forename><surname>Gudelek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">B</forename><surname>Sezer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Appl. Soft Comput</title>
		<imprint>
			<biblScope unit="volume">93</biblScope>
			<biblScope unit="page">106384</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Demonstrating complementary provenance from noworkflow &amp; yesworkflow</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Pimentel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>Dey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Mcphillips</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Belhajjame</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Koop</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Murta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Braganholo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ludäscher</surname></persName>
		</author>
		<author>
			<persName><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Provenance and Annotation of Data and Processes -6th International Provenance and Annotation Workshop, IPAW 2016</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Proceedings</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><surname>Mattoso</surname></persName>
		</editor>
		<editor>
			<persName><surname>Glavic</surname></persName>
		</editor>
		<meeting><address><addrLine>McLean, VA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">June 7-8, 2016. 2016</date>
			<biblScope unit="volume">9672</biblScope>
			<biblScope unit="page" from="161" to="165" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">noworkflow: a tool for collecting, analyzing, and managing provenance from python scripts</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Pimentel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Murta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Braganholo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Freire</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. VLDB Endow</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1841" to="1844" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Provenance supporting hyperparameter analysis in deep neural networks</title>
		<author>
			<persName><forename type="first">D</forename><surname>Pina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Kunstmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>De Oliveira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Valduriez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mattoso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Provenance and Annotation Workshop</title>
		<imprint>
			<publisher>IPAW</publisher>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Uma abordagem para coleta e análise de dados de configurações em redes neurais profundas</title>
		<author>
			<persName><forename type="first">D</forename><surname>Pina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Kunstmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Oliveira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Valduriez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mattoso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Anais do XXXV Simpósio Brasileiro de Bancos de Dados</title>
		<meeting><address><addrLine>Porto Alegre, RS, Brasil</addrLine></address></meeting>
		<imprint>
			<publisher>SBC</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="187" to="192" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Análise de hiperparâmetros em aplicações de aprendizado profundo por meio de dados de proveniência</title>
		<author>
			<persName><forename type="first">D</forename><surname>Pina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Neves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Paes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>De Oliveira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mattoso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Anais do XXXIV Simpósio Brasileiro de Banco de Dados</title>
		<meeting><address><addrLine>Porto Alegre, RS, Brasil</addrLine></address></meeting>
		<imprint>
			<publisher>SBC</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="223" to="228" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Bilingual word embedding with sentence combination CNN for 1-to-n sentence alignment</title>
		<author>
			<persName><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Peng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NLPIR 2020: 4th International Conference on Natural Language Processing and Information Retrieval</title>
		<meeting><address><addrLine>Seoul, Republic of Korea</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2020">December 18-20, 2020. 2020</date>
			<biblScope unit="page" from="119" to="124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">An overview of gradient descent optimization algorithms</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ruder</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.04747</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Automatically tracking metadata and provenance of machine learning experiments</title>
		<author>
			<persName><forename type="first">S</forename><surname>Schelter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-H</forename><surname>Böse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kirschnick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Seufert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ML Systems workshop</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Runtime dataflow analysis tool for computational science and engineering applications</title>
		<author>
			<persName><forename type="first">V</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Campos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Guedes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Camata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>De Oliveira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L G A</forename><surname>Coutinho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Valduriez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mattoso</surname></persName>
		</author>
		<author>
			<persName><surname>Dfanalyzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SoftwareX</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">100592</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Runtime dataflow analysis of scientific applications using provenance</title>
		<author>
			<persName><forename type="first">V</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>De Oliveira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mattoso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Valduriez</surname></persName>
		</author>
		<author>
			<persName><surname>Dfanalyzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. VLDB Endow</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2082" to="2085" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Analyzing related raw data files through dataflows</title>
		<author>
			<persName><forename type="first">V</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>De Oliveira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Valduriez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mattoso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Concurrency and Computation: Practice and Experience</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2528" to="2545" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Workflow provenance in the lifecycle of scientific machine learning</title>
		<author>
			<persName><forename type="first">R</forename><surname>Souza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">G</forename><surname>Azevedo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Lourenço</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Soares</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Thiago</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Brandão</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Civitarese</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Brazil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Moreno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Valduriez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mattoso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cerqueira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A S</forename><surname>Netto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Concurrency and Computation: Practice and Experience n/a (n/a)</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page">6544</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Runway: machine learning model experiment management tool</title>
		<author>
			<persName><forename type="first">J</forename><surname>Tsay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mummert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Bobroff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Braz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Westerink</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hirzel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SysML</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Runtime aware architectures</title>
		<author>
			<persName><forename type="first">M</forename><surname>Valero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 9th Annual Workshop on General Purpose Processing using Graphics Processing Unit, GPGPU@PPoPP 2016</title>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Kaeli</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Cavazos</surname></persName>
		</editor>
		<meeting>the 9th Annual Workshop on General Purpose Processing using Graphics Processing Unit, GPGPU@PPoPP 2016<address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016">March 12 -16, 2016. 2016</date>
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Human-ai collaboration in data science: Exploring data scientists&apos; perceptions of automated ai</title>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Weisz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Geyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Dugan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tausczik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Samulowitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM on Human-Computer Interaction 3 (CSCW)</title>
		<meeting>the ACM on Human-Computer Interaction 3 (CSCW)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">B-pinns: Bayesian physics-informed neural networks for forward and inverse PDE problems with noisy data</title>
		<author>
			<persName><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Karniadakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Comput. Phys</title>
		<imprint>
			<biblScope unit="volume">425</biblScope>
			<biblScope unit="page">109913</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Accelerating the machine learning lifecycle with mlflow</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zaharia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Davidson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ghodsi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Konwinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Murching</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Nykodym</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ogilvie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Parkhe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Data Eng. Bull</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="39" to="45" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Adadelta: an adaptive learning rate method</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1212.5701</idno>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Bayesian deep convolutional encoder-decoder networks for surrogate modeling and uncertainty quantification</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Zabaras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computational Physics</title>
		<imprint>
			<biblScope unit="volume">366</biblScope>
			<biblScope unit="page" from="415" to="447" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
