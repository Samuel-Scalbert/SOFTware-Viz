<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Exploring Category Structure with Contextual Language Models and Lexical Semantic Networks</title>
				<funder>
					<orgName type="full">DFKI</orgName>
				</funder>
				<funder>
					<orgName type="full">Inria</orgName>
				</funder>
				<funder>
					<orgName type="full">Inria Exploratory Action COMANCHE</orgName>
				</funder>
				<funder>
					<orgName type="full">IM-PRESS</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Joseph</forename><surname>Renner</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">UMR 9189</orgName>
								<orgName type="institution" key="instit1">Univ. Lille</orgName>
								<orgName type="institution" key="instit2">Inria</orgName>
								<orgName type="institution" key="instit3">CNRS</orgName>
								<orgName type="institution" key="instit4">Centrale Lille</orgName>
								<address>
									<addrLine>-CRIStAL</addrLine>
									<postCode>F-59000</postCode>
									<settlement>Lille</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Pascal</forename><surname>Denis</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">UMR 9189</orgName>
								<orgName type="institution" key="instit1">Univ. Lille</orgName>
								<orgName type="institution" key="instit2">Inria</orgName>
								<orgName type="institution" key="instit3">CNRS</orgName>
								<orgName type="institution" key="instit4">Centrale Lille</orgName>
								<address>
									<addrLine>-CRIStAL</addrLine>
									<postCode>F-59000</postCode>
									<settlement>Lille</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Rémi</forename><surname>Gilleron</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">UMR 9189</orgName>
								<orgName type="institution" key="instit1">Univ. Lille</orgName>
								<orgName type="institution" key="instit2">Inria</orgName>
								<orgName type="institution" key="instit3">CNRS</orgName>
								<orgName type="institution" key="instit4">Centrale Lille</orgName>
								<address>
									<addrLine>-CRIStAL</addrLine>
									<postCode>F-59000</postCode>
									<settlement>Lille</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Angèle</forename><surname>Brunellière</surname></persName>
							<email>angele.brunelliere@univ-lille.fr</email>
							<affiliation key="aff1">
								<orgName type="laboratory">UMR 9193</orgName>
								<orgName type="institution" key="instit1">Univ. Lille</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
								<address>
									<addrLine>-SCALab</addrLine>
									<postCode>F-59000</postCode>
									<settlement>Lille</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Exploring Category Structure with Contextual Language Models and Lexical Semantic Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">55602DEA5CD78F0E21444692D01F690D</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-04-12T14:46+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recent work on predicting category structure with distributional models, using either static word embeddings <ref type="bibr" target="#b11">(Heyman and Heyman, 2019)</ref> or contextualized language models (CLMs) <ref type="bibr" target="#b23">(Misra et al., 2021)</ref>, report low correlations with human ratings, thus calling into question their plausibility as models of human semantic memory. In this work, we revisit this question testing a wider array of methods for probing CLMs for predicting typicality scores. Our experiments, using BERT (Devlin et al., 2018), show the importance of using the right type of CLM probes, as our best BERT-based typicality prediction methods substantially improve over previous works. Second, our results highlight the importance of polysemy in this task: our best results are obtained when using a disambiguation mechanism. Finally, additional experiments reveal that Information Contentbased WordNet (Miller, 1995), also endowed with disambiguation, match the performance of the best BERT-based method, and in fact capture complementary information, which can be combined with BERT to achieve enhanced typicality predictions.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The empirical success of contextual language models (CLMs) <ref type="bibr" target="#b28">(Peters et al., 2018;</ref><ref type="bibr" target="#b4">Devlin et al., 2018;</ref><ref type="bibr" target="#b17">Liu et al., 2019)</ref> has led to much research analyzing their functionality <ref type="bibr" target="#b31">(Rogers et al., 2020;</ref><ref type="bibr">Wu et al., 2020;</ref><ref type="bibr" target="#b6">Ethayarajh, 2019)</ref> and how they acquire semantic and world knowledge <ref type="bibr" target="#b29">(Petroni et al., 2019)</ref>. It also raises the question of their plausibility as models of human semantic memory <ref type="bibr" target="#b3">(Chronis and Erk, 2020;</ref><ref type="bibr" target="#b7">Ettinger, 2020;</ref><ref type="bibr" target="#b8">Garí Soler and Apidianaki, 2021)</ref>. The study of categorical knowledge, in particular typicality, provides a window into this question <ref type="bibr" target="#b25">(Murphy, 2004)</ref>. As initially observed by <ref type="bibr" target="#b32">Rosch (1975)</ref>, native speakers of English consider that certain exemplars (e.g., robin, crow) are more representative than others (e.g., penguin, ostrich) of a conceptual category (birds). That is, categorical knowledge is organized along a graded structure. According to prototype theory <ref type="bibr" target="#b32">(Rosch, 1975)</ref>, the structure follows from the fact that properties frequently shared among category members tend to be integrated into its prototype.</p><p>The main question we raise in this paper is whether distributional models, and CLMs in particular, are indeed aware of category structure, as captured in existing human typicality norms for English. The broader question for cognitive science is whether or not this type of knowledge can be learned through text-based exposure alone, thus contributing to the larger debate on the role of language in learning semantic knowledge <ref type="bibr" target="#b18">(Lupyan and Lewis, 2019)</ref>. But there are also more practical motivations for this work, as many NLP tasks (such as information retrieval, question answering, natural language inference) can arguably benefit from typicality relations by understanding which exemplars are more relevant to particular concepts.</p><p>Previous work on this topic provide mostly negative results. <ref type="bibr" target="#b11">Heyman and Heyman (2019)</ref> use classical static embeddings and represent typicality scores between categories and their exemplars as the cosine distance between their corresponding word embeddings. They conclude that static embeddings poorly account for human typicality scores, as obtained from <ref type="bibr" target="#b24">Morrow and Duffy (2005)</ref>. More recently, <ref type="bibr" target="#b23">Misra et al. (2021)</ref> use various CLMs to predict typicality. While CLMs have a larger, more expressive parameter space and are tuned over larger corpora with arguably better learning objectives than static word embeddings, <ref type="bibr" target="#b23">Misra et al. (2021)</ref> report only slightly better, and still modest, correlations with human typicality judgements (in this case, <ref type="bibr" target="#b32">Rosch (1975)</ref>). Their probing approach uses a Cloze test formulation over taxonomic sentences (e.g., A robin is a bird).</p><p>We revisit this question by first introducing an expanded suite of methods to extract typicality scores from CLMs, some of which improve correlations with human norms, showing the importance of using the right probe for typicality. How to reliably and efficiently probe CLMs is still an open question, including supervised and unsupervised approaches <ref type="bibr" target="#b31">(Rogers et al., 2020;</ref><ref type="bibr" target="#b5">Elazar et al., 2021;</ref><ref type="bibr">Wu et al., 2020)</ref>. The problem is exacerbated by the fact that typicality is a relation between pairs of concepts, abstracted away from contexts, while CLMs produce representations for contextualized word tokens (not types). We consider a wider array of approaches, all unsupervised, for predicting typicality from CLMs, using BERT as representative test case. Our probing approaches fall into two main classes: (i) BERT as a probabilistic language model, and (ii) generating "static" word embeddings from contextual BERT embeddings.</p><p>Our second contribution is showing the importance of polysemy (e.g., orange can be a fruit, a color, or a company) for typicality, an issue that has been largely overlooked in previous works. When judging category-exemplar pair typicality, humans arguably don't consider all the senses of the category and exemplar words. Instead they consider the senses of the category and exemplar that are most compatible with one another, in effect performing a joint disambiguation. Polysemy is problematic for static word embeddings as they collapse word senses into a single vector. CLMs, on the other hand, provide some form of sense selection through contextualization <ref type="bibr" target="#b6">(Ethayarajh, 2019;</ref><ref type="bibr" target="#b8">Garí Soler and Apidianaki, 2021)</ref>, but there are many possible ways to use CLMs and to provide contexts to these models (such as sampled or taxonomic sentences). We find that using a simple disambiguation mechanism, such as the method of deriving multi-prototype embeddings introduced in Chronis and Erk (2020), leads to typicality predictions more closely correlated with human rankings.</p><p>Finally, another research question we address in this paper is whether categorical structure is present in lexical semantic networks, such as WordNet <ref type="bibr" target="#b22">(Miller, 1995)</ref>. The hierarchical concept organization in WordNet was indeed initially inspired by theories of human semantic memory <ref type="bibr" target="#b1">(Beckwith et al., 2021)</ref>. Note that polysemy is also an issue in using WordNet, as we need to map category and exemplars to specific synsets. However, using the structure of WordNet, a simple disambiguation heuristic, and taking into account word frequency information leads to competitive results. Further-more, we find that WordNet-based and BERT-based typicality predictions contain complementary information, and creating a simple ensemble method of the two further increases performance.</p><p>In summary, our main contributions are:</p><p>• We introduce and compare a large array of unsupervised probing approaches for assessing whether BERT capture the internal structure of semantic categories. • Our experiments reveal BERT can predict human typicality rankings more reliably than previously found in <ref type="bibr" target="#b11">Heyman and Heyman (2019)</ref> and <ref type="bibr" target="#b23">Misra et al. (2021)</ref>, but only when endowed with a disambiguation mechanism. • Similarly, typicality predictions based on the structure of WordNet (again with a disambiguation mechanism) can achieve similar but complementary performance. • A simple combination of BERT and WordNet predictions leads to a higher level of correlation (Spearman greater than 0.5) with human typicality rankings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Recent years have seen an ever growing body of work on assessing whether distributional models constitute realistic models of human semantic memory, within both NLP <ref type="bibr" target="#b3">(Chronis and Erk, 2020;</ref><ref type="bibr" target="#b7">Ettinger, 2020)</ref> and Cognitive Science <ref type="bibr" target="#b12">(Hollis, 2017;</ref><ref type="bibr" target="#b13">Hollis and Westbury, 2016;</ref><ref type="bibr" target="#b19">Mandera et al., 2017;</ref><ref type="bibr" target="#b10">Günther et al., 2019;</ref><ref type="bibr" target="#b18">Lupyan and Lewis, 2019;</ref><ref type="bibr" target="#b14">Kumar, 2021)</ref>. In this context, the study of categorical knowledge, and specifically graded typicality, plays a crucial role, given it is one of the most reliable findings in the study of human categorical knowledge <ref type="bibr" target="#b32">(Rosch, 1975;</ref><ref type="bibr" target="#b25">Murphy, 2004)</ref>. The first work on using distributional models to predict typicality scores is <ref type="bibr" target="#b11">Heyman and Heyman (2019)</ref>, who estimate these scores, in English and Dutch, using cosine distances between the static embeddings of the exemplar and category words. These show that static embeddings, whether trained in a counted-or prediction-based fashion, yield poor correlations with human typicality norms. On the related task of lexical entailment (LE), <ref type="bibr" target="#b35">Vulić et al. (2017)</ref> evaluated different unsupervised and supervised methods that use static word embeddings. They also found a significant gap between static word embeddings and human norms. While LE is closely related to typicality, it differs in that LE predicts the relationship between two words, while typicality compares the relationship between two concepts (in which words are used as a proxy, in our case). Also, LE ratings are not restricted to pairs of words belonging to the same category. We differ from these earlier work on typicality by considering more expressive CLMs instead of static embeddings. The most closely related work is <ref type="bibr" target="#b23">Misra et al. (2021)</ref> who probe a wide range of CLMs (i.e., different variants of BERT and GPT) for predicting typicality scores: these are extracted using a Cloze task formulation over hand crafted taxonomic sentences (e.g. A robin is a bird.). They report better correlation scores with human norms, although still modest, which they take as indication that text exposure is not sufficient to learn category structure. Our focus in this work is different and somewhat broader in that we consider a wider array of probing methods of CLMs, though restricted to unsupervised ones. Supervised probing approaches, based on classifiers taking CLM representations as inputs, are problematic as they provide a less direct probing approach, possibly adding additional confounds <ref type="bibr" target="#b5">(Elazar et al., 2021;</ref><ref type="bibr">Wu et al., 2020)</ref>.</p><p>Another distinctive aspect of our work is that we compare CLMs to lexical networks like WordNet <ref type="bibr" target="#b22">(Miller, 1995)</ref>. Furthermore, we study the impact of polysemy in typicality, which was not controlled in <ref type="bibr" target="#b23">Misra et al. (2021)</ref> or in <ref type="bibr" target="#b11">Heyman and Heyman (2019)</ref>. The work of <ref type="bibr" target="#b0">Apidianaki and Garí Soler (2021)</ref> shares some similarities with <ref type="bibr" target="#b23">(Misra et al., 2021)</ref>: they also use BERT under a Cloze task setting to study typicality, but they focus on identifying whether the model has access to prototypical properties of concepts (e.g., a ball is round).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Problem Setting and Framework</head><p>This section presents our general framework for assessing whether a lexical representation model (i.e., a CLM or semantic network) is aware of category structure; the specific probing approaches will be described in Sec. 4. Let us assume a generic conceptual space consisting of a set C of categories as well as a set E of exemplars of these categories.</p><p>For each category c in C, we assume a set of n c exemplars e c i in E for i = 1 to n c . Following the findings of <ref type="bibr" target="#b32">Rosch (1975)</ref>, we posit that this conceptual space has a graded structure, in that certain exemplars (e.g., robin, crow) are more closely representative of their category (e.g., birds) than others (e.g., penguin, ostrich). This can be expressed through a typicality score t c i ∈ R + 0 , often an ordinal scale, for each e c i , where t c i &gt; t c j indicates that exemplar e c i is more typical of category c than e c j . These typicality scores can be obtained from human subjects through various kinds of stimuli, such as direct scoring of category-exemplar word pairs (e.g., robin-bird) and scoring of a taxonomic sentence (e.g., A robin is a bird.)(see Sec. 5.1).</p><p>To assess whether a lexical representation model is aware of such a structure, three main components are needed. As the conceptual space is latent, we first need a concept-to-word mapping function from category and exemplar concepts to words. As is done in human studies and previous work, we assume that categories and exemplars are reliably accessed through their corresponding singular words: the concepts robin and bird are accessed via the words robin and bird, respectively. This is a simplification, as the same concept can be realized by plural forms, as well as by different (synonymous) words. Given this functional mapping, we will conflate a category c with its category word c and an exemplar e c i with its exemplar word e c i . Second, we define a typicality scoring function over exemplar-category word pairs, which captures how typical the exemplar is of the category. Each model will assign a typicality score s c i to each category c and exemplar e c i pair. Some of our methods will make use of a text corpus, denoted S, taking the form of a pre-extracted set of sentences containing exemplar and/or category words. This corpus S can be viewed as an extra parameter of the typicality scoring function, as it will directly impact the predicted typicality values s c i . Such a corpus is required to be able to fully leverage CLMs, whether they are used as probabilistic language models or to produce word vector representations. It is also through this corpus that one hopes to capture contextual word realizations that approximate the different senses associated with the exemplar and category words. Indeed, we hypothesize that one important aspect of assessing typicality between category and exemplar words is to be able to deal with the polysemy of these words, predicting that the best methods should be able to disambiguate these words in such a way that the exemplar word is interpreted in a category-compatible sense and the category word in an exemplar-compatible sense. Using WordNet synsets as a proxy for senses, we find that on average, category words are linked to 4.2 and 4.9 senses, while exemplars are linked to 3.3 and 4.7 senses on the typicality datasets of <ref type="bibr" target="#b24">Morrow and Duffy (2005)</ref> and <ref type="bibr" target="#b32">Rosch (1975)</ref>, respectively<ref type="foot" target="#foot_0">1</ref> (see Sec. 5.1 for dataset details). This shows the depth of the polysemy problem for typicality, as each word on average has multiple meanings.</p><p>Finally, we need an evaluation metric for measuring how well the predicted typicality scores s c i are able to mirror the human judgements t c i . The most obvious choice for this metric is to use Spearman rank correlation <ref type="bibr" target="#b34">(Spearman, 1904)</ref>, as this correlation measure is non-parametric and makes no strong (e.g., linear) assumption on the data distribution or the underlying scoring functions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Methods</head><p>We distinguish two classes of methods, depending on whether they use CLMs or Word-Net. For CLM-based methods, we use BERT (bert-base-uncased specifically) as a prototypical CLM to assess whether CLMs are aware of category structure. The framework and methods easily generalize to other CLMs. While the concept-toword mapping is the same for these methods, the scoring function and the probing corpus are different. The BERT methods fall into two categories: (i) those that use BERT as a language model (BERT-MLM, BERT-SentEmb, BERT-MLM-Taxo), and (ii) those that extract word vectors from BERT (BERT-Avg, BERT-MPro). Note that these are all unsupervised probing methods: when an additional corpus S is used, it is only used to probe BERT, not to fine-tune it. These methods embody different linguistic hypotheses and adopt different ways of dealing with polysemy. Note that for embedding methods, we test all layers.</p><p>Secondly, we consider lexical semantic networks to compute typicality scores, using WordNet <ref type="bibr" target="#b22">(Miller, 1995)</ref> specifically. For this, we use the Shortest-Path and the Lin <ref type="bibr" target="#b16">(Lin et al., 1998)</ref> similarity measures computed on WordNet <ref type="bibr" target="#b22">(Miller, 1995)</ref>. We compute the information content (IC) values <ref type="bibr" target="#b30">(Resnik, 1995)</ref> used in the Lin measure using the additional corpus S. We deal with polysemy by using the maximum similarity between exemplar and category synset pairs to compute typicality scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Using BERT's Language Model</head><p>The first class of methods relies on the language modeling abilities of BERT, using the following hypothesis: the more central (or peripheral) an exemplar is to a category, the more (or less) likely it is to be used in the category context(s). This hypothesis can be turned into two different distributional hypotheses, depending on whether we take a paradigmatic perspective (i.e., how likely can the exemplar word be substituted for the category word in the category contexts) or a syntagmatic perspective (i.e., how likely can the exemplar be used next to its category). Our first two probing methods take a paradigmatic perspective and are reminiscent of the Distributional Inclusion Hypothesis, first proposed by <ref type="bibr" target="#b9">Geffet and Dagan (2005)</ref>. They compute typicality scores that reflect how successful the substitution of a category word by an exemplar is, measured by conditional word probability (BERT-MLM) or by cosine distance between sentence embeddings before and after substitution (BERT-SentEmb). In both, the set of sentences is restricted to sentences that contain the category words and is denoted by S c .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Masked Language Modeling (BERT-MLM)</head><p>Under this approach, typicality is computed as the conditional probability of seeing the exemplar word in its category's contexts. Specifically, for each sentence in S c , the category name is masked using the [MASK] token and the resulting sentence is passed through BERT and the MLM classification, yielding MLM logits for each subtoken in the vocabulary, which are softmaxed to probabilities. Typicality scores are obtained by averaging across sentences the MLM probabilities for each exemplar subtoken sequence. Formally, the typicality score s c i for an exemplar e c i with l i subtokens and category c, given BERT and masked sentences S c , is computed by</p><formula xml:id="formula_0">s c i = 1 |S c | |Sc| j=1 l i k=1 BERT M LM (S j c k )<label>(1)</label></formula><p>where S j c k is the kth subtoken of exemplar e c i in the jth sentence of S c , and BERT M LM (•) gives the probability of this subtoken.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Sentence Embedding Modeling (BERT-SentEmb)</head><p>In this method, typicality is taken to be a measure of how well a category word can be substituted by its exemplars in sentences S c without altering the overall sentence meaning, which we approximate as the sentence embedding. Recall that in BERT's NSP objective, the [CLS] sentence classification token is input to the NSP classification head. For each category name sentence, we compute layer wise activations of this sentence token. Then, for each exemplar, we replace the category word in each sentence with the exemplar and obtain the same activations. The typicality score is the cosine similarity of the original sentence embedding and that of the replaced sentence, averaged across sentences. The sentence embeddings can be obtained from any layer in the model; we test the method separately for all layers. Formally, given a category c and an examplar e c i , a set of sentences S c containing the category word c, and a SE(•) function which outputs a sentence embedding from a specific layer, the typicality score is computed by</p><formula xml:id="formula_1">s c i = 1 |S c | |Sc| j=1 cos(SE(S c ), SE(S j c→e c i )) (2)</formula><p>where S j c→e c i is S j with c replaced by e c i , and cos(•, •) is the cosine similarity operator.</p><p>As they rely on paradigmatic substitution, neither BERT-MLM nor BERT-SentEmb explicitly attempt to disambiguate exemplar or category words, but one can argue that some form of sense selection happens through BERT's contextual modeling and the selection of category contexts. This ensures that the word orange is used in fruit compatible contexts, thus hopefully filtering out contexts compatible with other senses (e.g. color), when predicting typicality within the fruit category. But there is no disambiguation of the category word, as all of its contexts are randomly sampled. This might introduce some noise as category examples with another sense (e.g., fruit of their labor) or even POS (e.g., the trees fruit early) might be sampled.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3">Masked Language Modeling with</head><p>Taxonomic Sentences (BERT-MLM-Taxo)</p><p>Inspired by <ref type="bibr" target="#b23">Misra et al. (2021)</ref>'s Taxonomic Sentence Verification method, our third CLM-based method is similar to BERT-MLM, but different in that it uses taxonomic propositions instead of sampled sentences. These propositions are in the form "A(n) X is a(n) Y", where X and Y are exemplars e c i and categories c, respectively. Typicality scores are obtained in the same way as BERT-MLM: exemplar subtokens are masked in the taxo-nomic proposition, MLM logits are obtained from the model, then the product of the softmaxed logits (probabilities) is the typicality score (same as Eq. 1). While these sentences provide only narrow contexts, and are somewhat artificial and different from the BERT's training data, they are informative in providing implicit mutual disambiguation of both the exemplar and category words.</p><p>BERT-MLM-Taxo is similar to Misra et al. ( <ref type="formula">2021</ref>), but it differs in two ways. First, <ref type="bibr" target="#b23">Misra et al. (2021)</ref>'s method uses CLMs to compute the probability of the category word in the taxonomic proposition (i.e. P (c|e c i )), while our method does the opposite (P (e c i |c)) to more resemble how humans are probed for the task. Second, <ref type="bibr" target="#b23">Misra et al. (2021)</ref> calculates conditional probabilities for masked language models such as BERT using the formulation introduced in Wang and Cho (2019): separately masking each subtoken in the word and passing these separately masked variants of the taxonomic sentence through the model, summing the masked subtokens across variants. While this method is more theoretically founded (as BERT is not an incremental language model and cannot compute probabilities through the chain rule), it is subject to skewed MLM logits if used on exemplars 2 , as a result of words being broken into subtokens (example from <ref type="bibr" target="#b23">Misra et al. (2021)</ref>: if the word ostrich is segmented into ostr and ich, then the probability of ich given that it is preceded by ostr is anomalously high, skewing the sequence probability). Thus, we mask all subtokens of the exemplar and treat the resulting probabilities as a sequence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">BERT-Based "Static" Representations</head><p>The next CLM-based methods derive static word representations from contextual representations computed over a corpus S which includes sentences containing category words as well as sentences containing exemplar words. We use cosine similarity between category and exemplar embeddings for typicality scores s c i = cos(R(e c i ), R(c)), where R(•) provides the static representations of the category and exemplar. We derive static word representations by averaging all hidden state activations of a word over S (BERT-Avg), or clustering these hidden states, allowing for sense modulation (BERT-MPro). For this set of methods, our hypothesis is that the vectorial space induced by BERT captures typicality, as opposed to the language mod-2 All category words are treated as one subtoken.</p><p>eling capabilities used in the methods of Sec. 4.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Averaged Contextual Embeddings (BERT-Avg)</head><p>The first approach for generating static word type vectors from contextualized word token embeddings is to simply average them <ref type="bibr" target="#b2">(Bommasani et al., 2020)</ref>. For each sentence in S, we compute and store the contextual representations of the word at each layer of the BERT model. We then average these representations over sentences for each word, giving a static embedding for each layer, for each category and exemplar. The typicality score of a pair (e c i , c) is computed as the cosine similarity between the static embeddings of the representation of e c i and of the representation of c. As with BERT-SentEmb, we test the method separately for all layers. Note that this approach inherits the problem found in classical static embeddings that it conflates all possible senses of a word.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Multi-Prototype Contextual Embeddings (BERT-MPro)</head><p>To further exploit context, we use multi-prototype BERT embeddings <ref type="bibr" target="#b3">(Chronis and Erk, 2020)</ref>. For each category name and exemplar, we use k-means to cluster the word's contextual embeddings computed from its sampled sentences, yielding, for a given k, k cluster centroids for each layer, which disambiguate the k different possible meanings of the word. Following Chronis and Erk (2020), we predict scores using the maxsim(•, •) function between the cluster centroids of the category name and exemplar, which yields the maximum similarity value of category-exemplar centroid pairings. Formally, for a given k and layer l, and set of cluster centroids τ (w) l 1 ...τ (w) l k for category c and exemplar e c i , the similarity s c i is defined by</p><formula xml:id="formula_2">maxsim(c, e c i ) = max 1≤j≤k,1≤t≤k cos(τ (c) l j , τ (e c i ) l k )</formula><p>(3) where cos(•, •) is the cosine similarity measure. Following <ref type="bibr" target="#b3">Chronis and Erk (2020)</ref>, we sidestep tuning the number of clusters k and simply take as input to the maxsim operator the union of all clusters from k ≤ 15. Furthermore, we also test the embeddings from all layers separately.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">WordNet-Based Methods</head><p>Our second class of methods uses WordNet <ref type="bibr" target="#b22">(Miller, 1995)</ref> to compute typicality scores. Let us sup-pose that a category word c and exemplar word e c i are mapped to synsets s(c) and s(e c i ), respectively. Only noun synsets are considered for c and e c i . The hypothesis here is that the more closely linked s(c) and s(e c i ) are in the WordNet graph, the more the exemplar is typical of the category. To measure this "linkage" between exemplars and categories, we use two WordNet similarity measures, thus yielding two sets of WordNet-based methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Shortest Path (WNSP, WNSP-noWSD)</head><p>Our first similarity is the shortest path between s(c) and s(e c i ) along hypernym/hyponym edges of WordNet. Given that exemplars and category words might be linked to multiple synsets, we need to aggregate the similarities between synsets. A first method, called WNSP-noWSD, is to average the similarities between all synsets s(c) and s(e c i ). The second method (WNSP) tries to disambiguate between possible synsets, relying on the maxsim(•, •) operator, but defined over synset similarities (instead of cosine similarity over clusters, as in BERT-MPro). Formally, for a category name c and exemplar e c i , the typicality score s c i is equated to maxsim(c, e c i ), which is defined by</p><formula xml:id="formula_3">maxsim(c, e c i ) = max s(c),s(e c i )</formula><p>sim(s(c), s(e c i )) (4)</p><p>where sim(•) denotes the shortest path similarity. Note that, irrespectively of the disambiguation process, the shortest path similarity possibly lacks expressivity, as synsets of different exemplars can be at the same distance of the category synset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Lin Similarity (WNIC, WNIC-noWSD)</head><p>For a more expressive method, we use the Lin similarity measure <ref type="bibr" target="#b16">(Lin et al., 1998)</ref>, which computes similarity using the most specific ancestor node and Information Content (IC) (computed via Wikipedia text dump), a measure of specificity for a concept closely linked to frequency <ref type="bibr" target="#b30">(Resnik, 1995)</ref>, thus combining frequency with hierarchical semantic knowledge. <ref type="bibr" target="#b26">Pedersen (2010)</ref> show that augmenting WordNet with IC results in higher correlation with human judgments on similarity and relatedness tasks. Formally, let IC(•) and lcs(•, •) denote information content and least common subsumer (specific ancestor node), respectively, the similarity between synsets s(c) and s(e c i ) is defined by lin(s(c), s(e c i )) = 2 * IC(lcs(s(c), s(e c i ))) IC(s(c)) + IC(s(e c i ))</p><p>(5)</p><p>We then can aggregate similarities by averaging, which defines the method WNIC-noWSD, or by using Equation <ref type="formula">4</ref>where sim(•) is chosen to be the Lin similarity. This latter method is called WNIC.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Datasets</head><p>As ground truth typicality ratings, we use three datasets of human typicality ratings, one from <ref type="bibr" target="#b32">Rosch (1975)</ref>  Comparing the substitution-based methods from Sec. 4.1, BERT-SentEmb and MLM-Taxo have similar performance. BERT-MLM-Taxo performs better than BERT-MLM, showing that taxonomic sentences provide narrow yet more informative contexts than a randomly sampled corpus simply based on the category word, as they allow for mutual disambiguation of category and exemplar words. <ref type="bibr" target="#b23">Misra et al. (2021)</ref>'s method performs worse than BERT-MLM-Taxo on the M&amp;D dataset but better on the Rosch dataset. <ref type="foot" target="#foot_4">6</ref> These discrepancies across datasets can be attributed to different protocols for obtaining human ratings: scoring categoryexemplar pairs for M&amp;D and scoring a taxonomic sentence for Rosch. They also account for the performance differences of the frequency baseline, which yields higher correlation scores on M&amp;D than on the Rosch dataset. Typicality scores are dissociable from lexical frequency when subjects judge whether a sentence is a good example of their idea of the category. Our results are in line with <ref type="bibr" target="#b20">Mervis et al. (1976)</ref>, showing that Rosch dataset scores are not correlated with frequency.</p><p>Comparing the word embedding methods from Sec. 4.2, the performance increase from BERT-Avg to BERT-MPro shows the importance of disam-biguation. Layer-wise performance can be found in Appendix A.8. It should be noted that for BERT-MPro, the best performing layer was layer 10, confirming Chronis and Erk (2020)'s claim that similarity-based task performance peaks in layers 8-10,<ref type="foot" target="#foot_5">7</ref> . Later layers show more contextual variation <ref type="bibr" target="#b6">(Ethayarajh, 2019)</ref> capturing finer-grained sense differences. Inversely, for BERT-Avg earlier layers perform best as they include less contextual variation, making the mean more stable.</p><p>WordNet-based Methods For the WordNetbased methods, WNIC achieves the highest correlations across datasets, and is competitive with the best BERT-based methods. This confirms the findings of <ref type="bibr" target="#b26">Pedersen (2010)</ref> on other intrinsic tasks such as similarity. Disambiguation in the WordNet methods seems crucial, as the variants using averaging perform worse than their maxsim versions. WNIC's high performance shows the importance of frequency information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>The experimental results confirm the importance of handling polysemy for both BERTand WordNet-based methods. Note that the best performing method (last row), which combines BERT-MPro and WNIC, is discussed in Sec. 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.2">Polysemy Analysis</head><p>Experimental results show that disambiguation allows the improvements of BERT-MPro over BERT-Avg and WNIC over WNIC-noWSD. We now study whether the improvement is larger when the number of senses is larger. Our hypothesis is that the more polysemous a word, the more imprecise its representation without disambiguation, and the larger the performance increase from using disambiguation with the maxsim operator. This analysis is difficult to do because: (i) we don't have access to "true" senses, and (ii) both exemplar and category words can be polysemous. One solution is to use WordNet synsets as a proxy for senses.</p><p>Looking at the estimated polysemy degree for the category words, we find that 8 of the 11 category words are associated with 2 senses or more, with the maximum ("tools") having 8 senses. Aligning with results for M&amp;D-young-adults, we find that BERT-MPro yields the largest correlation increases over BERT-Avg for the categories associated with polysemous category words: BERT-MPro yields above-average ρ increase for 6 of the 8 polysemous categories, with the largest increase (+0.2) for the most ambiguous category word "tools". However, the same analysis on the same dataset with WNIC and WNIC-noWSD does not yield the same trend: while disambiguation leads to a greater relative increase, some of the category words with the least polysemy (animals, insects) have the largest performance improvement from noWSD to WSD (see Appendix A.9 for more details). Looking at the average number of synsets for exemplars, it is not easy to see a trend in the performance improvements. Further analysis is needed and we leave a detailed study on polysemy and typicality for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Combining CLMs and Lexical Networks</head><p>Given that WNIC and BERT-MPro rankings achieve the highest correlations, questions arise: how similar are the rankings produced by the two methods? If sufficiently different, how can we combine them? To answer these questions, we perform a series of analyses. We constrict this depth-first analysis to just the younger adults M&amp;D dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Method Complementarity Study</head><p>BERT and WordNet provide different models of lexical meaning: BERT is word-oriented and exploits distributional statistical patterns, while Word-Net is sense-oriented and exploits more abstract relations. To test whether this leads to different predictions, we compute category-wise Spearman correlations between BERT-MPro and WNIC rankings. The results are given in the fourth column in Table <ref type="table" target="#tab_3">2</ref>. The values range from 0.274 to 0.570 with six categories below 0.4. This moderate correlation shows some complementarity between the two rankings. Next, we present how different the two methods' rankings of certain exemplars can be in the first four columns of Table <ref type="table" target="#tab_5">3</ref>, showing how complementarity manifests on a granular level. This study raises the question of combining BERTbased methods with WordNet-based methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">A Simple Ensemble Method</head><p>As an ensemble method, we take the raw scores of BERT-MPro and WNIC, convert them to z-scores (separately for each method and category), then sum them. <ref type="foot" target="#foot_6">8</ref> As shown in the last row of BERT-MPro combined with WNIC achieves the highest correlation (greater than 0.5) with human rankings across all datasets, closing the gap on the human performance given by the inter-dataset correlation between Rosch and M&amp;D older adults. The category Spearman correlations for BERT-MPro, WNIC and the ensemble are shown in Table <ref type="table" target="#tab_3">2</ref>. We find that the lower the correlation between the two methods (second from right column), the larger the improvement by combining: we find a rank correlation of -0.555 between the intermethod correlation and the average correlation increase of the ensemble (the last column of Table <ref type="table" target="#tab_3">2</ref>), showing that the more different the two methods rankings, the more ensembling increases performance. In Table <ref type="table" target="#tab_5">3</ref> are rankings of certain words within their category, showing how the ensemble improves rankings for exemplars that are badly ranked by one of the methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>We show that BERT and WordNet-based methods are able to outperform previous methods in typicality prediction, but only with a disambiguation  mechanism, either implicit through the selection of contexts or explicit via the estimation of distinct word senses. These results emphasize the importance of polysemy in assessing typicality, an issue that had been overlooked in previous work. We also show that BERT and WordNet provide complementary information that are relevant to modeling category structure: a simple ensemble of the two leads to the best correlations with human judgements.</p><p>We plan to further analyze the differences between CLMs and WordNet in typicality, and find more sophisticated ways to inject semantic knowledge into CLMs. Also, we want to use datasets in other languages, to find if the results generalize.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Limitations</head><p>The human typicality judgements we use from <ref type="bibr" target="#b24">Morrow and Duffy (2005)</ref> is limited to English and uses only participants all from the UK, while the rankings from <ref type="bibr" target="#b32">Rosch (1975)</ref> are all from the United States; thus, geographical/cultural biases could affect the typicality judgements (certain bird species or clothing exemplars could be more common in different areas, which could affect how human's judge their typicality).</p><p>While we try to analyze exactly why disambiguation increases performance (BERT-MPro, WNIC-WSD) in Sec. 5.3.2 and Appendix 8, we acknowledge this analysis is far from perfect: we don't have access to "gold" senses to measure polysemy (just linked WordNet synsets), and no single trend emerges from the analysis across all the methods.</p><p>Lastly, one limitation in our BERT-based methods is that we use the uncased pretrained model. In hindsight, we might have avoided some ambiguous word uses by using a cased model instead: for example, the model might have been able to more easily distinguish between "Apple", the company, and "apple", the fruit. However, the impact might have been marginal, as the model can still rely on contextual clues of the surrounding text, and such cases of ambiguity are arguably rare in the typicality datasets. Another possible limitation, specific to the BERT-MLM and BERT-SentEmb methods, comes from the current sentence sampling procedure, as it is based on the category word only, thus possibly introducing noise for words that have homonyms with different POS (e.g., nounverb Zhiyong Wu, Yun Chen, Ben Kao, and Qun Liu.</p><p>2020. Perturbed masking: Parameter-free probing for analyzing and interpreting bert. arXiv preprint arXiv:2004.14786. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Appendix</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Dataset Agreement</head><p>Tables <ref type="table" target="#tab_7">5</ref> and<ref type="table" target="#tab_8">6</ref> show the category-wise agreement between the human rankings of the three datasets.</p><p>Notice that the agreement between the two Morrow and Duffy datasets are much higher than that of Rosch and the two M&amp;D datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Sentence Sampling</head><p>For an auxiliary textual dataset S, we use Wikipedia, specifically searching for sentences that contain the singular form of each category word or exemplar, in order to have comparable contexts.</p><p>We found text from Wikipedia dumps can be noisy (such as: summarization tables, lists of related topics, other sequences that are not actual sentences), so we remove sentences that are too short or long (number of words must be between 5 and 200).</p><p>For BERT-AVG and BERT-MPro, we sampled 300 sentences for every exemplar and category word.</p><p>For BERT-MLM and BERT-SentEmb, we sampled 10000 sentences for each category.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 PPMI-SVD and Word2Vec Details</head><p>The PPMI matrix was computed from English Wikipedia using a window size of 2. We experimented with a window size of 5, but found the performance to be worse. We also use off-theshelf Word2Vec <ref type="bibr" target="#b21">(Mikolov et al., 2013)</ref> embeddings (skip-gram with negative sampling, 300 dimen-sions, GoogleNews training corpus).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.5 Runtimes</head><p>Frequency, PPMI-SVD, Word2vec, and both Word-Net baseline methods take &lt;5 seconds to compute typicality scores on CPU. The Wikipedia word counts used in Frequency and WordNet-IC take 1hr to compute on CPU. The sentence sampling from Wikipedia takes 2hrs to complete. BERT-WordPiece takes 1 min to complete on CPU. BERT-AVG and BERT-MLM take 3hrs to complete on the GPU mentioned in the text; BERT-SentEmb, 5hr; and BERT-MPro, 10hr.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.6 Results by Category</head><p>Table <ref type="table" target="#tab_9">7</ref> shows the Spearman's correlation values for each individual category. Note that just the top performing methods are shown.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.7 BERT-SentEmb Performance by Layer</head><p>The above figure shows the mean Spearman correlations by layer for the BERT NSP method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.8 BERT-AVG and MultiPrototype Hyperparameter Performance</head><p>The above image shows average Spearman's correlation across all categories for BERT multiprototype embeddings for each combination of layer index and number of cluster. Note that the top row (number of clusters = 1) is the same as the simple average of contextual representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.9 Detailed Polysemy Analysis</head><p>To see whether the maxsim disambiguation mechanism is actually increasing performance more for more polysemous examples, we can compare the performance of the same methods with and without disambiguation when evaluated on words with different number of linked WordNet synsets (a proxy for the degree of polysemy). As shown in Table <ref type="table" target="#tab_10">8</ref>, in general, BERT-MPro's disambiguation increases performance over BERT-Avg more for more polysemous categories (as measured by the synsets linked to the category word). However, this general trend is not seen when comparing WNIC to WNIC-noWSD.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.10 KnowBERT Results</head><p>The combination of CLMs and knowledge graphs is an active research area, mainly the design of knowledge enhanced CLMs (i.e. <ref type="bibr" target="#b37">Wang et al. (2020)</ref>, Rosset et al. ( <ref type="formula">2020</ref>)). We use KnowBERT <ref type="bibr" target="#b27">(Peters et al., 2019)</ref>, in which knowledge bases are embedded into BERT via an integrated entity linker, which retrieves relevant entity embeddings and updates the hidden states (we leave an in-depth description to the paper). We consider a KnowBERT model with WordNet as the knowledge base 9 with the multiprototype method for computing typicality scores. We get an average Spearman correlation of 0.446, below that of WNIC and BERT-MPro, showing that a general method for enhancing CLMs with WordNet does not work well for typicality.</p><p>9 Candidate WordNet synsets are found for KnowBERT using a similar procedure as we use to find all possible synsets.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>) and from using WNIC over WNIC-noWSD (middle right column) for each category. The number of linked WordNet synsets for each category and their exemplars are shown in the right columns. Results are shown for the Morrow and Duffy (2005) young adult dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="13,306.14,114.42,226.77,340.16" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>PPMI-SVD) and predictionbased (Word2Vec skip-gram negative sampling) algorithms. For baseline details, see Appendix A.4. BERT-based Methods Our first main result is that the BERT-based methods 5 all improve over baselines on the M&amp;D datasets. For the Rosch dataset, only BERT-MPro and Misra et al. (2021)'s method improve over Word2Vec. BERT-MPro has the highest correlations of the BERT based methods across datasets, with Misra et al. (2021)'s method only slightly edging it on the Rosch dataset.</figDesc><table><row><cell></cell><cell>Taxonomic Sentence Verification We use the</cell></row><row><cell></cell><cell>Taxonomic Sentence Verification method of Misra</cell></row><row><cell></cell><cell>et al. (2021), using bert-base-uncased. The dif-</cell></row><row><cell></cell><cell>ferences with BERT-MLM-Taxo can be found in</cell></row><row><cell></cell><cell>Sec. 4.1.3.</cell></row><row><cell></cell><cell>5.3 Results and Analysis</cell></row><row><cell>(young adult ratings) and two from Morrow and Duffy (2005) (young and older adult</cell><cell>5.3.1 Method Performance</cell></row><row><cell>ratings). The two dataset sources were produced</cell><cell>Mean Spearman correlations for all methods are</cell></row><row><cell>at different time periods (1975 for Rosch and 2005</cell><cell>reported in Table 1. 4</cell></row><row><cell>for M&amp;D), with different sample sizes (209 partic-</cell><cell></cell></row><row><cell>ipants and 54, respectively) and potential cultural</cell><cell></cell></row><row><cell>differences (US residents and UK residents, respec-</cell><cell></cell></row><row><cell>tively) and with different protocols. Human ratings</cell><cell></cell></row><row><cell>were obtained by scoring a taxonomic sentence for</cell><cell></cell></row><row><cell>Rosch and by scoring category-exemplar pairs for</cell><cell></cell></row><row><cell>M&amp;D. The datasets contain varying number of cat-</cell><cell></cell></row><row><cell>egories (11 for M&amp;D, 10 for Rosch) and exemplars</cell><cell></cell></row><row><cell>per category (29-128 for M&amp;D, 42-54 for Rosch),</cell><cell></cell></row><row><cell>with 7 overlapping categories between the two</cell><cell></cell></row><row><cell>sources. 3 The human ratings between the two splits</cell><cell></cell></row><row><cell>of M&amp;D are strongly correlated (average category</cell><cell></cell></row><row><cell>Spearman correlation: 0.857, see Appendix A.2).</cell><cell></cell></row><row><cell>However, the correlation between the Rosch rank-</cell><cell></cell></row><row><cell>ings and M&amp;D rankings on the category-exemplar</cell><cell></cell></row><row><cell>intersection of the datasets are much lower, likely</cell><cell></cell></row><row><cell>because of the differences outlined above (average</cell><cell></cell></row><row><cell>category Spearman: 0.656 and 0.574 for Rosch vs</cell><cell></cell></row><row><cell>M&amp;D young and older adults, respectively). We</cell><cell></cell></row><row><cell>follow the preprocessing procedure of Heyman and</cell><cell></cell></row><row><cell>Heyman (2019), (see Appendix A.1). Our auxiliary</cell><cell></cell></row><row><cell>corpus S is extracted by sampling sentences from</cell><cell></cell></row><row><cell>Wikipedia, described in Appendix A.3.</cell><cell></cell></row><row><cell>5.2 Baseline and Competing Methods</cell><cell></cell></row><row><cell>Word Frequency The first baseline is typicality</cell><cell></cell></row><row><cell>scores as the exemplar's number of occurrences in</cell><cell></cell></row><row><cell>the corpus S. It is natural to think that more typical</cell><cell></cell></row><row><cell>examples are more popular words, and Heyman</cell><cell></cell></row><row><cell>and Heyman (2019) show that frequency partly</cell><cell></cell></row><row><cell>accounts for human ratings in the M&amp;D datasets.</cell><cell></cell></row><row><cell>PPMI-SVD, Word2Vec Other baselines are the</cell><cell></cell></row><row><cell>cosine similarities between the category-exemplar</cell><cell></cell></row><row><cell>static word embeddings from count-based (singular</cell><cell></cell></row><row><cell>value decomposition of positive pointwise mutual</cell><cell></cell></row></table><note><p><p><p>3 See Table</p>4</p>in Appendix for category/exemplar statistics. information matrix:</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 ,</head><label>1</label><figDesc></figDesc><table><row><cell cols="2">Method Class Method</cell><cell cols="3">M&amp;D-Young M&amp;D-Old Rosch</cell></row><row><cell>Baselines</cell><cell>Frequency</cell><cell>0.323</cell><cell>0.296</cell><cell>0.059</cell></row><row><cell></cell><cell>PPMI-SVD</cell><cell>0.236</cell><cell>0.227</cell><cell>0.294</cell></row><row><cell></cell><cell>W2V</cell><cell>0.260</cell><cell>0.296</cell><cell>0.338</cell></row><row><cell>BERT</cell><cell>BERT-SentEmb</cell><cell>0.381</cell><cell>0.373</cell><cell>0.289</cell></row><row><cell></cell><cell>BERT-MLM</cell><cell>0.354</cell><cell>0.358</cell><cell>0.238</cell></row><row><cell></cell><cell>BERT-MLM-Taxo</cell><cell>0.396</cell><cell>0.393</cell><cell>0.303</cell></row><row><cell></cell><cell>Misra et al</cell><cell>0.338</cell><cell>0.280</cell><cell>0.396</cell></row><row><cell></cell><cell>BERT-Avg</cell><cell>0.428</cell><cell>0.405</cell><cell>0.298</cell></row><row><cell></cell><cell>BERT-MPro</cell><cell>0.473</cell><cell>0.451</cell><cell>0.386</cell></row><row><cell>WordNet</cell><cell>WNSP-noWSD</cell><cell>0.114</cell><cell>0.045</cell><cell>0.266</cell></row><row><cell></cell><cell>WNIC-noWSD</cell><cell>0.152</cell><cell>0.128</cell><cell>0.318</cell></row><row><cell></cell><cell>WNSP</cell><cell>0.213</cell><cell>0.136</cell><cell>0.295</cell></row><row><cell></cell><cell>WNIC</cell><cell>0.467</cell><cell>0.456</cell><cell>0.448</cell></row><row><cell>Ensemble</cell><cell>BERT-MPro WNIC</cell><cell>0.547</cell><cell>0.531</cell><cell>0.528</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Mean across categories (See Table7in Appendix A.6 for results by category) of Spearman correlations for each method for the three datasets (p &lt; .001). Largest correlations among singular methods for each dataset are bold-faced for BERT-based methods and WordNet-based methods. The last line is the overall best method and scores are also bold-faced. Note that for BERT-AVG, BERT-MPro, and BERT-SentEmb, the best performing layer is shown (layers 1, 10, and 11, respectively; see Appendices A.7 and A.8 for a layer wise analysis).</figDesc><table><row><cell>Category</cell><cell cols="5">MPro WNIC MPro + WNIC MPro vs WNIC Avg. Increase</cell></row><row><cell>Animals</cell><cell>0.665</cell><cell>0.522</cell><cell>0.697</cell><cell>0.524</cell><cell>0.125</cell></row><row><cell>Birds</cell><cell>0.461</cell><cell>0.391</cell><cell>0.533</cell><cell>0.331</cell><cell>0.100</cell></row><row><cell>Clothes</cell><cell>0.498</cell><cell>0.491</cell><cell>0.541</cell><cell>0.570</cell><cell>0.024</cell></row><row><cell>Flowers</cell><cell>0.206</cell><cell>0.148</cell><cell>0.224</cell><cell>0.351</cell><cell>0.006</cell></row><row><cell>Fruits</cell><cell>0.486</cell><cell>0.365</cell><cell>0.469</cell><cell>0.397</cell><cell>0.076</cell></row><row><cell>Furniture</cell><cell>0.657</cell><cell>0.635</cell><cell>0.763</cell><cell>0.369</cell><cell>0.140</cell></row><row><cell>Insects</cell><cell>0.366</cell><cell>0.429</cell><cell>0.408</cell><cell>0.476</cell><cell>0.043</cell></row><row><cell cols="2">Instruments 0.527</cell><cell>0.656</cell><cell>0.620</cell><cell>0.498</cell><cell>0.038</cell></row><row><cell>Tools</cell><cell>0.366</cell><cell>0.584</cell><cell>0.607</cell><cell>0.274</cell><cell>0.134</cell></row><row><cell>Vegetables</cell><cell>0.579</cell><cell>0.600</cell><cell>0.739</cell><cell>0.346</cell><cell>0.102</cell></row><row><cell>Vehicles</cell><cell>0.389</cell><cell>0.314</cell><cell>0.418</cell><cell>0.529</cell><cell>0.053</cell></row><row><cell>Mean</cell><cell>0.473</cell><cell>0.467</cell><cell>0.547</cell><cell>0.424</cell><cell>0.077</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Category wise Spearman correlations between each method and human rankings from (first three columns), Spearman correlations between BERT-MPro and WNIC rankings (fourth column), and the increase of performance of the ensemble method over MPro and WNIC (increases are averaged over the two individual methods). Results are shown for the Morrow and Duffy young adult dataset.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Rankings of certain exemplars within their category, as scored by WNIC, BERT-MPro, WNIC + BERT-MPro, and the human ranking. Examples are from the Morrow and Duffy young adult dataset.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>Spearman correlations between young adult and older adult typicality rankings for Morrow and Duffy (2005).</figDesc><table><row><cell></cell><cell>Category</cell><cell>Spearman</cell><cell></cell></row><row><cell></cell><cell>Animals</cell><cell>0.916</cell><cell></cell></row><row><cell></cell><cell>Birds</cell><cell>0.868</cell><cell></cell></row><row><cell></cell><cell>Clothes</cell><cell>0.858</cell><cell></cell></row><row><cell></cell><cell>Flowers</cell><cell>0.747</cell><cell></cell></row><row><cell></cell><cell>Fruits</cell><cell>0.891</cell><cell></cell></row><row><cell></cell><cell>Furniture</cell><cell>0.778</cell><cell></cell></row><row><cell></cell><cell>Insects</cell><cell>0.897</cell><cell></cell></row><row><cell></cell><cell cols="2">Instruments 0.909</cell><cell></cell></row><row><cell></cell><cell>Tools</cell><cell>0.791</cell><cell></cell></row><row><cell></cell><cell>Vegetables</cell><cell>0.885</cell><cell></cell></row><row><cell></cell><cell>Vehicles</cell><cell>0.887</cell><cell></cell></row><row><cell></cell><cell>Mean</cell><cell>0.857</cell><cell></cell></row><row><cell></cell><cell cols="3">Young vs Rosch Old vs Rosch N</cell></row><row><cell>Birds</cell><cell>0.561</cell><cell>0.449</cell><cell>43</cell></row><row><cell>Clothes</cell><cell>0.736</cell><cell>0.647</cell><cell>33</cell></row><row><cell>Fruits</cell><cell>0.871</cell><cell>0.680</cell><cell>38</cell></row><row><cell>Furniture</cell><cell>0.762</cell><cell>0.547</cell><cell>20</cell></row><row><cell>Tools</cell><cell>0.535</cell><cell>0.613</cell><cell>25</cell></row><row><cell cols="2">Vegetables 0.326</cell><cell>0.227</cell><cell>22</cell></row><row><cell>Vehicles</cell><cell>0.802</cell><cell>0.852</cell><cell>29</cell></row><row><cell>Mean</cell><cell>0.656</cell><cell>0.574</cell><cell>30</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 :</head><label>6</label><figDesc>Spearman correlations between the young adult and older adult rankings of<ref type="bibr" target="#b24">Morrow and Duffy (2005)</ref> and<ref type="bibr" target="#b32">Rosch (1975)</ref> on the intersection of categories and exemplars between the two datasets. The size of the intersection is shown on the far right column.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 7 :</head><label>7</label><figDesc>Category wise Spearman correlations with human typicality rankings for the four best BERT-based models and the best WordNet-based model.</figDesc><table><row><cell></cell><cell cols="2">Misra et al</cell><cell cols="3">Bert-MLM-Taxo</cell><cell></cell><cell cols="2">Bert-Avg</cell><cell></cell><cell cols="2">Bert-MPro</cell><cell>WNIC</cell></row><row><cell>Category</cell><cell>M&amp;D-</cell><cell>M&amp;D-</cell><cell cols="2">Rosch M&amp;D-</cell><cell>M&amp;D-</cell><cell cols="2">Rosch M&amp;D-</cell><cell>M&amp;D-</cell><cell cols="2">Rosch M&amp;D-</cell><cell>M&amp;D-</cell><cell>Rosch M&amp;D-</cell><cell>M&amp;D-</cell><cell>Rosch</cell></row><row><cell></cell><cell>Y</cell><cell>O</cell><cell>Y</cell><cell></cell><cell>O</cell><cell></cell><cell>Y</cell><cell>O</cell><cell></cell><cell>Y</cell><cell>O</cell><cell>Y</cell><cell>O</cell></row><row><cell>Animals</cell><cell cols="2">0.558 0.525</cell><cell cols="3">0.528 0.508</cell><cell></cell><cell cols="2">0.569 0.530</cell><cell></cell><cell cols="2">0.665 0.649</cell><cell>0.522 0.555</cell></row><row><cell>Birds</cell><cell cols="6">0.284 0.130 0.308 0.442 0.332 0.191</cell><cell cols="3">0.440 0.311 0.203</cell><cell cols="3">0.461 0.317 0.169 0.391 0.310 -0.014</cell></row><row><cell>Clothes</cell><cell cols="6">0.205 0.115 0.461 0.527 0.587 0.171</cell><cell cols="3">0.506 0.528 0.301</cell><cell cols="3">0.498 0.573 0.484 0.491 0.487 0.570</cell></row><row><cell>Fruits</cell><cell cols="6">0.232 0.191 0.354 0.287 0.357 0.278</cell><cell cols="3">0.390 0.338 0.406</cell><cell cols="3">0.486 0.433 0.592 0.365 0.403 0.442</cell></row><row><cell>Furniture</cell><cell cols="6">0.600 0.440 0.548 0.577 0.429 0.484</cell><cell cols="3">0.629 0.598 0.320</cell><cell cols="3">0.657 0.602 0.235 0.635 0.553 0.658</cell></row><row><cell>Flowers</cell><cell cols="2">0.240 0.146</cell><cell cols="3">0.122 0.068</cell><cell></cell><cell cols="2">0.152 0.011</cell><cell></cell><cell cols="2">0.206 0.111</cell><cell>0.148 0.105</cell></row><row><cell>Insects</cell><cell cols="2">0.175 0.328</cell><cell cols="3">0.411 0.379</cell><cell></cell><cell cols="2">0.437 0.473</cell><cell></cell><cell cols="2">0.366 0.503</cell><cell>0.429 0.415</cell></row><row><cell cols="3">Instruments 0.309 0.331</cell><cell cols="3">0.667 0.715</cell><cell></cell><cell cols="2">0.626 0.686</cell><cell></cell><cell cols="2">0.527 0.602</cell><cell>0.656 0.659</cell></row><row><cell>Sports</cell><cell></cell><cell></cell><cell>0.467</cell><cell></cell><cell></cell><cell>0.536</cell><cell></cell><cell></cell><cell>0.530</cell><cell></cell><cell></cell><cell>0.379</cell><cell>0.498</cell></row><row><cell>Tools</cell><cell cols="6">0.421 0.259 0.100 0.066 0.220 0.220</cell><cell cols="3">0.160 0.257 0.040</cell><cell cols="3">0.366 0.318 0.086 0.584 0.486 0.494</cell></row><row><cell>Toys</cell><cell></cell><cell></cell><cell>0.143</cell><cell></cell><cell></cell><cell>-0.001</cell><cell></cell><cell></cell><cell>-0.089</cell><cell></cell><cell></cell><cell>0.398</cell><cell>0.155</cell></row><row><cell>Vegetables</cell><cell cols="9">0.390 0.312 0.241 0.364 0.281 -0.006 0.501 0.397 0.073</cell><cell cols="3">0.579 0.482 0.088 0.600 0.646 0.409</cell></row><row><cell>Vehicles</cell><cell cols="6">0.307 0.301 0.682 0.364 0.444 0.733</cell><cell cols="3">0.293 0.323 0.743</cell><cell cols="3">0.389 0.373 0.651 0.314 0.396 0.701</cell></row><row><cell>Weapons</cell><cell></cell><cell></cell><cell>0.655</cell><cell></cell><cell></cell><cell>0.429</cell><cell></cell><cell></cell><cell>0.454</cell><cell></cell><cell></cell><cell>0.779</cell><cell>0.567</cell></row><row><cell></cell><cell></cell><cell cols="2">Category</cell><cell cols="9">MPro inc. WNIC inc. Cat. Synsets Exem. Synsets</cell></row><row><cell></cell><cell></cell><cell cols="2">Animals</cell><cell cols="2">0.096</cell><cell cols="2">0.517</cell><cell>1</cell><cell></cell><cell cols="2">2.992</cell></row><row><cell></cell><cell></cell><cell cols="2">Birds</cell><cell cols="2">0.021</cell><cell cols="2">0.362</cell><cell>6</cell><cell></cell><cell cols="2">2.712</cell></row><row><cell></cell><cell></cell><cell cols="2">Clothes</cell><cell cols="2">-0.009</cell><cell cols="2">0.397</cell><cell>4</cell><cell></cell><cell cols="2">4.241</cell></row><row><cell></cell><cell></cell><cell cols="2">Flowers</cell><cell cols="2">0.055</cell><cell cols="2">0.028</cell><cell>4</cell><cell></cell><cell cols="2">2.022</cell></row><row><cell></cell><cell></cell><cell cols="2">Fruits</cell><cell cols="2">0.096</cell><cell cols="2">0.409</cell><cell>5</cell><cell></cell><cell cols="2">3.102</cell></row><row><cell></cell><cell></cell><cell cols="2">Furniture</cell><cell cols="2">0.028</cell><cell cols="2">0.165</cell><cell>1</cell><cell></cell><cell cols="2">4.590</cell></row><row><cell></cell><cell></cell><cell cols="2">Insects</cell><cell cols="2">-0.071</cell><cell cols="2">0.432</cell><cell>2</cell><cell></cell><cell cols="2">2.705</cell></row><row><cell></cell><cell></cell><cell cols="4">Instruments -0.099</cell><cell cols="2">0.456</cell><cell>1</cell><cell></cell><cell cols="2">2.423</cell></row><row><cell></cell><cell></cell><cell cols="2">Tools</cell><cell cols="2">0.206</cell><cell cols="2">0.155</cell><cell>8</cell><cell></cell><cell cols="2">5.136</cell></row><row><cell></cell><cell></cell><cell cols="2">Vegetables</cell><cell cols="2">0.078</cell><cell cols="2">0.317</cell><cell>2</cell><cell></cell><cell cols="2">3.345</cell></row><row><cell></cell><cell></cell><cell cols="2">Vehicles</cell><cell cols="2">0.095</cell><cell cols="2">0.222</cell><cell>4</cell><cell></cell><cell cols="2">3.412</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 8 :</head><label>8</label><figDesc>Increase in Spearman's correlation from using BERT-MPro over BERT-Avg (middle left column</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>If restricting synsets to only nouns, the average number of category senses</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_1"><p>2.9 and 3.4 for the two respective datasets, and 2.4 and 3.1 for exemplars.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2"><p>See Table 7 in Appendix A.6 for results by category.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_3"><p>Recall that we use bert-base-uncased.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_4"><p>This was the only dataset used in<ref type="bibr" target="#b23">Misra et al. (2021)</ref>.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_5"><p>They denote this as layers 7-9 in their paper (0-indexed); we index starting at 1 to denote the embedding layer as 0.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_6"><p>As an alternative ensemble method, we present a short study of KnowBERT, a CLM enhanced with WordNet knowledge, in Appendix A.10.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>We would like to thank the three anonymous EACL reviewers for their helpful comments on this paper. This research was funded by <rs type="funder">Inria Exploratory Action COMANCHE</rs>, as well as by the joint <rs type="funder">IM-PRESS</rs> project between <rs type="funder">Inria</rs> and <rs type="funder">DFKI</rs>.</p></div>
			</div>
			<listOrg type="funding">
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Category</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">ALL dolphins are intelligent and SOME are friendly: Probing BERT for nouns&apos; semantic properties and their prototypicality</title>
		<author>
			<persName><forename type="first">Marianna</forename><surname>Apidianaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aina</forename><surname>Garí</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soler</forename></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.blackboxnlp-1.7</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourth Black-boxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP</title>
		<meeting>the Fourth Black-boxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP<address><addrLine>Punta Cana, Dominican Republic</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="79" to="94" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Wordnet: A lexical database organized on psycholinguistic principles</title>
		<author>
			<persName><forename type="first">Richard</forename><surname>Beckwith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christiane</forename><surname>Fellbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Derek</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><forename type="middle">A</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Lexical acquisition: Exploiting on-line resources to build a lexicon</title>
		<imprint>
			<publisher>Psychology Press</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="211" to="232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Interpreting Pretrained Contextualized Representations via Reductions to Static Embeddings</title>
		<author>
			<persName><forename type="first">Rishi</forename><surname>Bommasani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kelly</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.431</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="4758" to="4781" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">When is a bishop not like a rook? when it&apos;s like a rabbi! multiprototype bert embeddings for estimating semantic relationships</title>
		<author>
			<persName><forename type="first">Gabriella</forename><surname>Chronis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katrin</forename><surname>Erk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th Conference on Computational Natural Language Learning</title>
		<meeting>the 24th Conference on Computational Natural Language Learning</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="227" to="244" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Amnesic probing: Behavioral explanation with amnesic counterfactuals</title>
		<author>
			<persName><forename type="first">Yanai</forename><surname>Elazar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shauli</forename><surname>Ravfogel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alon</forename><surname>Jacovi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<idno type="DOI">10.1162/tacl_a_00359</idno>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="160" to="175" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">How contextual are contextualized word representations? Comparing the geometry of BERT, ELMo, and GPT-2 embeddings</title>
		<author>
			<persName><forename type="first">Kawin</forename><surname>Ethayarajh</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1006</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="55" to="65" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">What bert is not: Lessons from a new suite of psycholinguistic diagnostics for language models</title>
		<author>
			<persName><forename type="first">Allyson</forename><surname>Ettinger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="34" to="48" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Let&apos;s play mono-poly: BERT can reveal words&apos; polysemy level and partitionability into senses</title>
		<author>
			<persName><forename type="first">Aina</forename><surname>Garí</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soler</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Marianna</forename><surname>Apidianaki</surname></persName>
		</author>
		<idno type="DOI">10.1162/tacl_a_00400</idno>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="825" to="844" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The distributional inclusion hypotheses and lexical entailment</title>
		<author>
			<persName><forename type="first">Maayan</forename><surname>Geffet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL&apos;05)</title>
		<meeting>the 43rd Annual Meeting of the Association for Computational Linguistics (ACL&apos;05)</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="107" to="114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Vector-space models of semantic representation from a cognitive perspective: A discussion of common misconceptions</title>
		<author>
			<persName><forename type="first">Fritz</forename><surname>Günther</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luca</forename><surname>Rinaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Marelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Perspectives on Psychological Science</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1006" to="1033" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Can predictionbased distributional semantic models predict typicality?</title>
		<author>
			<persName><forename type="first">Tom</forename><surname>Heyman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geert</forename><surname>Heyman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Quarterly Journal of Experimental Psychology</title>
		<imprint>
			<biblScope unit="volume">72</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2084" to="2109" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Estimating the average need of semantic knowledge from distributional semantic models</title>
		<author>
			<persName><forename type="first">Geoff</forename><surname>Hollis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Memory &amp; Cognition</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1350" to="1370" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">The principals of meaning: Extracting semantic dimensions from co-occurrence models of semantics</title>
		<author>
			<persName><forename type="first">Geoff</forename><surname>Hollis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Westbury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychonomic bulletin &amp; review</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1744" to="1756" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Semantic memory: A review of methods, models, and current challenges</title>
		<author>
			<persName><forename type="first">A</forename><surname>Abhilasha</surname></persName>
		</author>
		<author>
			<persName><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychonomic Bulletin &amp; Review</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="40" to="80" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Distributional models of word meaning</title>
		<author>
			<persName><forename type="first">Alessandro</forename><surname>Lenci</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annual review of Linguistics</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="151" to="171" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">An information-theoretic definition of similarity</title>
		<author>
			<persName><forename type="first">Dekang</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Icml</title>
		<imprint>
			<biblScope unit="volume">98</biblScope>
			<biblScope unit="page" from="296" to="304" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m">Roberta: A robustly optimized bert pretraining approach</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">From words-asmappings to words-as-cues: The role of language in semantic knowledge</title>
		<author>
			<persName><forename type="first">Gary</forename><surname>Lupyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Molly</forename><surname>Lewis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Language, Cognition and Neuroscience</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1319" to="1337" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Explaining human performance in psycholinguistic tasks with models of semantic similarity based on prediction and counting: A review and empirical validation</title>
		<author>
			<persName><forename type="first">Paweł</forename><surname>Mandera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emmanuel</forename><surname>Keuleers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Brysbaert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Memory and Language</title>
		<imprint>
			<biblScope unit="volume">92</biblScope>
			<biblScope unit="page" from="57" to="78" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Relationships among goodness-of-example, category norms, and word frequency</title>
		<author>
			<persName><forename type="first">Carolyn</forename><forename type="middle">B</forename><surname>Mervis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><surname>Catlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eleanor</forename><surname>Rosch</surname></persName>
		</author>
		<idno type="DOI">10.3758/bf03337190</idno>
	</analytic>
	<monogr>
		<title level="j">Bulletin of the Psychonomic Society</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="283" to="284" />
			<date type="published" when="1976">1976</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Wordnet: a lexical database for english</title>
		<author>
			<persName><forename type="first">George</forename><forename type="middle">A</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="39" to="41" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Do language models learn typicality judgments from text?</title>
		<author>
			<persName><forename type="first">Kanishka</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Allyson</forename><surname>Ettinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julia</forename><forename type="middle">Taylor</forename><surname>Rayz</surname></persName>
		</author>
		<idno>CoRR, abs/2105.02987</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">The representation of ontological category concepts as affected by healthy aging: Normative data and theoretical implications</title>
		<author>
			<persName><forename type="first">I</forename><surname>Lorna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Morrow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Duffy</forename><surname>Frances</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Behavior research methods</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="608" to="625" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">The big book of concepts</title>
		<author>
			<persName><forename type="first">Gregory</forename><surname>Murphy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Information content measures of semantic similarity perform better without sensetagged text</title>
		<author>
			<persName><forename type="first">Ted</forename><surname>Pedersen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics</title>
		<meeting><address><addrLine>Los Angeles, California</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="329" to="332" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Knowledge enhanced contextual word representations</title>
		<author>
			<persName><forename type="first">Matthew</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><forename type="middle">L</forename><surname>Logan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">V</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Roy</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vidur</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sameer</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<idno>CoRR, abs/1909.04164</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Deep contextualized word representations</title>
		<author>
			<persName><forename type="first">Matthew</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno>CoRR, abs/1802.05365</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Language models as knowledge bases?</title>
		<author>
			<persName><forename type="first">Fabio</forename><surname>Petroni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Rocktäschel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anton</forename><surname>Bakhtin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Miller</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1250</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2463" to="2473" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Using information content to evaluate semantic similarity in a taxonomy</title>
		<author>
			<persName><forename type="first">Philip</forename><surname>Resnik</surname></persName>
		</author>
		<idno>arXiv preprint cmp-lg/9511007</idno>
		<imprint>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">A primer in bertology: What we know about how BERT works</title>
		<author>
			<persName><forename type="first">Anna</forename><surname>Rogers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olga</forename><surname>Kovaleva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anna</forename><surname>Rumshisky</surname></persName>
		</author>
		<idno>CoRR, abs/2002.12327</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Cognitive representations of semantic categories</title>
		<author>
			<persName><forename type="first">Eleanor</forename><surname>Rosch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of experimental psychology: General</title>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">192</biblScope>
			<date type="published" when="1975">1975</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Knowledge-aware language model pretraining</title>
		<author>
			<persName><forename type="first">Corby</forename><surname>Rosset</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenyan</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minh</forename><surname>Phan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xia</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><forename type="middle">N</forename><surname>Bennett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saurabh</forename><surname>Tiwary</surname></persName>
		</author>
		<idno>CoRR, abs/2007.00655</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">The proof and measurement of association between two things</title>
		<author>
			<persName><forename type="first">Charles</forename><surname>Spearman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1904">1904</date>
			<publisher>University of Illinois Press</publisher>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="72" to="101" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Hyperlex: A large-scale evaluation of graded lexical entailment</title>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Vulić</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniela</forename><surname>Gerz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Douwe</forename><surname>Kiela</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anna</forename><surname>Korhonen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="781" to="835" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">BERT has a mouth, and it must speak: BERT as a markov random field language model</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<idno>CoRR, abs/1902.04094</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">K-adapter: Infusing knowledge into pre-trained models with adapters</title>
		<author>
			<persName><forename type="first">Ruize</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Duyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhongyu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianshu</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guihong</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daxin</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<idno>CoRR, abs/2002.01808</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
