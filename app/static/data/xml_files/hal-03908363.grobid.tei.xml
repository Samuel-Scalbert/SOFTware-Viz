<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">When Should We Use Linear Explanations?</title>
				<funder ref="#_MhHbH6K">
					<orgName type="full">Agence Nationale de la Recherche</orgName>
					<orgName type="abbreviated">ANR</orgName>
				</funder>
				<funder ref="#_gyEpmuk">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Julien</forename><surname>Delaunay</surname></persName>
							<email>julien.delaunay@inria.fr</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Inria</orgName>
								<orgName type="institution" key="instit2">IRISA Rennes</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Luis</forename><surname>Galárraga</surname></persName>
							<email>luis.galarraga@inria.fr</email>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">Inria</orgName>
								<orgName type="institution" key="instit2">IRISA Rennes</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Christine</forename><surname>Largouët</surname></persName>
							<email>christine.largouet@irisa.fr</email>
							<affiliation key="aff2">
								<orgName type="department">L&apos;Institut Agro</orgName>
								<orgName type="institution">IRISA Rennes</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">When Should We Use Linear Explanations?</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">AF1CA877DA02743C5F868CBFF005110A</idno>
					<idno type="DOI">10.1145/3511808.3557489</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-04-12T14:47+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Computing methodologies → Machine learning</term>
					<term>Machine learning approaches</term>
					<term>Learning linear models</term>
					<term>Instance-based learning</term>
					<term>Rule learning Interpretability, Explainability, Linear Explanations, Rule-based Explanations, Counterfactual Explanations</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The increasing interest in transparent and fair AI systems has propelled the research in explainable AI (XAI). One of the main research lines in XAI is post-hoc explainability, the task of explaining the logic of an already deployed black-box model. This is usually achieved by learning an interpretable surrogate function that approximates the black box. Among the existing explanation paradigms, local linear explanations are one of the most popular due to their simplicity and fidelity. Despite their advantages, linear surrogates may not always be the most adapted method to produce reliable, i.e., unambiguous and faithful explanations. Hence, this paper introduces Adapted Post-hoc Explanations (APE), a novel method that characterizes the decision boundary of a black-box classifier and identifies when a linear model constitutes a reliable explanation. Besides, characterizing the black-box frontier allows us to provide complementary counterfactual explanations. Our experimental evaluation shows that APE identifies accurately the situations where linear surrogates are suitable while also providing meaningful counterfactual explanations.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>decision support. This interest has given rise to initiatives such as the GDPR 1 , and have propelled the research in explainable AI (XAI), a branch of AI that focuses on models and systems that can explain their decisions to the layman.</p><p>An important line of research in XAI is post-hoc explainability, a subfield of XAI that studies the techniques to compute explanations for the answers of an already deployed system. This may be necessary if the system is either too complex or its specifications inaccessible to the user. The explanation usually consists of a surrogate white-box model that mimics the black box, either globally, i.e., in the general case, or locally, that is, w.r.t. an instance of interest. LIME <ref type="bibr" target="#b16">[17]</ref>, one of the most popular post-hoc explanation methods, relies on local linear surrogates whose coefficients are used to rank the input features according to their contribution to the black box's outcome on an instance of interest.</p><p>Despite the popularity of local linear explanations, they may not always be the most adapted method to explain a black-box outcome. Consider the two cases depicted in Figure <ref type="figure" target="#fig_0">1</ref>. In Figure <ref type="figure" target="#fig_0">1a</ref>, the instance of interest lies in a zone where there is clearly a single local linear approximation for the black-box classifier. In contrast, the target instance in Figure <ref type="figure" target="#fig_0">1b</ref> depicts a scenario where three possible linear explanations are possible. Since these approximations exhibit different inclinations, the attribution scores assigned to the input features are obviously contradictory -a situation that would harden interpretation. While we could provide one of the explanations for Figure <ref type="figure" target="#fig_0">1b</ref>, that would tell an incomplete story.</p><p>Based on the aforementioned arguments, this article proposes APE, which stands for Adapted Post-hoc Explanations, a novel method to determine a priori whether a black-box classifier and a target instance admit a faithful and unambiguous local linear explanation. When this is not the case, APE recommends a different explanation paradigm -a rule-based explanation in our experiments. APE operates by characterizing the classifier's decision boundary, which is achieved by identifying the target's closest counterfactual instance. Counterfactual instances (also called enemies) are instances that are close to the target instance but are classified differently by the black box. Such instances can be used as contrastive explanations that highlight the minimal changes required on the target instance to change the classifier's outcome. All in all, our contributions are: • A definition of suitability for explanations based on local linear surrogates. This definition builds upon existing notions such as adherence and locality, which we also define formally. • The Growing Fields (GF) algorithm for counterfactual search.</p><p>GF extends the Growing Spheres (GS) algorithm <ref type="bibr" target="#b12">[13]</ref> to account for categorical attributes as well as the distribution of the input features using the standardized Euclidean distance as metric. • The APE oracle, a linear suitability test that tells users whether a black-box classifier can be locally approximated by a single and faithful linear surrogate. To do so, APE characterizes the distribution of the instances around the decision boundary. • The APE algorithm that returns a linear explanation if suitable.</p><p>Otherwise APE proposes a rule-based explanation. In all cases APE computes complementary counterfactual explanations. The article is structured as follows. After formulating the problem and introducing preliminary concepts in Section 2, Section 3 elaborates on our approach. Then we evaluate APE on a handful of datasets and classifiers in Section 4. This is followed by a survey of the related work in Section 5, and a discussion of our insights. 𝑋 , our goal is to construct an oracle that tells us whether a linear surrogate 𝑔 learned on a locality Φ ⊂ 𝑋 (defined below) is suitable to explain 𝑓 (𝑥). By "suitable" we mean that two contradictory linear explanations 𝑔, and 𝑔 ′ may not have the highest adherence in Φthe adherence being the outcome agreement between 𝑓 and 𝑔. In this formulation, 𝑌 is a finite set of classes, 𝑋 is a multidimensional domain defined on numerical and categorical features, and Φ is a region of the space that (i) covers 𝑥, (ii) is traversed by 𝑓 's decision boundary, and (iii) is maximal, otherwise stated, the surrogate 𝑔 cannot attain the quality guarantee 𝑚(𝑔) ≥ 𝜏 in any locality Φ ′ ⊃ Φ for some adherence metric 𝑚. Table <ref type="table" target="#tab_0">1</ref> provides an overview of the notation used throughout the paper.</p><p>Requirement (i) guarantees that the target instance 𝑥 is included in the surrogate's training set. Moreover, requirement (ii) ensures that this training set is balanced, that is, it contains both instances inside and outside the class 𝑓 (𝑥). It follows that the minimal locality satisfying these two requirements should be centered on the decision boundary -more precisely on 𝑥's closest counterfactual -, and have the target instance 𝑥 on the boundary. This is depicted by the inner dotted circle in Figure <ref type="figure" target="#fig_1">2</ref>. Requirement (iii) implies that Φ could actually be larger if the surrogate 𝑔 still attains a good adherence as depicted by the bigger dashed circle in Figure <ref type="figure" target="#fig_1">2</ref>. In such a case, the explanation generalizes to larger regions of the data space.  In line with existing approaches to compute local explanations <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b17">18]</ref>, we learn 𝑔 on a sample of instances issued from a generative process that produces artificial instances 𝑧 ∈ 𝑍 ⊂ Φ in 𝑥's neighborhood. If available, we also consider real instances that fall in the neighborhood, i.e., training instances 𝑡 ∈ 𝑇 ∩ Φ. We call a counterfactual or an enemy <ref type="bibr" target="#b12">[13]</ref> any instance 𝑒 ∈ 𝐸 ⊂ 𝑋 such that 𝑓 (𝑒) ≠ 𝑓 (𝑥). Conversely, if 𝑓 (𝑥 ′ ) = 𝑓 (𝑥), we say that 𝑥 ′ is a friend of 𝑥. Counterfactuals close to the target instance 𝑥 can serve as informative contrastive explanations for 𝑓 (𝑥).</p><p>We say two linear explanations 𝑔 and 𝑔 ′ for 𝑓 (𝑥) are contradictory if they induce different attribute rankings, more formally, if 𝑅(𝑔) ≠ 𝑅(𝑔 ′ ). We remark that the implementations of existing linear explanation modules may be subject to minor stability issues due to the non-determinism (randomization) in the instance generation process. This may produce different linear explanations across multiple executions of the module on the same inputs. That said, such issues mostly affect the individual rankings within 𝑅 + (𝑔) and 𝑅 -(𝑔). In other words, instability episodes will rarely change the sign of the feature attribution. We therefore assume that signs are stable across multiple executions of the explanation module on the same input. Adherence and Fidelity. The quality of a surrogate model 𝑔 for a black-box classifier 𝑓 is evaluated through the notions of adherence and fidelity. The adherence of a surrogate model 𝑔 for a black-box model 𝑓 is the degree of agreement between 𝑓 's and 𝑔's outcomes. The fidelity, on the other hand, assesses the surrogate's ability to identify the features truly employed by the black-box model. When 𝑓 is a true black box, users can only rely on the adherence to estimate the quality of explanations. Existing Methods. LIME <ref type="bibr" target="#b16">[17]</ref> is the most prominent approach to compute local linear explanations. For tabular data, LIME learns the surrogate 𝑔 on a weighted neighborhood 𝑍 ⊂ Φ generated by perturbing the numerical attributes of 𝑥 according to a 𝜇-centered and 𝜎-scaled normal distribution, where 𝜇 and 𝜎 are the attribute's mean and std. deviation in the training set. For categorical attributes, LIME uses the empirical distribution of the attribute values. The neighbors' weights are assigned according to an exponential kernel on the 𝑙 2 -distance to 𝑥 so that closer neighbors are given more importance when learning the surrogate. It has been shown <ref type="bibr" target="#b13">[14]</ref> that we can learn more locally faithful explanations if we apply LIME on a neighborhood traversed by 𝑓 's decision boundary. In that vibe, the Local Surrogate (LS) approach <ref type="bibr" target="#b13">[14]</ref> centers the generative process not on the target instance 𝑥 but on its closest enemy 𝑒which by itself constitutes a complementary explanation for 𝑓 (𝑥). LS then learns a linear surrogate on a neighborhood defined by a hyper-sphere centered at 𝑒, as depicted by the inner circle in Figure <ref type="figure" target="#fig_1">2</ref>. On the downside, LS does not support categorical attributes. One of our contributions -the Growing Fields algorithm -proposes a solution to this limitation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">ADAPTED POST-HOC EXPLANATIONS</head><p>We now elaborate on APE, our approach to compute adapted posthoc explanations on tabular data for a target instance 𝑥 and a blackbox classifier 𝑓 . When the decision frontier of 𝑓 admits a single local linear surrogate according to our problem statement in Section 2, APE returns a linear-based explanation complemented with a counterfactual explanation. Otherwise, APE recommends a different explanation paradigm such as a rule-based surrogate.</p><p>APE is detailed in Algorithm 1. In a first stage (line 1), APE invokes the Growing Fields algorithm to find the black-box decision boundary. This is achieved by identifying 𝑥's closest enemy -denoted by 𝑒. Then, APE generates a set of random instances 𝑍 uniformly distributed in a locality around 𝑒 (line 3). This locality constitutes a field, which APE samples using the F generation process explained later. The size of the field depends on a radius parameter that is proportional to dist (𝑥, 𝑒), i.e., the distance between 𝑥 and its closest enemy. More precisely, we set 𝑟 = 1 /𝛿 × dist (𝑥, 𝑒), where 𝛿 is the farthest distance from 𝑥 to a real instance in 𝑇 , i.e., 𝑓 's training set. By normalizing the radius, we (a) provide users with a data-agnostic notion of distance, and (b) reduce the risk of sampling instances beyond the limits of the attribute domains. By centering the generative process at 𝑒 with radius 𝑟 , APE makes sure that 𝑍 covers 𝑥 and contains diverse subsets 𝐸 and 𝐹 of friends and enemies of 𝑥 -in concordance with the requirements (i) and (ii) in the problem statement in Section 2. The F generation procedure as well as the Growing Fields algorithm are detailed in Section 3.1.</p><p>In the next step (line 4), APE characterizes the decision boundary of 𝑓 . To this end, the algorithm invokes the APE oracle (Section 3.2), which runs efficient unimodality and linear separability tests <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b22">23]</ref> on 𝐸 and 𝐹 to determine whether a linear surrogate is suitable or not. The oracle recommends a linear explanation if both sets 𝐸 and 𝐹 exhibit a unimodal distribution, that is, if there is only one cluster per class and we can separate those clusters with a single linear surrogate. In that case, APE returns a linear explanation and the closest enemy of 𝑥 as a counterfactual explanation for 𝑓 (𝑥). The linear explanation is learned via an extension of Local Surrogate <ref type="bibr" target="#b13">[14]</ref>, called LS APE , applied on a superset of 𝑍 , consisting of real and artificial instances. Those instances constitute a field with a radius of at least 𝑟 . We elaborate on those details in Section 3.3.</p><p>When the APE oracle deems linear explanations unsuitable, namely because the instances in 𝐸 or 𝐹 form multiple clusters, or because 𝑍 is not linearly separable, APE proposes a rule-based surrogate. Alternatives are Anchors <ref type="bibr" target="#b17">[18]</ref> or shallow decision trees. In the first case, the user obtains a single rule of the form 𝑅 : 𝑝 ⇒ 𝑓 (𝑥) where 𝑝 is a set of conditions, and 𝑅 has a precision of at least 𝜏 <ref type="bibr" target="#b17">[18]</ref>. In the second case, the user gets a decision tree trained on a superset of 𝑍 . Since the decision boundary may consist of several disconnected instance clusters, APE completes its explanation with a counterfactual instance per cluster in 𝐸 (see Section 3.4). That way users can have a comprehensive view of the different ways to change the black box's outcome 𝑓 (𝑥).</p><p>In the next sections we elaborate on APE's building blocks, namely the F instance generation process, the Growing Fields algorithm, the APE oracle, and the procedures to compute the linear and rule-based surrogates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 APE</head><p>Require: a training dataset 𝑇 ⊂ 𝑋 , a target instance 𝑥 = (𝑥 1 , . . . , 𝑥 𝑑 ) ∈ 𝑋 , a black-box classifier 𝑓 : 𝑋 → 𝑌 ; number of samples 𝑛 Ensure: one or multiple counterfactual instances, a surrogate classifier 𝑔</p><formula xml:id="formula_0">1: 𝑒 ← GROWING FIELDS (𝑇 , 𝑥, 𝑓 ) 2: 𝑟 ← 1 /𝛿 × dist (𝑥, 𝑒) // 𝛿 is the largest distance in 𝑇 3: 𝑍 ∼ F (𝑇 , 𝑟, 𝑒) 𝑖 ≤𝑛 4: if APE ORACLE (𝑍, 𝑥, 𝑓 ) then 5:</formula><p>return 𝑒, 𝐿𝑆 APE (𝑍, 𝑓 , 𝑥, 𝑒) trained on 𝑒-centered field of radius 𝑟 ′ ≥ 𝑟 6: else</p><formula xml:id="formula_1">7: return {𝑒 1 , . . . , 𝑒 𝑘 } ⊂ 𝑍 , RULE-BASED SURR. (𝑓 ) 8: end if</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Growing Fields</head><p>To compute the closest enemies to a target instance 𝑥 given a classifier 𝑓 (line 1 in Algorithm 1), APE resorts to an enhancement of the Growing Spheres (GS) algorithm <ref type="bibr" target="#b12">[13]</ref> that we call Growing Fields (GF). GS searches for enemies of 𝑥 by drawing instances uniformly within the volume of a 𝑙 2 -sphere of radius 𝑟 centered at 𝑥. The value of 𝑟 is adjusted so that the resulting sphere traverses 𝑓 's decision boundary and encompasses enemies of 𝑥 lying close to the border. GF proceeds likewise, but tackles some of the limitations of GS as explained next. Attribute-dependant perturbations. By drawing instances uniformly in a 𝑙 2 -sphere, GS assumes that all numerical attributes should be perturbed at the same rate. In reality, the attributes may have different amplitudes, variances, and distributions. Consequently, in GF the perturbation added to a numerical attribute 𝑥 𝑖 follows a uniform distribution that depends on both the radius 𝑟 and the attribute's domain amplitude 𝐴 𝑖 (𝑇 ), and at the same time preserves the attribute's std. deviation in the input dataset 𝑇 -denoted by 𝜎 𝑖 (𝑇 ). This implies that the vicinity generated by GF around an instance 𝑥 is not anymore a sphere, but rather a volume or, as we call it, a field. The actual shape of this field depends on the distance function. We highlight that taking into account the data distribution guarantees a data-aware exploration of the space, which results in a speed-up of up to 2 orders of magnitude w.r.t. GS. Distance. Another limitation of GS is that all attributes have the same impact when computing the distance between two instances. That said, a salary "distance" of 30 EUR is insignificant compared to an age "distance" of 30 years. On those grounds, APE normalizes the contribution of attribute 𝑖 using the mean 𝜇 𝑖 and std. deviation 𝜎 𝑖 in the training set, which boils down to the standardized Euclidean distance <ref type="foot" target="#foot_0">2</ref> :</p><formula xml:id="formula_2">dist (𝑥, 𝑥 ′ ) = 𝑑 ∑︁ 𝑖=1 ( (𝑥 𝑖 -𝜇 𝑖 ) (𝑥 ′ 𝑖 -𝜇 𝑖 ) 𝜎 𝑖 ) 2<label>(1)</label></formula><p>Equation 1 assumes that the categorical attributes have been one-hot encoded.</p><p>Support for categorical features. The original GS algorithm does not support categorical attributes such as the sex or the marital status of a person. We can now handle those attributes by treating them as random continuous variables uniformly distributed in [0, 𝑟 ]. Consider a field with radius 𝑟 = 0.5 and a target instance with the attribute sex = 𝐹 . If by drawing a random value in [0, 0.5] we obtain for example, a value of 0.2, we interpret it as throwing a biased coin that keeps the sex of the target instance with probability 1 -0.2 = 0.8. If the attribute defines more than two categories, e.g., {single, married, divorced, widowed} and we have to change the category, we use the re-adjusted empirical probabilities of the other categories in the input dataset 𝑇 to randomly choose the new category. Algorithm 2 details the resulting generation process, called F (which stands for field), used to draw random artificial instances with both numerical and categorical attributes. The result of integrating F into GS gives rise to the Growing Fields algorithm detailed in Algorithm 3. Growing Fields starts with an initial field of radius 𝑟 0 and reduces it until no enemies are found (lines 3-6). In a second stage, the field is gradually expanded until the decision boundary is crossed and close counterfactual instances can be reported (lines 7-10). The algorithm then returns 𝑥's closest counterfactual. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">APE Oracle</head><p>The core of the APE algorithm is the APE oracle described in Algorithm 4. This oracle determines whether the black-box decision boundary is separable by a single linear approximation. To achieve this, the oracle applies the Libfolding unimodality test <ref type="bibr" target="#b19">[20]</ref> separately on the sets of friends 𝐹 and enemies 𝐸 of the target instance 𝑥 in 𝑍 . If the test is passed, it means that 𝐹 and 𝐸 form each a single cluster in 𝑍 . This, however, does not suffice for linear separability; ergo the oracle carries out a quick linear separability test to determine whether these clusters of friends and enemies can be told apart with a linear approximation. The test is actually carried out on a balanced sample 𝑍 𝑏 ⊆ 𝑍 . We enforce 𝑍 𝑏 to contain an equal number of friends and enemies of 𝑥, because 𝑍 can be highly imbalanced towards the enemies of 𝑥 for very small localities.</p><p>There are multiple methods to determine whether there exists a linear function that separates a two-class dataset. Such methods range from linear and quadratic programming to approaches based on computational geometry and neural networks <ref type="bibr" target="#b3">[4]</ref>. Nevertheless, all these strategies are at least as expensive as running a linear regression on the input dataset. On those grounds, APE resorts to a simple test based on the Thornton's separability index si <ref type="bibr" target="#b22">[23]</ref>. If Γ 𝑋 ′ (𝑥) returns the closest neighbor 𝑥 ′ of 𝑥 in a set 𝑋 ′ ⊆ 𝑋 , the separability index measures the ratio of instances for which that closest neighbor is a friend of 𝑥. In our setting, this can be computed according to the following formula:</p><formula xml:id="formula_3">si(𝑋 ′ ) = 𝑥 ′ ∈𝑋 ′ 1 𝑓 (Γ 𝑋 ′ (𝑥 ′ ))=𝑓 (𝑥 ′ ) |𝑋 ′ | .</formula><p>We remark that si lies between 0 and 1 and that higher values denote higher separability. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Linear Explanations</head><p>If the APE oracle estimates that 𝑓 's decision boundary is linearly separable around the target instance 𝑥, APE resorts to the routine LS APE (described in Algorithm 5) to learn a linear surrogate 𝑔 on 𝑍 and to provide an explanation for 𝑓 (𝑥). We could center the generative process to learn 𝑔 on the target instance 𝑥 as in standard LIME, or around the decision boundary as in LS. We opt for the latter alternative since LS has been shown to identify more accurately the features that influence the black box locally <ref type="bibr" target="#b13">[14]</ref>.</p><p>We recall that 𝑍 is a sample drawn from a field centered on 𝑒 with radius 𝑟 = 1 /𝛿 × dist(𝑥, 𝑒) where 𝑒 is 𝑥's closest enemy. We could therefore learn 𝑔 from the instances used for the linear separability test, because these are exactly what LS needs for training. We highlight, however, that nothing prevents our linear surrogate from attaining a good adherence in larger scopes. In concordance with our maximality requirement (Section 2), LS APE carries out a posteriori expansion of the training field before reporting the linear explanation to the user. While the adherence does not decrease, that is while 𝑚(𝑔) ≥ 𝜏, LS APE extends the field radius and trains a new linear explanation (line 5-10 in Algorithm 5). The threshold 𝜏 is set to the adherence of 𝑔 in the initial field. The radius is increased using the same expansion strategy of Growing Fields (lines 8-9 in Algorithm 3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Rule-based Explanations</head><p>If the decision frontier in the vicinity of our target instance is too complex to be approximated with a single linear surrogate, users may apply clustering techniques on the neighborhood 𝑍 and provide different linear explanations for each of the instance clusters at the decision boundary. This would provide a complete picture of the black box behavior around the target. However, such an explanation 𝑔 ← LINEAR REGRESSION (𝑍 train , 𝑓 (𝑍 train ))</p><p>10:</p><p>𝑎 ← 𝑚 (𝑔) on 𝑍 test 11: end while 12: return 𝑔 is potentially difficult to grasp for users, because it might consist of potentially contradicting feature-attribution rankings. On those grounds, APE proposes by default a rule-based explanation when linear surrogates are considered unsuitable. Alternatives are anchors or shallow decision trees. Anchors <ref type="bibr" target="#b17">[18]</ref> learns a single explanation rule of the form 𝑝 ⇒ 𝑓 (𝑥) such that 𝑝 is a conjunction of conditions of maximal coverage and the rule has a precision of at least 𝜏. The decision tree is learned on the set 𝑍 containing both friends 𝐹 and enemies 𝐸 of 𝑥 in the field centered on 𝑒, the closest enemy of 𝑥. We remark, nevertheless, that our framework could be coupled with other explanation approaches <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b14">15]</ref>. This is an interesting avenue for future research.</p><p>Finally, APE complements the rule-based explanation with a set of counterfactual instances. These are the centroids of the clusters defined by an extended set of enemies 𝐸 * ⊇ 𝐸 (generated using the F generation process from Algorithm 2). This set can be obtained by increasing the field ratio 𝑟 while the precision of the explanation is above 𝜏. The clusters are computed using K-means <ref type="bibr" target="#b10">[11]</ref> and the number of clusters 𝑘 is determined using the Elbow method <ref type="bibr" target="#b21">[22]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head><p>We conduct four rounds of experiments to evaluate APE:</p><p>• The first round of experiments (Section 4.1) assesses APE's oracle, specifically its ability to distinguish the cases where a linear explanation can yield a single accurate approximation for a given black-box classifier and target instance. • In the second round, we compare APE's explanations to those of LIME <ref type="bibr" target="#b16">[17]</ref> and LS <ref type="bibr" target="#b13">[14]</ref> in terms of adherence (Section 4.3). • In a third round (Section 4.4), we conduct an ablation study of the two components of the APE oracle through an evaluation of their impact on the adherence of APE. • The last round in Section 4.5 compares the quality of APE's counterfactual explanations -computed with Growing Fieldswith those output by Growing Spheres <ref type="bibr" target="#b12">[13]</ref>.</p><p>The source code of APE as well as the experimental datasets and additional results are available on Github<ref type="foot" target="#foot_1">3</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Setup</head><p>Datasets. Table <ref type="table" target="#tab_4">2</ref> describes our experimental datasets. The list comprises 6 real and 6 synthetic datasets, the latter generated with scikitlearn <ref type="foot" target="#foot_2">4</ref> . Five of those synthetic datasets contain only numerical features. The real datasets were chosen to provide a mix of numerical and categorical features. All datasets define two target classes. We highlight though, that a multi-class classification problem can always be formulated in terms of a set of binary classification problemsone per class. In addition to the class of an instance, the classifiers can provide class probabilities. The classifiers were trained on 70% of the data points and their accuracy tested on the remaining 30%. They exhibit accuracy scores between 0.65 and 0.99. Explanation modules. APE and the competitors were tested on a random sample of 100 target instances drawn from the test instances of the experimental datasets. All the explanation modules had access to the training set used to learn the classifiers (argument 𝑇 in Algorithm 1). We tested APE with Anchors and shallow decision trees (maximal depth of 3) as explanation solutions when linear explanations are considered unsuitable. We denote these variants by APE 𝑎 and APE 𝑡 . Anchors requires a precision goal 𝜏 for rules, that we set to 0.95. Nevertheless, the semantics of 𝜏 are purely indicative, because the algorithm will always report an explanation even if this goal is not attainable in the surrogate's training set. In line with LIME and LS, the training instances for learning the linear surrogate are labeled with the class probabilities of the target class 𝑓 (𝑥) output by the black-box classifiers.</p><p>Metrics. We measure the adherence of our explanations via the accuracy score of the surrogate models on the region (e.g., field) where they were trained. We use 60% of the generated artificial instances (lines 5 and 7 in Algorithm 1) for training the surrogates and keep 40% for evaluating their accuracy. When we know the features actually used by the input classifier, we measure the explanation fidelity through the precision and the Kendall rank correlation coefficient on the sets of features reported by the explanations. The precision score gives the proportion of features in the explanation that are indeed used by the black-box classifier. The Kendall coefficient quantifies the agreement between the feature attribution rankings of the explanation and the actual contribution ranking in the black-box.</p><p>We evaluate the APE oracle by comparing the adherence and fidelity of the linear surrogates learned with LS APE across the two outcomes of the oracle.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">APE Oracle Evaluation</head><p>Adherence Evaluation. Table <ref type="table" target="#tab_5">3</ref> presents the mean adherence (accuracy) of the linear surrogates computed for each black-box classifier across 100 test instances on our experimental datasets. The surrogates were computed using LS APE . For each target instance, the APE oracle determines whether or not the decision boundary admits a single accurate linear approximation (Yes or No). The results show the pertinence of APE's linear suitability test. When the oracle predicts a linearly separable decision boundary, the surrogate's accuracy is on average 0.124 points higher than in the opposite case. Moreover, we observe that the proportion of linearly separable cases is mostly explained by the dataset. That said, the architecture of the black-box classifier can also have an impact on this proportion as suggested by the Adult dataset where 25% of the target instances of the Voting Ensemble (VC) are deemed unsuitable for a linear explanation, in contrast to the other datasets for which this proportion is higher. This happens in contrast to the Gradient Boosting (GB) classifiers where 65% of the target instances do not admit a linear explanation according to the oracle. We remark, however, that even when the oracle rejects linear suitability, the adherence of the linear surrogate can still be high, e.g., Cat Blobs dataset with GB black box. This can be explained by the fact that multimodal, e.g., clustered data, can still exhibit some level of linear separability if the individual clusters contain mostly instances of the same class. In such cases APE favors a rule-based explanation with multiple counterfactual instances in order to highlight the complexity of the decision boundary and illustrate the different ways to change the classifier's outcome. That is why APE tests first for unimodality and then for linear separability.</p><p>The interest of the APE oracle can be illustrated through this example drawn from the moons dataset -which contains 2 features. The Libfolding unimodality test on the set of closest enemies 𝐸 around the target instance 𝑥 = [1.37, -0.65] detects a multimodal distribution, and the k-elbow method reports three enemy clusters whose centers are 𝑧 1 = [1.23, 0.25], 𝑧 2 = [0.90, -0.07], and 𝑧 3 = [0.76, 0.26]. Applying LS APE on those counterfactual instances as centers of the generative process reveals contradictory explanations, since the attribution of the first feature for 𝑧 1 is 0.079 whereas it is -0.003 for 𝑧 3 . Fidelity Evaluation. To compare the fidelity of the linear surrogates across the two possible outcomes of the oracle, we resort to a set Columns "Yes" and "No" are the average accuracy of LS APE when the oracle indicates that a linear explanation is suitable or unsuitable. \ denotes a non-meaningful accuracy score, i.e., there were less than 3 instances in that case. Columns Prop 𝑛𝑜 denote the ratio of cases when the oracle does not predict linear suitability. The colors blue, orange, and red indicate Prop 𝑛𝑜 ≤ 33%, 33% &gt; Prop 𝑛𝑜 ≥ 66%, and Prop 𝑛𝑜 ≥ 66% respectively. Each row reports the results for a particular dataset, such as Adult in the first row.</p><p>of "glass" black-box classifiers, i.e., white-box classifiers treated as black boxes. The classifiers are trained on half of the dataset features, which we chose randomly. We restrict our evaluation to datasets with at least 8 features. We apply LS APE and use as explanation the ranking given by the top half features (by the absolute value of the attribution coefficient) of the linear surrogates. Figure <ref type="figure" target="#fig_2">3</ref> depicts the Kendall rank correlation coefficient for gradient boosting (GB), decision tree (DT), random forest (RF), and logistic regression (LR) classifiers. For LR, the ground truth is given by the feature coefficients of the logistic function. Similarly, we can extract the ground truth for DT by collecting the features encountered along the classification path of the instances. For the GB and RF classifiers, we construct feature rankings by means of the Gini importance score <ref type="bibr" target="#b1">[2]</ref> provided by scikit-learn. We observe that whenever the APE oracle predicts linear suitability, the rank correlation is on average very close to 1. This means that LS APE fully recovers the actual importance ranking of the features within the complex model. When the oracle discourages linear explanations, LS APE has indeed difficulties at finding the actual features used by the "glass" black-box classifier. These results confirm that the APE's linear suitability test is a good indicator of the expected quality of a linear surrogate, which translates into faithful explanations for black-box classifiers. Similar results are obtained when using the precision as fidelity metric.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Competitors Evaluation</head><p>We report the average accuracy on 100 target instances for linear surrogates learned with LIME, LS, and the APE's variants APE 𝑎 and APE 𝑡 . We exclude SHAP <ref type="bibr" target="#b14">[15]</ref> from this evaluation because, even though the Kernel SHAP variant resorts to linear regression, it approximates the shapley values unlike linear surrogate such as LIME and LS that compute the gradient of the underlying model <ref type="bibr" target="#b6">[7]</ref>. APE's variants return respectively an anchor or a shallow decision tree when the APE oracle does not predict linear suitability, otherwise they both invoke LS APE . The results are shown in Figure <ref type="figure" target="#fig_3">4</ref>. For LS, we omit the datasets with categorical attributes since these are not supported by this method.</p><p>The results show that regardless of the rule-based surrogate, APE achieves the best accuracy, and that the performance of its two variants depends on the black-box model. On average APE 𝑎 offers higher adherence, but also exhibits higher variability. All in all, this evaluation shows that judiciously choosing between linear and rulebased explanations in a per-instance basis brings a fidelity gain of 0.21 points on average when compared to always choosing LIME or LS. We also remark that when APE chooses to report a linear explanation, the decision frontier is indeed linearly separable: this is confirmed by the fact that both APE 𝑎 and APE 𝑡 outperform LS and LIME by a large margin even for black boxes with a relatively high proportion of linearly suitable frontiers, e.g., SVM and RF (see Table <ref type="table" target="#tab_5">3</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Ablation Study</head><p>We now carry an ablation study to assess the contribution of the APE oracle's components, namely the Libfolding unimodality test <ref type="bibr" target="#b19">[20]</ref> and the Thornton's linear separability test <ref type="bibr" target="#b22">[23]</ref>, on the adherence of APE. We report the average accuracy of APE 𝑎 and APE 𝑡 in Figures <ref type="figure" target="#fig_5">5a</ref> and<ref type="figure" target="#fig_5">5b</ref>, compared to the same variant of APE excluding the libfolding unimodality test (APE \{Libfolding}) and the Thornton's separability test (APE \{Thornton}). The results suggest that the unimodality and separability tests are complementary, and that simply testing for linear separability around the decision boundary is not enough to predict linear suitability as suggested by the accuracy of APE \{Libfolding}.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Counterfactuals Evaluation</head><p>In line with the literature in counterfactual explanations <ref type="bibr" target="#b24">[25]</ref>, we assess the quality of APE's reported counterfactual instances for our 100 test instances, by measuring their resemblance to real instances. To this end we resort to the Mahalanobis distance computed against the entire set of enemies in the test instances. The Mahalanobis distance measures to which extent our counterfactual explanations are outliers w.r.t. the distribution of non-synthetic enemies. We report the quality of the counterfactual instances when computed using Growing Fields and Growing Spheres <ref type="bibr" target="#b12">[13]</ref>. The distance values show that overall, APE finds more realistic counterfactual instances than GS, i.e., the instances lie in average 0.356 points closer to actual instances. This originates from the fact that, unlike GS, APE -more precisely Growing Fields -takes into account the variance and amplitude of the attributes when generating synthetic instances. This also incurs a speed-up of two orders of magnitude because taking into account the data distribution guarantees a data-aware expansion speed for the field radius during the quest for enemies (runtime results are provided on Github.)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">STATE OF THE ART</head><p>The fundamental question of what makes an explanation suitable for a particular use case lies at the junction of XAI and cognitive sciences. For this reason, this research question has not been addressed from a holistic perspective but rather from different, still complementary, angles.</p><p>On the one hand, the XAI community has put emphasis on the development of post-hoc explanation paradigms and methods <ref type="bibr" target="#b9">[10]</ref>, e.g., attribution scores, linear surrogates, rule-based surrogates, counterfactual explanations, sensitivity coefficients, etc. All these approaches aim to identify the features that play a role in the predictions of an AI model. Among those, feature attribution rankings based on linear surrogates such as LIME <ref type="bibr" target="#b16">[17]</ref> or LS <ref type="bibr" target="#b13">[14]</ref> enjoy notable popularity, because they can provide accurate per-instance explanations. Besides, practitioners from most disciplines are familiar with linear models. While SHAP <ref type="bibr" target="#b14">[15]</ref> -more precisely its variant Kernel SHAP -may resort to linear regression to compute attribution scores, the obtained coefficients are not a linear approximation of the black box, but actually approximations of the Shapley values of the input features. These values are based on coalitional game theory and measure the average change in the model's expected prediction when conditioning on each feature. Shapley values are therefore akin to discrete gradients as computed by methods such as DeepLift <ref type="bibr" target="#b18">[19]</ref> or Integrated Gradients <ref type="bibr" target="#b20">[21]</ref>. All these explanations models are learned so that they optimize for user-agnostic criteria such as the adherence, which is usually an accurate proxy for fidelity <ref type="bibr" target="#b7">[8]</ref>. Adherence is generally quantified by means of classical ML scores that depend on the black box's main task, e.g., classification, regression, etc. Other desiderata for explanations include low complexity <ref type="bibr" target="#b5">[6]</ref> and stability <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b25">26]</ref>, however the bulk of the literature in classical XAI has pushed the state of the art towards novel approaches -or improvements of existing ones -that primarily optimize for fidelity in the general case. None of these works tackles the question of when a linear surrogate is objectively a reliable explanation. This is the primary driver of our work that focuses on adherence and fidelity for surrogate classifiers in a per-case basis.</p><p>At the other side of the spectrum, cognitive and social sciences study the subjective and human aspects of explaining AI models. In that spirit, the suitability of an explanation is characterized by its comprehensibility and plausibility <ref type="bibr" target="#b5">[6]</ref>. Comprehensibility captures the extent to which a user grasps an explanation and can use it to accomplish well-defined tasks <ref type="bibr" target="#b0">[1]</ref>, e.g., determine the features used by the black-box system, predict the black box's answer, etc. That "understanding" is operationalized via objective measures on execution time or accuracy w.r.t. those tasks. On the other hand, the plausibility dimension models the cognitive preferences and background of the users. As pointed out by several studies <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b11">12]</ref>, users can reject an explanation if it contradicts common sense, for instance, if the explanation is too simplistic given that the underlying problem is deemed complex. The consensus seems to indicate that showing plausible and sound explanations increases trust in AI systems <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b23">24]</ref>, whereas the effects on comprehensibility and task efficiency are mixed.</p><p>While the XAI and cognitive science communities may appear somehow unreconciled, the relevance of the quality dimensions targeted by classical XAI methods has been justified by user studies. It   has been suggested <ref type="bibr" target="#b11">[12]</ref> that in the context of recommender systems, low adherence harms trust in explanations. Evidence also suggests that multi-paradigm explanations can have a positive impact on comprehensibility <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b26">27]</ref>. In particular, counterfactual explanations can be a complement to attribution-based or rule-based explanations.</p><p>In this line of thought, our approach APE (a) informs the user of whether a use case is explainable with a faithful and unambiguous linear approximation, and (b) enriches the resulting explanation with counterfactual instances. When the decision boundary is unadapted to a linear surrogate, APE offers the possibility of computing a rule-based explanation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">DISCUSSION AND CONCLUSION</head><p>We have presented a method to decide a priori the pertinence of a local linear explanation for a given use case. Our decision is driven by standard user-agnostic desirata, namely the adherence and fidelity of the explanations. The experimental results suggest that it is possible to characterize the decision boundary of a black-box classifier around a target instance and select between linear and rule-based explanations. In that spirit, the answers of APE can provide valuable insights to the users of AI systems and linear surrogates. If APE discourages a linear explanation, then we can conclude that the classification boundary is probably complex and that a unique explanation based on feature attribution will be incomplete or inaccurate. Moreover, our use of counterfactual explanations provides users with a diverse and representative set of scenarios that can change the classifier's output. That being said, we emphasize that the most adapted explanation for a use case must take into account the human and cognitive aspects of explaining complex AI systems to end users. We focused on local linear explanations, because practitioners often resort to these models without questioning their pertinence.</p><p>Existing studies <ref type="bibr" target="#b15">[16]</ref> suggest that multifaceted explanations, e.g., an anchor plus a counterfactual, can be more effective than singleparadigm explanations at illustrating the logic behind a classifier. Since this argument does not exclude the combination of attributionbased and rule-based explanations, this work does not discourage such a conjunction of paradigms. Instead it provides hints about the nature of the classifier's decision border. This could be useful in scenarios where the goal is to replace the black-box model, e.g., for reverse engineering or when a single and complete unambiguous explanation is required.</p><p>As future work we envision to port APE to other data types, e.g., text, and ML tasks, e.g., regression. Moreover, we would like to adapt our framework to other explanation paradigms such as rules, Shapley values, and different sorts of discrete gradients. This could be formulated in two ways: (i) by replacing rule-based surrogates with a different paradigm in APE, or (ii) by defining new oracles that can tell us when a particular explanation type is adapted to a classifier and target instance. Another interesting research avenue is the integration of the notions of coverage, complexity, and plausibility when deciding for the best explanations for a given use case.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Two explanation scenarios for a classifier and a target instance (the filled star): (a) a suitable single linear explanation; (b) three contradictory linear explanations.</figDesc><graphic coords="3,53.80,205.38,240.25,152.08" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: A linear explanation for a classifier and a target instance 𝑥. The inner circle (dotted in blue) is the minimal locality Φ that covers 𝑥 and is traversed by the decision boundary. Locality can be extended (orange circle) and still provide an equally good linear approximation for the black-box. Friends 𝐹 of 𝑥 are represented by yellow stars, and enemies 𝐸 by blue circles.</figDesc><graphic coords="3,329.97,181.05,216.22,146.88" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Average Kendall's rank correlation coefficient of the LS APE 's explanations computed on 100 instances for 7 datasets and 4 "glass" black-box models across the oracle's outcomes.</figDesc><graphic coords="8,317.96,332.64,240.24,153.11" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Average accuracy per black-box model on 100 instances of the experimental datasets for APE 𝑎 and APE 𝑡 .</figDesc><graphic coords="9,53.80,83.68,240.24,153.11" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>(a) Average accuracy of APE 𝑎 (b) Average accuracy of APE 𝑡</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Average accuracy per black box computed on 100 instances of the experimental datasets for APE 𝑎 in (a) and for APE 𝑡 in (b) when we remove the libfolding unimodality and linear separibility tests from the APE oracle.</figDesc><graphic coords="10,53.80,299.53,240.23,156.60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Average Mahalanobis distance between the counterfactual instances generated by Growing Spheres (GS) and Growing Fields (GF).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Notation used in the paper.</figDesc><table><row><cell>Symbol</cell><cell>Definition</cell><cell>Symbol</cell><cell>Definition</cell></row><row><cell>𝑓 (•)</cell><cell>Black-box classifier</cell><cell>𝑔 (•)</cell><cell>Linear surrogate</cell></row><row><cell>𝑋 , 𝑥</cell><cell>Input domain, target instance</cell><cell>𝑌</cell><cell>Output domain</cell></row><row><cell>𝑇 , 𝑡</cell><cell>Input dataset, instance</cell><cell>𝐹</cell><cell>Target's friend instances</cell></row><row><cell>𝐸, 𝑒</cell><cell></cell><cell></cell><cell>Locality, Locality function</cell></row><row><cell>𝑍 , 𝑧</cell><cell>Artificial instances, instance</cell><cell>𝑅</cell><cell>Feature-attribution ranking</cell></row><row><cell>𝑚 (•)</cell><cell>Adherence metric</cell><cell>𝜏</cell><cell>Adherence threshold</cell></row></table><note><p>Target's enemies, enemy Φ, 𝜈 𝑥 (•)</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Algorithm 2 The F instance generation processRequire: a dataset 𝑇 ⊂ 𝑋 , a radius 𝑟 ∈ (0, 1], an instance 𝑥 = (𝑥 1 , . . . , 𝑥 𝑑 ) ∈ 𝑋 Ensure: An artificial instance 𝑧 = (𝑧 1 , . . . , 𝑧 𝑑 )𝑧 𝑖 ← 𝑥 𝑖 + 𝜌 𝑘 with 𝜌 𝑘 ∼ U (𝑎, 𝑏) 𝑧 𝑖 ← (𝑥 𝑖 with prob. 1 -𝜌 𝑘 ) with 𝜌 𝑘 ∼ U (0, 𝑟 )Require: a dataset 𝑇 ⊂ 𝑋 , a target instance 𝑥 = (𝑥 1 , . . . , 𝑥</figDesc><table><row><cell>6:</cell><cell>else</cell></row><row><cell>7:</cell><cell></cell></row><row><cell>8:</cell><cell>end if</cell></row><row><cell cols="2">9: end for</cell></row><row><cell cols="2">10: return 𝑧</cell></row><row><cell cols="2">Algorithm 3 GROWING FIELDS (GF)</cell></row></table><note><p>1: for 𝑖 ∈ 1 . . . 𝑑 do 2: if 𝑥 𝑖 is numerical then // 𝐴 𝑖 = max 𝑖min 𝑖 3: 𝑎 = min(0, 𝑟 × 𝐴 𝑖 (𝑇 ) -𝜎 𝑖 (𝑇 )) 4: 𝑏 = 𝑎 + 𝜎 𝑖 (𝑇 ) 5: 𝑑 ) ∈ 𝑋 , a classifier 𝑓 : 𝑋 → 𝑌 , Hyper-parameters: 𝑟 0 = 0.1, 𝜃 = 1.8, 𝑛 = 2000 as defined by GS [13] Ensure: Set 𝑍 of instances; resulting field radius 𝑟 1: 𝑟 ← 𝑟 0 2: 𝑍 ∼ F (𝑇 , 𝑟, 𝑥) 𝑖 ≤𝑛 3: while ∃ 𝑒 ∈ 𝑍 | 𝑓 (𝑒) ≠ 𝑓 (𝑥) do 4: 𝑟 ← 𝑟 /2 5: Update 𝑍 ∼ F (𝑇 , 𝑟, 𝑥) 𝑖 ≤𝑛 6: end while 7: while 𝑒 ∈ 𝑍 | 𝑓 (𝑒) ≠ 𝑓 (𝑥) do 8: 𝑟 ← min(1, 𝜃 × 𝑟 ) 9: Update 𝑍 ∼ F (𝑇 , 𝑟, 𝑥) 𝑖 ≤𝑛 10: end while 11: return arg min 𝑒 {dist(𝑥, 𝑒) | 𝑒 ∈ 𝑍 }</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Line 2 in Algorithm 4 checks if si((𝑇 ∩Φ)∪𝑍 𝑏 ) = 1. That is, the test also considers real instances that fall within the field from which 𝑍 was drawn. If the test is passed, the decision boundary is considered linearly separable enough and the oracle returns true.Require: instances 𝑍 ⊂ 𝑋 , target instance 𝑥 = {𝑥 1 , . . . , 𝑥 𝑑 } ∈ 𝑋 , classifier 𝑓 : 𝑋 → 𝑌 Ensure: Is 𝑓 linearly separable in 𝑍 w.r.t. the class 𝑓 (𝑥)?</figDesc><table><row><cell cols="2">Algorithm 4 APE ORACLE</cell></row><row><cell>3:</cell><cell>return True</cell></row><row><cell>4:</cell><cell>end if</cell></row><row><cell cols="2">5: end if</cell></row><row><cell cols="2">6: return False</cell></row></table><note><p>1: if 𝐸 ⊂ 𝑍 and 𝐹 ⊂ 𝑍 are unimodal then 2: if 𝑍 is linearly separable w.r.t. 𝑓 then</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>Algorithm 5 EXTENDED LOCAL SURROGATE (LS APE ) Require: instances 𝑍 ⊂ 𝑋 drawn from a field, a classifier 𝑓 : 𝑋 → 𝑌 , target and counterfactual instance 𝑥, 𝑒 ∈ 𝑋 , an adherence metric 𝑚; Hyper-parameters: 𝜃 = 0.05 Ensure: a linear surrogate classifier 𝑔 1: 𝑟 ← 1 /𝛿 × dist (𝑥, 𝑒) 2: Split 𝑍 into 𝑍 train , 𝑍 test 3: 𝑔 ← LINEAR REGRESSION (𝑍 train , 𝑓 (𝑍 train ))</figDesc><table><row><cell>6:</cell><cell>𝑟 ← 𝜃 × 𝑟</cell></row><row><cell>7:</cell><cell>𝑍 ∼ F (𝑇 , 𝑟, 𝑒) 𝑖 ≤𝑛</cell></row><row><cell>8:</cell><cell>Split 𝑍 into 𝑍 train , 𝑍 test</cell></row><row><cell>9:</cell><cell></cell></row></table><note><p>4: 𝑎 ← 𝜏 ← 𝑚 (𝑔) on 𝑍 test 5: while 𝑎 ≥ 𝜏 ∧ 𝑟 &lt; 1 do</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table><row><cell>Name</cell><cell>Features</cell><cell></cell><cell>Instances</cell></row><row><cell></cell><cell cols="2">Numerical Categorical</cell><cell></cell></row><row><cell>Adult</cell><cell>2</cell><cell>10</cell><cell>48842</cell></row><row><cell>Blob  †</cell><cell>2</cell><cell>0</cell><cell>1000</cell></row><row><cell>Blobs  †</cell><cell>12</cell><cell>0</cell><cell>5000</cell></row><row><cell>Blood</cell><cell>4</cell><cell>0</cell><cell>748</cell></row><row><cell>Cat Blobs  †</cell><cell>4</cell><cell>4</cell><cell>5000</cell></row><row><cell>Cancer</cell><cell>10</cell><cell>20</cell><cell>569</cell></row><row><cell>Circles  †</cell><cell>2</cell><cell>0</cell><cell>1000</cell></row><row><cell>Diabetes</cell><cell>8</cell><cell>0</cell><cell>768</cell></row><row><cell>M Blobs  †</cell><cell>20</cell><cell>0</cell><cell>7500</cell></row><row><cell>Moons  †</cell><cell>2</cell><cell>0</cell><cell>1000</cell></row><row><cell>Mortality</cell><cell>15</cell><cell>52</cell><cell>1614</cell></row><row><cell>Titanic</cell><cell>1</cell><cell>5</cell><cell>1046</cell></row></table><note><p><p>Experimental datasets ( † indicates synthetic datasets)</p>Black-box Classifiers. We evaluate APE on a handful of classifiers of different architectures -i.e., ensemble methods, piecewiseconstant functions, smooth functions -implemented in scikit-learn with default values of hyperparameters unless stated otherwise: (i) Gradient Boosting (GB) with 20 tree estimators, (ii) Multi-layer Perceptron (MLP) with a logistic activation function, (iii) Random Forest (RF) with 20 tree estimators, (iv) Gaussian Naive Bayes (NB), (v) Support Vector Machine (SVM) with a balanced class weight, (vi) Decision Tree (DT), (vii) Logistic Regression (LR) and (viii) a Voting ensemble (VC) composed of LR, SVM, and NB classifiers.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Average accuracy computed on 100 instances per black-box model and dataset of LS APE for both the oracle's outcomes.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">Is a Linear Explanation Suitable?</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>GB</cell><cell></cell><cell></cell><cell>MLP</cell><cell></cell><cell></cell><cell>RF</cell><cell></cell><cell></cell><cell>VC</cell><cell></cell><cell></cell><cell>SVM</cell><cell></cell></row><row><cell></cell><cell>Yes</cell><cell>No</cell><cell>𝑃𝑟𝑜𝑝 𝑛𝑜</cell><cell>Yes</cell><cell>No</cell><cell>𝑃𝑟𝑜𝑝 𝑛𝑜</cell><cell>Yes</cell><cell>No</cell><cell>𝑃𝑟𝑜𝑝 𝑛𝑜</cell><cell>Yes</cell><cell>No</cell><cell>𝑃𝑟𝑜𝑝 𝑛𝑜</cell><cell>Yes</cell><cell>No</cell><cell>𝑃𝑟𝑜𝑝 𝑛𝑜</cell></row><row><cell>Adult</cell><cell cols="2">0.555 0.486</cell><cell>0.65</cell><cell cols="2">0.507 0.397</cell><cell>0.60</cell><cell cols="2">0.659 0.483</cell><cell>0.47</cell><cell cols="2">0.334 0.304</cell><cell>0.25</cell><cell cols="2">0.679 0.643</cell><cell>0.35</cell></row><row><cell>Blob</cell><cell cols="2">0.891 0.782</cell><cell>0.57</cell><cell cols="2">0.890 0.760</cell><cell>0.49</cell><cell cols="2">0.874 0.730</cell><cell>0.56</cell><cell cols="2">0.899 0.748</cell><cell>0.46</cell><cell cols="2">0.894 0.744</cell><cell>0.43</cell></row><row><cell>Blobs</cell><cell cols="2">0.855 0.636</cell><cell>0.78</cell><cell cols="2">0.723 0.606</cell><cell>0.86</cell><cell cols="2">0.783 0.655</cell><cell>0.82</cell><cell cols="2">0.745 0.610</cell><cell>0.68</cell><cell cols="2">0.717 0.599</cell><cell>0.80</cell></row><row><cell>Blood</cell><cell>\</cell><cell>0.437</cell><cell>0.99</cell><cell>\</cell><cell>0.497</cell><cell>1.00</cell><cell>\</cell><cell>0.283</cell><cell>1.00</cell><cell>\</cell><cell>0.223</cell><cell>1.00</cell><cell>\</cell><cell>0.622</cell><cell>1.00</cell></row><row><cell>Cancer</cell><cell cols="2">0.502 0.381</cell><cell>0.20</cell><cell cols="2">0.501 0.499</cell><cell>0.12</cell><cell>0.510</cell><cell>\</cell><cell>0.00</cell><cell cols="2">0.411 0.382</cell><cell>0.21</cell><cell>0.499</cell><cell>\</cell><cell>0.02</cell></row><row><cell cols="3">Cat Blobs 0.910 0.898</cell><cell>0.70</cell><cell cols="2">0.958 0.900</cell><cell>0.86</cell><cell cols="2">0.874 0.958</cell><cell>0.50</cell><cell cols="2">0.967 0.936</cell><cell>0.72</cell><cell cols="2">0.883 0.794</cell><cell>0.48</cell></row><row><cell>Circles</cell><cell cols="2">0.945 0.723</cell><cell>0.09</cell><cell>0.958</cell><cell>\</cell><cell>0.00</cell><cell cols="2">0.950 0.708</cell><cell>0.04</cell><cell>0.948</cell><cell>\</cell><cell>0.00</cell><cell>0.949</cell><cell>\</cell><cell>0.00</cell></row><row><cell>Diabetes</cell><cell cols="2">0.630 0.399</cell><cell>0.92</cell><cell cols="2">0.802 0.585</cell><cell>0.96</cell><cell>\</cell><cell>0.453</cell><cell>0.98</cell><cell cols="2">0.673 0.258</cell><cell>0.96</cell><cell cols="2">0.717 0.518</cell><cell>0.88</cell></row><row><cell>M Blobs</cell><cell>\</cell><cell>0.833</cell><cell>0.97</cell><cell>\</cell><cell>0.967</cell><cell>1.00</cell><cell cols="2">0.863 0.845</cell><cell>0.82</cell><cell>\</cell><cell>0.947</cell><cell>0.99</cell><cell cols="2">0.944 0.942</cell><cell>0.71</cell></row><row><cell>Moons</cell><cell cols="2">0.923 0.708</cell><cell>0.55</cell><cell cols="2">0.917 0.802</cell><cell>0.59</cell><cell cols="2">0.918 0.727</cell><cell>0.42</cell><cell cols="2">0.916 0.881</cell><cell>0.85</cell><cell cols="2">0.920 0.750</cell><cell>0.50</cell></row><row><cell>Mortality</cell><cell>\</cell><cell>0.826</cell><cell>1.00</cell><cell>\</cell><cell>1.000</cell><cell>1.00</cell><cell>\</cell><cell>0.839</cell><cell>1.00</cell><cell>\</cell><cell>0.518</cell><cell>1.00</cell><cell>\</cell><cell>0.420</cell><cell>1.00</cell></row><row><cell>Titanic</cell><cell cols="2">0.761 0.667</cell><cell>0.06</cell><cell>0.919</cell><cell>\</cell><cell>0.00</cell><cell cols="2">0.973 1.000</cell><cell>0.04</cell><cell cols="2">0.999 0.997</cell><cell>0.16</cell><cell>0.715</cell><cell>\</cell><cell>0.00</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>This is a special case of the Mahalanobis distance when the covariance matrix is diagonal.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1"><p>https://github.com/j2launay/APE</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2"><p>http://scikit-learn.org</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgments. This research was funded by the <rs type="funder">Agence Nationale de la Recherche (ANR)</rs> under grant agreement <rs type="grantNumber">ANR-19-CE23-0019-01</rs> and by the <rs type="programName">TAILOR Network</rs> (<rs type="programName">EU Horizon 2020 research and innovation program</rs> under grant agreement <rs type="grantNumber">952215</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_MhHbH6K">
					<idno type="grant-number">ANR-19-CE23-0019-01</idno>
					<orgName type="program" subtype="full">TAILOR Network</orgName>
				</org>
				<org type="funding" xml:id="_gyEpmuk">
					<idno type="grant-number">952215</idno>
					<orgName type="program" subtype="full">EU Horizon 2020 research and innovation program</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">User-Based Experiment Guidelines for Measuring Interpretability in Machine Learning</title>
		<author>
			<persName><forename type="first">Adrien</forename><surname>Bibal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bruno</forename><surname>Dumas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benoît</forename><surname>Frénay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Advances in Interpretable Machine Learning and AI</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Classification and Regression Trees</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Leo Breiman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Olshen</surname></persName>
		</author>
		<author>
			<persName><surname>Stone</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1984">1984</date>
			<publisher>Wadsworth</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Improving Anchorbased Explanations</title>
		<author>
			<persName><forename type="first">Julien</forename><surname>Delaunay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luis</forename><surname>Galárraga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christine</forename><surname>Largouët</surname></persName>
		</author>
		<idno type="DOI">10.1145/3340531.3417461</idno>
		<ptr target="https://doi.org/10.1145/3340531.3417461" />
	</analytic>
	<monogr>
		<title level="m">CIKM &apos;20: The 29th ACM International Conference on Information and Knowledge Management</title>
		<meeting><address><addrLine>Virtual Event, Ireland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-10-19">2020. October 19-23, 2020</date>
			<biblScope unit="page" from="3269" to="3272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">The linear separability problem: some testing methods</title>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">A</forename><surname>Elizondo</surname></persName>
		</author>
		<idno type="DOI">10.1109/TNN.2005.860871</idno>
		<ptr target="https://doi.org/10.1109/TNN.2005.860871" />
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Networks</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="330" to="344" />
			<date type="published" when="2006">2006. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">ILIME: Local and Global Interpretable Model-Agnostic Explainer of Black-Box Decision</title>
		<author>
			<persName><forename type="first">Radwa</forename><surname>Elshawi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Youssef</forename><surname>Sherif</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mouaz</forename><surname>Al-Mallah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sherif</forename><surname>Sakr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Advances in Databases and Information Systems</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="53" to="68" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">On cognitive preferences and the plausibility of rule-based models</title>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Fürnkranz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomáš</forename><surname>Kliegr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heiko</forename><surname>Paulheim</surname></persName>
		</author>
		<idno type="DOI">10.1007/s10994-019-05856-5</idno>
		<ptr target="https://doi.org/10.1007/s10994-019-05856-5" />
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">109</biblScope>
			<biblScope unit="page" from="853" to="898" />
			<date type="published" when="2020-04-01">2020. 01 Apr 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Explaining the Explainer: A First Theoretical Analysis of LIME</title>
		<author>
			<persName><forename type="first">Damien</forename><surname>Garreau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ulrike</forename><surname>Von</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luxburg</forename></persName>
		</author>
		<ptr target="http://proceedings.mlr.press/v108/garreau20a.html" />
	</analytic>
	<monogr>
		<title level="m">The 23rd International Conference on Artificial Intelligence and Statistics, AISTATS 2020</title>
		<title level="s">Proceedings of Machine Learning Research</title>
		<editor>
			<persName><forename type="first">Silvia</forename><surname>Chiappa</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Roberto</forename><surname>Calandra</surname></persName>
		</editor>
		<meeting><address><addrLine>Palermo, Sicily, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020-08">2020. August 2020</date>
			<biblScope unit="volume">108</biblScope>
			<biblScope unit="page" from="1287" to="1296" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">s-LIME: Reconciling Locality and Fidelity in Linear Explanations</title>
		<author>
			<persName><forename type="first">Romaric</forename><surname>Gaudel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luis</forename><surname>Galárraga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Delaunay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurence</forename><surname>Rozé</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vaishnavi</forename><surname>Bhargava</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Intelligent Data Analysis</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="102" to="114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Local Rule-Based Explanations of Black Box Decision Systems</title>
		<author>
			<persName><forename type="first">Riccardo</forename><surname>Guidotti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anna</forename><surname>Monreale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Salvatore</forename><surname>Ruggieri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dino</forename><surname>Pedreschi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Franco</forename><surname>Turini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fosca</forename><surname>Giannotti</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1805.10820" />
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A Survey of Methods for Explaining Black Box Models</title>
		<author>
			<persName><forename type="first">Riccardo</forename><surname>Guidotti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anna</forename><surname>Monreale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Salvatore</forename><surname>Ruggieri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Franco</forename><surname>Turini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fosca</forename><surname>Giannotti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dino</forename><surname>Pedreschi</surname></persName>
		</author>
		<idno type="DOI">10.1145/3236009</idno>
		<ptr target="https://doi.org/10.1145/3236009" />
	</analytic>
	<monogr>
		<title level="j">ACM Comput. Surv</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1" to="93" />
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Algorithms for Clustering Data</title>
		<author>
			<persName><forename type="first">K</forename><surname>Anil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><forename type="middle">C</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><surname>Dubes</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1988">1988</date>
			<publisher>Prentice-Hall</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Too much, too little, or just right? Ways Explanations Impact End Users&apos; Mental Models</title>
		<author>
			<persName><forename type="first">Todd</forename><surname>Kulesza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simone</forename><surname>Stumpf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Margaret</forename><surname>Burnett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sherry</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Irwin</forename><surname>Kwan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weng-Keen</forename><surname>Wong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Symposium on Visual Languages and Human Centric Computing</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="3" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Inverse Classification for Comparison-based Interpretability in Machine Learning</title>
		<author>
			<persName><forename type="first">Thibault</forename><surname>Laugel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marie-Jeanne</forename><surname>Lesot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christophe</forename><surname>Marsala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Renard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marcin</forename><surname>Detyniecki</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.08443</idno>
		<ptr target="http://arxiv.org/abs/1712.08443" />
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Defining Locality for Surrogates in Post-hoc Interpretablity</title>
		<author>
			<persName><forename type="first">Thibault</forename><surname>Laugel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Renard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marie-Jeanne</forename><surname>Lesot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christophe</forename><surname>Marsala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marcin</forename><surname>Detyniecki</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.07498</idno>
		<ptr target="http://arxiv.org/abs/1806.07498" />
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A Unified Approach to Interpreting Model Predictions</title>
		<author>
			<persName><forename type="first">M</forename><surname>Scott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Su-In</forename><surname>Lundberg</surname></persName>
		</author>
		<author>
			<persName><surname>Lee</surname></persName>
		</author>
		<ptr target="http://papers.nips.cc/paper/7062-a-unified-approach-to-interpreting-model-predictions" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="4765" to="4774" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Explanation in Artificial Intelligence: Insights from the Social Sciences</title>
		<author>
			<persName><forename type="first">Tim</forename><surname>Miller</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.artint.2018.07.007</idno>
		<ptr target="https://doi.org/10.1016/j.artint.2018.07.007" />
	</analytic>
	<monogr>
		<title level="j">Artif. Intell</title>
		<imprint>
			<biblScope unit="volume">267</biblScope>
			<biblScope unit="page" from="1" to="38" />
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Why should I trust you?: Explaining the predictions of any classifier</title>
		<author>
			<persName><forename type="first">Marco</forename><surname>Tulio Ribeiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sameer</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlos</forename><surname>Guestrin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
			<biblScope unit="page" from="1135" to="1144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Anchors: High-Precision Model-Agnostic Explanations</title>
		<author>
			<persName><forename type="first">Marco</forename><surname>Túlio Ribeiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sameer</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlos</forename><surname>Guestrin</surname></persName>
		</author>
		<ptr target="https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16982" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence</title>
		<meeting>the Thirty-Second AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1527" to="1535" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning Important Features Through Propagating Activation Differences</title>
		<author>
			<persName><forename type="first">Avanti</forename><surname>Shrikumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peyton</forename><surname>Greenside</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anshul</forename><surname>Kundaje</surname></persName>
		</author>
		<ptr target="http://proceedings.mlr.press/v70/shrikumar17a.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="3145" to="3153" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Are your data gathered?</title>
		<author>
			<persName><forename type="first">Alban</forename><surname>Siffer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre-Alain</forename><surname>Fouque</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandre</forename><surname>Termier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christine</forename><surname>Largouët</surname></persName>
		</author>
		<idno type="DOI">10.1145/3219819.3219994</idno>
		<ptr target="https://doi.org/10.1145/3219819.3219994" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018">2018. 2210-2218</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Axiomatic Attribution for Deep Networks</title>
		<author>
			<persName><forename type="first">Mukund</forename><surname>Sundararajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ankur</forename><surname>Taly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiqi</forename><surname>Yan</surname></persName>
		</author>
		<ptr target="http://proceedings.mlr.press/v70/sundararajan17a.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="3319" to="3328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Who belongs in the family?</title>
		<author>
			<persName><forename type="first">Robert</forename><forename type="middle">L</forename><surname>Thorndike</surname></persName>
		</author>
		<idno type="DOI">10.1007/BF02289263</idno>
		<ptr target="https://doi.org/10.1007/BF02289263" />
	</analytic>
	<monogr>
		<title level="j">Psychometrika</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="267" to="276" />
			<date type="published" when="1953-12-01">1953. 01 Dec 1953</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Truth from trash: How learning makes sense</title>
		<author>
			<persName><forename type="first">Chris</forename><surname>Thornton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002">2002</date>
			<publisher>Mit Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Evaluating XAI: A comparison of rule-based and example-based explanations</title>
		<author>
			<persName><forename type="first">Elisabeth</forename><surname>Jasper Van Der Waa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anita</forename><surname>Nieuwburg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName><surname>Neerincx</surname></persName>
		</author>
		<ptr target="https://www.sciencedirect.com/science/article/pii/S0004370220301533" />
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">291</biblScope>
			<biblScope unit="page">103404</biblScope>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Counterfactual Explanations for Machine Learning: A Review</title>
		<author>
			<persName><forename type="first">Sahil</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">P</forename><surname>Dickerson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Keegan</forename><surname>Hines</surname></persName>
		</author>
		<idno>CoRR abs/2010.10596</idno>
		<ptr target="https://arxiv.org/abs/2010.10596" />
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">OptiLIME: Optimized LIME Explanations for Diagnostic Computer Algorithms</title>
		<author>
			<persName><forename type="first">Giorgio</forename><surname>Visani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Enrico</forename><surname>Bagli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Federico</forename><surname>Chesani</surname></persName>
		</author>
		<ptr target="http://ceur-ws.org/Vol-2699/paper03.pdf" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the CIKM 2020 Workshops</title>
		<meeting>the CIKM 2020 Workshops</meeting>
		<imprint>
			<date type="published" when="2020">2020. 2699</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Counterfactual Explanations without Opening the Black Box: Automated Decisions and the GDPR</title>
		<author>
			<persName><forename type="first">Sandra</forename><surname>Wachter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brent</forename><forename type="middle">D</forename><surname>Mittelstadt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Russell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.00399</idno>
		<ptr target="http://arxiv.org/abs/1711.00399" />
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
