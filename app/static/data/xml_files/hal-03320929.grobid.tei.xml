<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Efficient Exploration of Interesting Aggregates in RDF Graphs</title>
				<funder ref="#_pQhMKVq #_svfvDxm">
					<orgName type="full">European Research Council</orgName>
				</funder>
				<funder ref="#_QvqS2Tz">
					<orgName type="full">Agence Nationale de la Recherche</orgName>
					<orgName type="abbreviated">ANR</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yanlei</forename><surname>Diao</surname></persName>
							<email>yanlei.diao@polytechnique.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Ecole Polytechnique</orgName>
								<orgName type="institution">Institut Polytechnique de Paris</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Inria</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Paweł</forename><surname>Guzewicz</surname></persName>
							<email>pawel.guzewicz@inria.fr</email>
							<affiliation key="aff0">
								<orgName type="department">Ecole Polytechnique</orgName>
								<orgName type="institution">Institut Polytechnique de Paris</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Inria</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ioana</forename><surname>Manolescu</surname></persName>
							<email>ioana.manolescu@inria.fr</email>
							<affiliation key="aff0">
								<orgName type="department">Ecole Polytechnique</orgName>
								<orgName type="institution">Institut Polytechnique de Paris</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Inria</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Mirjana</forename><surname>Mazuran</surname></persName>
							<email>mirjana.mazuran@inria.fr</email>
							<affiliation key="aff0">
								<orgName type="department">Ecole Polytechnique</orgName>
								<orgName type="institution">Institut Polytechnique de Paris</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Inria</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Efficient Exploration of Interesting Aggregates in RDF Graphs</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">212770963508428AE46139105619B8E5</idno>
					<idno type="DOI">10.1145/3448016.3457307</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-04-12T14:46+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>data analytics</term>
					<term>data exploration</term>
					<term>graphs</term>
					<term>RDF</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>As large Open Data are increasingly shared as RDF graphs today, there is a growing demand to help users discover the most interesting facets of a graph, which are often hard to grasp without automatic tools. We consider the problem of automatically identifying the k most interesting aggregate queries that can be evaluated on an RDF graph, given an integer k and a user-specified interestingness function. Our problem departs from analytics in relational data warehouses in that (i) in an RDF graph we are not given but we must identify the facts, dimensions, and measures of candidate aggregates; (ii) the classical approach to efficiently evaluating multiple aggregates breaks in the face of multi-valued dimensions in RDF data. In this work, we propose an extensible end-to-end framework that enables the identification and evaluation of interesting aggregates based on a new RDF-compatible one-pass algorithm for efficiently evaluating a lattice of aggregates and a novel early-stop technique (with probabilistic guarantees) that can prune uninteresting aggregates. Experiments using both real and synthetic graphs demonstrate the ability of our framework to find interesting aggregates in a large search space, the efficiency of our algorithms (with up to 2.9× speedup over a similar pipeline based on existing algorithms), and scalability as the data size and complexity grow.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CCS CONCEPTS</head><p>• Information systems → Database management system engines; Graph-based database models.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>RDF graphs are increasingly being published and shared as part of the Linked Open Data movement. Given the size, heterogeneity, and complexity of these graphs, their information content is hard to grasp, in particular for non-expert users. In this work, we explore automatic insight extraction from RDF graphs <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b35">36]</ref>. Given a graph and an integer k, we seek to automatically identify the k most interesting insights in the graph. An insight is an RDF analytical query that results in aggregated measures over the data, grouped by a set of dimensions. The query can be expressed in a language such as SPARQL 1.1, the W3C's standard RDF query language <ref type="bibr" target="#b12">[13]</ref>, and evaluated by any RDF query engine. The interestingness of an insight is assessed based on a statistical measure of the query result.</p><p>Motivating application. Computational Lead Finding (CLF) <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b46">47]</ref> is one of the target applications of our work. For journalists, a "lead" is an idea based on which they may write an interesting article. Given a dataset, CLF aims to automatically identify the interesting leads from the data. Below, we outline our approach to RDF insight extraction using examples from statistical lead discovery. Running examples. Consider an RDF graph comprising politicians, CEOs, and connections between them. We can extract such a graph, for instance, from the WikiData open-source RDF repository. Figure <ref type="figure" target="#fig_0">1</ref>(a) shows an example RDF graph where CEOs are linked with politicians, e.g., Isabel dos Santos, a wealthy Angolan CEO (at the heart of the Luanda Leaks scandal), is the daughter of a former president of Angola. Starting from the graph, we aim to automatically identify a small set of aggregate queries that are statistically interesting. Here, interestingness is a statistical measure that indicates deviation from the prior knowledge of the journalists. For example, the interesting aggregate results may deviate from a uniform distribution of values over different aggregate groups, or a normal distribution over numeric dimensions such as age.</p><p>Table <ref type="table" target="#tab_0">1</ref> shows three example aggregates, whose dimensions and measures are either properties in the RDF graph or properties that we derive to enrich the scope of the analysis. In Example 1, CEOs, politicalConnections, countryOfOrigin, and netWorth are either types or properties in the RDF graph in Figure <ref type="figure" target="#fig_24">1(a)</ref>. Example 2 analyzes the CEOs along the number of managed companies, which is not a property in the graph: we derive it by counting the properties of each CEO. This enables us to discover, e.g., that the average age of Angolan CEOs that manage two companies is low compared to other nationalities. Example 3 analyzes CEOs by areas of companies; we derive this from the graph by following a path from the CEOs to the companies they manage, then to their areas. Similar path examples include company/headquarters, politicalConnection/role; longer paths produce a larger number of novel angles for the analysis. Among all possible aggregate queries that we can generate, the above three examples are selected because their results show significant deviation from uniform values (having outliers). For Examples 1 and 2, Figure <ref type="figure" target="#fig_24">1(b)</ref> shows respectively a histogram that exhibits an outlier in sum(netWorth) for Angola, and a heat map where the dark color reflects a low value of avд(age) of CEOs, both due to Dos Santos. We can show to the user such interesting insights as (i) histograms (if one-dimensional), (ii) heat maps (if two-dimensional), or (iii) tables (for high-dimensional aggregates).</p><p>Our goal to discover the k most interesting aggregates from an RDF graph poses two unique challenges:</p><p>Challenge C1 -Aggregate identification. Automatic extraction of interesting aggregates is one among many existing techniques for data exploration and visualization recommendation. Yet, most prior works assume a fixed relational schema <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b42">43]</ref>. In contrast, in RDF graphs, facts, dimensions, and measures are not specified but must be identified therein. To address this challenge, given an RDF graph, we provide a variety of strategies to create new dimensions and measures, which enable us to examine a rich space of candidate aggregates and to discover the most interesting ones. We further develop a modular framework for aggregate identification, which can be extended or customized as needs arise.</p><p>Challenge C2 -Efficient and correct aggregate evaluation. Since we define interestingness on an aggregate result, we must evaluate candidate aggregates to determine if they are among the k most interesting ones. A key feature of our work is that we look for multidimensional aggregates (MDAs), such as Examples 2 and 3. A set of N dimensions, among which we enumerate candidate aggregates, leads to a lattice <ref type="bibr" target="#b28">[29]</ref> of 2 N nodes, each of which is an MDA (see Figure <ref type="figure" target="#fig_24">1(c)</ref>). We may have many such lattices to consider at once, and efficiently evaluating them all poses a salient challenge.</p><p>To address the challenge, first, we revisit a classical framework for lattice-based MDA computation in relational data warehouses (DWs). Efficient algorithms, such as ArrayCube <ref type="bibr" target="#b48">[49]</ref>, compute an aggregate in the lattice from the result of one of its parents, and compute all aggregates in the lattice in a single pass over the data. However, a crucial observation we make in this work is that the classical one-pass approach to lattice computation is incorrect for RDF data, due to a phenomenon called multi-valued dimensions, that is, an RDF node (fact) may have multiple values along a given dimension. To tackle the issue, while retaining the benefits of one-pass algorithms, we provide a theoretical analysis of how the classical approach produces errors. Furthermore, we develop a new RDFcompatible one-pass algorithm that (i) correctly and efficiently handles lattice-based MDA computation where the aggregates use multi-valued dimensions; (ii) for each node in the lattice (with a given set of dimensions), simultaneously handles many aggregates that differ in the measure (among many possible ones) and the aggregate function in use; (iii) saves computation cost by sharing measures across all lattices that analyze the same set of facts.</p><p>Second, to further improve efficiency, we develop a new technique to stop the evaluation of an MDA as soon as we can determine (with high probability) that it will not be among the top k. Our technique builds on the work in <ref type="bibr" target="#b29">[30]</ref>, which provides confidence-interval (CI) bounds on an approximate aggregate result. Our problem is harder because we want to approximate the interestingness score computed over the aggregate result, which amounts to estimating the result of a nested aggregate query, whereas the prior work does not support such nested queries. Using advanced statistical tools, we construct CIs for the interestingness function including variance, skewness, and kurtosis over estimated results of candidate aggregates, enabling early pruning of uninteresting aggregates.</p><p>In summary, the contributions we make in this work include:</p><p>• Spade, a new RDF-oriented end-to-end framework that automatically identifies, enumerates, and efficiently evaluates RDF MDAs to determine the most interesting ones (Section 3);</p><p>• MVDCube, the first correct and efficient algorithm for one-pass lattice-based computation of RDF MDAs (Section 4);</p><p>• A novel early-stop technique that stops the evaluation of MDAs that, with a high probability, will not be in the top-k list (Section 5);</p><p>• Experimental results validating (i) the ability of Spade to extract insights from a large space of candidate aggregates; (ii) the frequent, and potentially high errors that existing algorithms introduce on real-life, heterogeneous RDF graphs; (iii) the efficiency of our onepass algorithm, which is faster than PostgreSQL's GROUP BY CUBE implementation by 20% to 80%; (iv) the extra speedup of 10% to 43% achieved by our early-stop technique, and (v) the scalability of Spade as the size and complexity of the graphs increase (Section 6).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">PROBLEM STATEMENT AND NOTATION</head><p>We consider RDF data defined over three pairwise disjoint sets: the set of URIs U, the set of literals L, and the set of blank nodes B. An RDF graph G is a finite set of triples of the form (s, p, o), called subject, property, and object, such that s ∈ (U ∪ B), p ∈ U, and o ∈ (U ∪ B ∪ L). The RDF property rdf:type is used to attach types to an RDF node, which may have zero, one or several types. Such an RDF graph may have an ontology stating relationships among its types and properties, e.g., any CEO is a BusinessPerson. An ontology leads to implicit triples that together with the triples explicitly present in G are the graph's semantics. All the implicit triples can be materialized via saturation, iteratively deriving new ones from G and the rules; we consider ontologies for which this process is finite as in <ref type="bibr" target="#b22">[23]</ref>, and apply it prior to our analysis.</p><p>A candidate fact set (CFS) is a set of RDF nodes that we build an aggregate on; we call a member of the set a candidate fact (CF).</p><p>An attribute is either a (direct) property (P) of a CF in the original RDF data, or a derived property (DP), which we create from the data and attach to a CF to enrich the analysis. For instance, one may attach to each CEO the number of companies they manage (the full set of derivation strategies is discussed in Section 3). An attribute can be used as a dimension, to group CFs by value, or as a measure, to be aggregated within each group of CFs.</p><p>We employ an aggregate function, f , that ranges over the common set Ω = {count, min, max, sum, avд}.</p><p>A multidimensional aggregate (MDA), A = ⟨CFS, D, M, f ⟩, is determined by: a CFS, a set D = {D 1 , D 2 , . . . , D N } of dimensions (which are attributes), a measure M (also an attribute), and an aggregate function f . The semantics of A is that of a SPARQL 1.1 aggregate query <ref type="bibr" target="#b12">[13]</ref>, which also agrees with that of the RDF analytical queries introduced in <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b10">11]</ref>. The result of A on an RDF graph G, denoted A(G), is the set of tuples, one per each distinct combination of dimension values (aggregate group) in the data:</p><formula xml:id="formula_0">A(G) = {(d 1 , d 2 , . . . , d N , f {m j | ∃ CF i ∈ CFS, CF i .D 1 = d 1 , CF i .D 2 = d 2 , . . . , CF i .D N = d N , CF i .M = m j })}</formula><p>where CF i has (at least) the values d 1 , d 2 , . . . , d N along the dimensions D 1 , D 2 , . . . , D N , and m j iterates over the set of values of the measure M on CF i . Finally, f {•} is the result of running the aggregate function f over the measure values from a given set.</p><p>Our semantics, unlike that of relational DWs, does account for heterogeneity in RDF data: (i) Some CFs may miss dimensions and/or measures, and thus they do not contribute to the result. For the graph in Figure <ref type="figure" target="#fig_0">1</ref>, the result for Example 1 is {(Angola, $2.8B)}, due to n 1 , whereas n 2 does not contribute to the result as it lacks the countryOfOrigin dimension; (ii) A CF may contribute to multiple groups in A (if it has multiple values for a dimension), and/or multiple times to the aggregated value (if it has several values for the measure). The result for Example 2 is {(Nigeria, 1, 65), (France, 1, 65), (Lebanon, 1, 65), (Brazil, 1, 65)}, all obtained from n 2 given its four distinct values of nationality. Although n 1 has both dimensions, it does not contribute to the result as it misses the age measure.</p><p>An interestingness function, h, is applied over the result of an aggregate A. Let W be the number of tuples in A(G) and, for each tuple t i ∈ A(G), let t i .v be the aggregated value computed by f . Then h takes the set {t 1 .v, t 2 .v, . . . , t W .v} and returns a score, i.e., a positive real number, reflecting a measure of interestingness of A. The user chooses the function to be used during the analysis.</p><p>Finally, the problem we address is stated as follows:</p><p>Problem 1. Given an RDF graph G, a positive integer k, and an interestingness function h of choice, find the aggregates A 1 (G), . . . , A k (G) whose interestingness on G is the highest.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">OVERVIEW OF THE APPROACH</head><p>In this section, we describe the system design of Spade, a new RDForiented end-to-end framework that automatically identifies, enumerates, and efficiently evaluates MDAs to determine the most interesting ones. Figure <ref type="figure" target="#fig_2">2</ref> shows Spade's analytics pipeline; it comprises an offline phase, where an RDF graph is loaded and pre-processed, and an online phase, where user-specific analysis is performed.</p><p>Offline Processing. Upon loading an RDF graph, we first build a structural summary thereof, using the open-source RDFQuotient tool <ref type="bibr" target="#b21">[22]</ref>. The summary captures all the properties occurring in the graph and proposes a set of RDF node groups such that the RDF nodes in each group are considered equivalent. Spade uses the summary to expedite several steps of the analysis, e.g., the enumeration of RDF types and properties, as described below. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RDF-DB</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Keyword extraction</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Language detection</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Derived Property Enumeration</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Paths</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Online Attribute Analysis</head><p>Online Processing</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Count generation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Typebased</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Propertybased</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Candidate Fact Set Selection</head><p>Summarybased 1)</p><p>2)</p><p>3)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Offline Attribute Analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Aggregate Enumeration</head><p>A1, A2, ... Next, we perform Offline Attribute Analysis with three main purposes: (i) to gather a set of statistics for each property in the graph, (ii) to determine if derivations should be generated for a given property, and (iii) to decide if pre-aggregated values of some properties should be computed and stored in the database.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Aggregate Evaluation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Aggregate</head><p>Derived properties are the key to a rich search space and to effectively addressing challenge C1. With this aim, we compute statistics including the type of property values (e.g., String, Integer, Date) and, if they are multi-valued, their number of distinct values, the lowest and highest values, etc. Based on these results, Derived Property Enumeration generates: (i) property counts for multi-valued properties, e.g., how many companies a CEO manages; (ii) keywords occurring in property values, e.g., if a company's description is "Sonangol oversees petroleum production", we attach to the company the multi-valued attribute kwInDescription with the values "Petroleum" and "Production"; (iii) the language of a text property, e.g., a company may gain the attribute langOfDescription with the value "English"; (iv) paths, e.g., a CEO politically connected to a "President" gains the attribute politicalConnection/role with the value "President". Finally, each derived property is also analyzed and stored along with its statistics in the database.</p><p>In addition, for each multi-valued attribute, we create a table in the database storing its values, pre-aggregated on the RDF nodes that have it. More specifically, for each RDF node, we compute and store the aggregated value for each (attribute, aggregate function) pair, e.g., the sum of a 1 , the count of a 1 , the minimum of a 2 . This allows Spade to account for facts with multiple measure values and improve Aggregate Evaluation during Online Processing.</p><p>Online Processing. The analysis of RDF graphs suits the specific needs of users and proceeds in the following steps.</p><p>Step 1 is Candidate Fact Set Selection. To address challenge C1, Spade identifies CFSs in three ways: (i) type-based: for each type T in the graph, the set of RDF nodes of typeT ; (ii) property-based: for a (user-specified) set of properties, all the RDF nodes having those outgoing properties; (iii) summary-based: each set of RDF nodes identified as equivalent by the RDFQuotient summary; RDF nodes in the same equivalence class tend to have many common properties, making them interesting candidates to be analyzed together.</p><p>Step 2 is Online Attribute Analysis. In this step, for each CFS, we first enumerate all direct and derived properties. Then, we enrich the offline-analysis results by adding CFS-dependent statistics, e.g., the support of an attribute among all the facts in the CFS, the number of CFs that have such an attribute more than once, and the number of distinct values. Spade exploits the gathered statistics in different steps, e.g., to guide the choice of dimensions, measures, and aggregate functions and to improve Aggregate Evaluation.</p><p>Step 3 is Aggregate Enumeration. Spade uses the pool of analyzed attributes to generate candidate MDAs. To address challenge C1, we generate a rich space of candidate aggregates while applying rule-based pruning to avoid meaningless candidates.</p><p>(a) Identifying dimensions and measures from (derived) properties: We first enumerate all the (derived) properties and consider them for dimensions or measures, subject to the following rules: (i) Dimensions and measures must be frequent, i.e., having a support greater than a defined threshold; (ii) Dimensions should not have too many distinct values when compared to the number of facts to examine (e.g., we do not consider counting the number of CEOs by their birthday as there are too many distinct values).</p><p>(b) Identifying the dimension set of each lattice: We compute the Maximal Frequent Sets of attributes <ref type="bibr" target="#b24">[25]</ref> in the CFS. Each of the found sets is the root of one lattice. We further filter them so that each lattice: (i) has at most N attributes, and (ii) does not contain attributes that are derived one from the other, e.g., nationality and numOfNationalities are not allowed as dimensions of the same lattice. Although we aim to offer a general approach, we also note that the readability of MDAs by human users is maximized at levels of relatively low dimensionality, i.e., N ∈ {1, 2, 3, 4}.</p><p>(c) Identifying the measures in each lattice: Once a lattice acquires dimensions D i , we assign it a measure set M i that comprises all the analyzed attributes of the CFS except those in D i , and those that are derived from a dimension in D i , e.g., numOfNationalities cannot be a measure in an aggregate whose dimension is nationality.</p><p>Several lattices may be found for a CFS, e.g., for CEOs, we have three: {countryOfOrigin}, {nationality, numOfCompanies}, and {nationality, gender, company/area} (Examples 1-3). They might partially overlap in dimensions and/or measures; e.g., Examples 2 and 3 share nationality. Spade ensures that the results of evaluated MDAs are reused (not recomputed) in the other lattices where they appear.</p><p>Step 4 is Aggregate Evaluation. This step triggers the actual evaluation of the enumerated MDAs. To address challenge C2, we combine: (i) our novel early-stop technique to quickly prune the unpromising MDAs, and (ii) our MVDCube algorithm to efficiently compute the remaining MDAs in a single pass. The final results are produced in an incremental fashion and handled by the Aggregate Result Manager (ARM). The ARM stores them and incrementally updates statistics such as minimum and maximum values, as we explain in Section 4. These are used to determine the interestingness of the computed MDAs (by applying h) in one pass over their results.</p><p>Step 5 finally performs Top-k Computation. Once the evaluation is complete, the ARM retrieves all the evaluated MDAs, computes their interestingness score by applying h, and returns the k best aggregates. Spade natively supports three interestingness functions, from which the user can choose to suit their preferences: (i) variance, (ii) skewness, and (iii) kurtosis, where variance can detect deviation from uniform aggregate values, whereas the latter two can detect deviation from a normal distribution of aggregated values over numeric dimensions. Figure <ref type="figure" target="#fig_0">1</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">LATTICE-BASED COMPUTATION</head><p>We first recall a classical optimized method of computing all aggregates in a lattice. We then explain its limitations and the errors it makes in our setting. Finally, we present our new algorithm to compute lattices of RDF aggregates correctly and efficiently.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Classical one-pass lattice computation</head><p>In relational DWs, nodes in a multidimensional lattice are often computed from one of their parents to reuse computation and limit the number of passes over the data. Among the existing algorithms <ref type="bibr" target="#b38">[39]</ref>, ArrayCube <ref type="bibr" target="#b48">[49]</ref> computes the whole lattice in a single pass. Given a set of N dimensions, a measure, and an aggregate function, it relies on an array representation of data and evaluates 2 N nodes through a Minimum Memory Spanning Tree, as we recall below.</p><p>Array representation of data. The distinct values of each dimension are ordered, leading to a set of cells, each corresponding to a unique combination of indices of values along the N dimensions (axes). In Example 3, assuming nationality ∈ {A, B, F, L, N}, gender ∈ {F, M} and company/area ∈ {A, D, M, N} (we denote initials of the respective values in Figure <ref type="figure">3</ref>), the multidimensional space has 40 cells, e.g., in cell 0, nationality=A, gender=F and company/area=A; in cell 1, nationality=B, gender=F and company/area=A. Each cell of the N -dimensional array contains the value of the aggregated measure over all facts in that cell; in Example 3, this is the count of CEOs. Further, cells are grouped in partitions: each partition is a contiguous part of the array, containing the cells corresponding to a predefined number of distinct values along each dimension, e.g., if this is 2, the 40-cell array has 6 partitions. Figure <ref type="figure">3</ref>(a) shows the array. Note that an initial pass over the data is required to bring it from the relational to the array representation.</p><p>Minimum Memory Spanning Tree (MMST). The dimensions in Example 3 determine the lattice in Figure <ref type="figure">3(b)</ref>. To evaluate all nodes, ArrayCube chooses, for each non-root node A, a parent node to compute A from, hence forming a spanning tree of the lattice. The memory needed to evaluate all the aggregates in one pass over the data depends on the ordering of dimensions, their numbers of distinct values, and the partition size. ArrayCube chooses the tree that minimizes the overall memory needed; it is called the MMST.</p><p>Lattice computation proceeds as follows. The MMST is instantiated, allocating to each node the required memory. Partitions are loaded from the array representation of data, one at a time, into the root of the MMST. The content of each cell in the root is propagated to the children and used to incrementally update the aggregated measures of all the nodes in the MMST. Once a partition is evaluated, each node checks if it is time to store its memory content to disk. For instance, after scanning partition P1 in Figure <ref type="figure">3</ref>(a), the subarray with nationality ∈ {A, B} and company/area ∈ {A, D} is exhausted. Thus, the counts of CEOs with either of the two nationalities, and A or D company area are computed. Now, A 6 (Figure <ref type="figure">3(b)</ref>) can store its result to disk and reuse the memory in the subsequent computation. Similarly, once processed, the two subarrays of P2, (i) nationality ∈ {A, B}, and company/area ∈ {M, N}; (ii) nationality ∈ {A, B}, are exhausted, and both A 6 and A 5 can store their results to disk. A 6 stores its result after every partition, A 5 after every two.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Incorrectness in the RDF setting</head><p>Results computed by ArrayCube may be incorrect in the presence of multi-valued dimensions. Consider our running examples that show CEOs with various nationalities and at most one gender who manage companies in several areas. In a relational DW, each such CEO would be stored as a tuple in the fact table, and their multiple nationalities (respectively, company areas) would be modeled as a dimension table associating them with each of their nationalities (company areas). We could then find the result for Example 3 with a query q that joins all the relations, groups the data by the dimensions, and finally aggregates the measure. To evaluate all MDAs in the lattice determined by the dimensions, ArrayCube would use the MMST in Figure <ref type="figure">3</ref>(b) and compute the aggregate A 1 by means of q, using its result to compute the rest of the lattice.</p><p>Figure <ref type="figure" target="#fig_3">4</ref> shows the result of A 1 when applied to the two CEOs in Figure <ref type="figure" target="#fig_0">1</ref>. The tuples t 1 to t 3 are derived from Dos Santos (the RDF node n 1 ), whereas t 4 to t 11 are due to Carlos Ghosn (the RDF node n 2 ). Since n 2 lacks gender information, the tuples t 4 to t 11 have gender=null. We need to keep them to compute the rest of the lattice correctly. Since n 2 has valid values for nationality and company/area, we must count this CEO when computing aggregates over one or both of these dimensions, e.g., A 4 in Figure <ref type="figure" target="#fig_3">4</ref>. We obtain the result of A 2 by aggregating A 1 's result to project away the nationality dimension. For instance, the tuples t 4 , t 6 , t 8 , and t 10 , which are all associated with n 2 , collapse into the tuple t 4 in A 2 where now this CEO counts as four. Then, A 2 is further aggregated by projecting away company/area to compute A 3 and separately gender to compute A 4 . The cardinality "bug" introduced in A 2 propagates down the lattice. In A 4 's result, we find five CEOs managing Manufacturer companies, whereas there are only two. A similar error occurs in A 3 where we count three female CEOs because the tuples t 1 to t 3 of A 2 are aggregated into the same tuple and are, thus, counted three times (although they all represent n 1 ).</p><p>The above example shows that multiple values for a dimension may lead to errors when an aggregate is computed from one of its parents. To correctly compute the whole lattice from the root aggregate, naïve solutions may: (i) require that each CEO fact be represented by at most one tuple, e.g., in our example, by ignoring all but one of Ghosn's nationalities (company areas); this would clearly miss an interesting part of the data; (ii) compute each of the 2 N aggregates in the lattice separately, missing the benefits of efficient one-pass algorithms; this would entail a high run time overhead.</p><p>Interestingly, if we alter the query q to count distinct CEOs in each group (in lieu of count( * )), no errors occur in the result of Example 3. By design, ArrayCube cannot compute aggregates including distinct: instead, it computes all aggregates from the result of the lattice root, where information about individual facts is no longer present. Other one-pass algorithms for lattice-based aggregate computation, such as PostgreSQL's GROUP BY CUBE implementation <ref type="bibr" target="#b26">[27]</ref> (PGCube), do support the counting of distinct values and can thus be used to obtain the correct result for Example 3. However, in the presence of multi-valued dimensions, computing aggregates from the result of one of their parents in the lattice may still lead to wrong results, as illustrated in the following variations of Example 3.</p><p>Variation 1. Consider the aggregate "sum of the net worth of CEOs by nationality, gender, and area of the companies they manage". We first augment the data in the root aggregate A 1 with the sum of netWorth (NW ). The tuples t 1 to t 3 contain the NW of Dos Santos: $2.8 billion. The tuples t 4 to t 11 contain the NW of Ghosn: $120 million. We then compute the sum of NW by company/area. The tuples t 2 , t 5 , t 7 , t 9 , and t 11 (all having company/area=M) sum up into one tuple, and result in the sums of $2.8B of Dos Santos, and 4 × $120M of Ghosn, whereas both CEOs should have contributed exactly once. Moreover, we cannot solve this issue with the sum(distinct NW ) aggregate. If both CEOs had the same NW , a sum(distinct) would sum NW once, instead of (correctly) summing it twice.</p><p>Similarly, the following variation illustrates another scenario leading to wrong results.</p><p>Variation 2. Consider the aggregate "average age of CEOs by nationality, gender, and area of the companies they manage". We obtain it as sum(age)/count(age), i.e., the sum in Variation 1 is divided by 5. Instead, the correct value is sum of ages of Dos Santos and Ghosn divided by 2. As in Variation 1, we cannot solve this issue by using avд(distinct age).</p><p>As our experiments show (Section 6.3), the number of incorrectly computed aggregates, and the magnitude of the error itself, can be quite significant. This is because of the flexible RDF model, which allows multi-valued dimensions. Conversely, in a relational DW, once a fact table is joined with dimension tables, ArrayCube assumes that each fact has exactly one value for a dimension (for instance, due to a functional dependency). Below, we formally characterize the situations when ArrayCube introduces errors on RDF data.</p><p>Analysis of ArrayCube errors on RDF. Consider an RDF graph G and a lattice of N dimensions (2 N nodes) on G. Whether a lattice node can be computed correctly from one of its parents, depends on the presence of multi-valued dimensions in the lattice:  When computing C(G) from P(G), the aggregated value v 3 is obtained from t 1 .v 1 and t 2 .v 2 based on the function f . For instance, if f is count( * ), the fact n will be counted twice, instead of just once. If f is sum(M), the M value(s) of n will be summed twice, which falsifies the result (except for the particular case where their sum is 0). Computing the avд may similarly lead to wrong results. □</p><p>How does Lemma 1 impact the one-pass lattice-based computation for a given graph G? We show the following result: Theorem 1. Given an RDF graph G and a lattice on G, let MD ⊆ D be the set of all the dimensions for which some fact(s) n ∈ CFS have more than one value, and let K &gt; 0 be the size of MD. (i) A one-pass algorithm cannot compute correctly all the lattice aggregates. (ii) The maximum number of MDAs (lattice nodes) that can be computed correctly (depending on the choice of the MMST) is 2 N -K .</p><p>Proof. (i) Among the N • 2 N -1 lattice edges, K • 2 N -1 are labeled with a dimension from MD, meaning that the dimension is projected away when computation follows this edge. As Lemma 1 shows, if the MMST contains one such edge, the result of the child node of that edge may contain errors. However, no spanning tree, thus, no MMST, can avoid all edges labeled with a dimension in MD. This is because to go from the root, whose dimensions are D, to a node lacking one dimension D ∈ MD, by the construction of the lattice, the MMST must traverse an edge labeled D.</p><p>(ii) The lattice nodes that can be computed correctly in one pass (starting from the root's result) are exactly those having all the MD dimensions: a node lacking one such dimension would be obtained by aggregating a parent's result along that dimension, and thus, by Lemma 1, be wrongly computed. The lattice has 2 N -K such nodes. Fewer nodes may be computed correctly if the MMST picks a "wrong" edge, even if it could have avoided doing so. □</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">MVDCube Algorithm</head><p>We now present Multi-Valued Data Cube (MVDCube), our new one-pass MDA evaluation method. Going beyond existing algorithms <ref type="bibr" target="#b48">[49]</ref>, MVDCube: (i) produces correct results even in the presence of missing or multi-valued dimensions and/or measures, (ii) computes several aggregate functions over a large set of measures in the same lattice, and (iii) saves computation cost by sharing measures across all lattices from a given CFS.</p><p>Before we move forward, we clarify that our RDF database uses the following storage: a CFS is represented by a single-column table storing the identifiers (IDs) of the facts; for each attribute a, a table t a stores (s, o) pairs for each (s, a, o) triple in the RDF graph.</p><p>Figure <ref type="figure" target="#fig_6">5</ref> depicts the main features of MVDCube. Our MVDCube evaluation method proceeds in the following steps, with the pseudocode of its core functions shown in Algorithm 1.</p><p>Building MMSTs. Given a CFS and a set of lattices (identified in Step 3 of Spade's pipeline), each with the dimensions D i and the measures M i , we construct one MMST per lattice as in <ref type="bibr" target="#b48">[49]</ref>.</p><p>Data Translation. For each lattice, we process the root node by sending a join query to the database to obtain all the CFs that have a value for at least one of the dimensions in D i . We then translate the join result to lay the data in a partitioned array representation of cells. A partition is a set of pairs (cell index, CF). We assign each RDF node a cell index based on its dimensions' values; in the case of multiple values for a dimension, we assign indexes of all corresponding cells. We add the special value null in the domain of each dimension to account for missing values. Therefore, each cell is associated with the set of RDF nodes that correspond to the combination of dimension values that this cell represents. Like ArrayCube, we take an initial pass over the data to bring it into the array representation, where the (conceptual) multidimensional array is stored as a serialized one-dimensional array. If the data does not fit into the available memory, we partition it, store to disk, and later read back, one partition at a time; otherwise, MVDCube accesses the array directly from the main memory, in a single pass, in subsequent steps.</p><p>Measure Loading is performed in parallel to the Data Translation step. For each measure M in M i , we query the database to retrieve, for each CF, the pre-aggregated values of M (which were computed and stored offline). We load the values ordered by the IDs of the CFs, and share them among all MMSTs in a given CFS. As they are stored at the granularity of a CF, they can be used to compute aggregate results for all cells, as we describe below.</p><p>Lattice Computation is then carried out in one pass over the data using the MMST. MVDCube associates an MMST node with a (large) set of aggregates; we denote such a node as A i = ⟨CFS, D j ⟩. Each node then represents all the MDAs that have dimensions D j (but might differ in their measure and aggregate function). Suppose that we want to compute the lattice with D={gender, company/area, nationality}, M={age, netWorth} and that age is associated with avд, and netWorth is associated with sum. Node A 2 in Figure <ref type="figure" target="#fig_6">5</ref> represents the two MDAs: (i) average age of CEOs, and (ii) sum of netWorth of CEOs, both grouped by gender and company/area.</p><p>In the MMST, we allocate, for each node, the needed memory. We load partitions successively into the root. In Figure <ref type="figure" target="#fig_6">5</ref>, we assume that each partition contains 3 distinct values of each dimension, hence 27 cells. For compactness, we encode each set of RDF nodes in a cell using a Roaring Bitmap <ref type="bibr" target="#b31">[32]</ref> (also adopted in Spark because of the strong compression and lookup performance). In Figure <ref type="figure" target="#fig_6">5</ref>, each cell stores a set of CEOs (a subset of the facts n 1 and n 2 ). The bitmaps follow the same ordering of the CFs applied during Measure Loading. The cell of index 3 in A 1 contains a bitmap of size 2, BM 3 = 10, representing that n 1 is in the set, whereas n 2 is not.</p><p>(a) Projection and bitmap propagation. We scan the bitmaps in the cells of the root node and immediately propagate them to the child nodes in the MMST as dimensions are projected away (line 4 in Algorithm 1). We union (OR) the bitmap in each cell in a child node with each bitmap received from the parent (line 9): this models the contribution of all facts in a parent node to the corresponding cell in the child node. In particular, as we project away a multi-valued dimension from a parent node to a child node, if a fact has multiple values of the dimension, it belongs to different cells in the parent node, but will be consolidated in the same cell in the child node.</p><p>Red arrows in Figure <ref type="figure" target="#fig_6">5</ref> show propagations. For example, the bitmap of cell 2 in node A 4 , BM 2 , is initially empty (i.e., 00). Then  it is updated to 01 when BM 8 from A 2 is propagated, and later to 11 when BM 2 from A 2 is also propagated.</p><p>Once a partition is evaluated, we apply the ArrayCube check (Section 4.1) in the nodes to learn if it is time to write results to disk (line 10). If so, we first propagate their memory content to their child nodes (line 11), and then we compute the values of the aggregated measures and store them (line 12).</p><p>(b) Measure computation (denoted as ⊗). When a node is ready to write to disk, we scan its memory one cell at a time. For each cell: (i) we identify the pre-aggregated measures of each RDF node in the cell's bitmap, and (ii) we apply the relevant aggregate functions to them. Note that measure computation is very fast as both the bitmaps and the pre-aggregated measures are ordered by the fact ID, and can aggregate different measures simultaneously.</p><p>Revisit A 4 in Figure <ref type="figure" target="#fig_6">5</ref>. Once P1 and P2 are evaluated, A 4 is ready to write current results to disk. We scan the three cell bitmaps, and for each bitmap: (i) identify the age and the net worth of each CEO in the bitmap by accessing the pre-aggregated measures, (ii) aggregate the respective measures by applying avд on the age and sum on the net worth. For example, for BM 2 , we identify the ages (respectively, net worth) of n 1 : 47 ($2.8B) and n 2 : 66 ($120M) because they are both present in the bitmap, then compute their average (respectively, sum). The Aggregate Result Manager (line 19) receives the computed measures, and the values of the dimensions obtained from the cell index. Finally, we empty A 4 's memory in the MMST and reuse it to evaluate the aggregate on the next partition (line 13).</p><p>Memory usage. Our memory analysis builds on the corresponding ArrayCube study <ref type="bibr" target="#b48">[49]</ref>. Assuming N dimensions with d distinct values each and c distinct values per partition, the MMST uses at most M T = c N + (d + 1 + c) N -1 array cells to compute one aggregated measure. In MVDCube, the memory for an MMST is also upper bounded by M T cells. However, cells have a variable size as each of them contains a Roaring Bitmap (RB). For this reason, we provide a worst-case estimation of MVDCube's memory needs for the MMST and the pre-aggregated measures.</p><p>(a) The size of an RB used to store Z integers in the interval [0, u) is bound in <ref type="bibr" target="#b31">[32]</ref> to M RB = 2 • Z + 9 • (u/65535 + 1) + 8, that is, beyond a fixed overhead for u, the universe size, RBs never use more than 2 bytes per integer. In the worst case,  is the set of aggregate functions assigned to the measure. As an optimization, we detect, offline, the numeric properties having at most one value for all their RDF nodes, e.g., the age of CEOs. To save memory, we allocate a single float number for all pre-aggregated results (min, max, and sum) for such properties.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EARLY-STOP AGGREGATE PRUNING</head><p>To reduce the effort required to compute lattices of aggregates, we have developed a novel technique called early-stop (ES).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">The early-stop principle</head><p>Given an aggregate A = ⟨CFS, D, M, f ⟩ and an interestingness function h, finding, how interesting A is, amounts to evaluating a query of the form:</p><formula xml:id="formula_1">SELECT h(aggregated) FROM (SELECT D 1 , D 2 , . . . D N , f (M ) AS aggregated FROM C F S D, M GROUP BY D 1 , D 2 , . . . , D N ) AS inner;</formula><p>where CFS D, M is CFS joined with dimensions D and the (preaggregated) measure M. Note that we only need to present the result of the inner query to the user, if A ends up in the top-k. This leads to the following idea: we could reduce the effort to compute some aggregates if we can determine (with high probability) that they will not be among the k most interesting ones.</p><p>The literature <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b29">30]</ref> introduced conservative and large-sample confidence intervals as means of estimating the result of a query such as inner but not the result of the full nested query, i.e., the interestingness score that we aim to obtain. Recent work on visualization recommendation <ref type="bibr" target="#b42">[43]</ref> shows how to stop the evaluation of low-utility one-dimensional aggregates early on relational data. In doing so, it relies on a worst-case (conservative) confidence-intervalbased pruning. In contrast, we extend the line of research on aggregate pruning by constructing a large-sample confidence interval around the interestingness score estimator. We provide our novel approach and formalize its probabilistic guarantees below.</p><p>To enable early-stop pruning, we estimate the interestingness of the aggregate A using an estimator H r , and bound this approximate score within our large-sample confidence interval. (We derive the formula for the interval in Section 5.2.) We draw from each aggregate group a sample containing the same number of facts. For the sake of efficiency, our sampling procedure proceeds in batches of a given size. After scanning a batch, we update the estimate of the aggregate's interestingness based on the (pre-aggregated) measure values of the facts in the batch. To prune some aggregates, if we find that the upper-bound on the estimate of A's interestingness is lower than the current lower-bound of the k-th best aggregate, we can give up evaluating A, and thus obtain the top-k aggregates more quickly. The central part of Figure <ref type="figure" target="#fig_6">5</ref> illustrates this with five aggregates and k = 3: the fifth aggregate can be stopped after the current batch, whereas the estimation of the fourth aggregate will continue in the next batch. This procedure terminates once the sample is exhausted or no aggregates have been pruned in a given number of batches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Estimating the interestingness score</head><p>Notation recall. A simple random sample of size r is a vector [v 1 , . . . , v r ] of values drawn uniformly without replacement from a population V of size R; the sample is modeled by a set of independent, identically distributed (i.i.d.) random variables X 1 , . . . , X r .</p><p>An estimator is a random variable equal to a linear or nonlinear combination of X 1 , . . . , X r (typically modeling a simple random sample). Evaluating the estimator on a vector [v 1 , . . . , v r ] of concrete values taken by these random variables yields an estimation.</p><p>Let S be a statistic of V , S r be an estimator of the true value of S based on a sample of size r , and (1 -α) be a confidence level for 0 ≤ α ≤ 1. Then, a (1 -α)-confidence interval (CI) is a random interval such that for each 1 ≤ r ≤ R, P( S r -ε r ≤ S ≤ S r + εr ) = 1 -α. One interval is derived deterministically from one sample; the probability is taken over all such intervals. We denote L r = S r -ε r and U r = S r + εr , respectively, the lower and the upper bounds at (1-α) confidence level on S r . As in <ref type="bibr" target="#b29">[30]</ref>, the large-sample confidence interval contains the true value with the probability approximately equal to 1 -α.</p><p>Constructing the estimator. We begin by developing formulas for the point estimator H r of the query's result when the aggregate function (f ) in use is count, sum, or avд and the interestingness function (h) is variance, skewness, or kurtosis. We first detail this for avд, and variance and then discuss extensions to other functions.</p><p>Let д 1 , д 2 , . . . , д G be the aggregate groups of A and µ = (µ 1 , µ 2 , . . . , µ G ) ⊺ be the true result of A, that is, the vector containing, for each group, the average of the pre-aggregated values of M for facts from that group. Further, for each group д i , let Ȳi = 1 r r j=1 X j be the sample mean estimator, where the variable X j has mean µ i and variance σ 2 i and models the (pre-aggregated) measure value of the j-th fact of the sample of size r , drawn from the facts in д i . Note that, from the Central Limit Theorem (Theorem 5.5.14 in <ref type="bibr" target="#b7">[8]</ref>), each Ȳi ∼ N (µ i ,</p><formula xml:id="formula_2">σ 2 i r ) as r → ∞, where N (µ i , σ 2 i ) is the normal distribution centered in µ i with standard error σ i .</formula><p>We estimate H r (µ) with H r ( Ȳ ), where Ȳ = Ȳ1 , Ȳ2 , . . . , ȲG ⊺ is the vector of all the group estimators. We thus obtain the (unbiased) estimator of the variance of a vector y = (y 1 , y 2 , . . . , y G ) ⊺ :</p><formula xml:id="formula_3">H r (y) = 1 G -1 G i=1 y i - 1 G G j=1 y j 2<label>(1)</label></formula><p>Deriving CI bounds. We aim at providing a large-sample confidence interval around H r ( Ȳ ). Our formal result is as follows: Theorem 2. Let H r be the estimator of variance. There exists an error ε r &gt; 0 such that H r (µ) ∈ [ H r ( Ȳ ) -ε r , H r ( Ȳ ) + ε r ] with the probability approximately equal to 1 -α.</p><p>Proof. We prove Theorem 2 constructively, thus exhibiting a concrete formula for ε r . To derive the confidence interval, first, we approximate H r ( Ȳ ) around µ using the first two terms of its Taylor series expansion: H r ( Ȳ ) ≈ H r (µ)+ ∇ H r (µ) • ( Ȳµ). Then, we apply the Multivariate Delta Method (Theorem 5.5.28 in <ref type="bibr" target="#b7">[8]</ref>) to state that √ r H r ( Ȳ ) -H r (µ)</p><formula xml:id="formula_4">D -→ N (0, τ 2 ) (<label>2</label></formula><formula xml:id="formula_5">)</formula><p>where</p><formula xml:id="formula_6">D -→ denotes convergence in distribution, τ 2 = G s=1 G t =1 σ s,t ∂ H r (µ) ∂y s ∂ H r (µ) ∂y t , σ s,t = Cov( Ȳs , Ȳt ) for 1 ≤ s, t ≤ G.</formula><p>In other words, the difference between the correct value of interestingness, H r (µ), and that on the estimator, H r ( Ȳ ), converges in distribution to a 0-centered normal distribution.</p><p>To apply this theorem, we must show that (1) H r has continuous first partial derivatives and that (2) τ 2 &gt; 0. Condition (1) can be easily shown by applying basic calculus on Eq. 1. For (2), we assume that Ȳ1 , Ȳ2 , . . . , ȲG are independent random variables. Hence, for</p><formula xml:id="formula_7">1 ≤ s, t ≤ G, if s t, then Cov( Ȳs , Ȳt ) = 0, else Cov( Ȳs , Ȳt ) = Var( Ȳs ) = σ 2 s r , and τ 2 = G s=1 σ 2 s r 2 G-1 µ s -1 G G i=1 µ i 2 is positive.</formula><p>We now move toward a formula for the confidence interval based on the samples in the groups. We derive it by "standardizing" the distribution of the difference obtained in Eq. 2, and taking quantiles of the standard normal distribution, N (0, 1), as the interval's ends.</p><formula xml:id="formula_8">Let τ 2 = G s=1 σ 2 s r 2 G-1 Ȳs -1 G G i=1 Ȳi 2</formula><p>, where σ 2 s are (unbiased) estimators of variances in all the G groups. From the Strong Law of Large Numbers (Theorem 5.5.9 in <ref type="bibr" target="#b7">[8]</ref>), we have that lim r →∞ τ 2 = τ 2 almost surely. Then, applying Slutsky's theorem (Theorem 5.5.17 in <ref type="bibr" target="#b7">[8]</ref>), we get</p><formula xml:id="formula_9">√ r H r ( Ȳ ) -H r (µ) / τ 2 D -→</formula><p>N (0, 1). In turn, for large r , we obtain:</p><formula xml:id="formula_10">P H r ( Ȳ ) -H r (µ) ≤ ε r = P √ r H r ( Ȳ )-H r (µ) √ τ 2 ≤ ε r √ r √ τ 2 ≈ 2Φ ε r √ r √ τ 2</formula><p>-1 , where Φ denotes the cumulative distribution function of a normally distributed variable.</p><p>Let z p be the r . Finally, choosing z p = z 1-α we obtain the approximation at the desired confidence level:</p><formula xml:id="formula_11">P H r ( Ȳ ) -H r (µ) ≤ z 2 1-α τ 2 r ≈ (1 -α) □</formula><p>Other interestingness functions. To derive confidence intervals for skewness and kurtosis, we follow similar derivations by replacing the definition of H r (Eq. 1) with their respective formulas. We derive the CIs based on the Delta Method -both cases exhibit continuous first partial derivatives; see Appendix A. In general, one can derive similar formulas for any interestingness function that meets conditions ( <ref type="formula" target="#formula_3">1</ref>) and ( <ref type="formula" target="#formula_4">2</ref>).</p><p>Other aggregate functions. For sum, we estimate the group sizes while sampling and compute the estimate as a product of the avд and count estimates. For min and max, we use the sample min and the sample max, respectively, as point estimates; we apply Popoviciu's and Szőkefalvi-Nagy's inequalities <ref type="bibr" target="#b40">[41]</ref> for the upper and lower bounds, respectively. See Appendices B, and C for details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Plugging early-stop into MVDCube</head><p>We integrate early-stop into MVDCube to speed up Aggregate Evaluation, and thus address challenge C2. The evaluation of an MMST begins with the Data Translation step, run in parallel with Measure Loading (recall Section 4.3). We exploit the data translation to create a stratified sample of facts for the early-stop pruning. Given the MMST, each address in the multidimensional space in the root corresponds to a unique group of facts. We allocate empty reservoirs R 1 , R 2 , . . . , R G , one per aggregate group, each with a capacity equal to the sample size: this way we ensure stratification. While reading each tuple, we determine its group, hence also the reservoir, and either put the fact in or not with some probability. If the reservoir is full, we discard one of the previously inserted facts. This strategy is known as reservoir sampling and guarantees a choice of a simple random sample <ref type="bibr" target="#b43">[44]</ref>. Figure <ref type="figure" target="#fig_6">5</ref> shows an on-going sampling process with four reservoirs R 1 to R 4 , each of size 3.</p><p>The sample thus obtained is used by early-stop as follows. Once the translation is finished, we propagate the facts sampled from the MMST's root down the tree using Roaring Bitmaps as in MVDCube (see Figure <ref type="figure" target="#fig_6">5</ref>): each node in the MMST receives its own sample. Then, we perform the early-stop pruning based on these samples. All the aggregates that have not been pruned (deemed sufficiently interesting) by early-stop are subsequently evaluated by MVDCube.   Systems. We implemented Spade in Java 1.8 (18k lines of code); it relies on OntoSQL 1.0.12, an efficient RDF storage and query answering platform on top of an RDBMS <ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref> (PostgreSQL in our case). We compare the performance of our aggregate evaluation method against the best-effort baseline, which uses PostgreSQL's GROUP BY CUBE implementation, since 2016 based on an efficient one-pass computation of all aggregates in a lattice <ref type="bibr" target="#b25">[26]</ref>, that supports additional features such as count(distinct), which were not available in ArrayCube <ref type="bibr" target="#b48">[49]</ref>. We denote this by PGCube. As discussed in Section 4.2, PGCube may fail to compute correct results in the presence of multi-valued dimensions. However, the support for counting of distinct values may help PGCube correct some wrong results. Thus, we consider two variants: (i) PGCube computing counts using count( * ), denoted PGCube * , and (ii) PGCube computing counts using count(distinct), denoted PGCube d . In both cases, our Java code is at a disadvantage against a C/C++ engine.</p><p>Real-world graphs. Our experiments involve a set of realapplication RDF graphs, for which Table <ref type="table" target="#tab_5">2</ref> shows: the number of triples, the number of CFSs, the number of (direct) properties and derived properties (#P and #DP, respectively) in the graph, and the number of aggregates without and with derivations (#A woD and #A w D , respectively). The graph sizes in this work are similar to the real-world dataset sizes used in comparable relational works, e.g., 20k tuples in <ref type="bibr" target="#b41">[42]</ref>, and up to 60M tuples in <ref type="bibr" target="#b42">[43]</ref>. Airline was originally a relational dataset on flight delays used in prior work <ref type="bibr" target="#b42">[43]</ref>; we converted it into RDF (each tuple becomes a CF with a fixed set of properties), whereas the others are natively RDF. We discuss differences between this and the other graphs shortly. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Analysis of example results</head><p>We begin by showing, in Figure <ref type="figure" target="#fig_10">6</ref>, example interesting aggregates found by Spade when using variance as an interestingness score:</p><p>(a) "Minimum net worth of CEOs by gender and occupation": (i) there are two outliers, male philanthropists and male shareholders: their minimum net worth is much higher than others'; (ii) the net worth value is known for all but one occupation for male CEOs, but only in a half of them for female CEOs; (iii) the minimum net worth of female CEOs is nearly the same across occupations.</p><p>(b) "Number of launches by launch site and nationality" in the NASA graph: (i) very high values for USSR spacecrafts launched from Plesetsk and Bajkonur; (ii) the most used USA launch sites are Cape Canaveral and Vandenberg Base.</p><p>(c) "Average mass of spacecrafts by discipline": here 4 disciplines, i.e., Human crew, Microgravity, Life sciences and Repair stand out with the average spacecraft mass significantly higher than others'.</p><p>Nonetheless, many candidate MDAs are uninteresting: Figure <ref type="figure" target="#fig_12">8</ref> shows the aggregate "minimum number of occupations of CEOs by gender and number of companies" in the CEOs dataset, where all aggregated values are uniformly equal to 1; or "average number of launched vehicles by launch site" in the NASA dataset, where most values are equal to 1, and only 8 out of 35 bars are slightly higher but still less than 1.05. These aggregates don't exhibit any significant outliers and were therefore ranked low by Spade. This confirms the need for using early-stop to prune such MDAs.</p><p>It could have been in principle envisioned to compare the interestingness of the aggregates found by our system with that of some manually chosen aggregates. However, doing so is hampered by the lack of feasible selection methods available to human users. For this reason, the starting point of our work is precisely the observation that it is very hard to select aggregates manually. There are several reasons for this: (i) The sheer size of the graph impedes human understanding, and it is hard to induce human users to attempt solving such a computationally expensive task at all. Even if they did try to solve it, typically, such users would use a simple SPARQL engine that can evaluate aggregates, and hence they would have to formulate the queries themselves, which requires expertise in writing such complex aggregate queries. (ii) Even if we reduce a graph to a modest size, e.g., through summarization <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b33">34]</ref> or sampling, the reduced graph may not reflect (a) all possible combinations of facts, dimensions, and measures in the original data; (b) the graph values, e.g., the frequent values and value distributions; or (c) any derived properties. Even under strong (unrealistic) assumptions, e.g., (b) and (c) are both known for a simple, regular RDF graph, users would still not know which aggregates are interesting (e.g., deviating from a uniform distribution) before enumerating and evaluating them all at least partially. (iii) Supporting interactions with the system leads users inevitably to inject some information about their preferences in the aggregate selection process. For example, in the NASA dataset, the users may prefer to investigate launches grouped by the launch site rather than the discipline of the spacecraft staff. In contrast, Spade is a fully automated approach to discovering statistically interesting aggregates, with no user input required. It defines and enumerates a large set of candidate aggregates by applying heuristics to generate potentially interesting dimensions and measures and evaluates them efficiently.</p><p>As examples in Figure <ref type="figure" target="#fig_10">6</ref> show, our highly-ranked results returned from the six real datasets reveal interesting insights. Due to the automatic nature of Spade, in some datasets, there may be a small fraction of aggregates that, despite being statistically sound, are unlikely to be chosen by the user. For example, the aggregate minimum net worth of CEOs by nationality/image uses a derived property, nationality/image, which is statistically similar to other meaningful dimensions, e.g., nationality/label, but the user is unlikely to choose it. This indicates that a "human-in-the-loop" approach can further improve the effectiveness of our automated approach. While for the above example, the user can simply add nationality/image to a stop list for dimensions, a full design of "human-in-the-loop" data exploration will be a focus of our future research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">The benefits of derived properties</head><p>We begin our evaluation by validating the benefits of Derived Property Enumeration (Section 3). This step is crucial to address challenge C1. We show that it allows us to increase the pool of attributes and to generate a large and rich space of interesting aggregates.</p><p>Experiment 1. We compare the results of our analytical strategy when: (i) only RDF graph properties were used for the analysis   (woD), and (ii) derived properties were also considered (wD). As Table <ref type="table" target="#tab_5">2</ref> shows, the Airline dataset (originally relational) leads to no derivations: tuples are not linked to each other, and thus no paths can be derived; it lacks multi-valued attributes, thus no count derivation applies; the data is mostly numeric, so keyword or language attributes are not derived. The other (native RDF) graphs differ drastically: they feature several CFSs, multi-valued properties, links among RDF nodes leading to many path derivations (Table <ref type="table" target="#tab_5">2</ref> shows counts of path derivations of length 1, as they are the most numerous); textual attributes are also quite frequent. Our first main observation, denoted as remark (R1), is that (i) derivations increase the total number of enumerated MDAs: for instance, on Foodista, no MDA exists without derivations, whereas we find several by deriving the recipe language, the count of ingredients, etc.; on DBLP, only year is a good dimension, whereas through derivations we obtain, e.g., keyword(title); (ii) derivations increase the interestingness of the best aggregates.</p><p>Henceforth, we enable derivations in our experimental analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Analysis of MVDCube against PGCube</head><p>Our next set of experiments focuses on Aggregate Evaluation, the last step of our online pipeline, where most computation takes place. Since PGCube is not able to prune unpromising aggregates, for fairness, in this section, early-stop is disabled. Experiment 2. We compare MVDCube with PGCube in run time and quality (correctness). Recall that PGCube's results may be erroneous (Section 4.2). We use the six real graphs with derivations.</p><p>Regarding the run time, Figure <ref type="figure" target="#fig_13">9</ref> shows MVDCube against PGCube * and PGCube d on our real datasets. We observe that MVD-Cube achieves a time gain of 20% to 80% over PGCube * and of 30% to 83% over PGCube d on most datasets (R2). Specifically, MVDCube outperforms PGCube when there are many (more than 15) aggregates to evaluate (R3). This is because MVDCube: (i) shares measures across all the aggregates from the same CFS, and (ii) computes each aggregate only once, even if it appears in several lattices. In contrast, PGCube evaluates each lattice in a separate query, each of which joins the facts with the measures. Except for the Foodista dataset, which has a small number of aggregates and both methods run under a second, MVDCube shows significant gains on CEOs, NASA and Nobel Prizes graphs, where many MDAs are evaluated, MVD-Cube gains 40% over PGCube. Similarly, Airline leads to almost 6k MDAs, the dataset is rather large (6M facts), and the repeated joins are expensive: PGCube * takes 5 times MVDCube's time.</p><p>Regarding the errors, Table <ref type="table" target="#tab_6">3</ref> shows, for each graph, the number of aggregates with incorrect results (#wrong aggs) for PGCube * and PGCube d . We observe that PGCube * and PGCube d produce errors in, respectively, 14% and 12% of all computed aggregates (R4). PGCube d , PGCube's best effort to generate correct results, still produces errors in 9% to 21% of the computed aggregates across different datasets. As shown in Section 4.2, errors are related to multi-valued attributes in the data. Indeed, CEOs, NASA, and Nobel Prizes datasets have the greatest number of multi-valued attributes and the highest error, ranging from 12% to 21%.</p><p>Experiment 3. We now quantify the error in those aggregates that are computed wrongly by PGCube d . Given an aggregate A, we denote m A j the value of the aggregated measure of the j-th group in A, as computed by MVDCube. We denote by p A j the value that PGCube d computes for the same group. As p A j can only be higher than or equal to the correct value m A j , ideally, this ratio should be 1. When an aggregate is shared by two lattices, it can be computed from either lattice, leading to different error ratios. When this happens, we record the maximum error, to measure the "worstcase risk" incurred by evaluating the lattice through PGCube. Each aggregate thus leads to a set of error ratios, one per group. Figure <ref type="figure" target="#fig_14">10</ref> shows their distribution, for count and sum aggregates, for the four datasets from Table <ref type="table" target="#tab_6">3</ref> where errors were detected. We note that errors can easily exceed one order of magnitude (R5): in 3 out of 4 cases, PGCube d produces at least 1 tuple whose value is more than 30 times the true value. In CEOs, one group records an error ratio greater than 10 3 ; it comes from a three-dimensional lattice where all dimensions were multi-valued. Such incorrect values would severely falsify the selection of the k most interesting aggregates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Impact of early-stop on MVDCube</head><p>Experiment 4. We next study the effectiveness of our early-stop technique (ES). For our real graphs, Table <ref type="table" target="#tab_8">4</ref> shows: (i) the evaluation time taken by MVDCube alone, (ii) the time with ES enabled, as described in Section 5.3, (iii) the time gain due to ES, (iv) the fraction of aggregates pruned and (v) the accuracy of ES. Following <ref type="bibr" target="#b42">[43]</ref>, |. We show this for k ∈ {3, 5, 10}, in keeping with comparable works in a relational DW setting <ref type="bibr" target="#b42">[43]</ref> and using a sample size of 60 with 2 batches, a configuration we found empirically to work well. Table <ref type="table" target="#tab_8">4</ref> leads to two observations. First, ES can bring significant evaluation time gains, from 10% to 43% in our experiments; and it aggressively prunes uninteresting aggregates, frequently as much as 70% (R6). ES is especially beneficial on graphs with more than 100 aggregates, except for DBLP, where translating the data into an array representation is much more expensive than evaluation, and thus, the saved evaluation effort appears small. In some cases, the impact of ES was negative (and very small), due to a sampling overhead. Second, MVDCube with ES is often quite accurate (R7): 100% accuracy is attained in the majority of cases, except for Nobel Prizes, where, e.g., the true top-10 contains aggregates with interestingness score greater than 10.49, whereas ES returns those greater than 9.45.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5">Scalability study</head><p>We finally analyze the scalability of our approach and compare it with PGCube, when varying different data characteristics. To be able to fully control them, we designed a synthetic benchmark (a set of graphs) with fixed numbers of facts |CFS |, N dimensions and M measures. All property values are numeric. We ensure that a single CFS is found and that each dimension D i , 1 ≤ i ≤ N , takes at most 100 values (so that they are considered good dimensions, recall</p><p>Step 2 in Section 3). We denote each graph by |D 1 | : |D 2 | : . . . : |D N |, the maximum number of distinct values along each dimension. To obtain realistic distributions of the facts in this multidimensional space, we randomly assign dimension values as in <ref type="bibr" target="#b0">[1]</ref>, controlled by a sparsity parameter s ∈ [0, 1]. To ensure PGCube correctness, each fact has only one value for each dimension. Experiment 5. We analyze the performance of the entire online pipeline of Spade on benchmark datasets. We use 12 configurations, each having |CFS |=1M, 3 dimensions, and 3, 5, or 10 measures. We also use (i) two different combinations of distinct values for dimensions, 100:100:100 (uniform) and 100:5:2 (decreasing), and (ii) two different sparsity coefficients, 0.1 and 0.5. In Figure <ref type="figure" target="#fig_16">11</ref>, each bar represents one configuration ("u" or "d" for value distribution | sparsity coefficient | number of measures) and reports the total execution time of Spade using MVDCube without early-stop. Each segment of a bar covers one computation step (recall Figure <ref type="figure" target="#fig_2">2</ref>). In the pipeline order of steps, we observe that: (i) Candidate Fact Set Selection is too fast to be visible; although there is only one CFS here, in all our experiments with real graphs, it was 5-10 ms. (ii) Online Attribute Analysis's time is noticeable, between 15% and 37% of the total time, and increases with the number of measures: Spade must analyze them before deciding that they are not suitable dimensions. (iii) Aggregate Evaluation dominates the processing time; it increases with the number of distinct groups and the number of measures as each measure leads to a different aggregate. (iv) The time to select the best aggregates (evaluate their interestingness and pick the top-k) is also noticeable and grows as expected with the number of aggregates. (v) Sparsity has a moderate impact. From these results, we conclude that for a fixed CFS, Aggregate Evaluation dominates Spade's execution, increasing with the number of distinct groups and the number of measures; Online Attribute Analysis has the second-highest cost, growing with the number of attributes (R8). Experiment 6. We now study the impact of |CFS |, N , and M on the performance of Spade. As a base configuration, we fixed the synthetic graph with |CFS | = 5M, 3 dimensions, and 15 measures (generated as above). For each dimension, we set the uniform value distribution (as above) and sparsity 0.1, as Experiment 5 proved this configuration to be the most difficult. Figures <ref type="figure" target="#fig_24">12a,</ref><ref type="figure" target="#fig_24">12b,</ref><ref type="figure" target="#fig_24">12c</ref> show the total execution time of Spade's online pipeline when we vary |CFS | ∈ {1M, 2.5M, 5M, 7.5M, 10M}, M ∈ {5, 10, 15, 20, 25, 30}, and N ∈ {1, 2, 3, 4}, respectively; the Aggregate Evaluation step was executed through PGCube * , MVDCube, and MVDCube with early-stop as evaluation modules. We chose PGCube * as on these graphs it is correct, and it is faster than PGCube d . The figures show that MVDCube scales linearly when |CFS | and M grow, and its run time increases more with N ; the latter is expected given the high number of lattices that are enabled by more dimensions. Further, Spade using MVDCube is consistently faster than using PGCube * by up to 2.9×; it also scales better as |CFS |, N and M grow, and MVDCube with early-stop is consistently the fastest (R9). Note that in Figure <ref type="figure" target="#fig_24">12b</ref>, MVDCube with early-stop took slightly longer for M=10 than for M=15: in these cases, the random samples drawn by early-stop (Section 5) were less helpful for M=10 than for M=15.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.6">Experimental conclusions</head><p>Our experimental results established, first, the need for a novel framework for finding interesting aggregates in RDF graphs: in heterogeneous graphs lacking well-defined facts, dimensions, and measures, Property Derivation increases significantly the space of interesting aggregates (R1). Due to multi-valued dimensions, relational aggregate evaluation algorithms often introduce errors (R4), which can be very significant (R5). On real-world graphs, our algorithm, MVDCube, not only produces correct results but is also faster (by 20% to 80%) than the best comparable (PostgreSQL) baseline (R2), (R3). Our novel early-stop technique reduces MVD-Cube's run time by 10% to 43% in many cases (R6), while remaining accurate (R7). In the entire online pipeline of Spade, the most timeconsuming steps are Aggregate Evaluation, followed by Online Attribute Analysis (R8). MVDCube consistently outperforms PGCube while scaling in the number of facts, measures, and dimensions; early-stop further improves the performance (R9).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">RELATED WORK</head><p>Graph exploration. By providing visually meaningful, interactive interfaces, RDF graph visualization <ref type="bibr" target="#b39">[40]</ref> allows casual users to access the data in RDF graphs. Based on the graph structure, content, and/or semantics, RDF summarization <ref type="bibr" target="#b8">[9]</ref> computes a synopsis (summary) of the data, encapsulating the essential information of the graph from a given perspective. Example-based graph exploration, such as in <ref type="bibr" target="#b32">[33]</ref>, helps users discover data based on examples they specify. Our work is complementary to these approaches.</p><p>Insight extraction from multidimensional data is a common technique for data exploration. Research conducted in <ref type="bibr" target="#b41">[42]</ref> and <ref type="bibr" target="#b15">[16]</ref> provides automatic extraction of the top-k insights from multidimensional relational data. An insight is an observation    derived from aggregation in multiple steps; it is considered interesting when it is remarkably different from others, or it exhibits a rising or falling trend. Multi-structural databases <ref type="bibr" target="#b19">[20]</ref> distribute data across a set of dimensions, compare two sets of data along given dimensions, and separate the data into cohesive groups. A smart drill-down operator <ref type="bibr" target="#b30">[31]</ref> is proposed for interactively exploring a relational table to discover groups of tuples that are frequent, specific, and diverse. Works in this area assume a fixed relational schema; more recently, they consider graphs as in <ref type="bibr" target="#b3">[4]</ref>, but, unlike Spade, they require them to have a very regular structure. Visualization recommendation. SeeDB <ref type="bibr" target="#b42">[43]</ref> identifies, in relational data, the one-dimensional aggregates that exhibit the largest deviation between a target dataset and a reference dataset. A study in <ref type="bibr" target="#b18">[19]</ref> lays out a recommendation scheme for top-k aggregate visualizations from relational data using a multi-objective utility function to prune as many low-utility views as possible. Recent work <ref type="bibr" target="#b47">[48]</ref> shows how to automatically discover the utility function to match the user intentions. DeepEye <ref type="bibr" target="#b34">[35]</ref> finds and ranks visualizations by combining a binary classifier, supervised learning, and expert rules. QAGView <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b45">46]</ref> provides summaries of high-valued aggregate query answers that ensure properties including coverage, diversity, and relevance, customized based on user preferences. LensXPlain <ref type="bibr" target="#b37">[38]</ref> helps users understand answers to aggregate queries by providing the top-k explanations.</p><p>In contrast to these works, Spade applies on a schemaless RDF graph, and hence must automatically derive those dimensions and measures that are good candidates to produce some insights.</p><p>Cube computation is at the heart of multidimensional data analysis and has been intensely studied <ref type="bibr" target="#b38">[39]</ref>. To limit the number of scans of the data and to share computation as much as possible, many algorithms compute the aggregates in the lattice from one of their parents <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b48">49]</ref>. ArrayCube <ref type="bibr" target="#b48">[49]</ref> is a widely accepted algorithm in this category proposing a one-pass solution that simultaneously aggregates along multiple dimensions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">CONCLUSIONS AND FUTURE WORK</head><p>Discovering interesting insights from RDF graphs requires automatic, expressive, and efficient methods. We presented Spade, an extensible framework that enumerates a large and rich space of insights in the form of RDF aggregate queries and produces top-k results that maximize a given interestingness function. To efficiently explore the large space of candidates aggregates, Spade introduces: (i) MVDCube, an efficient algorithm for evaluating many aggregates in a single pass over the data, 20% to 80% faster than the best comparable method implemented in PostgreSQL, and (ii) a novel probabilistic technique that prunes uninteresting aggregates early. Spade scales well with the data size and the number of measures.</p><p>In future work, we plan to study more insight extraction methods to support numeric trends <ref type="bibr" target="#b41">[42]</ref>, time series, and geo-referenced data. Another research direction is "human-in-the-loop" data exploration that allows the user to work synergistically with the system to broaden the set of insights discovered from large graphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX A SKEWNESS AND KURTOSIS AS INTERESTINGNESS FUNCTIONS IN EARLY-STOP</head><p>In case of variance In case of skewness, I r (y) =</p><formula xml:id="formula_12">      1 G G i=1 y i -1 G G j=1 y j 3      • H r (y)<label>2 3</label></formula><p>.</p><p>First, we derive ∂ I r (y) ∂y s :</p><formula xml:id="formula_13">∂ I r (y) ∂y s = ∂ ∂y s              1 G G i=1 y i - 1 G G j=1 y j 3       • H r (y) 2 3        =        ∂ ∂y s       1 G G i=1 y i - 1 G G j=1 y j 3             </formula><p>• H r (y)</p><formula xml:id="formula_14">2 3 +       1 G G i=1 y i - 1 G G j=1 y j 3       • ∂ ∂y s H r (y) 2 3</formula><p>Second, we derive the sub-expressions Then, coming back to the original equation, we have that</p><formula xml:id="formula_15">∂ ∂y s       1 G G i=1 y i - 1 G G j=1 y j 3       = 3 G       y 2 s - 1 G G i=1</formula><formula xml:id="formula_16">∂ I r (y) ∂y s = 3 G       y 2 s - 1 G G i=1 y 2 i - 2y i G G j=1 y j + 2y s G j=1 y j       • H r (y) 2 3 + 2 3       1 G G i=1 y i - G G j=1 y j 3       • H r (y) -1 3 • ∂ H r (y) ∂y s</formula><p>Therefore,</p><formula xml:id="formula_17">∂ I r (y) ∂y s</formula><p>, as a combination of:</p><p>(1) H r (y), which is itself a combination of elementary (thus continuous) functions</p><p>∂ H r (y) ∂y s</p><p>, which we showed previously to be continuous (3) other elementary (thus continuous) functions is also continuous. </p><formula xml:id="formula_19">∂y s = ∂ ∂y s              1 G G i=1 y i - 1 G G j=1 y j 4       • G -1 G H r (y) -2 -3        =        ∂ ∂y s       1 G G i=1 y i - 1 G G j=1 y j 4              • G -1 G H r (y) -2 +       1 G G i=1 y i - 1 G G j=1 y j 4       • ∂ ∂y s G -1 G H r (y) -2</formula><p>Second, we derive the sub-expressions </p><formula xml:id="formula_20">∂ ∂y s       1 G G i=1 y i - 1 G G j=1 y j 4       = 4 G        y 3 s - 1 G G i=1 y 3 i -</formula><formula xml:id="formula_21">y j + 3y s G G j=1 y j 2        • G -1 G H r (y) -2 + 2(G -1) 2 G 2 [ H r (y)] 3       1 G G i=1 y i - 1 G G j=1 y j 4       • ∂ H r (y) ∂y s</formula><p>Therefore,</p><formula xml:id="formula_22">∂ J r (y) ∂y s</formula><p>, as a combination of:</p><p>(1) H r (y), which is itself a combination of elementary (thus continuous) functions</p><p>∂ H r (y) ∂y s</p><p>, which we showed previously to be continuous (3) other elementary (thus continuous) functions is also continuous.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B SUM AS AN AGGREGATE FUNCTION IN EARLY-STOP</head><p>To obtain the sum estimate, we compute the product of the size of the i-th aggregate group c i and the sample mean. We estimate c i while sampling during Data Translation: the count in the root node of the lattice is always correct, whereas in the other lattice nodes, depending on the presence of multi-valued dimensions, it may be overestimated. Recall from Section 5.2 the sample mean estimator Ȳi = 1 r r j=1 X j , and that Ȳi ∼ N (µ i , r to obtain τ 2 . Finally, the CI bounds are scaled by the constant factor of c 2 for each aggregate group w.r.t. the case of the average estimate: the impact of the scaling is hidden within τ 2 , the estimator of τ 2 . We thus obtain the formula for our sum-estimate confidence interval: P H r (S) -H r (cµ) ≤ . We obtain the global statistics for b for each attribute during Online Attribute Analysis step (Section 3).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Running examples using the CEOs dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The architecture of Spade.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>FemaleFigure 4 :</head><label>4</label><figDesc>Figure 4: Relational aggregation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Lemma 1 .</head><label>1</label><figDesc>Let G be an RDF graph. Let P = ⟨CFS, D P , M, f ⟩, C = ⟨CFS, D C , M, f ⟩ be two aggregates in a lattice on G such that P is a parent of C, D P = D C ∪ {D} where D is a dimension, f ∈ {count( * ), count(M), sum(M), avд(M)}, and there exists a fact n ∈ CFS with more than one value along the dimension D. Then, computing C(G) from the result of P(G) may lead to wrong results. Proof. Let the fact n ∈ CFS have the values n.D = {a, b} and, for each D j ∈ D P , D j D, n.D j = d j and</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>n2),(26,n2)}; P2:{(18,n2),(19,n2), (24,n2),(25,n2)}; P3:{(18,n1)};</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Aggregate evaluation using MVDCube and early-stop.</figDesc><graphic coords="8,208.75,121.49,110.42,64.21" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>we could have |CFS | facts in each cell, occupying a total of M T • M RB bytes. (b) For m measures, MVDCube needs |CFS | • m i=1 |S M i | float numbers in the worst case, where M i refers to each measure and S M i</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Algorithm 1 : 15 foreach</head><label>115</label><figDesc>MVDCube(root, partitions) 1 Function Main(root, partitions): 2 foreach P ∈ partitions do 3 root.loadPartition(P); 4 root.updateSubtree(); 5 root.computeAndStoreAggregatedMeasures(); 6 Function updateSubtree(): 7 foreach child ∈ children do 8 foreach pair (partition, offset) ∈ memory do 9 child.updateBitmap(partition, offset); pair (partition, offset) ∈ memory do 16 currentBitmap = getBitmap(partition, offset); 17 foreach pair (measure, aggFunction) do 18 aggregatedMeasure = currentBitmap ⊗ preAggregatedMeasure(measure,aggFunction); 19 resultManager.add(partition,offset,aggregatedMeasure);</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Examples of interesting aggregates found by Spade.</figDesc><graphic coords="10,397.08,196.78,86.77,113.91" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Interestingness of MDAs due to derivations.</figDesc><graphic coords="11,61.09,80.85,223.43,106.11" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Example uninteresting aggregates found by Spade.</figDesc><graphic coords="11,317.96,166.48,240.25,184.25" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Run times (on log scale) of MVDCube and PGCube. Dataset PGCube * PGCube d #wrong aggs #wrong aggs Airline 0 0 CEOs 4,723 3,998 DBLP 102 87 Foodista 2 0 NASA 378 312 Nobel 4,154 3,821</figDesc><graphic coords="12,197.95,193.48,81.68,99.39" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: Distribution of PGCube d errors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Fig- ure 7</head><label>7</label><figDesc>further shows, for each graph, the interestingness of its MDAs (measured with variance) in woD and wD settings (left and right lines, respectively); a horizontal tick in a line depicts an MDA.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Figure 11 :</head><label>11</label><figDesc>Figure 11: Run times of the steps in Spade's online pipeline.</figDesc><graphic coords="12,340.86,83.69,192.17,81.12" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head></head><label></label><figDesc>if T w /o k and T w k are the sets of the top-k aggregates returned by MVDCube without and with ES, the accuracy is computed as the fraction of true positives in T w k : |T</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head></head><label></label><figDesc>(a) Varying the number of facts |C F S |. (b) Varying the number of measures M . (c) Varying the number of dimensions N .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head>Figure 12 :</head><label>12</label><figDesc>Figure 12: Scalability of Spade in the number of facts, measures, and dimensions.</figDesc><graphic coords="14,68.06,178.29,136.25,84.67" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_20"><head>∂ H r (y) ∂y s = 2 G-1 y s - 1 G G i=1 y i for 1 ≤</head><label>211</label><figDesc>s ≤ G (recall Section 5).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_21"><head>2 - 3 .:∂</head><label>23</label><figDesc>In case of kurtosis, J r (y) First, we derive ∂ J r (y) ∂y s J r (y)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_22"><head>2 G 2 [</head><label>22</label><figDesc>H r (y)] 3 • ∂ H r (y) ∂y sThen, coming back to the original equation, we have that ∂ J r (y)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_23"><head>σ 2 ir 2 i σ 2 ir 2 s σ 2 s</head><label>22222</label><figDesc>) as r → ∞. We now construct a new estimator S i = c i r r j=1 X j = c i Ȳi . As a consequence, we have that S i ∼ N (cµ i , c ) as r → ∞. This leads to the correct sum estimate thanks to the estimator mean equal to c i µ i . While deriving the CI bounds in the proof, we account for the different variance of the estimator by applying Var(S s ) = c</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_24"><head>1 4(</head><label>1</label><figDesc>where S = (S 1 , S 2 , . . . , S G ) ⊺ and c = (c 1 , c 2 , . . . , c G ) ⊺ (the correct aggregate group sizes).C MIN AND MAX AS AGGREGATE FUNCTIONS IN EARLY-STOPPoint estimates for min, and max are sample min, respectively, max: the function applied over the sample, i.e., Z r (x) = min r (x) or Z r (x) = max r (x). We then bound H r (y), the variance of y = Z r (x), with Popoviciu's inequality for the upper bound: H r (y) ≤ Z r (x) -b) 2 , where b is the lower bound on min (respectively the upper bound on max). Analogically, we apply Szőkefalvi-Nagy's inequality for the lower bound: H r (y) ≤ ( Z r (x )-b) 2 2r</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Example 1</head><label>1</label><figDesc>Sum of the net worth of CEOs with political connections grouped by country of origin. Example 2 Average age of CEOs grouped by nationality and number of managed companies. Example 3 Number of CEOs grouped by nationality, gender, and area of the companies they manage.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Examples of interesting aggregates.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>count of CEOs by nationality, gender, company/area A2: count of CEOs by gender, company/area Brazil Brazil</head><label></label><figDesc>(b)  shows two example aggregates with high variance scores. More sophisticated interestingness functions for insight detection can be applied on the Step 4 results via the ARM; we discuss early-stop extensions in Section 5.2.</figDesc><table><row><cell></cell><cell cols="2">P4 e r ia F r a n c e r a z il B n g o la A ig n a t io n a li t y L e b a n o n N</cell><cell>P6</cell><cell cols="2">A2</cell><cell>2x2</cell><cell>A1</cell><cell>gender,company/area, nationality 2x2x2 2x2 A5</cell><cell>A6</cell><cell>2x2</cell></row><row><cell>company/area</cell><cell>Diamond Automotive Manufacturer Natural gas</cell><cell>P2 P1</cell><cell>P3</cell><cell>P5</cell><cell cols="4">A3 company/area A4 gender, gender 2</cell><cell>gender, nationality company/area 2</cell><cell>company/area, nationality nationality 2</cell></row><row><cell></cell><cell cols="2">F e m a le M a le gender</cell><cell cols="2">(a) Cube.</cell><cell></cell><cell cols="3">(b) Minimum Memory Spanning Tree.</cell></row><row><cell cols="9">Figure 3: Multidimensional space and MMST for Example 3.</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>tID</cell><cell>gender company/area</cell><cell>count(*)</cell></row><row><cell cols="5">Angola Female Angola Angola Female Manufacturer Diamond Female Natural gas nationality gender company/area Nigeria Nigeria France France Lebanon Lebanon null null null null null null Automotive Manufacturer Automotive Manufacturer Automotive Manufacturer A 1 : null t1 t2 t3 tID t4 t5 t6 t7 t8 t9 t10 Automotive</cell><cell cols="2">count(*) 1 1 1 1 1 1 1 1 1 1</cell><cell>t1 t2 t3 t4 t5</cell><cell>Female Female Manufacturer Diamond Female Natural gas null null Automotive Manufacturer A3: count of 1 1 1 4 4</cell></row><row><cell></cell><cell>t11</cell><cell>null</cell><cell cols="2">Manufacturer</cell><cell>1</cell><cell></cell><cell></cell><cell>CEOs by gender</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>d j is not null. By definition of P, there exist tuples t 1 , t 2 ∈ P(G) such that t 1 = (d 1 , . . . , a, . . . , d N , v 1 ) and t 2 = (d 1 , . . . , b, . . . , d N , v 2 ), to both of which n contributes. Hence, there exists a tuple t 3 ∈ C(G) such that t 3 = (d 1 , . . . , d N , v 3 ), in which the dimension D does not appear.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 2 :</head><label>2</label><figDesc>Real datasets used for testing.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 :</head><label>3</label><figDesc>PGCube</figDesc><table /><note><p>* and PGCube d errors on real-graph aggregates.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 4 :</head><label>4</label><figDesc>Early-stop effectiveness on real datasets. All times in ms; in bold: gain% &gt; 10%, pruned% &gt; 70%, and acc% = 100%.</figDesc><table /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>ACKNOWLEDGMENTS</head><p><rs type="person">Yanlei Diao</rs> and <rs type="person">Paweł Guzewicz</rs> are supported by the <rs type="funder">European Research Council</rs>, <rs type="programName">H2020 research program</rs> under GrantNo.: <rs type="grantNumber">725561</rs>, and by the <rs type="funder">Agence Nationale de la Recherche</rs> under GrantNo.: <rs type="grantNumber">ANR-16-CE23-0010-01</rs>. <rs type="person">Mirjana Mazuran</rs> is supported by the <rs type="funder">European Research Council</rs>, <rs type="programName">H2020 research program</rs> under GrantNo.: <rs type="grantNumber">800192</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_pQhMKVq">
					<idno type="grant-number">725561</idno>
					<orgName type="program" subtype="full">H2020 research program</orgName>
				</org>
				<org type="funding" xml:id="_QvqS2Tz">
					<idno type="grant-number">ANR-16-CE23-0010-01</idno>
				</org>
				<org type="funding" xml:id="_svfvDxm">
					<idno type="grant-number">800192</idno>
					<orgName type="program" subtype="full">H2020 research program</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">On the Computation of Multidimensional Aggregates</title>
		<author>
			<persName><forename type="first">Sameet</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rakesh</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prasad</forename><surname>Deshpande</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><forename type="middle">F</forename><surname>Naughton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raghu</forename><surname>Ramakrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sunita</forename><surname>Sarawagi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">VLDB. VLDB Endowment</title>
		<meeting><address><addrLine>Mumbai; Bombay), India</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="506" to="521" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Efficient OLAP operations for RDF analytics</title>
		<author>
			<persName><forename type="first">Elham</forename><surname>Akbari Azirani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">François</forename><surname>Goasdoué</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ioana</forename><surname>Manolescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandra</forename><surname>Roatis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDE Workshops</title>
		<meeting><address><addrLine>Seoul, South Korea</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="71" to="76" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Identifying exceptional (dis)agreement between groups</title>
		<author>
			<persName><forename type="first">Adnene</forename><surname>Belfodil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sylvie</forename><surname>Cazalens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philippe</forename><surname>Lamarre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Plantevit</surname></persName>
		</author>
		<idno type="DOI">10.1007/s10618-019-00665-9</idno>
		<ptr target="https://doi.org/10.1007/s10618-019-00665-9" />
	</analytic>
	<monogr>
		<title level="j">Data Mining and Knowledge Discovery</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="394" to="442" />
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Using entropy metrics for pruning very large graph cubes</title>
		<author>
			<persName><forename type="first">Dritan</forename><surname>Bleco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yannis</forename><surname>Kotidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Systems</title>
		<imprint>
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="page" from="49" to="62" />
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Reformulation-based query answering for RDF graphs with RDFS ontologies</title>
		<author>
			<persName><forename type="first">Maxime</forename><surname>Buron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">François</forename><surname>Goasdoué</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ioana</forename><surname>Manolescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marie-Laure</forename><surname>Mugnier</surname></persName>
		</author>
		<ptr target="https://hal.archives-ouvertes.fr/hal-02051413" />
	</analytic>
	<monogr>
		<title level="m">ESWC. Association for Computing Machinery</title>
		<meeting><address><addrLine>Portorož, Slovenia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Ontology-Based RDF Integration of Heterogeneous Data</title>
		<author>
			<persName><forename type="first">Maxime</forename><surname>Buron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">François</forename><surname>Goasdoué</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ioana</forename><surname>Manolescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marie-Laure</forename><surname>Mugnier</surname></persName>
		</author>
		<ptr target="https://hal.inria.fr/hal-02446427" />
	</analytic>
	<monogr>
		<title level="m">EDBT</title>
		<meeting><address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Teaching an RDBMS about ontological constraints</title>
		<author>
			<persName><forename type="first">Damian</forename><surname>Bursztyn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">François</forename><surname>Goasdoué</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ioana</forename><surname>Manolescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PVLDB</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1161" to="1172" />
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Statistical Inference</title>
		<author>
			<persName><forename type="first">George</forename><surname>Casella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roger</forename><surname>Berger</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
			<publisher>Duxbury Resource Center</publisher>
			<pubPlace>Pacific Grove, California, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Summarizing Semantic Graphs: A Survey</title>
		<author>
			<persName><forename type="first">Šejla</forename><surname>Cebiric</surname></persName>
		</author>
		<author>
			<persName><forename type="first">François</forename><surname>Goasdoué</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haridimos</forename><surname>Kondylakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dimitris</forename><surname>Kotzinos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ioana</forename><surname>Manolescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georgia</forename><surname>Troullinou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mussab</forename><surname>Zneika</surname></persName>
		</author>
		<ptr target="https://hal.inria.fr/hal-01925496" />
	</analytic>
	<monogr>
		<title level="j">The VLDB Journal</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Efficient Computation of Multiple Group By Queries</title>
		<author>
			<persName><forename type="first">Zhimin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Vivek</surname></persName>
		</author>
		<author>
			<persName><surname>Narasayya</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005</date>
			<publisher>SIGMOD. Association for Computing Machinery</publisher>
			<biblScope unit="page" from="263" to="274" />
			<pubPlace>Baltimore, Maryland, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">RDF Analytics: Lenses over Semantic Graphs</title>
		<author>
			<persName><forename type="first">Dario</forename><surname>Colazzo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">François</forename><surname>Goasdoué</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ioana</forename><surname>Manolescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandra</forename><surname>Roatis</surname></persName>
		</author>
		<idno type="DOI">10.1145/2566486.2567982</idno>
		<ptr target="https://doi.org/10.1145/2566486.2567982" />
		<imprint>
			<date type="published" when="2014">2014</date>
			<publisher>WWW. Association for Computing Machinery</publisher>
			<pubPlace>Seoul, South Korea</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<ptr target="http://data.nobelprize.org/dump.nt" />
		<title level="m">Nobel Prizes dataset</title>
		<imprint>
			<publisher>The Nobel Prize Committee</publisher>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<ptr target="https://www.w3.org/TR/sparql11-query/" />
		<title level="m">SPARQL 1.1 Query Language</title>
		<imprint>
			<publisher>World Wide Web Consortium</publisher>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Spade: A Modular Framework for Analytical Exploration of RDF Graphs (demonstration)</title>
		<author>
			<persName><forename type="first">Yanlei</forename><surname>Diao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paweł</forename><surname>Guzewicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ioana</forename><surname>Manolescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mirjana</forename><surname>Mazuran</surname></persName>
		</author>
		<idno type="DOI">10.14778/3352063.3352101</idno>
		<ptr target="https://doi.org/10.14778/3352063.3352101" />
	</analytic>
	<monogr>
		<title level="m">PVLDB. VLDB Endowment</title>
		<meeting><address><addrLine>Los Angeles, California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1926">2019. 1926-1929</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Dagger: Digging for Interesting Aggregates in RDF Graphs</title>
		<author>
			<persName><forename type="first">Yanlei</forename><surname>Diao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ioana</forename><surname>Manolescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shu</forename><surname>Shang</surname></persName>
		</author>
		<ptr target="CEUR-WS.org" />
	</analytic>
	<monogr>
		<title level="m">ISWC Posters &amp; Demonstrations and Industry Tracks</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">QuickInsights: Quick and Automatic Discovery of Insights from Multi-Dimensional Data</title>
		<author>
			<persName><forename type="first">Rui</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shi</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haidong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongmei</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<publisher>SIGMOD. Association for Computing Machinery</publisher>
			<biblScope unit="page" from="317" to="332" />
			<pubPlace>Amsterdam, The Netherlands</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName><forename type="first">Leigh</forename><surname>Dodds</surname></persName>
		</author>
		<ptr target="https://data.nasa.gov/" />
		<title level="m">NASA dataset</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">Leigh</forename><surname>Dodds</surname></persName>
		</author>
		<ptr target="https://old.datahub.io/dataset/foodista" />
		<title level="m">Foodista dataset</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Efficient Recommendation of Aggregate Data Visualizations</title>
		<author>
			<persName><forename type="first">Mohamed</forename><forename type="middle">A</forename><surname>Humaira Ehsan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Panos</forename><forename type="middle">K</forename><surname>Sharaf</surname></persName>
		</author>
		<author>
			<persName><surname>Chrysanthis</surname></persName>
		</author>
		<idno type="DOI">10.1109/TKDE.2017.2765634</idno>
		<ptr target="https://doi.org/10.1109/TKDE.2017.2765634" />
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="263" to="277" />
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Multi-Structural Databases</title>
		<author>
			<persName><forename type="first">R</forename><surname>Ronald Fagin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ravi</forename><surname>Guha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jasmine</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Novak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Sivakumar</surname></persName>
		</author>
		<author>
			<persName><surname>Tomkins</surname></persName>
		</author>
		<idno type="DOI">10.1145/1065167.1065191</idno>
		<ptr target="https://doi.org/10.1145/1065167.1065191" />
	</analytic>
	<monogr>
		<title level="m">PODS. Association for Computing Machinery</title>
		<meeting><address><addrLine>Baltimore, Maryland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="184" to="195" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Linked Data Fragments</title>
		<ptr target="http://downloads" />
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">RDF graph summarization for first-sight structure discovery</title>
		<author>
			<persName><forename type="first">François</forename><surname>Goasdoué</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paweł</forename><surname>Guzewicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ioana</forename><surname>Manolescu</surname></persName>
		</author>
		<idno type="DOI">10.1007/s00778-020-00611-y</idno>
		<ptr target="https://doi.org/10.1007/s00778-020-00611-y" />
	</analytic>
	<monogr>
		<title level="j">The VLDB Journal</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1191" to="1218" />
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Efficient Query Answering against Dynamic RDF Databases</title>
		<author>
			<persName><forename type="first">François</forename><surname>Goasdoué</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ioana</forename><surname>Manolescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandra</forename><surname>Roatis</surname></persName>
		</author>
		<idno type="DOI">10.1145/2452376.2452412</idno>
		<ptr target="https://doi.org/10.1145/2452376.2452412" />
	</analytic>
	<monogr>
		<title level="m">EDBT</title>
		<meeting><address><addrLine>Genoa, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><forename type="first">Giovanni</forename><surname>Gonzalez</surname></persName>
		</author>
		<ptr target="https://www.kaggle.com/giovamata/airlinedelaycauses" />
		<title level="m">Airline delays causes dataset</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Efficiently Mining Maximal Frequent Itemsets</title>
		<author>
			<persName><forename type="first">Karam</forename><surname>Gouda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammed</forename><surname>Javeed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zaki</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDM. IEEE</title>
		<meeting><address><addrLine>San Jose, California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="163" to="170" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<author>
			<orgName type="collaboration">PostgreSQL Global Development Group</orgName>
		</author>
		<ptr target="https://git.postgresql.org/gitweb/?p=postgresql.git" />
		<title level="m">PostgreSQL support for GROUPING SETS, CUBE and ROLLUP in one pass over the input</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note>h= f3d3118532175541a9a96ed78881a3b04a057128</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<author>
			<orgName type="collaboration">PostgreSQL Global Development Group</orgName>
		</author>
		<ptr target="https://www.postgresql.org/docs/12/cube.html" />
		<title level="m">PostgreSQL 12 CUBE</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Large-sample and deterministic confidence intervals for online aggregation</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Haas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SSDBM. Association for Computing Machinery</title>
		<meeting><address><addrLine>Olympia, Washington, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="51" to="62" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Implementing Data Cubes Efficiently</title>
		<author>
			<persName><forename type="first">Anand</forename><surname>Venky Harinarayan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><forename type="middle">D</forename><surname>Rajaraman</surname></persName>
		</author>
		<author>
			<persName><surname>Ullman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996">1996</date>
			<publisher>SIGMOD. Association for Computing Machinery</publisher>
			<biblScope unit="page" from="205" to="216" />
			<pubPlace>Montreal, Quebec, Canada</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Online Aggregation</title>
		<author>
			<persName><forename type="first">Joseph</forename><forename type="middle">M</forename><surname>Hellerstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Haas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Helen</forename><forename type="middle">J</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997">1997</date>
			<publisher>SIGMOD. Association for Computing Machinery</publisher>
			<biblScope unit="page" from="171" to="182" />
			<pubPlace>Tucson, Arizona, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Interactive Data Exploration with Smart Drill-Down</title>
		<author>
			<persName><forename type="first">Manas</forename><surname>Joglekar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hector</forename><surname>Garcia-Molina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><forename type="middle">G</forename><surname>Parameswaran</surname></persName>
		</author>
		<idno type="DOI">10.1109/TKDE.2017.2685998</idno>
		<ptr target="https://doi.org/10.1109/TKDE.2017.2685998" />
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="46" to="60" />
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Consistently faster and smaller compressed bitmaps with Roaring</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Lemire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><surname>Ssi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Kai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Owen</forename><surname>Kaser</surname></persName>
		</author>
		<idno type="DOI">10.1002/spe.2402</idno>
		<ptr target="https://doi.org/10.1002/spe.2402" />
	</analytic>
	<monogr>
		<title level="j">Software: Practice and Experience</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1547" to="1569" />
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Multi-Example Search in Rich Information Graphs</title>
		<author>
			<persName><forename type="first">Matteo</forename><surname>Lissandrini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Davide</forename><surname>Mottin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Themis</forename><surname>Palpanas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yannis</forename><surname>Velegrakis</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICDE.2018.00078</idno>
		<ptr target="https://doi.org/10.1109/ICDE.2018.00078" />
	</analytic>
	<monogr>
		<title level="m">ICDE. IEEE</title>
		<meeting><address><addrLine>Paris, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="809" to="820" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Graph Summarization Methods and Applications: A Survey</title>
		<author>
			<persName><forename type="first">Yike</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tara</forename><surname>Safavi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhilash</forename><surname>Dighe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danai</forename><surname>Koutra</surname></persName>
		</author>
		<idno type="DOI">10.1145/3186727</idno>
		<ptr target="https://doi.org/10.1145/3186727" />
	</analytic>
	<monogr>
		<title level="j">Comput. Surveys</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">34</biblScope>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">DeepEye: Creating Good Data Visualizations by Keyword Search</title>
		<author>
			<persName><forename type="first">Yuyu</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuedi</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guoliang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinran</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>SIGMOD. Association for Computing Machinery</publisher>
			<biblScope unit="page" from="1733" to="1736" />
			<pubPlace>Houston, Texas, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Speeding up RDF Aggregate Discovery through Sampling</title>
		<author>
			<persName><forename type="first">Ioana</forename><surname>Manolescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mirjana</forename><surname>Mazuran</surname></persName>
		</author>
		<ptr target="CEUR-WS.org" />
	</analytic>
	<monogr>
		<title level="m">EDBT Workshops</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<author>
			<persName><forename type="first">Mirjana</forename><surname>Mazuran</surname></persName>
		</author>
		<ptr target="https://www.dropbox.com/s/af8kzjwesz1vs2y/CEOsWithAllTheirData%5FPlus2hops" />
		<title level="m">CEOs dataset</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">LensXPlain: Visualizing and Explaining Contributing Subsets for Aggregate Query Answers</title>
		<author>
			<persName><forename type="first">Zhengjie</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sudeepa</forename><surname>Roy</surname></persName>
		</author>
		<idno type="DOI">10.14778/3352063.3352094</idno>
		<ptr target="https://doi.org/10.14778/3352063.3352094" />
	</analytic>
	<monogr>
		<title level="j">PVLDB</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1898" to="1901" />
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">ROLAP implementations of the data cube</title>
		<author>
			<persName><forename type="first">Konstantinos</forename><surname>Morfonios</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stratis</forename><surname>Konakas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yannis</forename><forename type="middle">E</forename><surname>Ioannidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikolaos</forename><surname>Kotsis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2007">2007. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">Laura</forename><surname>Po</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikos</forename><surname>Bikakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Federico</forename><surname>Desimoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Papastefanatos</surname></persName>
		</author>
		<idno type="DOI">10.2200/S00967ED1V01Y201911WBE019</idno>
		<ptr target="https://doi.org/10.2200/S00967ED1V01Y201911WBE019" />
	</analytic>
	<monogr>
		<title level="j">Linked Data Visualization: Techniques, Tools, and Big Data</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="1" to="157" />
			<date type="published" when="2020">2020</date>
			<publisher>Morgan &amp; Claypool Publishers</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><surname>Popoviciu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Béla</forename><surname>Szőkefalvi-Nagy</surname></persName>
		</author>
		<ptr target="https://en.wikipedia.org/wiki/Popoviciu%27s%5Finequality%5Fon%5Fvariances" />
		<title level="m">Popoviciu&apos;s and Szőkefalvi-Nagy&apos;s inequalities on variances</title>
		<imprint>
			<date type="published" when="1935">1935</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Extracting Top-K Insights from Multi-dimensional Data</title>
		<author>
			<persName><forename type="first">Bo</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shi</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Man</forename><forename type="middle">Lung</forename><surname>Yiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongmei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGMOD. Association for Computing Machinery</title>
		<meeting><address><addrLine>Chicago, Illinois, Unirted States</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1509" to="1524" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">SeeDB: Efficient Data-Driven Visualization Recommendations to Support Visual Analytics</title>
		<author>
			<persName><forename type="first">Manasi</forename><surname>Vartak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sajjadur</forename><surname>Rahman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><surname>Madden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Parameswaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neoklis</forename><surname>Polyzotis</surname></persName>
		</author>
		<idno type="DOI">10.14778/2831360.2831371</idno>
		<ptr target="https://doi.org/10.14778/2831360.2831371" />
	</analytic>
	<monogr>
		<title level="j">PVLDB</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">13</biblScope>
			<biblScope unit="page" from="2182" to="2193" />
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Random Sampling with a Reservoir</title>
		<author>
			<persName><forename type="first">Jeffrey</forename><forename type="middle">S</forename><surname>Vitter</surname></persName>
		</author>
		<idno type="DOI">10.1145/3147.3165</idno>
		<ptr target="https://doi.org/10.1145/3147.3165" />
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Mathematical Software</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="37" to="57" />
			<date type="published" when="1985">1985. 1985</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Interactive Summarization and Exploration of Top Aggregate Query Answers</title>
		<author>
			<persName><forename type="first">Yuhao</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sudeepa</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Yang</surname></persName>
		</author>
		<idno type="DOI">10.14778/3275366.3275369</idno>
		<ptr target="https://doi.org/10.14778/3275366.3275369" />
	</analytic>
	<monogr>
		<title level="j">PVLDB</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">13</biblScope>
			<biblScope unit="page" from="2196" to="2208" />
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">QAGView: Interactively Summarizing High-Valued Aggregate Query Answers</title>
		<author>
			<persName><forename type="first">Yuhao</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sudeepa</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>SIGMOD. Association for Computing Machinery</publisher>
			<biblScope unit="page" from="1709" to="1712" />
			<pubPlace>Houston, Texas, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Computational Fact Checking through Query Perturbations</title>
		<author>
			<persName><forename type="first">You</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pankaj</forename><forename type="middle">K</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengkai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cong</forename><surname>Yu</surname></persName>
		</author>
		<idno type="DOI">10.1145/2996453</idno>
		<ptr target="https://doi.org/10.1145/2996453" />
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Database Systems</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">41</biblScope>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Interactive View Recommendation with a Utility Function of a General Form</title>
		<author>
			<persName><forename type="first">Xiaozhong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoyu</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Panos</forename><forename type="middle">K</forename><surname>Chrysanthis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGMOD Workshops</title>
		<meeting><address><addrLine>Portland, Oregon, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">An Array-Based Algorithm for Simultaneous Multidimensional Aggregates</title>
		<author>
			<persName><forename type="first">Yihong</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prasad</forename><surname>Deshpande</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><forename type="middle">F</forename><surname>Naughton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997">1997</date>
			<publisher>SIGMOD. Association for Computing Machinery</publisher>
			<biblScope unit="page" from="159" to="170" />
			<pubPlace>Tucson, Arizona, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
