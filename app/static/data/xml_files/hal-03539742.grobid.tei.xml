<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">PRIVACY ATTACKS FOR AUTOMATIC SPEECH RECOGNITION ACOUSTIC MODELS IN A FEDERATED LEARNING FRAMEWORK</title>
				<funder ref="#_bPXgxrh #_pQAs9k9">
					<orgName type="full">Agence Nationale de la Recherche</orgName>
					<orgName type="abbreviated">ANR</orgName>
				</funder>
				<funder>
					<orgName type="full">VoicePersonae</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Natalia</forename><surname>Tomashenko</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">LIA</orgName>
								<orgName type="institution">Avignon Université</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Salima</forename><surname>Mdhaffar</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">LIA</orgName>
								<orgName type="institution">Avignon Université</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Marc</forename><surname>Tommasi</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">UMR 9189</orgName>
								<orgName type="institution" key="instit1">Université de Lille</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
								<orgName type="institution" key="instit3">Inria</orgName>
								<orgName type="institution" key="instit4">Centrale Lille -CRIStAL</orgName>
								<address>
									<settlement>Lille</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yannick</forename><surname>Estève</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">LIA</orgName>
								<orgName type="institution">Avignon Université</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jean-Franc</forename><surname>¸ois Bonastre</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">LIA</orgName>
								<orgName type="institution">Avignon Université</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">PRIVACY ATTACKS FOR AUTOMATIC SPEECH RECOGNITION ACOUSTIC MODELS IN A FEDERATED LEARNING FRAMEWORK</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">8D68E488953CBA5CB4196DCDFEF689C9</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-04-12T14:46+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Privacy</term>
					<term>federated learning</term>
					<term>acoustic models</term>
					<term>attack models</term>
					<term>speech recognition</term>
					<term>speaker verification</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper investigates methods to effectively retrieve speaker information from the personalized speaker adapted neural network acoustic models (AMs) in automatic speech recognition (ASR). This problem is especially important in the context of federated learning of ASR acoustic models where a global model is learnt on the server based on the updates received from multiple clients. We propose an approach to analyze information in neural network AMs based on a neural network footprint on the so-called Indicator dataset. Using this method, we develop two attack models that aim to infer speaker identity from the updated personalized models without access to the actual users' speech data. Experiments on the TED-LIUM 3 corpus demonstrate that the proposed approaches are very effective and can provide equal error rate (EER) of 1-2%.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Federated learning (FL) for automatic speech recognition (ASR) has recently become an active area of research <ref type="bibr" target="#b0">[1]</ref><ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref>. To preserve the privacy of the users' data in the FL framework, the model is updated in a distributed fashion instead of communicating the data directly from clients to a server.</p><p>Privacy is one of the major challenges in FL <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8]</ref>. Sharing model updates, i.e. gradient information, instead of raw user data aims to protect user personal data that are processed locally on devices. However, these updates may still reveal some sensitive information to a server or to a third party <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10]</ref>. According to recent research, FL has various privacy risks and may be vulnerable to different types of attacks, i.e. membership inference attacks <ref type="bibr" target="#b10">[11]</ref> or generative adversarial network (GAN) inference attacks <ref type="bibr" target="#b11">[12]</ref>. Techniques to enhance the privacy in a FL framework are mainly based on two categories <ref type="bibr" target="#b7">[8]</ref>: secure multiparty computation <ref type="bibr" target="#b12">[13]</ref> and differential privacy <ref type="bibr" target="#b13">[14]</ref>. Encryption methods <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16]</ref> such as fully homomorphic encryption <ref type="bibr" target="#b15">[16]</ref> and secure multiparty computation perform computation in the encrypted domain. These methods increase computational complexity. In a FL framework, this increase is not so significant compared to standard centralized training, since only the transmitted parameters need to be encrypted instead of large amounts of data, however with an increased number of participants, computational complexity becomes a critical issue. Differential privacy methods preserve privacy by adding noise to users' parameters <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b16">17]</ref>, however such solutions may degrade learning performance due to the uncertainty they introduce into the parameters.</p><p>Alternative methods to privacy protection for speech include deletion methods <ref type="bibr" target="#b17">[18]</ref> that are meant for ambient sound analysis, and anonymization <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20]</ref> that aims to suppress personally identifiable information in the speech signal keeping unchanged all other attributes. These privacy preservation methods can be combined and integrated in a hybrid fashion into a FL framework.</p><p>Despite the recent interest in FL for ASR and other speechrelated tasks such as keyword spotting <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b21">22]</ref>, emotion recognition <ref type="bibr" target="#b22">[23]</ref>, and speaker verification <ref type="bibr" target="#b23">[24]</ref>, there is a lack of research on vulnerability of ASR acoustic models (AMs) to privacy attacks in a FL framework. In this work, we make a step towards this direction by analyzing speaker information that can be retrieved from the personalized AM locally updated on the user's data. To achieve this goal, we developed two privacy attack models that operate directly on the updated model parameters without access to the actual user's data. Parameters of neural network (NN) personalized AMs contain a wealth of information about the speakers <ref type="bibr" target="#b24">[25]</ref>. In this paper, we propose novel methods to efficiently and easily retrieve speaker information from the adapted AMs. The main idea of the proposed methods is to use an external Indicator dataset to analyze the footprint of AMs on this data. Another important contribution of this work is understanding how the speaker information is distributed in the adapted NN AMs.</p><p>This paper is structured as follows. Section 2 briefly introduces a considered FL framework for AM training. Section 3 describes the privacy preservation scenario and proposes two attack models. Experimental evaluation is presented in Section 4. We conclude in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">FEDERATED LEARNING FOR ASR ACOUSTIC MODELS</head><p>We consider a classical FL scenario where a global NN AM is trained on a server from the data stored locally on multiple remote devices <ref type="bibr" target="#b6">[7]</ref>. The training of the global model is performed under the constraint that the training speech data are stored and processed locally on the user devices (clients), while only model updates are transmitted to the server from each client. The global model is learnt on the server based on the updates received from multiple clients.</p><p>The FL in a distributed network of clients is illustrated in Figure <ref type="figure" target="#fig_0">1</ref>.</p><p>First, an initial global speech recognition AM Wg is distributed to the group of devices of N users (speakers). Then, the initial global model is run on every user si (i ∈ 1..N ) device and updated locally on the private user data. The updated models Ws i are then transmitted to the server where they are aggregated to obtain a new global model W * g . Typically, the personalized updated models are aggregated using federated averaging and its variations <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b25">26]</ref>. Then, the updated global model W * g is shared with the clients. The process restarts and loops until convergence or after a fixed number of rounds. The utility and training efficiency of the FL AMs have been successfully studied in recent works <ref type="bibr" target="#b0">[1]</ref><ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref>, and these topics are beyond the scope of the current paper. Alternatively, we focus on the privacy aspect of this framework.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Server</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">ATTACK MODELS</head><p>In this section, we describe the privacy preservation scenario and present two attack models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Privacy preservation scenario</head><p>Privacy preservation is formulated as a game between users who share some data and attackers who access this data or data derived from it and aim to infer information about the users <ref type="bibr" target="#b18">[19]</ref>. To preserve the user data, in FL, there is no speech data exchange between a server and clients, only model updates are transmitted between the clients and server (or between some clients). Attackers aim to attack users using information owned by the server. They can get access to some updated personalized models.</p><p>In this work, we assume that an attacker has access to the following data:</p><p>• An initial global NN AM Wg;</p><p>• A personalized model Ws of the target speaker s who is enrolled in the FL system. The corresponding personalized model was obtained from the global model Wg by fine-tuning Wg using speaker data. We consider this model as enrollment data for an attacker.</p><p>• Other personalized models of non-target and target speakers: Ws 1 ,...,Ws N . We will refer to these models as test trial data.</p><p>The attacker's objective is to conduct an automatic speaker verification (ASV) task by using the enrollment data model in the form of Ws and test trial data in the form of models Ws 1 ,...,Ws N .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Attack models</head><p>The motivation of the proposed approaches is based on the hypothesis that we can capture information about the identity of speaker s from the corresponding speaker-adapted model Ws and the global model Wg by comparing the outputs of these two neural AMs taken from hidden layers h on some speech data. We will refer to this speech data as Indicator data. Note, that the Indicator data is not related to any test or AM training data and can be chosen arbitrarily from any speakers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1.">Attack model A1</head><p>The ASV task with the proposed attack model is performed in several steps as illustrated in Figure <ref type="figure">2</ref>. Let denote a set of utterances in the Indicator dataset I as u1, . . . , uJ ∈ I; a sequence of vectors in utterance uj as {u </p><formula xml:id="formula_0">W h s i (uj) = {w h,t s i ,j } T j t=1 and W h g (uj) = {w h,t g,j } T j</formula><p>t=1 , and per-frame differences between corresponding outputs:</p><formula xml:id="formula_1">∆ h s i (uj) = {∆ h,t s i ,j } T j t=1 ,<label>(1)</label></formula><p>where ∆ h,t s i ,j = w h,t s i ,j -w h,t g,j , t ∈ 1..Tj. 2. For each personalized model, we compute mean and standard deviation vectors for ∆ h,t s i ,j over all speech frames in the Indicator dataset I:</p><formula xml:id="formula_2">µ h s i = I j=1 T j t=1 ∆ h,t s i ,j I j=1 Tj ,<label>(2)</label></formula><formula xml:id="formula_3">σ h s i = I j=1 T j t=1 (∆ h,t s i ,j -µ h s i ) 2 I j=1 Tj 1 2<label>(3)</label></formula><p>3. For a pair of personalized models Ws i and Ws k , we compute a similarity score ρ at hidden level h on the Indicator dataset based on the L2-normalised Euclidean distance between the corresponding vector pairs for means and standard deviations:</p><formula xml:id="formula_4">ρ(W h s i , W h s k ) = αµ ∥µ h s i -µ h s k ∥2 ∥µ h s i ∥2∥µ h s k ∥2 + ασ ∥σ h s i -σ h s k ∥2 ∥σ h s i ∥2∥σ h s k ∥2 ,<label>(4)</label></formula><p>where αµ, ασ are fixed parameters in all experiments.</p><p>4. Given similarity scores for all matrix pairs, we can complete a speaker verification task based on these scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pool of personalized models</head><p>Indicator speech data</p><formula xml:id="formula_5">𝑊 𝑠 1 ℎ (𝑢) 𝑊 𝑔 ℎ (𝑢) 𝑊 𝑠 𝑁 𝑊 𝑠 1 𝑊 𝑠 2 𝑊 𝑔 𝑢 𝑢 𝑊 𝑠 𝑁 ℎ (𝑢) Δ 𝑠 1 ℎ (𝑢) Δ 𝑠 𝑁 ℎ (𝑢) [μ 𝑠 1 ℎ , 𝜎 𝑠 1 ℎ ] [μ 𝑠 𝑁 ℎ , 𝜎 𝑠 𝑁 ℎ ]</formula><p>… Fig. <ref type="figure">2</ref>. Statistic computation for the attack model A1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2.">Attack model A2</head><p>For the second attack model, we train a NN model as shown in Fig- <ref type="figure">ure</ref> 3. This NN model uses personalized and global models and the speech Indicator dataset for training. It is trained to predict a speaker identity provided the corresponding personalized model. When the model is trained, we use it in evaluation time to extract speaker embeddings similarly to x-vectors and apply probabilistic linear discriminant analysis (PLDA) <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b27">28]</ref>. As shown in Figure <ref type="figure" target="#fig_2">3</ref>, the model consists of two parts (frozen and trained). The outputs of the frozen part are ∆ h s i sequences of vectors computed per utterance of the Indicator data as defined in Formula (1). For every personalized model Ws i , we compute ∆ h s i for all the utterances u of the Indicator corpus; then ∆ h s i (u) is used as input to the second (trained) part of the NN which comprises several time delay neural network (TDNN) layers <ref type="bibr" target="#b28">[29]</ref> and one statistical pooling layer. The experiments were conducted on the speaker adaptation partition of the TED-LIUM 3 corpus <ref type="bibr" target="#b29">[30]</ref>. This publicly available data set contains TED talks that amount to 452 hours speech data in English from about 2K speakers, 16kHz. Similarly to <ref type="bibr" target="#b2">[3]</ref>, we selected from the TED-LIUM 3 training dataset three datasets: Train-G, Part-1, Part-2 with disjoint speaker subsets as shown in Table <ref type="table">1</ref>. The Indicator dataset was used to train an attack model. It is comprised of 320 utterances selected from all 32 speakers of test and development datasets of the TED-LIUM 3 corpus. The speakers in the Indicator dataset are disjoint from speakers in Train-G, Part-1, and Part-2.</p><p>For each speaker in the Indicator dataset we select 10 utterances. The size of the Indicator dataset is 32 minutes. The Train-G dataset was used to train an initial global AM Wg. Part-1 and Part-2 were used to obtain two sets of personalized models. 1   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">ASR acoustic models</head><p>The ASR AMs have a TDNN model architecture <ref type="bibr" target="#b28">[29]</ref> and were trained using the Kaldi speech recognition toolkit <ref type="bibr" target="#b30">[31]</ref>. 40-dimensional Mel-frequency cepstral coefficients (MFCCs) without cepstral 1 Data partitions and scripts will be available online: https:// github.com/Natalia-T/privacy-attacks-asr  <ref type="table">1</ref>. Data sets statistics truncation appended with 100-dimensional i-vectors were used as the input into the NNs. Each model has thirteen 512-dimensional hidden layers followed by a softmax layer where 3664 triphone states were used as targets <ref type="foot" target="#foot_0">2</ref> . The initial global model Wg was trained using the lattice-free maximum mutual information (LF-MMI) criterion with a 3-fold reduced frame rate as in <ref type="bibr" target="#b31">[32]</ref>. The two types of speech data augmentation strategies were applied for the training and adaptation data: speed perturbation (with factors 0.9, 1.0, 1.1) and volume perturbation, as in <ref type="bibr" target="#b28">[29]</ref>. Each model has about 13.8M parameters. The initial global model Wg was trained on the Train-G. Personalized models Ws i were obtained by fine-tuning all the parameters of Wg on the speakers' data from Part-1 and Part-2 as described in <ref type="bibr" target="#b2">[3]</ref>. For all personalized speaker models, we use approximately the same amount of speech data to perform fine-tuning (speaker adaptation) -about 4 minutes per model. For most of the speakers (564 in Part-1, 463 in Part-2) we obtained two different personalized models (per speaker) on disjoint adaptation subsets, for the rest speakers we have adaptation data only for one model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Attack models</head><p>We investigate two approaches for attack models: A1 -a simple approach based on the comparative statistical analysis of the NN outputs and the associated similarity score between personalized models, and A2 -a NN based approach. For the test target trials, we use comparisons between different personalized models of the same speakers (564 in Part-2 and 1027 in the Part-1+Part-2), and for the non-target trials we randomly selected 10K pairs of models from different speakers in a corresponding dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1.">Attack model A1</head><p>The first attack model was applied as described in Section 3.2.1. The parameters αµ, ασ in Formula (4) equal to 1 and 10 respectively. This model was evaluated on two datasets of personalized models corresponding to Part-2 and combined Part-1+Part-2 datasets. The Indicator dataset is the same in all experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2.">Attack model A2</head><p>For training the attack model A2, we use 1300 personalized speaker models corresponding to 736 unique speakers from Part-1. When we applied the frozen part of the architecture shown in Figure <ref type="figure" target="#fig_2">3</ref> to the 32-minute Indicator dataset for each speaker model in Part-1, we obtained the training data with the amount corresponding to about 693h (32×1300). The trained part of the NN model, illustrated in Figure <ref type="figure" target="#fig_2">3</ref>, has a similar topology to a conventional x-vector extractor <ref type="bibr" target="#b26">[27]</ref>. However, unlike the standard NN x-vector extractor, that is trained to predict speaker id-s by the input speech segment, our proposed model learns to predict a speaker identity from the W h s part of a speaker personalized model. We trained 2 attack models corresponding to the two values of parameter h ∈ {1, 5} -a hidden layer in the ASR neural AMs at which we compute the activations.</p><p>Values h were chosen based on the results for the attack model A1.</p><p>The output dimension of the frozen part is 512. The frozen part is followed by the trained part that consists of seven hidden TDNN layers and one statistical pooling layer introduced after the fifth TDNN layer. The output is a softmax layer with the targets corresponding to speakers in the pool of speaker personalized models (number of unique speakers in Part-1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Results</head><p>The attack models were evaluated in terms of equal error rate (EER).</p><p>Denoting by Pfa(θ) and Pmiss(θ) the false alarm and miss rates at threshold θ, the EER corresponds to the threshold θEER at which the two detection error rates are equal, i.e., EER = Pfa(θEER) = Pmiss(θEER).</p><p>Results for the attack model A1 are shown in Figure <ref type="figure" target="#fig_4">4</ref> for Part-2 and combined Part-1 and Part-2 datasets. Speaker information can be captured for all values h with varying success: EER ranges from 0.86% (for the first hidden layer) up to 20.51% (for the top hidden layer) on Part-2. For the Part-1+Part-2 we observe similar results.    To analyze the impact of each component in Formula (4) on the ASV performance, we separately compute similarity score ρ either using only means (ασ = 0) or only standard deviations (αµ = 0). Results on the Part-2 dataset are shown in Figure <ref type="figure" target="#fig_5">5</ref>. Black bars correspond to ασ = 0 when only means were used to compute similarity scores ρ between personalized models. Blue bars represent results for αµ = 0 when only standard deviations were used to compute ρ. Orange bars correspond to the combined usage of means and standard deviations as in Figure <ref type="figure" target="#fig_4">4</ref> (αµ = 1, ασ = 10). The impact of each component in the sum changes for different hidden layers. When we use only standard deviations, we observe the lowest EER on the first layer. In case of using only means, the first layer is, on the contrary, one of the least informative for speaker verification. For all other layers, combination of means and standard deviations provided superior results over the cases when only one of these components were used. These surprising results for the first hidden layer could possibly be explained by the fact that the personalized models incorporate i-vectors in their inputs, thus the speaker information can be easily learnt at this level of the NN. We plan to investigate this phenomena in detail in our future research.</p><p>We choose two values h ∈ {1, 5} which demonstrate promising results for the model A1, and use the corresponding outputs to train two attack models with the configuration A2. The comparative results for the two attack models are presented in Table <ref type="table">2</ref>. For h = 5, the second attack model provides significant improvement in performance over the first one and reduces EER from 7% down to 2%. For h = 1, we could not obtain any improvement by training a NN based attack model: the results for A2 in this case are worse than for the simple approach A1. One explanation for this phenomenon could be the following. The first layers of the AMs provide highly informative features for speaker classification, however, training the proposed NN model on these features results in overfitting because training criterion of the NN is speaker accuracy, but not the target EER metric, and the number of targets is relatively small, hence, the NN overfits to classify the seen speakers in the training dataset. Table <ref type="table">2</ref>. EER, % evaluated on Part-2, h -indicator of a hidden layer</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">CONCLUSIONS</head><p>In this work, we focused on the privacy protection problem for ASR AMs trained in a FL framework. We explored to what extent ASR AMs are vulnerable to privacy attacks. We developed two attack models that aim to infer speaker identity from the locally updated personalized models without access to any speech data of the target speakers. One attack model is based on the proposed similarity score between personalized AMs computed on some external Indicator dataset, and another one is a NN model. We demonstrated on the TED-LIUM 3 corpus that both attack models are very effective and can provide EER of about 1% for the simple attack model A1 and 2% for the NN attack model A2. Another important contribution of this work is the finding that the first layer of personalized AMs contains a large amount of speaker information that is mainly contained in the standard deviation values computed on Indicator data. This interesting property of NN adapted AMs opens new perspectives also for ASV, and in future work, we plan to use it for developing an efficient ASV system.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Federated learning in a distributed network of clients: 1) Download of the global model Wg by clients. 2) Speaker adaptation of Wg on the local devices using user private data. 3) Collection and aggregation of multiple personalized models Ws 1 ,...,Ws N on the sever. 4) Sharing the resulted model W * g with the clients.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>( 1 )</head><label>1</label><figDesc>j , . . . , u (T j ) j }; a set of personalized models as Ws 1 , . . . , Ws N ∈ W; and an identifier of a hidden layer in the global or personalized AM as h. 1. ∀ Ws i ∈ W, ∀ uj ∈ I we compute activation values from the layer h for model pairs:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Training a speaker embedding extractor for the attack model A2.4. EXPERIMENTS</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. EER, % for the attack model A1 depending on the hidden layer h (in Wg and Ws i ) which was used to compute outputs, evaluated on Part-2 and on the combined Part-1+Part-2 dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. EER, % for the attack model A1 depending on the hidden layer h, evaluated on Part-2 dataset. µ+σ -both means and standard deviations were used to compute similarity score ρ; µ -only means; and σ -only standard deviations were used.</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>Following the notation from<ref type="bibr" target="#b28">[29]</ref>, the model configuration can be described as follows: {-1,0,1} × 6 layers; {-3,0,3} × 7 layers.</p></note>
		</body>
		<back>

			<div type="funding">
<div><p>This work was funded by <rs type="funder">VoicePersonae</rs> and <rs type="funder">DEEP-PRIVACY</rs> (<rs type="grantNumber">ANR-18-CE23-0018</rs>) projects. This work was performed using HPC resources from <rs type="institution">GENCI-IDRIS</rs> (Grant <rs type="grantNumber">2021-AD011013331</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_bPXgxrh">
					<idno type="grant-number">ANR-18-CE23-0018</idno>
				</org>
				<org type="funding" xml:id="_pQAs9k9">
					<idno type="grant-number">2021-AD011013331</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Federated acoustic modeling for automatic speech recognition</title>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Songtao</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Kingsbury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="6748" to="6752" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A federated approach in training acoustic models</title>
		<author>
			<persName><forename type="first">Dimitrios</forename><surname>Dimitriadis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenichi</forename><surname>Kumatani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Gmyr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Interspeech</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="981" to="985" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Study on acoustic model personalization in a context of collaborative learning constrained by privacy preservation</title>
		<author>
			<persName><forename type="first">Salima</forename><surname>Mdhaffar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Tommasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yannick</forename><surname>Estève</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Speech and Computer</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="426" to="436" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Training speech recognition models with federated learning: A quality/cost framework</title>
		<author>
			<persName><forename type="first">Dhruv</forename><surname>Guliani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Giovanni</forename><surname>Franc ¸oise Beaufays</surname></persName>
		</author>
		<author>
			<persName><surname>Motta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="3080" to="3084" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Federated transfer learning with dynamic gradient aggregation</title>
		<author>
			<persName><forename type="first">Dimitrios</forename><surname>Dimitriadis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenichi</forename><surname>Kumatani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Gmyr</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.02452</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Federated learning in ASR: Not as easy as you think</title>
		<author>
			<persName><forename type="first">Wentao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Freiwald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sören</forename><surname>Tewes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabien</forename><surname>Huennemeyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dorothea</forename><surname>Kolossa</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.15108</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Federated learning: Challenges, methods, and future directions</title>
		<author>
			<persName><forename type="first">Tian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anit</forename><surname>Kumar Sahu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ameet</forename><surname>Talwalkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Virginia</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="50" to="60" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A survey on security and privacy of federated learning</title>
		<author>
			<persName><forename type="first">Reza</forename><forename type="middle">M</forename><surname>Viraaji Mothukuri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seyedamin</forename><surname>Parizi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Pouriyeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gautam</forename><surname>Dehghantanha</surname></persName>
		</author>
		<author>
			<persName><surname>Srivastava</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Future Generation Computer Systems</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="page" from="619" to="640" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Inverting gradients-how easy is it to break privacy in federated learning?</title>
		<author>
			<persName><forename type="first">Jonas</forename><surname>Geiping</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hartmut</forename><surname>Bauermeister</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannah</forename><surname>Dröge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Moeller</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.14053</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The secret sharer: Evaluating and testing unintended memorization in neural networks</title>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Úlfar</forename><surname>Erlingsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jernej</forename><surname>Kos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawn</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">28th Security Symposium</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="267" to="284" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Demystifying membership inference attacks in machine learning as a service</title>
		<author>
			<persName><forename type="first">Stacey</forename><surname>Truex</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ling</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mehmet</forename><forename type="middle">Emre</forename><surname>Gursoy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenqi</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Services Computing</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Beyond inferring class representatives: User-level privacy leakage from federated learning</title>
		<author>
			<persName><forename type="first">Zhibo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mengkai</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhifei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qian</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hairong</forename><surname>Qi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE INFOCOM. IEEE</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2512" to="2520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Practical secure aggregation for federated learning on user-held data</title>
		<author>
			<persName><forename type="first">Keith</forename><surname>Bonawitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vladimir</forename><surname>Ivanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Kreuter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.04482</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Differential privacy</title>
		<author>
			<persName><forename type="first">Cynthia</forename><surname>Dwork</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Colloquium on Automata, Languages, and Programming</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Privacy-preserving speech processing: cryptographic and string-matching frameworks show promise</title>
		<author>
			<persName><forename type="first">A</forename><surname>Manas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bhiksha</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName><surname>Raj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paris</forename><surname>Shantanu D Rane</surname></persName>
		</author>
		<author>
			<persName><surname>Smaragdis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE signal processing magazine</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="62" to="74" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A framework for secure speech recognition</title>
		<author>
			<persName><forename type="first">Paris</forename><surname>Smaragdis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Madhusudana</forename><surname>Shashanka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1404" to="1413" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Differentially private generative adversarial network</title>
		<author>
			<persName><forename type="first">Liyang</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaixiang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiayu</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.06739</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Voice anonymization in urban sound recordings</title>
		<author>
			<persName><forename type="first">Alice</forename><surname>Cohen-Hadria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Cartwright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Mcfee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juan</forename><forename type="middle">Pablo</forename><surname>Bello</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 29th International Workshop on Machine Learning for Signal Processing (MLSP)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Introducing the VoicePrivacy initiative</title>
		<author>
			<persName><forename type="first">Natalia</forename><surname>Tomashenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brij</forename><surname>Mohan Lal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emmanuel</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junichi</forename><surname>Nautsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Yamagishi</surname></persName>
		</author>
		<author>
			<persName><surname>Evans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Interspeech</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1693" to="1697" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">The VoicePrivacy 2020 Challenge: Results and findings</title>
		<author>
			<persName><forename type="first">Natalia</forename><surname>Tomashenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emmanuel</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jose</forename><surname>Patino</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Submitted to Computer Speech and Language</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Federated learning for keyword spotting</title>
		<author>
			<persName><forename type="first">David</forename><surname>Leroy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alice</forename><surname>Coucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thibaut</forename><surname>Lavril</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thibault</forename><surname>Gisselbrecht</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><surname>Dureau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP. IEEE</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6341" to="6345" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Training keyword spotting models on non-iid data with federated learning</title>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Hard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kurt</forename><surname>Partridge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cameron</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niranjan</forename><surname>Subrahmanya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aishanee</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pai</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ignacio</forename><forename type="middle">Lopez</forename><surname>Moreno</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rajiv</forename><surname>Mathews</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.10406</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Federated learning for speech emotion recognition applications</title>
		<author>
			<persName><forename type="first">Siddique</forename><surname>Latif</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sara</forename><surname>Khalifa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rajib</forename><surname>Rana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raja</forename><surname>Jurdak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 19th ACM/IEEE International Conference on Information Processing in Sensor Networks (IPSN)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="341" to="342" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Improving on-device speaker verification using federated learning with privacy</title>
		<author>
			<persName><forename type="first">Filip</forename><surname>Granqvist</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matt</forename><surname>Seigel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Áine</forename><surname>Rogier Van Dalen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Cahill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Shum</surname></persName>
		</author>
		<author>
			<persName><surname>Paulik</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.02651</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Retrieving speaker information from personalized acoustic models for speech recognition</title>
		<author>
			<persName><forename type="first">Salima</forename><surname>Mdhaffar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Submitted to ICASSP</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Communication-efficient learning of deep networks from decentralized data</title>
		<author>
			<persName><forename type="first">Brendan</forename><surname>Mcmahan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eider</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Ramage</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Artificial intelligence and statistics</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1273" to="1282" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">X-vectors: Robust DNN embeddings for speaker recognition</title>
		<author>
			<persName><forename type="first">David</forename><surname>Snyder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Garcia-Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><surname>Sell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjeev</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP. IEEE</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="5329" to="5333" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Probabilistic linear discriminant analysis</title>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="531" to="542" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A time delay neural network architecture for efficient modeling of long temporal contexts</title>
		<author>
			<persName><forename type="first">Vijayaditya</forename><surname>Peddinti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjeev</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Sixteenth annual conference of the international speech communication association</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">TED-LIUM 3: twice as much data and corpus repartition for experiments on speaker adaptation</title>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Franc ¸ois Hernandez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sahar</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Natalia</forename><surname>Ghannay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yannick</forename><surname>Tomashenko</surname></persName>
		</author>
		<author>
			<persName><surname>Estève</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Speech and Computer</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="198" to="208" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">The Kaldi speech recognition toolkit</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arnab</forename><surname>Ghoshal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gilles</forename><surname>Boulianne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukas</forename><surname>Burget</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ASRU</title>
		<imprint>
			<publisher>IEEE Signal Processing Society</publisher>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Purely sequence-trained neural networks for ASR based on latticefree MMI</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vijayaditya</forename><surname>Peddinti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Galvez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pegah</forename><surname>Ghahremani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vimal</forename><surname>Manohar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xingyu</forename><surname>Na</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Interspeech</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2751" to="2755" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
