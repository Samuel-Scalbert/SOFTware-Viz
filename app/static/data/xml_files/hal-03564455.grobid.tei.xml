<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">TAG: Learning Timed Automata from Logs</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Lénaïg</forename><surname>Cornanguer</surname></persName>
							<email>lenaig.cornanguer@irisa.fr</email>
							<affiliation key="aff0">
								<orgName type="laboratory">IRISA</orgName>
								<orgName type="institution" key="instit1">Inria</orgName>
								<orgName type="institution" key="instit2">Univ Rennes</orgName>
								<orgName type="institution" key="instit3">CNRS</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Christine</forename><surname>Largouët</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Institut Agro</orgName>
								<orgName type="institution" key="instit1">Univ Rennes</orgName>
								<orgName type="institution" key="instit2">Inria</orgName>
								<orgName type="institution" key="instit3">CNRS</orgName>
								<orgName type="institution" key="instit4">IRISA</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Laurence</forename><surname>Rozé</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution" key="instit1">Univ Rennes</orgName>
								<orgName type="institution" key="instit2">INSA Rennes</orgName>
								<orgName type="institution" key="instit3">CNRS</orgName>
								<orgName type="institution" key="instit4">Inria</orgName>
								<orgName type="institution" key="instit5">IRISA</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Alexandre</forename><surname>Termier</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution" key="instit1">Univ Rennes</orgName>
								<orgName type="institution" key="instit2">Inria</orgName>
								<orgName type="institution" key="instit3">CNRS</orgName>
								<orgName type="institution" key="instit4">IRISA</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">TAG: Learning Timed Automata from Logs</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">052848143408668FE549B631F67F79DD</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-04-12T14:52+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Event logs are often one of the main sources of information to understand the behavior of a system. While numerous approaches have extracted partial information from event logs, in this work, we aim at inferring a global model of a system from its event logs. We consider real-time systems, which can be modeled with Timed Automata: our approach is thus a Timed Automata learner. There is a handful of related work, however, they might require a lot of parameters or produce Timed Automata that either are undeterministic or lack precision. In contrast, our proposed approach, called TAG, requires only one parameter and learns a deterministic Timed Automaton having a good tradeoff between accuracy and complexity of the automata. This allows getting an interpretable and accurate global model of the real-time system considered. Our experiments compare our approach to the related work and demonstrate its merits.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>The classic way of understanding a complex physical, or software system is to perform a manual analysis of this system, in order to produce a model, that should be as accurate as possible. This process is tedious and extremely timeconsuming for the human(s) performing it, especially if the system is large and with many possible states. Due to this complexity, it is not done in practice for most running systems. Instead, the systems are equipped to produce detailed logs, that can help to detect odd behaviors on the fly, or at least allow to perform a post-mortem analysis after a failure happened. Data mining techniques can help analyzing these logs to discover human-understandable local regularities in the log, for example in the form of episodes or chronicles <ref type="bibr" target="#b27">(Zou et al. 2010;</ref><ref type="bibr" target="#b21">Tatti and Vreeken 2012;</ref><ref type="bibr" target="#b19">Sahuguède, Le Corronc, and Le Lann 2018)</ref>, that can directly correspond to parts of the global model. Process mining techniques (van der Aalst 2016) are dedicated to logs exploration, with notably one part dedicated to the construction of a global untimed model in the form of Petri net or BPMN (Business Process Model and Notation) to realize performance analysis or conformance checking. There are recent attempts to include time in the construction of the model and not only in the posterior model analysis <ref type="bibr" target="#b22">(Tax et al. 2019)</ref> but time is still considered in a qualitative way. One can also men-tion the specification mining field which focuses on program logs to learn specifications in the form of untimed state machines <ref type="bibr" target="#b1">(Ammons, Bodík, and Larus 2002)</ref> or Timed Regular Expressions (given its untimed template) <ref type="bibr" target="#b17">(Narayan and Fischmeister 2019)</ref>. However, none of those approaches considers the problem of discovering a human-understandable global temporal model of the real-time system having generated the log. This is the problem tackled in this paper.</p><p>Including time quantitatively in models is crucial for many applications such as self-driving cars or security protocols. To model real-time systems, the formalism of Timed Automata (TA) has been defined by <ref type="bibr" target="#b0">Alur and Dill (Alur and Dill 1994)</ref>. TA extends the formalism of automata by adding clocks expressing explicit timing constraints on the model. TA are well studied, both in theory and practice. The success of TA comes from a powerful formalism with high expressiveness associated with efficient algorithms and tool support. TA have been applied for the analysis of many real-time systems for solving problems as diverse as optimal planning, scheduling, or controlled synthesis <ref type="bibr" target="#b7">(Clarke et al. 2018)</ref>. They have proved to be relevant outside the field of real-time systems, for biological ecosystems, home care plans, agricultural processes, or software systems. In the wide range of applications possible with TA, we can cite optimization of animal waste allocation to crops <ref type="bibr" target="#b13">(Hélias, Guerrin, and Steyer 2008)</ref>, synthetic data generation from real data for privacy matter <ref type="bibr" target="#b8">(Connes, De La Higuera, and Le Capitaine 2021)</ref>, or anomaly detection in water treatment plant <ref type="bibr" target="#b26">(Xu, Ali, and Yue 2021)</ref>. Moreover, the graphical representation and explicit timing constraints between events can offer a human-understandable view of the model, as long as the size remains limited.</p><p>Our objective is thus to learn a timed automaton consistent with the logs of the observed system. There is a large body of work on learning (non-timed) automata and state machines stemming from the grammatical inference field <ref type="bibr" target="#b3">(Angluin 1987;</ref><ref type="bibr" target="#b9">de la Higuera 2010)</ref> and extended for various classes of models (e.g., DFAs, mealy machines, Moore machines). Several algorithms exist (e.g., EDSM, Alergia, MDI) and efficient implementations can be found in LearnLib <ref type="bibr" target="#b14">(Isberner, Howar, and Steffen 2015)</ref>.</p><p>However, learning a model where time constraints play a key role is much more complicated and only a few works have tackled this problem. Learning timed automata is def-initely a new and promising field of research. While some of the existing approaches rely on interactions with a user (active learning setting) <ref type="bibr" target="#b2">(An et al. 2020;</ref><ref type="bibr" target="#b12">Henry, Jéron, and Markey 2020;</ref><ref type="bibr" target="#b11">Grinchtein, Jonsson, and Pettersson 2006)</ref>, we focus our contribution on the classical "passive learning" setting, where the learner has only access to the input logs. In this setting, three main works have been proposed: RTI+ <ref type="bibr" target="#b25">(Verwer, de Weerdt, and Witteveen 2012)</ref>, GenProgTA <ref type="bibr" target="#b20">(Tappler et al. 2018)</ref>, and Timed k-Tail <ref type="bibr" target="#b18">(Pastore, Micucci, and Mariani 2017)</ref>. These works can achieve the discovery of TA from logs, however, they have several drawbacks, either in the number of parameters, the lack of precision of the TA learned, or not outputting a deterministic TA.</p><p>In this paper we propose a new algorithm, called Timed Automata Generator (TAG). TAG learns a timed automaton to describe and understand a time-dependent system only from its generated logs, without any a priori knowledge. Compared to other works dedicated to TA learning, our contributions are the following :</p><p>• TAG, a new algorithm that easily automates the TA learning process from logs since it only requires one parameter. This parameter controls the level of generalization of the learned TA, and thus the level of interpretability of the generated model. The learned TA is deterministic which contributes to the model understanding. • An original pipeline that allows to apply model-checking techniques to query the most complex learned models that are not visually understandable. • The first extensive experimental study and comparison with state-of-the-art algorithms for learning TA. These experiments show that TAG provide the best performances both for scalability and for the precision/recall trade-off. We also provide an experimentation on realworld data from logs of TV programs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Background</head><p>We start by introducing formally the notion of Timed Automata, first through a simple example and then with the definitions and notations used throughout the paper. Our example considers modeling the behavior of a simple light in a meeting room with a timed automaton, depicted in Figure <ref type="figure" target="#fig_0">1</ref>. A single sensor sends a press event when the switch is pressed. The bulb can be off or on with low or high lighting. When the bulb is off, a single press on the switch turns the low light on, while a double press (in less than 2 sec.) makes the light bright. In case the delay is too long between the first and second press, the light turns off.</p><p>The TA defines the behavior of the light by three states (off, light, bright) and the event press tags the transitions. A single clock c measures the time between each event. In TA, transitions are instantaneous and allow the reset of clocks. When a press event occurs, a transition is triggered between the states off and light, and the clock c is reset. If the next press event occurs after 3 time units, the system moves back to off.</p><p>In this illustrative example, we notice that time is of paramount importance. Without knowledge about the value of the clock, the light state cannot be inferred. We now define formally the notions of clock and timed automata, followed by the semantics of the TA and the language it entails.</p><p>Clocks. Let T be a set of time domains. Let C be a finite set of variables called clocks. A clock valuation is a mapping v : C → T which assigns to each clock a time value. Let T C be the set of all clock valuations over C. The set of clock constraints Const(C, T ) is the set of constraints c ∼ t with c ∈ C, ∼ ∈ {&lt;, ≤, =, ≥, &gt;}, and t ∈ T .</p><p>Timed Automaton. A non-deterministic timed automaton A ∈ N T A is a tuple A = (Q, Σ, C, E, q 0 , F) where Q is a finite set of locations, Σ is a finite set of events or symbols, C is a finite set of clocks, E is a finite set of transitions, and q 0 ∈ Q is the initial location. E ⊆ Q × Σ × Const(C, T ), ×P(C) × Q is a finite set of transitions of the form (q, a, g, r, q ) where q and q are respectively the source location and destination location, g is a guard i.e., a constraint on the value of a clock, and r is the set of clocks being reset on the transition.</p><p>Semantics. The semantics of A ∈ N T A is given by the timed transition system (S, s 0 , T, Σ, E) where S = {(q, v) ∈ Q × T C } is the set of states with s 0 = (q 0 , 0). E ⊆ S × (Σ ∪ T ) × S is the transition relation. The set E contains two kinds of transitions: timed transitions (delays) when the clocks valuations evolve in a location, and discrete transitions (jumps) denoted (q, v) → a (q , v ) with a ∈ Σ, expressing that there exists a transition (q, a, g, r, q ) ∈ E such that the valuation v satisfies the guard g and ∀c</p><formula xml:id="formula_0">∈ C if c ∈ r then v (c) = 0 otherwise v (c) = v(c).</formula><p>Language of Timed Automaton. An untimed word w ∈ Σ * is a finite sequence of input symbols (elements from Σ, called alphabet). A timed word tw ∈ (Σ × T ) * is a finite sequence of input symbols and timestamps (non-decreasing). In the following, the set of all possible timed words over Σ and T is called T S(Σ, T ). A timed word tw induces a set of runs over a timed automaton. A word tw is consistent with an automaton A if there exists a run ending in an accepting location. For a timed automaton A, the language L(A) denotes the set of timed words accepted by A.</p><p>Sub-classes of Timed Automata. To learn efficiently timed automata from logs, timed-automata formalism has been restricted to sub-classes <ref type="bibr" target="#b25">(Verwer, de Weerdt, and Witteveen 2012)</ref>. Deterministic timed automaton (DT A) does not allow, from a location, two transitions having the same symbol and overlapping guards towards two different target locations. 1DT A is a DT A restricted to one clock. DRT A (Deterministic Real-Time Automata) is a sub-class of 1DT A where the clock represents the time delay between two consecutive events. The guard is then defined as a closed interval. Each transition resets the clock, and to be triggered the time value of the event should satisfy the guard transition. A PDRTA (Probabilistic Deterministic Real-Time Automata) adds probabilities on the DRTA events.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>State-of-the-Art</head><p>Learning a TA from logs is a complex problem and it has already been proved that the identification of an untimed automaton is NP-complete <ref type="bibr" target="#b10">(Gold 1978)</ref>. All the algorithms presented in this section focus on TA sub-classes and take as input a set of timestamped event sequences, or timed words. In our context, we don't need to distinguish the notions of source and location, and we will use the term "state" for both.</p><p>GenProgTA. GenProgTA <ref type="bibr" target="#b20">(Tappler et al. 2018</ref>) is an algorithm to learn TAs which are deterministic. It is based on genetic programming. At each generation, three operations can be performed (randomly chosen): a mutation (state merging, transition splitting, clock reset addition, state addition), a cross-over (exchange of a part of the automata) between two TA of the same population, or a cross-over between TAs from each population. These operations aren't based on the observations and thus can introduce inconsistencies. The evaluation step scores the individuals with several criteria such as the automaton size, the fitness with the observations, or the determinism. This algorithm has numerous parameters such as the probability of each operation, the population size, or the weights on the evaluation criteria. It also requires information about the timed properties of the system (number of clocks, approximate largest constant in clock constraints). Therefore, this algorithm is more suitable for expert users as it requires a lot of prior knowledge to set the parameters and to choose the final model. Due to the stochastic nature of the approach, different local optima may be found.</p><p>RTI+. RTI+ (Verwer, de Weerdt, and Witteveen 2010) learns the PDRTA sub-class of TA where transitions are labeled with their probability of occurrence. RTI+ is based on the EDSM (Evidence Driven State Merging) algorithm <ref type="bibr" target="#b15">(Lang, Pearlmutter, and Price 1998)</ref> used in grammatical inference to learn untimed automata. The first step is to create an automaton where there exists only one path leading to each state, and consistent with the input sample. Every transition is assigned to the same interval guard bounded by the minimal and maximal observed time value in the whole sample. Thereafter, this tree-shaped automaton is modified in a red-blue framework, i.e. progressively from the initial state to the extremities. Two modification operations can be realized: merging two states and splitting a transition. To decide whether two states should be merged or a transition should be split, a likelihood ratio test is computed. The operation among all the possible operations increasing the most the likelihood of the data with the model is selected. Due to the red-blue framework, the split operation is only applied to transitions which are followed by parts of the automaton not modified (factorized by the merges) yet. RTI+ algorithm has been used to model driving behaviors in the field of autonomous vehicles <ref type="bibr" target="#b16">(Lin et al. 2019)</ref>, however, its management of temporal constraints leads to TAs that lack sensitivity.</p><p>Timed k-Tail. Timed k-Tail (TkT) <ref type="bibr" target="#b18">(Pastore, Micucci, and Mariani 2017)</ref> is based on the k-Tail algorithm <ref type="bibr" target="#b5">(Biermann and Feldman 1972)</ref> used to learn untimed automata. In these two algorithms, the states from which the same future symbol sequences of length k are possible are merged. The sequences are compared on the transition symbols and the set of sequences is called the k-future of the state. Timed k-Tail learns a sub-class of TA that has clock resets, local clocks, and a global clock that measures the time since the beginning of a run. It is designed for systems with nested operations, i.e., an operation (B) can occur while another operation (A) is in progress, as long as B ends before the end of operation A. In the absence of nested operations, the local clocks can be resumed to a unique clock that measures the delay between two operations, as in a RTA. In the automaton learned by Timed k-Tail, the choice of transition at each state is determinist only if we consider the k next symbols.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TAG Algorithm</head><p>State-of-the-art algorithms have drawbacks in terms of number of parameters and the determinism or precision of their output. Based on this observation, our objective was to develop a new algorithm to overcome these limitations and combine their strengths. The outcome is TAG, which stands for Timed Automata Generator, a novel algorithm to learn DRTA from positive timestamped event sequences. In addition to the previously presented DRTA characteristics, the transitions of DRTA learned by TAG are enriched with an indicative probability of occurrence, and a guard on a global clock never reinitialized which measures the time since the beginning of a run. The idea of the algorithm is to first produce an automaton which is basically a graphical representation of the input sample with all its redundancies. The automaton will then be factorized on these structurally redundant parts to obtain a more compact TA. After the size reduction, the temporal values are recomputed, and it may be necessary to refine the automaton in function of the time. The three main operations are the automaton initialization, the states merging, and the transition splitting. Algorithm 1 summarizes the learning process. After a treeshaped automaton initialization (initTA function), the first step of the TAG Algorithm consists in reducing the automaton size by merging all the states that can be merged together (all possible merge), disregarding the temporal values. Algorithm 2 describes the merge operation. Two states can be merged if, from both states, the same events sequences can happen to the system within the k next transitions (the k-future). When no more states can be merged, the algorithm attempts to capture the temporal logic of the system with the split operation (described in algorithm 3). TAG's splits, unlike in RTI+, are applied after the merging step because the factorization has gathered the paths considering the events, so a split that should have been done multiple times only needs to be done once, and only the event sequences that remain specific to a time window are isolated. The split of transitions creates a temporal determinism where time influences the system's evolution. During this step, merges can also be realized but only if no more transition split is needed and if the merge won't be cancelled by a split. Splits and merges are realized in a breadth-first order fashion. TAG ceases when no more split or merge can be done. Each transition is associated with a probability corresponding to the proportion of time the transition has been taken in the input sample. These probabilities are initialized in the automaton initialization and updated after each merge or split with respect to the new distribution. All the subfunctions of the algorithm as well as the consistency proof are given in the supplementary material and an implementation of the algorithm is available 1 . (q 1 , q 2 ) = choice locations to merge(A) for all t ∈ E in (q 2 ) do replace t = (q, a, g, q 2 ) with (q, a, g, q 1 ) end for for all t ∈ E out (q 2 ) do replace t = (q 2 , a, g, q) with (q 1 , a, g, q). end for Q = Q -{q 2 } if q 2 = q 0 then q 0 = q 1 transf orm(A, q 1 ) return A We now illustrate the three major steps of the algorithm through simple examples.</p><p>Automaton Initialization. The first step is to create an automaton consistent with the input sequences. In such preliminary automaton, there exists one unique path leading to each state. Each sequence is represented by one of these paths and two sequences share a portion of path only if they have the same symbolic prefix. As an example, let's consider a set of two sequences: { r:2 p:6 t:5 , r:3 s:1 }. Each element of a sequence is a pair of symbol and delay. Figure <ref type="figure" target="#fig_2">2</ref> presents the resulting preliminary automaton for these input sequences after the initialization step. First, an initial state 1 TAG source code can be found here: https://gitlab.inria.fr/lcornang/tag/. The remainder of the supplementary materials can be found here: https://gitlab.inria.fr/lcornang/aaai22 tag supplementary materials.</p><p>Algorithm 3: split : A × P(T S(Σ, T )) → A Require: a DRTA A = (Q, Σ, E, q 0 ) and S ∈ P(T S(Σ, T )) Return: a DRTA consistent with S (t, g 1 , Q 1 ) = choice transition to split(A) with t = (q 1 , a, g, q 2 ) add a new location q 2,split in Q for (q 2 , e, g, q 3 ) ∈ E with q 3 ∈ Q 1 do replace in the transition q 2 by q 2,split end for In the transition q 1 → a g q 2 replace the guard g by g 2 where g 2 ∪ g 1 = g and g 2 ∩ g 1 = ∅. Add the transition q 1 → a g1 q 2,split in E. return A S0 is created as the starting state for each sequence. The first pair of the first sequence is r:2. Starting from S0, there is no transition labeled with the symbol r, therefore, a transition is created towards a new state, S1, and labeled with the symbol r and the guard [2, 2]. Now considering the second pair p:6, a transition must be created from S1 to a new state S2, and this transition is labeled with the symbol p and the guard <ref type="bibr">[6,</ref><ref type="bibr">6]</ref>. This process is repeated until the end of the sequence. The evaluation of the next sequence restarts from the initial state. Since the first pair r:3 displays a symbol already carried by an outgoing transition of S0, there is no need to create a new transition, and the guard of the transition labeled with r is enlarged to <ref type="bibr">[2,</ref><ref type="bibr">3]</ref> to accept this new temporal value. Then, a new transition is created from S1 for the last pair s:1. Unlike the RTI+ approach, each transition is associated with a specific guard characterizing the observed temporal constraints in the sequences. Finally, the transitions are given a probability corresponding to the learning sample sequences distribution, and an interval guard corresponding to the time elapsed since the beginning of the sequences when they use the given transition. The guards on the global clock aren't shown in the section's figures since they are not used for the model construction. States Merging. As in k-Tail and Timed k-Tail, two states are considered for merging if their k-futures are identical. Let's consider the automaton of Figure <ref type="figure" target="#fig_2">2</ref>, and k = 2, the number of future events to consider per sequence. The k-future of the state S0 is a set of two sequences: { r, p , r, s }, with r, p corresponding to the path going through S0, S1 and S2, and r, s corresponding to the path going through S0, S1, and S4. In this automaton, there is no other state having the same k-future. Otherwise, the two</p><formula xml:id="formula_1">S3 S4 S5 S6 a [2, 3] b [6, 6] c [0, 5] S7 S8 S9 S10 a [4, 6] b [0, 5] d [1, 4] (a) Unmerged parts S3 S4 S5 S6 S10 a [2, 6] b [0, 6] c [0, 5] d [1, 4] (b) Merged parts</formula><p>Figure <ref type="figure">3</ref>: Two parts of the automaton that would have been successively merged and then split in the absence of the guard overlap requirement.</p><p>states would have been merged following the procedure presented in Algorithm 2. Merging two states consists in accumulating their outgoing and incoming transitions on the first and to delete the second. Due to our determinism requirement, if this operation induces a situation where two transitions have the same symbol and the same source, these transitions and their destination states will also be merged.</p><p>Merging two transitions leads to the guard enlargement to include both former guards. This is called the determinization process. After a merge, the probabilities are updated with the new word's distribution. In TAG's last step (splits and merges succession), an overlapping-guards requirement is also added to ensure convergence so a merge doesn't cancel the result of splits (Figure <ref type="figure">3</ref>). k is the only parameter of TAG. By controlling the length of the event sequences to compare, it allows tuning the trade-off between generalization and over-fitting of the model. If the input sample is exhaustive or if detecting wrong behavior is more important than having a small and easily interpretable model, k should be increased. Its default value is set to 2.</p><p>Transitions Splitting. The split procedure relies on both automaton analysis and input sequences. Let's consider a transition and guard [τ min , τ max ] from state q to q , q leading to different parts of the automaton: part1 and part2. Given the analysis of the input sequences, if we observe that some timed words moving through from q to q are always going to part1 with a delay in [τ min , τ ], with τ ∈ [τ min , τ max ], while others lead to part2 with a delay in [τ + 1, τ max ], the transition labeled can be split. A new state q" reached from q is created, such that q leads to part1 and q" to part2. Both transitions outgoing from q are labeled by the symbol of but associated to different guards [τ min , τ ] and [τ + 1, τ max ]. Given the automaton displayed in Figure <ref type="figure" target="#fig_3">4</ref> (left), the traces (not shown here) inform us that some events b at the beginning of the sequences, associated with a delay lower than 5, are followed by an event a. Some other events b at the beginning, associated with a greater delay, are followed by an event c. These events b correspond to the transition between S0 and S3, which will be split as presented in Figure <ref type="figure" target="#fig_3">4</ref>  </p><formula xml:id="formula_2">(right). S0 S1 S2 S3 S4 S5 a [0, 10] b [2, 7] b [5, 8] a [0, 4] c [1, 3] S0 S1 S2 S3 S4 S6 S5 a [0, 10] b [2, 4] b [5, 8] a [0, 4] b [5, 7] c [1, 3]</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiments</head><p>Following the presentation of the state-of-the-art algorithms and our contribution, this section will tackle the evaluation and comparison of the algorithms on synthetic and realworld data. Full data, scripts and results can be found in the supplementary materials.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>To evaluate TAG, we addressed four questions: Q1. How does TAG scale with the complexity of the data? Q2. How does its parameter k impact the result? Q3. What is the contribution of each operation to the result? Q4. How does TAG cope with real-world data?</p><p>We studied the first concern by challenging RTI+, Timed k-Tail, and our algorithm TAG on the quality of their models and their runtime in various situations using synthetic data (we couldn't test GenProgTA because its interface doesn't allow the addition of new data and the source code isn't available). These same data were used to investigate Q2 and Q3. We also carried out a real-data experiment to demonstrate TAG's ability to deal with real-world data and to produce interpretable models, using the logs of the programs of a Canadian TV channel and querying TAG's output with model-checking.</p><p>Experiment on synthetic data. To study how TAG and the state-of-the-art algorithms scale with the complexity of the input data, we identified multiple factors possibly inferring the runtime and model quality. There are two types of factors. The first factors are inherent to the expected model:</p><p>• The alphabet size (number of different events or symbols), • The number of states of the system, • The outdegree (average number of outgoing transitions per state), • The proportion of twinned-transitions (transitions from the same state labeled with the same symbol but having non-overlapping guards), these twinned-transitions create an untimed underminism.</p><p>In real life, these factors are not controllable as the expected model depends on the studied system. The second type of factor is related to the learning process and these can be, to some extent, adjusted by the user:</p><p>• The size of the input sample i.e., the number of timed words the algorithms will have to process,</p><p>• TAG's parameter k, only studied for our algorithm.</p><p>We defined a set of values to test for each factor (Table <ref type="table" target="#tab_0">1</ref>). To evaluate the impact of the factors independently from each other, we varied the factors one after the other. When one factor varies, the others have a fixed value (in bold in Table <ref type="table" target="#tab_0">1</ref>). We then ranked these factors according to the estimated impact they would have on the model quality and the runtime. We tested the factors in order of increasing impact (average impact on model quality and runtime), which corresponds to the order of the factors in trol the factors related to the expected model and to have a ground truth, we started from synthetic model TAs, generated to have a given value for each factor. We generated 200 TAs per combination to gain confidence in the results. The datasets are composed of runs of these model TAs, each run being a timed word. For the evaluation of the learned TA, we also generated 100 other timed words consistent with the model and 100 words inconsistent with the model. We used the same protocol for the ablation study. 200 TA were generated with the default value for every factor. The runtime was measured on a MacBook Pro with an Intel Core i9 processor clocked at 2,4 GHz and a memory of 16 Go 2667 MHz DDR4. TAG is implemented in Python, Timed k-Tail's author implementation (where k is hard coded to 2) is in Java and RTI+ in C++. RTI+ was executed with a significance value of 0.05 for the likelihood ratio test (default value). TkT was executed with no nested events considerations (absent in our data) and no enlargement of the guards (default value). To evaluate the learned TAs, two accuracy scores were selected. The True Positive Rate (TPR), also called recall, is the probability for a word consistent with the model to be recognized by the learned automaton. The Positive Predictive Value (PPV), also called precision, is the probability for a word recognized by the learned automaton to be consistent with the model automaton. A high recall and precision are both desired but are in practice rather impossible to have simultaneously. As we haven't a specific field of application, we consider them equally important. The F1-score is the harmonic mean of these two measures. Aiming for a good F1-score leads us to a good trade-off between recall and precision.</p><p>Experiment on real-world data. To assess TAG's ability to learn an interpretable and exploitable model from real data, we used the logs of the programs of the Canadian TV channel CBC <ref type="bibr">Windsor (Canadian Radio-television and Telecommunications Commission 2015)</ref>. We first used the data of the Friday mornings of August 2020 (from 6:00 AM to 12:00 AM, one word per day). Then, we used the data of every day of July and August 2020 (Canadian summer school vacations months). The entries of the logs were summarized by their class (commercial message, promotion for a program ...) or category (program for children, news...) in case of a program. A word consists of the sequence of entries class/category and their duration for a day.</p><p>To query the automaton learned by TAG with the summer data, we can take advantage of both the classical expressiveness of TA and the probabilities associated with the TAG's TA transitions. We used UPPAAL SMC <ref type="bibr" target="#b6">(Bulychev et al. 2012</ref>), a model-checking tool for Timed Automata with stochastic properties. UPPAAL SMC extends the basic query language of UPPAAL <ref type="bibr" target="#b4">(Bengtsson et al. 1996)</ref>, which is a subset of Timed Computation Tree Logic (TCTL), with queries related to the stochastic behavior of systems. The result of UPPAAL SCM queries is obtained by monitoring simulations of the system and by statistical hypothesis testing. The answer is an interval of probability with a confidence of 0.95 of being within. We used both classical UP-PAAL and UPPAAL SMC queries and for the latter, the number of simulations was fixed at 10000.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>Experiment on synthetic data. The whole results of the comparison of the algorithms for every factor are in the supplementary materials. We only present here the most significant results. For all the statistical tests, the significance level is of α = 0.05. On the figures, one asterisk indicates a significant difference with a p-value of at least α.</p><p>Q1. How does TAG scale with the complexity of the data?</p><p>In the first place, we focus on the runtime. Globally, the runtime of the three algorithms is comparable, with RTI+ (median x = 0.22s, interquartile range IQR = 0.25s) followed by TAG (x = 0.46s, IQR = 0.21s) and Timed k-Tail (x = 0.64s, IQR = 0.06s). The factors positively correlated with TAG's runtime are the following: the outdegree as more merges are necessary; the number of states since TAG process the pair of states in a breadth-first order to find split candidates; the parameter k as the states kfuture is constructed recursively; and obviously the number of timed words (Figure <ref type="figure" target="#fig_6">6b</ref>). The twinned transition proportion is slightly negatively correlated to the runtime.</p><p>Turning now on the quality of the obtained models, evaluated with the recall, the precision, and the F1-score, Figure 5 presents these scores for the three algorithms and for all the tested combinations. Globally, TAG's precision always stays good (0.92 on average) and significantly better than the other algorithms (rate of increase of 0.48 with RTI+ and 0.34 with Timed k-Tail). Due to the trade-off, it leads to a slight loss of recall in comparison to Timed k-Tails (rate loss of 0.02, giving a recall of 0.97 on average) but the gain in accuracy is significant with an F1-score above the others on average. Timed k-Tail has the best recall overall but tends to produce models too generalized because of the absence of splits. RTI+ produces models not precise enough because of its bad management of the time constraints that leads to guards too wide. Let's now look at the effect of the different factors. As shown in Figure <ref type="figure" target="#fig_7">7</ref>, TAG is less sensitive to the alphabet size than Timed k-Tail, which produces models too compact and thus not much precise when there are few different symbols, and RTI+, which doesn't generalize enough in presence of many symbols and therefore doesn't recognize well new words of the language. TAG is sensitive to the complexity of the model automaton (outdegree, number of states, and twinned transitions proportion). As it becomes more complex, the recall decreases since the represented part of the language in the sample data becomes smaller, and thus the new words will be less probably recognized. However, its precision stays good (0.79 on average for the worst case corresponding to an outdegree of 2.5) and systematically above the other algorithms, as well as the F1-score. This means that TAG automata are accurate and not too generalized. Finally, the recall (and F1-score) of the TA of the three algorithms unsurprisingly increases with the number of timed words in the input data sample (Figure <ref type="figure" target="#fig_6">6a</ref>), with a stabilization of the score at 500 timed words for our parameter's combination. In summary, these results show that TAG offers a better trade-off between recall and precision than the state-of-theart algorithms with a comparable runtime. Globally, TAG automata are accurate and not too generalized. The factor impacting the most TAG is the number of timed words in the input sample, which increases the runtime but also the quality of the obtained TA. The complexity factors penalizing TAG's quality are the number of states and the outdegree. Q2. How does TAG's parameter k impact the result?</p><p>As k increases, the precision tends to increase while the recall decreases (Figure <ref type="figure">8</ref>). However, the gain in precision is inferior to the loss of recall. Therefore, the default value is set to k = 2, letting the user freedom to tune it in function to its application and its needs. Particularly, there can be a visual impact and thus an interpretability impact if the model is meant to be human-understandable. Q3. What is the contribution of each operation to the result?</p><p>We performed an ablation study to confirm the importance of each operation of TAG. Figure <ref type="figure" target="#fig_8">9</ref>     precision, the recall, and the F1-score obtained for the TA produced with only the tree-shaped automaton initialization, then with the initialization and the merges, and finally with the initialization, the merges, and the splits. With only the initialization, the precision is maximal since there is no generalization, the automaton language corresponds to the input traces (plus the values inside the guard intervals). For the same reason, the recall is lower than with the merges and the splits. Merges induce a significant augmentation of the recall by generalizing the model. Lastly, TAG recall and precision are improved by the splits leading to a better F1-score. Beyond these positive results on synthetic and random data, the importance of the splits becomes more evident in the case of systems where some events would happen after a first event only within a limited time window and never otherwise. In an TA learned with splits, the timed condition would be necessary to access this part of the TA.</p><p>Experiment on real-world data.  Q4. How does TAG cope with real-world data?</p><p>The TA learned by TAG with the TV logs of the Friday mornings (329 events) is shown in Figure <ref type="figure" target="#fig_9">10</ref>. The guards are originally in seconds and have been formatted to make the figure more apprenhensible. The first guard in bracket limits the delay of occurrence after the last event. The second guard in bracket preceded by the letter "t" corresponds to the value of a global clock started at 6:00 AM in the initial state and never reset.  This TA has 6 states, 10 transitions, and 6 different symbols. This TA shows that after the morning news, which lasts about one hour (A on the figure), and a session of ads or program promotions, films and/or children's programs fol- The TA learned with the whole data of July and August (54065 events) is naturally much bigger since the time slot is wider and the day types differ. 15240 merges and 12 splits were necessary. It has 65 states, 125 transitions, and 14 different symbols. Table <ref type="table" target="#tab_5">2</ref> presents the queries submitted to the model-checker, the results, and the execution time. In less time than a human would take to analyze even the first TV programs automaton, the responses to useful questions can be obtained with model-checking queries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>This paper introduces a new algorithm, TAG, to learn Timed Automata from logs of real-time systems. This learned automaton can be used to model a time-dependent system to understand its behavior without any a-priori knowledge about it. The unique parameter k offers a trade-off between precision and recall of the learned model, depending on the application domain or the desired level of interpretability. On this relatively new subject, this study is the only one that compares the existing approaches. Experiments have shown that TAG is fully capable of inferring a TA describing the system including realistic time constraints while remaining interpretable, visually if the model is small, and thanks to model-checking otherwise. The model can then be used to perform anomaly detection, data generation, prediction, or else verification. An interesting perspective would be to learn a model composed of various interacting subsystems to limit the global model size. As more and more physical systems are being equipped with sensors producing time series, it could also be useful to develop a method to obtain a TA from this kind of data.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Timed Automaton of a light.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Algorithm 1 :</head><label>1</label><figDesc>TAG : P(T S(Σ, T )) → A Require: an input S = {S + } Return: a DRTA consistent with S meaning that ∀s ∈ S, s ∈ L(A) A = initTA(S) A=all possible merge(A) repeat A=all possible split(A) SAVE=A A=merge(A) until SAVE=A return A Algorithm 2: merge : A → A Require: a DRTA A Return: a DRTA merge(A) such that L(A) ⊆ L(merge(A)) and |merge(A)| &lt; |A|.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: A TA initialized with a set of events sequences.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: A transition division (left: before, right: after).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Scores per algorithm.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>Evolution of the runtime.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Evolution of the recall and the runtime w.r.t.the number of timed words.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Precision and recall when the alphabet size varies.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Ablation study on recall, precision and F1-score.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: Learned TA from the Friday morning logs of a Canadian TV channel.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>To con-</figDesc><table><row><cell>Factor</cell><cell>Tested values</cell></row><row><cell>Alphabet size</cell><cell>2, 4, 5, 6, 8, 10, 15</cell></row><row><cell>Outdegree</cell><cell>1, 1.25, 1.5, 1.75, 2, 2.25, 2.5</cell></row><row><cell>State number</cell><cell>4, 6, 8, 10, 15, 25, 50, 75, 100, 500</cell></row><row><cell>Twinned transitions proportion</cell><cell>0, 0.10, 0.25, 0.50, 0.7</cell></row><row><cell cols="2">Number of timed words 50, 100, 500, 1000, 2500</cell></row><row><cell>TAG parameter k</cell><cell>2, 3, 4, 5, 6</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Factors order and values (default value in bold).</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 2 :</head><label>2</label><figDesc>Queries, result, and execution time. low one another until 11:00 AM (B on the figure). Children's programs are more probable and these programs are frequently cut by interstitials. Then, the ads and program's teasers are back and cut children's programs until the 12:00 AM news (C on the figure). The interest of the probabilities and the guards on the global clock is demonstrated here since they provide substantial information for the system comprehension.</figDesc><table><row><cell>Query</cell><cell>Result</cell><cell>Time (s)</cell></row><row><cell>What is the probability to watch one</cell><cell></cell><cell></cell></row><row><cell>hour of children's program without interruption?</cell><cell>[0.07,0.08]</cell><cell>0.722</cell></row><row><cell>If we watch this channel all the day</cell><cell></cell><cell></cell></row><row><cell>long, are we sure to have, at some</cell><cell>No</cell><cell>0.003</cell></row><row><cell>point, a children's program?</cell><cell></cell><cell></cell></row><row><cell>Between 10h and 11h, what is the</cell><cell></cell><cell></cell></row><row><cell>probability to watch a children's pro-gram?</cell><cell>[0.46,0.56]</cell><cell>0.049</cell></row><row><cell>Globally, is it more probable to have a film than a children's program?</cell><cell>No</cell><cell>0.011</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A theory of timed automata</title>
		<author>
			<persName><forename type="first">R</forename><surname>Alur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Dill</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Theoretical Computer Science</title>
		<imprint>
			<biblScope unit="volume">126</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="183" to="235" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Mining specifications</title>
		<author>
			<persName><forename type="first">G</forename><surname>Ammons</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bodík</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Larus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th ACM SIGPLAN-SIGACT symposium on Principles of programming languages -POPL &apos;02</title>
		<meeting>the 29th ACM SIGPLAN-SIGACT symposium on Principles of programming languages -POPL &apos;02<address><addrLine>Portland, Oregon</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="4" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning One-Clock Timed Automata</title>
		<author>
			<persName><forename type="first">J</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Tools and Algorithms for the Construction and Analysis of Systems (TACAS)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="444" to="462" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning regular sets from queries and counterexamples</title>
		<author>
			<persName><forename type="first">D</forename><surname>Angluin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information and Computation</title>
		<imprint>
			<biblScope unit="volume">75</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="87" to="106" />
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">UPPAAL -a tool suite for automatic verification of real-time systems</title>
		<author>
			<persName><forename type="first">J</forename><surname>Bengtsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Larsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Larsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Pettersson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Yi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Hybrid Systems III</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">G</forename><surname>Goos</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Hartmanis</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Van Leeuwen</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Alur</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><forename type="middle">A</forename><surname>Henzinger</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><forename type="middle">D</forename><surname>Sontag</surname></persName>
		</editor>
		<meeting><address><addrLine>Berlin, Heidelberg; Berlin Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1996">1996</date>
			<biblScope unit="volume">1066</biblScope>
			<biblScope unit="page" from="232" to="243" />
		</imprint>
	</monogr>
	<note>Series Title</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">On the Synthesis of Finite-State Machines from Samples of Their Behavior</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">W</forename><surname>Biermann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Feldman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Computers, C</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="592" to="597" />
			<date type="published" when="1972">1972</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">UPPAAL-SMC: Statistical Model Checking for Priced Timed Automata</title>
		<author>
			<persName><forename type="first">P</forename><surname>Bulychev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">G</forename><surname>Larsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mikučionis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bøgsted Poulsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Legay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<ptr target="https://open.canada.ca/data/en/dataset/800106c1-0b08-401e-8be2-ac45d62e662e" />
	</analytic>
	<monogr>
		<title level="j">Electronic Proceedings in Theoretical Computer Science</title>
		<imprint>
			<biblScope unit="volume">85</biblScope>
			<biblScope unit="page" from="1" to="16" />
			<date type="published" when="2012">2012. 2015. Aug. 23, 2021</date>
			<publisher>Canadian Radio-television and Telecommunications Commission</publisher>
		</imprint>
	</monogr>
	<note>Television program logs</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Handbook of Model Checking</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Clarke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">A</forename><surname>Henzinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Veith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bloem</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Using Grammatical Inference to Build Privacy Preserving Data-sets of User Logs</title>
		<author>
			<persName><forename type="first">V</forename><surname>Connes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>De La Higuera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Le Capitaine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Grammatical Inference</title>
		<meeting><address><addrLine>Nantes, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Grammatical Inference: Learning Automata and Grammars</title>
		<author>
			<persName><forename type="first">C</forename><surname>De La Higuera</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Complexity of automaton identification from given data</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Gold</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information and Control</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="302" to="320" />
			<date type="published" when="1978">1978</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Inference of Event-Recording Automata Using Timed Decision Trees</title>
		<author>
			<persName><forename type="first">O</forename><surname>Grinchtein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Jonsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Pettersson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CONCUR 2006 -Concurrency Theory</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="435" to="449" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Active Learning of Timed Automata with Unobservable Resets</title>
		<author>
			<persName><forename type="first">L</forename><surname>Henry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Jéron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Markey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Formal Modeling and Analysis of Timed Systems FORMATS</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="144" to="160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Using timed automata and model-checking to simulate material flow in agricultural production systems-Application to animal waste management</title>
		<author>
			<persName><forename type="first">A</forename><surname>Hélias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Guerrin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-P</forename><surname>Steyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computers and Electronics in Agriculture</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="183" to="192" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">The Open-Source LearnLib -A Framework for Active Automata Learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Isberner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Howar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Steffen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Aided Verification (CAV)</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="487" to="495" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Results of the Abbadingo one DFA learning competition and a new evidence-driven state merging algorithm</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">J</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">A</forename><surname>Pearlmutter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Price</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Grammatical Inference LNCS</title>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="volume">1433</biblScope>
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">MOHA: A Multi-Mode Hybrid Automaton Model for Learning Car-Following Behaviors</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Verwer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="790" to="796" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Mining Time for Timed Regular Specifications</title>
		<author>
			<persName><forename type="first">A</forename><surname>Narayan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Fischmeister</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE International Conference on Systems, Man and Cybernetics (SMC)</title>
		<meeting><address><addrLine>Bari, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="63" to="69" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Timed ktail: Automatic inference of timed automata</title>
		<author>
			<persName><forename type="first">F</forename><surname>Pastore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Micucci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Mariani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International conference on software testing, verification and validation (ICST)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="401" to="411" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Chronicle Discovery for Diagnosis from Raw Data: A Clustering Approach</title>
		<author>
			<persName><forename type="first">A</forename><surname>Sahuguède</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Le Corronc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-V</forename><forename type="middle">V</forename><surname>Le Lann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">10th IFAC Symposium on Fault Detection, Supervision and Safety for Technical Processes, SAFE-PROCESS 2018</title>
		<meeting><address><addrLine>Varsaw, Poland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Learning Timed Automata via Genetic Programming</title>
		<author>
			<persName><forename type="first">M</forename><surname>Tappler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">K</forename><surname>Aichernig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">G</forename><surname>Larsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Lorber</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.07744</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">The long and the short of it: summarising event sequences with serial episodes</title>
		<author>
			<persName><forename type="first">N</forename><surname>Tatti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Vreeken</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<editor>
			<persName><forename type="first">Q</forename><surname>Yang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Agarwal</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Pei</surname></persName>
		</editor>
		<imprint>
			<publisher>KDD</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="462" to="470" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Generating time-based label refinements to discover more precise process models</title>
		<author>
			<persName><forename type="first">N</forename><surname>Tax</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Alasgarov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Sidorova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Haakma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">M P</forename><surname>Van Der Aalst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Ambient Intell. Smart Environ</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="165" to="182" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Process Mining: Data Science in Action</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">M P</forename><surname>Van Der Aalst</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>Springer</publisher>
			<pubPlace>Berlin, Heidelberg; Berlin Heidelberg</pubPlace>
		</imprint>
	</monogr>
	<note>2nd ed. 2016 edition</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A likelihood-ratio test for identifying probabilistic deterministic real-time automata from positive data</title>
		<author>
			<persName><forename type="first">S</forename><surname>Verwer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>De Weerdt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Witteveen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Colloquium on Grammatical Inference</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="203" to="216" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Efficiently identifying deterministic real-time automata from labeled data</title>
		<author>
			<persName><forename type="first">S</forename><surname>Verwer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>De Weerdt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Witteveen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="295" to="333" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Digital Twin-based Anomaly Detection in Cyber-physical Systems</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Yue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 14th IEEE Conference on Software Testing, Verification and Validation (ICST)</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="205" to="216" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Frequent Instruction Sequential Pattern Mining in Hardware Sample Data</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Data Mining (ICDM)</title>
		<editor>
			<persName><forename type="first">G</forename><forename type="middle">I</forename><surname>Webb</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><surname>Liu</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Gunopulos</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">X</forename><surname>Wu</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="1205" to="1210" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
