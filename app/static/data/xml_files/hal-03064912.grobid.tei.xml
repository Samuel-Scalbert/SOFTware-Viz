<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning URI Selection Criteria to Improve the Crawling of Linked Open Data (Extended Abstract) *</title>
				<funder ref="#_gymesbR">
					<orgName type="full">Inria</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Hai</forename><surname>Huang</surname></persName>
							<email>hai.huang@inria.fr</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Inria</orgName>
								<orgName type="institution" key="instit2">Université Côte d&apos;Azur</orgName>
								<orgName type="institution" key="instit3">CNRS</orgName>
								<address>
									<region>I3S</region>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Fabien</forename><surname>Gandon</surname></persName>
							<email>fabien.gandon@inria.fr</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Inria</orgName>
								<orgName type="institution" key="instit2">Université Côte d&apos;Azur</orgName>
								<orgName type="institution" key="instit3">CNRS</orgName>
								<address>
									<region>I3S</region>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Learning URI Selection Criteria to Improve the Crawling of Linked Open Data (Extended Abstract) *</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">B55A6187E0C948E92290681908FE16F2</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-04-12T14:51+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>A Linked Data crawler performs a selection to focus on collecting linked RDF (including RDFa) data on the Web. From the perspectives of throughput and coverage, given a newly discovered and targeted URI, the key issue of Linked Data crawlers is to decide whether this URI is likely to dereference into an RDF data source and therefore it is worth downloading the representation it points to. Current solutions adopt heuristic rules to filter irrelevant URIs. But when the heuristics are too restrictive this hampers the coverage of crawling. In this paper, we propose and compare approaches to learn strategies for crawling Linked Data on the Web by predicting whether a newly discovered URI will lead to an RDF data source or not. We detail the features used in predicting the relevance and the methods we evaluated including a promising adaptation of FTRL-proximal online learning algorithm. We compare several options through extensive experiments including existing crawlers as baseline methods to evaluate their efficiency.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Linked Data extends the principles of the World Wide Web from linking documents to that of linking pieces of data to weave a Web of Data. This relies on the well-known linked data principles <ref type="bibr" target="#b12">[Heath and Bizer, 2011;</ref><ref type="bibr">Berners-Lee, 2006]</ref> including the use of HTTP URIs that can be dereferenced and the provision of useful and linked descriptions upon access so that we can discover more things.</p><p>A large amount of data are now being made available as linked data in various domains such as health, publication, agriculture, music, etc., and the Web of Linked Data is growing exponentially <ref type="bibr" target="#b8">[Ermilov et al., 2016]</ref>. In order to harvest this enormous data repository, crawling techniques for Linked Data are becoming increasingly important. Different from conventional crawlers, crawling for Linked Data is performed selectively to collect structured data connected by RDF links. The design objective of Linked Data crawlers is to fetch linked RDF data in different formats -RDF/XML, N3, RDFa, JSON-LD, etc. -as much as possible within a reasonable time while minimizing the download of irrelevant URIs -i.e. leading to resources without RDF content. Therefore our challenge is to identify as soon as possible the URIs referencing RDF sources without downloading them first. Our research question here is: can we learn efficient URI selection criteria to identify sources of Linked Open Data?</p><p>To solve this problem, we propose and compare methods on real data to predict whether a newly discovered URI will lead to RDF data source or not. We extract information from the targeting and referring URI and from the context (RDF data graph) where URIs and their links were discovered in order to produce features fed to learning algorithms and in particular to an FTRL-proximal online method employed to build the prediction model.</p><p>The contributions of this work include: 1) we identify the features to predict whether a target URI will lead to some RDF data or not; 2) we adapt the FTRL-proximal algorithm to our task and build an online prediction model; and 3) we implement a Linked Data crawler with the online prediction model and evaluate its performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Semantic web crawlers differ from traditional web crawlers in only two aspects: the format of the source (RDF format) it is traversing, and the means to link RDF across data sources. There exist some work <ref type="bibr">[Dodds, 2006;</ref><ref type="bibr" target="#b20">Isele et al., 2010;</ref><ref type="bibr" target="#b16">Hogan et al., 2011]</ref> in the field of Semantic Web/Linked Data crawling. The two main representative crawlers for Linked Data are LDSpider <ref type="bibr" target="#b20">[Isele et al., 2010]</ref> and SWSE crawler <ref type="bibr" target="#b16">[Hogan et al., 2011]</ref>. They crawl the Web of Linked Data by traversing RDF links between data sources and follow the Linked Data principles <ref type="bibr" target="#b12">[Heath and Bizer, 2011;</ref><ref type="bibr">Berners-Lee, 2006]</ref>.</p><p>In order to reduce the amount of HTTP lookups and downloading wasted on URIs referencing non-RDF resources, these previous works apply heuristic rules to identify relevant URIs <ref type="bibr" target="#b20">[Isele et al., 2010;</ref><ref type="bibr" target="#b16">Hogan et al., 2011]</ref>. The URIs with common file extensions (e.g., html/htm, jpg, pdf, etc.) and those without appropriate HTTP Content-Type Header (such as application/rdf+xml) are classified as non-RDF content URIs. The content of these URIs would not be retrieved by these crawlers. Although this heuristic-based method is efficient, it impairs the recall of the crawling. This method makes the assumption that data publishers have provided correct HTTP Content-Type Header but this is not always the case. It can happen that the server does not provide the desired content type. Moreover, it may happen that the server returns an incorrect content type. For example, Freebase did not provide any content negotiation or HTML resource representation <ref type="bibr" target="#b10">[Färber et al., 2018]</ref>, and only text/plain was returned as content type. In <ref type="bibr" target="#b14">[Hogan et al., 2010]</ref>, it is reported that 17% of RDF/XML documents are returned with a content-type other than application/rdf+xml. As a result, a huge volume of RDF data is missed by these methods.</p><p>Traditional focused crawlers on the Web aim to crawl a subset of Web pages based on their relevance to a specific topic <ref type="bibr" target="#b2">[Chakrabarti et al., 1999;</ref><ref type="bibr" target="#b4">Diligenti et al., 2000]</ref>. These methods build a relevancy model typically encoded as a classifier to evaluate the web documents for topical relevancy. Our work here is different in the sense that we do not filter on the topics but on the type of content. Meusel et al. <ref type="bibr" target="#b28">[Meusel et al., 2014]</ref> proposed a focused crawling approach to fetch microdata embedded in HTML pages. <ref type="bibr" target="#b30">Umbrich et al. [Umbrich et al., 2008]</ref> built a crawler focused on gathering files of a particular media type and based on heuristics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Prediction Model for Crawling Criteria</head><p>In this section, we present the prediction task, the feature sets extracted for the task of prediction and then describe the prediction model based on FTRL-proximal online learning algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Task Description</head><p>Since the task of Linked Data crawlers is to fetch RDF data on the Web, we are interested in RDF-relevant URIs: Definition 1. (RDF-Relevant) Given a URI u, we consider that u is RDF-relevant if the representation obtained by dereferencing u contains RDF data. Otherwise, u is called non RDF-relevant. We note U R the set of RDF relevant URIs and U I the set of non RDF-relevant URIs with U = U R ∪U I .</p><p>For the URIs that have certain file extensions such as *.rdf/owl or HTTP Content Types Headers such as application/rdf+xml, text/turtle, etc., it is trivial to know the RDFrelevance of them. In this work, we focus on a knid of URIs called hard URIs whose RDF-relevance cannot be known by these heuristics. Definition 2. (Hard URI) We call u a hard URI if the RDF relevance of u cannot be known straightforwardly by its file extension or HTTP Content-Type Header.</p><p>For example, URI u with HTTP Content-Type header text/html is a hard URI since RDFa data could be embedded in u. As reported in <ref type="bibr" target="#b10">[Färber et al., 2018]</ref>, the URIs with HTTP Content-Type Header text/plain may contain RDF data so they are hard URIs too.</p><p>Then, for our prediction task, we consider four types of URIs involved in the prediction: the target URI, the referring URI, direct property URIs and sibling URIs. Definition 3. (Target URI) We call target URI and note u t the URI for which we want to predict if the deref (u t ) will lead to a representation that contains RDF data i.e. if u t is RDF-Relevant. Definition 4. (Context RDF Graph) Given an RDF relevant URI u r containing the RDF graph G r t , we define G r t = (V, E) as the context RDF graph of URI u t if u t appears in G r t as a node, i.e., u t ∈ V . Definition 5. (Referring URI) Given a target URI u t , we call "referring URI" and note u r the RDF-relevant URI that was dereferenced into a representation deref (u r ) containing the context RDF graph G r t in which we found the target URI u t . Definition 6. (Sibling Set) The sibling set of u t in the context RDF Graph G r t denoted by Sib ut is the set of all other URIs that have common subject-property or property-object pair with u t in G r t . Sib ut is formally defined as:</p><formula xml:id="formula_0">Sib ut = {s|∃p, o ∈ G r t , s.t.(u t , p, o) ∈ G r t ∧ (s, p, o) ∈ G r t } ∨ {o|∃s, p ∈ G r t , s.t.(s, p, u t ) ∈ G r t ∧ (s, p, o) ∈ G r t } Definition 7. (Direct Property Set)</formula><p>The direct property set P S t is the set of properties that connect u t in the context graph G r t :</p><formula xml:id="formula_1">P S t = {p|∃s, o ∈ G c t , s.t.(u t , p, o) ∈ G r t ∨ (s, p, u t ) ∈ G r t } Prediction Task.</formula><p>Using these definitions we can now define our prediction task. Suppose that a hard URI u t is discovered from the context graph G r t obtained by dereferencing a referring URI u r . We want to predict if u t is RDFrelevant based on some features extracted from u t , u r , P S t and Sib ut . Our task is to learn the mapping Relevant :</p><formula xml:id="formula_2">U → {0, 1} with Relevant(u) equals 1 if u is RDF-relevant ( Relevant| U R → 1) and equals 0 if u is not RDF-relevant (Relevant| U I → 0)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Feature Extraction</head><p>We distinguish between two kinds of features that can be exploited for the prediction intrinsic and extrinsic. Definition 8. (Intrinsic URI Features) The intrinsic features of a URI u are features obtained by an extraction F int : U → F that relies exclusively on the URI identifier itself.</p><p>An example of an intrinsic feature is the protocol e.g. http. Definition 9. (Extrinsic URI Features) The extrinsic features of a URI u are features obtained by performing some network call on the URI F ext : U → F .</p><p>The URI generic syntax consists of a hierarchical sequence of several components. An example of URI is shown in Fig. <ref type="figure" target="#fig_0">1</ref>.</p><p>The intrinsic features F int (u) consider that the different components of a URI u are informative and contain helpful information for prediction. The components include: scheme, authority, host, path, query, fragment information. We generate the intrinsic features of u based on these components.</p><p>We further distinguish two kinds of extrinsic features: Definition 10. (URI Header Features) These extrinsic features of a URI u are obtained by an HTTP HEAD call F head : U → F and do not require to download the representation of the resource identified by u. Definition 11. (URI Representation Features) These extrinsic features of a URI u are obtained by an HTTP GET call F get : U → F and characterize the content obtained after a complete download of the representation of the resource identified by u.</p><p>We distinguish F head and F get because they have different costs in terms of network access time. The URI header feature we will consider in this paper is the content type e.g. feature u t contentType='text/html'.</p><p>URI representation features come from the content of the referring URI u r , namely the context graph G r t of u t which includes the direct properties and siblings information of u t . Definition 12. (URI Similarity Features) The similarity between two URIs u s and u t denoted by simV alue(u s , u t ) is defined using the Levenshtein distance <ref type="bibr">[Levenshtein, 1966]</ref> between the two strings representing these URIs. In order to reduce the feature space, a threshold τ is set for the similarity value and if the similarity value is larger than τ , we discretize the similarity as simV alue(u t , u s ) = high and otherwise simV alue(u t , u s ) = low. Definition 13. (RDF Relevance Features) The boolean characteristic of being RDF-relevant for a URI u is noted F RDF rel (u) and returns true if u is RDF-revelant and false otherwise.</p><p>With the types of features explained above we can now define four atomic feature sets based on the sources the features come from to explore and evaluate when training and predicting the RDF-relevance of a target URI u t :</p><p>• F +t = F int (u t ) + F head (u t ) is a feature set considering the intrinsic and header features of the target URI u t . • F +r = F int (u r ) is a feature set considering the intrinsic features of the referring URI u r .</p><formula xml:id="formula_3">• F +p = p∈P St F int (p) is the set of intrinsic features of each direct property of the target URI u t . • F +x = us∈Sibu t F x (u s ) is a feature set including fea-</formula><p>ture crosses that combine the intrinsic, header, similarity and relevance features of the sibling URIs of u t . This supports predictive abilities beyond what those features can provide individually and can be interpreted as using a logical conjunction 'AND' to combine these features</p><formula xml:id="formula_4">F x (u s ) = F int (u s )× F head (u s ) × F sim (u s , u t ) × F RDF rel (u s )</formula><p>With these definitions we can now consider and evaluate any combination of feature sets. We note F +a the feature set with a being a combination of the atomic feature sets as defined above. For instance an experiment using the feature set Algorithm 1: Online prediction with the FTRL-Proximal algorithm Data: F +t+r will only consider as inputs the intrinsic and header features of the target URI u t and the intrinsic feature of referring URI u r . In Section 4, the predictive abilities of different combinations of the feature sets are examined.</p><formula xml:id="formula_5">URIs u 1 , u 2 , • • • , u T Result: ŷ1 , • • • , ŷT 1 for t = 1 to T do</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Online Prediction</head><p>During the process of crawling, the crawler will encounter URIs belonging to millions of domains, and the number of properties could be tens of thousands. Obviously, the potential feature space will be huge. Thus, we use feature hashing technique <ref type="bibr" target="#b32">[Weinberger et al., 2009]</ref> to map high-dimensional features into binary vectors.</p><p>We assume that data becomes available in a sequential order during the crawling process. The predictor makes a prediction at each step and can also update itself: it operates in an online style. Compared to batch methods, online methods are more appropriate for the task of crawling since the predictor has to work in a dynamic learning environment. FTRL-Proximal <ref type="bibr" target="#b24">[McMahan et al., 2013;</ref><ref type="bibr" target="#b26">McMahan, 2011]</ref> developed by Google has been proven to work well on the massive online learning problems. FTRLproximal outperforms the other online learning algorithms in terms of accuracy and sparsity, and has been widely used in industry, e.g., recommender systems and advertisement systems. We adopt this learning algorithm in our work.</p><p>We use x t to denote the feature vector of URI u t and y t ∈ {0, 1} the true class label of u t . Given a sequence of URIs</p><formula xml:id="formula_6">u 1 , u 2 , • • • , u r , • • • , u t ,</formula><p>the process of online prediction based on FTRL-Proximal algorithm is shown in Algorithm 1. We adapted the original algorithm to make it output binary values by setting a decision threshold τ . If the predicted probability p t is greater than the decision threshold τ ∈ [0, 1], it outputs prediction ŷt = 1; otherwise ŷt = 0.</p><p>At round t + 1, the FTRL-Proximal algorithm uses the update formula (1):</p><formula xml:id="formula_7">w t+1 = argmin w ( t s=1 w • g s + 1 2 t s=1 σ s w -w s 2 2</formula><p>+ λ 1 w 1 ).</p><p>(1) In equation ( <ref type="formula">1</ref>), w t+1 is the target model parameters to be updated in each round. In the first item of equation (1), g s is the gradient of loss function for training instance s. The second item of equation ( <ref type="formula">1</ref>) is a smoothing term which aims to speed up convergence and improve accuracy, and σ s is a non-increasing learning rate defined as</p><formula xml:id="formula_8">t s=1 σ s = √ t.</formula><p>The third item of equation ( <ref type="formula">1</ref>) is a convex regularization term used to prevent over-fitting and induce sparsity. Subsampling. Not all URIs are considered as training instances since we are interested in hard URIs. We exclude from training URIs with the extensions such as *.rdf/owl. Inversely, URIs with the file extension *.html/htm are included in the training set since they may contain RDFa data. The true class label y t is required to update the predictor online. In our scenario, to observe the true class label of a URI we have to download it and check whether it contains RDF data or not. We cannot afford to download all URIs because of the network overhead, and our target is to build a prediction model that avoids downloading unnecessary URIs. There, an appropriate subsampling strategy is needed. We found that the positive URIs are rare and relatively more valuable. For each round, if URI u t is predicted positive namely ŷt = 1, we retrieve the content of u t and observe the real class label y t . Then the predictor can be updated by new training instance (y t , x t ). For those URIs predicted negative, we only select a fraction ∈]0, 1] of them to download and observe their true class label. Here is a balance between online prediction precision (which requires as many URIs as possible for online training) and downloading overhead. To deal with the bias of this subsampled data, we assign an importance weight 1 to these examples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Empirical Evaluation</head><p>In the first experiment, we evaluate the predictive ability of different combinations of feature sets introduced in Section 3.2 by using offline classifiers including SVM, KNN and Naive Bayes. The dataset including 103K URIs and 9,825 different hosts is generated by operating a Breadth-First Search (BFS) crawl. As described in Section 3.2, the feature sets include F +t derived from the target URI u t , F +r derived from the referring URI u r , F +p derived from the direct properties of u t and F +x derived from the siblings of u t .</p><p>In Figure <ref type="figure" target="#fig_1">2</ref>(a) we report the results for the 4-element combination of feature sets ( F +t+r+p+x ), all 3-element combinations and the 2-element combinations ( F +t+x , F r+p , F +p+x ) which have the best performance in their class. Among all combinations, the 3-element combination F +t+r+p outperforms the other combinations with F-measure 0.8216 and accuracy 0.7902. The 4-element combination F +t+r+p+x as the second best combination scores F-measure 0.7944 and accuracy 0.7722. We found that augmenting sibling features to F +t+r+p is not helpful to improve the performance in the cases of three classifiers. We also found that the performance of F +r+p+x which is derived by excluding feature set F +t from F +t+r+p+x decreases a lot (the worst in all 3-element combinations) compared to the performance of F +t+r+p+x . It indicates that the features from target URI u t are important.</p><p>In the next experiment, we evaluate the performance of the proposed online prediction method against several baseline methods. We implemented the proposed crawler denoted by LDCOC (Linked Data Crawler with Online Classifier). The implementation detail of LDCOC is provided in <ref type="bibr" target="#b18">[Huang and Gandon, 2019]</ref>. The decision threshold τ in Algorithm 1 is set to 0.5 and the parameter of LDCOC is set to 0.17 according to the ratio of the number of positive URIs to the number of negative URIs in the previous training set. We also implemented three crawlers with offline classifiers including SVM, KNN and Naive Bayes to select URIs. The classifiers are pre-trained with two training sets (with size 20K and 40K). The BFS crawler is another baseline to be compared to. As suggested above, we use the feature set F +t+r+p for the experiment. Figure <ref type="figure" target="#fig_1">2</ref>(b) shows the percentage of retrieved RDF relevant URIs by different crawlers after crawling 300K URIs. Since the Linked Data Web is a dynamic environment, the crawlers with offline classifiers pre-trained by a small size training set would not improve performance a lot and our proposed crawler LDCOC outperforms these crawlers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We have presented a solution to learn URI selection criteria to improve the crawling of Linked Open Data. The method is able to predict whether a newly discovered URI contains RDF content or not by extracting features from several sources and building a prediction model based on FTRL-proximal learning algorithm. The experimental results demonstrate that the coverage of the crawl is improved compared to baseline methods. Our method can also be generalized to crawl other kinds of Linked Data such as JSON-LD, Microdata, etc.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Example of a URI with component parts.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: (a) Performance of the combinations of feature sets; (b) Percentage of retrieved RDF relevant URIs by different crawlers.</figDesc><graphic coords="5,54.00,54.00,496.06,129.12" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>do 2 get feature vector x t of u t ; 3 probability p t = sigmoid(x t • w t );</figDesc><table><row><cell>4</cell><cell>if p t &gt; τ then</cell></row><row><cell>5</cell><cell>output ŷt =1;</cell></row><row><cell>6</cell><cell>else</cell></row><row><cell>7</cell><cell>output ŷt =0;</cell></row><row><cell>8</cell><cell>end</cell></row><row><cell>9</cell><cell>observe real label y t ∈ {0, 1};</cell></row><row><cell>10</cell><cell>update w t+1 by equation (1);</cell></row><row><cell cols="2">11 end</cell></row></table></figure>
		</body>
		<back>

			<div type="funding">
<div> <ref type="bibr" target="#b18">Huang and Gandon, 2019</ref><p>] that won the best paper award at ESWC-2019. This work is supported by the <rs type="projectName">ANSWER</rs> project <rs type="grantNumber">PIA FSN2 N • P159564-2661789/DOS0060094</rs> between <rs type="funder">Inria</rs> and Qwant.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_gymesbR">
					<idno type="grant-number">PIA FSN2 N • P159564-2661789/DOS0060094</idno>
					<orgName type="project" subtype="full">ANSWER</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName><surname>Berners-Lee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Linked data -design issues</title>
		<author>
			<persName><forename type="first">Tim</forename><surname>Berners-Lee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName><surname>Chakrabarti</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Focused crawling: A new approach to topic-specific web resource discovery</title>
		<author>
			<persName><forename type="first">Soumen</forename><surname>Chakrabarti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Byron</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><surname>Dom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Networks</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">11-16</biblScope>
			<biblScope unit="page" from="1623" to="1640" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title/>
		<author>
			<persName><surname>Diligenti</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Focused crawling using context graphs</title>
		<author>
			<persName><forename type="first">Michelangelo</forename><surname>Diligenti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frans</forename><surname>Coetzee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steve</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">Lee</forename><surname>Giles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Gori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">VLDB</title>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="527" to="534" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title/>
		<author>
			<persName><surname>Dodds</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Slug : A Semantic Web Crawler</title>
		<author>
			<persName><forename type="first">Leigh</forename><surname>Dodds</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title/>
		<author>
			<persName><surname>Ermilov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Lodstats: The data web census dataset</title>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Ermilov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jens</forename><surname>Lehmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sören</forename><surname>Auer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Semantic Web Conference</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<imprint>
			<date type="published" when="2016">9982. 2016</date>
			<biblScope unit="page" from="38" to="46" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title/>
		<author>
			<persName><surname>Färber</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Linked data quality of dbpedia, freebase, opencyc, wikidata, and YAGO</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Färber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frederic</forename><surname>Bartscherer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carsten</forename><surname>Menne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Achim</forename><surname>Rettinger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Semantic Web</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="77" to="129" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Bizer</forename><surname>Heath</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Linked Data: Evolving the Web Into a Global Data Space</title>
		<author>
			<persName><forename type="first">Tom</forename><surname>Heath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Bizer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
			<publisher>Morgan &amp; Claypool Publishers</publisher>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title/>
		<author>
			<persName><surname>Hogan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Weaving the pedantic web</title>
		<author>
			<persName><forename type="first">Aidan</forename><surname>Hogan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Harth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandre</forename><surname>Passant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Decker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Axel</forename><surname>Polleres</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">LDOW</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title/>
		<author>
			<persName><surname>Hogan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Searching and browsing linked data with SWSE: the semantic web search engine</title>
		<author>
			<persName><forename type="first">Aidan</forename><surname>Hogan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Harth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jürgen</forename><surname>Umbrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sheila</forename><surname>Kinsella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Axel</forename><surname>Polleres</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Decker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Web Sem</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="365" to="401" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Gandon</forename><surname>Huang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning URI selection criteria to improve the crawling of linked open data</title>
		<author>
			<persName><forename type="first">Hai</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabien</forename><surname>Gandon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ESWC</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="194" to="208" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title/>
		<author>
			<persName><surname>Isele</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Ldspider: An open-source crawling framework for the web of linked data</title>
		<author>
			<persName><forename type="first">Robert</forename><surname>Isele</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jürgen</forename><surname>Umbrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Bizer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Harth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ISWC 2010 Posters &amp; Demonstrations Track</title>
		<meeting>the ISWC 2010 Posters &amp; Demonstrations Track</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title/>
		<author>
			<persName><surname>Levenshtein</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1966">1966</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Binary codes capable of correcting deletions, insertions and reversals</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">I</forename><surname>Levenshtein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sov. Phys. Dokl</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="707" to="710" />
			<date type="published" when="1966">1966</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title/>
		<author>
			<persName><surname>Mcmahan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Ad click prediction: a view from the trenches</title>
		<author>
			<persName><forename type="first">H</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Brendan</forename><surname>Mcmahan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gary</forename><surname>Holt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Sculley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dietmar</forename><surname>Ebner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Grady</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lan</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Todd</forename><surname>Phillips</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eugene</forename><surname>Davydov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Golovin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharat</forename><surname>Chikkerur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Wattenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arnar</forename><surname>Mar Hrafnkelsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Boulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeremy</forename><surname>Kubica</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1222" to="1230" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title/>
		<author>
			<persName><surname>Mcmahan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Follow-theregularized-leader and mirror descent: Equivalence theorems and L1 regularization</title>
		<author>
			<persName><forename type="first">H</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Brendan</forename><surname>Mcmahan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AISTATS</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="525" to="533" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title/>
		<author>
			<persName><surname>Meusel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Focused crawling for structured data</title>
		<author>
			<persName><forename type="first">Robert</forename><surname>Meusel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Mika</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roi</forename><surname>Blanco</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1039" to="1048" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title/>
		<author>
			<persName><surname>Umbrich</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Four heuristics to guide structured content crawling</title>
		<author>
			<persName><forename type="first">Jürgen</forename><surname>Umbrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Harth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><surname>Hogan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Decker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICWE</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="196" to="202" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title/>
		<author>
			<persName><surname>Weinberger</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Feature hashing for large scale multitask learning</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Kilian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anirban</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Dasgupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">J</forename><surname>Langford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Josh</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName><surname>Attenberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="1113" to="1120" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
