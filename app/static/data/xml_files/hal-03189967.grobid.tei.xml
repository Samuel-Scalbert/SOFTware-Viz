<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">CS-ARF: Compressed Adaptive Random Forests for Evolving Data Stream Classification</title>
				<funder ref="#_yasHMuZ">
					<orgName type="full">Labex DigiCosme</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Maroua</forename><surname>Bahri</surname></persName>
							<email>maroua.bahri@telecom-paris.fr</email>
						</author>
						<author>
							<persName><forename type="first">Heitor</forename><forename type="middle">Murilo</forename><surname>Gomes</surname></persName>
							<email>heitor.gomes@waikato.ac.nz</email>
						</author>
						<author>
							<persName><forename type="first">Albert</forename><surname>Bifet</surname></persName>
							<email>albert.bifet@telecom-paris.fr</email>
						</author>
						<author>
							<persName><forename type="first">Silviu</forename><surname>Maniu</surname></persName>
							<email>silviu.maniu@lri.fr</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="laboratory">LTCI</orgName>
								<orgName type="institution" key="instit1">Télécom Paris</orgName>
								<orgName type="institution" key="instit2">IP</orgName>
								<address>
									<settlement>-Paris Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">LRI</orgName>
								<orgName type="institution" key="instit2">Université Paris-Sud Orsay</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="laboratory">LTCI</orgName>
								<orgName type="institution" key="instit1">University of Waikato Hamilton</orgName>
								<orgName type="institution" key="instit2">Télécom Paris</orgName>
								<orgName type="institution" key="instit3">IP</orgName>
								<address>
									<settlement>-Paris Paris</settlement>
									<region>New Zealand</region>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="laboratory">LTCI</orgName>
								<orgName type="institution" key="instit1">University of Waikato Hamilton</orgName>
								<orgName type="institution" key="instit2">Télécom Paris</orgName>
								<orgName type="institution" key="instit3">IP</orgName>
								<address>
									<settlement>-Paris Paris</settlement>
									<region>New Zealand</region>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="laboratory">LRI</orgName>
								<orgName type="institution" key="instit1">CNRS</orgName>
								<orgName type="institution" key="instit2">Université Paris-Sud</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="laboratory">ENS DI</orgName>
								<orgName type="institution">Université Paris-Saclay Orsay</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff6">
								<orgName type="institution" key="instit1">ENS</orgName>
								<orgName type="institution" key="instit2">Université PSL Paris</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">CS-ARF: Compressed Adaptive Random Forests for Evolving Data Stream Classification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">6EA6D721B5412342BFEE6E3A49FA0E58</idno>
					<idno type="DOI">10.1109/IJCNN48605.2020.9207188</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-04-12T14:48+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Data stream mining</term>
					<term>compressed sensing</term>
					<term>ensemble learning</term>
					<term>adaptive random forests</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Ensemble-based methods are one of the most often used methods in the classification task that have been adapted to the stream setting because of their high learning performance achievement. For instance, Adaptive Random Forests (ARF) is a recent ensemble method for evolving data streams that proved to be of a good predictive performance but, as all ensemble methods, it suffers from a severe drawback related to the high computational demand which prevents it from being efficient and further exacerbates with high-dimensional data. In this context, the application of a dimensionality reduction technique is crucial while processing the Internet of Things (IoT) data stream with ultrahigh dimensionality. In this paper, we aim to alleviate this deficiency and improve ARF performance, so we introduce the CS-ARF approach that uses Compressed Sensing (CS) as an internal pre-processing task, to reduce the dimensionality of data before starting the learning process, that will potentially lead to a meaningful improvement in memory usage. Experiments on various datasets show the high classification performance of our CS-ARF approach compared against current state-of-the-art methods while reducing resource usage.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Mining Internet of Things (IoT) data streams is a very attractive field that has gained popularity and attracted the attention of the data mining community for the last few years <ref type="bibr" target="#b0">[1]</ref>. In the IoT era, there has been a lot of interest in data arriving in the form of continuous and infinite data streams. In fact, many applications generate indefinitely, at high rates, massive streams of data that need to be processed in an incremental fashion requiring real-time processing systems. Due to the enormous volume of data generated daily and the storage limitation, there is an information overload in most sciences. Experimental life sciences in different domains such as social networks, call records, text mining, and more.</p><p>Streaming classification is an active area of research in data mining field. This task for data streams is similar to the batch classification where both operate in order to predict the class labels of new incoming unlabeled instances composed by vectors of attributes. The stream classification processes instances from the stream while updating continuously, after prediction, the models as the stream emerges to follow the current distribution of the data. Traditional-or batchclassification algorithms have been proved to be of limited effectiveness under streaming environments, so a variety of algorithms have been proposed to cope with the evolving data stream challenges <ref type="bibr" target="#b1">[2]</ref>- <ref type="bibr" target="#b7">[8]</ref>. Moreover, ensemble learning is receiving increased attention for data stream learning to improve learning performance <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>. Unlike single classifiers, ensemble-based methods predict by combining the predictions of several classifiers. Several empirical and theoretical studies have shown the reasoning that combining multiple "weak" individual classifiers leads to better predictive performance than a single classifier <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>. The difference between batch and data stream classifiers resides in the way how the learning and prediction are performed. Moreover, the stream setting raises several challenges related primarily to the resource constraints because of the unbounded size of the evolving streams. Unlike learning algorithms for static datasets, data streams algorithms must process data incrementally using the one-pass processing.</p><p>Handling high-dimensional data has become a big challenge since many domains <ref type="bibr" target="#b12">[13]</ref>, such as biology, social media, spam email filters and so forth, generate data with ultrahigh dimensionality that need to be processed in a stream fashion considering their online nature. Nevertheless, other than their sensitivity to the learning algorithm used as a base learner, most of the existing stream ensemble-based methods are often expensive and time-consuming when dealing with sparse and high-dimensional data streams <ref type="bibr" target="#b1">[2]</ref>. Despite their good classification performance, the major drawback of ensembles is the high computational cost that exacerbates gradually with the dimensionality of the data.</p><p>In a recent work <ref type="bibr" target="#b8">[9]</ref>, the adaptive random forests method (ARF) was proposed to deal with evolving data streams by extending the random forest algorithm (RF) using a concept drift mechanism to deal with changes in the distribution over time and decide when to change an obsolete tree with a new one inside the ensemble. However, it appears that ARF is effective (in terms of accuracy) but inefficient (in terms of resource usage) with high-dimensional data streams.</p><p>In attempt to improve the performance of the ARF method, we propose the compressed adaptive random forests (CS-ARF); an ensemble-based method that extends the ARF <ref type="bibr" target="#b8">[9]</ref> to handle high-dimensional and sparse data streams. To do so, we incorporate a dimensionality reduction technique, Compressed Sensing (CS)-also called Compressing Sampling <ref type="bibr" target="#b13">[14]</ref>, to project the data into a lower-dimensional space by removing redundancy and finding useful combinations of existing features. Therefore, instead of building trees using highdimensional instances, we will use a smaller representation of these instances that will boost the efficiency of the CS-ARF approach. The main contributions of this paper are the following:</p><p>• Compressed-Adaptive Random Forests (CS-ARF): a new ensemble method to support high-dimensional data streams classification. We aim to enhance the resource usage of the ARF method by compressing the input data, using CS internally, and then fed them to the ensemble members which are built upon different CS independent matrices; • Empirical results: we compare our novel approach against several popular algorithms from the literature using a various set of datasets. Results show that our method obtains a good trade-off among the three axes (accuracy, memory and time). This paper is organized as follows. Section II reviews the prominent related work. In Section III, we introduce basics about compressed sensing followed by its application in conjunction with the ARF algorithm. Section IV presents and discusses the experimental results, in terms of accuracy and resource usage, performed on synthetic and real-world datasets. We finally draw concluding remarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>There are several algorithms in the literature that address the classification task in the streaming framework. For instance, naive Bayes <ref type="bibr" target="#b14">[15]</ref> which uses the assumption that the attributes are all independent of each other and w.r.t. the class label uses Bayes's theorem to compute the posterior probability of a class given the training data. The k-Nearest Neighbors (kNN) is another algorithm that has been adapted to the data stream setting. It does not require any work during training but it uses the entire dataset to predict the class labels for test examples. The challenge with adapting kNN to the stream setting is that it is not possible to store the entire stream for the prediction phase <ref type="bibr" target="#b15">[16]</ref>. An envisaged solution to solve this issue is to manage the examples that are remembered so that they fit into limited memory (window) and to merge new observations with the closest ones already in the window. Another new kNN approach that has been proposed recently is Self-Adjusting Memory kNN (samkNN) <ref type="bibr" target="#b7">[8]</ref>. SamkNN uses a dual-memory model to capture drifts in data streams by building an ensemble with models targeting current or former concepts. Several tree-based algorithms have been proposed to handle evolving data streams <ref type="bibr" target="#b4">[5]</ref>- <ref type="bibr" target="#b6">[7]</ref>. A well-known decision tree learner is the Hoeffding Adaptive Tree (HAT) <ref type="bibr" target="#b2">[3]</ref> which extends Hoeffding decision trees <ref type="bibr" target="#b4">[5]</ref> to deal with concept drifts by incorporating the ADaptive WINdowing (ADWIN) algorithm <ref type="bibr" target="#b16">[17]</ref>, to monitor the performance of branches on the tree and replace them with new branches when their accuracy decreases.</p><p>In order to achieve higher predictive performance than single classifiers, one could use ensembles. This category of classifiers has been widely studied and often used when dealing with evolving data streams because, other than improving the accuracy by aggregating the predictions of several weak learners (the ensemble members), it is applied to handle concept drifts by resetting or updating current models for each ensemble member <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b17">[18]</ref>. An extensive review about the related work is provided in <ref type="bibr" target="#b17">[18]</ref>. Among those algorithms, we represent briefly the well-known ones. Leveraging Bagging (LB) <ref type="bibr" target="#b18">[19]</ref> which is a streaming version of Bagging <ref type="bibr" target="#b10">[11]</ref> that handles drifts using ADWIN <ref type="bibr" target="#b16">[17]</ref>, where if a change is detected, the worst classifier is erased and a new one is added to the ensemble. LB also induces more diversity to the ensemble via randomization. Adaptive Random Forests (ARF) <ref type="bibr" target="#b8">[9]</ref> is a recent ensemble method that uses Hoeffding tree as a base learner and a drift detection scheme where we replace a tree once a drift is detected. Streaming Random Patches (SRP) <ref type="bibr" target="#b19">[20]</ref> is also a novel ensemble method that combines random subspaces and bagging while using a strategy to detect drifts similar to the one introduced in ARF <ref type="bibr" target="#b8">[9]</ref>.</p><p>One notable issue related to the ensemble-based methods with evolving data streams is the massive computational demand (in terms of memory usage and running time). Ensembles require more resources than single classifiers which become significantly worse with high-dimensional data streams. To cope with this problem without importantly affecting the predictive performance of the ARF method, we need to incorporate an efficient dimensionality reduction technique that can internally transform high-dimensional data into a lower space before the learning task.</p><p>Feature transformation, also known as feature extraction, plays a critical role when dealing with high-dimensional data and is often used in data mining and machine learning. This task consists on extracting a subset of relevant features (in low-dimensional space) from a set of input features in highdimensional space <ref type="bibr" target="#b20">[21]</ref>. This pre-processing step provides potential benefits to stream mining algorithms, such as re-ducing the storage usage, decreasing the processing time, and enhancing-or not losing much in-the prediction performance.</p><p>In this context, a well-known technique has been proposed, Compressed Sensing (CS) <ref type="bibr" target="#b13">[14]</ref>, also called Compressed Sampling, that deals with redundancy while transforming and reconstructing data. The basic idea is to use orthogonal features or samples, i.e. complementary features, to provably and properly represent data as well as reconstruct them from a small number of samples. More details about the basic notions of CS are available in the following.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. COMPRESSED ADAPTIVE RANDOM FORESTS</head><p>Ensemble-based methods have recently attracted a lot of attention in the machine learning community thanks to their high predictive performance <ref type="bibr" target="#b17">[18]</ref>. Since ensembles combine several single classifiers however, their resource usage is accordingly huge in comparison to one classifier, which makes them inappropriate in high-dimensional contexts. To address this weakness, we use an efficient feature transformation technique, such as compressed sensing, to reduce the dimensionality of data for the following learning task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Notation</head><p>In the following, we assume a data stream S is a sequence of instances X 1 , X 2 , . . . , X N , • • • , where N denotes the number of available observations so far. Each instance X i is composed of a vector of d features (x 1 i , . . . , x d i ). The feature transformation comprises the process of finding some a mapping function A : R d → R p , where p d, to be applied on each instance X from the stream.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Compressed Sensing</head><p>CS has been firstly proposed for signal processing <ref type="bibr" target="#b13">[14]</ref> to efficiently compress and reconstruct a signal. It has been thoroughly studied and used in different domains with the offline setting, such as image processing <ref type="bibr" target="#b21">[22]</ref>, face recognition <ref type="bibr" target="#b22">[23]</ref>, and vehicle classification <ref type="bibr" target="#b23">[24]</ref>. The principle of CS is based on the exploitation of the sparsity of high-dimensional data to recover them from a small set of features. Given a sparse vector X ∈ R d , CS measures Y ∈ R p as follows:</p><formula xml:id="formula_0">Y = AX,<label>(1)</label></formula><p>where p d and A ∈ R p×d is called measurementsampling or sensingmatrix. This matrix is used to assure to transformation from high-dimensional space to a lower dimensional one.</p><p>Three basic principles under which CS enables the data recovery from a small set of possibly noisy features with high probability:</p><p>• Sparsity: the sparsity of data in some basis can be exploited for compression by keeping the non-zero values and removing irrelevant features without much loss. Given an instance X = {x 1 , . . . , x d }, X is said to be s-sparse if X 0 = |{j : x j = 0}| ≤ s.</p><p>• Restricted Isometry Property (RIP): A satisfies the RIP for all s-sparse instance X if there exists ∈ [0, 1] such that:</p><formula xml:id="formula_1">(1 -) X 2 2 ≤ AX 2 2 ≤ (1 + ) X 2 2 .<label>(2)</label></formula><p>It has been proved that a matrix A will satisfy the RIP in CS with high probability if p = O(s log(d)) <ref type="bibr" target="#b13">[14]</ref>. • Incoherence: this principle is applied through the RIP by measuring similarities and capturing the correlation between any two columns between features of sparse data. The coherence measures the largest similarity between any columns of data (all the values for a given dimension). CS aims to identify low coherence pairs, characterizing the dependence between columns, to capture enough information for reconstruction purposes <ref type="bibr" target="#b24">[25]</ref>. Two related properties have been pointed out for the characterization of the sensing matrix, the largest coherence and the RIP. The latter is both a necessary and a sufficient condition in data reconstruction and the randomization is a key component in the construction of the sampling matrix <ref type="bibr" target="#b25">[26]</ref>. Two well-known random sensing matrices, that honor the RIP with high probability and are prominent thanks to their simplicity, are used in CS: (i) Gaussian random matrix, which is generated from a Gaussian distribution having entries with zero mean and variance equals to 1 ((A i,j ∼ N (0, 1)); and (ii) Bernoulli random matrix, which is generated from a Bernoulli distribution taking values 1 or -1 with equal probability (A i,j ∈ {1/ √ p, -1/ √ p}). These matrices are universal and can be applied to any sparse data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. CS-ARF Approach</head><p>Random forest algorithm <ref type="bibr" target="#b26">[27]</ref> is widely used in the batch learning classification. It grows several trees while randomly selecting features at each split node from an entire set of input features. Nonetheless, this is inapplicable on evolving data streams because random forest algorithm performs multiple passes to establish bootstraps which is inappropriate in the streaming framework. For this to happen, an adaptive random forests (ARF) algorithm <ref type="bibr" target="#b8">[9]</ref> has been proposed to adapt random forest to work under the streaming setting. This adaptation includes the use of: (i) an online bootstrap process to approximate the original data explained in <ref type="bibr" target="#b8">[9]</ref>; and (ii) a random subset of features to limit the size of input set during each leaf split. To cope with concept drifts, ARF method includes a warning and drift detection operators in order to adapt to changes in the data distribution over time which will lead to a superior classification performance.</p><p>One major drawback of ensemble-based methods despite their good classification performance, and particularly the ARF method, is the important amount of computational resources needed to deal with high-dimensional data streams. However, this kind of data solicits additional resources that could be avoided using a feature extraction technique. The main idea to mitigate this curse of dimensionality and improve the resource usage of the ARF method is to use an efficient technique with relevant properties, such as compressed sensing (CS) <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b27">[28]</ref>.</p><p>In this vein, we propose our novel approach Compressed Adaptive Random Forests, denoted CS-ARF in the following, that combines the simplicity of compressed sensing and the high learning performance of the reputed ARF method for evolving data streams. Given an infinite stream of highdimensional instances X ∈ R d , we wish to construct a lowdimensional representation Y ∈ R p , where p d and Y is the dense representation of X after the application of the dimensionality reduction using CS projection.</p><p>We assume that all the instances X in the stream S are s-sparse to adhere to the CS requirements and use a RIP matrix in order to transform data into lower dimensional space of O(s log(d)) <ref type="bibr" target="#b13">[14]</ref>. This compression space size is easy to obtain, since it depends on the size of the input features, which makes it convenient for applications in the streaming context where the number of instances N is unknown. CS is also different from random projection which satisfies the Johnson-Lindenstrauss (JL) lemma <ref type="bibr" target="#b28">[29]</ref> asserting that N instances from an Euclidean space can be projected into a lower dimensional space of O(log N/ 2 ) dimensions.</p><p>Fundamentally, CS is composed of two main phases: (i) the compression phase, during which occurs the projection of the high-dimensional data into a smaller dimensional space; and (ii) the decompression phase, where the data are recovered from their low-dimensional representation. To assure the recovery of the original data with high probability, avoid the information loss that may occur under this transformation and make it minimal, we must use a sensing matrix that respects the restricted isometry property of the CS technique.</p><p>In this work, we are only concerned by the compression phase that will alleviate the need of resources in the ARF classification task while dealing with high-dimensional streams. So, for each tree inside our ensemble approach, CS-ARF, we apply a pre-processing step consisting in the CS transformation on every incoming instance via solving <ref type="bibr" target="#b0">(1)</ref>. Therefore, the lowdimensional representation of the current instance will be fed to the underlying ARF ensemble member for prediction and then used to update the corresponding model.</p><p>For the purpose of obtaining sufficiently good-or with minor loss in-accuracy and reducing the use computational resources, we need perform projection using an effective sensing matrix A that respects the RIP for the CS application. In this regard, recent studies <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b30">[31]</ref> assessed the performance of different sensing matrices that satisfy the restricted isometry property with high probability and showed that compressed sensing, using Gaussian random matrices, achieves good results in comparison with other sensing matrices. In the light of this, we focus on using Gaussian random matrices because of their simplicity and data-independent nature, which is suitable to the evolving data streams. We do not need the instances from the stream to achieve the projection of highdimensional data. Instead, we build the sensing matrix A such that its elements are independently generated from a Gaussian distribution A i,j ∼ N (0, 1).</p><p>The main novelty of our approach is in how we internally couple the CS technique with the ARF method to deal with evolving data streams. In fact, we use several CS matrices by generating a different Gaussian matrix for each tree in order to promote diversity inside the ensemble and lose as little as possible in terms predictive performance. Thus, each ensemble member in our CS-ARF approach will be preceded by a dimensionality reduction step that uses a different sensing matrix. Therefore, models-or trees-are going to be different inside the ensemble method CS-ARF because of: (i) the randomization due to the generation of different Gaussian matrices; and (ii) the construction of the trees by using random subsets of features for node splits. for all X ∈ S do ĉ ← predict(t, y) T reeT rain(m, b, Y )</p><p>Algorithm 1 shows the pseudo-code of the proposed CS-ARF approach. As explained previously, for each ensemble member t, we apply the CS transformation by generating a Gaussian random matrix gm (different from the ones generated for the rest of the ensemble members) and therefore represent the current instance using e low-dimensional representation to fed it to each of the e trees (lines 6 -8), instead of feeding the high-dimensional instance X. Then, we predict the class label for the current compressed dense instance Y ∈ R p (line 9) before using it to train the trees (line 10). For more details about the tree training task and how trees are updated, we redirect readers to the work of Gomes et al. <ref type="bibr" target="#b8">[9]</ref>. To cope with concept drifts, the ARF method includes a warning and drift detection mechanisms, where once a warning is detected for an ensemble member, a background tree is created (lines 11-13). This tree will be replaced by its corresponding background tree if this warning signal becomes a drift (lines 14 -15).</p><p>IV. EXPERIMENTS In this section, we assess the impact of compressed sensing on the novel ensemble-based method, adaptive random forests for evolving data streams. To do so, we conduct several experiments to evaluate the performance of the CS-ARF approach. Hence, we aim to find the best trade-off over three main axes: (i) the classification accuracy, i.e., the proportion of correctly predicted instances; (ii) the memory usage (megabytes), which is the cost of maintaining the current models of our ensemble in the main memory; and (iii) the overall running time, which comprises the data transformation, prediction, and learning. All such aspects are strongly related, so that the drastic reduction of time and space complexities would make our approach much faster than using all features. Of course, one should weigh the classification performance while assessing all those factors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Data</head><p>We use 4 synthetic and 5 real datasets that have been thoroughly used in the literature to evaluate the performance of stream classifiers. Table I presents a short description of each dataset, further details are provided in what follows.</p><p>Tweets. Tweets was created using the tweets text data generator provided by MOA <ref type="bibr" target="#b31">[32]</ref> that simulates sentiment analysis on tweets, where messages can be classified into two categories depending on whether they convey positive or negative feelings. Tweets 1 , Tweets 2 , and Tweets 3 produce 1, 000, 000 instances of 500, 1, 000, and 1, 500 features, respectively.</p><p>RBF. The Radial Basis Function (RBF) generator provided also by MOA. It creates centroids at random positions, and each one has a standard deviation, a weight and a class label. This dataset simulates drift by moving the centroids with constant speed.</p><p>Enron. The Enron corpus dataset is a large set of email messages that was made public during the legal investigation concerning the Enron corporation <ref type="bibr" target="#b32">[33]</ref>. This cleaned version of Enron consists of 1, 702 instances and 1, 000 features.</p><p>IMDB. IMDB 1 movie reviews dataset was first proposed for sentiment analysis <ref type="bibr" target="#b33">[34]</ref>, where reviews have been pre-1 http://waikato.github.io/meka/datasets/ processed, and each review is encoded as a sequence of word indexes (integers).</p><p>Nomao. Nomao <ref type="bibr" target="#b34">[35]</ref> is a large dataset that has been provided by Nomao Labs. It contains data coming from several sources on the web about places (name, website, address, localization, fax, etc• • • ).</p><p>Har. Human Activity Recognition dataset <ref type="bibr" target="#b35">[36]</ref> built from several subjects performing daily living activities, such as walking upstairs/downstairs, sitting, standing and laying, while wearing a waist-mounted smartphone equipped with sensors. The sensor signals were pre-processed using noise filters and attributes were normalized and bounded within [-1, 1].</p><p>ADS. Advertisements dataset 2 is a set of possible advertisements on internet pages, where each row represents one image tagged as ad or nonad (which are the class labels).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Results and Discussions</head><p>The experiments were implemented and evaluated in Java by extending the MOA framework <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b31">[32]</ref> using the datasets described above. We use the online learning setting for Interleaved Test-Then-Train method <ref type="bibr" target="#b36">[37]</ref> for evaluation, where each instance is provided as input for testing the model and, right after, given as training input to adapt the learning model. For a fair comparison, we evaluate the CS-ARF approach against state-of-the-art classifiers coupled with compressed sensing as a filter, where we use one CS matrix for dimensionality reduction with all the ensemble members. For the state-of-theart classification comparison, we use Leveraging Bagging <ref type="bibr" target="#b18">[19]</ref> (LB cs ), Streaming Random Patches <ref type="bibr" target="#b19">[20]</ref> (SRP cs ), Hoeffding Adaptive Trees <ref type="bibr" target="#b2">[3]</ref> (HAT cs ), Self-Adjusting Memory kNN <ref type="bibr" target="#b7">[8]</ref> (SAMkNN cs ), and Naive Bayes <ref type="bibr" target="#b14">[15]</ref> (NB cs ) algorithms. We include single classifiers (HAT, SAMkNN, NB) in our comparison, because they are often used as baselines in the stream classification. It has been proved in <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b19">[20]</ref> that the ensemblebased methods, LB and SRP, are the best outperforming other ensemble classifiers using a similar set of datasets to the one used in this work.</p><p>Parameterization: we use k = 11 for number of neighbors in the SAMkNN algorithm. We use a similar configuration for the HAT algorithm and Hoeffding tree (HT)-the base learner for all the ensemble methods-with the grace period, the split confidence, and subspace size set to g = 50, c = 0.01, and m = 80%, respectively. To cope with drifts, the ensemble methods are coupled with the change detector and estimator ADWIN <ref type="bibr" target="#b16">[17]</ref> using the default parameters for; (i) the warning threshold δ w = 0.00001; and (ii) the drift threshold δ d = 0.0001 <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref>. For all the ensemble-based methods, the ensemble size is fixed to e = 30 learners.</p><p>Figure <ref type="figure" target="#fig_0">1</ref> presents the results of the CS-ARF approach, while applying a CS transformation over all the datasets into different space sizes <ref type="bibr" target="#b9">(10,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr">50,</ref><ref type="bibr">70,</ref><ref type="bibr">90)</ref>, and the vanilla ARF method, whilst using all the input features of the data stream without any projection (all on the X-axis). We notice that for almost all the datasets, the accuracy of our CS-ARF approach 2 https://www.kaggle.com/uciml/internet-advertisements-data-set is moderately affected while varying the output dimension p (Figure <ref type="figure" target="#fig_0">1</ref>(a)). It slightly improves when we increases the p, because we are using random subspaces from a dense set of features and not sparse ones (with many zeros). On the other hand, the ARF method using the original data (presented by all in the X-axis) somewhat outperforms the CS-ARF approach for almost all datasets. This behaviour is explained by the fact that when we use a dimensionality reduction technique we are removing features that may impact the accuracy of any classifier. In contrast, Figure <ref type="figure" target="#fig_0">1</ref>(b) illustrates the behavior of the memory usage which is different in the sense that vanilla ARF, using the entire data without projection (all), is more memory consuming than the CS-ARF approach. Figure <ref type="figure" target="#fig_0">1</ref>(c) depicts the CS-ARF processing time that increases with p and becomes slower than the ARF method. This is due to the fact that with the CS-ARF approach, we have the additional processing of the CS computation that increases when the CS matrix becomes larger. We highlight that this is an accuracyresource usage tradeoff, because for a low value of p, our approach is able to be as accurate as the ARF method while using much smaller computational resources. Moreover, the accuracy increases slightly when we increase the number of dimensions to reach the accuracy of the ARF method.</p><p>Figure <ref type="figure" target="#fig_3">2</ref> shows an accuracy comparison of the CS-ARF approach against reputed state-of-the-art algorithms, coupled with a compressed sensing filter, on the Tweet 1 dataset (this behavior is practically the same with the other datasets). We notice that our approach achieves consistently better accuracy than its competitors for different output dimensions. Single classifiers (HAT cs , SAMkNN cs , NB cs ) are less accurate than the ensemble-based methods because the latter combine the predictions of several single "weak" classifiers and are all coupled with drift detection techniques.</p><p>Due to the stochastic nature of the CS technique and therefore our CS-ARF approach, all the results reported in this paper are an average of several runs (with different random Gaussian matrices). Figure <ref type="figure" target="#fig_4">3</ref> depicts the standard deviation based on the accuracies obtained over several runs for different output dimensions using Tweet 3 and Har datasets (Figure <ref type="figure" target="#fig_4">3</ref>(a) and 3(b), respectively). For both datasets, our approach has a small standard deviation (too close to zero), i.e. for all the runs, the accuracies obtained are close to the mean reported in this paper. On the other hand, a larger standard deviation is obtained with the other algorithms showing that the classification accuracies obtained for the different runs are farther away from the mean. This difference is explained by the fact that the competitors use one CS matrix as an internal filter while our approach uses a different Gaussian matrix for each ensemble member. This strategy somewhat increases the diversity inside the ensemble and thus a better predictive performance is obtained, guaranteeing some close approximation (with a CS perturbation ) to the accuracy that would be obtained using the original stream. Based on these results, we use p = 50 in the following, because the standard deviation is minimal for most of the algorithms. The results presented in Table <ref type="table" target="#tab_2">II</ref> show the classification performance of the CS-ARF approach against other algorithms  for all datasets projected in a space of 50-dimensions using the compressed sensing technique. We note that the CS-ARF performs the best on most of the datasets and highlight the difference that is statistically insignificant when outperformed by other algorithms, as reported in <ref type="bibr" target="#b19">[20]</ref>.</p><p>To assess the benefits in terms of resources-where small values are desirable-Figure <ref type="figure" target="#fig_5">4</ref> shows the memory behavior for the ensemble-based methods. This figure depicts the large gains on almost all datasets of our approach, CS-ARF, which outperforms the LB cs and the SRP cs methods, confirming previous studies <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b19">[20]</ref> that reveal the high consumption of the LB. We also note that with small datasets, such as Enron and ADS, the CS-ARF does not achieve a prominent gain. Indeed, with large datasets our proposed approach is efficient which makes it highly convenient for high-dimensional data streams where the stream size is potentially infinite, which is not the case of the Enron and ADS datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSIONS</head><p>In this work, we presented the compressed adaptive random forests approach (CS-ARF) to enable the ARF method to be both efficient (in terms of resource usage) and effective (in terms of classification accuracy) with high-dimensional data streams. The CS-ARF approach combines the Compressed Sensing (CS) technique, given its ability to preserve pairwise distances within 1± -factor, in conjunction with the strength of the reputed ARF method, that achieves high predictive performance. Our proposed approach transforms high-dimensional data streams, using the CS technique as an internal online pre-processing step, afterwards it uses the obtained lowdimensional representation of data for the learning task using the ARF method. We evaluated and discussed the proposed method via extensive experiments using a diverse set of datasets. Results showed the ability of our approach to achieve good performance, close to what would be obtained using the original datasets without projections, and outperform well-known state-of-the-art algorithms. We also showed that, despite its stochastic nature, the CS-ARF approach achieves good stable accuracy, by extracting relevant features from sparse data in different low-dimensional spaces, while using feasible amount of resources.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Algorithm 1</head><label>1</label><figDesc>CS-ARF algorithm. Symbols: S ∈ R d : data stream; p: output dimension; e: ensemble size; m: maximum features evaluated per split; C: change detector; B: set of background trees; δ w : warning threshold; δ d : drift threshold. 1: function CS-ARF(p, e, m, δ w , δ d ) 2: T ← CreateT rees(e) 3: G ← GaussianM atrix(e, d, p) generate e random matrices 4:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>7 :</head><label>7</label><figDesc>for all t ∈ T and gm ∈ G do 8:y ← CS(x, p, gm)y is the projection of x into p-dimensions using CS 9:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 1 :</head><label>1</label><figDesc>Fig. 1: CS-ARF and ARF comparison: the CS-ARF while projecting into different dimensions (10, 30, 50, 70, 90); the ARF with the entire datasets (all on x-axis): (a) accuracy (%), (b) memory (MB), (c) running time (sec).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 2 :</head><label>2</label><figDesc>Fig. 2: Accuracy comparison over different output dimensions on Tweet1 dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 3 :</head><label>3</label><figDesc>Fig. 3: The standard deviation of the methods while projecting into different dimensions: (a) Tweet 3 , (b) Har datasets.</figDesc><graphic coords="8,356.78,131.61,164.55,77.27" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 4 :</head><label>4</label><figDesc>Fig. 4: Memory comparison of the ensemble-based methods on all the datasets while projecting into 50-dimensions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I :</head><label>I</label><figDesc>Overview of the datasets</figDesc><table><row><cell>Dataset</cell><cell>#Instances</cell><cell>#Attributes</cell><cell>#Classes</cell><cell>Type</cell></row><row><cell>Tweets 1</cell><cell>1,000,000</cell><cell>500</cell><cell>2</cell><cell>Synthetic</cell></row><row><cell>Tweets 2</cell><cell>1,000,000</cell><cell>1,000</cell><cell>2</cell><cell>Synthetic</cell></row><row><cell>Tweets 3</cell><cell>1,000,000</cell><cell>1,500</cell><cell>2</cell><cell>Synthetic</cell></row><row><cell>RBF</cell><cell>1,000,000</cell><cell>200</cell><cell>10</cell><cell>Synthetic</cell></row><row><cell>Enron</cell><cell>1,702</cell><cell>1,000</cell><cell>2</cell><cell>Real</cell></row><row><cell>IMDB</cell><cell>120,919</cell><cell>1,001</cell><cell>2</cell><cell>Real</cell></row><row><cell>Nomao</cell><cell>34,465</cell><cell>119</cell><cell>2</cell><cell>Real</cell></row><row><cell>Har</cell><cell>10,299</cell><cell>561</cell><cell>6</cell><cell>Real</cell></row><row><cell>ADS</cell><cell>3,279</cell><cell>1,558</cell><cell>2</cell><cell>Real</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE II :</head><label>II</label><figDesc>Accuracy (%) comparison of CS-ARF, LB cs , SRP cs , SAMkNN cs , and NB cs while projecting into 50dimensions.DatasetCS-ARF LB cs SRP cs HAT cs SkNN cs NB cs</figDesc><table><row><cell>Tweets 1</cell><cell>86.46</cell><cell>82.64 81.08</cell><cell>76.35</cell><cell>76.29</cell><cell>79.82</cell></row><row><cell>Tweets 2</cell><cell>85.53</cell><cell>81.88 80.93</cell><cell>76.69</cell><cell>74.06</cell><cell>79.48</cell></row><row><cell>Tweets 3</cell><cell>86.96</cell><cell>79.65 78.58</cell><cell>71.30</cell><cell>72.61</cell><cell>78.24</cell></row><row><cell>RBF</cell><cell>99.55</cell><cell>99.50 99.74</cell><cell>96.20</cell><cell>99.77</cell><cell>96.41</cell></row><row><cell>Enron</cell><cell>92.11</cell><cell>96.18 96.35</cell><cell>94.59</cell><cell>96.17</cell><cell>91.37</cell></row><row><cell>IMDB</cell><cell>74.90</cell><cell>74.86 74.87</cell><cell>74.04</cell><cell>74.55</cell><cell>74.27</cell></row><row><cell>Nomao</cell><cell>96.74</cell><cell>96.70 96.68</cell><cell>95.02</cell><cell>96.63</cell><cell>86.25</cell></row><row><cell>Har</cell><cell>88.14</cell><cell>88.61 88.65</cell><cell>80.22</cell><cell>82.07</cell><cell>81.72</cell></row><row><cell>ADS</cell><cell>98.25</cell><cell>99.74 99.81</cell><cell>98.71</cell><cell>98.52</cell><cell>89.48</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>ACKNOWLEDGEMENTS</head><p>This work was done in the context of <rs type="projectName">IoTA AAP Emergence DigiCosme Project</rs> and was funded by <rs type="funder">Labex DigiCosme</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_yasHMuZ">
					<orgName type="project" subtype="full">IoTA AAP Emergence DigiCosme Project</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Hand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Mannila</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Smyth</surname></persName>
		</author>
		<title level="m">Principles of data mining</title>
		<imprint>
			<publisher>MIT press</publisher>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A sketch-based naive bayes algorithms for evolving data streams</title>
		<author>
			<persName><forename type="first">M</forename><surname>Bahri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Maniu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bifet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Big Data</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="604" to="613" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Adaptive learning from evolving data streams</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bifet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Gavaldà</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Intelligent Data Analysis</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="249" to="260" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Bifet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Gavaldà</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Holmes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Pfahringer</surname></persName>
		</author>
		<title level="m">Machine learning for data streams: with practical examples in MOA</title>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Mining high-speed data streams</title>
		<author>
			<persName><forename type="first">P</forename><surname>Domingos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hulten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD. ACM</title>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="71" to="80" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Accurate decision trees for mining high-speed data streams</title>
		<author>
			<persName><forename type="first">J</forename><surname>Gama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Rocha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Medas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD. ACM</title>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="523" to="528" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Decision trees for mining data streams</title>
		<author>
			<persName><forename type="first">J</forename><surname>Gama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fernandes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Rocha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Intelligent Data Analysis</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="23" to="45" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Knn classifier with self adjusting memory for heterogeneous concept drift</title>
		<author>
			<persName><forename type="first">V</forename><surname>Losing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Hammer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wersing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDM. IEEE</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="291" to="300" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Adaptive random forests for evolving data stream classification</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">M</forename><surname>Gomes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bifet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Read</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Barddal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Enembreck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Pfharinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Holmes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Abdessalem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1" to="27" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Ensemble based systems in decision making</title>
		<author>
			<persName><forename type="first">R</forename><surname>Polikar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Circuits and Systems Magazine</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="21" to="45" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Bagging predictors</title>
		<author>
			<persName><forename type="first">L</forename><surname>Breiman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ML</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="123" to="140" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Ensemble methods in machine learning</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">G</forename><surname>Dietterich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Workshop on Multiple Classifier Systems</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="1" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Bellman</surname></persName>
		</author>
		<title level="m">Adaptive control processes: a guided tour</title>
		<imprint>
			<publisher>Princeton university press</publisher>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Compressed sensing</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Donoho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1289" to="1306" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Bayesian network classifiers</title>
		<author>
			<persName><forename type="first">N</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Goldszmidt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">2-3</biblScope>
			<biblScope unit="page" from="131" to="163" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Efficient data stream classification via probabilistic adaptive windows</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bifet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Pfahringer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Read</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Holmes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGAPP. ACM</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="801" to="806" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning from time-changing data with adaptive windowing</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bifet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Gavalda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIAM International Conference on Data Mining</title>
		<imprint>
			<publisher>SIAM</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="443" to="448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A survey on ensemble learning for data stream classification</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">M</forename><surname>Gomes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Barddal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Enembreck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bifet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM CSUR</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">23</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Leveraging bagging for evolving data streams</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bifet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Holmes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Pfahringer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Joint European conference on machine learning and knowledge discovery in databases</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="135" to="150" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Streaming random patches for evolving data stream classification</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">M</forename><surname>Gomes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Read</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bifet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDM. IEEE</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Feature extraction, construction and selection: A data mining perspective</title>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Motoda</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
			<publisher>Springer Science &amp; Business Media</publisher>
			<biblScope unit="volume">453</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Real-time dynamic mr image reconstruction using kalman filtered compressed sensing</title>
		<author>
			<persName><forename type="first">C</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Vaswani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP. IEEE</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="393" to="396" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Real-time visual tracking using compressive sensing</title>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR. IEEE</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1305" to="1312" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Vehicle classification using compressive sensing</title>
		<author>
			<persName><forename type="first">M</forename><surname>Uttarakumari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">V</forename><surname>Achary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">D</forename><surname>Badiger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Avinash</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mukherjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kothari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">RTEICT. IEEE</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="692" to="696" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">From sparse solutions of systems of equations to sparse modeling of signals and images</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Bruckstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Donoho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Elad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM review</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="34" to="81" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Compressed sensing and sparse filtering</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Carmi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Mihaylova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Godsill</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Random forests</title>
		<author>
			<persName><forename type="first">L</forename><surname>Breiman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="5" to="32" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Learning compressed sensing</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">S</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Snowbird Learning Workshop</title>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Extensions of lipschitz maps into banach spaces</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">B</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lindenstrauss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Schechtman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Israel Journal of Mathematics</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="129" to="138" />
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A performance comparison of measurement matrices in compressive sensing</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Arjoune</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kaabouch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">El</forename><surname>Ghazi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tamtaoui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Communication Systems</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page">3576</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Deterministic sensing matrices in compressive sensing: a survey</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">L</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Scientific World Journal</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Moa: Massive online analysis</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bifet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Holmes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kirkby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Pfahringer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="1601" to="1604" />
			<date type="published" when="2010-05">May. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">The enron corpus: A new dataset for email classification research</title>
		<author>
			<persName><forename type="first">B</forename><surname>Klimt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECML. Springer</title>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="217" to="226" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning word vectors for sentiment analysis</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Maas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Daly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">T</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL-HLT. Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="142" to="150" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Design and analysis of the nomao challenge active learning in the real-world</title>
		<author>
			<persName><forename type="first">L</forename><surname>Candillier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Lemaire</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ALRA, Workshop ECML-PKDD. sn</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Human activity recognition on smartphones using a multiclass hardware-friendly support vector machine</title>
		<author>
			<persName><forename type="first">D</forename><surname>Anguita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ghio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Oneto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Parra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Reyes-Ortiz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IWAAL. Springer</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="216" to="223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Present position and potential developments: Some personal views statistical theory the prequential approach</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Dawid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society: Series A (General)</title>
		<imprint>
			<biblScope unit="volume">147</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="278" to="290" />
			<date type="published" when="1984">1984</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
