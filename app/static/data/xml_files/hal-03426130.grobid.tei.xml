<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="fr">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Knowledge improvement and diversity under interaction-driven adaptation of learned ontologies</title>
				<funder ref="#_b8dhqPp">
					<orgName type="full">Agence Nationale de la Recherche</orgName>
					<orgName type="abbreviated">ANR</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yasser</forename><surname>Bourahla</surname></persName>
							<email>yasser.bourahla@inria.fr</email>
						</author>
						<author>
							<persName><forename type="first">Manuel</forename><surname>Atencia</surname></persName>
							<email>manuel.atencia@inria.fr</email>
						</author>
						<author>
							<persName><forename type="first">Jérôme</forename><surname>Euzenat</surname></persName>
							<email>jerome.euzenat@inria.fr</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Univ. Grenoble Alpes</orgName>
								<orgName type="institution" key="instit2">Inria</orgName>
								<orgName type="institution" key="instit3">CNRS</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">Grenoble INP</orgName>
								<orgName type="institution" key="instit2">LIG</orgName>
								<address>
									<postCode>F-38000</postCode>
									<settlement>Grenoble</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution" key="instit1">Univ. Grenoble Alpes</orgName>
								<orgName type="institution" key="instit2">Inria</orgName>
								<orgName type="institution" key="instit3">CNRS</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution" key="instit1">Grenoble INP</orgName>
								<orgName type="institution" key="instit2">LIG</orgName>
								<address>
									<postCode>F-38000</postCode>
									<settlement>Grenoble</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="institution" key="instit1">Univ. Grenoble Alpes</orgName>
								<orgName type="institution" key="instit2">Inria</orgName>
								<orgName type="institution" key="instit3">CNRS</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="institution" key="instit1">Grenoble INP</orgName>
								<orgName type="institution" key="instit2">LIG</orgName>
								<address>
									<postCode>F-38000</postCode>
									<settlement>Grenoble</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Knowledge improvement and diversity under interaction-driven adaptation of learned ontologies</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">19B2E1BF23C8E0BA745DF47CE5416959</idno>
					<note type="submission">Submitted on 12 Nov 2021</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-04-12T14:44+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Ontologies</term>
					<term>Multi-agent social simulation</term>
					<term>Multi-agent learning</term>
					<term>Knowledge diversity</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>de niveau recherche, publiés ou non, émanant des établissements d'enseignement et de recherche français ou étrangers, des laboratoires publics ou privés.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="fr">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>In multi-agent systems, agents can rely on ontologies to understand their environment and to interact with each other <ref type="bibr" target="#b21">[22]</ref>. When ontologies are learned by agents independently, they may be diverse, incorrect or incomplete. This can cause agent interactions to fail <ref type="bibr" target="#b27">[28]</ref>. The origin of these failures differs depending on the interacting agents. Agents may be satisfied by understanding what others mean without necessarily agreeing with them. This may be the case of negotiating agents <ref type="bibr" target="#b14">[15]</ref>. For example, a seller agent may consider an object, e.g. a tomato, to be a "vegetable" while a buyer agent would consider it a "fruit". The agents do not agree on what the object is, but they can still negotiate to reach a price that would satisfy both parties. In contrast, agents may need that others agree with them to carry out tasks together. For example, in a multi-agent cooperation scenario <ref type="bibr" target="#b13">[14]</ref>, agents that carry goods together will need to agree on what the targeted objects are to achieve their tasks. For that purpose, agents may have to change what they learned from the environment.</p><p>However, it is unclear how such changes influence agent knowledge. This raises three fundamental but different questions: Can agents adapt to reach a state with only successful interactions? Is it possible for agents to improve the accuracy of their knowledge about the environment in the process? Do agents preserve the diversity of their knowledge?</p><p>To answer these questions, we introduce an experimental framework. It reflects the considered scenarios by two key features. First, agents have to agree with each other about the environment to act successfully. Second, their payoffs depend on the accuracy of their knowledge about the environment. From this we can monitor the evolution of agent performance in their tasks and their knowledge about the environment as they adapt it to agree with each other.</p><p>Based on this, a two-stage experiment is designed in which agents first learn to take decisions about objects in the environment and then interact with each other about the decisions to take. In the first stage, from sample objects, agents learn decision classifiers that are converted into ontologies. In the second stage, agents perform tasks which consist in taking decisions in face of various objects. They also interact in random pairs by disclosing the decisions they would take about an object. When agents do not agree on a decision, they adapt their knowledge to correct the cause of the disagreement: the agent considered less skilled, measured through its immediate past ability to accomplish tasks, integrates some of the other's knowledge.</p><p>Results show that agents reach a state in which interactions are always successful. Most of the time, they improve their knowledge about the environment but, under specific conditions, knowledge may be forgotten. In addition, they reach this state not necessarily having the same knowledge.</p><p>We also applied the same experiment to a real classification dataset and compared its results with a state-of-the-art coordinated learning approach.</p><p>The remainder of the paper is organised as follows: in Section 2 we review the related work. Section 3 informally describes the targeted scenario before Section 4 fully specifies the environment, agent abilities and how they interact with each other and perform their tasks. Section 5 presents how the experiments are carried out,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Main Track</head><p>AAMAS 2021, May 3-7, 2021, Online introducing hypotheses, factors and measures. Finally, Section 6 reports and discusses the obtained results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Approaches exist that tackle knowledge heterogeneity by finding relationships between different ontologies' entities through ontology matching <ref type="bibr" target="#b12">[13]</ref>. These relationships could be precomputed from ontologies or they could be generated dynamically by pairs of agents <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b28">29]</ref>. In these approaches, agents find relationships between their classes without altering them. This means that agents are only interested in understanding concepts of others through the ones that already exist in their ontologies. However, an agent's ontology may not include the required concepts to understand another agent's concepts. In this case, agents can extend both their ontologies by negotiating to understand each other's concepts <ref type="bibr" target="#b16">[17]</ref>. ANEMONE <ref type="bibr" target="#b27">[28]</ref> proposes a communication protocol that allows agents to gradually share parts of their ontologies as they communicate. Agents in ANEMONE can send sample instances in order to help others learn a concept if no satisfactory definition of it could be given. This allows agents to enhance their ontologies and understand the concepts of others. However, ANEMONE does not address the problem when two agents have different understanding of a supposedly same concept. This could be crucial if agents need to agree on a concept to achieve tasks together. This is the case considered here in which agent interactions fail if there is a disagreement about a concept.</p><p>Interactions have been exploited as a means for agents to adapt. In interaction-situated semantic alignment <ref type="bibr" target="#b4">[5]</ref>, agents find alignments between their ontologies based on the success of their interactions. Cultural language evolution <ref type="bibr" target="#b23">[24]</ref> showed how agents are able to evolve their language through interaction games. This was also applied to knowledge used to communicate, in the form of ontology alignments <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b26">27]</ref>. We apply this type of approach to evolve agent ontologies, used to represent the environment, by adapting their classes following interaction failures.</p><p>Through such adaptations, agents change what they know about the environment which affects how they perform their classification tasks. A-MAIL <ref type="bibr" target="#b18">[19]</ref> is a tool for agents to align learned classifiers, in which agents also can change the classes of objects. Agents aim to coordinate what they learned to be consistent with each other using argumentation, resulting in decentralised inductive learning. Other approaches also rely on argumentation similarly to A-MAIL as a means for agents to reach an agreement on concept meaning <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref>. In these approaches, agents deliberately engage in argumentation on what they learned. They are able to generate arguments from the datasets they learned from. In the present paper, agents adapt their knowledge after the learning step and they do not use anymore the data from which they learned but eventually exchange knowledge pieces.</p><p>By going through this process, agents end up improving their classifiers and agree on the decisions they take. This is also explored in classifier fusion <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b22">23]</ref> where multiple classifiers are merged into one model to improve classification performance. This was applied to merging classifiers trained from one data stream <ref type="bibr" target="#b24">[25]</ref> as well as from multiple data streams <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b17">18]</ref>. This is done in a centralised way and results in one classifier while in our case each agent keeps its classifier and adapts it in a decentralised way.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">INFORMAL SCENARIO DESCRIPTION</head><p>To answer the questions raised in the introduction, we consider the following scenario. Agents live in an environment containing various objects. Objects are described by several boolean properties. For example, canMove, hasSharpN ails and hasEyes can be the properties describing the objects of an environment. An object cow in that environment would have the properties {canMove, ¬hasSharpN ails, hasEyes} meaning that it can move, it does not have sharp nails and it has eyes.</p><p>Agents take decisions when they encounter objects of the environment. We consider only one type of agent for which each object has one correct decision. For instance, if we consider the decisions Hunt, Leave and Collect, the decision for the object apple could be Collect, the decision for a rock is Leave and the decision for rabbit is Hunt. However, agents do not initially know the correct decisions. Instead, each agent is given a set of sample objects and the corresponding correct decision and learns to take decisions.</p><p>When an agent is presented the initial set of samples, it generates a decision tree. For example, two agents a and b learn from two different sets of objects S 1 and S 2 , respectively, where S 1 = {rock, tiдer , rabbit } and S 2 = {rabbit, tree}. Agent a may learn that objects that cannot move or can move but have sharp nails are to be left and objects that can move and do not have sharp nails are to be hunted. Figure <ref type="figure" target="#fig_2">2</ref> illustrates the corresponding decision tree. Agent b may simply learn that objects that have eyes are hunted and those who do not are left.</p><p>After this initial learning phase, agents perform tasks in the environment and interact with each other. Their task is to take decisions about objects of the environment. Taking the right decision provides agents with a payoff correlated to how good its performance was. The payoff could be seen as having food, not being injured, etc.</p><p>Each interaction between agents is focused on one object in the environment. Figure <ref type="figure" target="#fig_0">1</ref> illustrates two interactions in which agents a and b interact about the objects "rock" and "lion". The agents disclose the decisions they would take when they encounter the object. Agent a classifies the object "rock" in the decision class Leave which is the same class in which agent b classifies it. Since the agents agree on the decisions they take, the interaction is considered successful. However, if the object is a "lion", agent b classifies it in Hunt while agent a classifies it in Leave, which causes this interaction to fail. This could be seen as two agents who are hunting together but do not agree on whether an object is "huntable" or not. Thus, the hunt will not proceed effectively.</p><p>When a failure happens, one of the agents adapts its knowledge to agree with the other agent on the decision to take. This corresponds to learning by communicating. Agents first determine which one of them will change its decision. To do this, they rely on the payoffs received from the environment. The agent having less payoff adapts its knowledge in order to adopt the decision of the other. The intuition behind this is that the payoff denotes success, if not wisdom, and agents tend to imitate the successful ones in an attempt to reach a similar situation. "rock" and "lion". For "rock", the decisions are the same so the interaction is successful. For "lion", the decisions are different, hence the interaction is considered a failure. cm = canMove, hsn = hasSharpN ails, he = hasEyes.</p><p>The initial questions may then be informally reformulated as (1) can agents achieve better communication through these adaptations? (2) Do they improve their knowledge? (3) Do they need to end up with the same ontologies to interact successfully?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTAL FRAMEWORK</head><p>To address these questions, we design an experimental framework allowing to deal with scenarios like the previous one. We more precisely introduce the environments and agents before defining the actions that agents perform: ontology learning, interacting and adaptating knowledge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Environment</head><p>The environment is composed of a set of objects I described by properties from a set P. For the sake of simplicity, the properties are considered binary, i.e. an object either has a property p ∈ P or it does not (which is represented by ¬p). P determines the set I containing one object per possible combination of properties. This means that the size of I is 2 | P | . To each of these objects, corresponds only one correct decision in D = {d 1 , ..., d k } given by the function</p><formula xml:id="formula_0">h * : I ∪ I → D.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Agents</head><p>We consider a set A of agents situated in the environment. When an agent encounters an object, it takes a decision about it. Agents know about the possible decisions they can take but not the correct decision about an object. They are also able to see the properties of the objects in the environment: this is shared knowledge between them.</p><p>The knowledge of agents is represented as ontologies. Ontologies allow agents to identify objects based on their properties. We use the description logic ALC <ref type="bibr" target="#b5">[6]</ref>  and ∀p.⊥ (objects having no value for property p) that we note as p and ¬p, respectively. The right-hand side of Figure <ref type="figure" target="#fig_2">2</ref> shows the ontology corresponding to the decision tree of agent a in Figure <ref type="figure" target="#fig_0">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Ontology Learning</head><p>Initially, each agent is provided with a possibly different training set. This training set contains a subset of objects of I , each with a different property combination, associated with the corresponding correct decision (labelled samples). The proportion r of objects in the sample with respect to I is called the training ratio. From the training set each agent learns a decision tree classifier, using the ID3 algorithm <ref type="bibr" target="#b20">[21]</ref>.</p><p>In a decision tree, each node corresponds to a test on a property. Each sub-branch of a node corresponds to one outcome of the test. Leaf nodes are associated to decisions. Each object satisfies the tests leading to only one leaf node from the root. It is classified in the decision associated to that leaf node.</p><p>Nodes can be viewed as classes of objects and each child node corresponds to a subclass restricting one property to a value. Based on this principle <ref type="bibr" target="#b8">[9]</ref>, the decision tree is transformed into an ontology in ALC (see Figure <ref type="figure" target="#fig_2">2</ref>). The ontologies obtained from decision trees are private to each agent.</p><p>Agents know about the possible decisions. This can be thought of as if they had access to a common external ontology O * of the possible decisions without knowing their definitions. This ontology contains for each decision d i a class D i , and every two different decision classes D i and D j are disjoint. Agents can express that objects of class C in its own ontology correspond to a decision d i by adding the correspondence ⟨C, ⊑, D i ⟩ as shown in Figure <ref type="figure" target="#fig_2">2</ref>.</p><p>This defines, for each agent a, a function h a : I ∪ I → D which assigns a decision to each object. The decision d i for object o is found using the correspondence ⟨C a i , ⊑, D i ⟩ attached to the most specific class C a i to which o belongs to ontology O a .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Tasks and Interactions</head><p>Agents perform tasks in the environment by classifying objects according to their decisions. Following this, the agent receives a payoff that reflects the correctness of its decisions at the classification task. An agent a performs a task by going through the following steps:</p><p>(1) A subset S of objects with different properties from I is presented to agent a. The proportion t of S over I is called the task ratio. (2) Agent a labels the objects o ∈ S of the set with the decisions h a (o).</p><p>(3) Agent a receives the payoff r S,a reflecting how well it performed the task. We choose to take the number of objects labelled correctly, i.e. r S,a = |{o ∈ S; h a (o) = h * (o)}|, as payoff.</p><p>Two agents a and b are able to interact with each other about an object o by going through the following steps:</p><p>(1) agents a and b disclose their decisions h a (o) and h b (o) .</p><p>(2) if h a (o) = h b (o) then they agree (success), (3) otherwise they do not agree (failure).  </p><formula xml:id="formula_1">⊤ b O b ¬he he ⊑ ⊑ he ⊓ ¬(cm ⊓ hsn) he ⊓ (cm ⊓ hsn) ⊑ ⊑ ⊤ b O b ¬he he ⊑ ⊑ ⊤ * O * Leave Collect Hunt ⊑ ⊑ ⊑ ⊕ ⊕ ⊕ ⊒ ⊒ ⊑ ⊒ ⊑ Adapt</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Adaptation</head><p>After a failure in communication, agents first compare their respective payoffs to determine which agent should adapt its ontology. Let agent w be the one having the higher payoff and agent l the one with the lower one (if they have an equal payoff, one of them will be selected randomly as agent w).</p><p>Then, agent l adapts its ontology. Let C w (resp. C l ) be the leaf class to which object o belongs in O w (resp. O l ). The effect of the adaptation operator is to split the objects of class C l into two sets. The set of objects that belong to C w will have their decision class changed to the one of C w , and those which do not, will keep the same decision, as shown in Figure <ref type="figure" target="#fig_3">3</ref>.</p><p>The adaptation happens as follows:</p><p>(1) Agent l asks agent w for the definition of its class C w .</p><p>(2) If C l ⊓ ¬C w ⊥ (or C l C w , i.e. some objects classified as C l are not classified as C w ), agent l creates the classes </p><formula xml:id="formula_2">C 1 l ≡ C l ⊓C w and C 2 l ≡ C l ⊓ ¬C w . Otherwise, it sets C 1 l = C l . (<label>3</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTS</head><p>To answer the raised questions, we run experiments based on the presented framework. The questions are reformulated as hypotheses tested through a systematic experiment plan whose output is measured.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Hypotheses</head><p>We test three hypotheses corresponding to the raised questions from Section 3.</p><p>• Hypothesis 1: The success rate converges to 1. This corresponds to the fact that, after a while, communication is always successful.</p><p>• Hypothesis 2: The average accuracy of the population improves at the end of the experiment.</p><p>• Hypothesis 3: Agents do not necessarily converge to the same ontologies, i.e. the average ontology distance of the experiments is not necessarily 0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Experiment Plan</head><p>In one run of the experiment, agents initially learn from the environment how to take decisions. Then, they go through n iterations of the following:</p><p>(1) A pair of agents is selected randomly.</p><p>(2) The two agents perform a task in the environment for which they receive payoff. (3) They interact to take a decision about one object selected randomly from the environment. (4) Agents adapt their ontologies if the interaction failed. Since the experiment depends on different factors, mentioned in Section 4, we define an experiment plan to vary these parameters as presented in Table <ref type="table" target="#tab_0">1</ref> and run each combination 10 times. This means that we processed q = 5×3×3×3×4×10 = 5400 simulations of 40000 iterations. At each iteration, the measures defined in Section 5.3 are recorded.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Measures</head><p>To assess the three presented hypotheses, we define: (1) the interaction success rate which indicates how often agents have agreed on their decision, (2) the accuracy of agents' classifiers on I and (3) a distance measure between ontologies. An experiment E p is identified by its identifier p and characterised by the tuple of parameters ⟨A p , P p , D p , r p , t p , n p ⟩ as defined in k at that iteration. We define the following measures computed at each iteration for each agent:</p><p>• accuracy(E p, j k ) is the accuracy of agent k's ontology with respect to the set I, i.e. the ratio of objects in I that are well classified by agent k's ontology to all objects of I.</p><formula xml:id="formula_3">accuracy(E p, j k ) = |{o ∈ I|h p, j k (o) = h * (o)}| |I| such that h p, j k (o)</formula><p>is the decision of agent k for object o in state E p, j .</p><p>• the distance between two ontologies O a and O b is</p><formula xml:id="formula_4">δ (O a , O b ) = 1 - eq(O a , O b ) max(|O a |, |O b |) eq(O a , O b )</formula><p>is the number of equivalent classes between ontologies O a and O b , i.e. the number of classes that are defined equivalently in terms of property values:</p><formula xml:id="formula_5">eq(O a , O b ) = |{(C a , C b ) ∈ O a × O b |O a , O b |= C a ≡ C b }|</formula><p>For each experiment state E p, j , we measure the success rate (srate(E p, j )) as the ratio of successful interactions until j, the acuracy (accuracy(E p, j )) as the average accuracy for all agents, and the distance (distance(E p, j )) as the average distance between each pair of distinct agent ontologies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Comparison with an Alternative Approach</head><p>To determine how agents perform on realistic data compared to a coordinated learning approach, we repeated the experiment by generating the environment from an existing classification dataset. We used the Zoology dataset from the UCI machine learning repository <ref type="bibr" target="#b10">[11]</ref> because its attributes are easily converted to binary. The obtained results were compared with those of A-MAIL <ref type="bibr" target="#b18">[19]</ref>.</p><p>The only differences with the settings of the main experiment are: (a) The environment objects and their decisions are taken from the dataset instead of generated randomly. We performed 10-fold cross-validation, thus using only 90% of the dataset as environment objects. (b) The task ratio is fixed to 0.2, the lowest value from the previous experiment. The training ratio is also set to 0.2 which corresponds to the ratio agents in A-MAIL use for training.</p><p>As mentioned in Section 2, A-MAIL is a coordinated inductive learning approach where agents engage in an explicit argumentation process on what they learned to reach a common classification. Agents do not have access to the same information in the two approaches. In A-MAIL agents keep the datasets they learned from in memory and are able to use them in argumentation. However, in our setting, agents do not keep their datasets but receive an evaluation from the environment, i.e. the payoff corresponding to their performances on achieving tasks. This is why the comparison is merely indicative.</p><p>We measure, in addition to the accuracy, the precision and recall of experiment p at iteration j. The precision (resp. recall) of agent k with respect to decision d is the ratio of objects of decision d that agent k correctly classifies to the objects that agent k classifies in decision d (resp. to the objects for which decision d is correct):</p><formula xml:id="formula_6">precision(E p, j k , d) = |I p, j k,d ∩ I d | |I p, j k,d | recall(E p, j k , d) = |I p, j k,d ∩ I d | |I d | such that I p, j k,d = {o ∈ I|h p, j k (o) = d } and I d = {o ∈ I|h * (o) = d }.</formula><p>The precision and recall are averaged per decision class and then per agents. As usual, the F-measure is the harmonic mean of these precision and recall.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">RESULTS AND DISCUSSION</head><p>Table <ref type="table" target="#tab_2">2</ref> contains the average of success rate, accuracy and ontology distance, grouped by factor values, at the first, intermediate and final iterations for all the experiments. The table includes the measures after the first interaction because some factors influence them from the start. For example, a larger training ratio corresponds to a higher success rate because agents have higher chances of getting overlapping training sets with larger training ratios and thus agree more on their decisions. It also includes some of the intermediate results which show that for some parameters, agents converge much faster to a stable state with successful communication. For instance, it can be observed that the success rate converges to 1 faster with fewer agents.</p><p>In what follows, we show how these results answer the three hypotheses. We also analyze the effects of different factors on the obtained results and discuss the main ones. Finally, the results of the experiment presented in Section 5.4 are provided.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Success Rate</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Accuracy</head><p>Figure <ref type="figure" target="#fig_7">5</ref> shows the difference in distributions of average agent accuracies at the start of the simulations and at their end. The distribution at the end of the simulations shifts towards 1 which indicates an overall improvement of agent accuracies. Table <ref type="table" target="#tab_2">2</ref> shows that this happens for all factor values. This corroborates a weak version of the second hypothesis, i.e. on average on all the runs accuracy improves.</p><p>To show that the difference in average accuracy is significant, we conducted a paired Student t-test between the average accuracy at the beginning of the simulation (Mean= 0.56, Standard Deviation= 0.14) and at the end (Mean= 0.79, Standard Deviation= 0.2). There is a significant difference with t = 100.06 and p &lt; 0.01. However, the left-hand extremity of Figure <ref type="figure" target="#fig_7">5</ref> tells us that there are cases in which accuracy actually decreases. This rebuts a strong version of the hypothesis, i.e. that accuracy increases at each run. In 3.5% of the runs the final average accuracy is lower than the initial one. This is explained by agents having a rare correct decision which is lost because they disagree with others which have gathered more payoff. Consider two agents a and b that classify all objects correctly except that a classifies o incorrectly and b classifies o ′ incorrectly. It may happen that agent a performs better than agent b in the task because S contains o ′ but not o; a would then gather more payoff than b. If o is the selected object, then as a result b will change its (correct) decision about o to the (incorrect) one held by a. If these are the only two agents, they now have ontologies whose average accuracy is lower. Section 6.4.3 discusses this further.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Ontology Distance</head><p>The boxplots in Figure <ref type="figure" target="#fig_8">6</ref> show the distribution of average ontology distances at the start of the simulation and at the end of it. We observe that the distribution slightly shifts towards 0 at the end of the simulation. As expected, agents end up with more similar ontologies. However, they do not necessarily share the same ontologies. Table <ref type="table" target="#tab_2">2</ref> shows that for all factor values, the average distance remains far from 0. In fact, 90.78% of the runs do not lead to the same ontologies. The reason behind this is that agents may consider different properties to take the same decision. Figure <ref type="figure" target="#fig_9">7</ref> shows the ontologies of agents a and b who take the same decisions for all objects. For example, both agents a and b would take decision D 2 for object o that has the properties {¬p 1 , p 2 , p 3 }. Agent a would take it because o has p 2 and agent b because it has ¬p 1 . This supports the third hypothesis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Effects of Simulation Factors</head><p>To determine which factors (independent variables) significantly affect which measures (dependent variables) we performed an analysis of variance (ANOVA) test on the final measured values of success rate, distance and accuracy of each simulation run. The following results are simplified as we study only the direct effect of each factor on its own (contrary to N-way ANOVA). ANOVA returns, for each pair of independent and dependent variables, the probability that the independent variable has no effect on the dependent variable (p-value). We consider a p-value that is lower than 0.01 as low enough to reject that the independent variable has no 0 0.   effect on the dependent variable. ANOVA resulted in a significant effect of all factors on all measures except for (a) the number of agents on the distance and (b) the number of properties on the accuracy. However, ANOVA only informs if there is an effect from the independent variables. To know how an independent variable affects a dependent variable we performed a post-hoc Tukey HSD (honestly significant difference) test. In the following, we discuss three of the main effects. The full analysis can be found in our experiment logbook <ref type="bibr" target="#b7">[8]</ref>.</p><formula xml:id="formula_7">⊤ b O b ¬p 1 p 1 ⊑ ⊑ p 1 ⊓ ¬p 3 p 1 ⊓ p 3 ⊑ ⊑ p 1 ⊓ ¬p 3 ⊓ ¬p 2 1 ⊓ ¬p 3 ⊓ p 2 ⊑ ⊑ ⊤ a O a ¬p 2 p 2 ⊑ ⊑ ¬p 2 ⊓ ¬(p 1 ⊓ ¬p 3 ) ¬p 2 ⊓ (p 1 ⊓ ¬p 3 ) ⊑ ⊑ ⊤ * O * D 1 D 2 ⊑ ⊑ ⊕ ⊑ ⊑ ⊑ ⊑ ⊑ ⊑ ⊑</formula><p>6.4.1 Effect of number of agents on accuracy. On the one hand, Figure <ref type="figure" target="#fig_10">8</ref> shows that the number of agents does not significantly affect the final ontology distance. On the other hand, it shows that the more agents there are, the higher the final accuracy is. This is because correct pieces of knowledge could be completely lost if they are part of an overall bad agent knowledge. To illustrate this, consider an object o of decision d j such that it is classified correctly by agents with lower payoffs. If the object is picked up in the early iterations, the agents may wrongly correct it, and since there are only few agents, the information "o has the decision d j " might completely disappear, as discussed in Section 6.2. However, with more agents, the chances that this information survives for later iterations are higher. Thus, agents with low payoffs get the chance to correct their other errors to increase their payoffs and start spreading the information that o is of decision d j .</p><p>6.4.2 Effect of number of properties on ontology distance. On the one hand, Figure <ref type="figure" target="#fig_12">9</ref> shows that the final average accuracy is not significantly affected by the number of properties. On the other hand, the average ontology distances at the beginning of the simulation are at different values, higher for the higher number of properties, and they all drop by about the same amount. As the number of properties increases, agents have more flexibility on which properties they consider to take decisions, which results in them having different class definitions and still agree on the decisions. For example, if all objects having the property p 1 are classified as d  6.4.3 Effect of task ratio and number of agents on accuracy difference. Both a higher task ratio and a larger number of agents lead to higher accuracy on average (Table <ref type="table" target="#tab_2">2</ref>). However, as noted in Section 6.2, the accuracy difference between the final and initial iteration is not always positive. We identified the task ratio and the number of With a higher task ratio, the received payoff better assesses the quality of the decisions taken by the agents. Thus agents have less chances of changing their decisions to a wrong one. However, even with a low task ratio, if the agent population is large, lost decisions are more easily recovered. This is clearly illustrated by Table <ref type="table" target="#tab_4">3</ref>. Obviously, if the problem illustrated in Section 6.2 occurs with 2 agents, the correct decision is definitively lost. When there are more agents, it may be recovered. When A-MAIL is used with 2, 3 and 4 agents, they only learn from 40%, 60% and 80% of the training dataset respectively, which explains the relatively low results compared to 5 agents in which 100% of the training set is used. A-MAIL results do not improve with more agents <ref type="bibr" target="#b18">[19]</ref>. In contrast, the performance of agents presented here depends more on their number. With enough agents, they can improve their knowledge significantly to reach results on par with A-MAIL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5">Results on Real Data</head><p>Agents perform better in a realistic dataset than on randomly generated objects and decisions. With 2 agents, the accuracy on a realistic dataset (16 binary properties and 7 decision classes) is 0.95 compared to an average of 0.61 in randomly generated datasets (3 to 5 binary properties and 2 to 4 decision classes). A similar improvement can be observed for the other numbers of agents. This is due to feature patterns existing in the real classes and not in randomly generated ones making the generalisation easier for agents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CONCLUSION</head><p>A first contribution of this paper is the experimental framework in which agents first learn the knowledge that they later adapt. This shows that it is possible to have continuity between these two tasks. This also provides an experimental setting in which it is possible to independently experiment with different ontology learning and adaptation methods.</p><p>Nonetheless, the main contribution is the experimental test of the three hypotheses: (a) agents can adapt knowledge, improving agreement and communication, (b) doing so they, most of the time, develop more accurate knowledge, and (c) this does not constrain them to have the same knowledge. Under specific conditions and in 3.5% of the cases, knowledge may be forgotten. This is realistic and it would be worth considering how agents could address this.</p><p>Finally, we tested this approach on a classification task with realistic data and contrasted the obtained results with a coordinated inductive learning approach. The achieved correctness and completeness of both approaches were on par.</p><p>This work shows how agents can globally evolve their knowledge by locally reacting to environment and society pressure. It constitutes a step towards agents able to effectively adapt to other individuals and their environment.</p><p>Several things were left aside in this paper. The payoff structure used to assess agents is uniform. However, in reality this is not always the case, e.g. the payoff received by running from a lion should be higher than the one received from collecting an apple. Such payoff structure may further constrain the evolution of agent ontologies. In addition, agents had perfect shared knowledge about the object properties and the possible decisions. In reality, there might be differences on how agents perceive the environment and what decisions they take. It would be worth exploring these issues. This work is not exclusive of other means such as developing alignments and adapting them. Future work may explore the impact of different learning methods as well as different agent behaviours, i.e. other adaptation operators or reinforcement learning, combined with other assessment methods that can originate from the environment, the society or both.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure1: Agents a and b share their decisions for objects "rock" and "lion". For "rock", the decisions are the same so the interaction is successful. For "lion", the decisions are different, hence the interaction is considered a failure. cm = canMove, hsn = hasSharpN ails, he = hasEyes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>to express ontologies. We denote that a class C is subsumed by another class D by C ⊑ D, equivalent to D by C ≡ D or disjoint from D by C ⊕ D. ⊤ and ⊥ are the top and bottom classes representing the class containing all individuals and the empty class, respectively. From the classes C and D, the union (C ⊔ D), the intersection (C ⊓ D) and the negation (¬C) can be formed. Constraint on properties may be ∃p.C (objects having at least a value of property p in class C) or ∀p.C (objects having all values of property p in class C). We restrict the use of ALC such that agents only use ∃p.⊤ (objects having a value for property p)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Example of how an agent learns a decision tree-like ontology from a set of samples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Example of agent b adaptation with C w ≡ (cm ⊓hsn) by adding two classes: he ⊓ (cm ⊓ hsn) and he ⊓ ¬(cm ⊓ hsn). It also removes ⟨he, ⊑, Hunt⟩ and adds ⟨he ⊓ ¬(cm ⊓hsn), ⊑, Hunt⟩ and ⟨he ⊓ (cm ⊓ hsn), ⊑, Leave⟩.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>) Let D l be the decision class for C l and D w be the decision class for C w . Agent l replaces ⟨C l , ⊑, D l ⟩ by ⟨C 1 l , ⊑, D w ⟩. If C 2 l has been created, it also adds ⟨C 2 l , ⊑, D l ⟩.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4</head><label>4</label><figDesc>Figure4shows the average success rate at each iteration.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Average success rate over 40000 iterations. The shaded part boundaries represent the standard deviation from the average.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Distribution of accuracies at start of the simulation (blue) and at the end of the simulation (red).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Distribution of average ontology distances at the start of the simulation (blue) and at the end of the simulation (red).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Example of two ontologies O a and O b that take the same decisions with only two equivalent classes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Average ontology accuracy (dotted) and ontology distance (plain) by number of agents |A|.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>1 and the rest as d 2 ,</head><label>2</label><figDesc>agent a might consider directly the property p 1 to take the decision while agent b can first consider the property p 2 then the property p 1 . As a result, the agents would have different class definitions. If one of the properties is removed, both agents will have to use the same property which would result in them having equivalent class definitions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Average ontology accuracy (plain) and ontology distance (dotted) by number of properties |P |.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>The state E</figDesc><table><row><cell>p, j</cell></row><row><cell>k</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Independent variable ranges agent k at iteration j in experiment p is described by its ontology</figDesc><table><row><cell>O</cell><cell>p, j</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>The success rate converges to 1, which supports the first hypothesis. The standard deviation gradually decreases as the number of Average of success rate, accuracy and ontology distance at the first (1), last (40000) and intermediate(2000, 10000)   iterations of all experiments grouped by factor values. In bold, the highest values of the cell.iterations increases. This indicates that the success rate of different simulation runs are converging similarly even though they start at different levels due to randomness in initial ontologies or different simulation factors.</figDesc><table><row><cell></cell><cell>2</cell><cell>number of agents 5 10 20</cell><cell>40</cell><cell cols="2">number of properties number of classes 3 4 5 2 3 4</cell><cell>training ratio 0.1 0.3 0.5 0.2 0.4 0.6 0.8 task ratio</cell></row><row><cell>srate accuracy distance</cell><cell cols="4">1 0.47 0.48 0.51 0.46 0.47 0.50 0.47 2000 1.00 0.98 0.91 0.71 0.94 0.89 10000 1.00 1.00 0.98 0.94 0.87 0.99 0.97 40000 1.00 1.00 1.00 0.99 0.96 1.00 0.99 1 0.57 0.56 0.56 0.56 0.56 0.58 0.56 2000 0.61 0.70 0.79 0.82 0.79 0.78 0.75 10000 0.61 0.70 0.80 0.88 0.92 0.79 0.78 40000 0.61 0.70 0.80 0.88 0.94 0.79 0.78 1 0.56 0.61 0.62 0.62 0.61 0.43 0.61 2000 0.47 0.47 0.47 0.49 0.57 0.33 0.50 10000 0.47 0.47 0.47 0.47 0.48 0.33 0.49 40000 0.47 0.47 0.47 0.47 0.48 0.33 0.49</cell><cell>0.47 0.81 0.92 0.98 0.56 0.70 0.77 0.79 0.77 0.65 0.60 0.60</cell><cell>0.58 0.48 0.37 0.43 0.47 0.54 0.47 0.47 0.48 0.50 0.92 0.88 0.85 0.90 0.86 0.88 0.86 0.88 0.89 0.90 0.97 0.96 0.94 0.96 0.95 0.96 0.94 0.96 0.96 0.97 0.99 0.99 0.98 0.99 0.99 0.99 0.98 0.99 0.99 0.99 0.66 0.54 0.49 0.45 0.56 0.68 0.56 0.56 0.56 0.57 0.81 0.73 0.69 0.59 0.77 0.86 0.70 0.74 0.75 0.78 0.84 0.77 0.74 0.63 0.82 0.90 0.75 0.78 0.79 0.81 0.84 0.77 0.74 0.64 0.82 0.90 0.76 0.78 0.79 0.81 0.58 0.61 0.62 0.39 0.69 0.73 0.61 0.60 0.60 0.60 0.52 0.49 0.47 0.38 0.55 0.55 0.49 0.49 0.50 0.50 0.50 0.47 0.44 0.35 0.52 0.54 0.46 0.47 0.48 0.48 0.50 0.47 0.44 0.35 0.52 0.54 0.46 0.47 0.48 0.48</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>as the main factors that influence this. As can be observed from Table3, a lower task ratio and fewer agents increase the chances of this to happen. Number of runs with negative difference by number of agents and task ratio (each cell = 360 runs).</figDesc><table><row><cell>Main Track</cell></row></table><note><p>agents</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4</head><label>4</label><figDesc>displays the results obtained in the additional experiment of Section 5.4.</figDesc><table><row><cell>Method Simulation A-MAIL</cell><cell>|A| Precision F-measure Recall Accuracy 2 0.88 0.87 0.86 0.951 5 0.91 0.89 0.88 0.964 10 0.94 0.92 0.91 0.977 20 0.96 0.94 0.93 0.984 40 0.95 0.94 0.93 0.983 2 0.97 0.85 0.75 0.950 3 0.98 0.89 0.81 0.968 4 0.97 0.90 0.84 0.966 5 0.98 0.93 0.88 0.980</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Final precision, F-measure, recall and accuracy of different methods on the test set.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>Proc.of the 20th International Conference on Autonomous Agents and Multiagent Systems (AAMAS 2021), U. Endriss, A. Nowé, F. Dignum, A. Lomuscio (eds.), May 3-7, 2021, Online. © 2021 International Foundation for Autonomous Agents and Multiagent Systems (www.ifaamas.org). All rights reserved.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>ACKNOWLEDGMENTS</head><p>This work has been partially supported by <rs type="funder">MIAI @ Grenoble Alpes</rs> (<rs type="grantNumber">ANR-19-P3IA-0003</rs>). The authors thank the reviewers for so many good suggestions that it was not possible to address them all here.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_b8dhqPp">
					<idno type="grant-number">ANR-19-P3IA-0003</idno>
				</org>
			</listOrg>

			<div type="availability">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DATA AVAILABILITY</head><p>All experiments were performed in the Lazy lavender software environment <ref type="bibr" target="#b0">[1]</ref>. Settings, output and data analysis notebooks are made available at <ref type="bibr" target="#b7">[8]</ref> and <ref type="bibr" target="#b6">[7]</ref>.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<ptr target="https://gitlab.inria.fr/moex/lazylav" />
		<title level="m">Lazy lavender</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">An approach to interaction-based concept convergence in multi-agent systems</title>
		<author>
			<persName><forename type="first">Kemo</forename><surname>Adrian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Enric</forename><surname>Plaza</surname></persName>
		</author>
		<ptr target="CEUR-WS.org" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Joint Ontology Workshops 2017 Episode 3: The Tyrolean Autumn of Ontology</title>
		<title level="s">CEUR Workshop Proceedings</title>
		<meeting>the Joint Ontology Workshops 2017 Episode 3: The Tyrolean Autumn of Ontology<address><addrLine>Bozen-Bolzano, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-09-21">2017. September 21-23, 2017. 2050</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Argumentation on meaning: a semiotic model for contrast set alignment</title>
		<author>
			<persName><forename type="first">Kemo</forename><surname>Adrian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Enric</forename><surname>Plaza</surname></persName>
		</author>
		<ptr target="CEUR-WS.org" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Joint Ontology Workshops 2019 Episode V: The Styrian Autumn of Ontology</title>
		<title level="s">CEUR Workshop Proceedings</title>
		<meeting>the Joint Ontology Workshops 2019 Episode V: The Styrian Autumn of Ontology<address><addrLine>Graz, Austria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-09-23">2019. September 23-25, 2019</date>
			<biblScope unit="volume">2518</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Aligning experientially grounded ontologies using language games</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Anslow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Rovatsos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Graph Structures for Knowledge Representation and Reasoning -4th International Workshop, GKR 2015</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<meeting><address><addrLine>Buenos Aires, Argentina</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015-07-25">2015. July 25, 2015. 9501</date>
			<biblScope unit="page" from="15" to="31" />
		</imprint>
	</monogr>
	<note>Revised Selected Papers</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">An interaction-based approach to semantic alignment</title>
		<author>
			<persName><forename type="first">Manuel</forename><surname>Atencia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Schorlemmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Web Semantics</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="131" to="147" />
			<date type="published" when="2012">2012. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">The description logic handbook: Theory, implementation and applications</title>
		<author>
			<persName><forename type="first">Franz</forename><surname>Baader</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diego</forename><surname>Calvanese</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deborah</forename><surname>Mcguinness</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniele</forename><surname>Nardi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Patel-Schneider</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
	<note>2 ed.</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">Yasser</forename><surname>Bourahla</surname></persName>
		</author>
		<idno type="DOI">10.5281/zenodo.4507093</idno>
		<ptr target="http://sake.re/20201001-DOLA" />
		<title level="m">DOLA Experiment description</title>
		<imprint>
			<date type="published" when="2020">2020. 20201001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><forename type="first">Yasser</forename><surname>Bourahla</surname></persName>
		</author>
		<idno type="DOI">10.5281/zenodo.4507546</idno>
		<ptr target="http://sake.re/20200623-DOLA" />
		<title level="m">DOLA Experiment description</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page">2020623</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Chemical hazard estimation and method comparison with OWL-Encoded toxicity decision trees</title>
		<author>
			<persName><forename type="first">Leonid</forename><forename type="middle">L</forename><surname>Chepelev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dana</forename><surname>Klassen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michel</forename><surname>Dumontier</surname></persName>
		</author>
		<idno type="DOI">10.5281/zenodo.4507546</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th International Workshop on OWL: Experiences and Directions (OWLED 2011)</title>
		<title level="s">CEUR Workshop Proceedings</title>
		<meeting>the 8th International Workshop on OWL: Experiences and Directions (OWLED 2011)<address><addrLine>San Francisco, California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011-06-05">2011. June 5-6, 2011</date>
			<biblScope unit="volume">796</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Ensemble methods in machine learning</title>
		<author>
			<persName><forename type="first">G</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><surname>Dietterich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Multiple Classifier Systems</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<meeting><address><addrLine>Berlin Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2000">2000</date>
			<biblScope unit="volume">1857</biblScope>
			<biblScope unit="page" from="1" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">Dheeru</forename><surname>Dua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Casey</forename><surname>Graff</surname></persName>
		</author>
		<ptr target="http://archive.ics.uci.edu/ml" />
		<title level="m">UCI machine learning repository</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Interaction-based ontology alignment repair with expansion and relaxation</title>
		<author>
			<persName><forename type="first">Jérôme</forename><surname>Euzenat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Sixth International Joint Conference on Artificial Intelligence, IJCAI 2017</title>
		<meeting>the Twenty-Sixth International Joint Conference on Artificial Intelligence, IJCAI 2017<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-08-19">2017. August 19-25, 2017</date>
			<biblScope unit="page" from="185" to="191" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Ontology matching</title>
		<author>
			<persName><forename type="first">Jérôme</forename><surname>Euzenat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pavel</forename><surname>Shvaiko</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
			<publisher>Springer Publishing Company, Incorporated</publisher>
		</imprint>
	</monogr>
	<note>2nd ed.</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Commitments and conventions: The foundation of coordination in multi-agent systems</title>
		<author>
			<persName><forename type="first">N</forename><surname>Jennings</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowl. Eng. Rev</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="223" to="250" />
			<date type="published" when="1993">1993. 1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Automated negotiation: Prospects, methods and challenges</title>
		<author>
			<persName><forename type="first">N</forename><surname>Jennings</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Faratin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lomuscio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Parsons</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wooldridge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Sierra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Group Decision and Negotiation</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="199" to="215" />
			<date type="published" when="2000">2000. 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Optimizing use of multistream influenza sentinel surveillance data</title>
		<author>
			<persName><forename type="first">H Y</forename><surname>Eric</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><forename type="middle">J</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lai-Ming</forename><surname>Cowling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriel</forename><forename type="middle">M</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><surname>Leung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Emerging Infectious Diseases</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1154" to="1157" />
			<date type="published" when="2008">2008. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A case-based approach to mutual adaptation of taxonomic ontologies</title>
		<author>
			<persName><forename type="first">Sergio</forename><surname>Manzano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Santiago</forename><surname>Ontañón</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Enric</forename><surname>Plaza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Case-Based Reasoning Research and Development -20th International Conference, ICCBR 2012</title>
		<title level="s">Proceedings (Lecture Notes in Computer Science</title>
		<meeting><address><addrLine>Lyon, France</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012-09-03">2012. September 3-6, 2012</date>
			<biblScope unit="volume">7466</biblScope>
			<biblScope unit="page" from="226" to="240" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Bayesian information fusion networks for biosurveillance applications</title>
		<author>
			<persName><forename type="first">R</forename><surname>Zaruhi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Howard</forename><forename type="middle">S</forename><surname>Mnatsakanyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacqueline</forename><forename type="middle">S</forename><surname>Burkom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><forename type="middle">S</forename><surname>Coberly</surname></persName>
		</author>
		<author>
			<persName><surname>Lombardo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Medical Informatics Association : JAMIA</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="855" to="863" />
			<date type="published" when="2009-11">2009. Nov. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Coordinated inductive learning using argumentation-based communication</title>
		<author>
			<persName><forename type="first">Santiago</forename><surname>Ontañón</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Enric</forename><surname>Plaza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Autonomous Agents and Multi-Agent Systems</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="266" to="304" />
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Negotiating over ontological correspondences with asymmetric and incomplete knowledge</title>
		<author>
			<persName><forename type="first">Terry</forename><forename type="middle">R</forename><surname>Payne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Valentina</forename><forename type="middle">A M</forename><surname>Tamma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on Autonomous Agents and Multi-Agent Systems, AAMAS&apos;14</title>
		<meeting><address><addrLine>Paris, France</addrLine></address></meeting>
		<imprint>
			<publisher>IFAAMAS/ACM</publisher>
			<date type="published" when="2014-05-05">2014. May 5-9, 2014</date>
			<biblScope unit="page" from="517" to="524" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Induction of decision trees</title>
		<author>
			<persName><forename type="first">John</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quinlan</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="81" to="106" />
			<date type="published" when="1986-03">1986. March 1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Artificial intelligence: A modern approach</title>
		<author>
			<persName><forename type="first">Stuart</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Norvig</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>Prentice Hall Press</publisher>
			<pubPlace>USA</pubPlace>
		</imprint>
	</monogr>
	<note>3rd ed.</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">An overview of classifier fusion methods</title>
		<author>
			<persName><forename type="first">Dymitr</forename><surname>Ruta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bogdan</forename><surname>Gabrys</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computing and Information Systems</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="1" to="10" />
			<date type="published" when="2000">2000. 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Experiments in cultural language evolution</title>
		<author>
			<persName><forename type="first">Luc</forename><surname>Steels</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<publisher>John Benjamins Publishing Company</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Using decision fusion methods to improve outbreak detection in disease surveillance</title>
		<author>
			<persName><forename type="first">Gaëtan</forename><surname>Texier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Rodrigue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Loty</forename><surname>Allodji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean-Baptiste</forename><surname>Diop</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liliane</forename><surname>Meynard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hervé</forename><surname>Pellegrin</surname></persName>
		</author>
		<author>
			<persName><surname>Chaudet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC Medical Informatics and Decision Making</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="38" to="49" />
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Argumentation for reconciling agent ontologies</title>
		<author>
			<persName><forename type="first">Cássia</forename><surname>Trojahn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jérôme</forename><surname>Euzenat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Valentina</forename><surname>Tamma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Terry</forename><forename type="middle">R</forename><surname>Payne</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-642-18308-9_5</idno>
		<ptr target="https://doi.org/10.1007/978-3-642-18308-9_5" />
		<imprint>
			<date type="published" when="2011">2011</date>
			<publisher>Springer</publisher>
			<biblScope unit="page" from="89" to="111" />
			<pubPlace>Berlin Heidelberg, Berlin, Heidelberg</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Agent ontology alignment repair through dynamic epistemic logic</title>
		<author>
			<persName><forename type="first">Line</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manuel</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jérôme</forename><surname>Atencia</surname></persName>
		</author>
		<author>
			<persName><surname>Euzenat</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-642-18308-9_5</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th International Conference on Autonomous Agents and Multiagent Systems, AA-MAS &apos;20</title>
		<meeting>the 19th International Conference on Autonomous Agents and Multiagent Systems, AA-MAS &apos;20<address><addrLine>Auckland, New Zealand</addrLine></address></meeting>
		<imprint>
			<publisher>International Foundation for Autonomous Agents and Multiagent Systems</publisher>
			<date type="published" when="2020-05-09">2020. May 9-13, 2020</date>
			<biblScope unit="page" from="1422" to="1430" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">ANEMONE: An effective minimal ontology negotiation environment</title>
		<author>
			<persName><forename type="first">Robbert-Jan</forename><surname>Jurriaan Van Diggelen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Beun</surname></persName>
		</author>
		<author>
			<persName><surname>Dignum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rogier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John-Jules</forename><surname>Van Eijk</surname></persName>
		</author>
		<author>
			<persName><surname>Ch</surname></persName>
		</author>
		<author>
			<persName><surname>Meyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">5th International Joint Conference on Autonomous Agents and Multiagent Systems (AAMAS 2006)</title>
		<meeting><address><addrLine>Hakodate, Japan</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2006-05-08">2006. May 8-12, 2006</date>
			<biblScope unit="page" from="899" to="906" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Truthful mechanisms for multi agent self-interested correspondence selection</title>
		<author>
			<persName><forename type="first">Nan</forename><surname>Zhi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Terry</forename><forename type="middle">R</forename><surname>Payne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Krysta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minming</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Semantic Web -ISWC 2019 -18th International Semantic Web Conference</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<meeting><address><addrLine>Auckland, New Zealand</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019-10-26">2019. October 26-30, 2019</date>
			<biblScope unit="volume">11778</biblScope>
			<biblScope unit="page" from="733" to="750" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
