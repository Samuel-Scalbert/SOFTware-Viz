<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Unmasking the Hidden Meaning: Bridging Implicit and Explicit Hate Speech Embedding Representations</title>
				<funder>
					<orgName type="full">French government</orgName>
				</funder>
				<funder ref="#_AGYdasG">
					<orgName type="full">Agence Nationale de la Recherche</orgName>
					<orgName type="abbreviated">ANR</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Nicolas</forename><surname>Ocampo</surname></persName>
							<email>nicolas-benjamin.ocampo@univ-cotedazur.fr</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Universite Côte d&apos;Azur</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
								<orgName type="institution" key="instit3">Inria</orgName>
								<address>
									<region>I3S</region>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Elena</forename><surname>Cabrio</surname></persName>
							<email>elena.cabrio@univ-cotedazur.fr</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Universite Côte d&apos;Azur</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
								<orgName type="institution" key="instit3">Inria</orgName>
								<address>
									<region>I3S</region>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Serena</forename><surname>Villata</surname></persName>
							<email>serena.villata@univ-cotedazur.fr</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Universite Côte d&apos;Azur</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
								<orgName type="institution" key="instit3">Inria</orgName>
								<address>
									<region>I3S</region>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Unmasking the Hidden Meaning: Bridging Implicit and Explicit Hate Speech Embedding Representations</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">4CF091D9ACF0B3C9C27CC682D49CFA05</idno>
					<idno type="DOI">10.18653/v1/2023.findings-emnlp.441</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-04-12T14:51+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Research on automatic hate speech (HS) detection has mainly focused on identifying explicit forms of hateful expressions on user-generated content. Recently, a few works have started to investigate methods to address more implicit and subtle abusive content. However, despite these efforts, automated systems still struggle to correctly recognize implicit and more veiled forms of HS. As these systems heavily rely on proper textual representations for classification, it is crucial to investigate the differences in embedding implicit and explicit messages. Our contribution to address this challenging task is fourfold. First, we present a comparative analysis of transformer-based models, evaluating their performance across five datasets containing implicit HS messages. Second, we examine the embedding representations of implicit messages across different targets, gaining insight into how veiled cases are encoded. Third, we compare and link explicit and implicit hateful messages across these datasets through their targets, enforcing the relation between explicitness and implicitness and obtaining more meaningful embedding representations. Lastly, we show how these newer representation maintains high performance on HS labels, while improving classification in borderline cases.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The proliferation of hate speech (HS) on social media platforms has become a pressing concern in online social communities. While significant progress has been made in the development of HS detection methods, current SOTA models focus on detecting explicit HS, leaving implicit hate cases undetected <ref type="bibr" target="#b6">(ElSherief et al., 2021;</ref><ref type="bibr" target="#b17">Ocampo et al., 2023)</ref>. This issue is aggravated by the sheer volume of implicit hate speech content being spread across various online platforms, necessitating automated approaches to detect them effectively.</p><p>Implicit HS detection poses unique challenges compared to its explicit counterpart: it contains coded, ambiguous or indirect language that does not immediately denote hate, but still disparages a person or a group based on protected characteristics such as race, gender, cultural identity, or religion (e.g., "I think it is a bit late to think to look after the safety and the future of white people in South-Africa" -the White Supremacy Forum Dataset <ref type="bibr" target="#b5">(de Gibert et al., 2018)</ref>). The performance of current HS systems heavily relies on how coded language is represented and how well classifiers can capture the underlying semantic meaning of messages through embeddings. Hence, obtaining better text representation becomes crucial in effectively identifying implicit HS messages.</p><p>In this direction, the goal of our work is to bridge the gap between explicit and implicit messages, aiming to enhance the embedding representations of SOTA models. Our contribution is fourfold: i) We analyze the embedding representations of five benchmark datasets with veiled hateful content, examining the levels of explicitness and implicitness, through cross-evaluation using state-of-theart transformer models. ii) We examine the embedding representations of implicit messages across different target groups. Through this analysis, we gain insights into how implicit HS messages are encoded based on their target groups. iii) We propose a novel approach to link explicit and implicit HS messages in the representation space. iv) We illustrate that the newer representation space preserves strong efficacy for HS labels, while also refining classification in borderline instances. Using contrastive learning techniques <ref type="bibr" target="#b9">(Gunel et al., 2020;</ref><ref type="bibr" target="#b21">Rethmeier and Augenstein, 2021;</ref><ref type="bibr" target="#b13">Kim et al., 2022;</ref><ref type="bibr" target="#b25">Tian et al., 2020)</ref>, we aim to push explicit and implicit messages effectively enforcing the uncovered relation between these two notions and thereby obtaining more meaningful representations than those obtained through fine-tuning learning methods. 1 NOTE: This paper contains examples of language which may be offensive to some readers. They do not represent the views of the authors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>HS detection has been extensively studied by the research community providing multiple resources, such as lexicons <ref type="bibr" target="#b30">(Wiegand et al., 2018;</ref><ref type="bibr" target="#b1">Bassignana et al., 2018)</ref>, datasets <ref type="bibr" target="#b31">(Zampieri et al., 2019;</ref><ref type="bibr" target="#b0">Basile et al., 2019;</ref><ref type="bibr" target="#b4">Davidson et al., 2017;</ref><ref type="bibr" target="#b7">Founta et al., 2018)</ref>, and supervised methods <ref type="bibr" target="#b18">(Park and Fung, 2017;</ref><ref type="bibr" target="#b8">Gambäck and Sikdar, 2017;</ref><ref type="bibr" target="#b27">Wang et al., 2020;</ref><ref type="bibr" target="#b15">Lee et al., 2019)</ref> (for a survey, see <ref type="bibr" target="#b20">(Poletto et al., 2021)</ref>). These studies provide a solid starting point to examine the problem of abusive language, especially in social media messages. Lately, there has been growing interest in the detection of implicit HS, which provides additional challenges. Datasets specifically designed for implicit HS <ref type="bibr" target="#b17">(Ocampo et al., 2023;</ref><ref type="bibr" target="#b11">Hartvigsen et al., 2022;</ref><ref type="bibr" target="#b6">ElSherief et al., 2021;</ref><ref type="bibr" target="#b26">Vidgen et al., 2021;</ref><ref type="bibr" target="#b24">Sap et al., 2020)</ref>, more solid veiled detectors <ref type="bibr" target="#b10">(Han and Tsvetkov, 2020)</ref>, guided augmentation strategies <ref type="bibr" target="#b16">(Nejadgholi et al., 2022;</ref><ref type="bibr" target="#b23">Roychowdhury and Gupta, 2023)</ref>, and theoretical analysis <ref type="bibr" target="#b12">(Jurgens et al., 2019;</ref><ref type="bibr" target="#b28">Waseem et al., 2017;</ref><ref type="bibr" target="#b29">Wiegand et al., 2021)</ref> have been recently proposed to advance in this direction.</p><p>However, little attention has been dedicated to effectively represent implicit messages through embeddings on these benchmarks. Embeddings play a crucial role in the performance of classifiers <ref type="bibr" target="#b19">(Pavlopoulos et al., 2017;</ref><ref type="bibr" target="#b14">Kshirsagar et al., 2018;</ref><ref type="bibr" target="#b17">Ocampo et al., 2023)</ref>, yet their application to capture the implicit nature of HS has been underinvestigated. In this direction, <ref type="bibr" target="#b13">(Kim et al., 2022)</ref> tackles cross-dataset underperforming issues on HS classifiers and proposes a contrastive learning method that encodes a hateful post and its corresponding implication close in representation space, closely depending on the annotated implications and without contra positioning explicitness with implicitness. <ref type="bibr" target="#b2">(Bourgeade et al., 2023)</ref> captures topic-generic and topic-specific knowledge when trained on different data to improve generalization.</p><p>3 Implicit and Explicit HS Embeddings</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Research Questions</head><p>We will focus on the behavior of SOTA models in cross-evaluation settings, specifically on datasets containing implicit hate. The study explores the models' behavior on different HS classes, including both explicit and implicit hate. In particular, we target the following research questions (RQ): RQ1: How do the models' embeddings capture the HS classes? Are explicit and implicit hateful messages encoded differently across different datasets? What is the extent of this variation? RQ2: Does grouping the test sets by target result in similar encoding patterns for explicit HS and distinct encoding patterns for implicit HS in the embeddings? RQ2 builds upon the analysis conducted in RQ1, but with a focus on target groups. RQ3: Can we link and bring explicit and implicit embedding representations closer together within the learned embedding space through their target groups? RQ4: How do these newer embedding representations capture HS classes in comparison with RQ1?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Datasets</head><p>We carried out our analysis on the following standard datasets, containing implicit HS messages: Implicit Subtle Hate (ISHate) <ref type="bibr" target="#b17">(Ocampo et al., 2023)</ref>, Social Bias Inference (SBIC) <ref type="bibr" target="#b24">(Sap et al., 2020)</ref>, Implicit Hate Corpus (IHC) <ref type="bibr" target="#b6">(ElSherief et al., 2021)</ref>, Dynahate (DYNA) <ref type="bibr" target="#b26">(Vidgen et al., 2021)</ref>, and Toxigen (TOX) <ref type="bibr" target="#b11">(Hartvigsen et al., 2022)</ref>. We ensured that the definitions of HS were consistent across the datasets. Specifically, for the SBIC dataset, messages are considered as HS if labeled as offensive and target a specific group. As for the explicitimplicit HS labeling across all datasets, the provided implicit labels are used for IHC and ISHate datasets. For SBIC, DYNA, and TOX, we computed the percentage of HS implicit messages as the ones where none of the words of the Google profanity words resource was present<ref type="foot" target="#foot_1">2</ref> . The datasets were divided into train, dev, and test sets. Existing dataset splits were retained, while datasets without predefined splits were divided using a stratified splitting method with a 60% train, 20% dev, and 20% test ratio. Table <ref type="table" target="#tab_4">4</ref> in Appendix shows the percentage of implicit/explicit instances per dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Experimental Settings</head><p>Concerning our research questions (Section 3.1), to answer to RQ1 we performed fine-tuning on two state-of-the-art models commonly used for HS detection: BERT and HateBERT <ref type="bibr" target="#b3">(Caselli et al., 2021)</ref>. Both models were fine-tuned on each dataset using a two-label classification approach, distinguishing between non-HS and HS messages. To ensure robustness and account for randomness in the training process, we repeated the fine-tuning procedure five times, each time employing a different random seed. This allowed us to evaluate the performance of the models consistency. To assess the performance of the models, we cross-evaluate the benchmarks calculating the average F1-score across all fine-tuning runs. Additionally, we calculated the standard deviation to quantify the variability in performance observed across the different runs. Finally, we calculated the embeddings of these models using TSNE highlighting how explicit and implicit messages were encoded. We used the base versions size of these models with batch size of 32, weight decay of 0.01, 4 epochs, and a learning rate of 2e-5. As for TSNE, we use perplexity of 30, and 1000 maximum iterations for convergence.</p><p>For RQ2, we grouped the embeddings per target in the plots. To ensure consistency across datasets, we standardized the target names, addressing label variations, e.g., we resolved differences like "asian" and "asian people" by using a unified label. Moreover, when a message targeted multiple offensive groups (e.g. Asians and Migrants), we selected the label corresponding to the predominant target (among MUSLIMS, WOMEN, JEWS, LGBTQ+, BLACK PEOPLE, WHITE PEOPLE, IMMIGRANTS, ASIAN, and DISEASE).</p><p>For RQ3, we aim to validate the potential linkage between explicit and implicit messages through their target groups. To achieve this, we employ contrastive learning techniques on the pre-trained and fine-tuned models. Contrastive learning involves defining pairs of positive and negative samples and training the model using a modified loss function. In our experimental settings, we designate pairs of implicit and explicit messages with the same target as positive samples. For each implicit ones, a randomly selected explicit message with the same target is paired. In cases where they are unavailable or when the implicit instance lacks a target label, we randomly assign any explicit message. Negative samples consist of pairs of HS and Non-HS instances. For every Non-HS instance, one HS instance is randomly selected. Using contrastive learning facilitates the training process by pushing positive pairs closer together while pushing negative pairs further apart within the embedding space.</p><p>The contrastive loss is defined as follows:</p><formula xml:id="formula_0">loss_cont = mean (1 -l) • s 2 + l • (max(0, m -s)) 2 (1)</formula><p>Where l represents the label pair (1 for positive pairs, 0 for negative pairs), s is the cosine similarity between paired messages, and m is the margin hyper-parameter. For classification, the crossentropy loss is:</p><formula xml:id="formula_1">loss_clf = - N -1 i=0 (g i log(p i ) + (1 -g i )log(1 -p i )) (2)</formula><p>Where g is the gold label (labels of the dataset on which the model is fine-tuned) and p is the prediction. The final loss is:</p><formula xml:id="formula_2">total_loss = loss_cont + loss_clf (3)</formula><p>By combining them, we optimize both the model's understanding of embeddings and classification.</p><p>For RQ4, we fine-tuned both BERT and Hate-BERT using our enhanced embeddings (same settings of our initial RQs). Additionally, to gain more targeted diagnostic insights, models' accuracy was evaluated on three categories defined on the SBIC dataset (Non-HS, Explicit HS, and Implicit HS), and the HateCheck dataset <ref type="bibr" target="#b22">(Röttger et al., 2021)</ref>, a suite of functional tests for HS detection models. Regarding RQ1, Table <ref type="table" target="#tab_0">1a</ref> shows that training and evaluating HateBERT (BERT results can be found in the Appendix) on the same dataset yields Train Test IHC SBIC DYNA ISHate TOX IHC 0,7618 ± 0,0027 0,6824 ± 0,0129 0,5679 ± 0,0522 0,6959 ± 0,0238 0,5896 ± 0,0119 SBIC 0,6385 ± 0,0310 0,8632 ± 0,0099 0,6355 ± 0,0399 0,7522 ± 0,0153 0,6998 ± 0,0086 DYNA 0,6717 ± 0,0310 0,7473 ± 0,0357 0,7860 ± 0,0111 0,7602 ± 0,0025 0,7526 ± 0,0075 ISHate 0,6188 ± 0,0052 0,7209 ± 0,0400 0,6190 ± 0,0064 0,8684 ± 0,0035 0,6034 ± 0,0045 TOX 0,5063 ± 0,0140 0,5428 ± 0,0133 0,4952 ± 0,0197 0,5900 ± 0,0195 0,7650 ± 0,0102 (a) Cross-evaluation results with HateBERT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Obtained Results and Discussion</head><p>Train Test IHC SBIC DYNA ISHate TOX IHC 0,7433 ± 0,0107 0,6618 ± 0,0173 0,5415 ± 0,0092 0,6678 ± 0,0058 0,5843 ± 0,0153 SBIC 0,6593 ± 0,0095 0,8620 ± 0,0064 0,6591 ± 0,0069 0,7583 ± 0,0086 0,6742 ± 0,0208 DYNA 0,6520 ± 0,0102 0,7323 ± 0,0116 0,7831 ± 0,0030 0,7566 ± 0,0135 0,7147 ± 0,0075 ISHate 0,6253 ± 0,0060 0,6753 ± 0,0442 0,6165 ± 0,0116 0,8394 ± 0,0064 0,5973 ± 0,0210 TOX 0,5354 ± 0,0210 0,5637 ± 0,0346 0,5202 ± 0,0098 0,6180 ± 0,0256 0,7610 ± 0,0103  better results overall, as could be expected. However, even in cross-evaluation scenarios, reasonable performances are observed. Notably, among the most generalizable models, HateBERT trained on DYNA exhibits better generalization. We therefore selected HateBERT trained on DYNA as the best configuration and we plot the embeddings for the test sets of all the datasets, applying the TSNE algorithm. Figure <ref type="figure" target="#fig_0">1</ref> shows how explicit HS and non-HS messages are encoded with clear separation, resulting in a noticeable distance between them.<ref type="foot" target="#foot_2">3</ref> On the other hand, implicit HS instances tend to be intertwined with both non-HS and explicit HS messages. This pattern holds true across all 5 datasets. As for the results for RQ2, Figure <ref type="figure" target="#fig_1">2</ref> shows ex-plicit and implicit text representations per target group highlighting how, in general, embeddings of explicit and implicit messages tend to be linked by their target groups in representation spaces. Finally, as for RQ3, Figure <ref type="figure" target="#fig_2">3</ref> demonstrates that the embedding representations of explicit and implicit instances starts to overlap across all datasets when using HateBERT trained on DYNA. Additionally, Figure <ref type="figure" target="#fig_3">4</ref> highlights that by leveraging the targets of HS using contrastive learning, explicit and implicit messages exhibit a similar representation. As for RQ4, Table <ref type="table" target="#tab_0">1b</ref> illustrates that the novel representation enhances the F1-score for certain datasets, such as SBIC, TOX, and ISHate. Conversely, for other datasets like IHC and DYNA, the performance remains comparable to that of the non-contrastive approach.   HateBERT in accurately classifying challenging Non-HS messages across all five datasets. A significant reduction in false positives is also observed in HateCheck categories such as quoted announcements (counter_quote_nh), direct references (counter_ref_nh), positive identifiers (ident_pos_nh), negated hateful remarks (negate_neg_nh), nonhateful profanity (profanity_nh), reclaimed slurs (slur_reclaimed_nh), homonym slurs (slur_homonym_nh), as well as targeted abuse directed at individuals (target_indiv_nh), objects (target_obj_nh), and non-protected groups (target_group_nh). Additionally, Table <ref type="table" target="#tab_2">3</ref> indicates that both Explicit and Implicit categories exhibit similarly high accuracy levels, highlighting their nearly indistinguishable impact on the model's aggregate performance. Also, the importance of the Non-HS category is underscored, varying with different training datasets, yet remaining a critical component.</p><p>Hence, our experiments emphasize the importance of studying implicit representations, as classical training strategies cannot encode them properly (RQ1). We showed that implicit and explicit messages share a connection conveying similar messages to the same target (RQ2) and how contrastive learning effectively forces that property by bridging explicit and implicit instances through their targets (RQ3), thereby obtaining more meaningful representations that the ones obtained through finetuning. Finally, we reduced biases in non-hateful implicit cases often misclassified due to trigger words or nuanced content. Our enhanced method maintains high performance on HS labels while improving classification in borderline cases, proving its robustness and precision (RQ4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusions</head><p>Our contribution in this study is fourfold: i) We studied how models' embeddings capture HS w.r.t. explicitness and implicitness, ii) We showed how explicit and implicit HS messages result in similar encodings if grouped by their protected target, iii) We analyzed a contrastive learning method to force this property when representing implicit text. We prove our research hypothesis on 5 HS benchmarks, moving a step forward in bridging the gap between explicitness and implicitness, and iv) We show how the newer representation space maintains high performance on HS labels while improving classification in borderline cases. In future work, we'll refine contrastive learning, delving into contextual pairing based on other semantic dependencies between explicit and implicit cues, aiming to sharpen nuanced hate speech detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Limitations</head><p>In this study, we are aware of some key issues, one of which pertains to the selection of positive and negative samples in contrastive learning. The effectiveness of the algorithm heavily relies on the careful selection of these pairs. While our investigation demonstrates that explicit and implicit messages exhibit a relationship through their target groups across five distinct datasets, it is important to acknowledge that this assumption may not always hold true. Additionally, ensuring a clear separation between non-hateful and HS instances can be challenging due to the heterogeneity of each category.</p><p>Moreover, the efficacy of our approach is contingent upon the availability and alignment of target information across the datasets. While target information is commonly provided in benchmark datasets, different datasets may address various protected characteristics. Our approach assumes that there is some degree of overlap in terms of target groups among the selected datasets.</p><p>Furthermore, the selection of pairs when linking explicit and implicit messages can vary in terms of the number of combinations. However, it is important to note that as the number of pairs increases, the training requirements tend to grow significantly, resulting in slower training processes. This trade-off between the number of pairs and training efficiency should be carefully considered when implementing the approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Datasets Statistics and Details</head><p>Table <ref type="table" target="#tab_4">4</ref> displays statistics related to the datasets used in our study, including IHC, SBIC, DYNA, TOX, and ISHate. The table presents the percentage of implicit and explicit instances per dataset, along with their distribution across set partitions, and target groups distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Evaluation results with BERT</head><p>In this section we show the evaluation of the BERT and Contrastive BERT models for RQ1 and RQ4 specified in sections 3.1 and 3.3.</p><p>Table <ref type="table" target="#tab_6">5a</ref> demonstrates BERT's efficacy in crossevaluation contexts, mirroring the results seen with HateBERT. Among the models, SBIC stands out, displaying superior generalization capabilities. Conversely, Table <ref type="table" target="#tab_6">5b</ref> illustrates that while models like SBIC, IHC, and TOX reap advantages from contrastive learning, others experience a slight dip in performance, though maintaining an overall high-quality output.</p><p>Moving on to Table <ref type="table" target="#tab_7">6</ref>, it's evident that a segment of the enhancement is attributed to the precise categorization of challenging Non-HS messages prevalent across all five datasets. This precision underscores a more conservative and meticulous approach in classifying a message as Hateful.</p><p>Finally, Table <ref type="table" target="#tab_8">7</ref> highlights BERT's consistent performance, boasting high accuracy in handling Non-HS instances for each dataset. This is achieved without compromising the emphasis on discerning between Explicit and Implicit labels, thereby ensuring that the model maintains a balanced focus on varied content nuances.  0,7625 ± 0,0063 0,6891 ± 0,0072 0,5511 ± 0,0077 0,6824 ± 0,0068 0,6074 ± 0,0329 SBIC 0,6603 ± 0,0301 0,8568 ± 0,0092 0,6500 ± 0,0310 0,7581 ± 0,0167 0,6939 ± 0,0086 DYNA 0,6660 ± 0,0046 0,7412 ± 0,0098 0,7831 ± 0,0027 0,7515 ± 0,0039 0,7501 ± 0,0036 ISHate 0,6214 ± 0,0040 0,7480 ± 0,0058 0,6279 ± 0,0056 0,8635 ± 0,0029 0,6012 ± 0,0090 TOX 0,5455 ± 0,0091 0,5855 ± 0,0312 0,5193 ± 0,0204 0,6140 ± 0,0275 0,7824 ± 0,0094     </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: RQ1: TSNE embeddings of the SBIC test set using HateBERT fine-tuned on DYNA.</figDesc><graphic coords="4,308.33,524.39,213.91,160.43" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: RQ2: TSNE embeddings of the SBIC test set using HateBERT fine-tuned on DYNA based on their target groups. Non-HS are excluded from the plot.</figDesc><graphic coords="5,74.13,297.23,211.73,158.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: RQ3: TSNE embeddings of the SBIC test set using HateBERT fine-tuned on DYNA and linking explicit and implicit instances.</figDesc><graphic coords="5,309.42,459.98,211.73,158.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: RQ3: TSNE embeddings of the SBIC test set using HateBERT fine-tuned on DYNA based on their target groups. Non-HS are excluded from the plot.</figDesc><graphic coords="6,74.13,332.07,211.73,158.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>Figure 5: RQ1: TSNE embeddings of the test sets for all the datasets using HateBERT fine-tuned on DYNA.</figDesc><graphic coords="12,151.13,233.44,145.14,108.86" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>Figure 6: RQ2: TSNE embeddings of the test sets for all the datasets using HateBERT fine-tuned on DYNA based on their target groups. Non-hateful instances are excluded from these plots.</figDesc><graphic coords="12,151.13,581.54,145.14,108.86" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>Figure 7: RQ3: TSNE embeddings of the test sets for all the datasets using HateBERT fine-tuned on DYNA and linking explicit and implicit instances</figDesc><graphic coords="13,151.13,230.45,145.14,108.86" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>HateBERT and Contrastive HateBERT cross-evaluation results with five different run seeds.</figDesc><table /><note><p><p><p>(b) Cross-evaluation results with Contrastive HateBERT. Bold values indicate improvements compared to Table</p>1a</p>.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Table 2 shows higher capability of the contrastive .082 .499 ± .085 .276 ± .213 .422 ± .114 .857 ± .048 .962 ± .027 .379 ± .079 .524 ± .160 0 ± 0 .010 ± .009 counter_ref_nh .576 ± .045 .617 ± .086 .240 ± .168 .393 ± .144 .882 ± .018 .902 ± .037 .387 ± .038 .521 ± .187 .177 ± .043 .295 ± .117 ident_pos_nh .441 ± .020 .446 ± .136 .301 ± .076 .361 ± .038 .849 ± .021 .767 ± .099 .272 ± .071 .350 ± .101 .470 ± .126 .575 ± .133 negate_neg_nh .502 ± .054 .517 ± .134 .120 ± .060 .161 ± .104 .448 ± .037 .486 ± .040 .164 ± .030 .211 ± .106 .087 ± .036 .194 ± .077 profanity_nh .796 ± .036 .774 ± .076 .922 ± .099 .994 ± .005 1 ± 0 1 ± 0 .992 ± .004 .996 ± .005 .292 ± .101 .456 ± .105 slur_homonym_nh .353 ± .061 .413 ± .084 .393 ± .162 .513 ± .099 .813 ± .030 .787 ± .073 .680 ± .056 .827 ± .101 .320 ± .051 .473 ± .064 slur_reclaimed_nh .217 ± .045 .277 ± .084 .472 ± .196 .711 ± .117 .891 ± .018 .879 ± .044 .741 ± .070 .802 ± .113 .272 ± .031 .346 ± .067 .042 .622 ± .105 .923 ± .086 .966 ± .037 .969 ± .011 .985 ± .015 .735 ± .013 .757 ± .073 .006 ± .008 .034 ± .035 Comparative accuracy performance of HateBERT vs Contrastive HateBERT trained in each dataset and evaluated across various test cases on HateCheck.</figDesc><table><row><cell>Test Case</cell><cell>HateBERT IHC</cell><cell>Contrastive IHC</cell><cell>HateBERT SBIC</cell><cell>Contrastive SBIC</cell><cell>HateBERT DYNA</cell><cell>Contrastive DYNA</cell><cell>HateBERT ISHate</cell><cell>Contrastive ISHate</cell><cell>HateBERT TOX</cell><cell>Contrastive TOX</cell></row><row><cell cols="5">counter_quote_nh .486 ± target_group_nh .710 ± .036 .700 ± .027 .623 ± .328 .810 ± .072 target_indiv_nh .538 ± .049 .572 ± .104 .655 ± .451 .951 ± .028 target_obj_nh .637 ± Train Explicit Implicit Non-HS IHC 0.8832 0.8842 0.4659 SBIC 0.8971 0.8440 0.8568 DYNA 0.6071 0.6665 0.8314 ISHate 0.5515 0.4676 0.8768 TOX 0.7938 0.8617 0.3439</cell><cell>.968 ± 0 1 ± 0</cell><cell cols="5">.971 ± .021 .448 ± .027 .561 ± .085 1 ± 0 .782 ± .032 .809 ± .061 .003 ± .007 .003 ± .007 0 ± 0 .006 ± .014</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table /><note><p>Contrastive HateBERT avg accuracy across Explicit, Implicit, and Non-HS (SBIC test set).</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Comparing toxic language datasets. % Hate Class, % Implicit, and % Explicit are the percent labeled as hate, implicit hate, and explicit hate, respectively.</figDesc><table><row><cell>Train IHC</cell><cell>Test</cell><cell>IHC</cell><cell>SBIC</cell><cell>DYNA</cell><cell>ISHate</cell><cell>TOX</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>BERT and Contrastive BERT cross-evaluation results with five different run seeds. .095 .410 ± .070 .282 ± .143 .414 ± .095 .843 ± .057 .870 ± .082 .319 ± .044 .434 ± .050 .097 ± .051 .109 ± .121 counter_ref_nh .329 ± .055 .467 ± .049 .201 ± .093 .264 ± .073 .848 ± .033 .908 ± .018 .410 ± .033 .555 ± .072 .380 ± .152 .359 ± .193 ident_pos_nh .340 ± .039 .406 ± .088 .233 ± .085 .361 ± .165 .846 ± .044 .808 ± .137 .317 ± .060 .454 ± .046 .556 ± .130 .624 ± .194 negate_neg_nh .346 ± .054 .427 ± .132 .048 ± .031 .128 ± .077 .406 ± .055 .502 ± .062 .093 ± .022 .194 ± .098 .218 ± .146 .262 ± .176 profanity_nh .810 ± .054 .822 ± .052 .946 ± .115 .203 .688 ± .168 slur_homonym_nh .513 ± .045 .500 ± .120 .520 ± .090 .513 ± .112 .860 ± .037 .875 ± .057 .880 ± .038 .953 ± .038 .667 ± .100 .667 ± .113 slur_reclaimed_nh .165 ± .060 .215 ± .094 .398 ± .113 .481 ± .081 .815 ± .045 .815 ± .035 .694 ± .064 .790 ± .049 .430 ± .050 .420 ± .156 target_group_nh .684 ± .031 .716 ± .049 .732 ± .267 .771 ± .070 .987 ± .013 .992 ± .016 .416 ± .031 .529 ± .080 .048 ± .036 .084 ± .119 target_indiv_nh .557 ± .063 .498 ± .135 .797 ± .351 .938 ± .038 1 ± 0 1 ± 0 .695 ± .028 .818 ± .077 .142 ± .127 .074 ± .073 target_obj_nh .677 ± .067 .711 ± .111 .988 ± .028 .978 ± .018 1 ± 0 1 ± 0 .785 ± .031 .855 ± .056 .228 ± .182 .218 ± .175</figDesc><table><row><cell>Test Case</cell><cell>BERT IHC</cell><cell>Contrastive IHC</cell><cell>BERT SBIC</cell><cell>Contrastive SBIC</cell><cell>BERT DynaHate</cell><cell>Contrastive DynaHate</cell><cell>BERT ISHate</cell><cell>Contrastive ISHate</cell><cell>BERT ToxiGen</cell><cell>Contrastive ToxiGen</cell></row><row><cell cols="5">counter_quote_nh .297 ± 1 ± 0</cell><cell>1 ± 0</cell><cell>1 ± 0</cell><cell>1 ± 0</cell><cell>1 ± 0</cell><cell>.780 ±</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>Comparative accuracy performance of BERT vs Contrastive BERT trained in each dataset and evaluated across various test cases on HateCheck.</figDesc><table><row><cell>Train</cell><cell cols="3">Explicit Implicit Non-HS</cell></row><row><cell>IHC</cell><cell>0.8754</cell><cell>0.8678</cell><cell>0.5206</cell></row><row><cell>SBIC</cell><cell>0.8920</cell><cell>0.8348</cell><cell>0.8607</cell></row><row><cell cols="2">DYNA 0.5796</cell><cell>0.6113</cell><cell>0.8068</cell></row><row><cell cols="2">ISHate 0.5542</cell><cell>0.4387</cell><cell>0.9067</cell></row><row><cell>TOX</cell><cell>0.6892</cell><cell>0.7727</cell><cell>0.4408</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7 :</head><label>7</label><figDesc>Contrastive BERT avg accuracy across Explicit, Implicit, and Non-HS (SBIC test set).</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>The accompanying software can be found at: https:// github.com/benjaminocampo/bridging_ie_hs_embs.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>List of swear words banned by Google: https:// github.com/RobertJGabriel/Google-profanity-words</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>Due to space constraints, we show only the plots of the TSNE embeddings of the SBIC test set using HateBERT finetuned on DYNA. The plots showing the embeddings for all the other test sets can be found in the Appendix.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_3"><p>This section presents the TSNE results for each research questions RQ1, RQ2, and RQ3, illustrated in Figures5, 6, 7, and 8. These visualizations are generated from the embeddings captured by Hate-BERT, specifically trained on the DYNA dataset and evaluated on all datasets.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>This work has been supported by the <rs type="funder">French government</rs>, through the <rs type="funder">3IA Côte d'Azur Investments</rs> in the Future project managed by the <rs type="projectName">National Re-search Agency (ANR</rs>) with the reference number <rs type="grantNumber">ANR-19-P3IA-0002</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_AGYdasG">
					<idno type="grant-number">ANR-19-P3IA-0002</idno>
					<orgName type="project" subtype="full">National Re-search Agency (ANR</orgName>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ethics Statement</head><p>This paper uses a collection of HS examples extracted from linguistic resources commonly employed for HS detection, ensuring their independence from the authors' personal opinions. The datasets used in this study have been meticulously handled to address privacy concerns associated with user data. While we acknowledge the potential for misuse, we firmly believe that developing robust HS classifiers is essential in combating the proliferation of harmful content. In this regard, our work represents a significant contribution towards this objective and encourages further exploration and investigation within the scientific community.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">SemEval-2019 task 5: Multilingual detection of hate speech against immigrants and women in Twitter</title>
		<author>
			<persName><forename type="first">Cristina</forename><surname>Valerio Basile</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elisabetta</forename><surname>Bosco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Debora</forename><surname>Fersini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Viviana</forename><surname>Nozza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francisco</forename><surname>Patti</surname></persName>
		</author>
		<author>
			<persName><surname>Manuel Rangel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paolo</forename><surname>Pardo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manuela</forename><surname>Rosso</surname></persName>
		</author>
		<author>
			<persName><surname>Sanguinetti</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/S19-2007</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th International Workshop on Semantic Evaluation</title>
		<meeting>the 13th International Workshop on Semantic Evaluation<address><addrLine>Minneapolis, Minnesota, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="54" to="63" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Hurtlex: A Multilingual Lexicon of Words to Hurt</title>
		<author>
			<persName><forename type="first">Elisa</forename><surname>Bassignana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Valerio</forename><surname>Basile</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Viviana</forename><surname>Patti</surname></persName>
		</author>
		<idno type="DOI">10.4000/books.aaccademia.3085</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fifth Italian Conference on Computational Linguistics CLiC-it 2018</title>
		<editor>
			<persName><forename type="first">Elena</forename><surname>Cabrio</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Alessandro</forename><surname>Mazzei</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Fabio</forename><surname>Tamburini</surname></persName>
		</editor>
		<meeting>the Fifth Italian Conference on Computational Linguistics CLiC-it 2018</meeting>
		<imprint>
			<publisher>Accademia University Press</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="51" to="56" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">What did you learn to hate? a topic-oriented analysis of generalization in hate speech detection</title>
		<author>
			<persName><forename type="first">Tom</forename><surname>Bourgeade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patricia</forename><surname>Chiril</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Farah</forename><surname>Benamara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Véronique</forename><surname>Moriceau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics</title>
		<meeting>the 17th Conference of the European Chapter of the Association for Computational Linguistics<address><addrLine>Dubrovnik, Croatia</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="3495" to="3508" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">HateBERT: Retraining BERT for abusive language detection in English</title>
		<author>
			<persName><forename type="first">Tommaso</forename><surname>Caselli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Valerio</forename><surname>Basile</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jelena</forename><surname>Mitrović</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Granitzer</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.woah-1.3</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th Workshop on Online Abuse and Harms (WOAH 2021)</title>
		<meeting>the 5th Workshop on Online Abuse and Harms (WOAH 2021)</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="17" to="25" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Automated Hate Speech Detection and the Problem of Offensive Language</title>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Davidson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dana</forename><surname>Warmsley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Macy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ingmar</forename><surname>Weber</surname></persName>
		</author>
		<idno type="DOI">10.1609/icwsm.v11i1.14955</idno>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the International AAAI Conference on Web and Social Media</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="512" to="515" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Hate speech dataset from a white supremacy forum</title>
		<author>
			<persName><forename type="first">Ona</forename><surname>De Gibert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naiara</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aitor</forename><surname>García-Pablos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Montse</forename><surname>Cuadros</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W18-5102</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd Workshop on Abusive Language Online (ALW2)</title>
		<meeting>the 2nd Workshop on Abusive Language Online (ALW2)<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="11" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Latent hatred: A benchmark for understanding implicit hate speech</title>
		<author>
			<persName><forename type="first">Mai</forename><surname>Elsherief</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caleb</forename><surname>Ziems</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Muchlinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vaishnavi</forename><surname>Anupindi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jordyn</forename><surname>Seybolt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Munmun</forename><forename type="middle">De</forename><surname>Choudhury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diyi</forename><surname>Yang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.emnlp-main.29</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2021 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Punta Cana, Dominican Republic</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="345" to="363" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Large Scale Crowdsourcing and Characterization of Twitter Abusive Behavior</title>
		<author>
			<persName><forename type="first">Antigoni</forename><surname>Founta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Constantinos</forename><surname>Djouvas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Despoina</forename><surname>Chatzakou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilias</forename><surname>Leontiadis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeremy</forename><surname>Blackburn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gianluca</forename><surname>Stringhini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Athena</forename><surname>Vakali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Sirivianos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Kourtellis</surname></persName>
		</author>
		<idno type="DOI">10.1609/icwsm.v12i1.14991</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International AAAI Conference on Web and Social Media</title>
		<meeting>the International AAAI Conference on Web and Social Media</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Using convolutional neural networks to classify hate-speech</title>
		<author>
			<persName><forename type="first">Björn</forename><surname>Gambäck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Utpal</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sikdar</forename></persName>
		</author>
		<idno type="DOI">10.18653/v1/W17-3013</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First Workshop on Abusive Language Online</title>
		<meeting>the First Workshop on Abusive Language Online<address><addrLine>Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="85" to="90" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Supervised contrastive learning for pre-trained language model fine-tuning</title>
		<author>
			<persName><forename type="first">Beliz</forename><surname>Gunel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ves</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno>CoRR, abs/2011.01403</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Fortifying toxic speech detectors against veiled toxicity</title>
		<author>
			<persName><forename type="first">Xiaochuang</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yulia</forename><surname>Tsvetkov</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.622</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="7732" to="7739" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">ToxiGen: A large-scale machine-generated dataset for adversarial and implicit hate speech detection</title>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Hartvigsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saadia</forename><surname>Gabriel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hamid</forename><surname>Palangi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maarten</forename><surname>Sap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dipankar</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ece</forename><surname>Kamar</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2022.acl-long.234</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 60th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="3309" to="3326" />
		</imprint>
	</monogr>
	<note>: Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A just and comprehensive strategy for using NLP to address online abuse</title>
		<author>
			<persName><forename type="first">David</forename><surname>Jurgens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Libby</forename><surname>Hemphill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eshwar</forename><surname>Chandrasekharan</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1357</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3658" to="3666" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Generalizable implicit hate speech detection using contrastive learning</title>
		<author>
			<persName><forename type="first">Youngwook</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shinwoo</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yo-Sub</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th International Conference on Computational Linguistics</title>
		<meeting>the 29th International Conference on Computational Linguistics<address><addrLine>Gyeongju, Republic of Korea</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="6667" to="6679" />
		</imprint>
	</monogr>
	<note>International Committee on Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Predictive embeddings for hate speech detection on Twitter</title>
		<author>
			<persName><forename type="first">Rohan</forename><surname>Kshirsagar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tyrus</forename><surname>Cukuvac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kathy</forename><surname>Mckeown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Susan</forename><surname>Mcgregor</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W18-5104</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd Workshop on Abusive Language Online (ALW2)</title>
		<meeting>the 2nd Workshop on Abusive Language Online (ALW2)<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="26" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Detecting context abusiveness using hierarchical deep learning</title>
		<author>
			<persName><forename type="first">Ju-Hyoung</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun-U</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeong-Won</forename><surname>Cha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yo-Sub</forename><surname>Han</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-5002</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second Workshop on Natural Language Processing for Internet Freedom: Censorship, Disinformation, and Propaganda</title>
		<meeting>the Second Workshop on Natural Language Processing for Internet Freedom: Censorship, Disinformation, and Propaganda<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="10" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Improving generalizability in implicitly abusive language detection with concept activation vectors</title>
		<author>
			<persName><forename type="first">Isar</forename><surname>Nejadgholi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kathleen</forename><surname>Fraser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Svetlana</forename><surname>Kiritchenko</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2022.acl-long.378</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 60th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="5517" to="5529" />
		</imprint>
	</monogr>
	<note>: Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">An in-depth analysis of implicit and subtle hate speech messages</title>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Ocampo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ekaterina</forename><surname>Sviridova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elena</forename><surname>Cabrio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Serena</forename><surname>Villata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics</title>
		<meeting>the 17th Conference of the European Chapter of the Association for Computational Linguistics<address><addrLine>Dubrovnik, Croatia</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="1997" to="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">One-step and twostep classification for abusive language detection on Twitter</title>
		<author>
			<persName><forename type="first">Ji</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Park</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Pascale</forename><surname>Fung</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W17-3006</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First Workshop on Abusive Language Online</title>
		<meeting>the First Workshop on Abusive Language Online<address><addrLine>Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="41" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Improved abusive comment moderation with user embeddings</title>
		<author>
			<persName><forename type="first">John</forename><surname>Pavlopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prodromos</forename><surname>Malakasiotis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juli</forename><surname>Bakagianni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ion</forename><surname>Androutsopoulos</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W17-4209</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 EMNLP Workshop: Natural Language Processing meets Journalism</title>
		<meeting>the 2017 EMNLP Workshop: Natural Language Processing meets Journalism<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="51" to="55" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Resources and benchmark corpora for hate speech detection: a systematic review</title>
		<author>
			<persName><forename type="first">Fabio</forename><surname>Poletto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Valerio</forename><surname>Basile</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manuela</forename><surname>Sanguinetti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cristina</forename><surname>Bosco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Viviana</forename><surname>Patti</surname></persName>
		</author>
		<idno type="DOI">10.1007/s10579-020-09502-8</idno>
	</analytic>
	<monogr>
		<title level="j">Language Resources and Evaluation</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="477" to="523" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">A primer on contrastive pretraining in language processing: Methods, lessons learned and perspectives</title>
		<author>
			<persName><forename type="first">Nils</forename><surname>Rethmeier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Isabelle</forename><surname>Augenstein</surname></persName>
		</author>
		<idno>CoRR, abs/2102.12982</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">HateCheck: Functional tests for hate speech detection models</title>
		<author>
			<persName><forename type="first">Paul</forename><surname>Röttger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bertie</forename><surname>Vidgen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dong</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeerak</forename><surname>Waseem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Helen</forename><surname>Margetts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Janet</forename><surname>Pierrehumbert</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.acl-long.4</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="41" to="58" />
		</imprint>
	</monogr>
	<note>Long Papers</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Dataefficient methods for improving hate speech detection</title>
		<author>
			<persName><forename type="first">Sumegh</forename><surname>Roychowdhury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vikram</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: EACL 2023</title>
		<meeting><address><addrLine>Dubrovnik, Croatia</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="125" to="132" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Social bias frames: Reasoning about social and power implications of language</title>
		<author>
			<persName><forename type="first">Maarten</forename><surname>Sap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saadia</forename><surname>Gabriel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lianhui</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.486</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="5477" to="5490" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">What makes for good views for contrastive learning</title>
		<author>
			<persName><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dilip</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<idno>CoRR, abs/2005.10243</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning from the worst: Dynamically generated datasets to improve online hate detection</title>
		<author>
			<persName><forename type="first">Bertie</forename><surname>Vidgen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tristan</forename><surname>Thrush</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeerak</forename><surname>Waseem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Douwe</forename><surname>Kiela</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.acl-long.132</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1667" to="1682" />
		</imprint>
	</monogr>
	<note>Long Papers</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Detect all abuse! toward universal abusive language detection models</title>
		<author>
			<persName><forename type="first">Kunze</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caren</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siqu</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Josiah</forename><surname>Poon</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.coling-main.560</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Computational Linguistics</title>
		<meeting>the 28th International Conference on Computational Linguistics<address><addrLine>Barcelona, Spain (Online</addrLine></address></meeting>
		<imprint>
			<publisher>International Committee on Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="6366" to="6376" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Understanding abuse: A typology of abusive language detection subtasks</title>
		<author>
			<persName><forename type="first">Zeerak</forename><surname>Waseem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Davidson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dana</forename><surname>Warmsley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ingmar</forename><surname>Weber</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W17-3012</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First Workshop on Abusive Language Online</title>
		<meeting>the First Workshop on Abusive Language Online<address><addrLine>Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="78" to="84" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Implicitly abusive language -what does it actually look like and why are we not getting there</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Wiegand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Josef</forename><surname>Ruppenhofer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elisabeth</forename><surname>Eder</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.naacl-main.48</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="576" to="587" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Inducing a lexicon of abusive words -a feature-based approach</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Wiegand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Josef</forename><surname>Ruppenhofer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anna</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clayton</forename><surname>Greenberg</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N18-1095</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>New Orleans, Louisiana</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1046" to="1056" />
		</imprint>
	</monogr>
	<note>Long Papers</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">SemEval-2019 task 6: Identifying and categorizing offensive language in social media (Of-fensEval)</title>
		<author>
			<persName><forename type="first">Marcos</forename><surname>Zampieri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shervin</forename><surname>Malmasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Preslav</forename><surname>Nakov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sara</forename><surname>Rosenthal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noura</forename><surname>Farra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ritesh</forename><surname>Kumar</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/S19-2010</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th International Workshop on Semantic Evaluation</title>
		<meeting>the 13th International Workshop on Semantic Evaluation<address><addrLine>Minneapolis, Minnesota, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="75" to="86" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
