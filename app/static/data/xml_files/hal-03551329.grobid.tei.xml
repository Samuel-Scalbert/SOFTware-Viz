<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="fr">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main"></title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<affiliation key="aff0">
								<address>
									<settlement>Orange, Lannion</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">Univ Rennes</orgName>
								<orgName type="institution" key="instit2">Inria</orgName>
								<orgName type="institution" key="instit3">CNRS</orgName>
								<orgName type="institution" key="instit4">IRISA</orgName>
								<address>
									<settlement>Rennes</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Inria -Grenoble Rhône-Alpes</orgName>
								<address>
									<settlement>Villeurbanne</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">36ED052187BE4FEE99EFF369891B24F0</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-04-12T14:48+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Dans cet article nous proposons une manière d'améliorer l'interprétabilité des explications contrefactuelles. Une explication contrefactuelle se présente sous la forme d'une version modifiée de la donnée à expliquer qui répond à la question : que faudrait-il changer pour obtenir une prédiction différente ? La solution proposée consiste à introduire dans le processus de génération du contrefactuel un terme basé sur un auto-encodeur supervisé. Ce terme contraint les explications générées à être proches de la distribution des données et de leur classe cible. La qualité des contrefactuels produits est évaluée sur un jeu de données d'images par le biais de différentes métriques. Nous montrons que notre solution s'avère compétitive par rapport à une méthode de référence de l'état de l'art.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="fr">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Générer des explications contrefactuelles à l'aide d'un autoencodeur supervisé</p><p>Victor Guyomard * , * * , Françoise Fessant * Tassadit Bouadi * * Thomas Guyet * * * 1 Introduction L'apprentissage automatique est désormais massivement utilisé pour automatiser la prise de décision dans de nombreux domaines, et en particulier dans des domaines qui impactent notre vie quotidienne. Les modèles utilisés sont généralement complexes et opaques. C'est le phénomène de la « boite noire ». L'IA explicable (ou XAI) vise à limiter ce problème en fournissant un ensemble de méthodes pour qu'un utilisateur humain comprenne les facteurs qui ont motivé la décision d'un modèle. L'enjeu de l'explicabilité devient crucial que ce soit pour l'acceptation de l'IA ou le respect des réglementations. Par exemple, le règlement général sur la protection des données de l'union européenne, entré en application en 2018, introduit un droit à l'explication pour les individus lorsque la prise de décision automatisée les affecte significativement. De nombreuses approches d'explicabilité ont été développées récemment et plusieurs typologies existent pour les classifier <ref type="bibr" target="#b7">(Molnar, 2019)</ref>. On s'intéresse ici aux méthodes d'interprétabilité post-hoc locales, c'est à dire qui s'appliquent après l'apprentissage du modèle de classification et pour une prédiction donnée. Les explications contrefactuelles appartiennent à cette catégorie. Le principe est d'expliquer la décision du modèle de classification à l'aide d'un exemple, proche de l'exemple à expliquer, qui montre comment celui-ci devrait changer pour que sa prédiction change.</p><p>La plupart des méthodes d'explications contrefactuelles sont basées sur la perturbation de l'instance originale grâce à l'optimisation d'une fonction de coût <ref type="bibr">(Wachter et al., 2018)</ref>. Se-lon les propriétés souhaitées pour l'explication on rajoute des contraintes dans le processus d'optimisation sous la forme de termes supplémentaires dans la fonction de coût. Par exemple, on peut souhaiter un contrefactuel le plus proche possible de l'exemple à expliquer, avec le moins de variables perturbées possible, actionnable (où seules certaines variables peuvent être perturbées) ou encore réaliste. Pour <ref type="bibr">Van Looveren et Klaise (2021)</ref>, <ref type="bibr" target="#b6">Mahajan et al. (2019)</ref> ou <ref type="bibr" target="#b1">Dhurandhar et al. (2018)</ref> le réalisme consiste à faire en sorte que le contrefactuel respecte la distribution des données (tout du moins qu'il ne s'en écarte pas trop). Cette propriété est atteinte par l'ajout dans la fonction de coût d'un terme basé sur un autoencodeur. L'autoencodeur est un réseau de neurones artificiels qui cherche à reconstruire le plus fidèlement possible en sortie les exemples qui lui ont été présentés en entrée <ref type="bibr" target="#b2">(Kramer, 1991)</ref>. On cherche donc à générer des contrefactuels qui minimisent cette erreur de reconstruction ; une erreur importante indiquant un contrefactuel loin de la distribution des données et ainsi moins réaliste.</p><p>Dans cet article nous proposons une extension du travail de Van Looveren et Klaise (2021) avec le remplacement de l'autoencodeur par un autoencodeur supervisé de manière à construire un contrefactuel qui respecte la distribution des données mais également la distribution des caractéristiques des classes cibles. Après avoir rappelé l'algorithme de Van Looveren et Klaise (2021), nous décrivons notre proposition et nous comparons la qualité des contrefactuels produits sur un jeu de données d'images par le biais de différentes métriques. Nous montrons que l'organisation de l'espace latent de l'autoencodeur par rapport aux classes des exemples lors de l'apprentissage, conduit à des explications contrefactuelles fidèles à l'ensemble de la distribution des données mais aussi à la distribution des données de la classe cible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Génération d'explications contrefactuelles post-hoc</head><p>Cette section formalise le processus de génération d'explications contrefactuelles post-hoc, ainsi que la méthode de l'état de l'art dont nous proposons l'extension.</p><p>On notera X ⊆ R p l'espace des variables de dimension p, D = {(x i , y i )} n i=1 un ensemble d'apprentissage tel que x i ∈ X et y i ∈ {1, . . . , l} pour tout i ∈ {1, . . . , n}. L'explication post-hoc consiste à appliquer une méthode d'explicabilité une fois le modèle de classification f pred : X → {1, . . . , l} entraîné. Ce modèle prédit, pour un exemple donné, l'appartenance à une classe parmi les l classes possibles et [f pred ( x)] y représente la probabilité d'appartenance à la classe y pour un exemple x. Dans ce contexte, la génération d'exemples contrefactuels peut être vue comme la solution d'un problème d'optimisation <ref type="bibr">(Wachter et al., 2018)</ref>. Soit x 0 l'exemple à expliquer, un contrefactuel sera alors un autre exemple x cf = x 0 + δ proche de x 0 mais classifié par le modèle de classification f pred avec une classe différente (δ est la perturbation à appliquer à l'exemple pour qu'il change de classe). Il est ainsi possible de définir un contrefactuel comme l'exemple le plus proche de l'exemple à expliquer mais dont la classe prédite est différente.</p><p>Cependant, cette approche peut conduire à des contrefactuels peu réalistes ou peu actionnables <ref type="bibr" target="#b0">(De Oliveira et Martens, 2021)</ref>. Plusieurs stratégies ont été proposées pour agir sur le réalisme des contrefactuels comme par exemple l'ajout de contraintes supplémentaires dans l'algorithme d'optimisation <ref type="bibr" target="#b8">(Mothilal et al., 2020)</ref>, ou dans le processus de génération de contrefactuels afin que ces derniers soient proches de la distribution des données. <ref type="bibr" target="#b5">Li et al. (2018</ref><ref type="bibr">) ou Pawelczyk et al. (2020)</ref> proposent d'utiliser pour cela un autoencodeur entraîné sur l'ensemble d'apprentissage.</p><p>Van Looveren et Klaise (2021) proposent de combiner les deux types d'approches présentés ci-dessus, pour construire de manière post-hoc des contrefactuels interprétables, par le biais d'une fonction de coût comprenant différents termes de pénalisation.</p><p>Plus précisément, un contrefactuel x cf = x 0 + δ ∈ X est obtenu en optimisant la fonction de coût suivante :</p><formula xml:id="formula_0">min δ (c • f τ (x 0 , δ) + ||δ|| + γ • L AE + θ • L proto )<label>(1)</label></formula><p>f τ (x 0 , δ) encourage la classe prédite y i du contrefactuel x cf à être différente de la classe y 0 de l'exemple à expliquer x 0 :</p><formula xml:id="formula_1">f τ (x 0 , δ) = max [f pred (x 0 + δ)] y0 - max yi∈{1,...,l},yi =y0 [f pred (x 0 + δ)] yi , -τ où τ ≥ 0 cape la différence entre [f pred (x 0 + δ)] y0 et [f pred (x 0 + δ)] yi . ||δ|| = β • δ 1 + δ 2</formula><p>2 cherche à minimiser la distance entre x 0 et x cf de manière à générer une perturbation "creuse", c'est-à-dire pour laquelle peu de valeurs d'attributs sont modifiées. </p><formula xml:id="formula_2">L AE = x 0 + δ -AE D (x 0 + δ)</formula><formula xml:id="formula_3">(x) = DEC D (ENC D (x)) pour tout x ∈ X .</formula><p>Intuitivement, cela pénalise les contrefactuels qui seraient loin des exemples dans X . Finalement, le dernier terme L proto est défini de la façon suivante :</p><formula xml:id="formula_4">L proto = ENC D (x 0 + δ) -proto yj 2 2 ,</formula><p>où proto yj est un prototype (un exemple représentatif) calculé comme la moyenne dans l'espace latent, des K plus proches exemples de l'exemple à expliquer x 0 parmi ceux dont la classe prédite par f pred est y j :</p><formula xml:id="formula_5">proto yj = 1 K K k=1 ENC D x j k</formula><p>(2) où x j k est le k-ième exemple le plus proche dans l'espace latent ayant pour classe prédite y j , avec y j = argmin yi∈{1,...,l},yi =y0 ENC(x 0 )proto yi 2 . Ce qui signifie que le prototype choisi est le plus proche de x 0 pour une classe y j = y 0 dans l'espace latent. L'idée est de guider la recherche de contrefactuels autour de proto yj . Notons que proto yj est un prototype abstrait car il est défini dans l'espace latent et non dans l'espace initial des exemples. Cependant, il est toujours possible d'obtenir un prototype dans l'espace des exemples en utilisant DEC D (proto yj ). Définir les prototypes dans l'espace latent est intéressant dans la mesure où les distances dans cet espace capturent mieux la sémantique des exemples <ref type="bibr" target="#b5">(Li et al., 2018;</ref><ref type="bibr">Pawelczyk et al., 2020)</ref>.</p><p>K est un hyperparamètre qui représente le nombre d'exemples utilisés pour calculer les prototypes. Les autres hyperparamètres (c, β, γ, θ), et τ sont les pondérations de la fonction objectif. Pour une description complète des paramètres et de leur impact sur la recherche de contrefactuels, se reporter à Van Looveren et Klaise (2021).</p><p>3 Autoencodeur supervisé pour la génération des contrefactuels</p><p>Dans cette section nous allons présenter notre contribution. La méthode proposée par Van Looveren et Klaise ( <ref type="formula">2021</ref>) qui guide la recherche des contrefactuels avec des prototypes dans l'espace latent permet d'obtenir, en théorie, des contrefactuels qui sont plus proches de la distribution des données d'une classe. Cependant, en pratique, on a pu observer que ce n'est pas toujours le cas, avec des contrefactuels parfois loin de la distribution de leur classe de prédiction. La Figure <ref type="figure" target="#fig_1">1</ref> donne une intuition de ce phénomène, avec un exemple issu de la base MNIST<ref type="foot" target="#foot_0">1</ref> . L'exemple à expliquer ici est prédit comme un 5 par le modèle de classification et le contrefactuel correspondant est prédit comme un 3. On peut observer que le contrefactuel est plus proche de la distribution des données de la classe 5 que de celle des données de sa classe prédite (classe 3). Intuitivement, le contrefactuel « ressemble plus » à un 5 qu'à un 3.</p><p>Les prototypes dans l'espace latent étant des exemples représentatifs de chaque classe, il semble ainsi pertinent de prendre en compte la notion de classe dans la construction de l'espace latent. Nous proposons donc d'apprendre l'espace latent avec un autoencodeur supervisé. Un autoencodeur supervisé est un type d'autoencodeur qui apprend conjointement une tâche d'apprentissage supervisé et une tâche de reconstruction <ref type="bibr" target="#b4">(Le et al., 2018)</ref>. Ce type de réseau possède deux sorties : l'une qui retourne l'exemple reconstruit et l'autre qui retourne la décision associée à l'exemple. La fonction d'erreur globale utilisée pour l'apprentissage est définie comme la somme pondérée des erreurs de classification et de reconstruction : </p><formula xml:id="formula_6">L((f pred , AE), D) = E(f pred , D)</formula><formula xml:id="formula_7">R(AE, D) = 1 n n i=1 AE (x i ) -x i 2 2</formula><p>-L'erreur de classification est définie comme une entropie croisée sur l'ensemble d'apprentissage (D) :</p><formula xml:id="formula_8">E(f pred , D) = 1 n n i=1 l k=1 -1 [yi=k] log ([f pred (x i )] k )</formula><p>λ est l'hyperparamètre qui permet d'ajuster le compromis entre les deux termes. Notre proposition est maintenant d'utiliser cet autoencodeur supervisé pour générer des contrefactuels selon le principe défini dans la Section 2. L'intuition étant que la supervision de l'autoencodeur va produire un espace latent organisé en fonction des classes. Les prototypes correspondent à des exemples moyens dans l'espace latent. L'organisation de ce dernier devrait </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Expérimentations</head><p>L'objectif des expérimentations présentées dans cette section est d'évaluer l'effet de la supervision de l'autoencodeur sur la qualité des contrefactuels générés.</p><p>La première question que l'on se pose est : les contrefactuels générés sont-ils plus interprétables quand ils sont obtenus avec un autoencoder supervisé que non supervisé ? Nous allons donc générer deux ensembles de contrefactuels à partir de la fonction d'optimisation décrite dans la Section 2 : l'un obtenu avec l'utilisation d'un autoencodeur supervisé (Section 3), et l'autre sans supervision de l'autoencodeur que l'on nommera baseline dans la suite de l'article. Les ensembles de contrefactuels seront comparés à l'aide de différentes métriques.</p><p>La deuxième question que l'on se pose est : La supervision d'un autoencodeur permetelle l'obtention d'un espace latent où les exemples sont mieux organisés suivant les classes ? Nous allons pour cela projeter des exemples dans un espace latent de dimension 2, avec et sans supervision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Métriques</head><p>Nous proposons d'évaluer la qualité des contrefactuels produits à l'aide de trois métriques différentes : le gain de prédiction, le réalisme ainsi que l'actionnabilité <ref type="bibr" target="#b9">(Nemirovsky et al., 2020)</ref> -Gain de prédiction : Le gain de prédiction est donné par la différence entre la probabilité prédite du contrefactuel et la probabilité prédite de l'exemple à expliquer, pour la classe du contrefactuel.</p><formula xml:id="formula_9">Gain = [f pred (x cf )] yi -[f pred (x 0 )] yi<label>(4)</label></formula><p>où y i est la classe prédite pour le contrefactuel.</p><p>Le gain de prédiction est borné dans [0, 1], et une valeur plus élevée implique plus de confiance dans le changement de classe du contrefactuel. -Réalisme : Le réalisme correspond à l'erreur de reconstruction du contrefactuel évaluée par un autoencodeur (AE evaluate ).</p><formula xml:id="formula_10">Réalisme = AE evaluate (x cf ) -x cf 2 2</formula><p>(5)</p><p>Une valeur faible indique que le contrefactuel est plus proche de la distribution des données. -Actionnabilité : L'actionnabilité est la distance L 1 entre l'exemple et le contrefactuel.</p><formula xml:id="formula_11">Actionnabilité = x cf -x 0 1 = δ 1 (6)</formula><p>Une faible valeur indique une perturbation plus creuse, ce qui correspond au changement de moins de caractéristiques. On notera que d'autres métriques d'évaluation ont été proposées, en particulier IM1 et IM2 (Van Looveren et Klaise, 2021) qui sont basées sur des erreurs de reconstruction d'autoencodeurs. Plusieurs auteurs ayant souligné le manque de robustesse de ces métriques <ref type="bibr" target="#b3">(Labaien et al., 2021;</ref><ref type="bibr">Schut et al., 2021)</ref>, nous ne les avons pas considérées dans le cadre de cet article. (d) Construire les contrefactuels pour les exemples de l'étape 1c, avec la méthode décrite dans la Section 2. 2. Contrefactuels basés sur un autoencodeur non supervisé (a) Entraîner un nouvel autoencodeur, avec la même architecture que l'autoencodeur supervisé, mais sans la supervision, (i.e. avec une fonction d'erreur de reconstruction uniquement) sur le même ensemble d'apprentissage que précédemment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Comparaison de la qualité des contrefactuels</head><p>(b) Construire les contrefactuels (pour les même exemples de test que dans 1) avec ce nouvel autoencodeur et le classifieur de l'étape 1a. 3. Calculer les métriques sur les deux ensembles de contrefactuels obtenus.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Comparaison des métriques</head><p>Ce protocole expérimental a été évalué sur le jeu de données MNIST pour les métriques présentées dans la Section 4.1. Un échantillon aléatoire de 5, 000 exemples de test a été sélectionné pour la génération des contrefactuels. Les architectures précises des modèles, ainsi que les hyperparamètres sont détaillés en annexe. Les performances relatives à l'apprentissage sont également fournies en annexe. Le Tableau 1 donne les résultats numériques (moyenne et écart type pour chacune des métriques sur les exemples de l'ensemble de test) pour notre approche (autoencodeur supervisé) et la baseline (autoencodeur sans supervision).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Métriques</head><p>Baseline Les résultats numériques montrent que notre méthode permet d'obtenir un meilleur gain de prédiction (la moyenne est supérieure de 0.287 points), avec un réalisme équivalent à la méthode baseline. Un meilleur gain de prédiction signifie plus de confiance dans la classe prédite du contrefactuel. Cependant nous perdons en actionnabilité ce qui signifie que les contrefactuels impliquent en moyenne la modification de plus nombreux pixels. Afin de mieux comprendre ces différences de comportement, il peut être intéressant de regarder ce qui se passe au niveau local, c'est-à-dire pour certains exemples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Comparaison des contrefactuels générés</head><p>La Figure <ref type="figure">2</ref> présente les contrefactuels produits par les deux méthodes pour trois exemples différents. La colonne de gauche montre les exemples à expliquer, les deuxième et troisième colonnes représentent respectivement les contrefactuels obtenus avec un autoencodeur supervisé et la méthode baseline.</p><p>Nous observons globalement plus de changements de pixels dans le cas de l'autoencodeur supervisé. Par exemple, sur la première ligne d'images, un exemple prédit comme un 3 aura un contrefactuel prédit comme un 6 dans les deux cas. Cependant, on observe que plus de pixels ont été modifiés avec la méthode basée sur l'autoencodeur supervisé (avec la création de la boucle qui permet d'obtenir un 6). De plus, le contrefactuel obtient une plus grande probabilité de prédiction pour la classe 6 (0.99 vs 0.41), ce qui conduit à un gain de prédiction plus important. Sur la deuxième ligne, l'exemple à expliquer est prédit comme un 9 et les contrefactuels comme un 5. Nous observons le même comportement que précédemment sur les contrefactuels. Dans le cas de l'autoencodeur supervisé, des pixels sont modifiés dans la boucle du 9 afin d'obtenir un 5 alors que moins de pixels sont modifiés dans le cas de la baseline, ce qui induit moins de changements visuels (le contrefactuel est prédit comme un 5 mais ressemble visuellement à un 9). Dans la dernière ligne, l'exemple est prédit comme un 2 et le contrefactuel produit est prédit comme un 1 dans le cas de l'autoencodeur supervisé, et comme un 8 dans le cas de la baseline. Pour la baseline, la perturbation produit un contrefactuel qui apparaît comme étant visuellement similaire à l'exemple alors que la classe prédite est 8 (avec une probabilité de prédiction de 0.43). Avec l'autoencodeur supervisé, le contrefactuel est FIG. <ref type="figure">2</ref> -Exemples à expliquer de MNIST (colonne de gauche) et les contrefactuels correspondants pour la méthode basée sur l'autoencodeur supervisé (au centre) et la méthode baseline (à droite).</p><p>converti en un 1 avec une probabilité de 0.99. Il a été obtenu par la modification des pixels aux extrémités. Cette expérience confirme notre intuition de départ (cf. Section 3). Dans le cas de l'autoencodeur supervisé, l'espace latent est organisé suivant les classes, avec des clusters de points séparés par classe. On n'observe pas la même configuration avec l'autoencodeur baseline où la représentation ne prend pas en compte les classes : des exemples de classes différentes sont mélangés dans l'espace latent. Nous observons également que, dans le cas de l'autoencodeur supervisé, cela a pour effet de conduire à des prototypes plus représentatifs de chaque classe. En effet, les exemples moyens décodés ont des représentations disjointes contrairement au cas de l'autoencodeur baseline pour lequel la superposition d'exemples de classes différentes dans l'espace latent conduit à des prototypes ayant des représentations similaires.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Impact de la supervision sur l'espace latent</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Bilan global</head><p>L'évaluation du premier protocole montre qu'un meilleur gain de prédiction est associé à une plus grande confiance dans la classe prédite du contrefactuel. Un meilleur gain s'obtient au prix d'une modification d'un plus grand nombre de caractéristiques de l'exemple à expliquer. La visualisation de l'espace latent confirme notre intuition qui est que la supervision permet une organisation suivant les classes cibles dans l'espace latent, ce qui conduit à des prototypes plus représentatifs de chacune des classes. Notre protocole a également été évalué sur un ensemble de données tabulaires numériques (cf. Annexe). Les résultats de l'évaluation du premier protocole sont fournis en annexe. Les conclusions sont globalement similaires à celles obtenues sur les données d'images.</p><p>Cet article a proposé une méthode de génération post-hoc d'explications contrefactuelles. La proposition consiste à introduire un terme basé sur un autoencodeur supervisé dans la fonction de coût qui permet d'obtenir le contrefactuel à partir de l'exemple à expliquer. Ce travail est une extension de celui de Van Looveren et Klaise (2021) avec l'objectif d'améliorer la fidélité des contrefactuels à la distribution des données de la classe cible. Les prototypes sont calculés dans un espace latent organisé suivant les classes, ce qui permet de guider la recherche de contrefactuels vers des prototypes plus représentatifs. La méthode a été évaluée sur un jeu de données d'images par le biais de différentes métriques. On a obtenu des contrefactuels avec une plus grande confiance dans leur classe de prédiction mais au prix d'une modification d'un plus grand nombre de caractéristiques de l'exemple à expliquer. Ces résultats restent à conforter sur plus de jeux de données de différents types.</p><p>Pawelczyk, M., <ref type="bibr">K. Broelemann, et G. Kasneci (2020)</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Annexe</head><p>Cette section décrit les architectures des différents modèles implémentés et donne les valeurs des hyperparamètres utilisés lors des expérimentations.</p><p>Autoencodeur supervisé Cette architecture est composée d'une partie autoencodeur ainsi que de 2 couches denses (couches de classification) à la suite de l'encodeur. La sortie finale est la concaténation de la dernière couche de classification et de la sortie du décodeur. La partie autoencodeur est la même que celle utilisée par <ref type="bibr">Van Looveren et Klaise (2021)</ref>. La partie classification est composée de deux couches denses à la suite de l'encodeur. Cette partie prend en entrée un vecteur d'exemples encodés (par l'encodeur) et est ensuite connectée à une couche dense de taille 128 avec une activation ReLU et une régularisation de type L 1 . Cette couche dense est suivie d'une autre couche dense de taille 10 avec activation softmax. La fonction d'erreur est une somme pondérée de fonctions d'erreur de reconstruction et de classification comme détaillé dans la Section 3. Afin de fixer λ dans l'équation 3, nous avons entraîné notre modèle pour différentes valeurs de λ et choisi le meilleur compromis entre précision et erreur de reconstruction sur le jeu de test. Les résultats de cette expérience sont disponibles dans le Tableau 2, et les meilleures performances sont obtenues pour une valeur de λ = 10. Autoencodeur baseline L'architecture de l'autoencodeur baseline est la même que celle de l'autoencodeur supervisé mais sans les couches de classification. La fonction d'erreur de reconstruction est une erreur quadratique moyenne, l'entraînement est effectué avec 128 exemples par batch, pour 18 époques, avec un optimiseur de type Adam. Notre modèle atteint une erreur de reconstruction de 0.0016 sur le jeu de donnée de test.</p><p>Hyperparamètres pour la construction des contrefactuels Les valeurs d'hyperparamètres sont fixées selon le paramétrage utilisé par Van Looveren et Klaise (2021) (γ = 100, τ = 0, c = 1, β = 0.1, θ = 100, K = 5).</p><p>Autoencodeur pour l'évaluation de métriques Nous avons utilisé la même architecture que <ref type="bibr">Van Looveren et Klaise (2021)</ref>. Le jeu d'entraînement est le même que celui utilisé pour l'entraînement des autoencodeurs baseline et supervisé.</p><p>Expérience sur données tabulaires Nous avons évalué nos résultats sur des données tabulaires numériques obtenues à partir de la fonction make_blobs de sklearn. Nous avons généré est un problème de classification à 6 classes à partir d'une mixture de 6 gaussiennes, chacune ayant un écart-type de 10 (une valeur choisie volontairement élevée afin de tester la capacité de l'autoencodeur supervisé à séparer les classes par rapport à l'autoencodeur baseline). Notre jeu de données contient 50, 000 exemples, dont 30, 000 en apprentissage, 10, 000 en validation et 10, 000 en test. Un échantillon aléatoire de 1, 000 exemples de test a été sélectionné pour la génération des contrefactuels. Les résultats d'évaluation des métriques de la section 4.1 sont reportés dans le Tableau 3. On observe ici également une amélioration du gain de prédiction au détriment de l'actionnabilité pour la solution basée sur l'autoencodeur supervisé. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Métriques</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Baseline</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Summary</head><p>In this work, we investigate the problem of generating counterfactuals explanations that are both close to the data distribution, and to the distribution of the target class. Our objective is to obtain counterfactuals with likely values (i.e. realistic). We propose a method for generating realistic counterfactuals by using class prototypes. The novelty of this approach is that these class prototypes are obtained using a supervised auto-encoder. Then, we performed an empirical evaluation across several interpretability metrics, that shows competitive results with a state-of-the-art method.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>erreur de reconstruction est donnée par la moyenne des distances L 2 sur un ensemble d'apprentissage de n exemples :</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>FIG. 1 -</head><label>1</label><figDesc>FIG. 1 -Exemple original issu de MNIST(à gauche) et contrefactuel correspondant généré par la méthode de Van Looveren et Klaise (2021) (à droite). Le contrefactuel proposé n'est pas un bon représentant de la classe à laquelle il appartient.</figDesc><graphic coords="6,212.60,150.81,170.08,98.10" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Le protocole expérimental peut être résumé à l'aide des étapes listées ci-dessous : 1. Contrefactuels basés sur un autoencodeur supervisé (a) Entraîner un autoencodeur supervisé, sur un ensemble de données d'apprentissage fixé, en minimisant la fonction d'erreur pondérée qui pénalise la reconstruction et la classification (Section 3). (b) Extraire l'autoencodeur et le classifieur correspondant. (c) Selectionner un ensemble d'exemples de test.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>FIG. 3 -Visualisation d'un espace latent de dimension 2 avec le jeu de donnée MNIST</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="10,110.55,150.82,374.17,163.17" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>2 2 représente l'erreur de reconstruction de x cf évaluée par un autoencoder AE entraîné sur D. Cet autoencodeur est composé d'un encodeur (ENC D ) et d'un décodeur (DEC D ) tels que AE D</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Comparaison des résultats pour les différentes métriques. Les flèches indiquent si une valeur faible ↓ ou élevée ↑ implique un meilleur résultat. Les meilleures performances sont indiquées en gras.</figDesc><table><row><cell></cell><cell></cell><cell>Autoencodeur supervisé</cell></row><row><cell>↑ Gain de prédiction</cell><cell>0.552±0.106</cell><cell>0.839±0.160</cell></row><row><cell>↓ Réalisme</cell><cell>0.253±0.010</cell><cell>0.249±0.012</cell></row><row><cell>↓ Actionnabilité</cell><cell>26.174±13.762</cell><cell>38.360 ±18.465</cell></row><row><cell>TAB. 1 -</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>. Learning model-agnostic counterfactual explanations for tabular data. In Proceedings of The Web Conference, pp. 3126-3132.</figDesc><table><row><cell>Schut, L., O. Key, R. McGrath, L. Costabello, B. Sacaleanu, M. Corcoran, et Y. Gal (2021).</cell></row><row><cell>Generating interpretable counterfactual explanations by implicit minimisation of epistemic</cell></row><row><cell>and aleatoric uncertainties. In Proceedings of the International Conference on Artificial</cell></row><row><cell>Intelligence and Statistics (AISTATS), Volume 130, pp. 1756-1764.</cell></row><row><cell>Van Looveren, A. et J. Klaise (2021). Interpretable counterfactual explanations guided by pro-</cell></row><row><cell>totypes. In Proceedings of the European Conference on Machine Learning and Knowledge</cell></row><row><cell>Discovery in Databases (ECML/PKDD), pp. 650-665.</cell></row><row><cell>Wachter, S., B. Mittelstadt, et C. Russell (2018). Counterfactual explanations without opening</cell></row><row><cell>the black box : Automated decisions and the GDPR. Harvard journal of law &amp; techno-</cell></row><row><cell>logy 31, 841-887.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>Précision et erreur de reconstruction pour chaque λ sur le jeu de données de test La fonction d'erreur de classification est une categorical cross-entropy et la fonction d'erreur de reconstruction est une erreur quadratique moyenne. L'entraînement est effectué avec 128 exemples par batch, pour 25 époques, avec un optimiseur de type Adam.</figDesc><table><row><cell>λ</cell><cell>↑ Précision</cell><cell>↓ Erreur de reconstruction</cell></row><row><cell>0.1</cell><cell>0.984</cell><cell>0.0069</cell></row><row><cell>1</cell><cell>0.984</cell><cell>0.0028</cell></row><row><cell>10</cell><cell>0.981</cell><cell>0.0012</cell></row><row><cell>100</cell><cell>0.979</cell><cell>0.0011</cell></row><row><cell>TAB. 2 -</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>Comparaison des résultats pour les différentes métriques des données numériques.</figDesc><table><row><cell></cell><cell></cell><cell>Autoencodeur supervisé</cell></row><row><cell>↑ Gain de prédiction</cell><cell>0.442±0.11</cell><cell>0.736±0.117</cell></row><row><cell>↓ Réalisme</cell><cell>0.021±0.009</cell><cell>0.035±0.056</cell></row><row><cell>↓ Actionnabilité</cell><cell>1.902±0.833</cell><cell>2.851 ±2.699</cell></row><row><cell>TAB. 3 -</cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>http://yann.lecun.com/exdb/mnist/</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A framework and benchmarking study for counterfactual generating methods on tabular data</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M B</forename><surname>De Oliveira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Martens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Sciences</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">16</biblScope>
			<biblScope unit="page">7274</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Explanations based on the missing : Towards contrastive explanations with pertinent negatives</title>
		<author>
			<persName><forename type="first">A</forename><surname>Dhurandhar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>P.-Y. Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-C</forename><surname>Luss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ting</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Shanmugam</surname></persName>
		</author>
		<author>
			<persName><surname>Das</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Neural Information Processing Systems (NIPS)</title>
		<meeting>the International Conference on Neural Information Processing Systems (NIPS)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="590" to="601" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Nonlinear principal component analysis using autoassociative neural networks</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Kramer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AIChE Journal</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="233" to="243" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">DA-DGCEx : Ensuring validity of deep guided counterfactual explanations with distribution-aware autoencoder loss</title>
		<author>
			<persName><forename type="first">J</forename><surname>Labaien</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Zugasti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">De</forename><surname>Carlos</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.09062</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Supervised autoencoders : Improving generalization performance with unsupervised regularizers</title>
		<author>
			<persName><forename type="first">L</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Patterson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>White</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Neural Information Processing Systems (NIPS)</title>
		<meeting>the International Conference on Neural Information Processing Systems (NIPS)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="107" to="117" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep learning for case-based reasoning through prototypes : A neural network that explains its predictions</title>
		<author>
			<persName><forename type="first">O</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Rudin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="3530" to="3537" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Preserving causal constraints in counterfactual explanations for machine learning classifiers</title>
		<author>
			<persName><forename type="first">D</forename><surname>Mahajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sharma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the workshop Microsoft at NIPS -&quot;CausalML : Machine Learning and Causal Inference for Improved Decision Making</title>
		<meeting>the workshop Microsoft at NIPS -&quot;CausalML : Machine Learning and Causal Inference for Improved Decision Making</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Interpretable Machine Learning</title>
		<author>
			<persName><forename type="first">C</forename><surname>Molnar</surname></persName>
		</author>
		<ptr target="https://christophm.github.io/interpretable-ml-book/" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Explaining machine learning classifiers through diverse counterfactual explanations</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">K</forename><surname>Mothilal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Fairness, Accountability, and Transparency (FAT)</title>
		<meeting>the Conference on Fairness, Accountability, and Transparency (FAT)</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="607" to="617" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">CounteRGAN : Generating realistic counterfactuals with residual generative adversarial nets</title>
		<author>
			<persName><forename type="first">D</forename><surname>Nemirovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Thiebaut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.05199</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
