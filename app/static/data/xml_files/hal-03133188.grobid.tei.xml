<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Love Me, Love Me, Say (and Write!) that You Love Me: Enriching the WASABI Song Corpus with Lyrics Annotations</title>
				<funder ref="#_pxt94gW">
					<orgName type="full">Agence Nationale de la Recherche</orgName>
					<orgName type="abbreviated">ANR</orgName>
				</funder>
				<funder ref="#_rDUn8mz">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Michael</forename><surname>Fell</surname></persName>
							<email>michael.fell@unice.fr</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Université Côte d&apos;Azur</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
								<orgName type="institution" key="instit3">Inria</orgName>
								<address>
									<region>I3S</region>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Elena</forename><surname>Cabrio</surname></persName>
							<email>elena.cabrio@unice.fr</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Université Côte d&apos;Azur</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
								<orgName type="institution" key="instit3">Inria</orgName>
								<address>
									<region>I3S</region>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Elmahdi</forename><surname>Korfed</surname></persName>
							<email>elmahdi.korfed@unice.fr</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Université Côte d&apos;Azur</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
								<orgName type="institution" key="instit3">Inria</orgName>
								<address>
									<region>I3S</region>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Michel</forename><surname>Buffa</surname></persName>
							<email>michel.buffa@unice.fr</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Université Côte d&apos;Azur</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
								<orgName type="institution" key="instit3">Inria</orgName>
								<address>
									<region>I3S</region>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Fabien</forename><surname>Gandon</surname></persName>
							<email>fabien.gandon@inria.fr</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Université Côte d&apos;Azur</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
								<orgName type="institution" key="instit3">Inria</orgName>
								<address>
									<region>I3S</region>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Love Me, Love Me, Say (and Write!) that You Love Me: Enriching the WASABI Song Corpus with Lyrics Annotations</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">8A940341C06661F7A9E8D1667EBE4BAE</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-04-12T14:54+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Corpus (Creation</term>
					<term>Annotation</term>
					<term>etc.)</term>
					<term>Information Extraction</term>
					<term>Information Retrieval</term>
					<term>Music and Song Lyrics</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present the WASABI Song Corpus, a large corpus of songs enriched with metadata extracted from music databases on the Web, and resulting from the processing of song lyrics and from audio analysis. More specifically, given that lyrics encode an important part of the semantics of a song, we focus here on the description of the methods we proposed to extract relevant information from the lyrics, such as their structure segmentation, their topics, the explicitness of the lyrics content, the salient passages of a song and the emotions conveyed. The creation of the resource is still ongoing: so far, the corpus contains 1.73M songs with lyrics (1.41M unique lyrics) annotated at different levels with the output of the above mentioned methods. Such corpus labels and the provided methods can be exploited by music search engines and music professionals (e.g. journalists, radio presenters) to better handle large collections of lyrics, allowing an intelligent browsing, categorization and recommendation of songs. We provide the files of the current version of the WASABI Song Corpus, the models we have built on it as well as updates here: https://github.com/micbuffa/WasabiDataset.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Let's imagine the following scenario: following David Bowie's death, a journalist plans to prepare a radio show about the artist's musical career to acknowledge his qualities. To discuss the topic from different angles, she needs to have at her disposal the artist biographical information to know the history of his career, the song lyrics to know what he was singing about, his musical style, the emotions his songs were conveying, live recordings and interviews. Similarly, streaming professionals such as Deezer, Spotify, Pandora or Apple Music aim at enriching music listening with artists' information, to offer suggestions for listening to other songs/albums from the same or similar artists, or automatically determining the emotion felt when listening to a track to propose coherent playlists to the user. To support such scenarios, the need for rich and accurate musical knowledge bases and tools to explore and exploit this knowledge becomes evident. In this paper, we present the WASABI Song Corpus, a large corpus of songs (2.10M songs, 1.73M with lyrics) enriched with metadata extracted from music databases on the Web, and resulting from the processing of song lyrics and from audio analysis. The corpus contains songs in 36 different languages, even if the vast majority are in English. As for the songs genres, the most common ones are Rock, Pop, Country and Hip Hop. More specifically, while an overview of the goals of the WASABI project supporting the dataset creation and the description of a preliminary version of the dataset can be found in <ref type="bibr" target="#b48">(Meseguer-Brocal et al., 2017)</ref>, this paper focuses on the description of the methods we proposed to annotate relevant information in the song lyrics. Given that lyrics encode an important part of the semantics of a song, we propose to label the WASABI dataset lyrics with their structure segmentation, the explicitness of the lyrics content, the salient passages of a song, the addressed topics and the emotions conveyed.</p><p>An analysis of the correlations among the above mentioned annotation layers reveals interesting insights about the song corpus. For instance, we demonstrate the change in corpus annotations diachronically: we show that certain topics become more important over time and others are diminished.</p><p>We also analyze such changes in explicit lyrics content and expressed emotion. The paper is organized as follows. Section 2. introduces the WASABI Song Corpus and the metadata initially extracted from music databases on the Web. Section 3. describes the segmentation method we applied to decompose lyrics in their building blocks in the corpus. Section 4. explains the method used to summarize song lyrics, leveraging their structural properties. Section 5. reports on the annotations resulting from the explicit content classifier, while Section 6. describes how information on the emotions are extracted from the lyrics. Section 7. describes the topic modeling algorithm to label each lyrics with the top 10 words, while Section 8. examines the changes in the annotations over the course of time. Section 9. reports on similar existing resources, while Section 10. concludes the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">The WASABI Song Corpus</head><p>In the context of the WASABI research project<ref type="foot" target="#foot_0">1</ref> that started in 2017, a two million song database has been built, with metadata on 77k artists, 208k albums, and 2.10M songs <ref type="bibr" target="#b48">(Meseguer-Brocal et al., 2017)</ref>. The metadata has been i) aggregated, merged and curated from different data sources on the Web, and ii) enriched by pre-computed or ondemand analyses of the lyrics and audio data. We have performed various levels of analysis, and interactive Web Audio applications have been built on top of the output. For example, the TimeSide analysis and annotation framework have been linked <ref type="bibr" target="#b23">(Fillon et al., 2014)</ref> to make on-demand audio analysis possible. In connection with the FAST project<ref type="foot" target="#foot_1">2</ref> , an offline chord analysis of 442k songs has been performed, and both an online enhanced audio player <ref type="bibr" target="#b35">(Pauwels and Sandler, 2019)</ref> and chord search engine <ref type="bibr" target="#b36">(Pauwels et al., 2018)</ref> have been built around it. A rich set of Web Audio applications and plugins has been proposed <ref type="bibr">(Buffa and Lebrun, 2017a;</ref><ref type="bibr" target="#b9">Buffa and Lebrun, 2017b;</ref><ref type="bibr" target="#b10">Buffa et al., 2018)</ref>, that allow, for example, songs to be played along with sounds similar to those used by artists. All these metadata, computational analyses and Web Audio applications have now been gathered in one easy-to-use web interface, the WASABI Interactive Navigator<ref type="foot" target="#foot_2">3</ref> , illustrated<ref type="foot" target="#foot_3">4</ref> in Figure <ref type="figure" target="#fig_0">1</ref>. We have started building the WASABI Song Corpus by collecting for each artist the complete discography, band members with their instruments, time line, equipment they use, and so on. For each song we collected its lyrics from LyricWiki<ref type="foot" target="#foot_4">5</ref> , the synchronized lyrics when available<ref type="foot" target="#foot_5">6</ref> , the DBpedia abstracts and the categories the song belongs to, e.g. genre, label, writer, release date, awards, producers, artist and band members, the stereo audio track from Deezer, the unmixed audio tracks of the song, its ISRC, bpm and duration. We matched the song ids from the WASABI Song Corpus with the ids from MusicBrainz, iTunes, Discogs, Spo- tify, Amazon, AllMusic, GoHear, YouTube. Figure <ref type="figure" target="#fig_1">2</ref> illustrates<ref type="foot" target="#foot_6">7</ref> all the data sources we have used to create the WASABI Song Corpus. We have also aligned the WASABI Song Corpus with the publicly available LastFM dataset<ref type="foot" target="#foot_7">8</ref> , resulting in 327k tracks in our corpus having a LastFM id. As of today, the corpus contains 1.73M songs with lyrics (1.41M unique lyrics). 73k songs have at least an abstract on DBpedia, and 11k have been identified as "classic songs" (they have been number one, or got a Grammy award, or have lots of cover versions). About 2k songs have a multi-track audio version, and on-demand source separation using open-unmix <ref type="bibr" target="#b39">(Stöter et al., 2019)</ref> or Spleeter <ref type="bibr" target="#b24">(Hennequin et al., 2019)</ref> is provided as a TimeSide plugin. Several Natural Language Processing methods have been applied to the lyrics of the songs included in the WASABI Song Corpus, as well as various analyses of the extracted information have been carried out. After providing some statistics on the WASABI corpus, the rest of the article describes the different annotations we added to the lyrics of the songs in the dataset. Based on the research we have conducted, the following lyrics annotations are added: lyrical structure (Section 3.), summarization (Section 4.), explicit lyrics (Section 5.), emotion in lyrics (Section 6.) and topics in lyrics (Section 7.).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Statistics on the WASABI Song Corpus</head><p>This section summarizes key statistics on the corpus, such as the language and genre distributions, the songs coverage in terms of publication years, and then gives the technical details on its accessibility.</p><p>Language Distribution Figure <ref type="figure">3a</ref> shows the distribution of the ten most frequent languages in our corpus. <ref type="foot" target="#foot_8">9</ref> In to- The vast majority (76.1%) is English, followed by Spanish (6.3%) and by four languages in the 2-3% range (German, French, Italian, Portugese). On the bottom end, Swahili and Latin amount to 0.1% (around 2k songs) each. Genre Distribution In Figure <ref type="figure">3b</ref> we depict the distribution of the ten most frequent genres in the corpus. <ref type="foot" target="#foot_9">10</ref> In total, 1.06M of the titles are tagged with a genre. It should be noted that the genres are very sparse with a total of 528 different ones. This high number is partially due to many subgenres such as Alternative Rock, Indie Rock, Pop Rock, etc. which we omitted in Figure <ref type="figure">3b</ref> for clarity. The most common genres are Rock (9.7%), Pop (8.6%), Coun-try (5.2%), Hip Hop (4.5%) and Folk (2.7%).</p><p>Publication Year Figure <ref type="figure">3c</ref> shows the number of songs published in our corpus, by decade. <ref type="foot" target="#foot_10">11</ref> We find that over 50% of all songs in the WASABI Song Corpus are from the 2000s or later and only around 10% are from the seventies or earlier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Accessibility of the WASABI Song Corpus</head><p>The WASABI Interactive Navigator relies on multiple database engines: it runs on a MongoDB server altogether with an indexation by Elasticsearch and also on a Virtuoso triple store as a RDF graph database. It comes with a REST API <ref type="foot" target="#foot_11">12</ref>and an upcoming SPARQL endpoint. All the database metadata is publicly available<ref type="foot" target="#foot_12">13</ref> under a CC licence through the WASABI Interactive Navigator as well as programmatically through the WASABI REST API. We provide the files of the current version of the WASABI Song Corpus, the models we have built on it as well as updates here: https://github.com/micbuffa/ WasabiDataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Lyrics Structure Annotations</head><p>Generally speaking, lyrics structure segmentation consists of two stages: text segmentation to divide lyrics into segments, and semantic labelling to label each segment with a structure type (e.g. Intro, Verse, Chorus). In <ref type="bibr" target="#b19">(Fell et al., 2018)</ref> we proposed a method to segment lyrics based on their repetitive structure in the form of a self-similarity matrix (SSM). Figure <ref type="figure" target="#fig_3">4</ref> shows a line-based SSM for the song text written on top of it<ref type="foot" target="#foot_13">14</ref> . The lyrics consists of seven segments and shows the typical repetitive structure of a Pop song. The main diagonal is trivial, since each line is maximally similar to itself. Notice further the additional diagonal stripes in segments 2, 4 and 7; this indicates a repeated part, typically the chorus. Based on the simple idea that eyeballing an SSM will reveal (parts of) a song's structure, we proposed a Convolutional Neural Network architecture that successfully learned to predict segment borders in the lyrics when "looking at" their SSM. Table <ref type="table" target="#tab_0">1</ref> shows the genre-wise results we obtained using our proposed architecture. One important insight was that more repetitive lyrics as often found in genres such as Country and Punk Rock are much easier to segment than lyrics in Rap or Hip Hop which often do not even contain a chorus. In the WASABI Interactive Navigator, the line-based SSM of a song text can be visualized. It is toggled by clicking on the violet-blue square on top of the song text. For a subset of songs the color opacity indicates how repetitive and representative a segment is, based on the fitness metric that we proposed in <ref type="bibr">(Fell et al., 2019b)</ref>. Note how in Figure <ref type="figure" target="#fig_3">4</ref>,  the segments 2, 4 and 7 are shaded more darkly than the surrounding ones. As highly fit (opaque) segments often coincide with a chorus, this is a first approximation of chorus detection. Given the variability in the set of structure types provided in the literature according to different genres <ref type="bibr" target="#b40">(Tagg, 1982;</ref><ref type="bibr" target="#b7">Brackett, 1995)</ref>, rare attempts have been made in the literature to achieve a more complete semantic labelling, labelling the lyrics segments as Intro, Verse, Bridge, Chorus etc.</p><formula xml:id="formula_0">Genre P R</formula><p>For each song text we provide an SSM based on a normalized character-based edit distance 15 on two levels of granu- 15 In our segmentation experiments we found this simple metric to outperform more complex metrics that take into account the </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Lyrics Summary</head><p>Given the repeating forms, peculiar structure and other unique characteristics of song lyrics, in <ref type="bibr">(Fell et al., 2019b)</ref> we introduced a method for extractive summarization of lyrics that takes advantage of these additional elements to more accurately identify relevant information in song lyrics. More specifically, it relies on the intimate relationship between the audio and the lyrics. The so-called audio thumbnails, snippets of usually 30 seconds of music, are a popular means to summarize a track in the audio community. The intuition is the more repeated and the longer a part, the better it represents the song. We transferred an audio thumbnailing approach to our domain of lyrics and showed that adding the thumbnail improves summary quality. We evaluated our method on 50k lyrics belonging to the top 10 genres of the WASABI Song Corpus and according to qualitative criteria such as Informativeness and Coherence. Figure <ref type="figure" target="#fig_4">5</ref> shows our results for different summarization models. Our model RankTopicFit, which combines graph-based, topic-based and thumbnail-based summarization, outperforms all other summarizers. We further find that the genres RnB and Country are highly overrepresented in the lyrics sample with respect to the full WASABI Song Corpus, indicating that songs belonging to these genres are more likely to contain a chorus. Finally, Figure <ref type="figure" target="#fig_5">6</ref> shows an example summary of four lines length obtained with our proposed RankTopicFit method. It is toggled in the WASABI Interactive Navigator by clicking on the green square on top of the song text. The four-line summaries of 50k English used in our experiments are freely available within the WASABI Song Corpus; the Python code of the applied summarization methods is also available 16 .</p><p>phonetics or the syntax.</p><p>16 https://github.com/TuringTrain/lyrics_ thumbnailing </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Explicit Language in Lyrics</head><p>On audio recordings, the Parental Advisory Label is placed in recognition of profanity and to warn parents of material potentially unsuitable for children. Nowadays, such labelling is carried out mainly manually on voluntary basis, with the drawbacks of being time consuming and therefore costly, error prone and partly a subjective task. In <ref type="bibr">(Fell et al., 2019a)</ref> we have tackled the task of automated explicit lyrics detection, based on the songs carrying such a label. We compared automated methods ranging from dictionarybased lookup to state-of-the-art deep neural networks to automatically detect explicit contents in English lyrics. More specifically, the dictionary-based methods rely on a swear word dictionary D n which is automatically created from example explicit and clean lyrics. Then, we use D n to predict the class of an unseen song text in one of two ways: (i) the Dictionary Lookup simply checks if a song text contains words from D n . (ii) the Dictionary Regression uses BOW made from D n as the feature set of a logistic regression classifier. In the Tf-idf BOW Regression the BOW is expanded to the whole vocabulary of a training sample instead of only the explicit terms. Furthermore, the model TDS Deconvolution is a deconvolutional neural network <ref type="bibr" target="#b41">(Vanni et al., 2018)</ref> that estimates the importance of each word of the input for the classifier decision. In our experiments, we worked with 179k lyrics that carry gold labels provided by Deezer (17k tagged as explicit) and obtained the results shown in Figure <ref type="figure" target="#fig_1">2</ref>. We found the very simple Dictionary Lookup method to perform on par with much more complex models such as the BERT Language Model <ref type="bibr" target="#b17">(Devlin et al., 2018)</ref> as a text classifier. Our analysis revealed that some genres are highly overrepresented among the explicit lyrics.</p><p>Inspecting the automatically induced explicit words dictionary reflects that genre bias. The dictionary of 32 terms used for the dictionary lookup method consists of around 50% of terms specific to the Rap genre, such as glock, gat, clip (gun-related), thug, beef, gangsta, pimp, blunt (crime and drugs). Finally, the terms holla, homie, and rapper are obviously no swear words, but highly correlated with explicit content lyrics.</p><p>Our corpus contains 52k tracks labelled as explicit and 663k clean (not explicit) tracks 17 . We have trained a classifier (77.3% f-score on test set) on the 438k English lyrics which are labelled and classified the remaining 455k previously untagged English tracks. We provide both the pre-17 Labels provided by Deezer. Furthermore, 625k songs have a different status such as unknown or censored version.  dicted labels in the WASABI Song Corpus and the trained classifier to apply it to unseen text.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Emotional Description</head><p>In sentiment analysis the task is to predict if a text has a positive or a negative emotional valence. In the recent years, a transition from detecting sentiment (positive vs. negative valence) to more complex formulations of emotion detection (e.g. joy, fear, surprise) <ref type="bibr" target="#b32">(Mohammad et al., 2018)</ref> has become more visible; even tackling the problem of emotion in context <ref type="bibr" target="#b14">(Chatterjee et al., 2019)</ref>. One family of emotion detection approaches is based on the valence-arousal model of emotion <ref type="bibr" target="#b37">(Russell, 1980)</ref>, locating every emotion in a two-dimensional plane based on its valence (positive vs. negative) and arousal (aroused vs. calm). <ref type="foot" target="#foot_14">18</ref> Figure <ref type="figure" target="#fig_7">7</ref> is an illustration of the valence-arousal model of Russell and shows exemplary where several emotions such as joyful, angry or calm are located in the plane. Manually labelling texts with multi-dimensional emotion descriptions is an inherently hard task. Therefore, researchers have resorted to distant supervision, obtaining gold labels from social tags from lastfm. These approaches <ref type="bibr">(Hu et al., 2009a</ref>; C ¸ano and Morisio, May 2017) define a list of social tags that are related to emotion, then project them into the valence-arousal space using an emotion lexicon <ref type="bibr" target="#b42">(Warriner et al., 2013;</ref><ref type="bibr" target="#b33">Mohammad, 2018)</ref>. Recently, Deezer made valence-arousal annotations for 18,000 English tracks available 19 they have derived by the aforementioned method <ref type="bibr" target="#b16">(Delbouys et al., 2018)</ref>. We aligned the valence-arousal annotations of Deezer to our songs. In Figure <ref type="figure" target="#fig_7">7</ref> the green dots visualize the emotion distribution of these songs. 20 Based on their annotations, we train an emotion regression model using BERT, with an evaluated 0.44/0.43 Pearson correlation/Spearman correlation for valence and 0.33/0.31 for arousal on the test set. We integrated Deezer's labels into our corpus and also provide the valence-arousal predictions for the 1.73M tracks with lyrics. We also provide the last.fm social tags (276k) and emotion tags (87k entries) to facilitate researchers to build variants of emotion recognition models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Topic Modelling</head><p>We built a topic model on the lyrics of our corpus using Latent Dirichlet Allocation (LDA) <ref type="bibr" target="#b6">(Blei et al., 2003)</ref>. We determined the hyperparameters α, η and the topic count such that the coherence was maximized on a subset of 200k lyrics. We then trained a topic model of 60 topics on the unique English lyrics (1.05M). We have manually labelled a number of more recognizable topics. Figures 9-13 illustrate these topics with word clouds 21 of the most characteristic words per topic. For instance, the topic Money contains words of both the field of earning money <ref type="bibr">(job, work, boss, sweat)</ref> as well as spending it (pay, buy). The topic Family is both about the people of the family (mother, daughter, wife) and the land (sea, valley, tree). We provide the topic distribution of our LDA topic model for each song and make available the trained topic model to enable its application to unseen lyrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Diachronic Corpus Analysis</head><p>We examine the changes in the annotations over the course of time by grouping the corpus into decades of songs according to the distribution shown in Figure <ref type="figure">3c</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Changes in Topics</head><p>The importance of certain topics has changed over the decades, as depicted in Figure <ref type="figure" target="#fig_3">14a</ref>. Some topics have become more important, others have declined, or stayed relatively the same. We define the importance of a topic for a decade of songs as follows: first, the LDA topic model trained on the full corpus gives the probability of the topic for each song separately. We then average these songwise probabilities over all songs of the decade. For each of the cases of growing, diminishing and constant importance, we display two topics. The topics War and Death have appreciated in importance over time. This is partially caused by the rise of Heavy Metal in the beginning of the 1970s, as the vocabulary of the Death topic is very typical for the   <ref type="bibr" target="#b18">(Fell and Sporleder, 2014)</ref>). We measure a decline in the importance of the topics Love and Family. The topics Money and Religion seem to be evergreens as their importance stayed rather constant over time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Changes in Explicitness</head><p>We find that newer songs are more likely being tagged as having explicit content lyrics.</p><p>Figure <ref type="figure" target="#fig_3">14b</ref> shows our estimates of explicitness per decade, the ratio of songs in the decade tagged as explicit to all songs of the decade. Note that the Parental Advisory Label was first distributed in 1985 and many older songs may not have been labelled retroactively. The depicted evolution of explicitness may therefore overestimate the "true explicitness" of newer music and underestimate it for music before 1985.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Changes in Emotion</head><p>We estimate the emotion of songs in a decade as the average valence and arousal of songs of that decade. We find songs to decrease both in valence and arousal over time. This decrease in positivity (valence) is in line with the diminishment of positively connotated topics such as Love and Family and the appreciation of topics with a more negative connotation such as War and Death.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.">Related Work</head><p>This section describes available songs and lyrics databases, and summarizes existing work on lyrics processing. with MSD tracks. However, no other processing of the lyrics is done, as is the case in our work. MusicWeb and its successor MusicLynx <ref type="bibr" target="#b1">(Allik et al., 2018)</ref> link music artists within a Web-based application for discovering connections between them and provides a browsing experience using extra-musical relations. The project shares some ideas with WASABI, but works on the artist level, and does not perform analyses on the audio and lyrics content itself. It reuses, for example, MIR metadata from AcousticBrainz.</p><p>The WASABI project has been built on a broader scope than these projects and mixes a wider set of metadata, including ones from audio and natural language processing of lyrics. In addition, as presented in this paper, it comes with a large set of Web Audio enhanced applications (multitrack player, online virtual instruments and effect, on-demand audio processing, audio player based on extracted, synchronized chords, etc.) Companies such as Spotify, GraceNote, Pandora, or Apple Music have sophisticated private knowledge bases of songs and lyrics to feed their search and recommendation algorithms, but such data are not available (and mainly rely on audio features).</p><p>Lyrics Segmentation. Only a few papers in the literature have focused on the automated detection of the structure of lyrics. <ref type="bibr" target="#b43">(Watanabe et al., 2016)</ref> propose the task to automatically identify segment boundaries in lyrics and train a logistic regression model for the task with the repeated pattern and textual features. <ref type="bibr" target="#b30">(Mahedero et al., 2005)</ref> report experiments on the use of standard NLP tools for the analysis of music lyrics. Among the tasks they address, for structure extraction they focus on a small sample of lyrics having a clearly recognizable structure (which is not always the case) divided into segments. More recently, <ref type="bibr" target="#b3">(Baratè et al., 2013)</ref> describe a semantics-driven approach to the automatic segmentation of song lyrics, and mainly focus on pop/rock music. Their goal is not to label a set of lines in a given way (e.g. verse, chorus), but rather identifying recurrent as well as non-recurrent groups of lines. They propose a rule-based method to estimate such structure labels of segmented lyrics.</p><p>Explicit Content Detection. <ref type="bibr" target="#b4">(Bergelid, 2018)</ref> consider a dataset of English lyrics to which they apply classical machine learning algorithms. The explicit labels are obtained from Soundtrack Your Brand<ref type="foot" target="#foot_16">24</ref> . They also experiment with adding lyrics metadata to the feature set, such as the artist name, the release year, the music energy level, and the valence/positiveness of a song. <ref type="bibr" target="#b15">(Chin et al., 2018)</ref> apply explicit lyrics detection to Korean song texts. They also use tf-idf weighted BOW as lyrics representation and aggregate multiple decision trees via boosting and bagging to classify the lyrics for explicit content. More recently, <ref type="bibr" target="#b27">(Kim and Mun, 2019)</ref> proposed a neural network method to create explicit words dictionaries automatically by weighting a vocabulary according to all words' frequencies in the explicit class vs. the clean class, accordingly. They work with a corpus of Korean lyrics.</p><p>Emotion Recognition Recently, <ref type="bibr" target="#b16">(Delbouys et al., 2018)</ref> address the task of multimodal music mood prediction based on the audio signal and the lyrics of a track. They propose a new model based on deep learning outperforming traditional feature engineering based approaches. Performances are evaluated on their published dataset with associated valence and arousal values which we introduced in Section 6. <ref type="bibr" target="#b44">(Xia et al., 2008)</ref> model song texts in a low-dimensional vector space as bags of concepts, the "emotional units"; those are combinations of emotions, modifiers and negations. <ref type="bibr" target="#b45">(Yang and Lee, 2009)</ref> leverage the music's emotion annotations from Allmusic which they map to a lower dimensional psychological model of emotion. They train a lyrics emotion classifier and show by qualitative interpretation of an ablated model (decision tree) that the deciding features leading to the classes are intuitively plausible. <ref type="bibr" target="#b26">(Hu et al., 2009b)</ref>  Topic Modelling Among the works addressing this task for song lyrics, <ref type="bibr" target="#b30">(Mahedero et al., 2005)</ref> define five ad hoc topics (Love, Violent, Antiwar, Christian, Drugs) into which they classify their corpus of 500 song texts using supervision. Related, <ref type="bibr" target="#b22">(Fell, 2014</ref>) also use supervision to find bags of genre-specific n-grams. Employing the view from the literature that BOWs define topics, the genre-specific terms can be seen as mixtures of genre-specific topics. <ref type="bibr" target="#b29">(Logan et al., 2004)</ref> apply the unsupervised topic model Probabilistic LSA to their ca. 40k song texts. They learn latent topics for both the lyrics corpus as well as a NYT newspaper corpus (for control) and show that the domainspecific topics slightly improve the performance in their MIR task. While their MIR task performs highly better when using acoustic features, they discover that both methods err differently. <ref type="bibr" target="#b28">(Kleedorfer et al., 2008)</ref> apply Nonnegative Matrix Factorization (NMF) to ca. 60k song texts and cluster them into 60 topics. They show the so discovered topics to be intrinsically meaningful. <ref type="bibr" target="#b38">(Sterckx, 2014)</ref> have worked on topic modelling of a large-scale lyrics corpus of 1M songs. They build models using Latent Dirichlet allocation with topic counts between 60 and 240 and show that the 60 topics model gives a good trade-off between topic coverage and topic redundancy. Since popular topic models such as LDA represent topics as weighted bags of words, these topics are not immediately interpretable. This gives rise to the need of an automatic labelling of topics with smaller labels. A recent approach <ref type="bibr" target="#b5">(Bhatia et al., 2016)</ref> relates the topical BOWs with titles of Wikipedia articles in a two step procedure: first, candidates are generated, then ranked.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10.">Conclusion</head><p>In this paper we have described the WASABI dataset of songs, focusing in particular on the lyrics annotations resulting from the applications of the methods we proposed to extract relevant information from the lyrics. So far, lyrics annotations concern their structure segmentation, their topic, the explicitness of the lyrics content, the summary of a song and the emotions conveyed. Some of those annotation layers are provided for all the 1.73M songs included in the WASABI corpus, while some others apply to subsets of the corpus, due to various constraints described in the paper. As the creation of the resource is still ongoing, we plan to integrate an improved emotional description in future work.</p><p>In <ref type="bibr" target="#b2">(Atherton and Kaneshiro, 2016)</ref> the authors have studied how song writers influence each other. We aim to learn a model that detects the border between heavy influence and plagiarism.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The WASABI Interactive Navigator.</figDesc><graphic coords="3,52.16,69.16,234.85,323.44" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The datasources connected to the WASABI Song Corpus.</figDesc><graphic coords="3,304.87,69.17,234.84,237.32" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Figure 3: Statistics on the WASABI Song Corpus</figDesc><graphic coords="4,52.16,424.17,234.86,122.17" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Structure of the lyrics of "Everytime" by Britney Spears as displayed in the WASABI Interactive Navigator.</figDesc><graphic coords="5,52.16,354.38,234.85,218.54" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Human ratings per summarization model (five point Likert scale). Models are Rank: graph-based, Topic: topic-based, Fit: thumbnail-based, and model combinations.</figDesc><graphic coords="5,304.87,69.16,234.85,120.38" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Summary of the lyrics of "Everytime" by Britney Spears as displayed in the WASABI Interactive Navigator.</figDesc><graphic coords="6,84.54,69.16,170.09,94.88" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>Performance comparison of our different models. Precision (P), Recall (R) and f-score (F 1 ) in %.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Emotion distribution in the corpus in the valencearousal plane.</figDesc><graphic coords="6,304.87,199.96,234.84,234.55" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>19 https://github.com/deezer/deezer_mood_ detection_dataset 20 Depiction without scatterplot taken from (Parisi et al., 2019) 21 made with https://www.wortwolken.com/</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure</head><label></label><figDesc>Figure 8: Topic War Figure 9: Topic Death</figDesc><graphic coords="7,422.29,69.16,117.43,77.45" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>Figure 14: Evolution of different annotations during the decades</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table><row><cell></cell><cell>F 1</cell></row><row><cell>Rock</cell><cell>73.8 57.7 64.8</cell></row><row><cell>Hip Hop</cell><cell>71.7 43.6 54.2</cell></row><row><cell>Pop</cell><cell>73.1 61.5 66.6</cell></row><row><cell>RnB</cell><cell>71.8 60.3 65.6</cell></row><row><cell>Alternative Rock</cell><cell>76.8 60.9 67.9</cell></row><row><cell>Country</cell><cell>74.5 66.4 70.2</cell></row><row><cell>Hard Rock</cell><cell>76.2 61.4 67.7</cell></row><row><cell>Pop Rock</cell><cell>73.3 59.6 65.8</cell></row><row><cell>Indie Rock</cell><cell>80.6 55.5 65.6</cell></row><row><cell>Heavy Metal</cell><cell>79.1 52.1 63.0</cell></row><row><cell cols="2">Southern Hip Hop 73.6 34.8 47.0</cell></row><row><cell>Punk Rock</cell><cell>80.7 63.2 70.9</cell></row><row><cell cols="2">Alternative Metal 77.3 61.3 68.5</cell></row><row><cell>Pop Punk</cell><cell>77.3 68.7 72.7</cell></row><row><cell>Gangsta Rap</cell><cell>73.6 35.2 47.7</cell></row><row><cell>Soul</cell><cell>70.9 57.0 63.0</cell></row></table><note><p>Lyrics segmentation performances across musical genres in terms of Precision (P), Recall (R) and F 1 in %. Underlined are the performances on genres with less repetitive text. Genres with highly repetitive structure are in bold.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>aim to detect emotions in song texts based on Russell's model of mood; rendering emotions continuously in the two dimensions of arousal and valence (positive/negative). They analyze each sentence as bag of "emotional units"; they reweight sentences' emotions by both adverbial modifiers and tense and even consider progressing and adversarial valence in consecutive sentences. Additionally, singing speed is taken into account. With the fully weighted sentences, they perform clustering in the 2D plane of valence and arousal. Although the method is unsupervised at runtime, there are many parameters tuned manually by the authors in this work.</figDesc><table><row><cell>(Mihalcea and Strapparava, 2012) render emotion detection</cell></row><row><cell>as a multi-label classification problem, songs express inten-</cell></row><row><cell>sities of six different basic emotions: anger, disgust, fear,</cell></row><row><cell>joy, sadness, surprise. Their corpus (100 song texts) has</cell></row><row><cell>time-aligned lyrics with information on musical key and</cell></row><row><cell>note progression. Using Mechanical Turk they each line</cell></row><row><cell>of song text is annotated with the six emotions. For emo-</cell></row><row><cell>tion classification, they use bags of words and concepts, as</cell></row><row><cell>musical features key and notes. Their classification results</cell></row><row><cell>using both modalities, textual and audio features, are sig-</cell></row><row><cell>nificantly improved compared to a single modality.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Table 3 summarizes the most relevant annotations in our corpus. Most relevant song-wise annotations in the WASABI Song Corpus. Annotations with p are predictions of our models.</figDesc><table><row><cell>Annotation</cell><cell cols="2">Labels Description</cell></row><row><cell>Lyrics</cell><cell cols="2">1.73M segments of lines of text</cell></row><row><cell>Languages</cell><cell cols="2">1.73M 36 different ones</cell></row><row><cell>Genre</cell><cell cols="2">1.06M 528 different ones</cell></row><row><cell>Last FM id</cell><cell>326k</cell><cell>UID</cell></row><row><cell>Structure</cell><cell cols="2">1.73M SSM ∈ R n×n (n: length)</cell></row><row><cell>Social tags</cell><cell>276k</cell><cell>S = {rock, joyful, 90s, ...}</cell></row><row><cell>Emotion tags</cell><cell>87k</cell><cell>E ⊂ S = {joyful, tragic, ...}</cell></row><row><cell>Explicitness</cell><cell>715k</cell><cell>True (52k), False (663k)</cell></row><row><cell cols="2">Explicitness p 455k</cell><cell>True (85k), False (370k)</cell></row><row><cell>Summary p</cell><cell>50k</cell><cell>four lines of song text</cell></row><row><cell>Emotion</cell><cell>16k</cell><cell>(valence, arousal) ∈ R 2</cell></row><row><cell>Emotion p</cell><cell cols="2">1.73M (valence, arousal) ∈ R 2</cell></row><row><cell>Topics p</cell><cell cols="2">1.05M Prob. distrib. ∈ R 60</cell></row><row><cell>Total tracks</cell><cell cols="2">2.10M diverse metadata</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>http://wasabihome.i3s.unice.fr/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>http://www.semanticaudio.ac.uk</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>http://wasabi.i3s.unice.fr/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"><p>Illustration taken from(Buffa et al., 2019a).</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4"><p>http://lyrics.wikia.com/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5"><p>from http://usdb.animux.de/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_6"><p>Illustration taken from<ref type="bibr" target="#b12">(Buffa et al., 2019b)</ref>.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_7"><p>http://millionsongdataset.com/lastfm/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_8"><p>Based on language detection performed on the lyrics.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10" xml:id="foot_9"><p>We take the genre of the album as ground truth since songwise genres are much rarer.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="11" xml:id="foot_10"><p>We take the album publication date as proxy since song-wise labels are too sparse.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="12" xml:id="foot_11"><p>https://wasabi.i3s.unice.fr/apidoc/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="13" xml:id="foot_12"><p>There is no public access to copyrighted data such as lyrics and full length audio files. Instructions on how to obtain lyrics are nevertheless provided and audio extracts of 30s length are available for nearly all songs.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="14" xml:id="foot_13"><p>14 https://wasabi.i3s.unice.fr/#/search/ artist/Britney%20Spears/album/In%20The% 20Zone/song/Everytime</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="18" xml:id="foot_14"><p>Sometimes, a third dimension of dominance is part of the model.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="22" xml:id="foot_15"><p>http://millionsongdataset.com</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="24" xml:id="foot_16"><p>https://www.soundtrackyourbrand.com</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgement</head><p>This work is partly funded by the <rs type="funder">French Research National Agency (ANR)</rs> under the <rs type="projectName">WASABI</rs> project (contract <rs type="grantNumber">ANR-16-CE23-0017-01</rs>) and by the <rs type="programName">EU Horizon 2020 research and innovation programme</rs> under the <rs type="grantName">Marie Sklodowska-Curie grant</rs> agreement No. <rs type="grantNumber">690974</rs> (MIREL).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_pxt94gW">
					<idno type="grant-number">ANR-16-CE23-0017-01</idno>
					<orgName type="grant-name">Marie Sklodowska-Curie grant</orgName>
					<orgName type="project" subtype="full">WASABI</orgName>
					<orgName type="program" subtype="full">EU Horizon 2020 research and innovation programme</orgName>
				</org>
				<org type="funding" xml:id="_rDUn8mz">
					<idno type="grant-number">690974</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Bibliographical References</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">MusicLynx: Exploring music through artist similarity graphs</title>
		<author>
			<persName><forename type="first">A</forename><surname>Allik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Thalmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sandler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Companion Proc. (Dev. Track) The Web Conf</title>
		<meeting><address><addrLine>WWW</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">I said it first: Topological analysis of lyrical influence networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Atherton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kaneshiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISMIR</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="654" to="660" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A semantics-driven approach to lyrics segmentation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Baratè</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Ludovico</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Santucci</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2013 8th International Workshop on Semantic and Social Media Adaptation and Personalization</title>
		<imprint>
			<date type="published" when="2013-12">2013. Dec</date>
			<biblScope unit="page" from="73" to="79" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Classification of explicit music content using lyrics and music metadata</title>
		<author>
			<persName><forename type="first">L</forename><surname>Bergelid</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Automatic labelling of topics with neural embeddings</title>
		<author>
			<persName><forename type="first">S</forename><surname>Bhatia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Baldwin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.05340</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Latent dirichlet allocation</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine Learning research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="993" to="1022" />
			<date type="published" when="2003-01">2003. Jan</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Brackett</surname></persName>
		</author>
		<title level="m">Interpreting Popular Music</title>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Real time tube guitar amplifier simulation using webaudio</title>
		<author>
			<persName><forename type="first">M</forename><surname>Buffa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lebrun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 3rd Web Audio Conference</title>
		<meeting>3rd Web Audio Conference<address><addrLine>WAC</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Web audio guitar tube amplifier vs native simulations</title>
		<author>
			<persName><forename type="first">M</forename><surname>Buffa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lebrun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 3rd Web Audio Conf</title>
		<meeting>3rd Web Audio Conf<address><addrLine>WAC</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017b. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Towards an open web audio plugin standard</title>
		<author>
			<persName><forename type="first">M</forename><surname>Buffa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lebrun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kleimola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Letz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Companion Proceedings of the The Web Conference</title>
		<imprint>
			<publisher>International World Wide Web Conferences Steering Committee</publisher>
			<date type="published" when="2018">2018. 2018</date>
			<biblScope unit="page" from="759" to="766" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A 2 Million Commercial Song Interactive Navigator</title>
		<author>
			<persName><forename type="first">M</forename><surname>Buffa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lebrun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pauwels</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Pellerin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WAC 2019 -5th WebAudio Conference 2019</title>
		<meeting><address><addrLine>Trondheim, Norway</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-12">2019. December</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Webaudio plugins in daws and for live performance</title>
		<author>
			<persName><forename type="first">M</forename><surname>Buffa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lebrun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Pellerin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Letz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">14th International Symposium on Computer Music Multidisciplinary Research (CMMR&apos;19)</title>
		<imprint>
			<date type="published" when="2019">2019b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Music mood dataset creation based on last.fm tags</title>
		<author>
			<persName><forename type="first">E</forename><surname>Morisio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Applications</title>
		<meeting><address><addrLine>Vienna Austria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-05">May 2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Semeval-2019 task 3: Emocontext contextual emotion detection in text</title>
		<author>
			<persName><forename type="first">A</forename><surname>Chatterjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">N</forename><surname>Narahari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Agrawal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th International Workshop on Semantic Evaluation</title>
		<meeting>the 13th International Workshop on Semantic Evaluation</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="39" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Explicit content detection in music lyrics using machine learning</title>
		<author>
			<persName><forename type="first">H</forename><surname>Chin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">Y</forename><surname>Yi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Conference on Big Data and Smart Computing (BigComp)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="517" to="521" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Music mood detection based on audio and lyrics with deep neural net</title>
		<author>
			<persName><forename type="first">R</forename><surname>Delbouys</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hennequin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Piccoli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Royo-Letelier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Moussallam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.07276</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Lyrics-based analysis and classification of music</title>
		<author>
			<persName><forename type="first">M</forename><surname>Fell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Sporleder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers</title>
		<meeting>COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="620" to="631" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Lyrics Segmentation: Textual Macrostructure Detection using Convolutions</title>
		<author>
			<persName><forename type="first">M</forename><surname>Fell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Nechaev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Cabrio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Gandon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computational Linguistics (COLING)</title>
		<meeting><address><addrLine>Santa Fe, New Mexico, United States</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-08">2018. August</date>
			<biblScope unit="page" from="2044" to="2054" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Comparing Automated Methods to Detect Explicit Content in Song Lyrics</title>
		<author>
			<persName><forename type="first">M</forename><surname>Fell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Cabrio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Corazza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Gandon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">RANLP 2019 -Recent Advances in Natural Language Processing</title>
		<meeting><address><addrLine>Varna, Bulgaria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-09">2019. September</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Song lyrics summarization inspired by audio thumbnailing</title>
		<author>
			<persName><forename type="first">M</forename><surname>Fell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Cabrio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Gandon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Giboin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">RANLP 2019 -Recent Advances in Natural Language Processing (RANLP)</title>
		<meeting><address><addrLine>Varna, Bulgaria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-09">2019. September</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Lyrics classification</title>
		<author>
			<persName><forename type="first">M</forename><surname>Fell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014. 2014</date>
			<biblScope unit="page">1</biblScope>
			<pubPlace>Germany</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Saarland University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Master&apos;s thesis</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Telemeta: An opensource web framework for ethnomusicological audio archives management and automatic analysis</title>
		<author>
			<persName><forename type="first">T</forename><surname>Fillon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Simonnot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-F</forename><surname>Mifune</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Khoury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Pellerin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Coz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st International Workshop on Digital Libraries for Musicology</title>
		<meeting>the 1st International Workshop on Digital Libraries for Musicology</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Spleeter: A fast and state-of-the art music source separation tool with pre-trained models</title>
		<author>
			<persName><forename type="first">R</forename><surname>Hennequin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Khlif</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Voituret</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Moussallam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Late-Breaking/Demo ISMIR 2019</title>
		<imprint>
			<date type="published" when="2019-11">2019. November</date>
		</imprint>
	</monogr>
	<note>Deezer Research</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Lyric text mining in music mood classification</title>
		<author>
			<persName><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Downie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Ehmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">American music</title>
		<imprint>
			<biblScope unit="volume">183</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="2" to="209" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Lyric-based song emotion detection with affective lexicon and fuzzy clustering method</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISMIR</title>
		<imprint>
			<date type="published" when="2009">2009b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A hybrid modeling approach for an automated lyrics-rating system for adolescents</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">Y</forename><surname>Mun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Information Retrieval</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="779" to="786" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Oh oh oh whoah! towards automatic topic detection in song lyrics</title>
		<author>
			<persName><forename type="first">F</forename><surname>Kleedorfer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Knees</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Pohle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISMIR</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Semantic analysis of song lyrics</title>
		<author>
			<persName><forename type="first">B</forename><surname>Logan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kositsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Moreno</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Multimedia and Expo (ICME) (IEEE Cat. No.04TH8763</title>
		<imprint>
			<date type="published" when="2004-06">2004. 2004. June</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="827" to="830" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Natural language processing of lyrics</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P G</forename><surname>Mahedero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mart Ínez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Cano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Koppenberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Gouyon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th Annual ACM International Conference on Multimedia, MULTIMEDIA &apos;05</title>
		<meeting>the 13th Annual ACM International Conference on Multimedia, MULTIMEDIA &apos;05<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="475" to="478" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Lyrics, music, and emotions</title>
		<author>
			<persName><forename type="first">R</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Strapparava</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</title>
		<meeting>the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning<address><addrLine>Jeju Island, Korea</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012-07">2012. July</date>
			<biblScope unit="page" from="590" to="599" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Semeval-2018 task 1: Affect in tweets</title>
		<author>
			<persName><forename type="first">S</forename><surname>Mohammad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bravo-Marquez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Salameh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kiritchenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th international workshop on semantic evaluation</title>
		<meeting>the 12th international workshop on semantic evaluation</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1" to="17" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Obtaining reliable human ratings of valence, arousal, and dominance for 20,000 english words</title>
		<author>
			<persName><forename type="first">S</forename><surname>Mohammad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="174" to="184" />
		</imprint>
	</monogr>
	<note>Long Papers</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Exploiting synchronized lyrics and vocal features for music emotion detection</title>
		<author>
			<persName><forename type="first">L</forename><surname>Parisi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Francia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Olivastri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Tavella</surname></persName>
		</author>
		<idno>CoRR, abs/1901.04831</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A web-based system for suggesting new practice material to music learners based on chord content</title>
		<author>
			<persName><forename type="first">J</forename><surname>Pauwels</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sandler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Joint Proc. 24th ACM IUI Workshops</title>
		<imprint>
			<date type="published" when="2019">2019. IUI2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Exploring real-time visualisations to support chord learning with a large music collection</title>
		<author>
			<persName><forename type="first">J</forename><surname>Pauwels</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Xambó</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Roma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Barthet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Fazekas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 4th Web Audio Conf</title>
		<meeting>4th Web Audio Conf<address><addrLine>WAC</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">A circumplex model of affect</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Russell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of personality and social psychology</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">1161</biblScope>
			<date type="published" when="1980">1980</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<author>
			<persName><forename type="first">L</forename><surname>Sterckx</surname></persName>
		</author>
		<title level="m">Topic detection in a million songs</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
		<respStmt>
			<orgName>Ghent University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Open-unmix-a reference implementation for music source separation</title>
		<author>
			<persName><forename type="first">F.-R</forename><surname>Stöter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Uhlich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Liutkus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Mitsufuji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Open Source Software</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Analysing popular music: theory, method and practice</title>
		<author>
			<persName><forename type="first">P</forename><surname>Tagg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Popular Music</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="37" to="67" />
			<date type="published" when="1982">1982</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Textual deconvolution saliency (tds): a deep tool box for linguistic analysis</title>
		<author>
			<persName><forename type="first">L</forename><surname>Vanni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ducoffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Aguilar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Precioso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mayaffre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="548" to="557" />
		</imprint>
	</monogr>
	<note>Long Papers</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Norms of valence, arousal, and dominance for 13,915 english lemmas</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">B</forename><surname>Warriner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Kuperman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Brysbaert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Behavior research methods</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1191" to="1207" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Modeling discourse segments in lyrics using repeated patterns</title>
		<author>
			<persName><forename type="first">K</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Matsubayashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Orita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Okazaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Inui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Fukayama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Nakano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Goto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers</title>
		<meeting>COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1959" to="1969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Sentiment vector space model for lyric-based song sentiment classification</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-F</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics on Human Language Technologies: Short Papers, HLT-Short &apos;08</title>
		<meeting>the 46th Annual Meeting of the Association for Computational Linguistics on Human Language Technologies: Short Papers, HLT-Short &apos;08<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="133" to="136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Music emotion identification from lyrics</title>
		<author>
			<persName><forename type="first">D</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 11th IEEE International Symposium on Multimedia</title>
		<imprint>
			<date type="published" when="2009-12">2009. Dec</date>
			<biblScope unit="page" from="624" to="629" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Language Resource References</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">The million song dataset</title>
		<author>
			<persName><forename type="first">T</forename><surname>Bertin-Mahieux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Ellis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Whitman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lamere</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th International Conference on Music Information Retrieval (ISMIR 2011)</title>
		<meeting>the 12th International Conference on Music Information Retrieval (ISMIR 2011)</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">WASABI: a Two Million Song Database Project with Audio and Cultural Metadata plus WebAudio enhanced Client Applications</title>
		<author>
			<persName><forename type="first">G</forename><surname>Meseguer-Brocal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Peeters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Pellerin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Buffa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Cabrio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Faron Zucker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Giboin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Mirbel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hennequin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Moussallam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Piccoli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Fillon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Web Audio Conference 2017 -Collaborative Audio #WAC2017</title>
		<meeting><address><addrLine>London, United Kingdom</addrLine></address></meeting>
		<imprint>
			<publisher>Queen Mary University of London</publisher>
			<date type="published" when="2017-08">2017. August</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
