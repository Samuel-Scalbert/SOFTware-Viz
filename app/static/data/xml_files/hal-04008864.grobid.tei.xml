<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Improving text mining in plant health domain with GAN and/or pre-trained language model</title>
				<funder>
					<orgName type="full">ISEP Paris</orgName>
				</funder>
				<funder>
					<orgName type="full">University of Reims Champagne-Ardenne</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>February</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Mathieu</forename><surname>Roche</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Emanuela</forename><surname>Boros</surname></persName>
						</author>
						<author role="corresp">
							<persName><forename type="first">Shufan</forename><surname>Jiang</surname></persName>
							<email>jiang.chou.fan@gmail.com</email>
						</author>
						<author>
							<persName><forename type="first">Stéphane</forename><surname>Cormier</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Rafael</forename><surname>Angarita</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Francis</forename><surname>Rousseaux</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Brett Drury</orgName>
								<orgName type="laboratory">French Agricultural Research Centre for International Development (CIRAD)</orgName>
								<orgName type="institution">Liverpool Hope University</orgName>
								<address>
									<country>France, United Kingdom</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Université de la Rochelle</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">Lentschat Martin</orgName>
								<orgName type="institution">Université Grenoble Alpes</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="laboratory">CReSTIC</orgName>
								<orgName type="institution">Université de Reims Champagne Ardenne</orgName>
								<address>
									<settlement>Reims</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="laboratory">LISITE</orgName>
								<orgName type="institution">Institut Supérieur d&apos;Electronique de Paris</orgName>
								<address>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Improving text mining in plant health domain with GAN and/or pre-trained language model</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published">February</date>
						</imprint>
					</monogr>
					<idno type="MD5">2D7F26E019549D3E88B128D615A7F6C1</idno>
					<idno type="DOI">10.3389/frai.2023.1072329</idno>
					<note type="submission">This article was submitted to AI in Food, Agriculture and Water, a section of the journal Frontiers in Artificial Intelligence RECEIVED October ACCEPTED January</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-04-12T14:53+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>GAN</term>
					<term>social media</term>
					<term>plant health monitoring</term>
					<term>text classification</term>
					<term>pre-trained BERT</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The Bidirectional Encoder Representations from Transformers (BERT) architecture o ers a cutting-edge approach to Natural Language Processing. It involves two steps: ) pre-training a language model to extract contextualized features and ) fine-tuning for specific downstream tasks. Although pre-trained language models (PLMs) have been successful in various text-mining applications, challenges remain, particularly in areas with limited labeled data such as plant health hazard detection from individuals' observations. To address this challenge, we propose to combine GAN-BERT, a model that extends the fine-tuning process with unlabeled data through a Generative Adversarial Network (GAN), with ChouBERT, a domain-specific PLM. Our results show that GAN-BERT outperforms traditional fine-tuning in multiple text classification tasks. In this paper, we examine the impact of further pre-training on the GAN-BERT model. We experiment with di erent hyper parameters to determine the best combination of models and fine-tuning parameters. Our findings suggest that the combination of GAN and ChouBERT can enhance the generalizability of the text classifier but may also lead to increased instability during training. Finally, we provide recommendations to mitigate these instabilities.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>. Introduction</head><p>Climate change is causing massive yield losses due to the disruption of cycles and the emergence of crop-affecting pests and plant diseases <ref type="bibr" target="#b15">(Massod et al., 2022)</ref>. More pest attacks may occur than that reported earlier, and their population may increase due to warmer temperatures. The CO 2 level and lower soil humidity can also affect the nature of plant diseases <ref type="bibr" target="#b19">(Mozaffari, 2022)</ref>. To tackle the emerging risks and increasingly unpredictable hazards acting as a menace to crops and plants, precision agriculture emerges as an alternative-or improvement-to existing agricultural practices. Indeed, researchers have experimented with technological innovations to find solutions to some specific goals, such as predicting the climate for agricultural purposes using simulation models <ref type="bibr" target="#b7">(Hammer et al., 2001)</ref>, improving the efficiency and effectiveness of grain production using computer vision and Artificial Intelligence <ref type="bibr" target="#b22">(Patrício and Rieder, 2018)</ref>, studying and evaluating soils with drones <ref type="bibr" target="#b28">(Tripicchio et al., 2015)</ref>, and collecting real-time data from the fields using sensors following the IoT and cloud computing paradigms <ref type="bibr" target="#b21">(Patil et al., 2012)</ref>. Although the application of these technological innovations produces important results, we suggest that the current observation data from precision agriculture cannot represent all forms of agricultural environments, especially small farms. Recently, the idea of how to encourage the participation of farmers to share their knowledge and observations is drawing the attention of researchers <ref type="bibr" target="#b10">(Jiménez et al., 2016;</ref><ref type="bibr" target="#b11">Kenny and Regan, 2021)</ref>. Indeed, new studies show that social media might enable farmers to reveal different aspects of their world and to share their experiences and perspectives among colleagues and non-farming audiences <ref type="bibr" target="#b24">(Riley and Robertson, 2021)</ref>.</p><p>The role of social media such as Twitter in farmer-to-farmer and in farmer-to-rural-profession knowledge exchange is increasing, and it suggests that their use among rural professionals and farmers is evolving with open participation (creating contributions), collaboration (sharing contributions), and fuller engagement (asking questions and providing answers/replies) dominating one-way messaging (new/original contributions) <ref type="bibr" target="#b23">(Phillips et al., 2018)</ref>. Following the social sensing paradigm <ref type="bibr" target="#b29">(Wang et al., 2015)</ref>, individuals-whether they are farmers or not-have more and more connectivity to information while on the move, at the field level. Each individual can become a broadcaster of information by posting realtime hazard observations in social media. Indeed, Twitter enables farmers to exchange experience with each other, to subscribe to topics of interest using hashtags, and to share real-time information on natural hazards. Compared to paid and specialized applications, information on Twitter, presented in the form of text, image, sound, video, or a mixture of the above, is more accessible to the public but less formalized or structured. More and more farmers get involved in online Twitter communities by adding hashtags on their publications to categorize their tweets and help others find them easily <ref type="bibr" target="#b4">(Defour, 2018)</ref>. Some hashtags are #AgriChatUK , #FrAgTw , and #Farming365.</p><p>Still, the extraction of useful plant health information from social media poses some challenges, including lack of context, irrelevancy, homographs, homophones, homonyms, slangs, and colloquialisms. In an earlier study, we developed ChouBERT <ref type="bibr" target="#b9">(Jiang et al., 2022)</ref> to detect farmers' observations from tweets for pest monitoring. ChouBERT takes a pre-trained CamemBERT <ref type="bibr" target="#b14">(Martin et al., 2020)</ref> model and further pre-trains it on a plant health domain corpus in French to improve the generalizability of plant health hazards detection on Twitter. Some potential applications of ChouBERT are as follows:</p><p>• The annotation and indexing of the key elements of plant healthrelated events in the text, including named entity recognition, entity linking, and relation extraction.</p><p>• Topic modeling for detecting emerging issues from a collection of texts. • Natural language inference for finding precursors of pest attacks.</p><p>In this article, we explored the combination of GAN-BERT <ref type="bibr" target="#b2">(Croce et al., 2020)</ref> and further pre-training with ChouBERT. We present a discussion on the results and perspectives of this combination on the text classification task for plant health hazard detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>. Background . . Pre-trained language models</head><p>Pre-trained language models (PLMs) are deep neural networks of pre-trained weights to vectorize sequences of words. Such vectorial representations obtain state-of-the-art results on NLP tasks, such as text classification, text clustering, question-answering, and information extraction. PLMs suggest an objective engineering paradigm for NLP: language model pre-training for extracting contextualized features from text and fine-tuning for downstream tasks. BERT <ref type="bibr" target="#b5">(Devlin et al., 2019)</ref> is a PLM introduced in 2018 by Google that led to significant improvements in this field. BERT is pretrained in two stages: first, a self-supervised task where the masked language model (MLM) must retrieve masked words in a text; and second, a supervised task where the model must refind whether a sentence B is the continuation of a sentence A or not (next-sentence prediction, NSP). The pre-training produces in the end 12 stacked encoders which take a sequence of tokens as input and add a special token "[CLS]" at the beginning of the sequence and a "[SEP]" at the end of each sentence, and calculates a fixed-length vector for each token. Each dimension of these vectors represents how much attention that token should pay to the other tokens. For the text classification task, the vector of "[CLS]" represents the whole text. Among the French varieties of BERT, CamemBERT <ref type="bibr" target="#b14">(Martin et al., 2020)</ref> is a model based on the same architecture as BERT but trained on a French corpus with MLM only. ChouBERT <ref type="bibr" target="#b9">(Jiang et al., 2022)</ref> takes a pre-trained CamemBERT-base checkpoint and further pretrains it with MLM over a corpus in French in the plant health domain to improve performance in detecting plant health issues from short texts, particularly, from Twitter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>. . Generative adversarial networks</head><p>Generative Adversarial Networks (GANs) <ref type="bibr" target="#b6">(Goodfellow et al., 2014;</ref><ref type="bibr" target="#b30">Wang et al., 2017)</ref> are a family of neural networks that can be commonly divided into two antagonistic parts: a generator and a discriminator, which compete during training. The generator aims to mimic real data by transforming noise, while the discriminator aims to determine if the data are real or produced by the generator. The discriminator's classification results then feed the generator's training in turn. The training of GANs is known to suffer from the following failure modes: gradient vanish, mode collapse, and non-convergence. Gradient vanish occurs when the discriminator cannot give enough information to improve the generator. Mode collapse occurs when the generator gets stuck generating only one mode. Non-convergence occurs when the generator tends to overfit to the discriminators instead of reproducing the real data distribution.</p><p>Many variants of GANs are proposed to improve sample generation and the stability of training. Some of these variants are the conditional GANs (CGANs), where the generator is conditional on one or more labels <ref type="bibr" target="#b17">(Mirza and Osindero, 2014)</ref>, and semi-supervised GANs <ref type="bibr" target="#b25">(Salimans et al., 2016)</ref> (SS-GANs), where the discriminator is trained over its k-labeled examples plus the data generated by the generator as a new label "k + 1"(see in Figure <ref type="figure">1</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>. . GAN-BERT architecture</head><p>Generative adversarial network-bidirectional encoder representations from transformers (GAN-BERT) <ref type="bibr" target="#b2">(Croce et al., 2020)</ref> extends the fine-tuning of BERT-like pre-trained language models (PLMs) for text classification with a semi-supervised discriminatorgenerator setting, introduced in the study by <ref type="bibr" target="#b25">Salimans et al. (2016)</ref>.</p><p>Let us project all the data points in a d-dimensional hidden space, then the data vector h ∈ R d . The generator G SSGAN is a multi-layer We define p m (ŷ = y|x, y ∈ (1, ..., k)) as the probability given by the model m that an example x belongs to one of the k target classes and p m (ŷ = y|x, y = k + 1) as the probability of x being fake data. Let P R and P G denote the real data distribution and the generated data, respectively. The loss function for training D SSGAN becomes:</p><formula xml:id="formula_0">L D = L D sup + L D unsup<label>(1)</label></formula><p>L D sup evaluates how well the real labeled data are classified:</p><formula xml:id="formula_1">L D sup = -E x,y∼P R log[p m (ŷ) = y|x, y ∈ (1, ..., k)]<label>(2)</label></formula><p>L D unsup punishes the discriminator when it fails to recognize a fake example or when it classifies a real unlabeled example to be fake. If the discriminator is free to assign any of the k target classes to the unlabeled data.</p><formula xml:id="formula_2">L D unsup = -E x∼P R log[1 -p m (ŷ = y|x, y = k + 1)] -E x∼P G log[p m (ŷ = y|x, y = k + 1)]<label>(3)</label></formula><p>As for the generator G SSGAN , <ref type="bibr" target="#b2">Croce et al. (2020)</ref> defines the loss function as:</p><formula xml:id="formula_3">L G = L G unsup + L G feat<label>(4)</label></formula><p>L G unsup penalizes G SSGAN when D SSGAN correctly finds fake examples:</p><formula xml:id="formula_4">L G unsup = -E x∼P G log[1 -p m (ŷ = y|x, y = k + 1)]</formula><p>(5) Let f D (x) denote the activation that D SSGAN uses to convert the input data to its inner representation h D . L G feat measures the statistical distance between the inner representation of real data h D R and the inner representation of generated data h D G .</p><formula xml:id="formula_5">L G unsup = E x∼P R f (x) -E x∼P G f (x) 2 2 (6)</formula><p>The PLM is part of the discriminator D SSGAN ; that is, when updating D SSGAN , the weights of the PLM are also fine-tuned. Moreover, at the beginning of each training epoch, the [CLS] vector of real examples is recalculated by the updated PLM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>. . GAN-BERT applications</head><p>Generative adversarial network-bidirectional encoder representations from transformers (GAN-BERT) has been assessed on different datasets with different PLMs. The original authors of GAN-BERT have applied it to English sentence-level classification tasks, including topic classification, question classification (QC), sentiment analysis, and natural language inference (NLI) with the original BERT model <ref type="bibr" target="#b5">(Devlin et al., 2019;</ref><ref type="bibr" target="#b2">Croce et al., 2020)</ref>.</p><p>Later, MT-GAN-BERT <ref type="bibr" target="#b1">(Breazzano et al., 2021)</ref> extends GAN-BERT to a multi-task learning (MTL) architecture to solve simultaneously several related sentence-level classification tasks reducing overfitting. MT-GAN-BERT is assessed with English and Italian datasets, using BERT and UmBERTo , respectively, for sentence embedding generation. The results of</p><p>In the PyTorch implementation of the feature reg loss, the authors use: g_feat_reg = torch.mean(torch.pow(torch.mean(D_real_features, dim= )torch.mean(D_fake_features, dim= ), )), which is not exactly the same as the definition given by the article; however, according to other author's experiments and our experience, there is no significant impact on the training result.</p><p>https://huggingface.co/Musixmatch/umberto-wikipedia-uncased-v</p><p>Frontiers in Artificial Intelligence frontiersin.org MT-GAN-BERT show that GAN-BERT-based models outperform BERT-based models with 100 and 200 labeled data. However, the performance worsens when training GAN-BERT with 500 labeled data.</p><p>In the study of <ref type="bibr" target="#b27">Ta et al. (2022)</ref>, the authors applied GAN-BERT for paraphrase identification. They propose to filter noises in the labeled set to improve the performance and claim that, for their use case, a lower learning rate helps the model to learn better. However, a too-small learning rate makes the accuracy to increase slowly. In the study of <ref type="bibr" target="#b26">Santos et al. (2022)</ref>, the authors applied GAN-BERT with Portuguese PLMs to find hate speech in social media. This study shows that text cleaning, including removing users' mentions, links, and repeated punctuation, improves the performance of GAN-BERT-based classification. Finally, the authors infer that GAN-BERT is nonetheless more susceptible to noise.</p><p>In the study of <ref type="bibr" target="#b20">Myszewski et al. (2022)</ref>, the authors showed that the combination of a GAN-BERT setting with a domain-specific PLM BioBERT <ref type="bibr" target="#b12">(Lee et al., 2019)</ref> outperforms the original GAN-BERT on a sentiment classification task for clinical trial abstracts. However, the authors do not compare the results with those of PLM-only classification. They neither provide a detailed analysis of the training. In this study, the authors presented 108 labeled examples. The small number (23) of labeled samples in their test set also makes the result unconvincing, which calls for more studies to validate the combination of GAN-BERT and domainspecific PLMs.</p><p>In the study of <ref type="bibr" target="#b3">Danielsson et al. (2022)</ref>, the authors studied whether and how GAN-BERT can help in the classification of patients bearing implants with a relatively small set of labeled electronic medical records (EMRs) written in Swedish. In practice, they further pre-trained a Swedish BERT model to provide the [CLS] representations of 64 and 512 tokens to the discriminator of GAN-BERT and perform experiments over varying training set sizes. Their results show that combining GAN-BERT and a domain-specific PLM improves the classification performance in specific challenging scenarios. However, the effective zone of such scenarios remains to be studied. The numerous applications of GAN-BERT witness its capacity for fine-tuning PLM on sentence-level classification tasks in a low resource setting. However, none of the works presented in this section have studied the correlation between the labeled/unlabeled data ratio and the performance of GAN-BERT or the impact of using domain-specific PLMs. The lack of specifications for these hyperparameters makes the GAN-BERT setting a black box to newcomers and could lead to expensive grid search experiments for optimization <ref type="bibr" target="#b0">(Bergstra and Bengio, 2012)</ref>. Furthermore, the granularities of different classification problems are not comparable. Therefore, it is unfair to compare the performances of GAN-BERT plus the PLMs pre-trained in different languages or domains over these tasks. In this study, we address these shortcomings by applying the GAN-BERT settings to CamemBERT <ref type="bibr" target="#b14">(Martin et al., 2020)</ref>, ChouBERT-16, and ChouBERT-32 and probing the different losses over varying labeled and unlabeled data sizes to give more insights into when and how to train GAN-BERT for domain-specific document classification.</p><p>https://github.com/Kungbib/swedish-bert-models . Method . . Data Data annotation by domain experts is expensive and timeconsuming. Therefore, the main challenge of detecting natural hazards from textual contents on social media is to identify unseen risks with low resources for training. We reuse the labeled tweets produced by ChouBERT <ref type="bibr" target="#b9">(Jiang et al., 2022)</ref>, tweets about corn borer, barley yellow dwarf virus (BYDV) and corvids for training and validation, and tweets about unseen and polysemous terms such as "taupin" (wireworm in English) for testing the generalizability of the classifier. Since the binary cross entropy loss adopted by the discriminator of GAN-BERT favors the majority class when data are unbalanced, for the different training experiments, we sampled ChouBERT's training data to 16, 32, 64, 128, 256, and 512 subsets, each subset having equal number of observations and nonobservations. We used the same validation data and test sets for all the experiments. In the validation set, there were 79 observations and 213 non-observations; in the test set, there were 58 observations and 447 non-observations. Among the data collected by ChouBERT, there is not only a small set of labeled tweets but also many unlabeled tweets. For the unsupervised learning, we have 12,308 unlabeled tweets containing common insect pest names (other than those in the labeled data) in France. We sampled 0, 1,024, <ref type="bibr">4,096, and 8,192</ref> unlabeled data to study the effect of adding unlabeled data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>. . Text classification with a pre-trained language model</head><p>Following the study of <ref type="bibr" target="#b9">Jiang et al. (2022)</ref>, the ChouBERT models are further-pre-trained CamemBERT-base models over French Plant Health Bulletins and Tweets and the ChouBERT pre-trained for 16 epochs (denoted as ChouBERT-16) and for 32 epochs (denoted as ChouBERT-32) are the most efficient in finding observations about plant health issues. Thus, in this study, we combine GAN-BERT settings with CamemBERT, ChouBERT-16, and ChouBERT-32.</p><p>To make our state-of-the-art model, we fine-tune CamemBERT, ChouBERT-16, and ChouBERT-32 for the sequence classification task over the same training/validation/test sets by adding a linear regression layer a to the final hidden state h of the [CLS] token to predict the probability of a label o:</p><formula xml:id="formula_6">p(o|h) = softmax(W a h),<label>(7)</label></formula><p>Where W a is the parameter matrix of this linear classifier.</p><p>During the training, the weights of the PLM are affected along with W a . We developed these experiments with CamemBertForSequenceClassification of the transformer package . To make the probability outputs of this linear regression layer comparable with the label outputs from the GAN-BERT classifier, we fixed the threshold of 0.5. For the predicted probability greater than 0.5, we considered it as an observation, else as a non-observation. It is worth mentioning that we used 0.5 as a threshold to simplify  the comparison with the PLM plus GAN-BERT classification. When applying the PLM-only classification to other datasets in other domains, we might need to find an optimal threshold depending on the real needs for precision or recall. Based on the results presented in the study of <ref type="bibr" target="#b9">Jiang et al. (2022)</ref>, we fixed the learning rate to 2e -5 , the maximum sequence length to 128, and fit the classifier for 10 epochs. We set the batch size to (training_data_size/8) to have the same steps for the different training data sizes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>. . Experimental setup</head><p>For our experiments, we used GAN-BERT's latest PyTorch implementation, which is compatible with the transformer package. We fixed the max sequence length of the PLM to 128. We fixed the number of hidden layers in G and in D to 1 and the size of G's input noisy vectors to 100. We used the following learning rate combinations (D, G): (5e-5, 5e-5), (1e-5, 1e-5), and (5e-6, 1e-6). We applied the AdamW <ref type="bibr" target="#b13">(Loshchilov and Hutter, 2017)</ref> optimizer with and without a cosine scheduler. To limit the number of variables, we https://github.com/crux /ganbert-pytorch conduct two groups of experiments. In the first group, we fixed the batch size per GPU to 32 and epochs to 30. We trained the GAN-BERT architectures over increasing labeled data sizes <ref type="bibr">(16, 32, 64, 128, 256, and 512)</ref> and unlabeled data sizes <ref type="bibr">(1,024, 4,096, and 8,192)</ref>. In the second group, we fixed the training steps of each (labeled and unlabeled) pair by setting the batch size to (unlabeled_data_size/256). Moreover in the second group, we trained the GAN without unlabeled data. That is, in this group, the unsupervised learning learns the features from labeled data only. We fixed the batch size to 4 and set epochs to (1, 024/train_data_size + log 2 (train_data_size)) to approximate the number of training steps in the experiments with unlabeled data.</p><p>. Results and evaluation</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>. . Overall metrics</head><p>As the validation set and the test set were unbalanced and that our interest is to find out the observations, we plot the F1 score of the observation class F1 observation and the macro average F1 score of the Frontiers in Artificial Intelligence frontiersin.org  whole classification.</p><formula xml:id="formula_7">F1 macro = (F1 observation + F1 non-observation )/2 (8)</formula><p>Let us consider a dummy classifier as our baseline model. If it predicts that all the examples in the validation set are nonobservations, the F1 observation , F1 macro , and accuracy become 0, 0.42, and 0.73, respectively; if it predicts that all are observations, the F1 observation , F1 macro , and accuracy become 0.43, 0.21, and 0, respectively.</p><p>We present the overall results of the fixed-step experiments in Figure <ref type="figure">2</ref>, which are the most representative and stable. By comparing the maximum F1 scores of each configuration during the training in Figures <ref type="figure">2</ref><ref type="figure">3</ref><ref type="figure">4</ref>, we believe the performance of the classifiers on both the validation and test sets to be continuous and relatively stable in a period, once the training converges. In other words, overfitting will not immediately cause huge drops. It is worth pointing out that, on the unbalanced validation and test sets, the F1 score (the objective of our classification task) and the binary cross entropy loss (the objective of GAN-BERT's training) are not completely aligned and may lead to suboptimal convergence.</p><p>Compared to PLM-only classification, the PLM plus GAN-BERT setting improves the scores over the validation and test sets of unseen hazards with 32, 64, 128, and 256 training data. In Figure <ref type="figure">3</ref>, we depict the performance with varying unlabeled data sizes. In both figures, we can see that the deep blue lines (ChouBERT-32) are above the yellow lines (CamemBERT), which is clearly coherent with the results presented in <ref type="bibr" target="#b9">Jiang et al. (2022)</ref>, indicating that pre-training helps to improve generalizability. The representational similarity analysis in <ref type="bibr" target="#b16">Merchant et al. (2020)</ref> shows that "fine-tuning has a much greater impact on the token representations of in-domain data" and suggests fine-tuning to be "conservative." In our experiments, we did not observe that the SSGAN setting without domain unlabeled data helps the model generalization for the identification of tweets about upcoming unseen hazards. For small training data sizes, adding unlabeled data helps to improve the performance on the test set, but adding more unlabeled data consumes more computational resources without making significant difference. We observe similar phenomena in the fixed batch size group results in Figure <ref type="figure">4</ref>, where adding more unlabeled data brings more training steps per epoch and eventually reduces the L sup steadily within the same training epochs.</p><p>In all our experiments with 512 labeled data, PLM-only solutions outperform PLM plus GAN-BERT solutions, while, in experiments, with between 32 and 256 labeled data, PLM+GAN-BERT improves the performance on the validation and test sets, which corresponds to the results presented in <ref type="bibr" target="#b1">Breazzano et al. (2021)</ref> and <ref type="bibr" target="#b3">Danielsson et al. (2022)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>. . The instability of the GAN-BERT setting with ChouBERT models</head><p>Even though the fine-tuning of pre-trained transformer-based language models such as BERT has achieved state-of-the-art results on NLP tasks, fine-tuning is still an unstable process. Training the same model with multiple random seeds can result in different performances on a task as described in the study of <ref type="bibr" target="#b18">Mosbach et al. (2021)</ref>. This training instability is the reason why we have not found the best labeled/unlabeled ratio to maximize the performance. In Figure <ref type="figure">5</ref>, we illustrate the training losses of the discriminator and the generator when given different sizes of labeled data with fixed unlabeled data size and learning rate.</p><p>The training with ChouBERT models has more difficulties to converge than with CamemBERT. Thus, we explored the evolution of different losses and the classifiers' performance metrics on the validation and test sets in Figures <ref type="figure">6,</ref><ref type="figure">7</ref>, where the discriminators' losses with ChouBERT-16 take more epochs to decrease than with CamemBERT.</p><p>It is immediately clear that discriminators' losses had the same shape as L sup . In particular, to present the evolution of L G feat at the same scale as the other losses, we multiplied its value by 10 to draw its line. Compared with ChouBERT-16, the L sup had more difficulties decreasing than with CamemBERT. We interpret the increase of L G feat as the generator catching up with the fine-tuning of PLM and the decrease of L G feat toward its initial value as the end of the major changes of fine-tuning.</p><p>According to the authors of SSGAN <ref type="bibr" target="#b25">(Salimans et al., 2016)</ref>, "in practice, L unsup will only help if it is not trivial to minimize for our classifier and we thus need to train G to approximate the data distribution, " which explains that, while the L unsup of D and G converge at the same rhythm with CamemBERT and with ChouBERT-16, the troubled decrease of L sup with ChouBERT-16 renders worse F1 scores than those with CamemBERT. For example, in the group with 16 training examples (see Figure <ref type="figure">6</ref>), the test F1 observation scores with ChouBERT-16 are switching between 0 and 0.43, which means that the classifier predicts either all as nonobservation or all as observation. Considering the unbalanced nature of our validation and test sets, all-observation predictions and allnon-observation predictions are two local Nash equilibria for the training of our SSGAN.</p><p>It is also remarkable that, in the group with 64 training examples (see Figure <ref type="figure">7</ref>), ChouBERT-16 gives better F1 scores than CamemBERT in the early stages. However, after the bounces of L sup , despite the fine-tuning, helps it to decrease again, the F1 scores are not as good as before because the effect of L unsup is already gone. We can also observe this phenomenon in the group with 32 training examples. Interestingly, when repeating the experiments with the same hyperparameters, the "troubled decrease" of L sup does not always happen, but statistically, most of them can happen with ChouBERT models, especially ChouBERT-16. Our strategies against the "troubled decrease" include the following:</p><p>• Using a smaller learning rate with more training epochs at the cost of computational resources (see <ref type="bibr" target="#b27">Ta et al., 2022</ref>). • Applying a smaller learning rate to G than to D (see <ref type="bibr" target="#b8">Heusel et al., 2017</ref>). • Applying schedulers and down-sampling the majority class to balance the training data-in our case, the upsampling proposed by the original code of GAN-BERT does not help.</p><p>With the optimizations mentioned above, L sup with CamemBERT decreases at a steady pace and "troubled decrease" happens less often with ChouBERT models. When we examined the embeddings of [CLS] produced by the PLM, we found that there is more variance in each dimension of CamemBERT embeddings than in each dimension of ChouBERT embeddings, before and after the fine-tuning: Var CamemBERT &gt; Var ChouBERT-32 &gt; Var ChouBERT-16 . Thus, ChouBERT models produce more homogeneous encodings than CamemBERT. This explains why ChouBERT embeddings are more generalizable for detecting unseen hazards: the embeddings of texts containing unseen hazards are more similar to those of seen hazards, so the downstream classifier is more familiar with these vectors. It also indicates that the differences between observations and non-observations are more subtle in ChouBERT's latent space. Thus, the training of GAN plus ChouBERT needs lower learning rates to converge, while GAN plus CamemBERT is a robust approach to converge in most configurations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>. Conclusion</head><p>In this article, we demonstrate that combining further-pretrained language models and GAN-BERT benefits from the generalizability of the domain-specific PLM to classify unseen hazards. We also demonstrate that training such a combination may also suffer from extra instabilities compared to using GAN-BERT with CamemBERT, a general PLM. Our results validate that the GAN-BERT setting improves the task of natural hazard classification for datasets containing between 32 and 256 instances of labeled data.</p><p>Based on our experimental studies, we give our suggestions to reduce the instability such as: (1) The L sup needs a certain minimum number of steps to decrease to zero. For a fixed batch size, adding unlabeled data makes more training steps to go through in each epoch, consequently allowing L sup to decrease at a similar pace as L unsup . When the number of unlabeled data is limited, using smaller batch sizes and training for more epochs is also a good approach.</p><p>(2) If the task is not too domain-specific, in other words, when the further pre-trained language model cannot significantly outperform the general language model in the PLM-only classification, using a general language model with the GAN-BERT setting is safer. On the other hand, if the task is highly domain-specific, it is better to apply schedulers, downsample the majority class to balance the training data, and use smaller learning rates to train GAN-BERT with furtherpre-trained language models. (3) We need to choose a suitable PLM. We proved that ChouBERT-32 outperforms ChouBERT-16 in an SSGAN setting. The perspectives and developments are numerous to increase the stability of domain-specific text classification using GAN-BERT; for example, how to further pre-train PLMs to adapt better SSGAN setting is yet to investigate.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>FIGURE</head><label></label><figDesc>FIGURETraining an SS-GAN architecture.</figDesc><graphic coords="4,127.64,89.91,340.16,205.26" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>FIGURE</head><label></label><figDesc>FIGURE PLM + GAN-BERT vs. PLM only with same training steps over varying sizes of training datasets.</figDesc><graphic coords="6,113.14,89.74,368.50,365.43" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>FIGURE</head><label></label><figDesc>FIGUREPLM + GAN-BERT performance with fixed steps, training dataset sizes = , , , , , and .</figDesc><graphic coords="7,161.14,90.55,272.89,623.62" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>FIGUREA</head><label></label><figDesc>FIGUREA macroscopic view of the evolution of training losses, with labeled sizes of and over epochs. We fix unlabeled size to , Learning rates of the Discriminator and Generator to e-and e-.</figDesc><graphic coords="9,113.14,90.36,368.50,242.81" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>FIGURE</head><label></label><figDesc>FIGURETraining data set size = .</figDesc><graphic coords="9,127.64,403.12,340.16,237.92" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>FIGURE</head><label></label><figDesc>FIGURETraining data set size = .</figDesc><graphic coords="10,127.64,90.11,340.16,240.06" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="8,161.14,90.55,272.83,623.62" type="bitmap" /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>Frontiers in Artificial Intelligence frontiersin.org</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_1"><p>http://www.agrichatuk.org https://franceagritwittos.com</p></note>
		</body>
		<back>

			<div type="funding">
<div><head>Funding</head><p>This study received funding from the <rs type="funder">University of Reims Champagne-Ardenne</rs> and <rs type="funder">ISEP Paris</rs>.</p></div>
			</div>
			<listOrg type="funding">
			</listOrg>

			<div type="availability">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data availability statement</head><p>The data analysed in this study are subject to the following licenses/restrictions: Redistribution of the collected Twitter data is restricted by the Twitter Terms of Service, Privacy Policy, Developer Agreement, and Developer Policy. The tweet IDs of the labeled tweets and the labels will be made available by the authors.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Author contributions</head><p>SJ and SC were responsible for the conception. RA, SC, and FR were responsible for administrative support and the provision of study materials. SJ was responsible for data collection and experiments. All authors contributed to the manuscript revision, read, and approved the submitted version.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conflict of interest</head><p>The authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Publisher's note</head><p>All claims expressed in this article are solely those of the authors and do not necessarily represent those of their affiliated organizations, or those of the publisher, the editors and the reviewers. Any product that may be evaluated in this article, or claim that may be made by its manufacturer, is not guaranteed or endorsed by the publisher.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Random search for hyper-parameter optimization</title>
		<author>
			<persName><forename type="first">J</forename><surname>Bergstra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="281" to="305" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Multi-task and generative adversarial learning for robust and sustainable text classification</title>
		<author>
			<persName><forename type="first">C</forename><surname>Breazzano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Croce</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Basili</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NL4AI@ AI* IA</title>
		<meeting><address><addrLine>Berlin; Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="228" to="244" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">GAN-BERT: generative adversarial learning for robust text classification with a bunch of labeled examples</title>
		<author>
			<persName><forename type="first">D</forename><surname>Croce</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Castellucci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Basili</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.191</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics (Association for Computational Linguistics)</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics (Association for Computational Linguistics)</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="2114" to="2119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Classifying implant-bearing patients via their medical histories: a pre-study on swedish emrs with semi-supervised gan-bert</title>
		<author>
			<persName><forename type="first">B</forename><surname>Danielsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Santini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lundberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Al-Abasse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jönsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Eneling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th LREC Conference (LREC2022)</title>
		<meeting>the 13th LREC Conference (LREC2022)<address><addrLine>Marseille)</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="21" to="23" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">EIP-AGRI Brochure Agricultural Knowledge and Innovation Systems</title>
		<author>
			<persName><forename type="first">T</forename><surname>Defour</surname></persName>
		</author>
		<ptr target="https://ec.europa.eu/eip/agriculture/en/publications/eip-agri-brochure-agricultural-knowledge-and" />
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>EIP-AGRI-European Commission</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">BERT: pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, MI</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
	<note>Long and Short Papers</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Neural Information Processing Systems</title>
		<meeting>the 27th International Conference on Neural Information Processing Systems<address><addrLine>Montreal, QC</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Advances in application of climate prediction in agriculture</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">L</forename><surname>Hammer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hansen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Phillips</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Mjelde</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Love</surname></persName>
		</author>
		<idno type="DOI">10.1016/S0308-521X(01)00058-0</idno>
	</analytic>
	<monogr>
		<title level="j">Agric Syst</title>
		<imprint>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="515" to="553" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">GANs trained by a two time-scale update rule converge to a local nash equilibrium</title>
		<author>
			<persName><forename type="first">M</forename><surname>Heusel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ramsauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Nessler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on Neural Information Processing Systems</title>
		<meeting>the 31st International Conference on Neural Information Processing Systems<address><addrLine>Long Beach, CA</addrLine></address></meeting>
		<imprint>
			<publisher>Curran Associates</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="6629" to="6640" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">ChouBERT: Pre-training french language model for crowdsensing with tweets in phytosanitary context</title>
		<author>
			<persName><forename type="first">S</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Angarita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Cormier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Orensanz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Rousseaux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Research Challenges in Information Science: 16th International Conference, RCIS 2022</title>
		<meeting><address><addrLine>Barcelona</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="653" to="661" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">From observation to information: data-driven understanding of on farm yield variation</title>
		<author>
			<persName><forename type="first">D</forename><surname>Jiménez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Dorado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">D</forename><surname>Prager</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Delerce</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Grillon</surname></persName>
		</author>
		<idno type="DOI">10.1371/journal.pone.0150015</idno>
	</analytic>
	<monogr>
		<title level="j">PLoS ONE</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">150015</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Co-designing a smartphone app for and with farmers: empathising with end-users&apos; values and needs</title>
		<author>
			<persName><forename type="first">U</forename><surname>Kenny</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Regan</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.jrurstud.2020.12.009</idno>
	</analytic>
	<monogr>
		<title level="j">J. Rural Stud</title>
		<imprint>
			<biblScope unit="volume">82</biblScope>
			<biblScope unit="page" from="148" to="160" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">BioBERT: a pre-trained biomedical language representation model for biomedical text mining</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>So</surname></persName>
		</author>
		<idno type="DOI">10.1093/bioinformatics/btz682</idno>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="1234" to="1240" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Decoupled weight decay regularization</title>
		<author>
			<persName><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.1711.05101</idno>
		<idno type="arXiv">arXiv:1711.05101</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Camem BERT: A tasty french language model</title>
		<author>
			<persName><forename type="first">L</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">P J</forename><surname>Ortiz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Dupont</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Romary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>De La Clergerie</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.645</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics (Association for Computational Linguistics)</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics (Association for Computational Linguistics)</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="7203" to="7219" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Insect pest management under climate change</title>
		<author>
			<persName><forename type="first">N</forename><surname>Massod</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Akram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fatime</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mubeen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hussain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shakeel</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-79408-8_15</idno>
	</analytic>
	<monogr>
		<title level="j">Build. Clim. Resil. Agri. Theor. Pract. Future Prespect</title>
		<imprint>
			<biblScope unit="page" from="225" to="237" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">What happens to BERT embeddings during fine-tuning?</title>
		<author>
			<persName><forename type="first">A</forename><surname>Merchant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Rahimtoroghi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Pavlick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Tenney</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.blackboxnlp-1.4</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Third BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP (Association for Computational Linguistics)</title>
		<meeting>the Third BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP (Association for Computational Linguistics)</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="33" to="44" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Conditional generative adversarial nets</title>
		<author>
			<persName><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Osindero</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.1784</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">On the stability of fine-tuning BERT: Misconceptions, explanations, and strong baselines</title>
		<author>
			<persName><forename type="first">M</forename><surname>Mosbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Andriushchenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hlakow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">9th International Conference on Learning Representations, ICLR 2021, Virtual Event</title>
		<meeting><address><addrLine>Austria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note>OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Climate change and its consequences in agriculture</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">A</forename><surname>Mozaffari</surname></persName>
		</author>
		<idno type="DOI">10.5772/intechopen.101444</idno>
	</analytic>
	<monogr>
		<title level="m">The Nature, Causes, Effects and Mitigation of Climate Change on the Environment</title>
		<editor>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Harris</surname></persName>
		</editor>
		<meeting><address><addrLine>Rijeka: IntechOpen</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Validating GAN-BioBERT: a methodology for assessing reporting trends in clinical trials</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Myszewski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Klossowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Meyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Bevil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Klesius</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Schroeder</surname></persName>
		</author>
		<idno type="DOI">10.3389/fdgth.2022.878369</idno>
	</analytic>
	<monogr>
		<title level="j">Front. Digit. Health</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">878369</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Internet of things (iot) and cloud computing for agriculture: an overview</title>
		<author>
			<persName><forename type="first">V</forename><surname>Patil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">A</forename><surname>Al-Gaadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Biradar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Madugundu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Agro-Informatics and Precision Agriculture</title>
		<meeting>Agro-Informatics and Precision Agriculture<address><addrLine>India</addrLine></address></meeting>
		<imprint>
			<publisher>AIPA</publisher>
			<date type="published" when="2012">2012. 2012</date>
			<biblScope unit="page" from="292" to="296" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Computer vision and artificial intelligence in precision agriculture for grain crops: a systematic review</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">I</forename><surname>Patrício</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Rieder</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.compag.2018.08.001</idno>
	</analytic>
	<monogr>
		<title level="j">Comput. Electron. Agric</title>
		<imprint>
			<biblScope unit="volume">153</biblScope>
			<biblScope unit="page" from="69" to="81" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">An investigation of social media&apos;s roles in knowledge exchange by farmers</title>
		<author>
			<persName><forename type="first">T</forename><surname>Phillips</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Klerkx</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mcentee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">13th European International Farming Systems Association (IFSA) Symposium, Farming systems: facing uncertainties and enhancing opportunities</title>
		<meeting><address><addrLine>Chania</addrLine></address></meeting>
		<imprint>
			<publisher>International Farming Systems Association Europe</publisher>
			<date type="published" when="2018-07">2018. July 2018</date>
			<biblScope unit="page" from="1" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">#farming365 -exploring farmers&apos; social media use and the (re)presentation of farming lives</title>
		<author>
			<persName><forename type="first">M</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Robertson</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.jrurstud.2021.08.028</idno>
	</analytic>
	<monogr>
		<title level="j">J. Rural Stud</title>
		<imprint>
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="page" from="99" to="111" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Improved techniques for training GANs</title>
		<author>
			<persName><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Chueng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th International Conference on Neural Information Processing Systems</title>
		<meeting>the 30th International Conference on Neural Information Processing Systems<address><addrLine>Barcelona</addrLine></address></meeting>
		<imprint>
			<publisher>Curran Associates</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2234" to="2242" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Semisupervised annotation of portuguese hate speech across social media domains</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">B</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">C</forename><surname>Matos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Carvalho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Batista</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ribeiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">11th Symposium on Languages, Applications and Technologies</title>
		<meeting><address><addrLine>SLATE</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022">2022. 2022</date>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">JGAN-BERT, an adversarial learning architecture for paraphrase identification</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">T</forename><surname>Ta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">B S</forename><surname>Rahman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Najjar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Geibukh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">CEUR Worksho Proc.</title>
		<imprint>
			<biblScope unit="volume">3202</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Towards smart farming and sustainable agriculture with drones</title>
		<author>
			<persName><forename type="first">P</forename><surname>Tripicchio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Satler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Dabisias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Ruffaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">A</forename><surname>Avizzano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 International Conference on Intelligent Environments</title>
		<meeting><address><addrLine>Prague</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="140" to="143" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Social Sensing: Building Reliable Systems on Unreliable Data, 1st Edn</title>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Abdelzaher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Kaplan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
			<publisher>Morgan Kaufmann Publishers Inc</publisher>
			<pubPlace>San Francisco, CA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Generative adversarial networks: introduction and outlook</title>
		<author>
			<persName><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F.-Y</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.1109/JAS.2017.7510583</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE/CAA J. Autom. Sin</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="588" to="598" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
