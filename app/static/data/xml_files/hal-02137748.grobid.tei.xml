<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Toward a large-scale and deep phenological stage annotation of herbarium specimens: Case studies</title>
				<funder ref="#_EHW4JYW">
					<orgName type="full">Agence Nationale de la Recherche (CEBA</orgName>
				</funder>
				<funder ref="#_saJFR8t">
					<orgName type="full">iDigBio (U.S. National Science Foundation [NSF</orgName>
				</funder>
				<funder ref="#_cTQFQTH">
					<orgName type="full">New England Vascular Plant</orgName>
				</funder>
				<funder ref="#_8GJnEd5 #_jm6kcYy">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Titouan</forename><surname>Lorieul</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Katelin</forename><forename type="middle">D</forename><surname>Pearson</surname></persName>
							<idno type="ORCID">0000-0001-5228-9238</idno>
						</author>
						<author>
							<persName><forename type="first">Elizabeth</forename><forename type="middle">R</forename><surname>Ellwood</surname></persName>
							<idno type="ORCID">0000-0001-5471-164X</idno>
						</author>
						<author>
							<persName><forename type="first">Hervé</forename><surname>Goëau</surname></persName>
							<idno type="ORCID">0000-0002-7851-4445</idno>
						</author>
						<author>
							<persName><forename type="first">Jean-Francois</forename><surname>Molino</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Patrick</forename><forename type="middle">W</forename><surname>Sweeney</surname></persName>
							<idno type="ORCID">0000-0003-1239-189X</idno>
						</author>
						<author>
							<persName><forename type="first">Jennifer</forename><forename type="middle">M</forename><surname>Yost</surname></persName>
							<idno type="ORCID">0000-0003-1239-189X</idno>
						</author>
						<author>
							<persName><forename type="first">Joel</forename><surname>Sachs</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Gil</forename><surname>Nelson</surname></persName>
							<idno type="ORCID">0000-0001-5471-164X</idno>
						</author>
						<author>
							<persName><forename type="first">Pamela</forename><forename type="middle">S</forename><surname>Soltis</surname></persName>
							<idno type="ORCID">0000-0002-7851-4445</idno>
						</author>
						<author>
							<persName><forename type="first">Jfrancois</forename><surname>Molino</surname></persName>
							<idno type="ORCID">0000-0001-9310-8659</idno>
						</author>
						<author>
							<persName><forename type="first">Erick</forename><surname>Mata-Montero</surname></persName>
						</author>
						<author role="corresp">
							<persName><forename type="first">Pierre</forename><surname>Bonnet</surname></persName>
							<email>pierre.bonnet@cirad.fr</email>
							<idno type="ORCID">0000-0002-2828-4389</idno>
						</author>
						<author>
							<persName><forename type="first">Alexis</forename><surname>Joly</surname></persName>
							<idno type="ORCID">0000-0002-2828-4389</idno>
						</author>
						<author>
							<affiliation>
								<orgName>1 University of Montpellier, </orgName>
								<address><addrLine>Montpellier CEDEX 5, France 34095, Montpellier CEDEX 5, France 319 Stadium Drive, Tallahassee, Florida 32306, USA 5801 Wilshire Boulevard, Los Angeles, California 90036, USA Montpellier, France Montpellier, France P.O. Box 208118, New Haven, Connecticut 06520, USA 1 Grand Avenue, San Luis Obispo, California 93407, USA Ottawa, Canada Cartago, Costa Rica Tallahassee, Florida 32306, USA Gainesville, Florida 32611, USA</addrLine></address>
							</affiliation>
						</author>
						<author>
							<affiliation>
								<orgName> 2 Institut national de recherche en informatique et en automatique (INRIA) Sophia-Antipolis, ZENITH team, Laboratory of Informatics, Robotics and Microelectronics-Joint Research Unit, </orgName>
							</affiliation>
						</author>
						<author>
							<affiliation>
								<orgName> 3 Department of Biological Science, Florida State University, </orgName>
							</affiliation>
						</author>
						<author>
							<affiliation>
								<orgName> 4 La Brea Tar Pits and Museum, Natural History Museum of Los Angeles County, </orgName>
							</affiliation>
						</author>
						<author>
							<affiliation>
								<orgName> 5 AMAP, Université de Montpellier, CIRAD, CNRS, INRA, IRD, </orgName>
							</affiliation>
						</author>
						<author>
							<affiliation>
								<orgName> 6 CIRAD, UMR AMAP, </orgName>
							</affiliation>
						</author>
						<author>
							<affiliation>
								<orgName> 7 Division of Botany, Peabody Museum of Natural History, Yale University, </orgName>
							</affiliation>
						</author>
						<author>
							<affiliation>
								<orgName> 8 Department of Biological Sciences, California Polytechnic State University, </orgName>
							</affiliation>
						</author>
						<author>
							<affiliation>
								<orgName> 9 Agriculture and Agri-Food Canada, </orgName>
							</affiliation>
						</author>
						<author>
							<affiliation>
								<orgName> 10 School of Computing, Costa Rica Institute of Technology, </orgName>
							</affiliation>
						</author>
						<author>
							<affiliation>
								<orgName> 11 iDigBio, Florida State University, </orgName>
							</affiliation>
						</author>
						<author>
							<affiliation>
								<orgName> 12 Florida Museum of Natural History, University of Florida,</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Toward a large-scale and deep phenological stage annotation of herbarium specimens: Case studies</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">0A50B9AF4D632E77E139B062F3E4E2A4</idno>
					<idno type="DOI">10.1002/aps3.1233</idno>
					<note type="submission">Submitted on 23 May 2019 received 7 September 2018; revision accepted 28 January 2019.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-04-12T14:44+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>convolutional neural network</term>
					<term>deep learning</term>
					<term>herbarium data</term>
					<term>natural history collections</term>
					<term>phenological stage annotation</term>
					<term>visual data classification</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>come from teaching and research institutions in France or abroad, or from public or private research centers. L'archive ouverte pluridisciplinaire</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Global changes that threaten biodiversity are numerous and rapidly increasing. To reduce the human impact on biodiversity loss, the scientific community must develop new, multi-disciplinary approaches that incorporate the most recent advances in biodiversity informatics, large occurrence and trait data sets, and large-scale taxonomic and ecological analyses. The development of the Open Science movement, citizen science initiatives, global digitization of natural history collections, and online cyberinfrastructure provide new opportunities for mobilizing and integrating massive amounts of biological data, driving the discovery of complex patterns and new hypotheses for further study <ref type="bibr" target="#b30">(Soltis and Soltis, 2016;</ref><ref type="bibr" target="#b0">Allen et al., 2018)</ref>. This large-scale data integration across disciplines, continents, and infrastructures allows new investigations in ecology, systematics, and evolution that offer the capacity to make biodiversity projections and provide crucial information for scientists and other stakeholders (e.g., land use managers, policy makers, agricultural producers, mining contractors). In this study, we investigate means to enable such data integration within the context of phenological studies using digitized herbarium specimens.</p><p>Herbarium specimens are dried and pressed plants or parts of plants that have been mounted on archival paper; labeled with data about, e.g., the identification of the plant, collection locality, collection date, and collector; and stored in natural history collections called herbaria. These plant specimens provide crucial data for the study of plant diversity, ecology, evolution, genetics, and biodiversity, to name only a few <ref type="bibr" target="#b13">(Graham et al., 2004)</ref>. When herbarium specimens are "digitized"-converted into a digital format by imaging and transcription of label data-they have even greater potential for answering major research questions related to the recent impact of humanity on biodiversity <ref type="bibr" target="#b7">(Davis et al., 2015;</ref><ref type="bibr" target="#b29">Soltis, 2017;</ref><ref type="bibr" target="#b15">James et al., 2018;</ref><ref type="bibr" target="#b21">Meineke et al., 2018;</ref><ref type="bibr" target="#b31">Soltis et al., 2018)</ref>. These millions of herbarium records have accumulated a valuable heritage and knowledge of plants over centuries, across all continents. Recent ambitious initiatives in the United States, Australia, Brazil, and Europe are digitizing this information and making it available online to the scientific community and general public. Herbariumbased phenological research offers the potential to provide novel insights into plant diversity and ecosystem processes under future climate change <ref type="bibr" target="#b39">(Zalamea et al., 2011;</ref><ref type="bibr" target="#b36">Willis et al., 2017;</ref><ref type="bibr" target="#b37">Yost et al., 2018)</ref>.</p><p>Rapid human-induced climate change has affected plant phenology over the past century, with likely impacts on reproductive success, plant-pollinator interactions, and even carbon and nutrient cycling <ref type="bibr" target="#b22">(Menzel et al., 2006;</ref><ref type="bibr" target="#b12">Gordo and Sanz, 2010;</ref><ref type="bibr" target="#b1">Bartomeus et al., 2011;</ref><ref type="bibr" target="#b9">Ellwood et al., 2013;</ref><ref type="bibr" target="#b27">Primack et al., 2015)</ref>. However, the study of phenological shifts is only possible with historical and long-term data sets that can establish the phenological patterns of plants before human-induced climate change. Herbarium data sets are therefore essential as unique, verifiable sources of historic information on species localities and phenological states. Most phenological studies are based on individual and manual phenological evaluation conducted by researchers or a small number of professionals, which is a laborious and resource-intensive process.</p><p>Annotating the tens of millions of existing digitized specimens for phenology requires an unrealistic amount of work for professional botanists to carry out in a reasonable time. Citizen scientists are capable of making substantial contributions to digital biodiversity data <ref type="bibr" target="#b10">(Ellwood et al., 2018)</ref>; however, using citizen science data for ecological studies often requires complementary annotations to ensure data quality. A remarkable example of the complementary contributions provided by automated and volunteer classifications is provided by <ref type="bibr" target="#b18">Jones et al. (2018)</ref> who have used them jointly to automatically identify wild animals from camera traps.</p><p>Automated approaches, such as computer vision and machine learning methods, can complement valuable citizen science data and may help bridge the "annotation gap" <ref type="bibr" target="#b33">(Unger et al., 2016)</ref> between existing data and research-ready data sets. Deep learning approaches, in particular, have been recently shown to achieve impressive performance on a variety of predictive tasks such as species identification <ref type="bibr" target="#b17">(Joly et al., 2017;</ref><ref type="bibr">Wäldchen et al., 2018)</ref>, plant trait recognition <ref type="bibr" target="#b38">(Younis et al., 2018)</ref>, plant species distribution modeling <ref type="bibr" target="#b2">(Botella et al., 2018)</ref>, and weed detection <ref type="bibr" target="#b23">(Milioto et al., 2018)</ref>. <ref type="bibr" target="#b4">Carranza-Rojas et al. (2017</ref><ref type="bibr">, 2018a)</ref> reported the first attempts to use deep learning to tackle the difficult task of identifying species in large natural history collections and showed that convolutional neural networks trained on thousands of digitized herbarium sheets are able to learn highly discriminative patterns from pressed and dried specimens. These results are very promising for extracting a broad range of other expert annotations in a fully automated way. However, as with any statistical learning method, convolutional neural networks are sensitive to bias issues, including the way in which the training data sets are built <ref type="bibr">(Carranza-Rojas et al., 2018b)</ref>, necessitating methodological considerations to avoid bias and misleading conclusions. Moreover, as good as the prediction might be on average, the quality of the produced annotations can be very heterogeneous from one sample to another, depending on various factors such as, e.g., the morphology of the species, the storage conditions in which the specimen was preserved, the age of the specimen, or the skill of the annotator.</p><p>The goal of this study is to evaluate the capacity of deep learning technologies for large-scale phenological annotation of herbarium data. We test new methods and algorithms to automate the scoring of reproductive phenological stages within a huge amount of digitized material, to provide significant resources for the ecological and organismal scientific communities. Specifically, we aim to answer three questions: (1) Can fertility, i.e., presence of reproductive structures, be automatically detected from digitized specimens using deep learning? (2) Are the detection models generalizable to different herbarium data sets? and (3) Is it possible to finely automatically record stages (i.e., phenophases) within longer phenological events on herbarium specimens? To our knowledge, this is the first time that such an analysis has been conducted at this scale, on such a large number of herbarium specimens and species. A study at this taxonomic scale reveals the opportunities and limits of such an approach for large ecological studies, which are discussed in the following sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>METHODS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data sets</head><p>To evaluate our approach at different levels (in terms of information precision) and on different floras (from temperate to equatorial), four data sets of specimens from American herbaria were used in this study.</p><p>Three data sets consist of selected specimens from herbaria located in different geographic and environmental regions. Each specimen of these three data sets was annotated with the following fields: family, genus, species name, fertile/non-fertile, presence/ absence of flower(s), and presence/absence of fruit(s). Based on a data curation pipeline, our resulting data set was composed of 163,233 herbarium specimens belonging to 7782 species, 1906 genera, and 236 families. Specimens were annotated as "fertile" if any reproductive structures were present, such as sporangia (ferns), cones (gymnosperms), flowers, or fruits (angiosperms). Non-fertile specimens were those that lacked any reproductive structures. Most herbarium specimens in this study were annotated by herbarium assistants, curators, technicians, or other personnel responsible for digitizing specimen label data (e.g., trained undergraduate student workers), often long after the collection event. Collectors may have included the phenological status of the sampled plant or population on the specimen label or in the field notes used to create the label, in which case the digitization technician may have annotated the specimen record accordingly. More often than not, however, the digitization technician must determine from the specimen whether reproductive structures are present. Occasionally, specimens are annotated after digitization, e.g., for specific research projects, in subsequent data quality steps, or with further identifications of the specimen. A detailed description of the herbarium specimen annotation process is provided in Appendix 1.</p><p>The fourth data set consists of 20,371 herbarium specimens from 11 genera in the sunflower family (Asteraceae). These specimens were annotated by one co-author (K.D.P.) for a study of phenological trends in the southeastern United States <ref type="bibr">(Pearson, 2019a)</ref>. The distinction of this data set from the other three data sets is that (1) it is annotated with fine-grained phenophase scores rather than presence/absence attributes (see description below), (2) it is annotated by one person only and not a diversity of persons distributed in different herbaria, and (3) all specimens were annotated from images of digitized specimens rather than from physical herbarium specimens or the wild plant.</p><p>Each of these data sets is described below and presented in Each recorded specimen was annotated by K.D.P. for quartile percentages (0%, 25%, 50%, 75%, or 100%) of (1) closed buds, (2) buds transformed into flowers, and (3) fruits. According to the distribution of these three categories for each specimen, a phenophase code was computed. The method used to compute this code is provided in Table <ref type="table" target="#tab_3">2</ref>. Figure <ref type="figure" target="#fig_1">2</ref> provides an illustration of the nine phenophases recorded for Coreopsis gladiata Walter in this data set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Evaluated deep learning framework</head><p>We considered each of our experiments as a classification task, and we focused on the use of convolutional neural networks (CNNs), which have been shown to considerably improve the accuracy of automated visual classification of botanical data compared to previous methods <ref type="bibr">(Wäldchen and Mäder, 2018)</ref>. The main strength of this technology is the ability to learn discriminant visual features directly from the raw pixels of the images without being negatively impacted by the high dimensionality of the input data. Because the sizes of our data sets are relatively small for training these types of models (tens of thousands and hundreds of thousands compared to tens of millions usually required), we used transfer learning techniques <ref type="bibr" target="#b28">(Shin et al., 2016)</ref> to improve our models. We took a ResNet50 network <ref type="bibr" target="#b14">(He et al., 2016)</ref> pre-trained on tens of millions of images from the ImageNet data set <ref type="bibr" target="#b8">(Deng et al., 2009)</ref> and fine-tuned it on our data sets. ResNet50 was chosen because it is a state-of-the-art model, and it is widely used in image classification tasks. Moreover, pre-trained parameters can be easily found for most Deep Learning frameworks, in particular, for PyTorch (https://pytorch.org/), which we used for the experiments. Although the input image size of the original ResNet50 model was fixed to 224 × 224 pixels, herbarium specimens are usually digitized at high resolution to record fine visual information. To cope with the rectangular shape of these specimens and to preserve as much detail as possible, we adapted the ResNet50 model to use higherresolution images. This is possible because the convolutional layers of the model are not constrained by the input image size; we can thus retain them while only modifying the final pooling layer to operate on all spatial dimensions and retraining the final classification layer from scratch. Although the dimension of the images is increased, this modification does not change the number of parameters in the network. Details on this model adaptation are provided in Appendix 4.</p><p>We tested two resolutions: 400 × 250 pixels and 850 × 550 pixels, which are both significantly larger than the usual resolution of CNNs (in most cases less than 300 × 300 pixels). In the following  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Description of experiments</head><p>Assessing performance of deep learning models-Three experiments were conducted in this study to evaluate the performances of four different models for automated annotation of phenology on herbarium specimens. The first two experiments used the three herbarium data sets (NEVP, FSU, CAY), and the last used the PHENO data set.</p><p>1. The first experiment (EXP1-Fertility) aimed to evaluate the capacity of the CNN to detect fertile material (i.e., specimens with reproductive structures present), based on the analysis of three test sets (A, B, C). Test set A (Random-split) was a random set of herbarium specimens that were not used as training data of the CNN model but that belonged to species and collections represented in the training data. This test set of 13,415 specimens allowed us to evaluate a scenario in which one herbarium collection uses its annotated specimens to train a model that automatically annotates the un-annotated specimens. Test set B (Species-split) was a selection of herbarium specimens belonging to species that were not present in the training data. For species selection, we first ordered species by decreasing number of herbarium specimens. We then selected from the full species list, one species out of 10, starting at the tenth. All specimens belonging to the species selected were used as test data. This test set of 14,539 specimens allowed us to evaluate a scenario in which the trained model must annotate specimens belonging to species that were never used for the training phase. This is important to evaluate as herbarium collections regularly receive specimens of species that were not previously in their collections. For herbaria selection, we first ordered them by decreasing number of herbarium specimens. We then selected one herbarium out of every two from the full herbaria list, starting at the second. All specimens belonging to the herbaria selected were used as test data. This test set of 14,540 specimens enabled evaluation of trained model performance for entirely new herbarium collections. Each collection has its own methodology for mounting plants (e.g., with particular glue or thread), as well as unique labels, annotations, imaging scale bars, and stamps, that can potentially influence annotation performance. Test set size and percentage of fertile specimens are provided in Table <ref type="table" target="#tab_5">3</ref>. 2. The second experiment (EXP2-Fl.Fr) evaluated the automated detection of flowers and fruits on herbarium specimens of angiosperms for our three test sets (A, B, C). Gymnosperms and ferns have been excluded from this experiment. This experiment extends one step beyond the previous one in terms of information precision, as it evaluated whether fertility is related to the presence of flower and/or fruit. 3. The third experiment (EXP3-Pheno) dealt with the automated phenophase evaluation, which involved a higher number of visual classes. The test set of this experiment consisted of a random sampling of 20% of the original PHENO data set (the remaining 80% being used for training).</p><p>Data and models used and produced for this study are accessible on Zenodo <ref type="bibr" target="#b19">(Lorieul, 2019;</ref><ref type="bibr" target="#b20">Lorieul et al., 2019)</ref>, a free and open platform for preserving and sharing research output.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comparing model results to secondary manual annotation-To</head><p>compare results obtained by our four trained models (i.e., two models for the first experiment, one model for the second experiment, and a last model for the third experiment) to human expertise, the co-author P.B., who had not previously been involved in annotating these data sets, manually annotated 100 herbarium specimens of each test set (a first subset of test set A used in EXP1-Fertility and in EXP2-Fl.Fr, a second subset from the test set used in EXP3-Pheno). For the first (EXP1-Fertility) and second (EXP2-Fl.Fr) experiments, 100 herbarium specimens were randomly selected from test set A with a proportion of 25% of specimens from the four different categories: (a) true positives (i.e., with flower and/or fruit and correctly annotated by our model), (b) true negatives (i.e., without flower and/or fruit and correctly annotated by our model), (c) false positives (i.e., without flower and/or fruit and wrongly annotated by our model), and (d) false negatives (i.e., with flower and/or fruit and wrongly annotated by our model). The subset of specimens chosen for secondary manual annotation was potentially highly difficult to annotate by visual analysis of digitized specimens for a human, as it contains 50% wrongly annotated specimens by our model (categories c and d), and it was designed with such proportions in order to particularly inspect cases of automated annotation errors. For the third experiment (EXP3-Pheno), 100 specimens randomly sampled from the test set were annotated by P.B., who did not use external resources to code phenophases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Assessing performance of deep learning models</head><p>Results from the three experiments are provided in Tables <ref type="table" target="#tab_9">3, 4, 5,  6,</ref> and<ref type="table" target="#tab_10">7</ref>. Results of EXP1-Fertility, presented in Table <ref type="table" target="#tab_5">3</ref>, show high performance for the correct detection of fertile material. Regardless of the strategy used to produce the three test sets, all achieved at least 92% correct detection. Performance of the models decreased from test set A (Random-split) to test set B (Species-split) and test set C (Herbarium-split), but the performance gap between the three test sets is small, with, respectively, 1.7% and 4.3% difference between the best and worst performing ResNet50-Large and ResNet50-VeryLarge models. In addition, use of the ResNet50-VeryLarge model slightly increased the number of correct detections for test sets A and B compared to ResNet50-Large.</p><p>The two models (i.e., ResNet50-Large and ResNet50-VeryLarge) showed distinct performances with each of the three test sets. Figure <ref type="figure">3</ref> shows the receiver operating characteristic (ROC) curves obtained for ResNet50-Large (Fig. <ref type="figure">3A</ref>) and ResNet50-VeryLarge (Fig. <ref type="figure">3B</ref>). At a false positive rate of 5%, the ResNet50-Large model achieved a true positive rate of 80.3%, and the ResNet50-VeryLarge model a true positive rate of 89.6%. At a false positive rate of 1%, the true positive rates of these models were 45.7% and 64.0%, respectively. These results highlight the importance of using higherresolution images for such tasks. Due to better performance of the ResNet50-VeryLarge model, results provided in the remainder of this article (EXP2-Fl.Fr and EXP3-Pheno) are based on this model architecture.</p><p>Results from test set A for angiosperms, gymnosperms, and ferns are provided in Table <ref type="table" target="#tab_7">4</ref>. Despite a low number of training images, the model achieved high performance with gymnosperms and ferns, with 100% and 95.7% correct detection, respectively. Figure <ref type="figure">4</ref> shows results from test set A for the NEVP, FSU, and CAY data sets. Detection of reproductive structures was more effective on specimens from the CAY data set than on specimens from NEVP and FSU. This can probably be explained by a combination of complementary factors such as: (1) a higher number of specimens per species in the CAY data set than the FSU data set (with a mean of 21 specimens per species in CAY and 14 specimens per species in FSU), and (2) highly visible reproductive structures of tropical and equatorial species compared to flowers and fruits of temperate species in the NEVP data set.</p><p>Regardless of the test set used, results of EXP2-Fl.Fr (Table <ref type="table" target="#tab_8">5</ref>) show correct detection of the presence of flowers and fruits in more than 81.0% of cases for flowers and 76.6% for fruits. Flower detection was more efficient than fruit detection in the three test sets as shown in the ROC curves of Figure <ref type="figure">5</ref>. It must be noted that there is a higher proportion of specimens with flowers than fruits in the training data as shown in Table <ref type="table" target="#tab_8">5</ref>. The model had more data to capture the concept of flower than fruit, resulting in lower fruit detection accuracy.</p><p>Results from the fine-grained phenology annotation (Table <ref type="table" target="#tab_9">6</ref>) conducted in EXP3-Pheno show that correct fine-grained classification accuracy is obtained in 43.4% of cases. This capacity increases to 69% for coarse classification accuracy. It is interesting to see that the best classification accuracy is provided for classes 1, 5, and 9, which correspond to all buds, peak flowering, and all fruit phenophases, respectively (Table <ref type="table" target="#tab_10">7</ref>). When we examine error distributions of these classifications (Fig. <ref type="figure">6</ref>), we see that consistent classifications are obtained in 67.1% of cases with one class of error and in 81.7% of cases with two classes of error. The confusion matrix (Fig. <ref type="figure" target="#fig_4">7</ref>) shows the most common confusions between phenophases. Phenophases 4 and 6 are the least well predicted. This is most likely related to the fact that they are the least common in the training data set, and they are visually very similar to phenophase 5. Indeed, these two phenophases have a potentially high percentage of flowering structures (more than 75%), whereas specimens of phenophase 5 have between 50% and 100% of their buds in flower. These three phenophases (i.e., phenophases 4, 5, 6) are also the only ones to combine (at different percentages) presence of buds, flowers, and fruits on the same specimens. All other categories involve specimens with a combination of two different reproductive attributes only (buds and flowers, buds and fruits, or flowers and fruits). For these reasons, most specimens of phenophases 4 and 6 could be easily mistaken as phenophase 5 by the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comparing model results to secondary manual annotation</head><p>The re-annotation of 100 herbarium specimens of test set A from images of specimens resulted in 80% accuracy with original phenological annotations. We emphasize that this test subset is focused on some of the most difficult specimens to annotate, as 50% of them were wrongly annotated by our ResNet50-VeryLarge model.  The fact that the human observer (co-author P.B.) did not achieve 100% accuracy on the subset of the 100 specimens of test set A can be explained by one of the following cases:</p><p>Case 1: Some specimens (6%) annotated as "fertile" in the original annotation were annotated as "non-fertile" by P.B. because no reproductive structures were visible on the digitized specimen. This could be because (a) the original annotation was based on label text, which may indicate a state different from the specimen duplicate (e.g., the population or some of the specimen duplicates had reproductive material, but the particular duplicate examined did not), or (b) the flowers or fruits had disappeared during specimen manipulation/ preparation (e.g., fallen off, been hidden from view in a fragment folder glued to the specimen, or been obscured by large leaves). Case 2: Some specimens (6%) with closed reproductive buds were annotated "fertile" by P.B. but were annotated as "non-fertile" by the original annotator. Case 3: Some specimens (5%) described as "non-fertile" by the original annotator were correctly detected as fertile by P.B. Case 4: Some specimens (2%) were described as "fertile" on the label of the herbarium sheet and annotated as "fertile" by P.B., but they were annotated as "non-fertile" by the original observer. Case 5: One fertile specimen was not detected by P.B., due to the small size of the reproductive structures.</p><p>Cases 1 and 2 particularly highlight the fact that original annotations produced by the collectors, herbarium assistants, or digitization technicians can be different from annotations produced by the visual analysis of digitized specimens, even if all of them correctly followed a strict procedure. Case 1 is intrinsically related to herbarium management practices (that try to protect reproductive structures as much as possible, sometimes by hiding them in a folder), while case 2 is more related to definition and perception of the "fertile stage" on a plant specimen, which is sometimes hard to define, as reproductive structures are a continuum from tiny, closed buds to large, obvious fruits.</p><p>It is noteworthy that five of the six herbarium specimens annotated as "non-fertile" by P.B. because no reproductive structures were visible (case 1) were similarly annotated by our trained model. Likewise, four of the five herbarium specimens annotated as "fertile" by P.B. in contrast to the incorrect original annotation (case 3) were annotated similarly by the trained model. This illustrates the potential of this technology to potentially detect incorrect annotations in herbarium databases. It should be noted that even if P.B. achieved 80% accuracy, the error rate in the original annotations of the very difficult subset of test set A is only 7% (cases 1, 2, and 5 cannot be considered as errors).</p><p>Human annotations of 100 randomly selected specimens from the PHENO test set by P.B., a non-expert of that flora, offered complementary results. This secondary annotator achieved 42% and 68% accuracy with the original annotations with zero and one class of error, respectively. These results, very close to our trained model, highlight the difficulty of the task for a non-expert who has not been trained on a particular taxonomic group, as well as the high likelihood for error when annotating specimens for fine-scale phenological stages. More than 70% of inconsistent annotations from our model were also wrongly annotated by the secondary annotator. More than 25% of these errors were exactly the same and were mostly within one class of difference from the original annotation. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DISCUSSION</head><p>These experiments clearly demonstrate the potential of deep learning technologies for automating phenological annotation of herbarium specimens. These promising results obtained for 7782 species of plants representing angiosperms, gymnosperms, and ferns suggest that it is possible to consider large-scale phenological annotation across broad phylogenetic groups. Results obtained for the fertility detection experiment (EXP1-Fertility, 96.3% consistent annotations) and for the flower and fruit experiments (EXP2-Fl.Fr, 84.3% consistency for flower annotations and 80.9% for fruit annotations) are similar for the different test sets examined. This is encouraging, as it confirms that in the case of the results for test set B, trained CNNs are able to recognize visual features that illustrate fertility on plants, even if they are not learned on the same species in the training and testing data sets. As most herbarium data sets present a long tail distribution of their data (with most species represented by a small number of specimens), it is important to confirm high capacity of correct annotations even for rare species for which training data are often not available.</p><p>It is interesting and perhaps surprising that the models were more successful at detecting specimens with flowers present than specimens with fruit present. This result may be explained by several potential factors: (1) flowers can be more conspicuous because of their lighter colors, compared to dry fruits which are often darker than or a similar color to leaves or stems; (2) for a particular species, numbers of flowers on an inflorescence may be greater than the number of fruits, as several flowers can abort before developing into fruits; (3) mature fruits are often less well attached to the rest of the plant compared to flowers and can thus be easily lost during specimen preparation or handling; and (4) less data were available for the training phase of the fruit detection than for the flower detection task.</p><p>Considering the difficulty of the task, the good performances achieved by models using data set C (Herbarium-split) indicate that new data sets coming from herbaria that were not in the training data set can still be correctly annotated for phenology. Because managing conditions, specimen preparations, digitization parameters, and taxonomic annotations can vary considerably among different herbaria, this result is promising.</p><p>The similar results on the three different data sets (NEVP, FSU, and CAY) representing three distinct American floras demonstrate that our approach can be efficient for diverse plant groups from different environments and habitats and with highly distinct morphologies. The efficiency among the three studied clades (angiosperms, gymnosperms, and ferns) is also of great importance, demonstrating that this approach is effective on distant clades with highly dissimilar reproductive structures.</p><p>The work with complementary annotations provided by P.B. on a subset of test set A has highlighted the difficulty of the task for some of the most difficult specimens. Indeed, fertility is expressed by a wide variety of reproductive structures in terms of size, shape, color, etc. Furthermore, these reproductive structures are in continuous development and can be very inconspicuous at some development stages, as illustrated in Figure <ref type="figure" target="#fig_6">8</ref>. Detection of fertility, in these cases, can be very difficult without the real specimen in hand. Because of the broad taxonomic coverage of the data analyzed, there are likely several taxa in the data set to which these circumstances apply.</p><p>The development of these technologies and their capacities to work on larger image sizes will undoubtedly benefit the herbarium, taxonomic, and research communities, as herbarium images are usually digitized and stored at very high resolution to permit investigation of small botanical characters. One strategy that could be explored to improve model performances is taking into account several different windows on each herbarium specimen, as CNNs are actually far from able to function at the original size of herbarium images. A complementary strategy could be to train models using complementary large, annotated data sets of observations of living plants in the field, which are now largely produced by networks of field botanists and/or amateur naturalists (e.g., iNaturalist or Pl@ntNet <ref type="bibr" target="#b16">[Joly et al., 2016]</ref> networks), to enrich CNNs with specific images of fertile material. This work could open the door to new avenues of citizen science initiatives, such as annotation of phenological stages of living plants. Furthermore, as multimedia data streams are now much more easily produced by, e.g., drones equipped with cameras, such automated tasks could offer new opportunities for production of large volumes of phenological data. The adoption of these automated techniques by collection managers, particularly within the framework of the established Plant Phenology Ontology <ref type="bibr" target="#b3">(Brenskelle et al., 2019)</ref>, could make it possible to (1) pre-annotate large volumes of herbarium specimens that have not yet been annotated, which could then be revised by collaborative approaches; (2) have a standardized methodology that avoids bias related to expertise and perception variability of annotators; (3) use pre-annotated herbarium specimens for phenological studies at large scales that would not be possible with human investment alone; and (4) identify and correct annotation mistakes made by human annotators.</p><p>The work on automated fine-scale phenophase detection was carried out on species of Asteraceae, a group usually known for its small flowers and fruits, which cannot be easily detected on several specimens (as illustrated in Fig. <ref type="figure" target="#fig_1">2</ref>). Such work on other plant families with more typical flowers and/or fruits (such as Fabaceae, Rosaceae, Rutaceae, and Bignoniaceae) could be informative as we can presume a stronger capacity of this approach to correctly detect fine phenological stages in these taxa. Other large herbaceous groups, such as Lamiaceae, Poaceae, or Cyperaceae, with completely different morphological attributes during their phenological cycles would also be interesting to evaluate using our model. The evaluation of the robustness of this fine-grained phenological classification model on tree species could also be informative, as it could significantly improve the capacity to study forest phenological cycles. <ref type="bibr">Pearson (2019b)</ref> and <ref type="bibr" target="#b11">Ellwood et al. (2019)</ref> demonstrate that fine-scale phenophases result in models that are more robust, yet without the use of a CNN annotating specimens for fine-scale phenophases is a resource-intensive task that is not likely to be broadly embraced. A possible way to improve performances of the model that we trained for phenophase detection, especially with an eye to research that is dependent on fine-scale phenophases, could be to take into account the order of the phenophases and develop a potential counting system in the trained models. These are some of the strategies that will be investigated in our future work.</p><p>The work presented here opens the door for the adaptation of similar approaches to other detection and annotation tasks on botanical material such as (1) counting reproductive structures (e.g., flowers, inflorescences, leaves, fruits), especially in agricultural contexts for yield evaluation; (2) pathology detection (for plant pathologists interested in investigating disease on herbarium specimens, e.g., <ref type="bibr" target="#b21">Meineke et al., 2018)</ref>; and (3) annotation of morphological features of specimens, e.g., for rapid selection by professional taxonomists. New and enriched visual botanical  The global accuracy on the whole test set is computed using the average of the accuracy on each subset weighted by their proportion in the whole test set, i.e., 84.1%, 1.7%, 12.2%, and 2.0%, respectively, for the true positive, false positive, true negative, and false negative subsets.  APPENDIX 1. Details of the herbarium specimen annotation process.</p><p>Data collected on herbarium specimens in this study were obtained according to the following method:</p><p>1. Plant specimens were collected in the field by a collector, who recorded information on the collecting location, date, and potential scientific name of the specimen. Additionally, a collector may have recorded a description of the environment in which the plant was growing and possibly a description of the plant specimen itself, including details about the plant's reproductive condition. In common botanical practice, a plant specimen is considered a gathering, or part of a gathering, of a single species or infraspecific taxon made at one time by a particular collector at a particular place. Thus, a specimen may consist of multiple individuals of the same taxon (each of these samples is called a "specimen duplicate"). In such cases, it is important to remember that the data provided by the collector are not necessarily a description of an individual specimen duplicate. This can result in a difference between the description on the specimen label and the visible information on the specimen duplicate in the herbarium. 2. Collector sends his specimens to one or several herbaria, accompanied by a label with the collector-provided data mentioned above.</p><p>http://www.wileyonlinelibrary.com/journal/AppsPlantSci © 2019 Lorieul et al.</p><p>3. Physical specimens are mounted on herbarium sheets with a herbarium label. Information from the herbarium label is then transcribed by herbarium personnel into an electronic database. In some workflows, details about the reproductive condition reported on the label are recorded (e.g., into a specific field or into a field containing the entire plant description).</p><p>4. The specimen-level information recorded in the herbarium database system can be updated when the herbarium specimen or image of the specimen is studied after its initial preparation. These updates can include annotations related to identification (based on opinions of experts who have analyzed the specimen).</p><p>Annotations related to the reproductive condition of the specimens can also be performed if, for example, that information was not provided on the label or was not recorded in the initial database transcription step. In some rare cases, an annotation may report that a herbarium specimen is without reproductive structures, while the label notes that plant was observed in the field with reproductive attributes. This second example can be understood by several possibilities: (1) the collector was not able to collect a fertile part of the plant (e.g., the fertile material was too high or too far from the collector, which can be often the case in tropical forests), ( <ref type="formula">2</ref>) not all of the duplicates have fertile material, or (3) flowers and fruits can be fragile and become separated from the specimen when specimens are old or frequently manipulated.</p><p>5. Our data are based on the most recent annotations present in institutional herbarium data management systems or in collaborative data sharing portals (e.g., in the case of NEVP). These annotations are not necessarily produced by the collector, but by various trained herbarium personnel.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>FIGURE 1 .</head><label>1</label><figDesc>FIGURE 1. Illustration of the different phenological stages of Tilia americana on the NEVP herbarium data set. (A) Non-fertile specimen, (B) specimen with open flowers, (C) specimen with ripe fruits.</figDesc><graphic coords="5,43.62,83.97,518.78,234.14" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>FIGURE 2 .</head><label>2</label><figDesc>FIGURE 2. Illustration of the nine different phenophases of Coreopsis gladiata recorded in the PHENO data set.</figDesc><graphic coords="6,49.63,84.00,518.74,303.67" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>FIGURE 3 .FIGURE 4 .</head><label>34</label><figDesc>FIGURE 3. Fertility receiver operating characteristic (ROC) curves for EXP1-Fertility, with ResNet50-Large (A) and ResNet50-VeryLarge (B). Blue = test set A (Random-split); orange = test set B (Species-split); green = test set C (Herbarium-split); red stars = percentage of fertile specimens correctly detected at a false positive rate of 5%; black stars = percentage of fertile specimens correctly detected at a false positive rate of 1%.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>FIGURE 5 .FIGURE 6 .</head><label>56</label><figDesc>FIGURE 5. Flower detection (A) and fruit detection (B) receiver operating characteristic (ROC) curves for EXP2-Fl.Fr, with ResNet50-VeryLarge. Blue = test set A (Random-split); orange = test set B (Species-split); green = test set C (Herbarium-split).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>FIGURE 7 .</head><label>7</label><figDesc>FIGURE 7. Row-wise normalized confusion matrix of phenophase classification experiment (EXP3-Pheno) using the PHENO data set.</figDesc><graphic coords="11,88.07,99.74,235.22,235.22" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>b</head><label></label><figDesc>Annotations were made by co-author P.B.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>FIGURE 8 .</head><label>8</label><figDesc>FIGURE 8. Illustration of some difficult specimens to annotate in EXP1-Fertility. (A) Fertile specimen of Hymenophyllum hirsutum, wrongly annotated by the ResNet50-VeryLarge model and the human observer (P.B.). Fertility is expressed by small terminal sori (1-1.5 mm in diameter) at the extremity of the lamina. Red arrows show them on a close-up of the lamina. (B) Fertile specimen Cordia fanchoniae, wrongly annotated by the ResNet50-VeryLarge model and correctly annotated by the human observer (P.B.). Fertility is expressed by a small young infructescence (1.3 cm high, marked by red arrows), just after anthesis and before the development of fruits.</figDesc><graphic coords="12,49.59,84.00,518.81,265.99" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1</head><label>1</label><figDesc></figDesc><table><row><cell>:</cell></row><row><cell>• NEVP: This data set of specimens from the New England</cell></row><row><cell>Vascular Plant (NEVP) project was produced by members of</cell></row><row><cell>the Consortium of Northeastern Herbaria (http://neherbaria.</cell></row><row><cell>org/). The data set comprises 42,658 digitized specimens that</cell></row><row><cell>belong to 1375 species and come from several North American</cell></row><row><cell>institutions (listed in Appendix 2). Most of the specimens in this</cell></row><row><cell>data set are from the north-temperate region of the northeastern</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE 1 .</head><label>1</label><figDesc>Description of data sets used on EXP1-Fertility, EXP2-Fl.Fr, and EXP3-Pheno.</figDesc><table /><note><p>a Full</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>data set names Data set acronyms No. of herbarium specimens Fertile proportion Flower proportion Fruit proportion No. of families No. of genera No. of species</head><label></label><figDesc></figDesc><table><row><cell>New England</cell><cell>NEVP</cell><cell>42,658</cell><cell>90.9%</cell><cell>64.9%</cell><cell>34.9%</cell><cell>16</cell><cell>340</cell><cell>1375</cell></row><row><cell>Vascular Plant</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>specimens</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Florida State</cell><cell>FSU</cell><cell>54,263</cell><cell>92.7%</cell><cell>73.9%</cell><cell>55.2%</cell><cell>202</cell><cell>1189</cell><cell>3870</cell></row><row><cell>University's</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Robert K. Godfrey</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Herbarium</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>IRD Herbarium of</cell><cell>CAY</cell><cell>66,312</cell><cell>79.4%</cell><cell>46.6%</cell><cell>35.1%</cell><cell>126</cell><cell>764</cell><cell>3024</cell></row><row><cell>Cayenne</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Asteraceae</cell><cell>PHENO</cell><cell>20,994</cell><cell>100%</cell><cell>NA</cell><cell>NA</cell><cell>1</cell><cell>16</cell><cell>139</cell></row><row><cell>phenophase</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>data set</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note><p>Note: IRD = Institut de Recherche pour le Développement; NA = not available. a See Appendices 2 and 3 for the lists of institutions contributing data to the NEVP and PHENO data sets. http://www.wileyonlinelibrary.com/journal/AppsPlantSci © 2019 Lorieul et al.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE 2 .</head><label>2</label><figDesc>Phenophases assigned to specimens in the PHENO data set with percentages of reproductive structures on a specimen that are closed buds, flowers, and fruits.</figDesc><table><row><cell>Phenophase</cell><cell>Phenophase</cell><cell></cell></row><row><cell>code</cell><cell>description</cell><cell>Distribution a</cell></row><row><cell>1</cell><cell>Specimen with</cell><cell>100% closed buds</cell></row><row><cell></cell><cell>unopened flowers</cell><cell></cell></row><row><cell>2</cell><cell>Specimen mainly in</cell><cell>75% closed buds, 25% flowers,</cell></row><row><cell></cell><cell>buds</cell><cell>0% fruits</cell></row><row><cell></cell><cell></cell><cell>75% closed buds, 0% flowers,</cell></row><row><cell></cell><cell></cell><cell>25% fruits</cell></row><row><cell>3</cell><cell>Specimen essentially in</cell><cell>50% closed buds, 50% flowers,</cell></row><row><cell></cell><cell>buds and flowers</cell><cell>0% fruits</cell></row><row><cell>4</cell><cell>Specimen mainly in</cell><cell>50% closed buds, 25% flowers,</cell></row><row><cell></cell><cell>buds and flowers</cell><cell>25% fruits</cell></row><row><cell></cell><cell></cell><cell>25% closed buds, 75% flowers,</cell></row><row><cell></cell><cell></cell><cell>0% fruits</cell></row><row><cell>5</cell><cell>Specimen mainly in</cell><cell>0% closed buds, 100% flowers,</cell></row><row><cell></cell><cell>flowers</cell><cell>0% fruits</cell></row><row><cell></cell><cell></cell><cell>25% closed buds, 50% flowers,</cell></row><row><cell></cell><cell></cell><cell>25% fruits</cell></row><row><cell>6</cell><cell>Specimen mainly in</cell><cell>0% closed buds, 75% flowers,</cell></row><row><cell></cell><cell>flowers and fruits</cell><cell>25% fruits</cell></row><row><cell></cell><cell></cell><cell>25% closed buds, 25% flowers,</cell></row><row><cell></cell><cell></cell><cell>50% fruits</cell></row><row><cell></cell><cell></cell><cell>50% closed buds, 0% flowers,</cell></row><row><cell></cell><cell></cell><cell>50% fruits</cell></row><row><cell>7</cell><cell>Specimen essentially in</cell><cell>0% closed buds, 50% flowers,</cell></row><row><cell></cell><cell>flowers and fruits</cell><cell>50% fruits</cell></row><row><cell>8</cell><cell>Specimen mainly in</cell><cell>0% closed buds, 25% flowers,</cell></row><row><cell></cell><cell>fruits</cell><cell>75% fruits</cell></row><row><cell></cell><cell></cell><cell>25% closed buds, 0% flowers,</cell></row><row><cell></cell><cell></cell><cell>75% fruits</cell></row><row><cell>9</cell><cell>Specimen essentially</cell><cell>0% closed buds, 0% flowers,</cell></row><row><cell></cell><cell>in fruits</cell><cell>100% fruits</cell></row></table><note><p>a Distribution of closed buds, flowers, and fruit on the specimen.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE 3 .</head><label>3</label><figDesc>Data distribution and results of the fertility detection accuracy obtained in EXP1-Fertility. a</figDesc><table><row><cell>Evaluated</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>models b</cell><cell cols="4">Training set Test set A Test set B Test set C</cell></row><row><cell>Data set size</cell><cell>120,739</cell><cell>13,415</cell><cell>14,539</cell><cell>14,540</cell></row><row><cell>Percentage of</cell><cell>86.4%</cell><cell>86.2%</cell><cell>87.4%</cell><cell>91.1%</cell></row><row><cell>fertile specimens</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>ResNet50-Large</cell><cell>-</cell><cell>94.9%</cell><cell>93.6%</cell><cell>93.2%</cell></row><row><cell>(400 × 250 pixels)</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>ResNet50-</cell><cell>-</cell><cell>96.3%</cell><cell>95.2%</cell><cell>92.0%</cell></row><row><cell>VeryLarge (800 ×</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>550 pixels)</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note><p>a Test set A = Random-split, test set B = Species-split (747 species), test set C = Herbariumsplit (nine NEVP herbaria). b Default image size 900 × 600 pixels. http://www.wileyonlinelibrary.com/journal/AppsPlantSci © 2019 Lorieul et al.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE 4 .</head><label>4</label><figDesc>Distribution of angiosperms, ferns, and gymnosperms in the data sets used for experiment EXP1-Fertility and results of the fertility detection accuracy obtained in that experiment for test set A (Random-split).</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Fertility detection</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Data distribution</cell><cell></cell><cell></cell><cell>accuracy</cell></row><row><cell>Evaluated clades</cell><cell>Whole data set</cell><cell>Training set</cell><cell>Test set A</cell><cell>Test set B</cell><cell>Test set C</cell><cell>Test set A</cell></row><row><cell>Angiosperms</cell><cell>91.47%</cell><cell>90.99%</cell><cell>90.49%</cell><cell>87.8%</cell><cell>100%</cell><cell>96.3%</cell></row><row><cell>Ferns and allies</cell><cell>8.51%</cell><cell>8.98%</cell><cell>9.47%</cell><cell>12.2%</cell><cell>0</cell><cell>95.7%</cell></row><row><cell>Gymnosperms</cell><cell>0.02%</cell><cell>0.03%</cell><cell>0.04%</cell><cell>0</cell><cell>0</cell><cell>100%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE 5 .</head><label>5</label><figDesc>Data distribution and results of the flower and fruit detection accuracy obtained in EXP2-Fl.Fr.</figDesc><table><row><cell>Evaluated models</cell><cell>Training set</cell><cell cols="2">Test set A (Random-split) Test set B (Species-split)</cell><cell>Test set C (Herbarium-split)</cell></row><row><cell>Data set size</cell><cell>109,467</cell><cell>12,095</cell><cell>12,723</cell><cell>14,066</cell></row><row><cell>Percentage of specimens in</cell><cell>60.9%</cell><cell>60.6%</cell><cell>62.5%</cell><cell>68.5%</cell></row><row><cell>flower</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Percentage of specimens in fruit</cell><cell>43.3%</cell><cell>43.4%</cell><cell>44.9%</cell><cell>32.5%</cell></row><row><cell>ResNet50-flowers</cell><cell>-</cell><cell>84.3%</cell><cell>81.0%</cell><cell>87.0%</cell></row><row><cell>ResNet50-fruits</cell><cell>-</cell><cell>80.5%</cell><cell>76.6%</cell><cell>79.6%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>TABLE 6 .</head><label>6</label><figDesc>Data distribution and results of the phenophase detection accuracy obtained in EXP3-Pheno. Coarse classification accuracy is computed based on grouping phenophase categories by 3.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Accuracy</cell><cell>Accuracy</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>when</cell><cell>when</cell><cell></cell><cell></cell></row><row><cell></cell><cell>Training set</cell><cell>Test set</cell><cell>Fine-grained</cell><cell>Coarse</cell><cell>tolerating</cell><cell>tolerating</cell><cell></cell><cell></cell></row><row><cell>Evaluated</cell><cell>size (No. of</cell><cell>size (No. of</cell><cell>classification</cell><cell>classification</cell><cell>a [±1] error</cell><cell>a [±2] error</cell><cell>Fine-grained</cell><cell>Coarse mean</cell></row><row><cell>model</cell><cell>images)</cell><cell>images)</cell><cell>accuracy</cell><cell>accuracy</cell><cell>range</cell><cell>range</cell><cell>mean L1 error</cell><cell>L1 error</cell></row><row><cell>ResNet50-</cell><cell>16,298</cell><cell>4073</cell><cell>43.4%</cell><cell>69.0%</cell><cell>67.1%</cell><cell>82.8%</cell><cell>1.35</cell><cell>0.37</cell></row><row><cell>Pheno</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>TABLE 7 .</head><label>7</label><figDesc>Data distribution and results of the phenophase detection accuracy obtained in EXP3-Pheno, per phenophase categories.</figDesc><table><row><cell>Phenophase</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Data distribution in the training data set a Data distribution in the test data set a Classification accuracy</head><label></label><figDesc></figDesc><table><row><cell>1</cell><cell>10.4%</cell><cell>10.4%</cell><cell>74.8%</cell></row><row><cell>2</cell><cell>7.6%</cell><cell>7.5%</cell><cell>24.4%</cell></row><row><cell>3</cell><cell>9.5%</cell><cell>9.5%</cell><cell>27.9%</cell></row><row><cell>4</cell><cell>7.1%</cell><cell>7.2%</cell><cell>8.6%</cell></row><row><cell>5</cell><cell>17.3%</cell><cell>17.2%</cell><cell>60.8%</cell></row><row><cell>6</cell><cell>7.2%</cell><cell>7.2%</cell><cell>6.8%</cell></row><row><cell>7</cell><cell>10.5%</cell><cell>10.6%</cell><cell>18.8%</cell></row><row><cell>8</cell><cell>10.5%</cell><cell>10.5%</cell><cell>18.0%</cell></row><row><cell>9</cell><cell>19.9%</cell><cell>19.9%</cell><cell>78.9%</cell></row></table><note><p><p>a Human annotated. http://www.wileyonlinelibrary.com/journal/AppsPlantSci © 2019 Lorieul et al.</p>observer (87%) shows a lower accuracy than that of the ResNet50-VeryLarge model (96.3%).</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>TABLE 8 .</head><label>8</label><figDesc>Comparison of model accuracy with human annotations based on the re-annotation of 100 herbarium specimens of test set A of EXP1-Fertility. a</figDesc><table><row><cell></cell><cell>True positive</cell><cell>False positive</cell><cell>True negative</cell><cell>False negative</cell><cell>Overall accuracy on</cell><cell>Global accuracy</cell></row><row><cell>Annotation types</cell><cell>subset accuracy</cell><cell>subset accuracy</cell><cell>subset accuracy</cell><cell>subset accuracy</cell><cell>these subsets</cell><cell>on test set A</cell></row><row><cell>ResNet50-VeryLarge</cell><cell>100.0%</cell><cell>0.0%</cell><cell>100.0%</cell><cell>0.0%</cell><cell>50.0%</cell><cell>96.3%</cell></row><row><cell>Human annotation b</cell><cell>88.0%</cell><cell>68.0%</cell><cell>88.0%</cell><cell>76.0%</cell><cell>80.0%</cell><cell>87.8%</cell></row><row><cell>a</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>http://www.wileyonlinelibrary.com/journal/AppsPlantSci © 2019 Lorieul et al.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>ACKNOWLEDGMENTS</head><p>The authors thank the herbarium staff, field botanists, students, and others who have invested considerable time to collect, prepare, store, curate, and annotate the thousands of herbarium specimens that are used to conduct these studies. This work was supported in part by <rs type="funder">iDigBio (U.S. National Science Foundation [NSF</rs>] grants <rs type="grantNumber">EF-1115210</rs> and <rs type="grantNumber">DBI-1547229</rs>), the <rs type="funder">New England Vascular Plant</rs> project (<rs type="grantNumber">NSF</rs> <rs type="grantNumber">DBI-1209149</rs>), and an "<rs type="programName">Investissement d' Avenir</rs>" grant managed by <rs type="funder">Agence Nationale de la Recherche (CEBA</rs>, ref. <rs type="grantNumber">ANR-10-LABX-25-01</rs>).</p></div>
<div><head>DATA ACCESSIBILITY</head><p>Data and models used and produced for this study are accessible on Zenodo (Lorieul, 2019;Lorieul et al., 2019), a free and open platform for preserving and sharing research output.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_saJFR8t">
					<idno type="grant-number">EF-1115210</idno>
				</org>
				<org type="funding" xml:id="_cTQFQTH">
					<idno type="grant-number">DBI-1547229</idno>
				</org>
				<org type="funding" xml:id="_8GJnEd5">
					<idno type="grant-number">NSF</idno>
				</org>
				<org type="funding" xml:id="_EHW4JYW">
					<idno type="grant-number">DBI-1209149</idno>
					<orgName type="program" subtype="full">Investissement d&apos; Avenir</orgName>
				</org>
				<org type="funding" xml:id="_jm6kcYy">
					<idno type="grant-number">ANR-10-LABX-25-01</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>APPENDIX 2. List of institutions contributing data to the NEVP phenology data set. Institution abbreviations are according to Index Herbariorum <ref type="bibr" target="#b32">(Thiers, 2016)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Institution names a</head><p>Institution codes  Like most image classification architectures, the original ResNet50 model consists mainly of a succession of convolution layers followed by a spatial pooling and a final fully connected layer. The convolution layers act as visual feature extractors, the pooling layer removes the residual spatial information, and the fully connected layer performs the final classification in the new feature space. Unlike this final layer, the convolution layers are agnostic to the spatial extent of their inputs. However, in transfer learning, the final fully connected layer is learned from scratch because it is specific to each task. Thus, to adapt the ResNet50 architecture to larger image size, we modified the spatial pooling layer in order to perform this pooling operation in the total spatial extent, which has increased due to the new image size. By doing so, we kept the input size of the fully connected layer the same as the original model while being able to exploit finer details in the images. Such a modification can be applied to many other architectures and is not limited to ResNet50.</p><p>APPENDIX 5. Details of the fine-tuning procedure.</p><p>Instead of initializing the parameters with random values, we used the pre-trained weights available from PyTorch's model zoo.</p><p>We then fine-tuned this model on our data sets by optimizing the binary cross-entropy for EXP1-Fertility and EXP2-Fl.Fr and the multinomial cross-entropy for EXP3-Pheno. As optimizer, we used stochastic gradient descent (SGD) with momentum. We kept 10% of the training data to build a validation set that was then used to choose the values of the hyperparameters of SGD. We detail these values in the next paragraph.</p><p>For every experiment, we used a fixed learning rate decay: the initial learning rate value was divided by 10 at one-third and two-thirds of the training. Nesterov's momentum was used with a value of 0.9. No weight decay was performed. The values of the initial learning rate, the batch size, and the number of epochs were different depending on the experiment. Details of these values are provided in Table <ref type="table">A5</ref>-1.</p><p>The gap in batch size between ResNet50-Large and ResNet50-VeryLarge was mainly due to memory restrictions of the graphics processing units (GPUs). Indeed, although using higher-resolution images does not impact the number of parameters in the model, it did result in more activations being computed during training: four times more memory was needed for ResNet50-VeryLarge than for ResNet50-Large. The difference in number of epochs was explained by the fact that EXP3-Pheno has fewer images than EXP1-Fertility and EXP2-Fl.Fr.</p><p>Before being fed to the model, for ResNet50-Large, the images were resized to 450 × 300 pixels (approximately, because their ratio was preserved), and they were cropped according to a centered rectangle of 400 × 250 pixels. Similarly, for ResNet50-VeryLarge, they were approximately resized to 900 × 600 pixels and cropped down to 850 × 550 pixels. Moreover, data augmentation was used by performing random horizontal and vertical flips and random rotations of ±45 degrees on the input images during training. </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Biodiversity synthesis across the green branches of the tree of life</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Allen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Folk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Soltis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Soltis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">P</forename><surname>Guralnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Plants</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="11" to="13" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Climate-associated phenological advances in bee pollinators and bee-pollinated plants</title>
		<author>
			<persName><forename type="first">I</forename><surname>Bartomeus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Ascher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wagner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">N</forename><surname>Danforth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Colla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kornbluth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Winfree</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences USA</title>
		<imprint>
			<biblScope unit="volume">108</biblScope>
			<biblScope unit="issue">51</biblScope>
			<biblScope unit="page" from="20645" to="20649" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A deep learning approach to species distribution modelling</title>
		<author>
			<persName><forename type="first">C</forename><surname>Botella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Monestiez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Munoz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Multimedia tools and applications for environmental and biodiversity informatics</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Joly</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Vrochidis</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Karatzas</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Karppinen</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Bonnet</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham, Switzerland</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="169" to="199" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Integrating herbarium specimen observations into global phenology data systems</title>
		<author>
			<persName><forename type="first">L</forename><surname>Brenskelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">J</forename><surname>Stucky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Deck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Walls</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">P</forename><surname>Guralnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applications in Plant Sciences</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">1231</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Going deeper in the automated identification of herbarium specimens</title>
		<author>
			<persName><forename type="first">J</forename><surname>Carranza-Rojas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Goeau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Mata-Montero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC Evolutionary Biology</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">181</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Automated identification of herbarium specimens at different taxonomic levels</title>
		<author>
			<persName><forename type="first">J</forename><surname>Carranza-Rojas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Goëau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Mata-Montero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bonnet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Multimedia tools and applications for environmental and biodiversity informatics</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Joly</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Vrochidis</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Karatzas</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Karppinen</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Bonnet</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham, Switzerland</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="151" to="167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Hidden biases in automated image-based plant identification</title>
		<author>
			<persName><forename type="first">J</forename><surname>Carranza-Rojas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Mata-Montero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Goëau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Work Conference on Bioinspired Intelligence (IWOBI) in</title>
		<meeting><address><addrLine>San Carlos, Costa Rica; Piscataway, New Jersey, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018-07">2018. July 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Herbarium records are reliable sources of phenological change driven by climate and provide novel insights into species&apos; phenological cueing mechanisms</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">G</forename><surname>Willis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Connolly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Kelly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Ellison</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">American Journal of Botany</title>
		<imprint>
			<biblScope unit="volume">102</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1599" to="1609" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">ImageNet: A large-scale hierarchical image database</title>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<ptr target="http://www.wileyonlinelibrary.com/journal/AppsPlantSci©2019Lorieuletal" />
	</analytic>
	<monogr>
		<title level="m">2009 IEEE Conference on Computer Vision and Pattern Recognition in</title>
		<meeting><address><addrLine>Miami, Florida, USA; Piscataway, New Jersey, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009-06">2009. June 2009</date>
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Record-breaking early flowering in the eastern United States</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">R</forename><surname>Ellwood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Temple</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">B</forename><surname>Primack</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">L</forename><surname>Bradley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PloS ONE</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">53788</biblScope>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Worldwide engagement for digitizing biocollections (WeDigBio): The biocollections community&apos;s citizen-science space on the calendar</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">R</forename><surname>Ellwood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kimberly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Guralnick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Flemons</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Love</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ellis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Allen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BioScience</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="112" to="124" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Phenology models using herbarium specimens are only slightly improved by using finerscale stages of reproduction</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">R</forename><surname>Ellwood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">B</forename><surname>Primack</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">G</forename><surname>Willis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hillerislambers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applications in Plant Sciences</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">1225</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Impact of climate change on plant phenology in Mediterranean ecosystems</title>
		<author>
			<persName><forename type="first">O</forename><surname>Gordo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Sanz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Global Change Biology</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1082" to="1106" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">New developments in museum-based informatics and applications in biodiversity analysis</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ferrier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Huettman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Moritz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">T</forename><surname>Peterson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trends in Ecology and Evolution</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="497" to="503" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition in</title>
		<meeting><address><addrLine>Las Vegas, Nevada, USA; Piscataway, New Jersey, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016-06-26">2016. 26 June-1 July 2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Herbarium data: Global biodiversity and societal botanical needs for novel research</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>James</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Soltis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Belbin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>Chapman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Nelson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applications in Plant Sciences</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">1024</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A look inside the Pl@ntnet experience</title>
		<author>
			<persName><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Goëau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Barbe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Selmi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Champ</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Dufour-Kowalski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Multimedia Systems</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="751" to="766" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">LifeCLEF 2017 lab overview: Multimedia species identification challenges</title>
		<author>
			<persName><forename type="first">A</forename><surname>Joly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Goëau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Glotin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Spampinato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W.-P</forename><surname>Vellinga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-C</forename><surname>Lombardo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Experimental IR meets multilinguality, multimodality, and interaction</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">G</forename><forename type="middle">J F</forename><surname>Jones</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Lawless</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Gonzalo</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Kelly</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Goeuriot</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><surname>Mandl</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Cappellato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Ferro</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham, Switzerland</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">10456</biblScope>
			<biblScope unit="page" from="255" to="274" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Time-lapse imagery and volunteer classifications from the Zooniverse Penguin Watch project</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">M</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Allen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Arteta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Arthur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">M</forename><surname>Emmerson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific Data</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">180124</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">tlorieul/herbarium-phenology-dnn v1.0-alpha (Version v1.0-alpha). Data set available at Zenodo repository</title>
		<author>
			<persName><forename type="first">T</forename><surname>Lorieul</surname></persName>
		</author>
		<idno type="DOI">10.5281/zenodo.2549996</idno>
		<ptr target="https://doi.org/10.5281/zenodo.2549996" />
		<imprint>
			<date type="published" when="2019-01">2019. January 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Large-scale and fine-grained phenological stage annotation of herbarium specimens datasets (Version 1.0.0)</title>
		<author>
			<persName><forename type="first">T</forename><surname>Lorieul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">D</forename><surname>Pearson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">R</forename><surname>Ellwood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Goëau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-F</forename><surname>Molino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">W</forename><surname>Sweeney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yost</surname></persName>
		</author>
		<idno type="DOI">10.5281/zenodo.2548630</idno>
		<ptr target="https://doi.org/10.5281/zenodo.2548630" />
		<imprint>
			<date type="published" when="2019-01">2019. January 2019</date>
		</imprint>
	</monogr>
	<note>Data set available at Zenodo repository</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">The unrealized potential of herbaria for global change biology</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">K</forename><surname>Meineke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">J</forename><surname>Davies</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ecological Monographs</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="page" from="505" to="525" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">European phenological response to climate change matches the warming pattern</title>
		<author>
			<persName><forename type="first">A</forename><surname>Menzel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">H</forename><surname>Sparks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Estrella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Aasa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ahas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Alm-Kübler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Global Change Biology</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1969" to="1976" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Real-time semantic segmentation of crop and weed for precision agriculture robots leveraging background knowledge in CNNs</title>
		<author>
			<persName><forename type="first">A</forename><surname>Milioto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lottes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Stachniss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Conference on Robotics and Automation in</title>
		<meeting><address><addrLine>Brisbane, Queensland, Australia; Piscataway, New Jersey, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018-05">2018. May 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">How global biodiversity hotspots may go unrecognized: Lessons from the North American Coastal Plain</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">F</forename><surname>Noss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">J</forename><surname>Platt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">A</forename><surname>Sorrie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Weakley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Means</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Costanza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">K</forename><surname>Peet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Diversity and Distributions</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="236" to="244" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Spring-and fall-flowering species show diverging phenological responses to climate in the southeast United States</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">D</forename><surname>Pearson</surname></persName>
		</author>
		<idno type="DOI">10.1007/s00484-019-01679-0</idno>
		<ptr target="https://doi.org/10.1007/s00484-019-01679-0" />
	</analytic>
	<monogr>
		<title level="j">International Journal of Biometeorology</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A new method and insights for estimating phenological events from herbarium specimens</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">D</forename><surname>Pearson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applications in Plant Sciences</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">1224</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">From observations to experiments in phenology research: Investigating climate change impacts on trees and shrubs using dormant twigs</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">B</forename><surname>Primack</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Laube</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Gallinat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Menzel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of Botany</title>
		<imprint>
			<biblScope unit="volume">116</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="889" to="897" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Deep convolutional neural networks for computer-aided detection: CNN architectures, dataset characteristics and transfer learning</title>
		<author>
			<persName><forename type="first">H.-C</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">R</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Nogues</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Medical Imaging</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1285" to="1298" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Digitization of herbaria enables novel research</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Soltis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">American Journal of Botany</title>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1281" to="1284" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Mobilizing and integrating big data in studies of spatial and phylogenetic patterns of biodiversity</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Soltis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Soltis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Plant Diversity</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="264" to="270" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Green digitization: Online botanical collections data answering real-world questions</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Soltis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Nelson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>James</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applications in Plant Sciences</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">1028</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<author>
			<persName><forename type="first">B</forename><surname>Thiers</surname></persName>
		</author>
		<ptr target="http://sweetgum.nybg.org/science/ih/" />
	</analytic>
	<monogr>
		<title level="m">Index Herbariorum: A global directory of public herbaria and associated staff</title>
		<imprint>
			<date type="published" when="2016-02">2016. February 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Computer vision applied to herbarium specimens of German trees: Testing the future utility of the millions of herbarium specimen images for automated identification</title>
		<author>
			<persName><forename type="first">J</forename><surname>Unger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Merhof</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Renner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC Evolutionary Biology</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">248</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Machine learning for image based species identification</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wäldchen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mäder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Methods in Ecology and Evolution</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="2216" to="2225" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Automated plant species identification-Trends and future directions</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wäldchen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rzanny</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Seeland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mäder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS Computational Biology</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">1005993</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Old plants, new tricks: Phenological research using herbarium specimens</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">G</forename><surname>Willis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">R</forename><surname>Ellwood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">B</forename><surname>Primack</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">D</forename><surname>Pearson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Gallinat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Yost</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trends in Ecology and Evolution</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="531" to="546" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Digitization protocol for scoring reproductive phenology from herbarium specimens of seed plants</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Yost</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">W</forename><surname>Sweeney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Gilbert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Nelson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Guralnick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Gallinat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">R</forename><surname>Ellwood</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applications in Plant Sciences</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">1022</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Taxon and trait recognition from digitized herbarium specimens using deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Younis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Weiland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Hoehndorf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Dressler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hickler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Seeger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Schmidt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Botany Letters</title>
		<imprint>
			<biblScope unit="volume">165</biblScope>
			<biblScope unit="page" from="377" to="383" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Continental-scale patterns of Cecropia reproductive phenology: Evidence from herbarium specimens</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">C</forename><surname>Zalamea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Munoz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">R</forename><surname>Stevenson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">T</forename><surname>Paine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Sarmiento</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Sabatier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Heuret</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the Royal Society of London B: Biological Sciences</title>
		<imprint>
			<biblScope unit="volume">278</biblScope>
			<biblScope unit="page">20102259</biblScope>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
