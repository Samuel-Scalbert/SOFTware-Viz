<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="fr">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Pl@ntNet-300K: a plant image dataset with high label ambiguity and a long-tailed distribution</title>
				<funder ref="#_jgcnu7t">
					<orgName type="full">European Union</orgName>
				</funder>
				<funder ref="#_MaJHUVR">
					<orgName type="full">Agence Nationale de la Recherche</orgName>
					<orgName type="abbreviated">ANR</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Camille</forename><surname>Garcin</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Alexis</forename><surname>Joly</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Pierre</forename><surname>Bonnet</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Jean-Christophe</forename><surname>Lombardo</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Antoine</forename><surname>Affouard</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Mathias</forename><surname>Chouet</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Maximilien</forename><surname>Servajean</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Titouan</forename><surname>Lorieul</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Joseph</forename><surname>Salmon</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="laboratory">IMAG</orgName>
								<orgName type="institution">Univ Montpellier</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">CNRS</orgName>
								<address>
									<settlement>Montpellier</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="laboratory" key="lab1">Inria</orgName>
								<orgName type="laboratory" key="lab2">LIRMM</orgName>
								<orgName type="institution">Univ Montpellier</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">CNRS</orgName>
								<address>
									<settlement>Montpellier</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="laboratory">CIRAD</orgName>
								<orgName type="institution">AMAP</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="laboratory" key="lab1">Inria</orgName>
								<orgName type="laboratory" key="lab2">LIRMM</orgName>
								<orgName type="institution">Univ Montpellier</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff6">
								<orgName type="institution">CNRS</orgName>
								<address>
									<settlement>Montpellier</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff7">
								<orgName type="laboratory" key="lab1">Inria</orgName>
								<orgName type="laboratory" key="lab2">LIRMM</orgName>
								<orgName type="institution">Univ Montpellier</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff8">
								<orgName type="institution">CNRS</orgName>
								<address>
									<settlement>Montpellier</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff9">
								<orgName type="laboratory" key="lab1">Inria</orgName>
								<orgName type="laboratory" key="lab2">LIRMM</orgName>
								<orgName type="institution">Univ Montpellier</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff10">
								<orgName type="institution">CNRS</orgName>
								<address>
									<settlement>Montpellier</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff11">
								<orgName type="laboratory" key="lab1">LIRMM</orgName>
								<orgName type="laboratory" key="lab2">AMIS</orgName>
								<orgName type="laboratory" key="lab3">UPVM</orgName>
								<orgName type="institution" key="instit1">Univ Montpellier</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
								<address>
									<settlement>Montpellier</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff12">
								<orgName type="laboratory" key="lab1">Titouan Lorieul Inria</orgName>
								<orgName type="laboratory" key="lab2">LIRMM</orgName>
								<orgName type="institution">Univ Montpellier</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff13">
								<orgName type="institution">CNRS</orgName>
								<address>
									<settlement>Montpellier</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff14">
								<orgName type="laboratory">IMAG</orgName>
								<orgName type="institution">Univ Montpellier</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff15">
								<orgName type="institution">CNRS</orgName>
								<address>
									<settlement>Montpellier</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff16">
								<orgName type="institution">Institut Universitaire de France (IUF)</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Pl@ntNet-300K: a plant image dataset with high label ambiguity and a long-tailed distribution</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">90D00461A72AC083FA0DF66E1A2CAC95</idno>
					<idno type="DOI">10.5281/zenodo.5645731</idno>
					<note type="submission">Submitted on 9 Feb 2022</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-04-12T14:50+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>ou non, émanant des établissements d'enseignement et de recherche français ou étrangers, des laboratoires publics ou privés.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="fr">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>When classifying images, we are faced with two main types of uncertainties <ref type="bibr" target="#b5">[Der Kiureghian and Ditlevsen, 2009]</ref>: (i) the aleatoric uncertainty that arises from the intrinsic randomness of the underlying process, which is considered irreducible, and (ii) the epistemic uncertainty that is caused by a lack of knowledge and is considered to be reducible with additional training data. In modern real-world applications, these two types of uncertainties are particularly difficult to handle. The evergrowing number of classes to distinguish tends to increase the class overlap (and thus the aleatoric uncertainty), and, on the other hand, the long-tailed distribution makes it difficult to learn the less populated classes (and thus increase the epistemic uncertainty). The presence of these two uncertainties is a central motivation for the use of set-valued classifiers, i.e., classifiers returning a set of candidate classes for each image <ref type="bibr" target="#b2">[Chzhen et al., 2021]</ref>. Although there are several datasets in the literature that have visually similar classes <ref type="bibr" target="#b24">[Nilsback and Zisserman, 2008</ref><ref type="bibr" target="#b23">, Maji et al., 2013</ref><ref type="bibr" target="#b36">, Yang et al., 2015</ref><ref type="bibr" target="#b27">, Russakovsky et al., 2015]</ref>, most of them do not aim to retain both the epistemic and the aleatoric ambiguity present in real-world data.</p><p>In this paper, we propose a dataset designed to remain representative of real-life ambiguity, making it well suited for the evaluation of set-valued classification methods. This dataset is extracted from real-world images collected as part of the Pl@ntNet project <ref type="bibr" target="#b0">[Affouard et al., 2017]</ref>, a large-scale citizen observatory dedicated to the collection of plant occurrences data through image-based plant identification. The key feature of Pl@ntNet is a mobile application that allows citizens to send a picture of a plant they encounter and get a list of the most likely species for that photo in return. The application is used by more than 10 million users in about 170 countries and is one of the main data publishers of GBIF 1 , an international platform funded by the governments of many countries around the world to provide free and open access to biodiversity data. Another essential feature of Pl@ntNet is that the training set used to train the classifier is collaboratively enriched and revised. Nowadays, Pl@ntNet covers over 35K species illustrated by nearly 12 million validated images.</p><p>The entire Pl@ntNet database would be an ideal candidate for the evaluation of set-valued classification methods. However, it is far too large to allow for widespread use by the machine learning community. Extracting a subsample from it must be done with care as we want to preserve the uncertainty naturally present in the whole database. The dataset presented in this paper is constructed by retaining only a subset of the genera of the entire Pl@ntNet database (sampled uniformly at random). All species belonging to the selected genera are then retained. Doing so maintains the original ambiguity as species in the same genus are likely to be visually similar and to share common visual features.</p><p>The rest of the paper is organized as follows. We first introduce the set-valued classification framework in Section 2, focusing on two special cases: top-k classification and average-k classification. In Section 3, we describe the construction procedure of the Pl@ntNet-300K dataset and show that it contains a large amount of ambiguity. Next, we present in Section 4 the metrics of interest for the dataset and propose benchmark results for these metrics, obtained by training several neural networks architectures. In Section 5, we compare Pl@ntNet-300K to several existing datasets. In Section 6, we discuss possible uses of Pl@ntNet-300K. Finally, we provide the link to the dataset in Section 7 before concluding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Set-valued classification</head><p>We adopt the classical statistical setup of multi-class classification. Let L be the number of classes. We denote by [L] the set {1, . . . , L} and by X the input space. Random couples of images and labels (X, Y ) ∈ X × [L] are assumed to be generated i.i.d. by an unknown joint distribution P. Note that only one label is associated with each image, which differs from the multi-label setting <ref type="bibr" target="#b38">[Zhang and Zhou, 2014]</ref>: P@ntNet-300K is composed of images containing a single specimen of a plant, so there is only one true label per image. For some plant images, predicting the correct class label (the correct species) does not present much difficulty (consider for instance a common species very distinctive from other species). For other images, however, classifying the photographed specimen with a high degree confidence is a much harder task, because some species differ only in subtle visual features (see Figure <ref type="figure" target="#fig_3">4</ref>). In these cases, it is desirable to provide the user with a list of likely species corresponding to the image. We thus need a classifier able to produce sets of classes, also known as a set-valued classifier in the literature <ref type="bibr" target="#b2">[Chzhen et al., 2021]</ref>. A set-valued classifier Γ is a function mapping the feature space X to the set of all subsets of [L] (which we denote by 2 [L] ). Using these notations, we thus have Γ : X → 2 [L] instead of Γ : X → [L] for the classical setting in which the predictor can only predict a single class. Our goal is to build a classifier with low risk, defined as P(Y / ∈ Γ(X)). However, it is not desirable to simply minimize the risk: a set-valued classifier that always returns all classes achieves zero risk but is useless. On the other hand, a classifier is most useful if it returns only the most likely classes given a query image. Therefore, a quantity of interest will be |Γ(x)|, the number of classes returned by the classifier Γ, given an image x ∈ X .</p><p>1 https://www.gbif.org/</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2</head><p>In this section we will examine two optimization problems that lead to different set-valued classifiers. Both of them aim to minimize the risk, but they differ in the way they constrain the set cardinality: either pointwise or on average. For x ∈ X and l ∈ [L], we define the conditional probability p l (x) := P(Y = l | X = x), and estimators of these quantities will be denoted by pl (x). In the following, k ∈ [L]. Finally, for x ∈ X , we define top p (x, k) as the set containing the k indexes corresponding to the k largest values of {p l (x)} l∈ <ref type="bibr">[L]</ref> .</p><p>The simplest constraint is to require that the number of returned classes is less than k for each input. This results in the following top-k error <ref type="bibr" target="#b18">[Lapin et al., 2015]</ref> minimization problem:</p><formula xml:id="formula_0">Γ * top-k ∈ arg min Γ P(Y / ∈ Γ(X)) s.t. |Γ(x)| ≤ k, ∀x ∈ X .</formula><p>(1)</p><p>The closed form solution to (1) exists and is equal to <ref type="bibr" target="#b20">[Lapin et al., 2017]</ref>:</p><formula xml:id="formula_1">Γ * top-k (x) = top p (x, k) .<label>(2)</label></formula><p>Yet, this is not practical since we do not know the distribution P and thus p l (x). However, if we have an estimator pl (x) of p l (x), we can naturally derive the plug-in estimator:</p><formula xml:id="formula_2">Γ top-k = top p(x, k).</formula><p>While the top-k accuracy is often reported in benchmarks, only a few works aim to directly optimize this metric <ref type="bibr" target="#b18">[Lapin et al., 2015</ref><ref type="bibr" target="#b26">, 2016</ref><ref type="bibr">, 2017</ref><ref type="bibr" target="#b1">, Berrada et al., 2018</ref>]. An obvious limitation of topk classification is that k classes are returned for every data sample, regardless of the difficulty of classifying that sample. Average-k classification allows for more adaptivity. In that setting, the constraint on the size of the predicted set is less restrictive and must be satisfied only on average, leading to:</p><formula xml:id="formula_3">Γ * average-k ∈ arg min Γ P(Y / ∈ Γ(X)) s.t. E X |Γ(X)| ≤ k .<label>(3)</label></formula><p>The closed form solution is derived in <ref type="bibr" target="#b4">[Denis and Hebiri, 2017]</ref>:</p><formula xml:id="formula_4">Γ * average-k (x) = {l ∈ [L] : p l (x) ≥ G -1 (k)} ,<label>(4)</label></formula><p>where the function G is defined as: ∀t ∈ [0, 1], G(t) = L l=1 P(p l (X) ≥ t), and G -1 refers to the generalized inverse of G, namely G -1 (u) = inf{t ∈ [0, 1] : G(t) ≤ u}.</p><p>Note that if we define the classifier Γ t by: ∀x ∈ X , Γ t (x) = {l ∈ [L], p l (x) ≥ t}, then G(t) is the average number of classes returned by Γ t : G(t) = E X |Γ t (X)|. From (4) we see that the optimal classifier corresponds to a thresholding operation: all classes having a conditional probability greater than G -1 (k) are returned, with the threshold chosen so that k classes are returned on average. To compute a plug-in counterpart, we just need to estimate the threshold on a calibration set such that on average on that set, k classes are returned. For technical details, we refer the reader to <ref type="bibr" target="#b4">Denis and Hebiri [2017]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Dataset</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Label validation and data cleaning</head><p>Label validation is based on a weighted majority voting algorithm taking as input the labels proposed by Pl@ntNet users with an adaptive weighting principle according to the user's expertise and commitment. Thus, a single trusted annotator can be enough to validate an image label. On the other hand, images whose labels are proposed by several novice users may not be validated because they do not have sufficient weight. The technical details of this algorithm can be found in the supplementary material. At the time of the construction of Pl@ntNet-300K, the total number of annotators in the Pl@ntNet database was equal to 2,079,003. The average number of annotators per image is equal to 2.03.</p><p>In addition to the label validation procedure, Pl@ntNet's pipeline includes other data cleaning procedures: (i) automated filtering of inappropriate or irrelevant content (faces, humans, animals, buildings, etc.) using a CNN and user reports, and (ii) filtering on image quality (evaluated by users). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Construction of Pl@ntNet-300K</head><p>In taxonomy, species are organized into genera, with each genus containing one or more species, and the different genera do not overlap, as illustrated in Figure <ref type="figure" target="#fig_0">1</ref>.</p><p>Instead of retaining randomly selected species or images from the entire Pl@ntNet dataset, we choose to retain randomly selected genera and keep all species belonging to these genera. This choice aims to preserve the large amount of ambiguity present in the original database, as species belonging to the same genus tend to share visual features. The dataset presented in this paper is constructed by sampling uniformly at random only 10% of the genera of the whole Pl@ntNet database.</p><p>We then retain only species with more than 4 images, resulting in a total of 303 genera and L = 1,081 species. The images are divided into a training set, a validation set and a test set.<ref type="foot" target="#foot_1">2</ref> For each species, 80% of the images are placed in the training set (n train = 243,916), 10% in the validation set (n val = 31,118), and 10% in the test set (n test = 31,112), with at least one image of each species in each set. More formally, given a class j containing n j images, n val,j = 0.1 × n j , n test,j = 0.1 × n j and n train,j = n j -n val,j -n test,j . This represents a total of n tot = n train + n val + n test = 306,146 color images. The average image size is (570, 570, 3), ranging from (180, 180, 3) to (900, 900, 3). The construction of the dataset preserves the class imbalance.</p><p>To show this, we plot the Lorenz curves <ref type="bibr" target="#b8">[Gastwirth, 1971]</ref> of the entire Pl@ntNet dataset and that of the Pl@ntNet-300K dataset in Figure <ref type="figure">2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Epistemic (model) uncertainty</head><p>Epistemic uncertainty refers mainly to the lack of data necessary to properly estimate the conditional probabilities. In Pl@ntNet, the most common species are easily observed by users in the wild and therefore represent a large fraction of the images, while the rarest species are harder to find and therefore less frequent in the database. In Figure <ref type="figure">2</ref>, we see that 80% of the species (the ones with the lowest number of images) account for only 11% of the total number of images. Hence, training machine learning models is challenging for such a dataset, since for many classes the model only has a handful of images to train on, making identification difficult for these species.</p><p>In addition to the long-tailed distribution issue, epistemic uncertainty also arises from the high intraspecies variability. Plants may take on different appearances depending on the season (e.g., , flowering time). Furthermore, a user of the application may photograph only a part of the plant (for instance, the trunk and not the leaves). As a last example, flowers belonging to the same species can have different colors. Figure <ref type="figure">3</ref> shows some examples of these phenomena which contribute to high intra-class variability, making it more challenging to model the species. Pl@ntNet-300K dataset Full Pl@ntNet dataset </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Aleatoric (data) uncertainty</head><p>In our case, the source of aleatoric uncertainty mostly resides in the limited information we are given to make a decision (assign a label to a plant). Some species, especially those belonging to the same genus, can be visually very similar. For example, consider the case where two species produce the same flowers but different leaves, typically because they have evolved differently from the same parent species. If a person photographs only the flower of a specimen of one of the two species, then it will be impossible, even for an expert, to know which species the flower belongs to. The discriminative information is not present in the image.</p><p>The combination of this irreducible ambiguity with images of non-optimal quality (non-adapted close-up, low-light conditions, etc.) results in pairs of images that belong to different species but are difficult or even impossible to distinguish, see Figure <ref type="figure" target="#fig_3">4</ref> for illustration. In this figure, we show the ambiguity between pairs of species, but we could find similar examples involving a larger number of species. Thus, even an expert botanist might fail to assign a label to such pictures with certainty. This is embodied by p l (x) : given an image, multiple classes are possible. 4 Evaluation</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Metric</head><p>We consider two main metrics to evaluate set valued predictors on Pl@ntNet-300K: topk accuracy and average-k accuracy. Let S denote a set of n (input, label) pairs: S = {(x 1 , y 1 ), (x 2 , y 2 ), . . . , (x n , y n )}. Top-k accuracy <ref type="bibr" target="#b19">[Lapin et al., 2016</ref>] is a widely used metric often reported in benchmarks. Average-k accuracy is a much less common metric that derives from average-k classification <ref type="bibr" target="#b4">[Denis and Hebiri, 2017]</ref>:</p><formula xml:id="formula_5">average-k accuracy(S) = 1 n (xi,yi)∈S 1 [yi∈ Γaverage-k(xi)] s.t. 1 n (xi,yi)∈S | Γ average-k (x i )| ≤ k ,<label>(5)</label></formula><p>where Γ average-k is a set-valued classifier constructed using the training data.</p><p>For Pl@ntNet-300K, both top-k accuracy and average-k accuracy mainly reflect the performance of the set-valued classifier on the few classes which represent most of the images. If we wish to capture the ability of a set-valued classifier to return pertinent set of species for all classes, we will examine macro-average top-k accuracy and macro-average average-k accuracy which simply consist in computing respectively top-k accuracy and average-k accuracy for each class, and then computing the average over classes. For macro-average average-k accuracy, the constraint on the average size of the set must hold for the entire set S.</p><p>To derive both classifiers, one can first obtain an estimate of the conditional probabilities pl (x) and then derive the plug-in classifiers, as explained in Section 2. Our hope is for the Pl@ntNet-300K dataset to encourage novel ways to derive the set-valued classifiers Γ top-k and Γ average-k to optimize respectively the top-k accuracy and the average-k accuracy. Notice that a few works already propose methods to optimize the top-k accuracy <ref type="bibr" target="#b18">[Lapin et al., 2015</ref><ref type="bibr" target="#b26">, 2016</ref><ref type="bibr">, 2017</ref><ref type="bibr" target="#b1">, Berrada et al., 2018]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Baseline</head><p>This section provides a baseline evaluation of the plug-in classifiers. We train several deep neural networks with the cross-entropy loss: ResNets <ref type="bibr" target="#b11">[He et al., 2016]</ref>, DenseNets, <ref type="bibr" target="#b14">[Huang et al., 2017]</ref>, InceptionResNet-v2 <ref type="bibr" target="#b33">[Szegedy et al., 2017]</ref>, MobileNetV2 <ref type="bibr" target="#b28">[Sandler et al., 2018]</ref>, MobileNetV3 <ref type="bibr" target="#b13">[Howard et al., 2019]</ref>, EfficientNets <ref type="bibr" target="#b34">[Tan and Le, 2019]</ref>, Wide ResNets <ref type="bibr" target="#b37">[Zagoruyko and Komodakis, 2016]</ref>, AlexNet <ref type="bibr" target="#b17">[Krizhevsky et al., 2012]</ref>, Inception-v3 <ref type="bibr" target="#b32">[Szegedy et al., 2016]</ref>, Inception-v4 <ref type="bibr" target="#b33">[Szegedy et al., 2017]</ref>, ShuffleNet <ref type="bibr">[Zhang et al., 2018]</ref>, SqueezeNet <ref type="bibr" target="#b15">[Iandola et al., 2016]</ref>, VGG <ref type="bibr" target="#b30">[Simonyan and Zisserman, 2015]</ref> and Vision Transformer <ref type="bibr" target="#b6">[Dosovitskiy et al., 2021]</ref>. The models are optimized with SGD with a momentum of 0.9 with the Nesterov acceleration <ref type="bibr" target="#b26">[Ruder, 2016]</ref>. We use a batch size of 32 for all models and a weight decay of 1.10 -4 . The number of epochs, initial learning rate and learning rate schedule used for each model can be found in the supplementary material. For the plug-in classifier Γ average-k, plug-in , we compute the threshold λ val on the validation set and use that same threshold to compute the average-k accuracy on the test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Difficulty of Pl@ntNet-300K</head><p>Figure <ref type="figure">5</ref> highlights the significant gap between Pl@ntNet-300K top-1 accuracy and macro-average top-1 accuracy. This is a consequence of the long-tailed distribution: the few classes that represent most of the images are easily identified, which results in high top-1 accuracy. However, this seemingly high top-1 accuracy is misleading, as models struggle with classes with few images (which are a majority, see Figure <ref type="figure">2</ref>). This effect is illustrated in Table <ref type="table">1</ref>, which shows that the top-1 accuracy depends strongly on the number of images in the class.</p><p>Figure <ref type="figure" target="#fig_5">8a</ref> shows the correlation between Pl@ntNet-300K macro-average top-1 accuracy and Im-ageNet macro-average top-1 accuracy (note that as the ImageNet test set is balanced, top-1 accuracy and macro-average top-1 accuracy coincide). As expected, the two metrics are positively correlated: deep networks allowing to model complex features work well both on ImageNet and Pl@ntNet-300K. Interestingly, due the difference between the two datasets (long-tailed distribution, class ambiguity, . . . ), some models which perform similarly on ImageNet yield very different on Pl@ntNet-300K (inception v3, densenet201), and vice versa.</p><p>In Figure <ref type="figure" target="#fig_5">8a</ref> we can notice that ImageNet macro-average top-1 accuracy and Pl@ntNet-300K macro-average top-1 accuracy vary at different scales: the former goes up to 80% while the latter does not exceed 40%, making Pl@ntNet-300K a challenging dataset with both epistemic and aleatoric uncertainty at play.</p><p>This can can also be seen in Figure <ref type="figure">6</ref>: some models reach a macro-average average-5 accuracy of 97% for ImageNet, while that metric does not exceed 80% for Pl@ntNet-300K, which suggests that progress could be made with appropriate learning strategies.</p><p>To support that claim, we asked a botanist to label a mini dataset extracted from the Pl@ntNet-300K test set. The dataset is constructed as follows: we extract all species from two groups (Crotalaria and Lupinus), and select at most 5 images per species (randomly sampled). This results in 83 images. The botanist was asked to provide a set of possible species for each image. This results in an error rate of 20.5% for an average of 4,1 species returned (ranging from 1 to 10). We compare the botanist performance with that of several neural networks by calibrating the conditional probabilities' threshold to obtain on average 4,1 species on the mini-dataset. The results are reported in Figure <ref type="figure" target="#fig_4">7</ref>, and show that the gap between the botanist error rate and the best performing model is large (from 0.2 to 0.33), which suggests that there is room for improvement in the performance of average-k classifiers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Top-k vs Average-k</head><p>From Equation ( <ref type="formula">1</ref>) and (3), it is clear that the Bayes average-k classifier has a lower risk than the Bayes top-k classifier. Therefore, a model that accurately estimates the conditional probabilities should yield a better average-k accuracy than top-k accuracy. This is what can be observed in Figure <ref type="figure" target="#fig_5">8b</ref> which shows the correlation between macro-average top-5 accuracy and macro-average average-5 accuracy. As expected, the two metrics are positively correlated. However, the relationship does not appear to be trivial and Figure <ref type="figure" target="#fig_5">8b</ref> shows models with similar macro-average average-5 accuracies having very different macro-average top-5 accuracy and vice versa. For an in-depth comparison of average-k classification and top-k classification, we refer the reader to <ref type="bibr" target="#b22">Lorieul [2020]</ref>.</p><p>Both metrics are of their own interest and deserve a specific treatment as they capture two different settings: top-k accuracy evaluates the performance of a classifier which systematically returns k classes, while average-k accuracy evaluates the performance of a classifier which returns sets of varying size (depending on the input), with the constraint to return k classes on average.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Evaluation of existing set-valued classification methods</head><p>To the best of our knowledge, there is no existing loss designed to specifically optimize averagek accuracy. For top-k classification, the most recent loss designed to optimize top-k accuracy is the one by <ref type="bibr" target="#b1">Berrada et al. [2018]</ref>. We report the top-5 accuracy obtained by training this loss with k = 5 on Pl@ntNet-300K in the supplementary material. The results are close with what is obtained with the cross entropy loss. However, this topic is still open research and our hope in releasing Pl@ntNet-300K is precisely to encourage novel methods for optimizing such metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related work</head><p>Fined-Grained Visual Categorization (FGVC) is about discriminating visually similar classes. In order to better learn fine-grained classes, several approaches have been proposed by the FGVC community, including multi-stage metric learning <ref type="bibr" target="#b25">[Qian et al., 2015]</ref>, high order feature interaction <ref type="bibr" target="#b21">[Lin et al., 2015</ref><ref type="bibr" target="#b3">, Cui et al., 2017]</ref>, and different network architectures <ref type="bibr" target="#b7">[Fu et al., 2017</ref><ref type="bibr" target="#b9">, Ge et al., 2016]</ref>. However, these approaches focus on optimizing top-1 accuracy. Set-valued classification, on the other hand, consists in returning more than a single class to reduce the error rate, with a constraint on the number of classes returned. Therefore, FGVC and set-valued classification methods are not mutually exclusive but rather complementary.  Several FGVC datasets, which exhibit visually similar classes, have been made publicly available by the community. They cover a variety of domains: aircraft <ref type="bibr" target="#b23">[Maji et al., 2013]</ref>, cars (Compcars <ref type="bibr" target="#b36">[Yang et al., 2015]</ref>, Census cars <ref type="bibr" target="#b10">[Gebru et al., 2017]</ref>), birds (CUB200 <ref type="bibr" target="#b35">[Welinder et al., 2010]</ref>), flowers (Oxford flower dataset <ref type="bibr" target="#b24">[Nilsback and Zisserman, 2008]</ref>). However, most of these datasets focus exclusively on proposing visually similar classes (aleatoric uncertainty) with a limited amount of epistemic uncertainty. This is the case for balanced datasets which have approximately the same number of images per class, or with small intra-class variability such as aircraft and cars datasets, where most examples within a class are nearly the same except for angle, lightning, etc. ImageNet <ref type="bibr" target="#b27">[Russakovsky et al., 2015]</ref> has several visually similar classes, organized in groups : it contains many bird species and dog breeds. However, these groups of classes are very different: dogs, vehicles, electronic devices, etc. Besides, ImageNet does not exhibit a strong class imbalance. Several of these datasets were constructed by web-scraping, which can be prone to noisy labels and low quality images. Most similar to our dataset is the iNat2017 dataset <ref type="bibr" target="#b12">[Horn et al., 2018]</ref>. It contains images from the citizen science website iNaturalist. The images, posted by naturalists, are validated by multiple citizen scientists. The iNat2017 dataset contains over 5000 classes that are highly unbalanced. However, iNat2017 does not only focus on plants but proposes several other 'super-classes' such as Fungi, Reptilia, Insecta, etc. Moreover, the authors selected all classes with a number of observations greater than 20, whereas we choose to randomly sample 10% of the genera of the entire Pl@ntNet database and keep all species belonging to these groups with a number of observations greater than 4. We argue that keeping all species of the same genus maximizes aleatoric uncertainty, because species belonging to the same genus tend to share visual features. Finally, a plant disease dataset is introduced in <ref type="bibr" target="#b31">[Sladojevic et al., 2016]</ref>, containing 4483 images downloaded from the web spread across 15 classes. This is a very different scale than Pl@ntNet-300K. We summarize the properties of the mentioned datasets in Table <ref type="table">2</ref>.</p><p>6 Possible uses of Pl@ntNet-300K</p><p>Although we are convinced of the need to design new set-valued methods due to the ever increasing amount of classes to discriminate, the properties of Pl@ntNet-300K described in Section 3 make it an ideal candidate for various other tasks. The strong class imbalance can be used by researchers to evaluate new algorithms specifically designed for tackling class imbalance <ref type="bibr">[Zhou et al., 2020, ?]</ref>. Pl@ntNet-300K contains a large amount of aleatoric uncertainty resulting from many visually similar classes. It can therefore be used as a FGVC dataset to evaluate methods that aim to optimize Table <ref type="table">2</ref>: Comparison of several datasets with Pl@ntNet-300K. "Focused domain" indicates whether the dataset is made up of a single category (i.e., cars) and "Ambiguity preserving sampling" indicates whether in the construction of the dataset, all classes belonging to the same parent in the class hierarchy were kept or not (in our case, the parent corresponds to the genus level). top-1 accuracy for such datasets, see for instance <ref type="bibr" target="#b21">[Lin et al., 2015</ref><ref type="bibr" target="#b7">, Fu et al., 2017</ref><ref type="bibr" target="#b3">, Cui et al., 2017]</ref>. Finally, in this paper we do not use the genus information and thus do not exploit the hierarchical structure of the problem. In this sense, we adopt the flat classification approach described in <ref type="bibr" target="#b29">[Silla and Freitas, 2011]</ref>. This is consistent with ImageNet <ref type="bibr" target="#b27">[Russakovsky et al., 2015]</ref> or CIFAR-100 <ref type="bibr" target="#b16">[Krizhevsky, 2009]</ref>, where a hierarchy does exist but is rarely used in benchmarks. However, researchers are free to use the genus information to evaluate hierarchical classification methods on Pl@ntNet-300K.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Data access and additional resources</head><p>The Pl@ntNet-300K dataset can be found here:</p><p>https://doi.org/10.5281/zenodo.5645731.</p><p>It is organized in three folders named "train", "val" and "test". Each of these folders contains L = 1,081 subfolders. We provide the correspondence between the names of the subfolders and the names of the classes in the file "plantnet300K species id 2 name.json". We also provide a metadata file named "plantnet300K metadata.json" containing for each image the following information: the species identifier (class), the organ of the plant (flower, leaf, bark, . . . ), the author's name, the license and the split (i.e., train, validation or test set). A github repository containing the code to reproduce the experiments of this paper (where potential issues related to the dataset can be reported too) is available at: https://github.com/plantnet/PlantNet-300K/.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion</head><p>In this paper, we share and discuss a novel plant image dataset, called Pl@ntNet-300K, obtained as a subset of the entire Pl@ntNet database and intended primarily for evaluating set-valued classification methods. Unlike previous datasets, Pl@ntNet-300K is designed to preserve the high level of ambiguity across classes of the initial real-world dataset as well as its long-tailed distribution.</p><p>To evaluate set-valued predictors on Pl@ntNet-300K, we investigate two different metrics: macroaverage top-k accuracy and macro-average average-k accuracy, which is a more challenging task requiring to predict sets of various size but still equal to k on average. Our results suggest that there is room for new set-valued prediction methods that would improve the performance of average-k classifiers. We hope that Pl@ntNet-300K can serve as a reference dataset for this problem, which is our main motivation for releasing and sharing it with the community. We also stress that Pl@ntNet-300K can also be used to evaluate new methods for long-tailed classification and FGVC.</p><p>Xiangyu Zhang, Xinyu Zhou, Mengxiao Lin, and Jian Sun. Shufflenet: An extremely efficient convolutional neural network for mobile devices. In CVPR, pages 6848-6856, 2018.</p><p>Boyan Zhou, Quan Cui, Xiu-Shen Wei, and Zhao-Min Chen. BBN: bilateral-branch network with cumulative learning for long-tailed visual recognition. In CVPR, pages 9716-9725, 2020.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SUPPLEMENTARY MATERIAL</head><p>A Pl@ntNet label validation process</p><p>Only Pl@ntNet observations with a valid species name were included in the Pl@ntNet-300K dataset.</p><p>The species name validation process of Pl@ntNet is based on a weighted majority voting algorithm taking as input the labels proposed by Pl@ntNet users with a principle of adaptive weights depending on the user's expertise and engagement. More precisely, the most probable label y i of an image x i is computed as:</p><formula xml:id="formula_6">y i = arg max k∈1,...,d u∈Ui w u 1(y u i = k) ,</formula><p>where U i is the set of users who suggested a label for the image x i and y u i is the label proposed by the user u. The weight w u of a user u is computed as:</p><formula xml:id="formula_7">w u = n α u -n β u + b 0 ,</formula><p>where n u is the number of species observed by user u (i.e., the number of species for which the user is author of at least one valid image), plus the number of distinct labels y u i proposed by a user u in the whole dataset. The constant power values α = 0.5, β = 0.2 and b 0 are determined empirically. To be considered as valid, the label y i of an image x i must satisfy:</p><formula xml:id="formula_8">u∈Ui w u 1(y u i = y i ) &gt; θ</formula><p>where θ is a fixed threshold. Images with non-valid labels were discarded from Pl@ntNet-300K dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Hyperparameters</head><p>The hyperparameters used for the experiments in Section 4 of the paper can be found in Table <ref type="table" target="#tab_3">3</ref>.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Motivation</head><p>• For what purpose was the dataset created? Was there a specific task in mind? Was there a specific gap that needed to be filled? Please provide a description. Pl@ntNet-300k dataset was created to evaluate set-valued classification, in particular for plant identification. Unlike previous datasets, Pl@ntNet-300k is designed so as to preserve the high level of ambiguity across classes of the initial real-world dataset (Pl@ntNet) as well as its long tail distribution. • Who created the dataset (e.g., which team, research group) and on behalf of which entity (e.g., company, institution, organization)?</p><p>The dataset was created by Pl@ntNet team, Pl@ntNet being a consortium composed of four French research organisms (Inria, INRAE, CIRAD and IRD).</p><p>• Who funded the creation of the dataset? If there is an associated grant, please provide the name of the grantor and the grant name and number.</p><p>The creation of the dataset was funded by the European Union's Horizon 2020 research and innovation program under grant agreement No 863463 (Cos4Cloud project) and by the French national research agency under the grant agreement ANR-20-CHIA-0001-01 (CaMeLOt project). Pl@ntNet has also received the support of Agropolis Fondation for the platform creation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Composition</head><p>• What do the instances that comprise the dataset represent (e.g., documents, photos, people, countries)? Are there multiple types of instances (e.g., movies, users, and ratings; people and interactions between them; nodes and edges)? Please provide a description.</p><p>The dataset is composed of pictures of plants. We are in the multi-class classification setting: there is a single plant species per image.</p><p>• How many instances are there in total (of each type, if appropriate)?</p><p>There are 306,146 plant images : 243,916 in the training set, 31,118 in the validation set and 31,112 in the test set.</p><p>• Does the dataset contain all possible instances or is it a sample (not necessarily random) of instances from a larger set? If the dataset is a sample, then what is the larger set? Is the sample representative of the larger set (e.g., geographic coverage)? If so, please describe how this representativeness was validated/verified. If it is not representative of the larger set, please describe why not (e.g., to cover a more diverse range of instances, because instances were withheld or unavailable).</p><p>The dataset is sampled from a larger set such that two particular features are preserved. These features are inherent to the way the images are acquired and to the intrinsic diversity of plants morphology: i) The dataset exhibits a strong class imbalance, meaning that a few species represent most of the images. ii) Many species are visually similar, making identification difficult even for the expert eye. More details about these properties are available in Section 3.</p><p>• What data does each instance consist of? "Raw" data (e.g., unprocessed text or images) or features? In either case, please provide a description. Each instance is an image of a single plant.</p><p>• Is there a label or target associated with each instance? If so, please provide a description. Each instance is associated to its species.</p><p>• Is any information missing from individual instances? If so, please provide a description, explaining why this information is missing (e.g., because it was unavailable). This does not include intentionally removed information, but might include, e.g., redacted text.</p><p>There is no missing information.</p><p>• Are relationships between individual instances made explicit (e.g., users' movie ratings, social network links)? If so, please describe how these relationships are made explicit.</p><p>There is no particular relationships between our instances.</p><p>• Are there recommended data splits (e.g., training, development/validation, testing)? If so, please provide a description of these splits, explaining the rationale behind them.</p><p>The dataset already provides a train/validation/test. For more detail see section 3.1.</p><p>• Are there any errors, sources of noise, or redundancies in the dataset? If so, please provide a description.</p><p>Only Pl@ntNet observations with a valid species name were included the dataset. The species name validation is based on a Bayesian inference taking as input the names proposed by Pl@ntNet users with a principle of adaptive weights depending on the user's expertise.</p><p>• Is the dataset self-contained, or does it link to or otherwise rely on external resources (e.g., websites, tweets, other datasets)? If it links to or relies on external resources, a) are there guarantees that they will exist, and remain constant, over time; b) are there official archival versions of the complete dataset (i.e., including the external resources as they existed at the time the dataset was created); c) are there any restrictions (e.g., licenses, fees) associated with any of the external resources that might apply to a future user? Please provide descriptions of all external resources and any restrictions associated with them, as well as links or other access points, as appropriate.</p><p>The dataset is self contained.</p><p>• Does the dataset contain data that might be considered confidential (e.g., data that is protected by legal privilege or by doctorpatient confidentiality, data that includes the content of individuals' non-public communications)? If so, please provide a description.</p><p>No protected data are available in the paper.</p><p>• Does the dataset contain data that, if viewed directly, might be offensive, insulting, threatening, or might otherwise cause anxiety? If so, please describe why.</p><p>No, the dataset only contains plant pictures.</p><p>• Does the dataset relate to people? If not, you may skip the remaining questions in this section.</p><p>No.</p><p>• Does the dataset identify any subpopulations (e.g., by age, gender)? If so, please describe how these subpopulations are identified and provide a description of their respective distributions within the dataset. Irrelevant.</p><p>• Is it possible to identify individuals (i.e., one or more natural persons), either directly or indirectly (i.e., in combination with other data) from the dataset? If so, please describe how.</p><p>Irrelevant.</p><p>• Does the dataset contain data that might be considered sensitive in any way (e.g., data that reveals racial or ethnic origins, sexual orientations, religious beliefs, political opinions or union memberships, or locations; financial or health data; biometric or genetic data; forms of government identification, such as social security numbers; criminal history)? If so, please provide a description. Irrelevant.</p><p>• Any other comments?</p><p>provided, and provide a link or other access point to, or otherwise reproduce, the exact language to which the individuals consented. not applicable • If consent was obtained, were the consenting individuals provided with a mechanism to revoke their consent in the future or for certain uses? If so, please provide a description, as well as a link or other access point to the mechanism (if appropriate). not applicable • Has an analysis of the potential impact of the dataset and its use on data subjects (e.g., a data protection impact analysis) been conducted? If so, please provide a description of this analysis, including the outcomes, as well as a link or other access point to any supporting documentation. not applicable G Preprocessing/cleaning/labeling</p><p>• Was any preprocessing/cleaning/labeling of the data done (e.g., discretization or bucketing, tokenization, part-of-speech tagging, SIFT feature extraction, removal of instances, processing of missing values)? If so, please provide a description. If not, you may skip the remainder of the questions in this section.</p><p>No pre-preprocessing was applied (appart from the curation process, see previous section).</p><p>• Was the "raw" data saved in addition to the preprocessed/cleaned/labeled data (e.g., to support unanticipated future uses)? If so, please provide a link or other access point to the "raw" data. not applicable</p><p>• Is the software used to preprocess/clean/label the instances available? If so, please provide a link or other access point.</p><p>No.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H Uses</head><p>• Has the dataset been used for any tasks already? Not this specific Pl@ntNet subset. If so, please provide a description. • Is there a repository that links to any or all papers or systems that use the dataset? If so, please provide a link or other access point. The list all or some papers that use our dataset will be displayed and updated at the following address: https://github. com/plantnet/PlantNet-300K/ • What (other) tasks could the dataset be used for?</p><p>The dataset can be used for any supervised or unsupervised classification tasks.</p><p>• Is there anything about the composition of the dataset or the way it was collected and preprocessed/cleaned/labeled that might impact future uses? For example, is there anything that a future user might need to know to avoid uses that could result in unfair treatment of individuals or groups (e.g., stereotyping, quality of service issues) or other undesirable harms (e.g., financial harms, legal risks) If so, please provide a description. Is there anything a future user could do to mitigate these undesirable harms? No • Are there tasks for which the dataset should not be used? If so, please provide a description.</p><p>No I Distribution</p><p>• Will the dataset be distributed to third parties outside the entity (e.g., company, institution, organization) on behalf of which the dataset was created? If so, please provide a description. the dataset will be publicly available • How will the dataset be distributed (e.g., tarball on website, API, GitHub)? Does the dataset have a digital object identifier (DOI)? the dataset will be distributed through zenodo under doi: https://doi.org/10.5281/zenodo.4726653</p><p>• When will the dataset be distributed? the dataset will be distributed after acceptance of the paper</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Genus taxonomy: we display three genera present in the proposed dataset-Fedia, Pereskia and Nyctaginia-which contain respectively two, three and one species.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :Figure 3 :</head><label>23</label><figDesc>Figure2: Lorenz curves of the Pl@ntNet database and the proposed dataset. Note that, for fair comparison, we discard species with less than 4 images in the Pl@ntNet database.</figDesc><graphic coords="6,127.80,335.22,67.72,67.72" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Examples of visually similar images belonging to two different classes.</figDesc><graphic coords="7,127.80,164.81,67.72,67.72" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 7 :</head><label>7</label><figDesc>Figure6: Pl@ntNet-300K vs ImageNet macro-average average-5 accuracy for several models (evaluated on the test set).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Benchmark for several popular deep neural network architectures 3 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 9</head><label>9</label><figDesc>Figure9compares the top-5 accuracy of several models when they are trained with either the cross entropy loss or the top-k loss by<ref type="bibr" target="#b1">Berrada et al. [2018]</ref>. The hyperparameters used for training the top-k loss are the same as in Section 4. The smoothing parameter τ of the top-k loss is set to 1.0.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Comparison of top-5 accuracy of models trained with either the cross entropy loss or the loss by Berrada et al. [2018] (with k = 5)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Learning rate, number of epochs and learning rate schedule for the different models. At each learning rate decay, the learning rate is divided by ten.</figDesc><table><row><cell>Models</cell><cell>Initial learning rate</cell><cell>Number of epochs</cell><cell>First decay</cell><cell>Second decay</cell></row><row><cell>mobilenet v2, mobilenet v3 large,</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>resnet 18, 34, 50, 101, 152,</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>densenet 121, 161, 169, 201, inception v3, inception v4,</cell><cell>0.01</cell><cell>30</cell><cell>20</cell><cell>25</cell></row><row><cell>inception resnet v2, wide resnet50 2,</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>wide resnet101 2, shufflenet</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>vgg11, alexnet, mobilenet v3 small, squeezenet</cell><cell>0.001</cell><cell>30</cell><cell>20</cell><cell>25</cell></row><row><cell>efficientnet b0, b1, b2, b3, b4</cell><cell>0.01</cell><cell>20</cell><cell>10</cell><cell>15</cell></row><row><cell>vit</cell><cell>5e-4</cell><cell>20</cell><cell>15</cell><cell>-</cell></row><row><cell cols="4">C Evaluation of existing set-valued classification methods</cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>35th Conference on Neural Information Processing Systems (NeurIPS 2021) Track on Datasets and Benchmarks.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>The division is performed at the species level due to the long-tailed distribution.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>The architectures chosen are: alexnet (1), densenet121 (2), densenet161 (3), densenet169 (4), densenet201 (5), efficientnet b1 (6), efficientnet b1 (7), efficientnet b2 (8), efficientnet b3 (9), efficientnet b4 (10), inception resnet v2 (11), inception v3 (12), inception v4 (13), mobilenet v2 (14), mobilenet v3 large (15), mobilenet v3 small (16), resnet101 (17), resnet152 (18), resnet18(19), resnet34 (20), resnet50 (21), shufflenet  (22), squeezenet (23), vgg11 (24), vit base patch16 224 (25), wide resnet101 2 (26), wide resnet50 2 (27).</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>This work was partially funded by the <rs type="funder">ANR CaMeLOt</rs> <rs type="grantNumber">ANR-20-CHIA-0001-01</rs>. It has received funding from the <rs type="funder">European Union</rs>'s <rs type="programName">Horizon 2020 research and innovation program</rs> under grant agreement No <rs type="grantNumber">863463</rs> (<rs type="projectName">Cos4Cloud</rs> project).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_MaJHUVR">
					<idno type="grant-number">ANR-20-CHIA-0001-01</idno>
				</org>
				<org type="funded-project" xml:id="_jgcnu7t">
					<idno type="grant-number">863463</idno>
					<orgName type="project" subtype="full">Cos4Cloud</orgName>
					<orgName type="program" subtype="full">Horizon 2020 research and innovation program</orgName>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F Collection Process</head><p>• How was the data associated with each instance acquired?</p><p>Was the data directly observable (e.g., raw text, movie ratings), reported by subjects (e.g., survey responses), or indirectly inferred/derived from other data (e.g., part-of-speech tags, model-based guesses for age or language)? Each image comes from the picture of a plant taken by a user of the Pl@ntNet application If data was reported by subjects or indirectly inferred/derived from other data, was the data validated/verified? If so, please describe how. Only Pl@ntNet observations with a valid species name were included the dataset. The species name validation is based on a Bayesian inference taking as input the names proposed by Pl@ntNet users with a principle of adaptive weights depending on the user's expertise.</p><p>• What mechanisms or procedures were used to collect the data (e.g., hardware apparatus or sensor, manual human curation, software program, software API)?</p><p>The data was collected through Pl@ntNet mobile application and curated through crowdsourcing (by Pl@ntNet users) in addition to the automated filtering (CNNbased) of unappropriated or irrelevant content (faces, humans, animals, buildings, etc.).</p><p>How were these mechanisms or procedures validated?</p><p>The mechanisms were validated by Pl@ntNet curators (expert botanists) and by the scientific and technical committee of Pl@ntNet.</p><p>• If the dataset is a sample from a larger set, what was the sampling strategy (e.g., deterministic, probabilistic with specific sampling probabilities)? The sampling is done at the genus level : 10% of the genus are randomly sampled, and all images that belong to these genera are kept. As a last step, we only retained species with at least 4 images.</p><p>• Who was involved in the data collection process (e.g., students, crowdworkers, contractors) and how were they compensated (e.g., how much were crowdworkers paid)? The data is collected by users of the Pl@ntNet application (which has more than 10 millions users). Pl@ntNet users are citizen scientist who gracefully participate to the project. Their reward is the acclaimed performance of the application which enables them to identify plant species.</p><p>• Over what timeframe was the data collected? Does this timeframe match the creation timeframe of the data associated with the instances (e.g., recent crawl of old news articles)?</p><p>If not, please describe the timeframe in which the data associated with the instances was created. The dataset was created with images collected by the Plantnet application from 2011 up to November 2020.</p><p>• Were any ethical review processes conducted (e.g., by an institutional review board)? If so, please provide a description of these review processes, including the outcomes, as well as a link or other access point to any supporting documentation. An ethical review was processed by CIRAD's institutional review board. The main outcome was the terms of use of Pl@ntNet application (https://api.plantnet.org/views/terms_of_use? lang=en) • Does the dataset relate to people? No. If not, you may skip the remainder of the questions in this section. • Did you collect the data from the individuals in question directly, or obtain it via third parties or other sources (e.g., websites)? not applicable • Were the individuals in question notified about the data collection? If so, please describe (or show with screenshots or other information) how notice was provided, and provide a link or other access point to, or otherwise reproduce, the exact language of the notification itself. not applicable • Did the individuals in question consent to the collection and use of their data? If so, please describe (or show with screenshots or other information) how consent was requested and</p><p>• Will the dataset be distributed under a copyright or other intellectual property (IP) license, and/or under applicable terms of use (ToU)? If so, please describe this license and/or ToU, and provide a link or other access point to, or otherwise reproduce, any relevant licensing terms or ToU, as well as any fees associated with these restrictions.</p><p>The dataset and all images composing it will be distributed under Creative-Common Attribution-ShareAlike 2.0 license.</p><p>• Have any third parties imposed IP-based or other restrictions on the data associated with the instances? If so, please describe these restrictions, and provide a link or other access point to, or otherwise reproduce, any relevant licensing terms, as well as any fees associated with these restrictions.</p><p>No.</p><p>• Do any export controls or other regulatory restrictions apply to the dataset or to individual instances? If so, please describe these restrictions, and provide a link or other access point to, or otherwise reproduce, any supporting documentation.</p><p>No.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>J Maintenance</head><p>• Who is supporting/hosting/maintaining the dataset?</p><p>The Pl@ntnet team will maintain the dataset and provide support. The dataset is hosted by http://zenodo.org.</p><p>• How can the owner/curator/manager of the dataset be contacted (e.g., email address)?</p><p>The owner/manager of the dataset can be contacted by mail at plantnet-300k@ inria.fr. • Is there an erratum? If so, please provide a link or other access point.</p><p>Zenodo will provide a versioning of any correction of the dataset. We will keep the users informed at https://github.com/plantnet/PlantNet-300K/</p><p>• Will the dataset be updated (e.g., to correct labeling errors, add new instances, delete instances)? If so, please describe how often, by whom, and how updates will be communicated to users (e.g., mailing list, GitHub)?</p><p>The dataset will be updated if errors are spotted. The update will be performed by the Plantnet team, and these modifications will be listed at https://github.com/ plantnet/PlantNet-300K/. • If the dataset relates to people, are there applicable limits on the retention of the data associated with the instances (e.g., were individuals in question told that their data would be retained for a fixed period of time and then deleted)? If so, please describe these limits and explain how they will be enforced. Irrelevant.</p><p>• Will older versions of the dataset continue to be supported/hosted/maintained? If so, please describe how. If not, please describe how its obsolescence will be communicated to users. Zenodo will provide a versioning of any correction of the dataset.</p><p>• If others want to extend/augment/build on/contribute to the dataset, is there a mechanism for them to do so? If so, please provide a description. Will these contributions be validated/verified? If so, please describe how. If not, why not? Is there a process for communicating/distributing these contributions to other users? If so, please provide a description.</p><p>No.</p><p>• Any other comments?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>K Author statement</head><p>The authors confirm that all data in Pl@ntNet-300K dataset are under a Creative-Common Attribution-ShareAlike 2.0 license (see terms of use here) and bear responsibility in case of violation of copyrights.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Pl@ntnet app in the era of deep learning</title>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Affouard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hervé</forename><surname>Goëau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Bonnet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean-Christophe</forename><surname>Lombardo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexis</forename><surname>Joly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR -Workshop Track</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Smooth loss functions for deep top-k classification</title>
		<author>
			<persName><forename type="first">Leonard</forename><surname>Berrada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">Pawan</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Set-valued classification -overview via a unified framework</title>
		<author>
			<persName><forename type="first">Evgenii</forename><surname>Chzhen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christophe</forename><surname>Denis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohamed</forename><surname>Hebiri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Titouan</forename><surname>Lorieul</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.12318</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Kernel pooling for convolutional neural networks</title>
		<author>
			<persName><forename type="first">Yin</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feng</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanqing</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Serge</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="3049" to="3058" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Confidence sets with expected sizes for multiclass classification</title>
		<author>
			<persName><forename type="first">Christophe</forename><surname>Denis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohamed</forename><surname>Hebiri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page">28</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Aleatory or epistemic? does it matter?</title>
		<author>
			<persName><forename type="first">Armen</forename><surname>Der</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kiureghian</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Ove</forename><surname>Ditlevsen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Structural safety</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="105" to="112" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR. OpenReview.net</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Look closer to see better: Recurrent attention convolutional neural network for fine-grained image recognition</title>
		<author>
			<persName><forename type="first">Jianlong</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heliang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="4476" to="4484" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A general definition of the Lorenz curve</title>
		<author>
			<persName><forename type="first">L</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName><surname>Gastwirth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Econometrica: Journal of the Econometric Society</title>
		<imprint>
			<biblScope unit="page" from="1037" to="1039" />
			<date type="published" when="1971">1971</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Fine-grained classification via mixture of deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">Zongyuan</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Bewley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Mccool</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">I</forename><surname>Corke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Upcroft</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Conrad</forename><surname>Sanderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Fine-grained car detection for visual census estimation</title>
		<author>
			<persName><forename type="first">Timnit</forename><surname>Gebru</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yilun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Duyun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AAAI</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">The inaturalist species classification and detection dataset</title>
		<author>
			<persName><forename type="first">Grant</forename><surname>Van Horn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oisin</forename><surname>Mac Aodha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yin</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Shepard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Serge</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="8769" to="8778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Searching for mobilenetv3</title>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruoming</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weijun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Grace</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1314" to="1324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kilian</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2261" to="2269" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Squeezenet: Alexnet-level accuracy with 50x fewer parameters and &lt;0.5MB model size</title>
		<author>
			<persName><forename type="first">Forrest</forename><forename type="middle">N</forename><surname>Iandola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><forename type="middle">W</forename><surname>Moskewicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Khalid</forename><surname>Ashraf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">J</forename><surname>Dally</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kurt</forename><surname>Keutzer</surname></persName>
		</author>
		<idno>CoRR, abs/1602.07360</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
		<respStmt>
			<orgName>University of Toronto</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Master&apos;s thesis</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1106" to="1114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Top-k multiclass SVM</title>
		<author>
			<persName><forename type="first">Maksim</forename><surname>Lapin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Hein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="325" to="333" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Loss functions for top-k error: Analysis and insights</title>
		<author>
			<persName><forename type="first">Maksim</forename><surname>Lapin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Hein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1468" to="1477" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Analysis and optimization of loss functions for multiclass, top-k, and multilabel classification</title>
		<author>
			<persName><forename type="first">Maksim</forename><surname>Lapin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Hein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1533" to="1554" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Bilinear CNN models for fine-grained visual recognition</title>
		<author>
			<persName><forename type="first">Tsung-Yu</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aruni</forename><surname>Roychowdhury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Subhransu</forename><surname>Maji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1449" to="1457" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Uncertainty in predictions of deep learning models for fine-grained classification</title>
		<author>
			<persName><forename type="first">Titouan</forename><surname>Lorieul</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020-12">December 2020</date>
		</imprint>
		<respStmt>
			<orgName>Université de Montpellier</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Fine-grained visual classification of aircraft</title>
		<author>
			<persName><forename type="first">Subhransu</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Esa</forename><surname>Rahtu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juho</forename><surname>Kannala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Blaschko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1306.5151</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Automated flower classification over a large number of classes</title>
		<author>
			<persName><forename type="first">Maria-Elena</forename><surname>Nilsback</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Sixth Indian Conference on Computer Vision, Graphics &amp; Image Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008">2008. 2008</date>
			<biblScope unit="page" from="722" to="729" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Fine-grained visual categorization via multistage metric learning</title>
		<author>
			<persName><forename type="first">Rong</forename><surname>Qi Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shenghuo</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanqing</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="3716" to="3724" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">An overview of gradient descent optimization algorithms</title>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.04747</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">S</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Mobilenetv2: Inverted residuals and linear bottlenecks</title>
		<author>
			<persName><forename type="first">Mark</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">G</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Menglong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrey</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="4510" to="4520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A survey of hierarchical classification across different application domains</title>
		<author>
			<persName><forename type="first">Carlos</forename><surname>Silla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><forename type="middle">Alves</forename><surname>Freitas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Data Min. Knowl. Discov</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="31" to="72" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Deep neural networks based recognition of plant diseases by leaf image classification</title>
		<author>
			<persName><forename type="first">Srdjan</forename><surname>Sladojevic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marko</forename><surname>Arsenovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andras</forename><surname>Anderla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dubravko</forename><surname>Culibrk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Darko</forename><surname>Stefanovic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Intell. Neurosci</title>
		<imprint>
			<biblScope unit="page" from="1" to="3289801" />
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zbigniew</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Inception-v4, inception-resnet and the impact of residual connections on learning</title>
		<author>
			<persName><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">A</forename><surname>Alemi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="4278" to="4284" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Efficientnet: Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="6105" to="6114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">P</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Mita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<idno>CNS-TR-2010-001</idno>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
		<respStmt>
			<orgName>California Institute of Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">A large-scale car dataset for finegrained categorization and verification</title>
		<author>
			<persName><forename type="first">Linjie</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="3973" to="3981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Wide residual networks</title>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">A review on multi-label learning algorithms</title>
		<author>
			<persName><forename type="first">Min-Ling</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhi-Hua</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Knowl. Data Eng</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1819" to="1837" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
