<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Extracting Definienda in Mathematical Scholarly Articles with Transformers</title>
				<funder>
					<orgName type="full">French government</orgName>
				</funder>
				<funder ref="#_XVcVsGt">
					<orgName type="full">Agence Nationale de la Recherche</orgName>
					<orgName type="abbreviated">ANR</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Shufan</forename><surname>Jiang</surname></persName>
							<email>shufan.jiang@ens.psl.eu</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">PSL University</orgName>
								<orgName type="institution" key="instit2">CNRS &amp; Inria Paris</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><roleName>ENS</roleName><forename type="first">D</forename><forename type="middle">I</forename><surname>Ens</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">PSL University</orgName>
								<orgName type="institution" key="instit2">CNRS &amp; Inria Paris</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Pierre</forename><surname>Senellart</surname></persName>
							<email>pierre@senellart.com</email>
							<affiliation key="aff1">
								<orgName type="laboratory">DI ENS</orgName>
								<orgName type="institution" key="instit1">ENS</orgName>
								<orgName type="institution" key="instit2">PSL University</orgName>
								<orgName type="institution" key="instit3">CNRS &amp; Inria</orgName>
								<orgName type="institution" key="instit4">Institut Universitaire de France Paris</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Extracting Definienda in Mathematical Scholarly Articles with Transformers</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">915E438C2267F5DBFF47F03CDC8B9D0F</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-04-12T14:53+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We consider automatically identifying the defined term within a mathematical definition from the text of an academic article. Inspired by the development of transformer-based natural language processing applications, we pose the problem as (a) a token-level classification task using fine-tuned pre-trained transformers; and (b) a question-answering task using a generalist large language model (GPT). We also propose a rule-based approach to build a labeled dataset from the L A T E X source of papers. Experimental results show that it is possible to reach high levels of precision and recall using either recent (and expensive) GPT 4 or simpler pre-trained models fine-tuned on our task.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Mathematical scholarly articles contain mathematical statements such as axioms, theorems, proofs, etc. These structures are not captured by traditional ways of navigating the scientific literature, e.g., keyword search. We consider initiatives aiming at better knowledge discovery from scientific papers such as sT E X <ref type="bibr" target="#b9">(Kohlhase, 2008)</ref>, a bottom-up solution for mathematical knowledge management that relies on authors adding explicit metadata when writing in L A T E X; MathRepo <ref type="bibr" target="#b5">(Fevola and GÃ¶rgen, 2022)</ref>, a crowd-sourced repository for mathematicians to share any additional research data alongside their papers; or TheoremKB <ref type="bibr" target="#b12">(Mishra et al., 2021)</ref>, a project that extracts the location of theorems and proofs in mathematical research articles. Following these ideas, we aim at automatically building a knowledge graph to automatically index articles with the terms defined therein.</p><p>As a first step, we consider the simpler problem of, given the text of a formal mathematical definition (which is typically obtained from the PDF article), extracting the definienda (terms defined within). As an example, we show in Figure <ref type="figure" target="#fig_0">1</ref> a mathematical definition (as rendered within a PDF article, accompanied with its L A T E X source code) that defines two terms (which we call the definienda): "spread" and "components". In this particular example, the two terms are emphasized in the PDF (by being set in a non-italic font within an italic paragraph) -this is not always the case but we will exploit the fact that some authors do this to build a labeled dataset of definitions and definienda.</p><p>After discussing some related work in Section 2, we describe our approach in Section 3 and show experimental results in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>The difficulties of our task lie in (1) the lack of labeled datasets; (2) the diversity in mathematicians' writing style; and (3) the interplay of discourse and formulae, which differentiate mathematical text and text in the general domain. We review potential corpora and existing approaches in this section.</p><p>The most relevant work to our objective is by <ref type="bibr" target="#b3">Berlioz (2023)</ref>. The author trains supervised classifiers to extract definitions from mathematical papers from arXiv. The best classifier takes static word embeddings built from arXiv papers, partof-speech features of the words, and hand-coded binary features, such as if a word is an acronym, and then applies a BiLSTM-CRF architecture for sequence tagging <ref type="bibr" target="#b7">(Huang et al., 2015)</ref>. The resulting precision, recall, and F 1 are of 0.69, 0.65, and 0.67 respectively. The author uses the classifier to automatically extract term-definition pairs from arXiv articles and Wikidata, resulting in the dataset ArGot <ref type="bibr" target="#b2">(Berlioz, 2021)</ref>. Note however that a limitation of ArGot, which makes it unsuitable in our setting, where the text of definitions is directly taken from PDFs, is that mathematical expressions and formulas are masked out in the training set.</p><p>Another related task is term-definition extraction in the general domain of scientific articles. For example, Scholarphi <ref type="bibr" target="#b6">(Head et al., 2021)</ref> is an augmented reading interface for papers with publicly available L A T E X sources. Given a paper (with its L A T E X source), it lets the reader click on specific words to view their definitions within the paper. The authors test several models for definition-term detection, including an original Heuristically Enhanced Deep Definition Extraction <ref type="bibr" target="#b8">(Kang et al., 2020)</ref>, syntactic features, heuristic rules, and different word representation technologies such as contextualized word representations based on transformers <ref type="bibr" target="#b15">(Vaswani et al., 2017)</ref>. The results show that models involving SciBERT <ref type="bibr" target="#b1">(Beltagy et al., 2019)</ref> achieved higher accuracy on most measurements due to the domain similarity between the scholarly documents for pre-training SciBERT and those used in the evaluation. Following this idea, cc_math_roberta <ref type="bibr" target="#b11">(Mishra et al., 2023</ref>) is a RoBERTa-based model pertained from scratch on mathematical articles from arXiv <ref type="bibr" target="#b11">(Mishra et al., 2023)</ref>. This model outperforms Roberta in a sentence-level classification task while the corpora size for pre-training cc_math_roberta is much smaller than Roberta's. We aim to determine in this work if contextualized word representations can improve the results of mathematical definienda extraction.</p><p>NaturalProofs <ref type="bibr" target="#b16">(Welleck et al., 2021</ref>) is a corpus of mathematical statements and their proofs. These statements are extracted from different sources with hand-crafted rules, such as the content being enclosed by \begin{theorem} and \end{theorem} in the L A T E X source of a textbook project on algebraic stacks<ref type="foot" target="#foot_0">1</ref> . Each statement is either a theorem or a definition. However, this dataset does not annotate the definienda of each definition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Proposed Approach</head><p>We describe our approach in two steps. First, we build a ground-truth dataset using the L A T E X source of papers. As the existing large datasets either concern term-definition extraction from general corpora like web pages or textbooks <ref type="bibr" target="#b16">(Welleck et al., 2021)</ref> or mask out mathematical expressions in the text <ref type="bibr" target="#b2">(Berlioz, 2021)</ref>, we decide to process plain text as it appears in scholarly papers so that our solution can be directly applied to texts extracted from PDF articles when the L A T E X source is unavailable. Second, we study different usages of transformerbased models to extract definienda. We are interested in fine-tuning and one-shot learning (prompt engineering). The source code of our approach, as well as the constructed dataset, is available on Github<ref type="foot" target="#foot_1">2</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Dataset Construction</head><p>To start with a reasonable corpus, we collected the L A T E X source of all 28 477 arXiv papers in the area of Combinatorics (arXiv's math.CO category) published before 1st Jan 2020 through arXiv's bulk access from AWS<ref type="foot" target="#foot_2">3</ref> . Our goal in building the dataset was not to be complete, but to produce as cheaply and reliably as possible a ground-truth dataset of definitions and definienda. For this purpose, we rely on two features of definitions that some authors (but definitely not all!) use: definienda are often written in italics within the definition (or, as in Figure <ref type="figure" target="#fig_0">1</ref>, in non-italics within an italics paragraph); and definienda are sometimes shown in parentheses after the definition header. As we do not need to completely capture all cases in the building of the dataset, we assume that definitions are within a definition L A T E X environment and thus extracted text blocks between \begin{definition} and \end{definition}; we ignored contents enclosed in other author-defined environments, such as \begin{Def}, which might bring us more definitions but also more noise. For defined terms, relying on the two features described above, we extracted the contents within \textit{} and \emph{} from the text blocks as well as the content potentially provided as optional argument to the \begin{definition}[] environment. We then converted the extracted partial L A T E X code into plain text with Unicode characters using pylatexenc<ref type="foot" target="#foot_3">4</ref> . After a brief glance at the most frequent extracted definienda values, we handcrafted regular expressions to filter out the following recurrent noises among them:</p><p>â¢ irrelevant or meaningless phrases such as repeating "i.e." and "\d"; â¢ Latin locutions such as "et al."; â¢ list entries such as "(i)" and "(iii)". After filtering, we got a list of 13 692 text blocks, of which the average length is 70 tokens, and the maximum length is 5 266 tokens. We removed 39 text blocks having more than 500 tokens. Finally, we labeled automatically the texts with IOB2 tagging, where the "B-MATH_TERM" tag denotes the first token of every defined term, "I-MATH_TERM" tag indicates any non-initial token in a defined term, and the "O" tag means that the token is outside any definiendum. Considering partially italics compound terms like "\emph{non}-k-equivalent", we annotate "non-k-equivalent" as a definiendum. We sorted the labeled texts by the last update time of the papers.</p><p>To evaluate the quality of this dataset, we examined by hand 1 024 labeled entries. We found that only 30 annotated texts out of 1 024 to be incorrectly labeled, confirming the quality of our annotation. We manually removed or corrected wrong annotations and got 999 labeled texts, which became our ground truth test data. We built training/validation sets for 10-fold cross-validation with the rest of the labeled texts, to separate them from our test data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Fine-tuning Pre-trained Language Models for Token Classification</head><p>For the fine-tuning setup, we consider the extraction of definienda as a token-level classification problem: given a text block, the classifier labels each token as B-MATH_TERM, I-MATH_TERM or O. We used the implementation for token classification RobertaForTokenClassification in the transformers package <ref type="bibr" target="#b17">(Wolf et al., 2020)</ref>. It loads a pre-trained language model and adds a linear layer on top of the token representation output. We experimented with an out-of-the-box and general language model Roberta-base <ref type="bibr" target="#b10">(Liu et al., 2019)</ref> and a domain-specific model cc_math_roberta <ref type="bibr" target="#b11">(Mishra et al., 2023)</ref>. Since <ref type="bibr" target="#b11">Mishra et al. (2023)</ref> do not report performance on token-level tasks, we used two checkpoints of it, one pretained for 1 epoch (denoted as cc_ep01)<ref type="foot" target="#foot_4">5</ref> , and another pre-trained for 10 epochs (denoted as cc_ep10) <ref type="foot" target="#foot_5">6</ref> . Then we fed the 10 train/validation sets to train the linear layer to predict the probability of a token's representation matching one of the three labels. We set the maximum sequence length of the model to 256. We ran all our experiments with a fixed learning rate of 5 â¢ 10 -5 and a fixed batch size of 16. We searched the best number of epochs among <ref type="bibr">[3,</ref><ref type="bibr">5,</ref><ref type="bibr">10]</ref>. We also experimented with 1 024, 2 048, and 10 240 samples from each training set to see the performance of the classifiers with low resources. As Roberta-base and cc_math_roberta have their own tokenizers, the models' output loss and accuracy are based on different numbers of word pieces and are not comparable. To evaluate the predictions, we used the predicted tag of the first word piece of each word and regrouped the IOB2-tagged word into definienda. We present our unified evaluation over ground truth data in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Querying GPT</head><p>Driven by the growing popularity of few-shot learning with pre-trained language models <ref type="bibr" target="#b4">(Brown et al., 2020)</ref>, we also query the GPT language model, using different available versions: we first experimented with ChatGPT 7 (based on GPT 3.5) and then used the API versions of GPT-3.5-Turbo and GPT-4. We initially gave ChatGPT only one example in our question and attempted to obtain a IOB2-compliant output. We quickly realized that the returned tagging was random, unstable, and incoherent with the expected terms. However, if we ask ChatGPT to return the definienda directly, we get more pertinent results. We thus asked GPT-3.5-Turbo and GPT-4 to identify the definienda in our ground truth data via OpenAI's API. For each request, we send the same task description (system input) and a text from our test data (user input). We fixed the max output length to 128 and temperature to 0. By the time of writing, the cost of these API are count by tokens -GPT-4 8K context model's input and output token prices are 20 and 30 times that of GPT-3.5 4K context model. Since GPT-4 tend to give more precise and shorter responses, the cost of GPT-4 on our task is roughly 20 times that of GPT-3.5. For our test, we spent $0.42 on GPT-3.5 and $7.80 on GPT-4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Evaluation</head><p>Now that we got the predictions from our fine-tuned token classifiers and the answers from GPT models, we compared them with ground truth data. We first removed the repeated expected definienda for each annotated text and got 1 552 unique definienda in total. Then we converted both expected terms and extracted terms to lowercase. For each unique expected term, if it is the same as an extracted term, we counted one "True Positive". We counted one "Cut Off" if it contains an extracted term. If it is contained in an extracted term, we counted one "Too Long". Finally, we removed all spaces in the expected term to make an expected no-space string, and we joined all extracted terms to make an extracted no-space string; if the extracted nospace string contains the expected no-space string, we considered that the expected term is extracted as one "True Positive or Split Term". We calculated the precision, recall, and F 1 -score using the "True Positive or Split Term" count to have a higher tolerance for boundary errors on all models. Table <ref type="table" target="#tab_1">1</ref> shows the results of GPT's answers. Tables <ref type="table" target="#tab_2">2</ref> and<ref type="table" target="#tab_3">3</ref> present the averaged performance of cc_ep01, cc_ep10 and Roberta over 10-fold cross-validation. We set the best precision, recall, and F 1 -scores in bold across these three tables. Our first remark is the high recall of GPTs' answers. Indeed, GPT models, especially GPT-3.5, tend to return everything in the given text, resulting in poor precision. After checking the outputs over the 1024 test data, we found an over-prediction of formulas and mathematical expressions, which corresponds to the analysis by <ref type="bibr" target="#b8">(Kang et al., 2020)</ref>.</p><p>Our second remark is that fine-tuned classifiers have more balanced precision and recall, as the numbers of extracted terms are closer to the expected number (1 552). To our surprise, although the tokenizer of cc_math_roberta models produced fewer word pieces than Roberta's tokenizer, Roberta-base yielded the best performance among the three models in our task, regardless the size of the training set. Moreover, cc_math_roberta models' performance varies more than Roberta's (see in for our task, implying the benefit of pre-training.</p><p>The performances of all fine-tuned models improve significantly as the training set size increases.</p><p>When given 10 240 training data, fine-tuning a pretrained model gives better overall predictions than GPT-4, and when given 2048 training data, finetuned Roberta-base already gives better precision than GPT-4. Finally, note that these finetuned language models are obviously much less computationally expensive than OpenAI's GPT models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this work, we have contributed to the efficient creation of a labeled dataset for definiendum extraction from mathematical papers. We have then compared two usages of transformers: asking GPT vs fine-tuning pre-trained language models. Our experimental results show GPT-4's capacity to understand mathematical texts with only one example in the prompt. We highlight the good precisionrecall balance and the relatively low cost of finetuning Roberta for this domain-specific information  extraction task. A constraint of our work comes from the nature of our labeled data because authors have their own writing styles: there could be more than one correct annotation for a phrase. For instance, our definition blocks are compiled from L A T E X sources, and we plan to test our fine-tuned models on definitions extracted from real PDF format papers without L A T E X sources. <ref type="bibr" target="#b14">Pluvinage (2020)</ref> proposes sentence-level classification and text segmentation to retrieve mathematical results from PDF and can provide a preliminary test set for us. For future work, we will explore the ambiguities of extracted entities and link them to classes. Our experience with cc_math_roberta models also open up research about improving the robustness over different NLP tasks of from-scratched domainspecific language models.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Rendering of a definition from a mathematical scholarly article (Nagy, 2013) accompanied with its L A T E X source code. The definienda are "spread" and "components".</figDesc><graphic coords="2,306.54,254.46,218.11,52.22" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 4 )</head><label>4</label><figDesc>, showing that cc_math_roberta models are less robust to different input data.In all the setups, cc_ep01 was always the worst</figDesc><table><row><cell>Model</cell><cell cols="2">GPT-3.5 GPT-4</cell></row><row><cell>Extracted</cell><cell>6867</cell><cell>2245</cell></row><row><cell>True Positive</cell><cell>1072</cell><cell>942</cell></row><row><cell>TP+Split Term</cell><cell>1315</cell><cell>1383</cell></row><row><cell>Too Long</cell><cell>379</cell><cell>595</cell></row><row><cell>Cut Off</cell><cell>656</cell><cell>138</cell></row><row><cell>Precision</cell><cell cols="2">0.1929 0.6248</cell></row><row><cell>Recall</cell><cell cols="2">0.8312 0.8821</cell></row><row><cell>F 1</cell><cell cols="2">0.3131 0.7315</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Performance comparison of extraction by GPT models. The huge number of extracted terms results in the poor precision of GPT-3.5 model.</figDesc><table><row><cell>Model</cell><cell cols="2">cc_ep01 cc_ep10</cell><cell>Rob.</cell></row><row><cell>Extracted</cell><cell>2093.0</cell><cell cols="2">1710.8 1764.2</cell></row><row><cell>True positive</cell><cell>514.9</cell><cell>881.2</cell><cell>934.2</cell></row><row><cell>TP+Split Term</cell><cell>693.8</cell><cell cols="2">1056.5 1127.5</cell></row><row><cell>Too Long</cell><cell>170.2</cell><cell>209.1</cell><cell>268.8</cell></row><row><cell>Cut Off</cell><cell>522.6</cell><cell>405.2</cell><cell>326.1</cell></row><row><cell>Precision</cell><cell>0.354</cell><cell>0.623</cell><cell>0.646</cell></row><row><cell>Recall</cell><cell>0.447</cell><cell>0.681</cell><cell>0.726</cell></row><row><cell>F 1</cell><cell>0.383</cell><cell>0.647</cell><cell>0.679</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Averaged performance of fine-tuned models, with 2048 training data.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Averaged performance of fine-tuned models, with 10 240 training data samples</figDesc><table><row><cell>Model</cell><cell cols="2">cc_ep01 cc_ep10</cell><cell>Rob.</cell></row><row><cell>Extracted</cell><cell>1775.2</cell><cell cols="2">1779.2 1770.5</cell></row><row><cell>True positive</cell><cell>540.3</cell><cell cols="2">972.6 1082.6</cell></row><row><cell>TP+Split Term</cell><cell>733.9</cell><cell>1152.5</cell><cell>1232</cell></row><row><cell>Too Long</cell><cell>143.5</cell><cell>201.3</cell><cell>233.7</cell></row><row><cell>Cut Off</cell><cell>509.6</cell><cell>438.2</cell><cell>274.1</cell></row><row><cell>Precision</cell><cell>0.420</cell><cell>0.652</cell><cell>0.697</cell></row><row><cell>Recall</cell><cell>0.473</cell><cell>0.743</cell><cell>0.794</cell></row><row><cell>F 1</cell><cell>0.442</cell><cell>0.692</cell><cell>0.742</cell></row><row><cell cols="4">Model cc_ep01 cc_ep10 Rob.</cell></row><row><cell>2048</cell><cell>0.044</cell><cell cols="2">0.052 0.031</cell></row><row><cell>10240</cell><cell>0.043</cell><cell cols="2">0.026 0.011</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>The standard deviation of the F 1 score of different fine-tuned models, with 2048 and with 10 240 training data samples</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>https://github.com/stacks/ stacks-project</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>https://github.com/sufianj/def_ extraction</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>https://info.arxiv.org/help/bulk_ data_s3.html</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"><p>https://github.com/phfaist/pylatexenc</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4"><p>https://huggingface.co/InriaValda/cc_ math_roberta_ep01</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5"><p>https://huggingface.co/InriaValda/cc_ math_roberta_ep10</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_6"><p>An example of our conversation: https://chat.openai.com/share/ c96b156f-cba1-4804-8f19-1622a9bc564e</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>This work was funded in part by the <rs type="funder">French government</rs> under management of <rs type="funder">Agence Nationale de la Recherche</rs> as part of the "<rs type="programName">Investissements d'avenir" program</rs>, reference <rs type="grantNumber">ANR-19-P3IA-0001</rs> (<rs type="projectName">PRAIRIE 3IA Institute</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_XVcVsGt">
					<idno type="grant-number">ANR-19-P3IA-0001</idno>
					<orgName type="project" subtype="full">PRAIRIE 3IA Institute</orgName>
					<orgName type="program" subtype="full">Investissements d&apos;avenir&quot; program</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Iz</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Scibert: A pretrained language model for scientific text</title>
		<author>
			<persName><forename type="first">Kyle</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arman</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName><surname>Cohan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3615" to="3620" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">ArGoT: A Glossary of Terms extracted from the arXiv</title>
		<author>
			<persName><forename type="first">Luis</forename><surname>Berlioz</surname></persName>
		</author>
		<idno type="DOI">10.4204/EPTCS.342.2</idno>
	</analytic>
	<monogr>
		<title level="j">Electronic Proceedings in Theoretical Computer Science</title>
		<imprint>
			<biblScope unit="volume">342</biblScope>
			<biblScope unit="page" from="14" to="21" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Hierarchical Representations from Large Mathematical Corpora</title>
		<author>
			<persName><forename type="first">Luis</forename><surname>Berlioz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
		<respStmt>
			<orgName>University of Pittsburgh</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Language Models are Few-Shot Learners</title>
		<author>
			<persName><forename type="first">B</forename><surname>Tom</surname></persName>
		</author>
		<author>
			<persName><surname>Brown</surname></persName>
		</author>
		<idno>ArXiv:2005.14165</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">Claudia</forename><surname>Fevola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christiane</forename><surname>GÃ¶rgen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2202.04022</idno>
		<title level="m">The mathematical research-data repository mathrepo</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Augmenting scientific papers with just-in-time, position-sensitive definitions of terms and symbols</title>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Head</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems</title>
		<meeting>the 2021 CHI Conference on Human Factors in Computing Systems</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Bidirectional lstm-crf models for sequence tagging</title>
		<author>
			<persName><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.01991</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Document-Level Definition Detection in Scholarly Documents: Existing Models, Error Analyses, and Future Directions</title>
		<author>
			<persName><forename type="first">Dongyeop</forename><surname>Kang</surname></persName>
		</author>
		<idno>ArXiv:2010.05129</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Using L A T E Xas a semantic markup format</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Kohlhase</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematics in Computer Science</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="279" to="304" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m">Roberta: A robustly optimized bert pretraining approach</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Multimodal machine learning for extraction of theorems and proofs in the scientific literature</title>
		<author>
			<persName><forename type="first">Shrey</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Gauquier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Senellart</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2307.09047</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Towards extraction of theorems and proofs in scholarly articles</title>
		<author>
			<persName><forename type="first">Shrey</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucas</forename><surname>Pluvinage</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Senellart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st ACM Symposium on Document Engineering</title>
		<meeting>the 21st ACM Symposium on Document Engineering<address><addrLine>Limerick Ireland</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Linear groups as right multiplication groups of quasifields</title>
		<author>
			<persName><forename type="first">P</forename><surname>GÃ¡bor</surname></persName>
		</author>
		<author>
			<persName><surname>Nagy</surname></persName>
		</author>
		<idno type="DOI">10.1007/s10623-013-9860-1</idno>
	</analytic>
	<monogr>
		<title level="j">Designs, Codes and Cryptography</title>
		<imprint>
			<biblScope unit="volume">72</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="153" to="164" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><forename type="first">Lucas</forename><surname>Pluvinage</surname></persName>
		</author>
		<title level="m">Extracting scientific results from research articles</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
		<respStmt>
			<orgName>Ecole Normale SupÃ©rieure (ENS)</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Master&apos;s thesis</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Naturalproofs: Mathematical theorem proving in natural language</title>
		<author>
			<persName><forename type="first">Sean</forename><surname>Welleck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 1)</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Transformers: State-of-theart natural language processing</title>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 conference on empirical methods in natural language processing: system demonstrations</title>
		<meeting>the 2020 conference on empirical methods in natural language processing: system demonstrations</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="38" to="45" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
