<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="fr">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Sur la vérification du locuteur à partir de traces d&apos;exécution de modèles acoustiques personnalisés</title>
				<funder ref="#_RDRFAA2 #_gnxyxav #_8aqefNF">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Natalia</forename><surname>Tomashenko</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">LIA</orgName>
								<orgName type="institution" key="instit2">Avignon Université</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Cet article repose sur</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Salima</forename><surname>Mdhaffar</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">LIA</orgName>
								<orgName type="institution" key="instit2">Avignon Université</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Cet article repose sur</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Marc</forename><surname>Tommasi</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Université de Lille</orgName>
								<address>
									<settlement>Inria</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yannick</forename><surname>Estève</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">LIA</orgName>
								<orgName type="institution" key="instit2">Avignon Université</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Cet article repose sur</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jean-François</forename><surname>Bonastre</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">LIA</orgName>
								<orgName type="institution" key="instit2">Avignon Université</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Cet article repose sur</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Sur la vérification du locuteur à partir de traces d&apos;exécution de modèles acoustiques personnalisés</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">071E8A38EF46A3C7C0000A833FB9DAC4</idno>
					<note type="submission">le travail présenté dans (Tomashenko et al.,</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-04-12T14:49+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Vie privée</term>
					<term>apprentissage fédéré</term>
					<term>modèles acoustiques</term>
					<term>modèles d&apos;attaques</term>
					<term>reconnaissance vocale</term>
					<term>vérification du locuteur Privacy</term>
					<term>federated learning</term>
					<term>acoustic models</term>
					<term>attack models</term>
					<term>speech recognition</term>
					<term>speaker verification</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Les modèles acoustiques personnalisés sont construits par entraînement à partir de données provenant d'un locuteur unique en raffinant un modèle générique. Une question importante est de savoir si l'accès à ces modèles personnalisés permet facilement de construire une attaque permettant d'identifier le locuteur associé. Ce problème est important dans le contexte de l'apprentissage fédéré de modèles pour la reconnaissance de la parole où un modèle global est appris sur un serveur à partir des modifications des paramètres des modèles reçues de plusieurs clients. Nous proposons une méthode qui consiste à construire des empreintes de ces modèles à partir des traces de leur application sur un jeu de données fixe et indépendant que nous appelons indicateur. Grâce à ces empreintes, nous développons deux modèles d'attaques très efficaces qui visent à inférer l'identité du locuteur.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="fr">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>L'apprentissage fédéré (AF) pour la reconnaissance automatique de la parole (RAP) connaît une grande popularité dans plusieurs tâches et travaux <ref type="bibr" target="#b3">(Cui et al., 2021;</ref><ref type="bibr" target="#b4">Dimitriadis et al., 2020;</ref><ref type="bibr" target="#b8">Guliani, 2021;</ref><ref type="bibr" target="#b26">Yu et al., 2021;</ref><ref type="bibr">Tomashenko et al., 2022a)</ref> 1 . Le respect de la vie privée est l'un des principaux défis de l'AF <ref type="bibr" target="#b12">(Li et al., 2020;</ref><ref type="bibr" target="#b15">Mothukuri et al., 2021)</ref>. Contrairement aux algorithmes d'apprentissage classiques qui utilisent un serveur contenant les données d'apprentissage, l'AF apprend sur des données stockées localement et communique uniquement les modifications (mises à jour). Ceci permet de protéger les données personnelles puisqu'elles ne sont ni stockées dans un serveur, ni partagées avec d'autres utilisateurs. Cependant, ces mises à jour peuvent encore contenir certaines informations sensibles <ref type="bibr" target="#b6">(Geiping et al., 2020;</ref><ref type="bibr" target="#b2">Carlini et al., 2019)</ref>. Des travaux récents ont montré que les modèles appris par l'AF sont vulnérables à différents types d'attaques <ref type="bibr" target="#b24">(Truex et al., 2019;</ref><ref type="bibr" target="#b25">Wang et al., 2019)</ref>. Les techniques pour améliorer la confidentialité dans un cadre de l'AF s'appuient principalement sur deux approches <ref type="bibr" target="#b15">(Mothukuri et al., 2021)</ref> : le calcul multipartite sécurisé <ref type="bibr" target="#b1">(Bonawitz et al., 2016)</ref> et la confidentialité différentielle <ref type="bibr" target="#b5">(Dwork, 2006)</ref>. Les méthodes de chiffrement <ref type="bibr" target="#b19">(Smaragdis &amp; Shashanka, 2007)</ref> comme le chiffrement entièrement homomorphe <ref type="bibr" target="#b19">(Smaragdis &amp; Shashanka, 2007)</ref> et le calcul multipartite sécurisé effectuent le calcul dans le domaine crypté. Ces méthodes sont trop coûteuses en termes de calcul. Les méthodes de confidentialité différentielle préservent la confidentialité en ajoutant du bruit aux paramètres des utilisateurs <ref type="bibr" target="#b5">(Dwork, 2006)</ref>. Cependant, ces solutions peuvent dégrader les performances d'apprentissage à cause de l'incertitude qu'elles introduisent dans les paramètres. Les méthodes alternatives à la protection de la confidentialité pour la parole comprennent les méthodes de suppression qui sont destinées à l'analyse des sons ambiants, et l'anonymisation <ref type="bibr" target="#b23">(Tomashenko et al., 2022b)</ref> qui vise à supprimer les informations personnelles identifiables dans le signal vocal en gardant tous les autres attributs. Ces méthodes de protection de la confidentialité peuvent être combinées et intégrées de manière hybride dans un cadre d'AF.</p><p>Malgré l'intérêt récent porté à l'AF pour la RAP et à d'autres tâches telles que le repérage de motsclés <ref type="bibr" target="#b11">(Leroy et al., 2019)</ref>, la reconnaissance des émotions <ref type="bibr" target="#b10">(Latif et al., 2020)</ref>, et la vérification du locuteur <ref type="bibr" target="#b7">(Granqvist et al., 2020)</ref>, il existe très peu d'études sur les attaques de confidentialité, dans un contexte d'AF, des modèles acoustiques (MA) pour la reconnaissance de la parole. Il a tout de même été montré récemment qu'il est possible d'extraire des informations sur le locuteur à partir des modifications portées sur les poids d'un modèle acoustique neuronal lors de sa personnalisation <ref type="bibr" target="#b14">(Mdhaffar et al., 2022)</ref>.</p><p>Nos travaux s'inscrivent dans le cadre de ces attaques : nous étudions les informations propres au locuteur qui peuvent être extraites à partir de modèles acoustiques personnalisés mis à jour localement. Nous explorons différentes modèles d'attaques qui opèrent directement sur les paramètres du modèle mis à jour sans avoir accès aux données réelles de l'utilisateur. L'idée principale des méthodes proposées est d'utiliser un jeu de données externe (indicateur) pour analyser l'empreinte des modèles acoustiques sur ces données. Une autre contribution importante de ce travail concerne l'analyse des informations sur le locuteur représentées dans les modèles acoustiques neuronaux adaptés.</p><p>2 Apprentissage fédéré pour les modèles acoustiques de RAP Nous considérons un scénario classique d'apprentissage fédéré où un modèle acoustique neuronal global est entraîné sur un serveur à l'aide des données stockées localement sur plusieurs dispositifs distants <ref type="bibr" target="#b12">(Li et al., 2020)</ref>. L'apprentissage du modèle global est effectué sous la contrainte que les données vocales d'apprentissage sont stockées et traitées localement sur les dispositifs des utilisateurs (clients). Seules les mises à jour du modèle sont transmises au serveur à partir de chaque client. Le modèle global est appris sur le serveur en fonction des mises à jour reçues de plusieurs clients. La Figure <ref type="figure" target="#fig_0">1</ref> illustre l'AF dans un réseau distribué de clients. Tout d'abord, le modèle acoustique initial de reconnaissance de la parole W g est distribué à l'ensemble des systèmes des N utilisateurs (locuteurs). Ensuite, le modèle global initial est exécuté sur chaque dispositif utilisateur s i (i ∈ 1..N ) et mis à jour localement sur les données privées de l'utilisateur. Les modèles mis à jour W si sont ensuite transmis au serveur où ils sont agrégés pour obtenir un nouveau modèle global W * g . En général, les modèles personnalisés mis à jour sont agrégés en utilisant la moyenne fédérée et ses variations <ref type="bibr" target="#b13">(McMahan et al., 2017)</ref>. Ensuite, le modèle global mis à jour W * g est partagé avec les clients. Ce processus est répété plusieurs fois jusqu'à la convergence du modèle ou en fixant un nombre d'itérations. L'utilité et l'efficacité de l'apprentissage des modèles entraînés dans le cadre d'AF ont été étudiées avec succès dans des travaux récents <ref type="bibr" target="#b3">(Cui et al., 2021;</ref><ref type="bibr" target="#b4">Dimitriadis et al., 2020;</ref><ref type="bibr" target="#b8">Guliani, 2021;</ref><ref type="bibr" target="#b26">Yu et al., 2021)</ref>. Dans cette étude, nous nous concentrons sur l'aspect de la protection de la vie privée. </p><formula xml:id="formula_0">Server Client 1 Client 2 Client N 𝑊 𝑔 𝑊 𝑔 * 𝑊 𝑠 1 𝑊 𝑠 2 𝑊 𝑠 𝑁 𝑊 𝑔 𝑊 𝑔 𝑊 𝑔</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Modèles d'attaques</head><p>Dans cette section, nous décrivons le scénario de protection de la confidentialité et nous présentons deux modèles d'attaques.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Scénario de préservation de la confidentialité</head><p>La préservation de la confidentialité est formulée comme un jeu entre des utilisateurs qui partagent certaines données et des attaquants qui accèdent à ces données ou à des données dérivées de celles-ci et visent à déduire des informations sur les utilisateurs <ref type="bibr" target="#b22">(Tomashenko et al., 2020)</ref>. Les attaquants visent à attaquer les utilisateurs en utilisant les informations détenues par le serveur. Ils peuvent avoir accès à des modèles personnalisés. Dans ce travail, nous supposons qu'un attaquant a accès aux données suivantes :</p><p>-Un modèle global initial W g .</p><p>-Un modèle personnalisé W s du locuteur cible (target) s qui est inscrit dans le système d'apprentissage fédéré. Le modèle personnalisé correspondant a été obtenu à partir du modèle global W g en ajustant par fine-tuning les poids de W g à l'aide des données du locuteur. Nous considérons ce modèle comme enrollment pour un attaquant.</p><p>- 1. ∀ W si ∈ W, ∀ u j ∈ I nous calculons les valeurs d'activation de la couche h pour les paires de modèles :</p><formula xml:id="formula_1">W h si (u j ) = {w h,t si,j } Tj t=1 et W h g (u j ) = {w h,t g,j } Tj t=1</formula><p>, et les différences par vecteur entre les sorties correspondantes :</p><formula xml:id="formula_2">∆ h s i (uj) = {∆ h,t s i ,j } T j t=1 , où ∆ h,t s i ,j = w h,t s i ,j -w h,t g,j , t ∈ 1..Tj.<label>(1)</label></formula><p>2. Pour chaque modèle personnalisé, nous calculons les vecteurs de moyenne et d'écart type pour ∆ h,t si,j sur tous les vecteurs de parole dans les données indicateur I :</p><formula xml:id="formula_3">µ h s i = I j=1 T j t=1 ∆ h,t s i ,j I j=1 Tj et σ h s i = I j=1 T j t=1 (∆ h,t s i ,j -µ h s i ) 2 I j=1 Tj 1 2</formula><p>.</p><p>(2)</p><p>3. Pour une paire de modèles personnalisés W si and W s k , nous calculons un score de similarité ρ pour la couche cachée h sur l'ensemble de données indicateur sur la base de la distance euclidienne normalisée L 2 entre les paires de vecteurs correspondants pour les moyennes et les écarts types :</p><formula xml:id="formula_4">ρ(W h s i , W h s k ) = αµ ∥µ h s i -µ h s k ∥2 ∥µ h s i ∥2∥µ h s k ∥2 + ασ ∥σ h s i -σ h s k ∥2 ∥σ h s i ∥2∥σ h s k ∥2 ,<label>(3)</label></formula><p>où α µ , α σ sont des paramètres fixes dans toutes les expériences.</p><p>4. En utilisant les scores de similarité obtenues pour toutes les paires de matrices, nous pouvons effectuer une tâche de vérification du locuteur.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pool de modèles personnalisés</head><p>Indicateur données vocales</p><formula xml:id="formula_5">𝑊 𝑠 1 ℎ (𝑢) 𝑊 𝑔 ℎ (𝑢) 𝑊 𝑠 𝑁 𝑊 𝑠 1 𝑊 𝑠 2 𝑊 𝑔 𝑢 𝑢 𝑊 𝑠 𝑁 ℎ (𝑢) Δ 𝑠 1 ℎ (𝑢) Δ 𝑠 𝑁 ℎ (𝑢) [μ 𝑠 1 ℎ , 𝜎 𝑠 1 ℎ ] [μ 𝑠 𝑁 ℎ , 𝜎 𝑠 𝑁 ℎ ]</formula><p>…   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Modèles acoustiques pour la reconnaissance automatique de la parole</head><p>Les modèles acoustiques pour la RAP suivent une architecture neuronale de type TDNN <ref type="bibr" target="#b16">(Peddinti et al., 2015)</ref> et ont été entraînés en utilisant la boîte à outils de reconnaissance vocale Kaldi <ref type="bibr" target="#b17">(Povey et al., 2011)</ref>. Les coefficients cepstraux de fréquence Mel (MFCC) à 40 dimensions, concaténés à des i-vecteurs à 100 dimensions, ont été utilisés comme entrée dans les réseaux de neurones. Chaque modèle comporte treize couches cachées de 512 dimensions, suivies d'une couche softmax dans laquelle 3664 états de triphonie ont été utilisés comme target<ref type="foot" target="#foot_0">2</ref> . Le modèle global initial W g a été entraîné en utilisant le critère lattice-free maximum mutual information (LF-MMI) <ref type="bibr" target="#b18">(Povey et al., 2016)</ref>. Les deux types de stratégies d'augmentation des données vocales ont été appliquées pour les données d'entraînement et d'adaptation : perturbation de la vitesse (avec des facteurs de 0,9, 1,0, 1,1) et perturbation du volume, comme dans <ref type="bibr" target="#b16">(Peddinti et al., 2015)</ref>. Chaque modèle est composé d'environ 13.8 millions de paramètres. Le modèle global initial W g a été entraîné sur le Train-G. Des modèles personnalisés W si ont été obtenus en ajustant finement tous les paramètres de W g sur les données des locuteurs de Part-1 et Part-2 comme décrit dans <ref type="bibr">(Tomashenko et al., 2022a)</ref>. Pour tous les modèles personnalisés de locuteurs, nous utilisons approximativement la même quantité de données vocales pour effectuer le réglage fin (adaptation du locuteur ou fine-tuning) -environ 4 minutes par modèle. Pour la plupart des locuteurs (564 dans Part-1, 463 dans Part-2), nous avons obtenu deux modèles personnalisés différents (par locuteur) sur des sous-ensembles d'adaptation disjoints, pour les autres locuteurs, nous ne disposons de données d'adaptation que pour un seul modèle.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Modèles d'attaques</head><p>Nous étudions deux approches pour les modèles d'attaques : A1 -une approche simple basée sur l'analyse statistique comparative des sorties du modèle de RN et le score de similarité associé entre les modèles personnalisés, et A2 -une approche basée sur les RN. Pour les essais sur les cibles (test target trials), nous utilisons des comparaisons entre différents modèles personnalisés des mêmes locuteurs (564 dans le Part-2), et pour les essais sur les non-cibles (test non-target trials), nous avons sélectionné aléatoirement 10K paires de modèles de différents locuteurs (choisis au hasard parmi toutes les 1079 × 1078/2 comparaisons possibles) dans un ensemble de données correspondant.</p><p>Modèle d'attaque A1. Le premier modèle d'attaque a été appliqué comme décrit dans la section (3.2). Les paramètres α µ , α σ dans la formule (3) sont respectivement égaux à 1 et 10. Ce modèle a été évalué sur un jeu de données de modèles personnalisés. Le jeu de données indicateur est le même dans toutes les expériences.</p><p>Modèle d'attaque A2. Pour entraîner le modèle d'attaque A2, nous utilisons 1300 modèles de locuteurs personnalisés correspondant à 736 locuteurs uniques de Part-1. Lorsque nous avons appliqué la partie fixe de l'architecture présentée dans la Figure <ref type="figure">3</ref> au jeu de données indicateur de 32 minutes pour chaque modèle de locuteur dans Part-1, nous avons obtenu les données d'entraînement avec la quantité correspondant à environ 693h (32×1300). La partie entraînée du modèle neuronal, illustrée dans la Figure <ref type="figure">3</ref>, a une topologie similaire à celle d'un extracteur de x-vecteur conventionnel <ref type="bibr" target="#b20">(Snyder et al., 2018)</ref>. Cependant, l'extracteur de x-vecteur permet de prédire l'identité du locuteur pour le segment de discours donné alors que notre modèle proposé apprend à prédire l'identité du locuteur à partir de la partie W h s d'un modèle personnalisé du locuteur. Nous avons entraîné deux modèles d'attaques correspondant aux deux valeurs du paramètre h ∈ {1, 5} -une couche cachée dans les MA neuronaux RAP à laquelle nous calculons les activations. Les valeurs h ont été choisies en fonction des résultats obtenus pour le modèle d'attaque A1. La dimension de sortie de la partie fixe est 512. La partie fixe est suivie par la partie entraînée qui consiste en sept couches TDNN cachées et une couche de regroupement statistique introduite après la cinquième couche TDNN. La sortie est une couche softmax avec les cibles (sorties) correspondant aux locuteurs dans le pool de modèles personnalisés de locuteurs (nombre de locuteurs uniques dans Part-1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Résultats</head><p>Les modèles d'attaques ont été évalués en termes de equal error rate (EER) 3 . Les résultats du modèle d'attaque A1 sont présentés dans la Figure <ref type="figure">4</ref>. Les informations de locuteur peuvent être capturées pour toutes les valeurs de h avec un succès variable : EER varie de 0,86% (pour la première couche cachée) à 20,51% (pour la couche cachée supérieure). Pour analyser l'impact de chaque partie de la somme de la formule (3) sur les performances de la VAL, nous calculons séparément le score de similarité ρ en utilisant uniquement les moyennes (α σ = 0) ou uniquement les écarts types (α µ = 0). L'impact de chaque terme de la somme change pour les différentes couches cachées. Lorsque nous utilisons uniquement les écarts-types, nous observons le plus faible EER sur la première couche. Dans le cas de l'utilisation des moyennes uniquement, la première couche est, au contraire, l'une des moins informatives pour la vérification du locuteur. Pour toutes les autres couches, la combinaison des moyennes et des écarts-types fournit des résultats supérieurs à ceux obtenus dans les cas où une seule de ces composantes est utilisée.</p><p>Nous choisissons deux valeurs h ∈ {1, 5} qui montrent des résultats prometteurs pour le modèle A1, et nous utilisons les sorties correspondantes pour entraîner deux modèles d'attaques avec la 3. En désignant par P fa (θ) et P miss (θ) les taux de faux positifs (false alarm) et de faux négatifs (miss rates) au seuil θ, l'EER correspond au seuil θ EER pour lequel les deux taux d'erreur de détection sont égaux : EER = P fa (θ EER ) = P miss (θ EER ). </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>FIGURE 1 -</head><label>1</label><figDesc>FIGURE 1 -Apprentissage fédéré dans un réseau distribué de clients : 1) Téléchargement du modèle global W g par les clients. 2) Adaptation au locuteur de W g sur les appareils locaux en utilisant les données privées de l'utilisateur. 3) Collecte et agrégation de plusieurs modèles personnalisés W s1 ,...,W s N sur le serveur. 4) Partage du modèle résultant W * g avec les différents clients.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>FIGURE 2 -FIGURE 3 -</head><label>23</label><figDesc>FIGURE 2 -Calcul des statistiques pour le modèle d'attaque A1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Des modèles personnalisés des locuteurs non-cibles (non-target) et cibles (target) : W s1 ,...,W s N . Nous appellerons ces modèles test trial.L'objectif de l'attaquant est de réaliser une tâche de vérification automatique du locuteur (VAL) en utilisant le modèle de données d'inscription (enrollment) sous la forme de W s et les données d'essai (test trial) de test sous la forme de modèles W s1 ,...,W s N . Autrement dit, étant donné un modèle W s correspondant au locuteur cible, la tâche consiste à identifier quels modèles parmi les W si correspondent ou non au locuteur cible<ref type="bibr" target="#b0">(Bonastre et al., 2021)</ref>.Les deux approches proposées reposent sur l'hypothèse que nous pouvons capturer des informations sur l'identité du locuteur s à partir du modèle correspondant adapté au locuteur W s et du modèle global W g en comparant les sorties de ces deux modèles acoustiques neuronaux provenant des couches cachées h sur certaines données vocales. Nous appellerons ces données vocales indicateur. Ces données ne sont liées ni aux données de test ni aux données d'apprentissage des modèles.</figDesc><table><row><cell>3.2 Modèles d'attaques</cell></row></table><note><p>Modèle d'attaque A1. La Figure 2 illustre la VAL pour le modèle d'attaque A1 proposé dans ce papier. Ce modèle d'attaque comporte plusieurs étapes. On considère un ensemble d'énoncés (utterances) dans l'ensemble de données indicateur I = {u 1 , . . . , u J } ; une séquence de vecteurs dans l'énoncé u j ={u 1 j , . . ., u Tj j } ; un ensemble de modèles personnalisés W = {W s1 , . . . , W s N } ; et un identifiant d'une couche cachée dans le MA global ou personnalisé représenté par h. Nous présentons ci-dessous les étapes pour le modèle d'attaque A1.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>heures de données vocales en anglais provenant d'environ 2K locuteurs, 16kHz. Nous avons sélectionné trois ensembles de données à partir du corpus d'entraînement TED-LIUM 3 : Train-G, Part-1, Part-2 avec des sous-ensembles de locuteurs disjoints comme indiqué dans le tableau 1. Le jeu de données indicateur a été utilisé pour entraîner les modèles d'attaques. Il est composé de 320 énoncés (utterances) sélectionnés parmi les 32 locuteurs des ensembles de données de test et de développement du corpus TED-LIUM 3. Les locuteurs du jeu de données indicateur sont disjoints des locuteurs de Train-G, Part-1, et Part-2. Pour chaque locuteur de l'ensemble de données indicateur, nous sélectionnons uniquement 10 énoncés. La taille totale du jeu de données indicateur est de 32 minutes. L'ensemble de données Train-G a été utilisé pour entraîner un modèle acoustique global initial W g . Part-1 et Part-2 ont été utilisés pour obtenir deux ensembles de modèles personnalisés.</figDesc><table><row><cell></cell><cell cols="4">Train-G Part-1 Part-2 Indicateur</cell></row><row><cell>Durée, heures</cell><cell>200</cell><cell>86</cell><cell>73</cell><cell>0.5</cell></row><row><cell>Nombre de locuteurs</cell><cell>880</cell><cell>736</cell><cell>634</cell><cell>32</cell></row><row><cell cols="2">Nombre de modèles personnalisés -</cell><cell cols="3">1300 1079 -</cell></row></table><note><p><p><p>Les expériences ont été menées sur la partition d'adaptation au locuteur du corpus TED-LIUM 3</p><ref type="bibr" target="#b9">(Hernandez et al., 2018)</ref></p>. Ce corpus disponible publiquement contient les conférences TED qui représentent 452</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE 1 -</head><label>1</label><figDesc>Statistiques de données</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE 2 -</head><label>2</label><figDesc>FIGURE 4 -EER, % pour le modèle d'attaque A1 en fonction de la couche cachée h, évalué sur Part-2. µ + σ -les moyennes et les écarts types ont été utilisés pour calculer le score de similarité. ρ µ -uniquement les moyennes ; et σ -uniquement les écarts types ont été utilisés. configuration A2. Les résultats comparatifs des deux modèles d'attaques sont présentés dans le tableau 2. Pour h = 5, le deuxième modèle d'attaque fournit une amélioration significative des performances par rapport au premier et réduit l'EER de 7% à 2%. Pour h = 1, nous n'avons pu obtenir aucune amélioration en entraînant un modèle d'attaque basé sur un réseau de neurones : les résultats pour A2 dans ce cas sont moins bons par rapport à l'approche simple A1. EER, % évalué sur Part-2, h -indicateur d'une couche cachée5 ConclusionsDans cette étude, nous nous sommes concentrés sur le problème de la protection de la vie privée pour les modèles acoustique de RAP construits dans un cadre d'apprentissage fédéré. Nous avons exploré dans quelle mesure ces modèles de RAP sont vulnérables aux attaques contre la confidentialité. Nous avons développé deux modèles d'attaques qui visent à déduire l'identité du locuteur à partir des modèles personnalisés mis à jour localement sans avoir accès aux données vocales des locuteurs cibles. Un modèle d'attaque est basé sur le score de similarité proposé entre les modèles acoustiques personnalisés, calculé sur un ensemble de données indicateur externe, et un autre est un modèle neuronal. Nous avons démontré sur le corpus TED-LIUM 3 que les deux modèles d'attaque sont très efficaces et peuvent fournir un EER d'environ 1% pour le modèle d'attaque simple A1 et 2% pour le modèle d'attaque neuronal A2. Une autre contribution importante de ce travail est la découverte que la première couche des modèles acoustiques personnalisés contient une grande quantité d'informations sur le locuteur qui sont principalement contenues dans les valeurs de déviation standard calculées sur les données indicateur. Cette propriété intéressante des modèles acoustiques neuronaux personnalisés ouvre de nouvelles perspectives également pour la VAL. Dans des travaux futurs, nous prévoyons de l'utiliser pour développer un système VAL efficace.</figDesc><table><row><cell></cell><cell>30</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>25</cell><cell>+</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>EER,%</cell><cell>15 20</cell><cell></cell><cell></cell><cell>12.97</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>11.98</cell><cell>13.17</cell><cell>14.15</cell><cell>16.08</cell><cell>18.96</cell><cell>20.51</cell></row><row><cell></cell><cell>10</cell><cell></cell><cell>8.41</cell><cell></cell><cell>7.94</cell><cell>7.11</cell><cell>7.98</cell><cell>8.86</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>0.86</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell><cell>6</cell><cell>7</cell><cell>8</cell><cell>9</cell><cell>10</cell><cell>11</cell><cell>12</cell><cell>13</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Couche cachée</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">Modèle d'attaque</cell><cell cols="2">h=1 h=5</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>A1</cell><cell></cell><cell></cell><cell></cell><cell cols="2">0.86 7.11</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>A2</cell><cell></cell><cell></cell><cell cols="3">12.31 1.94</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>En suivant la notation de<ref type="bibr" target="#b16">(Peddinti et al., 2015)</ref>, la configuration du modèle peut être décrite comme suit : {-1,0,1} × 6 couches ; {-3,0,3} × 7 couches.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Remerciements</head><p><rs type="programName">Ces travaux ont été financé par les projets VoicePersonae</rs> (<rs type="grantNumber">ANR-18-JSTS-0001</rs>), <rs type="projectName">DEEP-PRIVACY</rs> (<rs type="grantNumber">ANR18-CE23-0018</rs>) et <rs type="programName">programme de Recherche et d'Innovation Horizon 2020</rs> (<rs type="grantName">Marie Skłodowska-Curie grant</rs>, No <rs type="grantNumber">101007666</rs>). Ces travaux ont bénéficié d'un <rs type="programName">accès aux moyens de calcul de l'IDRIS au travers de l</rs>'allocation de ressources 2021-AD011013331 attribuée par GENCI.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_RDRFAA2">
					<idno type="grant-number">ANR-18-JSTS-0001</idno>
					<orgName type="project" subtype="full">DEEP-PRIVACY</orgName>
					<orgName type="program" subtype="full">Ces travaux ont été financé par les projets VoicePersonae</orgName>
				</org>
				<org type="funding" xml:id="_gnxyxav">
					<idno type="grant-number">ANR18-CE23-0018</idno>
					<orgName type="grant-name">Marie Skłodowska-Curie grant</orgName>
					<orgName type="program" subtype="full">programme de Recherche et d&apos;Innovation Horizon 2020</orgName>
				</org>
				<org type="funding" xml:id="_8aqefNF">
					<idno type="grant-number">101007666</idno>
					<orgName type="program" subtype="full">accès aux moyens de calcul de l&apos;IDRIS au travers de l</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Benchmarking and challenges in security and privacy for voice biometrics</title>
		<author>
			<persName><forename type="first">Bonastre J.-F</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 2021 ISCA Symposium on Security and Privacy in Speech Communication</title>
		<meeting>2021 ISCA Symposium on Security and Privacy in Speech Communication</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="52" to="56" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">K</forename><surname>Bonawitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Ivanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kreuter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.04482</idno>
		<title level="m">Practical secure aggregation for federated learning on user-held data</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">The secret sharer : Evaluating and testing unintended memorization in neural networks</title>
		<author>
			<persName><forename type="first">N</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ú</forename><surname>Erlingsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Song</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="267" to="284" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Federated acoustic modeling for automatic speech recognition</title>
		<author>
			<persName><forename type="first">X</forename><surname>Cui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">A federated approach in training acoustic models</title>
		<author>
			<persName><forename type="first">D</forename><surname>Dimitriadis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="981" to="985" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Differential privacy</title>
		<author>
			<persName><forename type="first">C</forename><surname>Dwork</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Inverting gradients-how easy is it to break privacy in federated learning ?</title>
		<author>
			<persName><forename type="first">J</forename><surname>Geiping</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Improving on-device speaker verification using federated learning with privacy</title>
		<author>
			<persName><forename type="first">F</forename><surname>Granqvist</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Training speech recognition models with federated learning : A quality/cost framework</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Guliani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">TED-LIUM 3 : twice as much data and corpus repartition for experiments on speaker adaptation</title>
		<author>
			<persName><forename type="first">F</forename><surname>Hernandez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ghannay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Tomashenko</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="198" to="208" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Federated learning for speech emotion recognition applications</title>
		<author>
			<persName><forename type="first">S</forename><surname>Latif</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="341" to="342" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Federated learning for keyword spotting</title>
		<author>
			<persName><forename type="first">D</forename><surname>Leroy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6341" to="6345" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Federated learning : Challenges, methods, and future directions</title>
		<author>
			<persName><forename type="first">Li</forename><forename type="middle">T</forename><surname>Sahu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Talwalkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="50" to="60" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Communication-efficient learning of deep networks from decentralized data</title>
		<author>
			<persName><forename type="first">B</forename><surname>Mcmahan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moore</forename><surname>Ramage D</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1273" to="1282" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Retrieving speaker information from personalized acoustic models for speech recognition</title>
		<author>
			<persName><forename type="first">S</forename><surname>Mdhaffar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A survey on security and privacy of federated learning</title>
		<author>
			<persName><forename type="first">V</forename><surname>Mothukuri</surname></persName>
		</author>
		<author>
			<persName><forename type="middle">M</forename><surname>Parizi R</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Pouriyeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Future Generation Computer Systems</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="page" from="619" to="640" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">A time delay neural network architecture for efficient modeling of long temporal contexts</title>
		<author>
			<persName><forename type="first">V</forename><surname>Peddinti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Khudanpur</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">The Kaldi speech recognition toolkit</title>
		<author>
			<persName><forename type="first">D</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ghoshal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Boulianne</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Purely sequence-trained neural networks for ASR based on lattice-free MMI</title>
		<author>
			<persName><forename type="first">D</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Peddinti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Galvez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ghahremani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Manohar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Na</forename><forename type="middle">X</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2751" to="2755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A framework for secure speech recognition</title>
		<author>
			<persName><forename type="first">P</forename><surname>Smaragdis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shashanka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1404" to="1413" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">X-vectors : Robust DNN embeddings for speaker recognition</title>
		<author>
			<persName><forename type="first">D</forename><surname>Snyder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Garcia-Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Khudanpur</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="5329" to="5333" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Privacy attacks for automatic speech recognition acoustic models in a federated learning framework</title>
		<author>
			<persName><forename type="first">N</forename><surname>Tomashenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mdhaffar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tommasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Estève</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bonastre J.-F</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
			<publisher>ICASSP</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Introducing the VoicePrivacy initiative</title>
		<author>
			<persName><forename type="first">N</forename><surname>Tomashenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">M L</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Nautsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yamagishi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Evans</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1693" to="1697" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">The VoicePrivacy 2020 Challenge : Results and findings</title>
		<author>
			<persName><forename type="first">N</forename><surname>Tomashenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Vincent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Speech &amp; Language</title>
		<imprint>
			<biblScope unit="volume">74</biblScope>
			<biblScope unit="page">101362</biblScope>
			<date type="published" when="2022">2022b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Demystifying membership inference attacks in machine learning as a service</title>
		<author>
			<persName><forename type="first">S</forename><surname>Truex</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Gursoy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><surname>Wei W</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Services Computing</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Beyond inferring class representatives : User-level privacy leakage from federated learning</title>
		<author>
			<persName><forename type="first">Wang</forename><forename type="middle">Z</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2512" to="2520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<author>
			<persName><forename type="first">Yu</forename><forename type="middle">W</forename><surname>Freiwald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tewes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Huennemeyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Kolossa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename></persName>
		</author>
		<idno type="arXiv">arXiv:2109.15108</idno>
		<title level="m">Federated learning in ASR : Not as easy as you think</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
