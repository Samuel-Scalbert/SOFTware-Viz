<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Compressed k-Nearest Neighbors Ensembles for Evolving Data Streams</title>
				<funder ref="#_46GWq4E">
					<orgName type="full">Labex DigiCosme</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Maroua</forename><surname>Bahri</surname></persName>
							<email>maroua.bahri@telecom-paris.fr</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Télécom Paris</orgName>
								<orgName type="institution" key="instit2">IP-Paris</orgName>
								<address>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Silviu</forename><surname>Maniu</surname></persName>
							<email>sil-viu.maniu@lri.fr</email>
							<affiliation key="aff2">
								<orgName type="institution" key="instit1">Université Paris-Saclay</orgName>
								<orgName type="institution" key="instit2">LRI</orgName>
								<orgName type="institution" key="instit3">CNRS</orgName>
								<address>
									<settlement>Orsay</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">Inria</orgName>
								<address>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="department">École Normale Supérieure</orgName>
								<orgName type="laboratory">ENS DI</orgName>
								<orgName type="institution" key="instit1">CNRS</orgName>
								<orgName type="institution" key="instit2">Université PSL</orgName>
								<address>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Albert</forename><surname>Bifet</surname></persName>
							<email>albert.bifet@telecom-paris.fr</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Télécom Paris</orgName>
								<orgName type="institution" key="instit2">IP-Paris</orgName>
								<address>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Waikato</orgName>
								<address>
									<settlement>Hamilton</settlement>
									<region>New Zealand</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Rodrigo</forename><surname>Fernandes De Mello</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Nikolaos</forename><surname>Tziortziotis</surname></persName>
							<affiliation key="aff6">
								<address>
									<settlement>Tradelab</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Rodrigo</forename><forename type="middle">F</forename><surname>De Mello</surname></persName>
							<affiliation key="aff5">
								<orgName type="institution">Universidade de São Paulo</orgName>
								<address>
									<country key="BR">Brazil</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Compressed k-Nearest Neighbors Ensembles for Evolving Data Streams</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">605541DF8BEA590D2073941ECD2270CF</idno>
					<note type="submission">Submitted on 5 Apr 2021</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-04-12T14:53+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>published or not. The documents may come from teaching and research institutions in France or abroad, or from public or private research centers. L'archive ouverte pluridisciplinaire</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Data streams are unbounded sequences of multidimensional observations made available along time, hence, it is impossible to maintain and process them using the main memory. In practice, mining tasks reduce time and space requirements while only processing relevant features out of those redundant streams, what corresponds to data summaries generally obtained as follows: (i) either by selecting only a subsample (sampling) of the input data along time; or (ii) by reducing/selecting data attributes via dimensionality reduction techniques, what turns out to work along features. Naturally, the choice of the technique depends on the problem being solved <ref type="bibr" target="#b12">[13]</ref>. The dimensionality reduction process inherently arises when one deals with a large number of attributes, especially when data are sparse. In this scenario, this is achieved by selecting only the most relevant features, or by transforming them into a smaller set.</p><p>A common taxonomy organizes those approaches as follows: (i) Feature selection, which consists in selecting a subset of the input features, i.e., the most relevant, non-redundant, without operating any sort of data transformation <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b26">27]</ref>; and (ii) Feature extraction, which consists in transforming the input attributes into a new set of features in some lower dimensional space <ref type="bibr" target="#b25">[26]</ref>. The main drawback of feature selection is that the simple feature removal could lead to data loss, when one inadvertently performs such selection that is even more challenging when dealing with potentially infinite and high-dimensional data streams that boost the use of computational resources, most precisely the memory and the processing time. In order to address this limitation, we apply Compressed Sensing (CS) (also referred to as Compressed Sampling) which is a feature extraction strategy that provides theoretical lower and upper bounds on pairwise data transformations. This data reduction is highly relevant in the context of data streams mining since it helps to diminish resource demands while ensuring the quality of learning (e.g. classification accuracy), addressing the evolving data streams framework requirements and avoiding more than a single data pass. This last issue would lead an algorithm either to lose information from next observations, given some sub-sampling along time, or to have an exponential amount of main memory available to save the stream, thus making sure that all observations were processed properly.</p><p>CS is a technique that has attracted a lot of attention and is based on the concept that a data compression method has to deal with redundancy while transforming and reconstructing data <ref type="bibr" target="#b14">[15]</ref>. The basic idea is to use orthogonal features, i.e. complementary features, to provably and properly represent data as well as reconstruct them from small number of samples. CS has been widely studied and used throughout different domains in the offline framework, such as image processing <ref type="bibr" target="#b30">[31]</ref>, face recognition <ref type="bibr" target="#b24">[25]</ref>, and vehicle classification <ref type="bibr" target="#b34">[35]</ref>.</p><p>Hence, we aim to find the best trade-off over three aspects: (i) the classifier accuracy: the proportion of correctly predicted instances; (ii) the memory usage: the cost of keeping the transformed data in main memory; and (iii) the overall processing time: comprising the data transformation, learning, and classification. All such aspects are strongly related, so the drastic reduction of time and space complexities would make our algorithm much faster than using all features. Of course, one should weigh the accuracy while assessing these factors.</p><p>The main contributions of this paper are the following:</p><p>• Compressed-k-Nearest Neighbors (CS-kNN): a new kNN algorithm to support data stream classification. Our main focus consists in improving its resource usage by compressing input streams using CS while theoretically guaranteeing close approximation to the accuracy that would be obtained using the original stream;</p><p>• Bagging with Compressed Sensing (CSB): an ensemble technique based on Leveraging Bagging <ref type="bibr" target="#b7">[8]</ref> where we combine several CS-kNN instances built upon different CS independent matrices; • Empirical results: we present experiments to show the abilities of our proposals in obtaining a good trade-off among the three axes (accuracy, memory and time) against several popular baselines;</p><p>The remainder of this work is organized as follows. We present the related work for dimensionality reduction and streaming classification algorithms in Section 2.2. Section 3 provides the basics of compressed sensing, followed by its application in conjunction with the kNN algorithm for evolving data streams. Section 4 discusses the experimental results performed on both synthetic and real datasets followed by the concluding remarks in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Preliminaries and Related work 2.1 Notation</head><p>In the following, we assume a data stream X is a sequence of instances x1, x2, . . . , xN , where N denotes the number of available observations so far. Each instance xi is composed of a vector of d attributes x 1 i , . . . , x d i . The dimensionality reduction comprises the process of finding some transformation function (or map) A : R d → R p , where p d, to be applied on each instance xi of X.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Related Work</head><p>There are two main different types of technique for addressing the dimensionality problem: (i) based on random projections, a.k.a. dataindependent, that are not derived from data; and (ii) data-dependent strategies derived from data to achieve the transformation itself. Among data-dependent techniques, we mention feature extraction based on component analysis where several variations have been proposed to handle evolving distributions such as IPCA <ref type="bibr" target="#b37">[38]</ref> and IKPCA <ref type="bibr" target="#b18">[19]</ref>. The most popular and straightforward technique is the Principal Component Analysis (PCA) <ref type="bibr" target="#b19">[20]</ref> which aims to find a lower-dimensional basis in which the sum of square distances between the original data and their projections is minimized, i.e. being as close as possible to zero while maximizing the variances. Random Projection (RP) is a cost-efficient alternative to PCA since it is data-independent and satisfies the Johnson-Lindenstrauss (JL) lemma <ref type="bibr" target="#b20">[21]</ref></p><formula xml:id="formula_0">: Let ∈ [0, 1], X = {x1, ..., xN } ∈ R d . Given a number p ≥ log(N/ 2 ), ∀xi, xj ∈ X there is a linear map A : R d → R p such that: (1 -) xi -xj 2 2 ≤ Axi -Axj 2 2 ≤ (1 + ) xi -xj 2 2 . (<label>1</label></formula><formula xml:id="formula_1">)</formula><p>The JL lemma (1) asserts that N instances from an Euclidean space can be projected into a lower dimensional space of O(log N/ 2 ) dimensions under which pairwise distances are preserved within a multiplicative factor 1 ± .</p><p>As detailed in the following section, random projection matrices have been used before in conjunction with compressed sensing <ref type="bibr" target="#b36">[37]</ref>. This approach applies a random linear transformation on vectors, changing their original space and leading to significant results, outperforming PCA. For example, given data in a 10 4 dimensional space, two random projections will give a perfect recovery while two PCA projections will only recover data with a probability equals to 2/10 4 . In short, RP achieves good performance with few projections whereas the PCA performance increases linearly with the number of output dimension which makes it slower <ref type="bibr" target="#b36">[37]</ref>.</p><p>Another well-known technique is the Hashing Trick (HT) <ref type="bibr" target="#b35">[36]</ref>, also known as feature hashing. It has been used to make the analysis of sparse and large data tractable in practice by mapping sparse instances or vectors into a lower feature space using a hash function. Given a list of keys that represents a set of features from the input instances, it calculates then the hash function for each key, which will ensure its mapping to a specific cell of a fixed size vector that constitutes the new compressed instance.</p><p>Those techniques could be combined to several machine learning algorithms while dealing with evolving data streams. Besides, an important issue to address in the data stream scenario is the computational efficiency of classifiers because of the potentially infinite nature of evolving data streams. Quite a number of classification algorithms for static datasets, that have already been thoroughly studied, proved to be of limited effectiveness when dealing with big data streams. Therefore, some of them have been extended to handle evolving data streams <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b2">3]</ref>, among others, Self-Adjusting Memory kNN (samkNN) <ref type="bibr" target="#b27">[28]</ref> which uses a dual-memory model to capture the drift in data streams. Hoeffding tree <ref type="bibr" target="#b13">[14]</ref>, also known as Very Fast Decision Tree (VFDT), which is an incremental, anytime decision tree induction algorithm that uses the Hoeffding bound to select the optimal splitting attributes. On the other hand, there exists ensemble learners which are popular when learning from evolving data streams because they achieve a high learning performance, such as Leveraging Bagging (LB) <ref type="bibr" target="#b7">[8]</ref> and Adaptive Random Forest (ARF) <ref type="bibr" target="#b17">[18]</ref>. However, their major drawback is the high computational demand.</p><p>3 Compressed Sensing Classification</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Basic Notions of Compressed Sensing</head><p>The goal of compressed sensing, given a sparse vector x ∈ R d , is to measure y ∈ R p and then reconstruct x, for p d, as follows:</p><formula xml:id="formula_2">y = Ax,<label>(2)</label></formula><p>where A ∈ R p×d is called a measurement (sampling or sensing) matrix. A is used to transform instances from high-dimensional space (x vectors) to a lower dimensional space (y vectors). Three concepts are crucial to the recovery of the stream with high probability <ref type="bibr" target="#b14">[15]</ref>: Sparsity: CS exploits the fact that data may be sparse and hence compressible in a concise representation. For an instance X with support supp(X) = {l : x l = 0}, we define the 0-norm X 0= |supp(X)|, so X is s-sparse if X 0≤ s. The implication of sparsity is important to remove irrelevant data without much loss.</p><p>Restricted Isometry Property (RIP): A is said to respect RIP if there exists ∈ [0, 1] such that:</p><formula xml:id="formula_3">(1 -) x 2 2 ≤ Ax 2 2 ≤ (1 + ) x 2 2 .<label>(3)</label></formula><p>This property holds for all s-sparse data x ∈ R d . CS relies on the aforementioned principles that provide, with high probability, a good data reconstruction from a limited number of incoherent and possibly noisy measurements. Mathematically, the decompression of the data that obeys the linear relation in Equation (2) consists in approximating the error by 1-norm minimization that provides a convex relaxation and when data are sparse, the recovery via 1-minimization is provably exact <ref type="bibr" target="#b33">[34]</ref>:</p><formula xml:id="formula_4">arg min x∈R d x 1 s.t. y = Ax.</formula><p>The goal is to find an efficient representation for each instance such that the sum of their reconstruction errors is minimized. The RIP guarantees the proper computation of the above-mentioned recovery problem <ref type="bibr" target="#b9">[10]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Construction of Sampling (Sensing) Matrices</head><p>The RIP is both a necessary and sufficient condition for an efficient data recovery. Randomization is a key ingredient in the construction of most of RIP matrices used in the CS transformation process <ref type="bibr" target="#b10">[11]</ref>.</p><p>In what follows, we cite examples of CS matrices:</p><p>• Fourier matrix is obtained by applying Fourier transform on data and thereafter selecting uniformly at random p rows from a d dimensional Fourier matrix; • Random Gaussian matrix is generated randomly from a Gaussian distribution having independent and identically distributed (i.i.d) entries with zero mean and variance one: Ai,j ∼ N (0, 1); • Random Bernoulli matrix has entries which are randomly sampled from a Bernoulli distribution with equal probability:</p><formula xml:id="formula_5">Ai,j ∈ {1/ √ p, -1/ √ p}.</formula><p>For the data-independent random matrices, it has been proved that any matrix A satisfying the JL lemma (1) will also satisfy the RIP in CS with high probability if p = O(s log(d)) <ref type="bibr" target="#b0">[1]</ref>. A comparison of the results obtained with these matrices and an explanation of the choice of the matrix used in this work are provided in the following.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Compressed Classification Using kNN</head><p>The k-Nearest Neighbors (kNN) is one of the most often used algorithms that has been adapted to the stream setting <ref type="bibr" target="#b31">[32]</ref>. It does not require any work during training but its offline version uses the entire dataset to predict class labels for test instances. The challenge with adapting kNN to evolving data streams lies in the impracticality of storing the entire stream for prediction. To tackle this issue, an adaptive solution fits new instances once they arrive into a fixed-length window and merge them with the closest ones already in memory.</p><p>The prediction of the class label for an instance is therefore made by taking the majority vote of its nearest neighbors, using a defined distance metric. Yet, the search for the nearest neighbors is still costly in terms of time and memory <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b2">3]</ref>. Thus, for high-dimensional data, a dimension reduction is imperative to avoid the curse of dimensionality which may increase the use of computational resources. The main idea to mitigate this drawback and improve kNN's performance is to use a simple strategy with relevant properties such as CS. We focus on the analysis of an infinite stream of instances xi ∈ R d from which we wish to construct a low dimensional space R p , where p d. We assume that instances are s-sparse in some basis, so we can use CS with a RIP matrix and work in a lower dimension of O(s log(d)). It is important to perform such reduction because it is related to the number of dimensions and independent of the stream Algorithm 1 Compressed-kNN algorithm. Symbols: X = {x1, x2, . . .}: stream; C = {c1, c2, . . .}: set of labels; w: window; k: the neighborhood size; p: the target dimension; S: subset of w. for all yj ∈ w do ∀ j = i 6:</p><p>compute Dy j (yi) Equation ( <ref type="formula" target="#formula_9">5</ref>)</p><formula xml:id="formula_6">7: c ← max c∈C D S,k (yi) Equation<label>(6) 8:</label></formula><p>w ← yi maintain the compressed xi in w size, making it useful in applications for data streams where the size is unknown. This transformation can lead to information loss, except if the sensing matrix respects the RIP, then with high probability, the information loss is minimal, and the original signal can be recovered. Figure <ref type="figure" target="#fig_0">1</ref> presents the main flow of the proposed approach combining the simplicity of kNN and the strong properties of CS to obtain the compressed kNN classifier, called CS-kNN in the following. Fundamentally, CS is composed of two phases: (i) the compression phase, where the data are projected onto a low-dimensional space; and (ii) the decompression phase, where the data are recovered. Nevertheless, the compressed nature of CS makes the paradigm a better fit to classification than the reconstruction. In this paper, we are only concerned with the first stage, so the extracted features from the high-dimensional space are fed to kNN classifier which predicts target class labels. This does not prevent, however, the guarantees over the recovery to hold true. Algorithm 1 shows the pseudo-code of the CS-kNN. We apply CS on each instance xi of the stream (lines 3-4), then we apply kNN by computing the distance of each yj in w with yi (line 6) and thus report the most frequent class label to yi (line 7). Finally, we feed the compressed version yi to w (line 8).</p><p>In this work, we opt to make kNN more efficient in terms of memory and speed taking into account the online aspect of evolving data streams. Our approach consists of the CS application on high-dimensional data obtained by compressing every new arrived instance via solving Equation <ref type="bibr" target="#b1">(2)</ref>. In order that our approach works well, we need to use an effective sampling matrix that gives sufficiently good (or with minor loss in) accuracy and potentially leads to computational savings. In recent work <ref type="bibr" target="#b1">[2]</ref>, authors reviewed different sampling matrices performance where the experiment results show that Gaussian random matrices perform nicely.</p><p>To motivate our choice in the following, we perform experiments to assess different sampling matrices. For this, we first generate synthetic random Bernoulli and Gaussian matrices, and we also construct the Fourier matrices on some datasets (see the description in Table <ref type="table" target="#tab_2">2</ref>). Thus, for each dataset, we build projections for 5 different settings of the target dimension {10, 20, . . . , 50}. Table <ref type="table" target="#tab_0">1</ref> shows the results for kNN (with k = 5) along with the overall average over the different targeting dimensions for each matrix. We notice that with the random Bernoulli matrix, kNN performs worse on average, confirming previous studies <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b29">30]</ref>, compared to the Random Gaussian and Fourier matrices which are very close to the kNN, in terms of accuracy, using the whole data without projections. Nevertheless, Fourier transformation relies on data, i.e., it requires the presence of all instances which is unrealistic in the context of data streams. In <ref type="bibr" target="#b16">[17]</ref>, authors proposed a recursive scheme for using Fourier matrices with data streams which constructs successive windows and uses the measurement in the previous window to obtain the next one. However, this approach is expensive in terms of memory since it keeps data on windows and it is still not as accurate as using a Gaussian matrix. In this work, we want to use a data-independent matrix to ensure fast processing. Following these experiments, we focus on the Gaussian matrix which not only provides good accuracy but satisfies with high probability the RIP and therefore allows recovery of instances <ref type="bibr" target="#b3">[4]</ref>. The matrix A in Equation ( <ref type="formula" target="#formula_2">2</ref>) satisfies the RIP so x can be recovered with minimum error from y, i.e., y preserves the important information that x contains.</p><p>First, we need to bound the probability of error related to the estimated instance and its expected value using the Hoeffding inequality. Given xi, ∀i ∈ [1, N ], where x j i are bounded by the interval [aj, bj], then for any , the probability of error is upper bounded as follows:</p><formula xml:id="formula_7">P (|xi -xi|≥ ) ≤ 2 exp - 2 2 d l=1 (b l -a l ) 2 , (<label>4</label></formula><formula xml:id="formula_8">)</formula><p>where xi is the reconstructed instance and N can simply be the size of a "sliding window" over which the error guarantee is provided.</p><p>To make the link between the kNN and the use of RIP matrices, we point out that the JL lemma <ref type="bibr" target="#b20">[21]</ref> asserts that a random projection preserves the distance between pairs of instances up to 1 ± guarantee with high probability. Similarly, the kNN algorithm is based on a function that measures the distances between instances to predict. Thus, we aim to provide theoretical guarantees on the connection between kNN and stream recovery by showing that the CS transformation using Gaussian matrix preserves the distance function and also approximately maintains the shape -in the neighborhood sense-as the original space, based on the concept of persistent homology.</p><p>Persistent homology <ref type="bibr" target="#b15">[16]</ref> is one of the main tools used to extract information from topological features of a space at different scales for an effective shape description. Given a dataset in some metric space, computing the persistent homology naturally involves nearest neighbors since we are constructing the topological space by building open balls around instances. In this regards, it has been shown in <ref type="bibr" target="#b32">[33]</ref> that the persistent homology of a distance such as in the JL lemma ( <ref type="formula" target="#formula_0">1</ref>) is (1 ± )-preserved under random projection into O(log N/ 2 ) dimensions. The basic idea in <ref type="bibr" target="#b32">[33]</ref> consists in preserving the radius of the minimum enclosing open ball of data up to a factor of (1 ± 4 ).</p><p>In the following, we deal with the Euclidean distance function in both kNN and data reconstruction guarantees. Given a window w, the distance between instances xi and xj is defined as follows:</p><formula xml:id="formula_9">Dx j (xi) = xi -xj 2 .<label>(5)</label></formula><p>Similarly, the k-nearest neighbors distance is defined as follows:</p><formula xml:id="formula_10">D w,k (xi) = min ( w k ),xj∈w k j=1 Dx j (xi),<label>(6)</label></formula><p>where w k denotes the subset of w of size k, i.e., the k-nearest neighbors to the instance xi in w.</p><p>CS random matrices satisfy RIP, so we need to show that our matrix preserves the neighborhood for kNN without significant loss through the JL lemma (1). This would allow us to conserve distances among instances and finally ensure distance-preservation among all neighbors. In <ref type="bibr" target="#b3">[4]</ref>, authors have indeed established a connection between the expressions in (1) and ( <ref type="formula" target="#formula_3">3</ref>), and proved that the JL lemma implies the RIP for s-sparse data within an -multiplicative factor. A converse result has been proved in <ref type="bibr" target="#b23">[24]</ref> wherein matrices having the RIP respect the JL lemma, i.e., preserve the distances in the transformation between any pairs of instances up to a (1 ± )-factor with target dimension in O(s log(d)).</p><p>Application to persistent homology. Now we want to prove, based on the aforementioned result <ref type="bibr" target="#b23">[24]</ref> derived from Equation (3), that CS preserves as well the distances between all instances up to (1 ± )-error and not only distances between pairs of instances. In other words, we prove that, given a RIP matrix, the resulting compressed instances preserve the kNN neighborhood of the data.</p><p>Theorem 1 Given a set of instances in a sliding window w = {xi}, i ∈ [1, N ] and ∈ [0, 1], if there exists a transformation matrix A : R d → R p having the RIP, such that p = O(s log(d)), where s is the sparsity of data, then ∀xi ∈ w:</p><formula xml:id="formula_11">(1 -)D 2 w,k (x) ≤ D 2 w,k (Ax) ≤ (1 + )D 2 w,k (x).<label>(7)</label></formula><p>Proof. Assume that x1, x2, • • • , x k are the k-nearest neighbors to an instance t ∈ w. We have:</p><p>(</p><formula xml:id="formula_12">-) t -xi 2 ≤ At -Axi 2 ≤ (1 + ) t -xi 2 .<label>1</label></formula><p>By summing these inequalities k times, we obtain:</p><formula xml:id="formula_13">(1 -) k i=1 t -xi 2 ≤ k i=1 At -Axi 2 ≤ (1 + ) k i=1 t -xi 2 .</formula><p>The distance of At to its k-nearest neighbors in w is minimal, so we have the lower bound as follows:</p><formula xml:id="formula_14">D 2 w,k (At) ≤ k i=1 At -Axi 2 .</formula><p>For the upper bound, we have:</p><formula xml:id="formula_15">D 2 w,k (At) ≤ k i=1 At -Axi 2 ≤ (1 + ) k i=1 t -xi 2 , D 2 w,k (At) ≤ k i=1 At -Axi 2 ≤ (1 + )D 2 w,k (t).</formula><p>Assume that Az1, Az2, • • • , Az k are the k-nearest neighbors to At, where z1, z2, • • • , z k ∈ w. So we have:</p><formula xml:id="formula_16">(1 -) k i=1 y -zi ≤ k i=1 At -Azi 2 = D 2 w,k (At).</formula><p>Given the fact that x1, x2, • • • , x k are the k-nearest neighbors to t, we found the lower bound as follows:</p><formula xml:id="formula_17">D 2 w,k (t) = k i=1 t -xi 2 ≤ k i=1 t -zi 2 .</formula><p>This completes the proof. So far, we demonstrated that the CS-kNN linked to geometrical properties by achieving homology preservation while being scaleinvariant in terms of distances, captures the neighborhood up to some ( )-divergence between the original and the compressed instances.</p><p>Bagging CS-kNN. Another application of this framework is to use the CS in an ensemble method which applies CS-kNN as a base learner under Leveraging Bagging (LB) <ref type="bibr" target="#b7">[8]</ref>, denoted CS-kNN LB .</p><p>To increase the diversity inside this LB ensemble, in addition to sampling with the Poisson distribution (λ), where λ ≥ 1, we can use several random matrices by generating a different CS matrix for each ensemble member (CS-kNN) instead of using only one for all the learners (the case of CS-kNN LB ). We refer to the aforementioned approach in the following as Compressed Sensing Bagging Ensemble (CSB), (CSB-kNN). The properties assessing the neighborhood preservation for CS-kNN hold also for the CSB-kNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Results</head><p>We conduct extensive experiments to evaluate the performance of our proposals. We are interested in three main results: the accuracy, the memory (megabytes), and the time (seconds).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Data</head><p>We use 4 synthetic and 5 real-world datasets from different scenarios.  Tweets. Tweets was created using the text data generator provided by MOA <ref type="bibr" target="#b6">[7]</ref>. It simulates sentiment analysis on tweets, where messages can be classified into two categories depending on whether they convey positive or negative feelings. Tweets1, Tweets2, and Tweets3 produce instances of 500, 1, 000, and 1, 500 attributes, respectively.</p><p>RBF. The Radial Basis Function generator creates centroids at random positions, and each one has a standard deviation, a weight and a class label.</p><p>CNAE. CNAE is the national classification of economic activities dataset, initially used in <ref type="bibr" target="#b11">[12]</ref>. Instances represent descriptions of Brazilian companies categorized into 9 classes. The original texts were pre-processed to obtain the current highly sparse dataset.</p><p>Enron. The Enron corpus is a cleaned version of a large set of emails that was made public during the legal investigation concerning the Enron corporation <ref type="bibr" target="#b22">[23]</ref>.</p><p>IMDB. IMDB movie reviews dataset was first proposed for sentiment analysis <ref type="bibr" target="#b28">[29]</ref>, where reviews have been pre-processed, and each review is encoded as a sequence of word indexes (integers).</p><p>Spam. The spam corpus is the result of a text mining on an online news dissemination system which intends on creating an incremental filtering of e-mails classifying them as spam or not <ref type="bibr" target="#b21">[22]</ref>. Each attribute represents the presence of a word in the instance (an e-mail).</p><p>Covt. The forest covertype dataset obtained from US Forest Service Region 2 Resource Information System (RIS) data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Results</head><p>The experiments were conducted on a machine equipped with an Intel Core i5 CPU and 4GB of main memory. They were implemented and evaluated in Java by extending the MOA framework <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7]</ref>. We used the online evaluation setting for Test-Then-Train method, where each instance is used first for testing and then for training.</p><p>Results with non-ensemble methods. We compare the performance of our proposed classifier, CS-kNN, to commonly-used techniques in the literature; self-adjusting memory kNN (CS-samkNN) with CS, kNN using hashing trick (HT-kNN), principal component analysis (PCA-kNN), and the standard kNN without projection as well (using the entire data). The streaming kNN has two principal parameters: the number of neighbors k and the window size w. Table <ref type="table" target="#tab_3">3</ref> presents the results for distinct sizes of w and shows that; for shorter windows (w = 100), the accuracy degrades, while for bigger windows the accuracy slightly increases. On the other hand, the processing time and memory usage increase as well. Therefore this parameter selection implies an accuracy-time-memory trade-off.</p><p>The following experiments are performed with w = 1000 for kNN, because using a greater window size yields indeed to a better accuracy but the resource consumption is more significant.</p><p>Tables <ref type="table" target="#tab_4">4,</ref><ref type="table" target="#tab_5">5</ref>, and 6 report the final accuracies, memory consumption, and speed of the classification models in a 40-dimensional space after the projections based on two setups of k = 5, 11. We choose 40 dimensions because we noticed that, starting from this size of space, improvements are statistically insignificant as showed in Figure <ref type="figure" target="#fig_3">3</ref>. The latter illustrates a detailed comparison with five different values of output dimension (10, 20, • • • , 50) on Tweet2 and Enron datasets.</p><p>Results with ensemble methods. We compare the proposed LB with CS-kNN as a base learner (CS-kNN LB ) and the CSB-kNN with a different CS matrix for each learner, both using 10 learners (the size of ensemble) and k = 5, against popular ensemble methods such as the adaptive random forest (ARF) <ref type="bibr" target="#b17">[18]</ref> and leveraging bagging using Hoeffding tree <ref type="bibr" target="#b13">[14]</ref> as base learner (HTree LB ), with 30 and 10 ensemble members, respectively. Tables 7, 8 and 9 display the performance of the ensembles. In this evaluation, each of the ensemble member uses the same CS matrix to perform the reduction into 40 dimensions, except CSB-kNN which sets up a different matrix for each member in attempt to assess the ensemble diversity impact.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Discussion</head><p>Our proposed CS-kNN has more accurate results (Table <ref type="table" target="#tab_4">4</ref>) than HT-kNN for all datasets and it is slightly outperformed by CS-samkNN, the standard kNN (without projection) and PCA-kNN; this quite a   natural result since kNN processes the whole data stream and PCA-kNN formally tries to find a lower-dimensional space under which the sum of square distances -representing the error, between the original data and its projection -is minimized. CS-kNN is moderately less accurate than CS-samkNN for some datasets containing drifts, because the latter deals with different types of concept drift which makes it stronger facing changes in data distributions.</p><p>To assess the benefits in terms of computational resources -where small values are desirable -Tables <ref type="table" target="#tab_5">5</ref> and<ref type="table" target="#tab_6">6</ref> point out the improvements of CS-kNN in terms of memory and time against CS-samkNN, PCA-kNN, and kNN which are significant enough to justify relatively minor losses in accuracy. In fact, CS-samkNN maintains models for current and past concepts which makes it memory inefficient. PCA-kNN performs worse, in terms of resource usage, than RP since it incrementally stores and updates the eigenvectors and eigenvalues, confirming previous studies <ref type="bibr" target="#b36">[37]</ref>. Our proposed approach is also faster than HT-kNN, although they have similar memory behavior, because both are based on RP and do not rely on data.</p><p>For some datasets such as Spam, CS-kNN outperforms kNN (using the whole data) simply because finding relevant combinations of existing features and presenting them in a different space can help supervised models to improve accuracy. Even if data are not sparse, CS surprisingly performs projections on suitable dimensions.</p><p>Figures <ref type="figure" target="#fig_3">3a</ref> and<ref type="figure" target="#fig_3">3d</ref> depict the typical trade-off for accuracy: a small feature space cannot properly represent data, therefore it can significantly degrade the accuracy; whereas a higher dimensional space (e.g., 50) increases the accuracy and makes it closer to the results with kNN. We also notice the stability of our CS-kNN, i.e., the accuracy is linearly boosted with the target space size and converges to the accuracy of kNN. On the other hand, CS-samkNN, HT-kNN and PCA-kNN have different behaviors, clearly illustrated in Figure <ref type="figure" target="#fig_3">3a</ref>; this results deduce that, in practice, it may be hard to fix a proper space size. We show that kNN, PCA-kNN and HT-kNN are outperformed in terms of processing time (Figures <ref type="figure" target="#fig_3">3b</ref> and<ref type="figure" target="#fig_3">3e</ref>) and that CS-kNN requires also less memory compared to these baselines. For instance, with Tweet2 and Enron in Figures <ref type="figure" target="#fig_3">3c</ref> and<ref type="figure" target="#fig_3">3f</ref> respectively, we observe large gains compared to kNN, PCA-kNN, and CS-samkNN, albeit our proposal has the same memory usage as HT-kNN because both do not rely on data. We also observe that the behavior of memory usage is correlated to the running time trends, i.e., when the memory usage increases, the processing time also increases accordingly.</p><p>Tables 7, 8 and 9 show, using only 10 learners, CSB-kNN performs better than the reputed CS-ARF <ref type="bibr" target="#b17">[18]</ref> using 30 learners (trees) on most of the datasets. We noticed that with CSB-kNN, when the features set is large (e.g. Spam), the memory usage is relatively high. On the other hand, for large datasets (e.g. Tweets), CS-ARF and CS-HTree LB require more memory whereas our approaches use less, making them useful for the stream setting. CS-kNN LB is the most memory efficient and even proving competitive with CS-HTree LB   and CS-ARF. However, this is at the price of being slower. Also, computational resources of CSB-kNN with different CS matrices increase considerably to allow higher accuracy and diversity (enabling the ensemble to generalize well).</p><p>In conclusion, our CSB-kNN ensemble method has good overall performance compared to other methods. We showed that our proposal can be used to classify accurately data streams with a large number of attributes using a relatively small number of base learners, in contrast with CS-ARF the number of base trees can considerably affect the classification performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>In this work, we presented a scheme to enable the k-nearest neighbors algorithm to be efficient with evolving high-dimensional data streams in terms of classification performance and computational resources (memory and time) after space transformations provided by compressed sensing given its ability to ensure theoretical lower and upper bounds on pairwise data transformations. Our first contribution is the mix of two main ingredients: compressed sensing and kNN, thus resulting in the CS-kNN algorithm designed to work on evolving data streams while operating on a reduced feature space.</p><p>We proposed also an ensemble method, CSB-kNN, that uses CS-kNN as base learner under the Leveraging Bagging, where each ensemble member has a different CS matrix to help increasing the overall accuracy. We showed theoretically that CS-kNN using Gaussian matrices, the neighborhood distance used in kNN is preserved up to some 1 ± -factor. The key idea is to show that squared kNN distances, in the original data, are too within the same factor. Consequently, our CS-kNN algorithm also conserves such distances.</p><p>We evaluated the proposed algorithms via extensive experiments using synthetic and real-world datasets with different parameters. Results show the potential of the CS-kNN and CSB-kNN algorithms to obtain close approximations to what it would be obtained using the input instances from data streams. We compared our proposals against well-known approaches from the literature, showing improvements along 3 dimensions: accuracy, memory usage, and time.</p><p>In future work, we intend to investigate how to optimize the processing time of CSB-kNN algorithm and provide guarantees along the number of output dimensions. Once we fix the latter, we could efficiently project the data streams knowing the input dimensions.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Compressed kNN Scheme.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>1 :</head><label>1</label><figDesc>function CS-kNN(X, w, k, p)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Sorted plots of accuracy, time and memory over different output dimensions. a Accuracy Tweet 2 . b Time Tweet 2 . c Memory Tweet 2 . d Accuracy Enron. e Time Enron. f Memory Enron.</figDesc><graphic coords="8,61.88,216.71,142.26,113.31" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Accuracy (%) comparison of compressed sensing with Bernoulli, Gaussian, Fourier matrices, and the entire dataset.</figDesc><table><row><cell>Dataset</cell><cell>Bernoulli</cell><cell>Gaussian</cell><cell>Fourier</cell><cell>Whole data</cell></row><row><cell>Tweet 1</cell><cell>64.78</cell><cell>77.89</cell><cell>77.90</cell><cell>79.80</cell></row><row><cell>Tweet 2</cell><cell>64.59</cell><cell>77.17</cell><cell>79.53</cell><cell>79.20</cell></row><row><cell>Tweet 3</cell><cell>60.24</cell><cell>75.59</cell><cell>77.13</cell><cell>78.86</cell></row><row><cell>CNAE</cell><cell>27.04</cell><cell>64.59</cell><cell>58.49</cell><cell>73.33</cell></row><row><cell>Enron</cell><cell>95.88</cell><cell>95.97</cell><cell>95.91</cell><cell>96.18</cell></row><row><cell>Overall ∅</cell><cell>62.51</cell><cell>78.24</cell><cell>77.79</cell><cell>81.58</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Table 2 presents a short description of each dataset, and further details are provided in what follows.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Overview of the datasets</figDesc><table><row><cell>Dataset</cell><cell cols="4">#Instances #Attributes #Classes Type</cell></row><row><cell>Tweets 1</cell><cell>1,000,000</cell><cell>500</cell><cell>2</cell><cell>Synthetic</cell></row><row><cell>Tweets 2</cell><cell>1,000,000</cell><cell>1,000</cell><cell>2</cell><cell>Synthetic</cell></row><row><cell>Tweets 3</cell><cell>1,000,000</cell><cell>1,500</cell><cell>2</cell><cell>Synthetic</cell></row><row><cell>RBF</cell><cell>1,000,000</cell><cell>200</cell><cell>10</cell><cell>Synthetic</cell></row><row><cell>CNAE</cell><cell>1,080</cell><cell>856</cell><cell>9</cell><cell>Real</cell></row><row><cell>Enron</cell><cell>1,702</cell><cell>1,000</cell><cell>2</cell><cell>Real</cell></row><row><cell>IMDB</cell><cell>120,919</cell><cell>1,001</cell><cell>2</cell><cell>Real</cell></row><row><cell>Spam</cell><cell>9,324</cell><cell>39,916</cell><cell>2</cell><cell>Real</cell></row><row><cell>Covt</cell><cell>581,012</cell><cell>54</cell><cell>7</cell><cell>Real</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Performance of kNN with different window sizes.</figDesc><table><row><cell></cell><cell cols="2">Accuracy (%)</cell><cell></cell></row><row><cell></cell><cell>w=100</cell><cell>w=1000</cell><cell>w=5000</cell></row><row><cell>Overall ∅</cell><cell>75.48</cell><cell>80.33</cell><cell>82.44</cell></row><row><cell></cell><cell cols="2">Time (sec)</cell><cell></cell></row><row><cell></cell><cell>w=100</cell><cell>w=1000</cell><cell>w=5000</cell></row><row><cell>Overall ∅</cell><cell>537.76</cell><cell>6592.72</cell><cell>20028.01</cell></row><row><cell></cell><cell cols="2">Memory (MB)</cell><cell></cell></row><row><cell></cell><cell>w=100</cell><cell>w=1000</cell><cell>w=5000</cell></row><row><cell>Overall ∅</cell><cell>46.85</cell><cell>269.65</cell><cell>2000</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Accuracy (%) comparison of CS-kNN, CS-samkNN, HT-kNN, PCA-kNN, and kNN over the whole dataset.</figDesc><table><row><cell>Dataset</cell><cell cols="2">CS-kNN k = 5 k = 11</cell><cell cols="2">CS-samkNN k = 5 k = 11</cell><cell cols="2">HT-kNN k = 5 k = 11</cell><cell cols="2">PCA-kNN k = 5 k = 11</cell><cell>k = 5</cell><cell>kNN k = 11</cell></row><row><cell>Tweet1</cell><cell>78.82</cell><cell>78.88</cell><cell>76.02</cell><cell>74.31</cell><cell>73.77</cell><cell>73.14</cell><cell>80.43</cell><cell>79.43</cell><cell>79.80</cell><cell>78.17</cell></row><row><cell>Tweet2</cell><cell>78.13</cell><cell>78.36</cell><cell>75.74</cell><cell>74.13</cell><cell>73.02</cell><cell>72.61</cell><cell>80.06</cell><cell>78.89</cell><cell>79.20</cell><cell>77.74</cell></row><row><cell>Tweet3</cell><cell>76.75</cell><cell>76.16</cell><cell>73.03</cell><cell>72.56</cell><cell>72.40</cell><cell>72.36</cell><cell>81.93</cell><cell>82.38</cell><cell>78.86</cell><cell>77.73</cell></row><row><cell>RBF</cell><cell>98.90</cell><cell>97.31</cell><cell>99.87</cell><cell>99.78</cell><cell>19.20</cell><cell>19.20</cell><cell>99.00</cell><cell>97.86</cell><cell>98.89</cell><cell>97.33</cell></row><row><cell>CNAE</cell><cell>70.00</cell><cell>68.70</cell><cell>73.77</cell><cell>72.19</cell><cell>65.00</cell><cell>65.28</cell><cell>75.83</cell><cell>72.08</cell><cell>73.33</cell><cell>71.48</cell></row><row><cell>Enron</cell><cell>96.02</cell><cell>95.65</cell><cell>96.23</cell><cell>96.06</cell><cell>95.76</cell><cell>95.48</cell><cell>94.59</cell><cell>93.18</cell><cell>96.18</cell><cell>96.00</cell></row><row><cell>IMDB</cell><cell>69.86</cell><cell>72.32</cell><cell>74.29</cell><cell>74.53</cell><cell>69.65</cell><cell>72.03</cell><cell>70.57</cell><cell>72.81</cell><cell>70.94</cell><cell>72.51</cell></row><row><cell>Spam</cell><cell>85.39</cell><cell>81.01</cell><cell>91.34</cell><cell>90.48</cell><cell>83.82</cell><cell>80.63</cell><cell>96.00</cell><cell>94.66</cell><cell>81.17</cell><cell>77.32</cell></row><row><cell>Covt</cell><cell>91.36</cell><cell>89.92</cell><cell>90.47</cell><cell>87.71</cell><cell>77.18</cell><cell>76.59</cell><cell>91.55</cell><cell>90.16</cell><cell>91.67</cell><cell>90.30</cell></row><row><cell>Overall ∅</cell><cell>82.80</cell><cell>82.04</cell><cell>83.42</cell><cell>82.42</cell><cell>69.98</cell><cell>69.70</cell><cell>85.55</cell><cell>84.61</cell><cell>83.34</cell><cell>82.06</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Time (sec) comparison of CS-kNN, CS-samkNN, HT-kNN, PCA-kNN, and kNN over the whole dataset.</figDesc><table><row><cell>Dataset</cell><cell cols="2">CS-kNN k = 5 k = 11</cell><cell cols="2">CS-samkNN k = 5 k = 11</cell><cell cols="2">HT-kNN k = 5 k = 11</cell><cell cols="2">PCA-kNN k = 5 k = 11</cell><cell>kNN k = 5</cell><cell>k = 11</cell></row><row><cell>Tweet1</cell><cell>62.55</cell><cell>91.06</cell><cell>41.81</cell><cell>59.20</cell><cell>93.24</cell><cell>99.78</cell><cell>622.65</cell><cell>629.60</cell><cell>1198.78</cell><cell>1432.47</cell></row><row><cell>Tweet2</cell><cell cols="2">107.48 112.97</cell><cell>74.92</cell><cell cols="3">99.77 120.83 127.95</cell><cell>705.71</cell><cell>712.84</cell><cell>2029.82</cell><cell>2502.92</cell></row><row><cell>Tweet3</cell><cell cols="2">126.73 142.95</cell><cell cols="4">83.01 101.43 154.22 165.11</cell><cell>988.25</cell><cell>995.93</cell><cell>2864.55</cell><cell>3643.26</cell></row><row><cell>RBF</cell><cell>59.47</cell><cell>80.52</cell><cell>60.08</cell><cell cols="3">77.00 168.31 169.88</cell><cell>243.26</cell><cell>258.12</cell><cell>284.34</cell><cell>439.23</cell></row><row><cell>CNAE</cell><cell>0.87</cell><cell>0.92</cell><cell>0.56</cell><cell>0.63</cell><cell>0.95</cell><cell>1.02</cell><cell>3.97</cell><cell>4.14</cell><cell>32.19</cell><cell>35.04</cell></row><row><cell>Enron</cell><cell>1.58</cell><cell>1.63</cell><cell>1.31</cell><cell>1.57</cell><cell>1.81</cell><cell>1.90</cell><cell>7.21</cell><cell>7.28</cell><cell>86.08</cell><cell>91.99</cell></row><row><cell>IMDB</cell><cell cols="2">95.62 120.66</cell><cell cols="4">80.82 103.51 125.62 129.27</cell><cell>1686.88</cell><cell>1692.28</cell><cell>7892.96</cell><cell>8217.06</cell></row><row><cell>Spam</cell><cell cols="10">159.92 183.19 197.22 208.94 194.07 216.37 11329.91 14820.26 34231.45 35031.76</cell></row><row><cell>Covt</cell><cell>30.94</cell><cell>51.08</cell><cell>39.25</cell><cell>45.55</cell><cell>88.17</cell><cell>90.85</cell><cell>161.00</cell><cell>164.16</cell><cell>252.69</cell><cell>268.28</cell></row><row><cell>Overall ∅</cell><cell>71.68</cell><cell>87.22</cell><cell>64.33</cell><cell cols="3">75.29 105.25 111.42</cell><cell>1749.87</cell><cell>2142.73</cell><cell>5430.32</cell><cell>5740.22</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>Memory (MB) comparison of CS-kNN, CS-samkNN, HT-kNN, PCA-kNN, and kNN over the whole dataset.</figDesc><table><row><cell>Dataset</cell><cell cols="5">CS-kNN CS-samkNN HT-kNN PCA-kNN kNN</cell></row><row><cell>Tweet 1</cell><cell>2.52</cell><cell>8.86</cell><cell>2.52</cell><cell>3.03</cell><cell>34.64</cell></row><row><cell>Tweet 2</cell><cell>2.52</cell><cell>10.48</cell><cell>2.52</cell><cell>5.97</cell><cell>70.97</cell></row><row><cell>Tweet 3</cell><cell>2.52</cell><cell>10.52</cell><cell>2.52</cell><cell cols="2">8.84 103.19</cell></row><row><cell>RBF</cell><cell>2.52</cell><cell>10.31</cell><cell>2.52</cell><cell>8.86</cell><cell>13.18</cell></row><row><cell>CNAE</cell><cell>2.52</cell><cell>10.22</cell><cell>2.52</cell><cell>3.09</cell><cell>61.37</cell></row><row><cell>Enron</cell><cell>2.52</cell><cell>9.84</cell><cell>2.52</cell><cell>3.51</cell><cell>70.60</cell></row><row><cell>IMDB</cell><cell>2.52</cell><cell>10.28</cell><cell>2.52</cell><cell>8.81</cell><cell>70.65</cell></row><row><cell>Spam</cell><cell>2.52</cell><cell>10.57</cell><cell>2.52</cell><cell cols="2">245.22 1476.11</cell></row><row><cell>Covt</cell><cell>2.52</cell><cell>9.96</cell><cell>2.52</cell><cell>3.02</cell><cell>3.47</cell></row><row><cell>Overall ∅</cell><cell>2.52</cell><cell>10,12</cell><cell>2.52</cell><cell cols="2">32.26 211.57</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 :</head><label>7</label><figDesc>Accuracy</figDesc><table><row><cell>Dataset</cell><cell>CS-kNN LB</cell><cell cols="2">CSB-kNN CS-HTree LB</cell><cell>CS-ARF</cell></row><row><cell>Tweets 1</cell><cell>78.94</cell><cell>81.80</cell><cell>81.35</cell><cell>81.53</cell></row><row><cell>Tweets 2</cell><cell>78.24</cell><cell>81.28</cell><cell>80.39</cell><cell>80.75</cell></row><row><cell>Tweets 3</cell><cell>76.06</cell><cell>80.40</cell><cell>78.59</cell><cell>79.54</cell></row><row><cell>RBF</cell><cell>98.90</cell><cell>99.68</cell><cell>99.24</cell><cell>99.25</cell></row><row><cell>CNAE</cell><cell>71.64</cell><cell>81.48</cell><cell>65.70</cell><cell>62.55</cell></row><row><cell>Enron</cell><cell>95.94</cell><cell>96.00</cell><cell>96.17</cell><cell>95.88</cell></row><row><cell>IMDB</cell><cell>70.02</cell><cell>74.27</cell><cell>74.80</cell><cell>74.88</cell></row><row><cell>Spam</cell><cell>86.08</cell><cell>90.28</cell><cell>90.02</cell><cell>89.04</cell></row><row><cell>Covt</cell><cell>91.09</cell><cell>91.76</cell><cell>88.48</cell><cell>88.01</cell></row><row><cell>Overall ∅</cell><cell>82.99</cell><cell>86.33</cell><cell>83.86</cell><cell>83.49</cell></row></table><note><p>(%) comparison of CS-kNN LB , CSB-kNN, CS-HTree LB , and CS-ARF</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 8 :</head><label>8</label><figDesc>Time (SEC) comparison of CS-kNN LB , CSB-kNN, CS-HTree LB ,</figDesc><table><row><cell>and CS-ARF</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Dataset</cell><cell>CS-kNN LB</cell><cell cols="2">CSB-kNN CS-HTree LB</cell><cell>CS-ARF</cell></row><row><cell>Tweets 1</cell><cell>1130.44</cell><cell>1251.77</cell><cell>82.18</cell><cell>170.52</cell></row><row><cell>Tweets 2</cell><cell>1449.44</cell><cell>1526.30</cell><cell>105.87</cell><cell>212.69</cell></row><row><cell>Tweets 3</cell><cell>1668.26</cell><cell>1825.41</cell><cell>127.19</cell><cell>239.97</cell></row><row><cell>RBF</cell><cell>735.21</cell><cell>772.62</cell><cell>90.22</cell><cell>223.08</cell></row><row><cell>CNAE</cell><cell>8.99</cell><cell>11.02</cell><cell>1.80</cell><cell>4.66</cell></row><row><cell>Enron</cell><cell>20.07</cell><cell>21.92</cell><cell>2.11</cell><cell>3.78</cell></row><row><cell>IMDB</cell><cell>1552.81</cell><cell>1649.94</cell><cell>90.17</cell><cell>174.54</cell></row><row><cell>Spam</cell><cell>359.07</cell><cell>2194.93</cell><cell>218.16</cell><cell>270.15</cell></row><row><cell>Covt</cell><cell>612.62</cell><cell>694.02</cell><cell>41.69</cell><cell>115.3</cell></row><row><cell>Overall ∅</cell><cell>837.43</cell><cell>1105.33</cell><cell>84.37</cell><cell>108.70</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 9 :</head><label>9</label><figDesc>Memory (MB) comparison of CS-kNN LB , CSB-kNN, CS-HTree LB , and CS-ARF</figDesc><table><row><cell>Dataset</cell><cell>CS-kNN LB</cell><cell cols="2">CSB-kNN CS-HTree LB</cell><cell>CS-ARF</cell></row><row><cell>Tweets 1</cell><cell>6.16</cell><cell>27.13</cell><cell>60.71</cell><cell>175.71</cell></row><row><cell>Tweets 2</cell><cell>6.16</cell><cell>28.97</cell><cell>66.75</cell><cell>177.32</cell></row><row><cell>Tweets 3</cell><cell>6.16</cell><cell>30.80</cell><cell>73.89</cell><cell>176.92</cell></row><row><cell>RBF</cell><cell>6.16</cell><cell>25.96</cell><cell>9.91</cell><cell>25.90</cell></row><row><cell>CNAE</cell><cell>6.15</cell><cell>28.11</cell><cell>0.48</cell><cell>1.31</cell></row><row><cell>Enron</cell><cell>6.15</cell><cell>28.59</cell><cell>1.59</cell><cell>4.10</cell></row><row><cell>IMDB</cell><cell>6.16</cell><cell>28.60</cell><cell>5.60</cell><cell>18.63</cell></row><row><cell>Spam</cell><cell>5.38</cell><cell>151.91</cell><cell>5.15</cell><cell>10.44</cell></row><row><cell>Covt</cell><cell>6.16</cell><cell>24.10</cell><cell>4.44</cell><cell>11.66</cell></row><row><cell>Overall ∅</cell><cell>6.07</cell><cell>41.57</cell><cell>25.39</cell><cell>66.89</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>This work was done in the context of <rs type="projectName">IoTA AAP Emergence Digi-Cosme</rs> Project and was funded by <rs type="funder">Labex DigiCosme</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_46GWq4E">
					<orgName type="project" subtype="full">IoTA AAP Emergence Digi-Cosme</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Database-friendly random projections: Johnsonlindenstrauss with binary coins</title>
		<author>
			<persName><forename type="first">Dimitris</forename><surname>Achlioptas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JCSS</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="671" to="687" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A performance comparison of measurement matrices in compressive sensing</title>
		<author>
			<persName><forename type="first">Youness</forename><surname>Arjoune</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naima</forename><surname>Kaabouch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hassan</forename><forename type="middle">El</forename><surname>Ghazi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ahmed</forename><surname>Tamtaoui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Communication Systems</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page">3576</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A sketch-based naive bayes algorithms for evolving data streams</title>
		<author>
			<persName><forename type="first">Maroua</forename><surname>Bahri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Silviu</forename><surname>Maniu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Albert</forename><surname>Bifet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Big Data</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="604" to="613" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A simple proof of the restricted isometry property for random matrices</title>
		<author>
			<persName><forename type="first">Richard</forename><surname>Baraniuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Davenport</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ronald</forename><surname>Devore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Wakin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Constructive Approximation</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="253" to="263" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A survey on feature drift adaptation: Definition, benchmark, challenges and future directions</title>
		<author>
			<persName><forename type="first">Jean</forename><surname>Paul Barddal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Murilo</forename><surname>Heitor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabrício</forename><surname>Gomes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Enembreck</surname></persName>
		</author>
		<author>
			<persName><surname>Pfahringer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Systems and Software</title>
		<imprint>
			<biblScope unit="volume">127</biblScope>
			<biblScope unit="page" from="278" to="294" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Machine learning for data streams: with practical examples in MOA</title>
		<author>
			<persName><forename type="first">Albert</forename><surname>Bifet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ricard</forename><surname>Gavaldà</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoff</forename><surname>Holmes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Pfahringer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Moa: Massive online analysis</title>
		<author>
			<persName><forename type="first">Albert</forename><surname>Bifet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoff</forename><surname>Holmes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Kirkby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Pfahringer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="1601" to="1604" />
			<date type="published" when="2010-05">May. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Leveraging bagging for evolving data streams</title>
		<author>
			<persName><forename type="first">Albert</forename><surname>Bifet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoff</forename><surname>Holmes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Pfahringer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Joint European conference on machine learning and knowledge discovery in databases</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="135" to="150" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Efficient data stream classification via probabilistic adaptive windows</title>
		<author>
			<persName><forename type="first">Albert</forename><surname>Bifet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Pfahringer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jesse</forename><surname>Read</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoff</forename><surname>Holmes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGAPP</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="801" to="806" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">From sparse solutions of systems of equations to sparse modeling of signals and images</title>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">L</forename><surname>Alfred M Bruckstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Donoho</surname></persName>
		</author>
		<author>
			<persName><surname>Elad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM review</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="34" to="81" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Compressed sensing and sparse filtering</title>
		<author>
			<persName><forename type="first">Lyudmila</forename><surname>Avishy Y Carmi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><forename type="middle">J</forename><surname>Mihaylova</surname></persName>
		</author>
		<author>
			<persName><surname>Godsill</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Agglomeration and elimination of terms for dimensionality reduction</title>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Marques</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ciarelli</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Elias</forename><surname>Oliveira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISDA</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="547" to="552" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Linear dimensionality reduction: Survey, insights, and generalizations</title>
		<author>
			<persName><forename type="first">P</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zoubin</forename><surname>Cunningham</surname></persName>
		</author>
		<author>
			<persName><surname>Ghahramani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="2859" to="2900" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Mining high-speed data streams</title>
		<author>
			<persName><forename type="first">Pedro</forename><surname>Domingos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoff</forename><surname>Hulten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="71" to="80" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Compressed sensing</title>
		<author>
			<persName><forename type="first">L</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName><surname>Donoho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1289" to="1306" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Persistent homology-a survey</title>
		<author>
			<persName><forename type="first">Herbert</forename><surname>Edelsbrunner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Harer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Contemporary Mathematics</title>
		<imprint>
			<biblScope unit="volume">453</biblScope>
			<biblScope unit="page" from="257" to="282" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Compressed sensing of streaming data</title>
		<author>
			<persName><forename type="first">Orhan</forename><surname>Nikolaos M Freris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Oc</surname></persName>
		</author>
		<author>
			<persName><surname>Vetterli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">51st Annual Allerton Conference on Communication, Control, and Computing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1242" to="1249" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Adaptive random forests for evolving data stream classification</title>
		<author>
			<persName><forename type="first">M</forename><surname>Heitor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Albert</forename><surname>Gomes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jesse</forename><surname>Bifet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean</forename><surname>Read</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabrício</forename><surname>Paul Barddal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Enembreck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoff</forename><surname>Pfharinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Talel</forename><surname>Holmes</surname></persName>
		</author>
		<author>
			<persName><surname>Abdessalem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="page" from="1" to="27" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Fast iterative kernel principal component analysis</title>
		<author>
			<persName><forename type="first">Simon</forename><surname>Günter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicol</forename><forename type="middle">N</forename><surname>Schraudolph</surname></persName>
		</author>
		<author>
			<persName><surname>Vishwanathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1893" to="1918" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Analysis of a complex of statistical variables into principal components</title>
		<author>
			<persName><forename type="first">Harold</forename><surname>Hotelling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Educational Psychology</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">417</biblScope>
			<date type="published" when="1933">1933</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Extensions of lipschitz maps into banach spaces</title>
		<author>
			<persName><forename type="first">Joram</forename><surname>William B Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gideon</forename><surname>Lindenstrauss</surname></persName>
		</author>
		<author>
			<persName><surname>Schechtman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Israel Journal of Mathematics</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="129" to="138" />
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">An adaptive personalized news dissemination system</title>
		<author>
			<persName><forename type="first">Ioannis</forename><surname>Katakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Grigorios</forename><surname>Tsoumakas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Evangelos</forename><surname>Banos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Bassiliades</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ioannis</forename><surname>Vlahavas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Intelligent Information Systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="191" to="212" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">The enron corpus: A new dataset for email classification research</title>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Klimt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECML</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="217" to="226" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">New and improved johnsonlindenstrauss embeddings via the restricted isometry property</title>
		<author>
			<persName><forename type="first">Felix</forename><surname>Krahmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rachel</forename><surname>Ward</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematical Analysis</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1269" to="1281" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Real-time visual tracking using compressive sensing</title>
		<author>
			<persName><forename type="first">Hanxi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qinfeng</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1305" to="1312" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Feature extraction, construction and selection: A data mining perspective</title>
		<author>
			<persName><forename type="first">Huan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hiroshi</forename><surname>Motoda</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
			<publisher>Springer Science &amp; Business Media</publisher>
			<biblScope unit="volume">453</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Computational methods of feature selection</title>
		<author>
			<persName><forename type="first">Huan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hiroshi</forename><surname>Motoda</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
			<publisher>CRC Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Knn classifier with self adjusting memory for heterogeneous concept drift</title>
		<author>
			<persName><forename type="first">Viktor</forename><surname>Losing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barbara</forename><surname>Hammer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heiko</forename><surname>Wersing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDM</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="291" to="300" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning word vectors for sentiment analysis</title>
		<author>
			<persName><forename type="first">Raymond</forename><forename type="middle">E</forename><surname>Andrew L Maas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">T</forename><surname>Daly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL-HLT</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="142" to="150" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Deterministic sensing matrices in compressive sensing: a survey</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">N</forename><surname>Thu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoan</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><surname>Shin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Scientific World Journal</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Real-time dynamic mr image reconstruction using kalman filtered compressed sensing</title>
		<author>
			<persName><forename type="first">Chenlu</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Namrata</forename><surname>Vaswani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="393" to="396" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Batch-incremental versus instance-incremental learning in dynamic and evolving data</title>
		<author>
			<persName><forename type="first">Jesse</forename><surname>Read</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Albert</forename><surname>Bifet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Pfahringer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoff</forename><surname>Holmes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IDA</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="313" to="323" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">The persistent homology of distance functions under random projection</title>
		<author>
			<persName><surname>Donald R Sheehy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SoCG</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page">328</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Sparse image and signal processing: Wavelets and related geometric multiscale analysis</title>
		<author>
			<persName><forename type="first">Jean-Luc</forename><surname>Starck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fionn</forename><surname>Murtagh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jalal</forename><surname>Fadili</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
			<publisher>Cambridge university press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Vehicle classification using compressive sensing</title>
		<author>
			<persName><surname>Uttarakumari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Ashray</surname></persName>
		</author>
		<author>
			<persName><surname>Achary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Sujata</surname></persName>
		</author>
		<author>
			<persName><surname>Badiger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anisha</forename><surname>Avinash</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nancy</forename><surname>Mukherjee</surname></persName>
		</author>
		<author>
			<persName><surname>Kothari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">RTEICT</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="692" to="696" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Feature hashing for large scale multitask learning</title>
		<author>
			<persName><forename type="first">Kilian</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anirban</forename><surname>Dasgupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Langford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Josh</forename><surname>Attenberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="1113" to="1120" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Learning compressed sensing</title>
		<author>
			<persName><forename type="first">Yair</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyun</forename><forename type="middle">Sung</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Snowbird Learning Workshop</title>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Candid covariance-free incremental principal component analysis</title>
		<author>
			<persName><forename type="first">Juyang</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yilu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wey-Shiuan</forename><surname>Hwang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1034" to="1040" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
