<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Fully Decentralized Joint Learning of Personalized Models and Collaboration Graphs</title>
				<funder ref="#_tNpGkQM">
					<orgName type="full">CPER Nord-Pas de Calais/FEDER</orgName>
				</funder>
				<funder ref="#_BvXnwtX">
					<orgName type="full">Agence Nationale de la Recherche</orgName>
					<orgName type="abbreviated">ANR</orgName>
				</funder>
				<funder ref="#_jctHnXk">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Valentina</forename><surname>Zantedeschi</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Aurélien</forename><surname>Bellet</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Marc</forename><surname>Tommasi</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">GE Global Research</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Inria</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Université de Lille &amp; Inria</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Fully Decentralized Joint Learning of Personalized Models and Collaboration Graphs</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">D2C0F77DCC4424D0E8C3B927BB4D4E76</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-04-12T14:46+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We consider the fully decentralized machine learning scenario where many users with personal datasets collaborate to learn models through local peer-to-peer exchanges, without a central coordinator. We propose to train personalized models that leverage a collaboration graph describing the relationships between user personal tasks, which we learn jointly with the models. Our fully decentralized optimization procedure alternates between training nonlinear models given the graph in a greedy boosting manner, and updating the collaboration graph (with controlled sparsity) given the models. Throughout the process, users exchange messages only with a small number of peers (their direct neighbors when updating the models, and a few random users when updating the graph), ensuring that the procedure naturally scales with the number of users. Overall, our approach is communication-efficient and avoids exchanging personal data. We provide an extensive analysis of the convergence rate, memory and communication complexity of our approach, and demonstrate its benefits compared to competing techniques on synthetic and real datasets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>In the era of big data, the classical paradigm is to build huge data centers to collect and process user data. This centralized access to resources and datasets is convenient to train machine learning models, but also comes with important drawbacks. The service provider needs to gather, store and analyze the data on a large central server, which induces high infrastructure costs. As the server represents a single point of entry, it must also be secure enough to prevent attacks that could put the entire user database in jeopardy. On the user end, disadvantages include limited control over one's personal data as well as possible privacy risks, which may come from the aforementioned attacks but also from potentially loose data governance policies on the part of service providers. A more subtle risk is to be trapped in a "single thought" model which fades individual users' specificities or leads to unfair predictions for some of the users.</p><p>For these reasons and thanks to the advent of powerful personal devices, we are currently witnessing a shift to a different paradigm where data is kept on the users' devices, whose computational resources are leveraged to train models in a collaborative manner. The resulting data is not typically balanced nor independent and identically distributed across machines, and additional constraints arise when many parties are involved. In particular, the specificities of each user result in an increase in model complexity and size, and information needs to be exchanged across users to compensate for the lack of local data. In this context, communication is usually a major bottleneck, so that solutions aiming at reaching an agreement between user models or requiring a central coordinator should be avoided.</p><p>In this work, we focus on fully decentralized learning, which has recently attracted a lot of interest <ref type="bibr" target="#b11">(Duchi et al., 2012;</ref><ref type="bibr" target="#b43">Wei and Ozdaglar, 2012;</ref><ref type="bibr" target="#b7">Colin et al., 2016;</ref><ref type="bibr" target="#b28">Lian et al., 2017;</ref><ref type="bibr" target="#b19">Jiang et al., 2017;</ref><ref type="bibr" target="#b36">Tang et al., 2018;</ref><ref type="bibr" target="#b29">Lian et al., 2018)</ref>. In this setting, users exchange information through local peer-to-peer exchanges in a sparse communication graph without relying on a central server that aggregates updates or coordinates the protocol. Unlike federated learning which requires such central coordination <ref type="bibr" target="#b31">(McMahan et al., 2017;</ref><ref type="bibr" target="#b23">Konečnỳ et al., 2016;</ref><ref type="bibr" target="#b20">Kairouz et al., 2019)</ref>, fully decentralized learning naturally scales to large numbers of users without single point of failure or communication bottlenecks <ref type="bibr" target="#b28">(Lian et al., 2017)</ref>.</p><p>The present work stands out from existing approaches in fully decentralized learning, which train a single global model that may not be adapted to all users. Instead, our idea is to leverage the fact that in many large-scale applications (e.g., predictive modeling in smartphones apps), each user exhibits distinct behaviors/preferences but is sufficiently similar to some other peers to benefit from sharing information with them. We thus propose to jointly discover the relationships between the personal tasks of users in the form of a sparse collaboration graph and learn personalized models that leverage this graph to achieve better generalization performance. For scalability reasons, the collaboration graph serves as an overlay to restrict the communication to pairs of users whose tasks appear to be sufficiently similar. In such a framework, it is crucial that the graph is well-aligned with the underlying similarity between the personal tasks to ensure that the collaboration is fruitful and avoid convergence to poorly-adapted models.</p><p>We formulate the problem as the optimization of a joint objective over the models and the collaboration graph, in which collaboration is achieved by introducing a trade-off between (i) having the personal model of each user accurate on its local dataset, and (ii) making the models and the collaboration graph smooth with respect to each other. We then design and analyze a fully decentralized algorithm to solve our collaborative problem in an alternating procedure, in which we iterate between updating personalized models given the current graph and updating the graph (with controlled sparsity) given the current models. We first propose an approach to learn personalized nonlinear classifiers as combinations of a set of base predictors inspired from l 1 -Adaboost <ref type="bibr" target="#b34">(Shen and Li, 2010)</ref>. In the proposed decentralized algorithm, users greedily update their personal models by incorporating a single base predictor at a time and send the update only to their direct neighbors in the graph. We establish the convergence rate of the procedure and show that it requires very low communication costs (linear in the number of edges in the graph and logarithmic in the number of base classifiers to combine). We then propose an approach to learn a sparse collaboration graph. From the decentralized system perspective, users update their neighborhood of similar peers by communicating only with small random subsets of peers obtained through a peer sampling service <ref type="bibr" target="#b18">(Jelasity et al., 2007)</ref>. Our approach is flexible enough to accommodate various graph regularizers allowing to easily control the spar-sity of the learned graph, which is key to the scalability of the model update step. For strongly convex regularizers, we prove a fast convergence for our algorithm and show how the number of random users requested from the peer sampling service rules a trade-off between communication and convergence speed.</p><p>To summarize, we propose the first approach to train in a fully decentralized way, i.e. without any central server, personalized and nonlinear models in a collaborative way while also learning the collaboration graph. Our main contributions are as follows. (1) We formalize the problem of learning with whom to collaborate, together with personalized models for collaborative decentralized learning. (2) We propose and analyze a fully decentralized algorithm to learn nonlinear personalized models with low communication costs. (3) We derive a generic and scalable approach to learn sparse collaboration graphs in the decentralized setting. (4) We show that our alternating optimization scheme leads to better personalized models at lower communication costs than existing methods on several datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Federated multi-task learning. Our work can be seen as multi-task learning (MTL) where each user is considered as a task. In MTL, multiple tasks are learned simultaneously with the assumption that a structure captures task relationships. A popular approach in MTL is to jointly optimize models for all tasks while enforcing similar models for similar tasks <ref type="bibr" target="#b13">(Evgeniou and Pontil, 2004;</ref><ref type="bibr" target="#b30">Maurer, 2006;</ref><ref type="bibr" target="#b8">Dhillon et al., 2011)</ref>. Task relationships are often considered as known a priori but recent work also tries to learn this structure (see <ref type="bibr">Zhang and Yang, 2017, and references therein)</ref>. However, in classical MTL approaches data is collected on a central server where the learning algorithm is performed (or it is iid over the machines of a computing cluster). Recently, distributed and federated learning approaches <ref type="bibr">(Wang et al., 2016b,a;</ref><ref type="bibr" target="#b2">Baytas et al., 2016;</ref><ref type="bibr" target="#b35">Smith et al., 2017)</ref> have been proposed to overcome these limitations. Each node holds data for one task (non iid data) but these approaches still rely on a central server to aggregate updates. The federated learning approach of <ref type="bibr" target="#b35">(Smith et al., 2017)</ref> is closest to our work for it jointly learns personalized (linear) models and pairwise similarities across tasks. However, the similarities are updated in a centralized way by the server which must regularly access all task models, creating a significant communication and computation bottleneck when the number of tasks is large. Furthermore, the task similarities do not form a valid weighted graph and are typically not sparse. This makes their problem formulation poorly suited to the fully decentralized setting, where sparsity is key to ensure scalability. Decentralized learning. There has been a recent surge of interest in fully decentralized machine learning. In most existing work, the goal is to learn the same global model for all users by minimizing the average of the local objectives <ref type="bibr" target="#b11">(Duchi et al., 2012;</ref><ref type="bibr" target="#b43">Wei and Ozdaglar, 2012;</ref><ref type="bibr" target="#b7">Colin et al., 2016;</ref><ref type="bibr" target="#b26">Lafond et al., 2016;</ref><ref type="bibr" target="#b28">Lian et al., 2017;</ref><ref type="bibr" target="#b19">Jiang et al., 2017;</ref><ref type="bibr" target="#b36">Tang et al., 2018;</ref><ref type="bibr" target="#b29">Lian et al., 2018)</ref>. In this case, there is no personalization: the graph merely encodes the communication topology without any semantic meaning and only affects the convergence speed. Our work is more closely inspired by recent decentralized approaches that have shown the benefits of collaboratively learning personalized models for each user by leveraging a similarity graph given as input to the algorithm <ref type="bibr" target="#b39">(Vanhaesebrouck et al., 2017;</ref><ref type="bibr" target="#b27">Li et al., 2017;</ref><ref type="bibr" target="#b3">Bellet et al., 2018;</ref><ref type="bibr" target="#b0">Almeida and Xavier, 2018)</ref>. As in our approach, this is achieved through a graph regularization term in the objective. A severe limitation to the applicability of these methods is that a relevant graph must be known beforehand, which is an unrealistic assumption in many practical scenarios. Crucially, our approach lifts this limitation by allowing to learn the graph along with the models. In fact, as we demonstrate in our experiments, our decentralized graph learning procedure of Section 5 can be readily combined with the algorithms of ( <ref type="bibr" target="#b39">Vanhaesebrouck et al., 2017;</ref><ref type="bibr" target="#b27">Li et al., 2017;</ref><ref type="bibr" target="#b3">Bellet et al., 2018;</ref><ref type="bibr" target="#b0">Almeida and Xavier, 2018)</ref> in our alternating optimization procedure, thereby broadening their scope. It is also worth mentioning that <ref type="bibr" target="#b39">(Vanhaesebrouck et al., 2017;</ref><ref type="bibr" target="#b27">Li et al., 2017;</ref><ref type="bibr" target="#b3">Bellet et al., 2018;</ref><ref type="bibr" target="#b0">Almeida and Xavier, 2018)</ref> are restricted to linear models and have per-iteration communication complexity linear in the data dimension. Our boosting-based approach (Section 4) learns nonlinear models with logarithmic communication cost, providing an interesting alternative for problems of high dimension and/or with complex decision boundaries, as illustrated in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">PROBLEM SETTING AND NOTATIONS</head><p>In this section, we formally describe the problem of interest. We consider a set of users (or agents)</p><p>[K] = {1, . . . , K}, each with a personal data distribution over some common feature space X and label space Y defining a personal supervised learning task.</p><p>For example, the personal task of each user could be to predict whether he/she likes a given item based on features describing the item. Each user k holds a local dataset S k of m k labeled examples drawn from its personal data distribution over X × Y, and aims to learn a model parameterized by α k ∈ R n which generalizes well to new data points drawn from its distribution. We assume that all users learn models from the same hypothesis class, and since they have datasets of different sizes we introduce a notion of "confidence" c k ∈ R + for each user k which should be thought of as proportional to m k (in practice we simply set c k = m k / max l m l ). In a non-collaborative setting, each user k would typically select the model parameters that minimize some (potentially regularized) loss function L k (α k ; S k ) over its local dataset S k . This leads to poor generalization performance when local data is scarce. Instead, we propose to study a collaborative learning setting in which users discover relationships between their personal tasks which are leveraged to learn better personalized models. We aim to solve this problem in a fully decentralized way without relying on a central coordinator node.</p><p>Decentralized collaborative learning. Following the standard practice in the fully decentralized literature <ref type="bibr" target="#b5">(Boyd et al., 2006)</ref>, each user regularly becomes active at the ticks of an independent local clock which follows a Poisson distribution. Equivalently, we consider a global clock (with counter t) which ticks each time one of the local clock ticks, which is convenient for stating and analyzing the algorithms. We assume that each user can send messages to any other user (like on the Internet) in a peer-to-peer manner. However, in order to scale to a large number of users and to achieve fruitful collaboration, we consider a semantic overlay on the communication layer whose goal is to restrict the message exchanges to pairs of users whose tasks are most similar. We call this overlay a collaboration graph, which is modeled as an undirected weighted graph G w = ([K], w) in which nodes correspond to users and edge weights w k,l ≥ 0 should reflect the similarity between the learning tasks of users k and l, with w k,l = 0 indicating the absence of edge. A user k only sends messages to its direct neighbors N k = {l : w k,l &gt; 0} in G w , and potentially to a small random set of peers obtained through a peer sampling service (see <ref type="bibr" target="#b18">Jelasity et al., 2007</ref>, for a decentralized version). Importantly, we do not enforce the graph to be connected: different connected components can be seen as modeling clusters of unrelated users. In our approach, the collaboration graph is not known beforehand and iteratively evolves (controlling its sparsity) in a learning scheme that alternates between learning the graph and learning the models. This scheme is designed to solve a global, joint optimization problem that we introduce below.</p><p>Objective function. We propose to learn the personal classifiers α = (α 1 , . . . , α K ) ∈ (R n ) K and the collaboration graph w ∈ R K(K-1)/2 to minimize the following joint optimization problem:</p><formula xml:id="formula_0">min α∈M w∈W J(α, w) = K k=1 d k (w)c k L k (α k ; S k ) + µ 1 2 k&lt;l w k,l α k -α l 2 + µ 2 g(w),<label>(1)</label></formula><p>where</p><formula xml:id="formula_1">M = M 1 × • • • × M K and W = {w ∈ R K(K-1</formula><p>)/<ref type="foot" target="#foot_1">2</ref> : w ≥ 0} are the feasible domains for the models and the graph, d(w) = (d 1 (w), . . . , d K (w)) ∈ R K is the degree vector with d k (w) = K l=1 w k,l , and µ 1 , µ 2 ≥ 0 are trade-off hyperparameters.</p><p>The joint objective function J(α, w) in ( <ref type="formula" target="#formula_0">1</ref>) is composed of three terms. The first one is a (weighted) sum of loss functions, each involving only the personal model and local dataset of a single user. The second term involves both the models and the graph: it enables collaboration by encouraging two users k and l to have a similar model for large edge weight w k,l . This principle, known as graph regularization, is well-established in the multi-task learning literature <ref type="bibr" target="#b13">(Evgeniou and Pontil, 2004;</ref><ref type="bibr" target="#b30">Maurer, 2006;</ref><ref type="bibr" target="#b8">Dhillon et al., 2011)</ref>. Importantly, the factor d k (w)c k in front of the local loss L k of each user k implements a useful inductive bias: users with larger datasets (large confidence) will tend to connect to other nodes as long as their local loss remains small so that they can positively influence their neighbors, while users with small datasets (low confidence) will tend to disregard their local loss and rely more on information from other users. Finally, the last term g(w) introduces some regularization on the graph weights w used to avoid degenerate solutions (e.g., edgeless graphs) and control structural properties such as sparsity (see Section 5 for concrete examples). We stress the fact that the formulation (1) allows for very flexible notions of relationships between the users' tasks. For instance, as µ 1 → +∞ the problem becomes equivalent to learning a shared model for all users in the same connected component of the graph, by minimizing the sum of the losses of users independently in each component. On the other hand, setting µ 1 = 0 corresponds to having each user k learn its classifier α k based on its local dataset only (no collaboration). Intermediate values of µ 1 let each user learn its own personal model but with the models of other (strongly connected) users acting as a regularizer.</p><p>While Problem (1) is not jointly convex in α and w in general, it is typically bi-convex. Our approach thus solves it by alternating decentralized optimization on the models α and the graph weights w. 2</p><p>Outline. In Section 4, we propose a decentralized algorithm to learn nonlinear models given the graph in a greedy boosting manner with communication-efficient updates. In Section 5, we design a decentralized algorithm to learn a (sparse) collaboration graph given the models with flexible regularizers g(w). We discuss related work in Section 2, and present some experiments in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">DECENTRALIZED COLLABORATIVE BOOSTING OF PERSONALIZED MODELS</head><p>In this section, given some fixed graph weights w ∈ W, we propose a decentralized algorithm for learning personalized nonlinear classifiers α = (α 1 , . . . , α K ) ∈ M in a boosting manner which is essential to ensure only logarithmic communication complexity in the number of model parameters while optimizing expressive models. For simplicity, we focus on binary classification with Y = {-1, 1}. We propose that each user k learns a personal classifier as a weighted combination of a set of n real-valued base predictors H = {h j : X → R} n j=1 , i.e. a mapping x → sign(</p><formula xml:id="formula_2">n j=1 [α k ] j h j (x)) parameter- ized by α k ∈ R n .</formula><p>The base predictors can be for instance weak classifiers (e.g., decision stumps) as in standard boosting, or stronger predictors pre-trained on separate data (e.g., public, crowdsourced, or collected from users who opted in to share personal data). We denote by A k ∈ R m k ×n the matrix whose (i, j)th entry gives the margin achieved by the j-th base classifier on the i-th training sample of user k, so that for</p><formula xml:id="formula_3">i ∈ [m k ], [A k α k ] i = y i n j=1 [α k ] j h j (x i )</formula><p>gives the margin achieved by the classifier α k on the i-th data point (x i , y i ) in S k . Only user k has access to A k .</p><p>Adapting the formulation of l 1 -Adaboost <ref type="bibr" target="#b34">(Shen and Li, 2010;</ref><ref type="bibr" target="#b40">Wang et al., 2015)</ref> to our personalized setting, we instantiate the local loss L k (α k ; S k ) and the feasible domain M k = {α k ∈ R n : α k 1 ≤ β} for each user k as follows:</p><formula xml:id="formula_4">L k (α k ; S k ) = log m k i=1 e -[A k α k ]i ,<label>(2)</label></formula><p>where β ≥ 0 is a hyperparameter to favor sparse models by controlling their l 1 -norm. Since the graph weights are fixed in this section, with a slight abuse of notation we denote by f (α) := J(α, w) the objective function in (1) instantiated with the loss function (2). Note that f is convex and continuously differentiable, and the domain</p><formula xml:id="formula_5">M = M 1 × • • • × M K is a compact and convex subset of (R n ) K .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Decentralized Algorithm</head><p>We propose a decentralized algorithm based on Frank-Wolfe (FW) <ref type="bibr" target="#b14">(Frank and Wolfe, 1956;</ref><ref type="bibr" target="#b17">Jaggi, 2013)</ref>, also known as conditional gradient descent. Our approach is inspired from a recent FW algorithm to solve l 1 -Adaboost in the centralized and non-personalized setting <ref type="bibr" target="#b40">(Wang et al., 2015)</ref>. For clarity of presentation, we set aside the decentralized setting for a moment and derive the FW update with respect to the model of a single user.</p><p>Classical FW update. Let t ≥ 1 and denote by ∇[f (α (t-1) )] k the partial derivative of f with respect to the k-th block of coordinates corresponding to the model α (t-1) k of user k. For step size γ ∈ [0, 1], a FW update for user k takes the form of a convex combination α</p><formula xml:id="formula_6">(t) k = (1 -γ)α (t-1) k + γs (t) k with s (t) k = arg min s 1≤β s ∇[f (α (t-1) )] k = β sign(-(∇[f (α (t-1) )] k ) j (t) k )e j (t) k ,<label>(3)</label></formula><p>where j</p><formula xml:id="formula_7">(t) k = arg max j [|∇[f (α (t-1) )] k |] j and e j (t)</formula><p>k is the unit vector with 1 in the j</p><formula xml:id="formula_8">(t)</formula><p>k -th entry <ref type="bibr" target="#b6">(Clarkson, 2010;</ref><ref type="bibr" target="#b17">Jaggi, 2013)</ref>. In other words, FW updates a single coordinate of the current model α (t-1) k which corresponds to the maximum absolute value entry of the partial gradient ∇[f (α (t-1) )] k . In our case, we have:</p><formula xml:id="formula_9">∇[f (α (t-1) )] k = -d k (w)c k η k A k + µ 1 (d k (w)α (t-1) k -l w k,l α (t-1) l</formula><p>), ( <ref type="formula">4</ref>)</p><formula xml:id="formula_10">with η k = exp(-A k α (t-1) k ) m k i=1 exp(-A k α (t-1) k )i</formula><p>. The first term in ∇[f (α (t-1) )] k plays the same role as in standard Adaboost: the j-th entry (corresponding to the base predictor h j ) is larger when h j achieves a large margin on the training sample S k reweighted by η k (i.e., points that are currently poorly classified get more importance). On the other hand, the more h j is used by the neighbors of k, the larger the j-th entry of the second term. The FW update (3) thus preserves the flavor of boosting (incorporating a single base classifier at a time which performs well on the reweighted sample) with an additional bias towards selecting base predictors that are popular amongst neighbors in the collaboration graph. The relative importance of the two terms depends on the user confidence c k .</p><p>Decentralized FW. We are now ready to state our decentralized FW algorithm to optimize f . Each user corresponds keeps its personal dataset locally. The fixed collaboration graph G w plays the role of an overlay: user k only needs to communicate with its direct neighborhood N k in G w . The size of N k , |N k |, is typically small so that updates can occur in parallel in different parts of the network, ensuring that the procedure scales well with the number of users.</p><p>Our algorithm proceeds as follows. Let us denote by α (t) ∈ M the current models at time step t. Each personal classifier is initialized to some feasible point α (0) k ∈ M k (such as the zero vector). Then, at each step t ≥ 1, a random user k wakes up and performs the following actions:</p><p>1. Update step: user k performs a FW update on its local model based on the most recent information α (t-1) l received from its neighbors l ∈ N k :</p><formula xml:id="formula_11">α (t) k = (1 -γ (t) )α (t-1) k + γ (t) s (t) k , with s (t)</formula><p>k as in (3) and γ (t) = 2K/(t + 2K). 2. Communication step: user k sends its updated model α</p><formula xml:id="formula_12">(t)</formula><p>k to its neighborhood N k .</p><p>Importantly, the above update only requires the knowledge of the models of neighboring users, which were received at earlier iterations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Convergence Analysis, Communication and Memory Costs</head><p>The convergence analysis of our algorithm essentially follows the proof technique proposed in <ref type="bibr" target="#b17">(Jaggi, 2013)</ref> and refined in <ref type="bibr" target="#b25">(Lacoste-Julien et al., 2013)</ref>  k . We obtain that our algorithm achieves an O(1/t) convergence rate (see supplementary for the proof).</p><p>Theorem 1. Our decentralized Frank-Wolfe algorithm takes at most 6K(C ⊗ f + p 0 )/ε iterations to find an approximate solution α that satisfies, in expectation,</p><formula xml:id="formula_13">f (α) -f (α * ) ≤ ε, where C ⊗ f ≤ 4β 2 K k=1 d k (w)(c k A k 2 + µ 1 ) and p 0 = f (α (0) ) - f (α * ) is the initial sub-optimality gap.</formula><p>Theorem 1 shows that large degrees for users with low confidence and small margins penalize the convergence rate much less than for users with large confidence and large margins. This is rather intuitive as users in the latter case have greater influence on the overall solution in Eq. ( <ref type="formula" target="#formula_0">1</ref>).</p><p>Remarkably, using a few tricks in the representation of the sparse updates, the communication and memory cost needed by our algorithm to converge to an -approximate solution can be shown to be linear in the number of edges of the graph and logarithmic in the number of base predictors. We refer to the supplementary material for details. For the classic case where base predictors consist of a constant number of decisions stumps per feature, this translates into a logarithmic cost in the dimensionality of the data leading to significantly better complexities than the state-of-the-art (see the experiments of Section 6).</p><p>Remark 1 (Other loss functions). We focus on the Adaboost log loss (2) to emphasize that we can learn nonlinear models while keeping the formulation convex. We point out that our algorithm and analysis readily extend to other convex loss functions, as long as we keep an L1-constraint on the parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">DECENTRALIZED LEARNING OF COLLABORATION GRAPH</head><p>In the previous section, we have proposed and analyzed an algorithm to learn the model parameters α given a fixed collaboration graph w. To make our fully decentralized alternating optimization scheme complete, we now turn to the converse problem of optimizing the graph weights w given fixed models α. We will work with flexible graph regularizers g(w) that are weight and degree-separable:</p><formula xml:id="formula_14">g(w) = k&lt;l g k,l (w k,l ) + K k=1 g k (d k (w))</formula><p>, where g k,l : R → R and g k : R → R are convex and smooth. This generic form allows to regularize weights and degrees in a flexible way (which encompasses some recent work from the graph signal processing community <ref type="bibr" target="#b9">(Dong et al., 2016;</ref><ref type="bibr" target="#b21">Kalofolias, 2016;</ref><ref type="bibr" target="#b4">Berger et al., 2018)</ref>), while the separable structure is key to the design of an efficient decentralized algorithm that relies only on local communication. We denote the graph learning objective function by h(w) := J(α, w) for fixed models α. Note that h(w) is convex in w.</p><p>Decentralized algorithm. Our goal is to design a fully decentralized algorithm to update the collaboration graph G w . We thus need users to communicate beyond their current direct neighbors in G w to discover new relevant neighbors. In order to preserve scalability to large numbers of users, a user can only communicate with small random batches of other users. In a decentralized system, this can be implemented by a classic primitive known as a peer sampling service <ref type="bibr" target="#b18">(Jelasity et al., 2007;</ref><ref type="bibr" target="#b22">Kermarrec et al., 2011)</ref>. Let κ ∈ [1..K -1] be a parameter of the algorithm, which in practice is much smaller than K. At each step, a random user k wakes up and samples uniformly and without replacement a set K of κ users from the set {1, . . . , K} \ {k} using the peer sampling service. We denote by w k,K the κ-dimensional subvector of a vector w ∈ R K(K-1)/2 corresponding to the entries</p><formula xml:id="formula_15">{(k, l)} l∈K . Let ∆ k,K = ( α k -α l 2 ) l∈K , p k,K = (c k L k (α k ; S k )+c l L l (α l ; S l )) l∈K and v k,K (w) = (g k (d k (w)) + g l (d l (w)) + g k,l (w k,l )) l∈K .</formula><p>The partial derivative of the objective h(w) with respect to the variables w k,K can be written as follows:</p><formula xml:id="formula_16">[∇h(w)] k,K = p k,K + (µ 1 /2)∆ k,K + µ 2 v k,K (w). (5)</formula><p>We denote by L k,K is the Lipschitz constant of ∇h with respect to block w k,K . We now state our algorithm. We start from some arbitrary weight vector w (0) ∈ W, each user having a local copy of its K -1 weights. At each time step t, a random user k wakes up and performs the following actions:</p><p>1. Draw a set K of κ users and request their current models, loss value and degree. 2. Update the associated weights:</p><formula xml:id="formula_17">w (t+1) k,K ← max 0, w (t) k,K -(1/L k,K )[∇h(w (t) )] k,K . 3. Send each updated weight w (t+1) k,l</formula><p>to the associated user in l ∈ K.</p><p>The algorithm is fully decentralized. Indeed, no global information is needed to update the weights: the information requested from users in K at step 1 of the algorithm is sufficient to compute (5). Updates can thus happen asynchronously and in parallel.</p><p>Convergence, communication and memory. Our analysis proceeds as follows. We first show that our algorithm can be seen as an instance of proximal coordinate descent (PCD) <ref type="bibr" target="#b38">(Tseng and Yun, 2009;</ref><ref type="bibr" target="#b33">Richtárik and Takác, 2014)</ref> on a slightly modified objective function. Unlike the standard PCD setting which focuses on disjoint blocks, our coordinate blocks exhibit a specific overlapping structure that arises as soon as κ &gt; 1 (as each weight is shared by two users). We build upon the PCD analysis due to <ref type="bibr" target="#b44">(Wright, 2015)</ref>, which we adapt to account for our overlapping block structure. The details of our analysis can be found in the supplementary material. For the case where g is strongly convex, we obtain the following convergence rate. <ref type="foot" target="#foot_2">3</ref>Theorem 2. Assume that g(w) is σ-strongly convex. Let T &gt; 0 and h * be the optimal objective value. Our algorithm cuts the expected suboptimality gap by a constant factor ρ at each iteration: we</p><formula xml:id="formula_18">have E[h(w (T ) ) -h * ] ≤ ρ T (h(w (0) ) -h * ) with ρ = 1 - 2κσ K(K-1)Lmax with L max = max (k,K) L k,K .</formula><p>The rate of Theorem 2 is typically faster than the sublinear rate of the boosting subproblem (Theorem 1), suggesting that a small number of updates per user is sufficient to reach reasonable optimization error before re-updating the models given the new graph. In the supplementary, we further analyze the trade-off between communication and memory costs and the convergence rate ruled by κ.</p><p>Proposed regularizer. In our experiments, we use a graph regularizer defined as g(w) = λ w 2 -1 log(d(w) + δ), which is inspired from <ref type="bibr" target="#b21">(Kalofolias, 2016)</ref>. The log term ensures that all nodes have nonzero degrees (the small positive constant δ is a simple trick to make the logarithm smooth on the feasible domain, see e.g., <ref type="bibr" target="#b24">(Koriche, 2018)</ref>) without ruling out non-connected graphs with several connected components. Crucially, λ &gt; 0 provides a direct way to tune the sparsity of the graph: the smaller λ, the more concentrated the weights of a given user on the peers with the closest models. This allows us to control the trade-off between accuracy and communication in the model update step of Section 4, whose communication cost is linear in the number of edges. The resulting objective is strongly convex and block-Lipschitz continuous (see supplementary for the derivation of the parameters and analysis of the trade-offs). Finally, as discussed in <ref type="bibr" target="#b21">(Kalofolias, 2016)</ref>, tuning the importance of the log-degree term with respect to the other graph terms has simply a scaling effect, thus we can simply set µ 2 = µ 1 in (1).</p><p>Remark 2 (Reducing the number of variables). To reduce the number of variables to optimize, each user can keep to 0 the weights corresponding to users whose current model is most different to theirs. This heuristic has a negligible impact on the solution quality in sparse regimes (small λ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">EXPERIMENTS</head><p>In this section, we study the practical behavior of our approach. Denoting our decentralized Adaboost method introduced in Section 4 as Dada, we study two variants: Dada-Oracle (which uses a fixed oracle graph given as input) and Dada-Learned (where the graph is learned along with the models). We compare against various competitors, which learn either global or personalized models in a centralized or decentralized manner. Global-boost and Global-lin learn a single global l 1 -Adaboost model (resp. linear model) over the centralized dataset S = ∪ k S k . Local-boost and Local-lin learn (Adaboost or linear) personalized models independently for each user without collaboration. Finally, Perso-lin is a decentralized method for collaboratively learning personalized linear models <ref type="bibr" target="#b39">(Vanhaesebrouck et al., 2017)</ref>. This approach requires an oracle graph as input (Perso-lin-Oracle) but it can also directly benefit from our graph learning approach of Section 5 (we denote this new variant by Perso-lin-Learned). We use the same set of base predictors for all boosting-based methods, namely n simple decision stumps uniformly split between all D dimensions and value ranges. For all methods we tune the hyper-parameters with 3-fold cross validation. Models are initialized to zero vectors and the initial graphs of Dada-Learned and Perso-lin-Learned are learned using the purely local classifiers, and then updated after every 100 iterations of optimizing the classifiers, with κ = 5. All reported accuracies are averaged over Synthetic data. To study the behavior of our approach in a controlled setting, our first set of experiments is carried out on a synthetic problem (Moons) constructed from the classic two interleaving Moons dataset which has nonlinear class boundaries. We consider K = 100 users, clustered in 4 groups of 10, 20, 30 and 40 users. Users in the same cluster are associated with a similar rotation of the feature space and hence have similar tasks. We construct an oracle collaboration graph based on the difference in rotation angles between users, which is given as input to Dada-Oracle and Perso-lin-Oracle. Each user k obtains a training sample random size m k ∼ U(3, 15). The data dimension is D = 20 and the number of base predictors is n = 200. We refer to the supplementary material for more details on the dataset generation.  <ref type="table" target="#tab_1">1</ref>, Dada-Learned and Perso-lin-Learned, which both make use of our alternating procedure, achieve the best performance. This demonstrates the wide applicability of our graph learning approach, for it enables the use of Perso-lin <ref type="bibr" target="#b39">(Vanhaesebrouck et al., 2017)</ref> on datasets where no prior information is available to build a predefined collaboration graph. Thanks to its logarithmic communication, our approach Dada-Learned achieves higher accuracy under limited communication budgets, especially on higher-dimensional data (Table <ref type="table" target="#tab_2">2</ref>). More details and results are given in the supplementary. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">FUTURE WORK</head><p>We plan to extend our approach to (functional) gradient boosting <ref type="bibr" target="#b15">(Friedman, 2001;</ref><ref type="bibr" target="#b40">Wang et al., 2015)</ref> where the graph regularization term would need to be applied to an infinite set of base predictors. Another promising direction is to make our approach differentially-private <ref type="bibr" target="#b12">(Dwork, 2006)</ref> to formally guarantee that personal datasets cannot be inferred from the information sent by users. As our algorithm communicates very scarcely, we think that the privacy/accuracy trade-off may be better than the one known for linear models <ref type="bibr" target="#b3">(Bellet et al., 2018)</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Proceedings of the 23 rd International Conference on Artificial Intelligence and Statistics (AISTATS) 2020, Palermo, Italy. PMLR: Volume 108. Copyright 2020 by the author(s).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Figure 1 (left)shows the accuracy of all methods. As expected, all linear models (including Perso-lin) perform poorly since the tasks have highly nonlinear decision boundaries. The results show the clear gain in accuracy provided by our method: both Dada-Oracle and Dada-Learned are successful in reducing the overfitting of Local-boost, and also achieve higher test accuracy than Global-boost. Dada-Oracle outperforms Dada-Learned as it makes use of the oracle graph computed from the true data distributions. Despite the noise introduced by the finite sample setting, Dada-Learned effectively makes up for not having access to any knowledge on the relations between the users' tasks. Figure1(right) shows that the graph learned by Dada-Learned remains sparse across time (in fact, always sparser than the oracle graph), ensuring a small communication cost for the model update steps. Figure2(left) confirms that the graph learned by Dada-Learned is able to approximately recover the ground-truth cluster structure. Figure2(right) provides a more detailed visualization of the learned graph. We can clearly see the effect of the inductive bias brought by the confidence-weighted loss term in Problem (1) discussed in Section 3. In particular, nodes with high confidence and high loss values tend to have small degrees while nodes with low confidence or low loss values are more densely connected.Real data. We present results on real datasets that are naturally collected at the user level: Human Activity Recognition With Smartphones (Harws, K = 30, D = 561)<ref type="bibr" target="#b1">(Anguita et al., 2013)</ref>, Vehicle Sensor<ref type="bibr" target="#b10">(Duarte and Hu, 2004)</ref> (K = 23, D = 100), Computer Buyers (K = 190, D = 14) and School (Goldstein, 1991) (K = 140, D = 17). As shown in Table</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Graph learned on Moons. Top: Graph weights for the oracle and learned graph (with users grouped by cluster). Bottom: Visualization of the graph. The node size is proportional to the confidence c k and the color reflects the relative value of the local loss (greener = smaller loss). Nodes are labeled with their rotation angle, and a darker edge color indicates a higher weight.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Test accuracy (%) on real data, averaged over 3 runs. Best results in boldface, second best in italic. Additional details and results can be found in the supplementary. The source code is available at https://github.com/vzantedeschi/Dada.</figDesc><table><row><cell>DATASET</cell><cell cols="4">HARWS VEH. COMP. SCH.</cell></row><row><cell>Global-linear</cell><cell>93.64</cell><cell>87.11</cell><cell>62.18</cell><cell>57.06</cell></row><row><cell>Local-linear</cell><cell>92.69</cell><cell>90.38</cell><cell>60.68</cell><cell>70.43</cell></row><row><cell>Perso-linear-Learned</cell><cell>96.87</cell><cell>91.45</cell><cell>69.10</cell><cell>71.78</cell></row><row><cell>Global-Adaboost</cell><cell>94.34</cell><cell>88.02</cell><cell>69.16</cell><cell>69.96</cell></row><row><cell>Local-Adaboost</cell><cell>93.16</cell><cell>90.59</cell><cell>66.61</cell><cell>70.69</cell></row><row><cell>Dada-Learned</cell><cell>95.57</cell><cell>91.04</cell><cell>73.55</cell><cell>72.47</cell></row><row><cell>users.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note><p>Figure 1: Results on the Moons dataset. Top: Training and test accuracy w.r.t. iterations (we display the performance of non-collaborative baselines at convergence with a straight line). Global-lin is off limits at ∼50% accuracy. Bottom: Average number of neighbors w.r.t. iterations for Dada-Learned.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Test accuracy (%) with different fixed communication budgets (# bits) on real datasets.</figDesc><table><row><cell cols="2">BUDGET MODEL</cell><cell cols="4">HARWS VEHICLE COMPUTER SCHOOL</cell></row><row><cell>DZ × 160</cell><cell>Perso-lin-Learned Dada-Learned</cell><cell>-95.70</cell><cell>-75.11</cell><cell>-52.03</cell><cell>-56.83</cell></row><row><cell>DZ × 500</cell><cell>Perso-lin-Learned Dada-Learned</cell><cell>81.06 95.70</cell><cell>89.82 89.57</cell><cell>-62.22</cell><cell>-71.90</cell></row><row><cell>DZ × 1000</cell><cell>Perso-lin-Learned Dada-Learned</cell><cell>87.55 95.70</cell><cell>90.52 90.81</cell><cell>68.95 68.83</cell><cell>71.90 72.22</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>This work was carried out while the author was affiliated with Univ Lyon, UJM-Saint-Etienne, CNRS, Institut d Optique Graduate School, Laboratoire Hubert Curien UMR 5516, France</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>Alternating optimization converges to a local optimum under mild technical conditions, see<ref type="bibr" target="#b37">(Tseng, 2001;</ref><ref type="bibr" target="#b38">Tseng and Yun, 2009;</ref><ref type="bibr" target="#b32">Razaviyayn et al., 2013)</ref>.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>For the general convex case, we can obtain a slower O(1/T ) convergence rate.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>The authors would like to thank <rs type="person">Rémi Gilleron</rs> for his useful feedback. This research was partially supported by grants <rs type="grantNumber">ANR-16-CE23-0016-01</rs> and <rs type="grantNumber">ANR-15-CE23-0026-03</rs>, by the <rs type="funder">European Union</rs>'s <rs type="programName">Horizon 2020 Research and Innovation Program</rs> under Grant Agreement No. <rs type="grantNumber">825081</rs> <rs type="projectName">COMPRISE</rs> and by a grant from <rs type="funder">CPER Nord-Pas de Calais/FEDER</rs> <rs type="programName">DATA Advanced data science</rs> and technologies 2015-2020.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_jctHnXk">
					<idno type="grant-number">ANR-16-CE23-0016-01</idno>
				</org>
				<org type="funding" xml:id="_BvXnwtX">
					<idno type="grant-number">ANR-15-CE23-0026-03</idno>
					<orgName type="program" subtype="full">Horizon 2020 Research and Innovation Program</orgName>
				</org>
				<org type="funded-project" xml:id="_tNpGkQM">
					<idno type="grant-number">825081</idno>
					<orgName type="project" subtype="full">COMPRISE</orgName>
					<orgName type="program" subtype="full">DATA Advanced data science</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">DJAM: Distributed Jacobi Asynchronous Method for Learning Personal Models</title>
		<author>
			<persName><forename type="first">I</forename><surname>Almeida</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xavier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1389" to="1392" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A public domain dataset for human activity recognition using smartphones</title>
		<author>
			<persName><forename type="first">D</forename><surname>Anguita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ghio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Oneto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Parra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Reyes-Ortiz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ESANN</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Asynchronous Multi-task Learning</title>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">M</forename><surname>Baytas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDM</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Personalized and Private Peer-to-Peer Machine Learning</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bellet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Guerraoui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Taziki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tommasi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AISTATS</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Graph Learning Based on Total Variation Minimization</title>
		<author>
			<persName><forename type="first">P</forename><surname>Berger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Buchacher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hannak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Matz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Randomized gossip algorithms</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">P</forename><surname>Boyd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Prabhakar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2508" to="2530" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Coresets, sparse greedy approximation, and the Frank-Wolfe algorithm</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">L</forename><surname>Clarkson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Algorithms</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="30" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Gossip dual averaging for decentralized optimization of pairwise functions</title>
		<author>
			<persName><forename type="first">I</forename><surname>Colin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bellet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Salmon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Clémençon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Semi-supervised multi-task learning of structured prediction models for web information extraction</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Dhillon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sellamanickam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Selvaraj</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="957" to="966" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning Laplacian matrix in smooth graph signal representations</title>
		<author>
			<persName><forename type="first">X</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Thanou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Frossard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Signal Processing</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="issue">23</biblScope>
			<biblScope unit="page" from="6160" to="6173" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Vehicle classification in distributed sensor networks</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>Duarte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">H</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Parallel and Distributed Computing</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="826" to="838" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Dual Averaging for Distributed Optimization: Convergence Analysis and Network Scaling</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Wainwright</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Automatic Control</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="592" to="606" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Differential Privacy</title>
		<author>
			<persName><forename type="first">C</forename><surname>Dwork</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICALP</title>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Regularized multitask learning</title>
		<author>
			<persName><forename type="first">T</forename><surname>Evgeniou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pontil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">An algorithm for quadratic programming</title>
		<author>
			<persName><forename type="first">M</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Wolfe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Naval Research Logistics (NRL)</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="95" to="110" />
			<date type="published" when="1956">1956</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Greedy function approximation: A gradient boosting machine</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Friedman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1189" to="1232" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Multilevel modelling of survey data</title>
		<author>
			<persName><forename type="first">H</forename><surname>Goldstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society. Series D (The Statistician)</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="235" to="244" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Revisiting Frank-Wolfe: Projection-Free Sparse Convex Optimization</title>
		<author>
			<persName><forename type="first">M</forename><surname>Jaggi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Gossip-based peer sampling</title>
		<author>
			<persName><forename type="first">M</forename><surname>Jelasity</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Voulgaris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Guerraoui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A.-M</forename><surname>Kermarrec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Van Steen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Comput. Syst</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Collaborative Deep Learning in Fixed Topology Networks</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Balu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hegde</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sarkar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Advances and Open Problems in Federated Learning</title>
		<author>
			<persName><forename type="first">P</forename><surname>Kairouz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">B</forename><surname>Mcmahan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Avent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bellet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bennis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Bhagoji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Bonawitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Charles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cormode</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cummings</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">G L</forename><surname>D'oliveira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">E</forename><surname>Rouayheb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Garrett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gascón</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ghazi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">B</forename><surname>Gibbons</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gruteser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Harchaoui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Huo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Hutchinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jaggi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Javidi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Khodak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Konečný</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Korolova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Koushanfar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Koyejo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lepoint</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mohri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Nock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Özgür</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Pagh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Raykova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ramage</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Raskar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">U</forename><surname>Stich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">T</forename><surname>Suresh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Tramèr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vepakomma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.04977</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">How to learn a graph from smooth signals</title>
		<author>
			<persName><forename type="first">V</forename><surname>Kalofolias</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AISTATS</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Converging quickly to independent uniform random topologies</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kermarrec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Leroy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Thraves</surname></persName>
		</author>
		<editor>PDP.</editor>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Federated learning: Strategies for improving communication efficiency</title>
		<author>
			<persName><forename type="first">J</forename><surname>Konečnỳ</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">B</forename><surname>Mcmahan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Richtárik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">T</forename><surname>Suresh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bacon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.05492</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Compiling Combinatorial Prediction Games</title>
		<author>
			<persName><forename type="first">F</forename><surname>Koriche</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Block-Coordinate Frank-Wolfe Optimization for Structural SVMs</title>
		<author>
			<persName><forename type="first">S</forename><surname>Lacoste-Julien</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jaggi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Pletscher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">D-FW: Communication efficient distributed algorithms for high-dimensional sparse optimization</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lafond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-T</forename><surname>Wai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Moulines</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Distributed Multi-task Learning for Sensor Network</title>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Arai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Baba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kashima</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Miwa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECML/PKDD</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Can Decentralized Algorithms Outperform Centralized Algorithms? A Case Study for Decentralized Parallel Stochastic Gradient Descent</title>
		<author>
			<persName><forename type="first">X</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-J</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Asynchronous Decentralized Parallel Stochastic Gradient Descent</title>
		<author>
			<persName><forename type="first">X</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">The Rademacher Complexity of Linear Transformation Classes</title>
		<author>
			<persName><forename type="first">A</forename><surname>Maurer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLT</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Communicationefficient learning of deep networks from decentralized data</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">B</forename><surname>Mcmahan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ramage</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hampson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Agüera Y Arcas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AISTATS</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A unified convergence analysis of block successive minimization methods for nonsmooth optimization</title>
		<author>
			<persName><forename type="first">M</forename><surname>Razaviyayn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z.-Q</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Optimization</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1126" to="1153" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Iteration complexity of randomized block-coordinate descent methods for minimizing a composite function</title>
		<author>
			<persName><forename type="first">P</forename><surname>Richtárik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Takác</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematical Programming</title>
		<imprint>
			<biblScope unit="volume">144</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="1" to="38" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">On the dual formulation of boosting algorithms</title>
		<author>
			<persName><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2216" to="2231" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Federated Multi-Task Learning</title>
		<author>
			<persName><forename type="first">V</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-K</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sanjabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Talwalkar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">D 2 : Decentralized Training over Decentralized Data</title>
		<author>
			<persName><forename type="first">H</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Convergence of a block coordinate descent method for nondifferentiable minimization</title>
		<author>
			<persName><forename type="first">P</forename><surname>Tseng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Optimization Theory and Applications</title>
		<imprint>
			<biblScope unit="volume">109</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="475" to="494" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Block-coordinate gradient descent method for linearly constrained nonsmooth separable optimization</title>
		<author>
			<persName><forename type="first">P</forename><surname>Tseng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Optimization Theory and Applications</title>
		<imprint>
			<biblScope unit="volume">140</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="140" to="513" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Decentralized Collaborative Learning of Personalized Models over Networks</title>
		<author>
			<persName><forename type="first">P</forename><surname>Vanhaesebrouck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bellet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tommasi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AISTATS</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Schapire</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1510.02558</idno>
		<title level="m">Functional Frank-Wolfe Boosting for General Loss Functions</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kolar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Srebro</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.02185</idno>
		<title level="m">Distributed Multi-Task Learning with Shared Representation</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Distributed Multitask Learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kolar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Srebro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AISTATS</title>
		<imprint>
			<date type="published" when="2016">2016b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Distributed Alternating Direction Method of Multipliers</title>
		<author>
			<persName><forename type="first">E</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">E</forename><surname>Ozdaglar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
		<respStmt>
			<orgName>CDC</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Coordinate descent algorithms</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Wright</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematical Programming</title>
		<imprint>
			<biblScope unit="volume">151</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3" to="34" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.08114</idno>
		<title level="m">A survey on multi-task learning</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
