<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Factorized Version Space Algorithm for &quot;Human-In-the-Loop&quot; Data Exploration</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Luciano</forename><forename type="middle">Di</forename><surname>Palma</surname></persName>
							<email>luciano.di-palma@polytechnique.edu</email>
						</author>
						<author>
							<persName><forename type="first">Yanlei</forename><surname>Diao</surname></persName>
							<email>yanlei.diao@polytechnique.edu</email>
						</author>
						<author>
							<persName><forename type="first">Anna</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">LIX Ecole Polytechnique</orgName>
								<address>
									<settlement>Palaiseau</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">LIX Ecole Polytechnique</orgName>
								<address>
									<settlement>Palaiseau</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">Dept. of Mathematics</orgName>
								<orgName type="institution">Statistics</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">University of Massachusetts Amherst</orgName>
								<address>
									<settlement>Amherst</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A Factorized Version Space Algorithm for &quot;Human-In-the-Loop&quot; Data Exploration</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">3D49B4079BC5DBE21D07D6F59A03DD1C</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-04-12T14:52+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Active learning</term>
					<term>version space</term>
					<term>data exploration</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>While active learning (AL) has been recently applied to help the user explore a large database to retrieve data instances of interest, existing methods often require a large number of instances to be labeled in order to achieve good accuracy. To address this slow convergence problem, our work augments version space-based AL algorithms, which have strong theoretical results on convergence but are very costly to run, with additional insights obtained in the user labeling process. These insights lead to a novel algorithm that factorizes the version space to perform active learning in a set of subspaces. Our work offers theoretical results on optimality and approximation for this algorithm, as well as optimizations for better performance. Evaluation results show that our factorized version space algorithm significantly outperforms other version space algorithms, as well as a recent factorization-aware algorithm, for large database exploration.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>In the setting of interactive data exploration (IDE), a user that comes to explore a large database is often driven by the goal of understanding some particular phenomenon. Such goals are treated as building a classification model over all objects in the database from no or very few labeled instances <ref type="bibr" target="#b0">[1]</ref>- <ref type="bibr" target="#b3">[4]</ref>; examples include classifying all the models in a car database as relevant or irrelevant to the interest of a customer, or classifying all the observations in a digital sky survey as relevant or irrelevant to the interest of a scientist. For IDE, active learning <ref type="bibr" target="#b4">[5]</ref> has been explored to derive an accurate model with minimum user labeling effort while offering interactive performance in presenting next data instances for the user to label <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>.</p><p>In IDE, however, existing active learning techniques <ref type="bibr" target="#b4">[5]</ref>- <ref type="bibr" target="#b7">[8]</ref> often fail to provide satisfactory performance when such models need to be built over large databases. For example, our evaluation results show that on a Sloan Digital Sky Survey (SDSS) dataset of 1.9 million data instances, the state-ofthe-art technique for IDE <ref type="bibr" target="#b2">[3]</ref> requires the user to label 200-300 instances in order to learn a classification model over 6 attributes with F-score <ref type="foot" target="#foot_0">1</ref> of 95%. Similar results can be observed for other active learning techniques <ref type="bibr" target="#b4">[5]</ref>- <ref type="bibr" target="#b6">[7]</ref>. Asking the user to label a large number of instances to achieve accuracy, referred to as slow convergence, is undesirable.</p><p>In this work, we aim to design new techniques to overcome the slow convergence problem by exploring two ideas:</p><p>Version Space Algorithms: First, we would like to leverage Version Space (VS) algorithms <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref> because they present a strong theoretical foundation for convergence. These algorithms model all possible configurations of a classifier that can correctly classify the current set of labeled data as a set of hypotheses forming a version space V, and aim to seek the next instance such that its acquired label will enable V to be reduced. The most well known example is the bisection rule <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, which among all unlabeled instances, looks for the one whose label allows V to be reduced by half or most close to that. It is shown theoretically to have near-optimal performance for convergence.</p><p>Implementing the bisection rule, however, is prohibitively expensive due to the exponential size of a version space. To reduce cost, various approximations have been proposed. Some of the most popular techniques are Simple Margin <ref type="bibr" target="#b6">[7]</ref>, Query-by-Disagreement <ref type="bibr" target="#b4">[5]</ref>, Query-by-Committee (QBC) <ref type="bibr" target="#b10">[11]</ref>, and ALuMA <ref type="bibr" target="#b5">[6]</ref>. The first two methods often suffer from suboptimal performance since they are very rough approximations of the bisection rule. On the other hand, QBC and ALuMA can better approximate the bisection rule by sampling the version space, which, however, is very costly to run on large databases. For example, ALuMA runs Hitand-Run sampling with thousands of steps to sample a single hypothesis, and repeats this procedure to sample 1000 hypotheses for estimating the instance's reduction power of the version space. As can be seen, new techniques are needed to enable VS algorithms to achieve both fast convergence and high efficiency on large databases.</p><p>Factorization. To make VS algorithms practical for large databases, our second idea is to augment them with additional insights obtained in the user labeling process. In particular, we observe that when a user labels a data instance, her decision making process often can be broken into a set of smaller questions, and the answers to these questions can be combined to derive the final answer. For example, when a customer decides whether a car model is of interest, she may have a set of questions in mind: "Is gas mileage good enough? Is the vehicle spacious enough? Is the color a preferred one?" While the user may have high-level intuition that each question is related to a subset of attributes (e.g., the space depends on the body type, length, width, and height), she is not able to specify these questions precisely. It is because she may not know the exact threshold value or the exact relation between a question and related attributes (e.g., how the space can be specified as a function of body type, length, width, and height). The above insight allows us to design new version space algorithms that leverage the high-level intuition a user has for breaking the decision making process, formally called a factorization structure, to combat the slow convergence problem.</p><p>More specifically, we make the following contributions: 1. A Factorized Version Space Algorithm (Section III): We propose a novel algorithm that leverages the factorization structure to create subspaces and factorizes the version space accordingly to perform active learning in the subspaces. Compared to recent work <ref type="bibr" target="#b2">[3]</ref> that also used factorization for active learning, our work explores it in the new setting of VS algorithms and completely eliminates the strong assumptions made in <ref type="bibr" target="#b2">[3]</ref> such as convex and conjunctive properties of user interest patterns, resulting in significant performance improvement.</p><p>2. Theoretical results (Section IV): We also provide theoretical results on the optimality of our factorized VS algorithm.</p><p>3. Optimizations (Section V): We further provide optimizations for sampling over a large version space.</p><p>4. Evaluation (Section VI): Evaluation results show that 1) for low dimensional problems, our optimized VS algorithm, without factorization, already outperforms existing VS algorithms including Simple Margin <ref type="bibr" target="#b6">[7]</ref>, Query-by-Disagreement <ref type="bibr" target="#b4">[5]</ref>, and ALuMA <ref type="bibr" target="#b5">[6]</ref>; 2) for higher dimensional problems, our factorized VS algorithm outperforms VS algorithms <ref type="bibr" target="#b4">[5]</ref>- <ref type="bibr" target="#b6">[7]</ref>, as well as DSM <ref type="bibr" target="#b2">[3]</ref>, a factorization-aware algorithm, often by a wide margin while maintaining interactive speed. For example, for a complex user interest pattern tested, our algorithm achieves F-score of over 90% after 100 iterations, while DSM is still at 40% and all other VS algorithms are at 10% or lower.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>In this section, we present most relevant results in Active Learning and Data Exploration Systems.</p><p>Active Learning. The recent results on active learning are surveyed in <ref type="bibr" target="#b4">[5]</ref>. Our work focuses on a common form called pool-based active learning. In this setting, there is a small set of labeled data L and a large pool of unlabeled data U available. In active learning, an example is chosen from the pool in a greedy fashion, according to a utility measure used to evaluate all instances in the pool (or, if U is large, a subsample thereof). In our setting of database exploration, the labeled data L is what the user has provided thus far. The pool U is a subsample of size m of the unlabeled part of the database. The utility measure varies with the classifier and algorithm used. We focus on version space algorithms below.</p><p>Version Space Algorithms. Version Space (VS) algorithms are a particular class of active learning procedures. In such a procedure, the learner starts with a set of possible classifiers (hypotheses), which we denote by H. The Version Space V is defined as the subset of classifiers h ∈ H consistent with the labeled data, i.e. h(x) = y for all labeled points (x, y). As new labeled data is obtained, the set V will shrink, until we are left with a single hypothesis. Various strategies have been developed to select new instances for labeling.</p><p>Generalized Binary Search <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>: Also called the Version Space Bisection rule, this algorithm searches for a point x for which the classifiers in V disagree the most; that is, the sets V x,y = {h ∈ V : h(x) = y} have roughly the same size, for all possible labels y. It has strong theoretical guarantees on convergence: the expected number of iterations needed to reach 100% accuracy is at most a constant factor larger than the optimal algorithm on average. Implementing the bisection rule, however, is prohibitively expensive: for each unlabeled instance x in the database, one must evaluate h(x) for each hypothesis h in the version space, which is exponential in size O(m d ), where m is number of unlabeled instances and d is the VC dimension <ref type="bibr" target="#b11">[12]</ref>. A number of approximations of the bisection rule have been introduced in the literature:</p><p>Simple Margin <ref type="bibr" target="#b6">[7]</ref>: As a rough approximation of the bisection rule for SVM classifiers, it leverages an heuristic that data points close to the SVM's decision boundary closely bisect the version space, V. However, it can suffer from suboptimal performance, specially when V is very asymmetric.</p><p>Query By Disagreement <ref type="bibr" target="#b4">[5]</ref>: This algorithm approximates the version space V by a positively biased and a negatively biased hypothesis, and selects a data point for which these two biased hypothesis disagree. Again, it also suffers from suboptimal performance since the selected point is only guaranteed to "cut" V, but possibly not by a large amount.</p><p>Query by Committee (QBC) <ref type="bibr" target="#b10">[11]</ref> and ALuMA <ref type="bibr" target="#b5">[6]</ref>: QBC <ref type="bibr" target="#b10">[11]</ref> attempts to estimate how much each point reduces the version space V through a sampling technique. It is a much better approximation of the bisection rule, but works by taking a single pass of the entire dataset, hence not suitable for poolbased sampling. ALuMA <ref type="bibr" target="#b5">[6]</ref> is a "pool-based" version of QBC and uses a different technique for sampling the version space. It is shown to outperform QBC. Hence, we use ALuMA as a baseline version space algorithm in this work.</p><p>Data Exploration Systems. In the data exploration domain, a main objective is to design a database system that guides the user towards discovering relevant records in a large database. One example is Snorkel <ref type="bibr" target="#b12">[13]</ref>, a data programming framework where an expert user writes several labeling functions representing simple heuristics used for labeling a data point. By leveraging the prediction of several such functions, Snorkel is capable of building an accurate classifier without having the user manually label any data point.</p><p>Another line of work is "explore-by-example" systems [1]- <ref type="bibr" target="#b3">[4]</ref>, which leverage active learning to obtain a small set of user labeled data points for building an accurate model of the user interest. These systems require minimizing both the user labeling effort and running time in each iteration to find a new data point for labeling. In particular, recent work <ref type="bibr" target="#b2">[3]</ref> is shown to outperform prior work <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref> via the following technique:</p><p>Dual-Space Model (DSM) <ref type="bibr" target="#b2">[3]</ref>: In this work, the user interest pattern P is assumed to form a convex object in the data space. For such a pattern, this work proposes a "dual-space model" (DSM), which builds not only a classifier but also a polytope model of the data space D, offering information including the areas known to be positive and areas known to be negative. It uses both the polytope model and the classifier to decide the best instance to choose for labeling next. In addition, DSM explores factorization in a limited form: when the user pattern P is a conjunction of sub-patterns on non-overlapping attributes, it factorizes the data space into low-dimensional spaces and runs the dual-space model in each subspace.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. A FACTORIZED VERSION SPACE ALGORITHM</head><p>To improve the efficiency of version space (VS) algorithms on large databases, we aim to augment them with additional insights obtained in the user labeling process. In particular, we observe that when a user labels a data instance, her decision making process often can be broken into a set of smaller "yes" or "no" questions, which can be answered independently, and these answers can be combined to derive the final answer. Revisit the previous example: when a customer decides whether a car model is of interest, she has three questions in mind:</p><p>Q 1 : Is gas mileage good enough? Q 2 : Is the vehicle spacious enough? Q 3 : Is the color a preferred one? We do not expect the user to specify her questions precisely as classification models (which requires knowing the exact shape of the decision function and all constants used), but rather to have a high-level intuition of the set of questions and to which attributes each question is related.</p><p>Factorization Structure. Formally, we model such an intuition of the set of questions and the relevant attributes as a factorization structure: Let us model the decision making process using a complex question Q defined on an attribute set A of size d. Based on the user intuition, Q can be broken into smaller independent questions, Q 1 , . . . , Q K , where each Q k is posed on a subset of attributes</p><formula xml:id="formula_0">A k = {A k1 , • • • , A kd k }. The family of attribute sets, (A 1 , • • • , A K ), |A 1 ∪ • • • ∪ A K | ≤ d may be disjoint,</formula><p>or overlapping in attributes as long as the user believes that decisions for these smaller questions can be made independently. In our work</p><formula xml:id="formula_1">(A 1 , • • • , A K ) is referred to as the factorization structure.</formula><p>Note that the factorization structure needs to be provided by the user as it reflects her understanding of her own decision making process. The independence assumption in the decision process should not be confused with the data correlation issue. For example, the color and the size of cars can be statistically correlated, e.g., large cars are often black in color. But the user decision does not have to follow the data characteristics; e.g., the user may be interested in large cars that are red. As long as the user believes that her decision for the color and that for the size are independent, the factorization structure ({color}, {size}) applies. To the contrary, if the two attributes are not independent in the decision making process, e.g., the user prefers red if the car is small and black if the car is large, but the year of production is an independent concern, then the factorization structure can be ({color size}, {year}).</p><p>Decision functions. Given a data instance x, we denote x k = proj(x, A k ) the projection of x over attributes A k . We use Q k (x k ) → {-, +} to denote the user decision on x k . Then we assume that the final decision from the answers to these questions is a boolean function F : {-, +} K → {-, +}:</p><formula xml:id="formula_2">Q(x) = F (Q 1 (x 1 ), . . . , Q K (x K ))<label>(1)</label></formula><p>In this work, we assume that the decision function F is given by the user. The most common example is the conjunctive form, Q 1 (x 1 ) ∧ . . . ∧ Q K (x K ), meaning that the user requires each small question to be + to label the overall instance with +. Given that any boolean expression can be written in the conjunctive normal form, Q 1 (x 1 )∧. . .∧Q K (x K ) already covers a large class of decision problems, while our work also supports other decision functions that use ∨.</p><p>Given the decision function F , we aim to learn the subspatial decision functions, {Q 1 (x 1 ), . . . , Q K (x K )}, efficiently from a small set of labeled instances. For a labeled instance x, the user provides a collection of subspatial labels (y 1 , . . . , y K ) ∈ {-, +} K to enable learning.</p><p>Generality. We note the differences of our factorization framework from <ref type="bibr" target="#b2">[3]</ref>: First, one of the main assumptions in <ref type="bibr" target="#b2">[3]</ref> is that the set</p><formula xml:id="formula_3">{x k : Q k (x k ) = +} or the set {x k : Q k (x k ) = -}</formula><p>must be a convex object, which is eliminated in this work. Second, the global decision function F must be conjunctive in <ref type="bibr" target="#b2">[3]</ref>, which is relaxed to any boolean expression in our work. Third, our factorization is applied to version space algorithms, as shown below, while <ref type="bibr" target="#b2">[3]</ref> does not consider them at all.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Introduction to Factorized Version Space</head><p>We now give an intuitive description of factorized version space (while we defer a formal description to Section IV).</p><p>Without factorization, our problem is to learn a classifier C (e.g., a SVM classifier) on the attribute set A from a labeled data set, L = {(x i , y i )}, where x i is a data instance containing values of A, and y i ∈ {-, +}. The version space V includes all possible configurations of C (e.g., all possible weight vectors of the SVM) that are consistent with L.</p><p>Given a factorization structure (A 1 , • • • , A K ), we define K subspaces, where the k th subspace includes all the data instances projected on A k . Then our goal is to learn a classifier C k for each subspace k from its labeled set, L k = {(x k i , y k i )}, where y k i is the subspatial label for x k i . For the classifier C k , its version space, V k , includes all possible configurations of C k that are consistent with L k . Across all subspaces, we can reconstruct the version space via Ṽ = V 1 × . . . V K . For any unlabeled instance, x, we can use F (C 1 (x 1 ), . . . , C K (x K )) to predict a label.</p><p>At this point, the reader may wonder what benefit factorization provides in the learning process. We use the following example to show that factorization may enable faster reduction of the version space, hence enabling faster convergence to the correct classification model.</p><p>Example. Figure <ref type="figure" target="#fig_0">1</ref> shows an example that the user considers the color and the size of cars, where the color can be black (B) or red (R) and the size can be large (L) or small (S). Therefore, there can be four types of cars corresponding to different color and size combinations. Figure <ref type="figure" target="#fig_1">1(a)</ref> shows that without factorization, and in the absence of any user labeled data, the version space contains 16 possible classifiers that correspond to 16 combinations of the {-, +} labels assigned to the four types of cars. Once we obtain the '-' label for the type BL (color = Black and size = Large), the version space is reduced to 8 classifiers that assign the '-' label to BL.</p><p>Next consider factorization. In the absence of labeled data, Figure <ref type="figure" target="#fig_0">1</ref>(b) shows that the subspace for color includes two types of cars, B and R, and its version space includes 4 possible classifiers that correspond to 4 combinations of the {-, +} labels of these two types of cars. Similarly, the subspace for size also includes 4 classifiers. Combining the color and size, we have 16 possible classifiers. Once BL is labeled, this time, with two subspatial labels, ((B,-), (L,+)), each subspace has only two classifiers left, yielding 4 remaining classifiers across the two subspaces. As can be seen, with factorization each labeled instance offers more information and hence can lead to faster reduction to the version space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Overview of A Factorized Version Space Algorithm</head><p>Based on the above insight, we propose a new active learning algorithm, called a factorized version space algorithm. It leverages the factorization structure provided by the user to create subspaces, and factorizes the version space accordingly to perform active learning in the subspaces.</p><p>Algorithm 1 shows the pseudo-code of our algorithm. It starts by taking a labeled dataset (which can be empty), and creating a memory-resident sample from the underlying large database as an unlabeled pool U (line 1) to enable efficiency for interactive performance. It then proceeds to an iterative procedure (lines 2-11): In each iteration, it may further subsample the unlabeled pool to obtain U to expedite learning (line 3). Then the algorithm considers each subspace (lines 4-8), including the both labeled instances, (x k , y k ) ∈ L k , and unlabeled instances, x k ∈ U , projected to this subspace. The key step is to compute for each unlabeled instance, x, for each subspace k do 5:</p><formula xml:id="formula_4">U k ← {x k , for x ∈ U } 6: L k ← {(x k , y k ), for (x, y) ∈ L} 7: {p k (x)} x∈U ← positive cut probability(U k , L k , M ) 8:</formula><p>end for 9:</p><formula xml:id="formula_5">x * ← arg min x∈U k (1 -2(1 -p k (x))p k (x)) 10:</formula><p>y * ← get labels from user(x * ) 11:</p><formula xml:id="formula_6">L ← L ∪ {(x * , y * )}, U ← U/{x * } 12: end while 13: C k ← train majority vote classifier(L k ), k = 1, . . . , K 14: return x → F (C 1 (x 1 ), . . . , C K (x K ))</formula><p>how much its projection x k can reduce the version space V k once its label is acquired (line 7). This step requires efficient sampling of the version space, V k , which is a main focus of Section V. Once the above computation completes for all subspaces, the algorithm chooses the next unlabeled instance that can offer best reduction of the factorized version space, Ṽ = V 1 × . . . V K (line 9). The derivation of this strategy, as well as the proof of its optimality, is detailed in Section IV. The selected instance is then presented to the user for labeling and the unlabeled pool is updated. The algorithm then proceeds to the next iteration. Once the user wishes to stop exploring, we train a majority vote classifier <ref type="bibr" target="#b5">[6]</ref> for each subspace k. The majority vote classifier, C, is constructed by first computing a sample of hypotheses from the version space. Then for any point x, C(x) is computed as the most frequent label across the sample. Given the classifiers, (C 1 , . . . , C K ), for the subspaces, we build a final classifier F (C 1 , . . . , C K ) (line 14), which can then be used to retrieve all the data instances of interest to the user from the database D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. THEORETICAL ANALYSIS A. Bisection Rule over Factorized Version Space</head><p>Let X = {x i } N i=1 be the collection of unlabeled data points, and let y i ∈ Y represent the unknown label of x i . The user interest pattern can be modeled as a hypothesis function h : X → Y, and we denote by H the set of all hypotheses. We also assume a known probability distribution π(h), representing our prior knowledge over which hypotheses are more likely to match the user preference.</p><p>The version space is the set of all hypotheses consistent with the labeled set L: V = {h ∈ H : h(x) = y, ∀(x, y) ∈ L}.</p><p>Our factorized version space algorithm is based on a well known concept called Version Space Bisection rule (or Generalized Binary Search). It searches for the point x which most evenly divides the version space across all classes:</p><formula xml:id="formula_7">arg max x 1 - y∈Y p 2 x,y<label>(2)</label></formula><p>where p x,y = π V (V x,y ). Here π V is the probability distribution π normalized over the version space V and V x,y = {h ∈ V : h(x) = y}. Thus, the greedy strategy searches for the point x for which the sets V x,y have approximately the same probability mass for every possible label y of x. Now, let's suppose that a factorization structure (A 1 , • • • , A K ) is given. For each subspace k, the user labels the projection x k of x over A k based on a hypothesis from a hypothesis class H k with prior probability distribution π k . The user then provides a binary label {-, +} for each subspace; in other words, for each x a collection of subspatial labels (y 1 , . . . , y K ) ∈ {-, +} K is provided by the user.</p><p>Definition 1: Factorized hypothesis function and factorized hypothesis space: Let Y f = {-, +} K . We define the factorized hypothesis function as the function H : X → Y f such that</p><formula xml:id="formula_8">H(x) = (h 1 , . . . , h K )(x) = (h 1 (x 1 ), . . . , h K (x K )).</formula><p>H belongs to the product space H = H 1 × . . . × H K , which we call the factorized hypothesis space.</p><p>We assume that the user labels the subspaces independently and consistently within each subspace.</p><p>Definition 2: Factorized version space: Let L be the set of instances that have been labeled over subspaces at any iteration of active learning. Define the factorized version space as Ṽ = {H ∈ H : H(x) = y for all (x, y) ∈ L}. We can see that</p><formula xml:id="formula_9">Ṽ = k V k = V 1 × . . . × V K , where V k = {h ∈ H k : h(x k ) = y k for all (x, y) ∈ L} is the version space at subspace k.</formula><p>Given the assumption of independent labeling among the subspaces, the prior probability distribution over the factorized hypothesis space is π = π 1 × . . . × π K .</p><p>Definition 3: Factorized greedy selection strategy: Let p x,y = π Ṽ ( Ṽx,y ), the strategy is defined as</p><formula xml:id="formula_10">arg max x 1 - y∈Y f p 2 x,y .<label>(3)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Optimal properties</head><p>For the version space bisection rule, it has been shown [10] that the greedy strategy in (2) enjoys a near-optimal performance guarantee: the average number of labeled instances by the greedy algorithm is no larger than:</p><formula xml:id="formula_11">OP T • 1 + ln 1 min h π(h) 2 ,<label>(4)</label></formula><p>where OP T is the minimum expected number of iterations across all active learning algorithms that continue until the hypothesis matching the user interest has been found. Factorized greedy selection strategy. Applying the bisection rule to (X , Y, H, π), and noting that min H∈ H π(H) = k min h k ∈H k π k (h k ), the following theorem is the direct extension of the above result.</p><p>Theorem 1 (Number of iterations with factorization): The factorized greedy strategy in (3) takes at most:</p><formula xml:id="formula_12">OP T f • 1 + k ln 1 min h k π k (h k ) 2 (5)</formula><p>iterations in expectation to identify a hypothesis randomly drawn from π, where OP T f is the minimum number of iterations across all strategies that continue until the true hypothesis over all subspaces has been found.</p><p>In the following, we derive a simplified computation of the factorization greedy strategy (3).</p><p>Theorem 2:</p><formula xml:id="formula_13">Let p x k ,+ = π k V k (V k x k ,+ ), the factorized greedy selection strategy (3) is equivalent to arg max x 1 - k (1 -2p x k ,+ (1 -p x k ,+ )).<label>(6)</label></formula><p>Proof: First, by noting that Ṽx,y</p><formula xml:id="formula_14">= k V k x k ,y k , it implies π Ṽ ( Ṽx,y ) = k π k V k (V k x k ,y k ). Therefore: y∈Y f p 2 x,y = y1=± . . . y K =± k π k V k (V k x k ,y k ) 2 = k y k =± π k V k (V k x k ,y k ) 2 = k (p 2 x k ,+ + (1 -p x k ,+ ) 2 ) = k (1 -2p x k ,+ (1 -p x k ,+ )) V. OPTIMIZATIONS</formula><p>The main difficulty in implementing the greedy selection strategies, as discussed in the previous section, lies in efficient computation of the cut probability p x,+ = π V (V x,+ ). For this, we adopt a sampling approximation <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b10">[11]</ref> :</p><formula xml:id="formula_15">p x,+ = P(H(x) = +) ≈ 1 M M i=1 1(H i (x) = +)<label>(7)</label></formula><p>where {H i } M i=1 is a i.i.d. sample from π V . Thus, our problem is to develop a sampling algorithm for hypotheses.</p><p>Our sampling procedure closely follows <ref type="bibr" target="#b5">[6]</ref>. We first consider the class of homogeneous linear classifiers:</p><formula xml:id="formula_16">H lin = {h w : h w (x) = sign(w T x) for w ≤ 1} (8)</formula><p>We can then improve the generalization power of this class in two ways. First, we can add a bias b to the linear classifier by simply adding a dummy feature of value 1 to each data point. Second, we can choose a kernel function k(•, •) and replace each data point x by its kernel representation (k(x, x 1 ), . . . , k(x, x t )), where {x 1 , . . . , x t } is the collection of data points labeled so far. Now, assume we have a labeled set L = {(x i , y i )}, with y i ∈ {-1, 1}. The version space is the set of all h w with w restricted to the convex set:</p><formula xml:id="formula_17">W = {w ∈ R d : w ≤ 1 ∧ y i x T i w &gt; 0}<label>(9)</label></formula><p>Luckily, sampling from convex sets is a well-know problem in computational geometry. It can be solved by the Hit-and-Run algorithm <ref type="bibr" target="#b13">[14]</ref>, which creates a random-walk inside the polytope which converges to the uniform distribution. A more detailed explanation on how to implement this step can be found in the appendix A.</p><p>Rounding. The hit-and-run algorithm itself can have a large mixing time, specially in cases where the convex body is very elongated in one direction. To solve this problem, we adopt a rounding procedure <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>. Basically, it consists of finding a linear transformation T for which the convex body T (W ) is more evenly elongated across all directions. The Hit-and-Run sampling is run over T (W ), with the final sampling over W being obtained through the inverse transformation T -1 . Computing the rounding transformation is done in two steps:</p><p>1) Find (an approximation of) the ellipsoid E of minimum volume containing W . 2) Set T as any linear isomorphism taking E into a ball of radius 1. More details on how to implement these steps can be found in the appendix B.</p><p>Hit-and-Run's starting point. Hit-and-Run starts by computing a point W 0 inside W . Although this could be done by solving a linear programming task, it can take a significant amount of time. Instead, we rely on the rounding procedure: the ellipsoid E computed satisfies γE ⊂ W ⊂ E, where γE is obtained by shrinking all axes of E by some 0 &lt; γ &lt; 1. This property guarantees that the center of E must be inside W , which can then be used as initial point.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. EXPERIMENTS</head><p>We implemented all of our techniques in a Java-based prototype, which connects to a PostgreSQL database. In this section, we evaluate these techniques against state-of-the-art active learning algorithms <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b4">[5]</ref>- <ref type="bibr" target="#b6">[7]</ref> in terms of accuracy (using F-score) and efficiency (execution time in each iteration).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Experimental Setup</head><p>We evaluate our techniques using the Sloan Digital Sky Survey dataset (SDSS, 190 million tuples). This dataset contains the "PhotoObjAll" table with 510 attributes<ref type="foot" target="#foot_1">2</ref> and 190 million sky observations. We used a 1% sample (1.9 million tuples, 4.9GB) for running active learning algorithms. SDSS also offers a query release, where the SQL queries reflect the data interest of scientists. We extracted 11 queries to build a benchmark, which can be found in Appendix C, and treated them as the ground truth of the positive classes of 11 classifiers to be learned -our system does not need to know these queries in advance, but can learn them via active learning. These queries reflect different dimensionality (2D-7D) and complexity of the decision boundary (e.g., various combinations of linear, quadratic, log patterns). Algorithms: We compare our algorithms to state-of-the-art VS algorithms, Simple Margin (SM) <ref type="bibr" target="#b6">[7]</ref>, Query By Disagreement (QBD) <ref type="bibr" target="#b4">[5]</ref>, and ALuMA <ref type="bibr" target="#b5">[6]</ref>, and another factorization-aware algorithm, DSM <ref type="bibr" target="#b2">[3]</ref>. In our experiments, each active learning algorithm starts with one positive example and one negative example, and runs up to 100 additional labeled examples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX A HIT-AND-RUN IMPLEMENTATION</head><p>Let W ⊂ R d be a convex body. The Hit-and-Run algorithm is a randomized algorithm for sampling a point x ∈ W uniformly at random. More precisely, it generates a Markov Chain inside W which converges to the uniform distribution; starting at any given point X 0 ∈ W , it iteratively performs two steps: 1) Sample a direction vector D uniformly at random over the unit sphere 2) Set X t+1 as a random point on the line segment {s ∈ R : X t + sD} ∩ W Implementing step 1 can be easily done through the Marsaglia Algorithm <ref type="bibr" target="#b16">[17]</ref>: simply sample D ∼ N (0, I d ) and set D ← D/ D . As for step 2, the main difficulty is to find the intersections of the line L = {s ∈ R : X t + sD} with the boundary of W . Although there are methods for finding these extremes for a general convex body W , we will focus to the particular case of a polytope P = {x : Ax ≥ 0} intersected with the unit ball: W = P ∩ B(0, 1). In this case, it is easy to see that for the polytope:</p><formula xml:id="formula_18">X t + sD ∈ P ⇐⇒ A(X t + sD) ≥ 0 ⇐⇒ sAD ≥ -AX t ⇐⇒ sa T i D ≥ -a T i X t , for all i ⇐⇒ max a T i D&gt;0 - a T i X t a T i D ≤ s ≤ min a T i D&lt;0 - a T i X t a T i D</formula><p>and, for the unit ball (keeping in mind that D = 1):</p><formula xml:id="formula_19">X t + sD ∈ B(0, 1) ⇐⇒ X t + sD 2 ≤ 1 ⇐⇒ s 2 + 2(X T t D)s + X t 2 -1 ≤ 0 ⇐⇒ -X T t D - √ ∆ ≤ s ≤ -X T t D + √ ∆ where ∆ = (X T t D) 2 -X t 2 + 1. Note that ∆ &gt; 0 since X t ∈ B(0, 1)</formula><p>. Finally, we simply need to sample S uniformly from the intersection of the above two intervals and set X t+1 = X t + SD .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX B ROUNDING IMPLEMENTATION</head><p>The Rounding algorithm <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b17">[18]</ref> is a preprocessing method devised to improve the mixing time of the Hit-and-Run Markov chain. Essentially, the mixing time tends to be very high when a convex body W ⊂ R d is very elongated into some direction. In order to counter this problem, the rounding procedure looks for a linear transformation T : R d → R d for which the image T (W ) is "rounder", i.e. evenly elongated into all directions. With this, Hit-and-Run can be run over T (W ) and the final sampling over W can be retrieved via T -1 .</p><p>First, let's see how the rounding transformation T affects the hit-and-run chain generation. Let X 0 ∈ W be the chain's starting point, and let's define Y 0 = T (X 0 ) ∈ T (W ). The usual Hit-and-Run algorithm over T (W ) gives rise to a chain {Y t }, which is incrementally defined by Y t+1 = Y t + s t+1 D t+1 . By applying T -1 on the previous equation, and setting X t = T -1 (Y t ), we obtain a revised version of the Hit-and-Run update rule:</p><formula xml:id="formula_20">X t+1 = X t + s t+1 T -1 D t+1<label>(10)</label></formula><p>Now, all it remains is how to compute T -1 . For this, we follow the algorithm described in <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>. In general terms, this algorithm finds an approximation of the minimum volume ellipsoid E containing W . The rounding transformation can then be chosen as any linear transformation taking E into a unit-radius ball. Implementation details can be found on algorithm 2.</p><p>As a last remark, this algorithm assumes that the convex body W possesses a separation oracle; in other words, for any point x / ∈ W , we can find a hyperplane H(x) = {y : b x + w T x y = 0} such that W ⊂ H(x) + = {y : b x + w T x y ≥ 0} and x ∈ H(x) -= {y : b x + w T x y ≤ 0}. In the particular case of W = {x : Ax ≥ 0} ∩ B(0, 1), H(x) is given by: H ← get separating hyperplane(W, z)</p><p>8:</p><p>z, P ← ellipsoid method update(z, P, H) Q, D ← eigendecomposition(P )</p><p>11:</p><formula xml:id="formula_21">a ± k ← z ± 1 d+1 q k 12:</formula><p>if any a ± k / ∈ W then 13:</p><p>converged ← f alse 14:</p><p>H ← get separating hyperplane(W, a ± k )</p><p>15:</p><p>z, P ← ellipsoid method update(z, P, H)  AND (g -r &gt; 1.0) AND ((r -i &lt; 0.08 + 0.42 * (g -r -0.96)) OR (g -r &gt; 2.26)) AND (i -z &lt; 0.25)</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Illustration of factorization on the version space.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Algorithm 1 A</head><label>1</label><figDesc>Factorized Version Space Algorithm Input: database D, initial labeled set L 0 , per iteration subsample size m, version space sample size M 1: L ← L 0 , U ← D 2: while user is still willing do 3: U ← subsample(U, m) 4:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Algorithm 2</head><label>2</label><figDesc>Rounding algorithmInput: convex body W ⊂ R d , any R ≥ sup w∈W w Output: T -1 , the inverse of the rounding transformation 1: z ← 0, P ← 1 R I d 2: converged ← f alse 3: while not converged do</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>3 : P ← d 2 ( 1 -α 2 ) d 2 - 1 P</head><label>321221</label><figDesc>Input: ellipsoid E(z, P ) in R d , cutting hyperplane H(b, w) Output: The minimum volume ellipsoid containingE ∩ H - 1: α ← b+w T z √ w T P w 2: z ← z --2(1-dα) (d+1)(1-α) P ww T P 4: return E(z , P ) H(x) = {y : -1 + 1 x x T y = 0}, if x &gt; 1 {y : a T i y = 0}, if a T i x &lt; 0 ,selectivity 0.1%): rowc ∈ (662.5, 702.5) AND colc ∈ (991.5, 1053.5) Q2 (2D, 0.1%): (rowc -682.5) 2 + (colc -1022.5) 2 &lt; 29 2 Q3 (2D, 0.1%): ra ∈ (190, 200) AND dec ∈ (53, 57) Q4 (2D, 0.1%): rowv 2 + colv 2 &gt; 0.5 2 Q5 (4D, 0.01%): (rowc -682.5) 2 + (colc -1022.5) 2 &lt; 90 2 AND ra ∈ (180, 210) AND dec ∈ (50, 60) Q6 (6D, 0.01%): (rowc -682.5) 2 + (colc -1022.5) 2 &lt; 280 2 AND ra ∈ (150, 240) AND dec ∈ (40, 70) AND rowv 2 + colv 2 &gt; 0.2 2 Q7 (4D, 7.8%) : x 1 &gt; (1.35 + 0.25 * x 2 ) AND x 3 + 2.5 * log(2 * 3.1415 * petror50 r 2 ) &lt; 23.3 Q8 (7D, 5.5%): (dered r -dered i) &lt; 2 AND cmodelmag iextinction i ∈ [17.5, 19.9] AND (dered r -dered i) -(dered gdered r)/8. &gt; 0.55 AND f iber2mag i &lt; 21.7 AND devrad i &lt; 20. AND dered i &lt; 19.86 + 1.60 * ((dered r -dered i) -(dered gdered r)/8. -0.80) Q9 (5D, 1.5%): u -g &lt; 0.4 AND g -r &lt; 0.7 AND r -i &gt; 0.4 AND i -z &gt; 0.4 Q10 (5D, 0.5%): (g &lt;= 22) AND (u -g ∈ [-0.27, 0.71]) AND (g -r ∈ [-0.24, 0.35]) AND (r -i ∈ [-0.27, 0.57]) AND (i -z ∈ [-0.35, 0.70]) Q11 (5D, 0.1%): ((u -g &gt; 2.0) OR (u &gt; 22.3)) AND (i ∈ [0, 19])</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>F-score is a better measure for exploring a large database than classification error given that the user interest, i.e., the positive class, often covers only a small fraction of the database; classifying all instances to the negative class has low classification error but poor F-score.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>http://www.sdss3.org/dr8/ Servers: Our experiments were run on four servers, each</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>with 40-core Intel(R) Xeon(R) CPU E5-2640 v4 @ 2.40GHz, 128GB memory, OpenJDK 1.8.0 on CentOS 7. Our factorized algorithm used one core for sampling in each subspace.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Evaluating Our Techniques using SDSS</head><p>Expt 1 (Optimization of Hit-and-Run): We first investigate the effect of Rounding on the Hit-and-Run sampling, as proposed in Section V. The resulting algorithm, denoted as "Opt VS", is compared to a baseline without such optimization, which is ALuMA <ref type="bibr" target="#b5">[6]</ref>. For all 11 SDSS queries, Opt VS significantly outperforms ALuMa, while Figure <ref type="figure">2</ref>(a) shows the F-scores of Q2 and Q3.</p><p>Expt 2 (Factorization): We next study the effect of factorization, by comparing Opt VS with its extension to factorization, denoted as "Fact VS". For factorization, each predicate in the target query corresponds to its own factorized subspace. Q1-Q4 are 2D queries and not factorized. Q5-Q7 are factorized where each predicate corresponds to each subspace with no overlap of attributes across subspaces, while Q8-Q11 are also factorized but with overlap of attributes across subspaces. For Q5-Q11, Fact VS outperforms Opt VS, often by a wide margin. Figure <ref type="figure">2(b)</ref> show the F-score for Q6 and Q10. In the particular case of Q6, the factorized version reaches &gt; 90% F-Score after 100 iterations while the non-factorized version is still at less &lt; 10%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Comparing to Other Methods using SDSS</head><p>Expt 3 (A comparative study): In figures 2(c)-2(e), we compare to three VS algorithms, Simple Margin (SM) <ref type="bibr" target="#b6">[7]</ref>, Query By Disagreement (QBD) <ref type="bibr" target="#b4">[5]</ref>, and ALuMA <ref type="bibr" target="#b5">[6]</ref>, as well as DSM <ref type="bibr" target="#b2">[3]</ref>, a factorization-aware algorithm.</p><p>We first consider the case without factorization. Our Opt VS algorithm outperforms the three VS algorithms <ref type="bibr" target="#b4">[5]</ref>- <ref type="bibr" target="#b6">[7]</ref> and DSM <ref type="bibr" target="#b2">[3]</ref> most time for the 2D queries, Q1-Q4, that do not use factorization. Figure <ref type="figure">2</ref>(c) show the results for Q2. During the initial iterations, Opt VS improves much faster than other algorithms, and remains the best across all iterations.</p><p>We next consider factorization. Our Fact VS almost always outperforms others, including DSM that uses factorization under stronger assumptions. Figures <ref type="figure">2(d</ref>) and 2(e) show the results for Q6 and Q10. In general, Fact VS outperforms DSM, which in turn outperforms all other (non-factorized) algorithms. In the particular case of 2(d), after 100 iterations Fact VS is at &gt; 90% accuracy, while DSM is still at merely 40% and all of the other alternatives are at 10% or lower.</p><p>Finally, Figure <ref type="figure">2</ref>(f) shows the running time of Fact VS, DSM, and ALuMA for Q10, our most expensive query. ALuMA is consistently more expensive than Fact VS. The two factorized algorithms take at most a couple of seconds per iteration, thus better suiting the interactive data exploration scenario. Moreover, DSM exhibits a large warm-up time and is slower than Fact VS during the first 60 iterations. Thus, Fact VS may be preferred to DSM and ALuMA given its better accuracy and lower time per iteration in initial iterations. To overcome the slow convergence of active learning (AL) in large database exploration, we presented a new algorithm that augments version space-based AL algorithms, which have strong theoretical results on convergence but are costly to run, with insights on the factorization structure employed in the user labeling process. The resulting algorithm factorizes the version space to perform active learning in a set of subspaces, with provable results on optimality and optimizations for performance. Evaluation results using real world datasets show that our algorithm significantly outperforms state-of-the-art version space algorithms, as well as a recent factorizationaware algorithm, for large database exploration.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Explore-by-example: an automatic query steering framework for interactive data exploration</title>
		<author>
			<persName><forename type="first">K</forename><surname>Dimitriadou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Papaemmanouil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Diao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGMOD Conference</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">AIDE: An active learning-based approach for interactive data exploration</title>
		<author>
			<persName><forename type="first">K</forename><surname>Dimitriadou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Papaemanouil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Diao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TKDE</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Optimization for active learning-based interactive database exploration</title>
		<author>
			<persName><forename type="first">E</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">D</forename><surname>Palma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Abdelkafi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Diao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the VLDB Endowment</title>
		<meeting>the VLDB Endowment</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">An analysis of query-agnostic sampling for interactive data exploration</title>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Diao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications in Statistics -Theory and Methods</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">B</forename><surname>Settles</surname></persName>
		</author>
		<title level="m">Active Learning</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Efficient Active Learning of Halfspaces: an Aggressive Approach</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gonen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sabato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shalev-Shwartz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Support Vector Machine Active Learning with Applications to Text Classification</title>
		<author>
			<persName><forename type="first">S</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Koller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Active Boosted Learning (ActBoost)</title>
		<author>
			<persName><forename type="first">K</forename><surname>Trapeznikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Saligrama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Castañón</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics</title>
		<meeting>the Fourteenth International Conference on Artificial Intelligence and Statistics</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Analysis of a greedy active learning strategy</title>
		<author>
			<persName><forename type="first">S</forename><surname>Dasgupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Adaptive Submodularity : A New Approach to Active Learning and Stochastic Optimization</title>
		<author>
			<persName><forename type="first">D</forename><surname>Golovin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Krause</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Time</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Query by Committee Made Real</title>
		<author>
			<persName><forename type="first">R</forename><surname>Gilad-Bachrach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Navot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Tishby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">An Introduction to Computational Learning Theory</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Kearns</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Vazirani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Data programming: Creating large training sets, quickly</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Ratner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>De Sa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Selsam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ré</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Hit-and-run mixes fast</title>
		<author>
			<persName><forename type="first">L</forename><surname>Lovász</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematical Programming</title>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><forename type="first">L</forename><surname>Lovász</surname></persName>
		</author>
		<title level="m">An Algorithmic Theory of Numbers, Graphs and Convexity</title>
		<imprint>
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Uniform sampling of steady states in metabolic networks:Heterogeneous scales and rounding</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">De</forename><surname>Martino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Parisi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS ONE</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Choosing a Point from the Surface of a Sphere</title>
		<author>
			<persName><forename type="first">G</forename><surname>Marsaglia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Mathematical Statistics</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">CHRR: Coordinate hit-and-run with rounding for uniform sampling of constraint-based models</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">S</forename><surname>Haraldsdóttir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Cousins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Thiele</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Fleming</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Vempala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
