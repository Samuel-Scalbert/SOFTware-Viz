<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning Class Disjointness Axioms Using Grammatical Evolution</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Thu</forename><forename type="middle">Huong</forename><surname>Nguyen</surname></persName>
							<email>thu-huong.nguyen@univ-cotedazur.fr</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Université Côte d&apos;Azur</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
								<orgName type="institution" key="instit3">Inria</orgName>
								<address>
									<region>I3S</region>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Andrea</forename><forename type="middle">G B</forename><surname>Tettamanzi</surname></persName>
							<email>andrea.tettamanzi@univ-cotedazur.fr</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Université Côte d&apos;Azur</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
								<orgName type="institution" key="instit3">Inria</orgName>
								<address>
									<region>I3S</region>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Learning Class Disjointness Axioms Using Grammatical Evolution</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">735915D97F0017A46466F2CB2F0BD78E</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-04-12T14:44+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Ontology learning</term>
					<term>OWL 2 axiom</term>
					<term>Grammatical Evolution</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Today, with the development of the Semantic Web, Linked Open Data (LOD), expressed using the Resource Description Framework (RDF), has reached the status of "big data" and can be considered as a giant data resource from which knowledge can be discovered. The process of learning knowledge defined in terms of OWL 2 axioms from the RDF datasets can be viewed as a special case of knowledge discovery from data or "data mining", which can be called "RDF mining". The approaches to automated generation of the axioms from recorded RDF facts on the Web may be regarded as a case of inductive reasoning and ontology learning. The instances, represented by RDF triples, play the role of specific observations, from which axioms can be extracted by generalization. Based on the insight that discovering new knowledge is essentially an evolutionary process, whereby hypotheses are generated by some heuristic mechanism and then tested against the available evidence, so that only the best hypotheses survive, we propose the use of Grammatical Evolution, one type of evolutionary algorithm, for mining disjointness OWL 2 axioms from an RDF data repository such as DBpedia. For the evaluation of candidate axioms against the DBpedia dataset, we adopt an approach based on possibility theory.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The manual acquisition of formal conceptualizations within domains of knowledge, i.e. ontologies <ref type="bibr" target="#b0">[1]</ref> is an expensive and time-consuming task because of the requirement of involving domain specialists and knowledge engineers. This is known as the "knowledge acquisition bottleneck ". Ontology learning, which comprises the set of methods and techniques used for building an ontology from scratch, enriching, or adapting an existing ontology in a semi-automatic fashion, using several knowledge and information sources <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref>, is a potential approach to overcome this obstacle. An overall classification of ontology learning methods can be found in <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b4">5]</ref>. Ontology learning may be viewed as a special case of knowledge discovery from data (KDD) or data mining, where the data are in a special format and knowledge can consist of concepts, relations, or axioms from a domain-specific application.</p><p>Linked Open Data (LOD) being Linked Data<ref type="foot" target="#foot_0">1</ref> published in the form of an Open Data Source can be considered as a giant real-world knowledge base. Such a huge knowledge base opens up exciting opportunities for learning new knowledge in the context of an open world. Based on URIs, HTTP, and RDF, Linked Data is a recommended best practice for exposing, sharing, and connecting pieces of data, information, and knowledge on the Semantic Web. Some approaches to ontology learning from linked data can be found in <ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref>. The advantages of LOD with respect to learning described in <ref type="bibr" target="#b7">[8]</ref> is that it is publicly available, highly structured, relational, and large compared with other resources. Ontology learning on the Semantic Web involves handling the enormous and diverse amount of data in the Web and thus enhancing existing approaches for knowledge acquisition instead of only focusing on mostly small and uniform data collections.</p><p>In ontology learning, one of the critical tasks is to increase the expressiveness and semantic richness of a knowledge base (KB), which is called ontology enrichment. Meanwhile, exploiting ontological axioms in the form of logical assertions to be added to an existing ontology can provide some tight constraints to it or support the inference of implicit information. Adding axioms to a KB can yield several benefits, as indicated in <ref type="bibr" target="#b8">[9]</ref>. In particular, class disjointness axioms are useful for checking the logical consistency and detecting undesired usage patterns or incorrect assertions. As for the definition of disjointness <ref type="bibr" target="#b9">[10]</ref>, two classes are disjoint if they do not possess any common individual according to their intended interpretation, i.e., the intersection of these classes is empty in a particular KB.</p><p>A simple example can demonstrate the potential advantages obtained by the addition of this kind of axioms to an ontology. A knowledge base defining terms of classes like Person, City and asserting that Sydney is both a Person and a City would be logically consistent, without any errors being recognized by a reasoner. However, if a constraint of disjointness between classes Person and City is added, the reasoner will be able to reveal an error in the modeling of such a knowledge base. As a consequence, logical inconsistencies of facts can be detected and excluded-thus enhancing the quality of ontologies.</p><p>As a matter of fact, very few DisjointClasses axioms are currently found in existing ontologies. For example, in the DBpedia ontology, the query SELECT ?x ?y { ?x owl:disjointWith ?y } executed on November 11, 2018 returned only 25 solutions, whereas the realistic number of class disjointness axioms generated from hundreds of classes in DBpedia (432 classes in DBpedia 2015-04, 753 classes in DBpedia 2016-04) is expected to be much more (in the thousands). Hence, learning implicit knowledge in terms of axioms from a LOD repository in the context of the Semantic Web has been the object of research in several different approaches. Recent methods <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12]</ref> apply top-down or intensional approaches to learning disjointness which rely on schema-level information, i.e., logical and lexical decriptions of the classes. The contributions based on bottom-up or extensional approaches <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10]</ref>, on the other hand, require the instances in the dataset to induce instance-driven patterns to suggest axioms, e.g., disjointness class axioms.</p><p>Along the lines of extensional (i.e. instance-based) methods, we propose an evolutionary approach, based on grammatical evolution, for mining implicit axioms from RDF datasets. The goal is to derive potential class disjointness axioms of more complex types, i.e., defined with the help of relational operators of intersection and union; in other words, axioms like Dis(C 1 , C 2 ), where C 1 and C 2 are complex class expressions including and operators. Also, an evaluation method based on possibility theory is adopted to assess the certainty level of induced axioms.</p><p>The rest of the paper is organized as follows: some related works are described briefly in Section 2. In Section 3, some background is provided. OWL 2 classes disjointness axioms discovery with a GE approach is presented in Section 4. An axiom evaluation method based on possibility theory is also presented in this section. Section 5 provides experimental evaluation and comparison. Conclusions and directions for future research are given in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>The most prominent related work relevant to learning disjointness axioms consists of the contributions by Johanna Völker and her collaborators <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b12">13]</ref>. In early work, Völker developed supervised classifiers from LOD incorporated in the LeDA tool <ref type="bibr" target="#b11">[12]</ref>. However, the learning algorithms need a set of labeled data for training that may demand expensive work by domain experts. In contrast to LeDA, statistical schema induction via associative rule mining <ref type="bibr" target="#b9">[10]</ref> was given in the tool GoldMiner, where association rules are representations of implicit patterns extracted from large amount of data and no training data is required. Association rules are compiled based on a transaction table, which is built from the results of SPARQL queries. That research only focused on generating axioms involving atomic classes, i.e., classes that do not consist of logical expressions, but only of a single class identifier.</p><p>Another relevant research is the one by Lorenz Bühmann and Jens Lehmann, whose proposed methodology is implemented in the DL-Learner system <ref type="bibr" target="#b10">[11]</ref> for learning general class descriptions (including disjointness) from training data. Their work relies on the capabilities of a reasoning component, but suffers from scalability problems for the application to large datasets like LOD. In <ref type="bibr" target="#b8">[9]</ref>, they tried to overcome these obstacles by obtaining predefined data queries, i.e., SPARQL queries to detect specific axioms hidden within relevant data in datasets for the purpose of ontology enrichment. That approach is very similar to ours in that it uses an evolutionary algorithm for learning concepts. Bühmann and Lehmann also developed methods for generating more complex axiom types <ref type="bibr" target="#b13">[14]</ref> by using frequent terminological axiom patterns from several data repositories. One important limitation of their method is the timeconsuming and computationally expensive process of learning frequent axioms patterns and converting them into SPARQL queries before generating actual axioms from instance data. Also, the most frequent patterns refer to inclusion and equivalence axioms like A ≡ B ∃r.C or A B ∃r.C.</p><p>Our solution is based on an evolutionary approach deriving from previous work, but concentrating on a specific algorithm, namely Grammatical Evolution (GE) <ref type="bibr" target="#b14">[15]</ref> to generate class disjointness axioms from an existing RDF repository which is different from the use of Genertic Algorithm as in the approach of Bühmann and Lehmann <ref type="bibr" target="#b13">[14]</ref>. GE aims at overcoming one shortcoming of GP, which is the growth of redundant code, also known as bloat. Furthermore, instead of using probability theory, we applied a possibilistic approach to assess the fitness of axioms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Background</head><p>This section provides a few background notions required to understand the application domain of our contribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">RDF Datasets</head><p>The Semantic Web<ref type="foot" target="#foot_1">2</ref> (SW) is an extension of the World Wide Web and it can be considered as the Web of data, which aims to make Web contents machinereadable. The Linked Open Data<ref type="foot" target="#foot_2">3</ref> (LOD) is a collection of linked RDF data. The LOD covers the data layer of the SW, where RDF plays the roles of its standard data model.</p><p>RDF uses as statements triples of the form (Subject, Predicate, Object). According to the World Wide Web Consortium (W3C), RDF <ref type="foot" target="#foot_3">4</ref> has features that facilitate data merging even if the underlying schemas differ, and it specifically supports the evolution of schemas over time without requiring all the data consumers to be changed. RDF data may be viewed as an oriented, labeled multigraph. The query language for RDF is SPARQL, <ref type="foot" target="#foot_4">5</ref> which can be used to express queries across diverse data sources, whether the data is stored natively as RDF or viewed as RDF via some middleware.</p><p>One of the prominent examples of LOD is DBpedia, <ref type="foot" target="#foot_5">6</ref> which comprises a rather rich collection of facts extracted from Wikipedia. DBpedia covers a broad variety of topics, which makes it a fascinating object of study for a knowledge extraction method. DBpedia owes to the collaborative nature of Wikipedia the characteristic of being incomplete and ridden with inconsistencies and errors. Also, the facts in DBpedia are dynamic, because they can change in time. DBpedia has become a giant repository of RDF triples and, therefore, it looks like a perfect testing ground for the automatic extraction of new knowledge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">OWL 2 Axioms</head><p>We are interested not only in extracting new knowledge from an existing knowledge base expressed in RDF, but also in being able to inject such extracted knowledge into an ontology in order to be able to exploit it to infer new logical consequences.</p><p>While the former objective calls for a target language, used to express the extracted knowledge, which is as expressive as possible, lest we throttle our method, the latter objective requires using at most a decidable fragment of firstorder logic and, possibly, a language which makes inference problems tractable. OWL 2 7 is an ontology language for the Semantic Web with formally defined meaning which strikes a good compromise between these two objectives. In addition, OWL 2 is standardized and promotes interoperability with different applications. Furthermore, depending on the applications, it will be possible to select an appropriate profile (corresponding to a different language fragment) exhibiting the desired trade-off between expressiveness and computational complexity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">A Grammatical Evolution Approach to Discovering OWL 2 Axioms</head><p>This section introduces a method based on Grammatical Evolution (GE) to mine an RDF repository for class disjointness axioms. GE is similar to GP in automatically generating variable-length programs or expressions in any language. In the context of OWL 2 axiom discovery, the "programs" are axioms. A population of individual axioms is maintained by the algorithm and iteratively refined to find the axioms with the highest level of credibility (one key measure of quality for discovered knowledge). In each iteration, known as a generation, the fitness of each individual in the population is evaluated using a possibilistic approach and is the base for the parent selection mechanism. The offspring of each generation is bred by applying genetic operators on the selected parents. The overall flow of such GE algorithm is shown in Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Representation</head><p>As in O'Neill et al <ref type="bibr" target="#b14">[15]</ref> and unlike GP, GE applies the evolutionary process on variable length binary strings instead of on the actual programs. GE has a clear distinction in representation between genotype and phenotype. The genotype to phenotype mapping is employed to generate axioms considered as phenotypic programs by using the Backus-Naur form (BNF) grammar <ref type="bibr" target="#b14">[15]</ref><ref type="bibr" target="#b15">[16]</ref><ref type="bibr" target="#b16">[17]</ref>. for each offspring {child1,child2 } do MUTATION (offspring) 16:</p><p>Compute fitness values for child1, child2 17:</p><p>Select w1, w2 -winners of competition between parents and offsprings w1,w2 ← CROWDING((parent1, parent2, child1, child2) 18:</p><p>Add w 1, w2 to new population newPop 19:</p><p>Pop= newPop 20:</p><p>Increase the number of generation curGeneration by 1 21: return Pop Structure of BNF Grammar We applied the extended BNF grammar consisting of the production rules extracted from the normative grammar<ref type="foot" target="#foot_6">8</ref> of OWL 2 in the format used in W3C documentation for constructing different types of OWL 2 axioms. The noteworthy thing is that the use of a BNF grammar here does not focus on defining what a well-formed axiom may be, but on generating well-formed axioms which may express the facts contained in a given RDF triple store. Hence, resources, literals, properties, and other elements of the language that actually occur in the RDF repository could be generated. We organized our BNF grammar in two main parts (namely static and dynamic) as follows:</p><p>the static part contains production rules defining the structure of the axioms loaded from the text file. Different grammars will generate different kinds of axioms.</p><p>the dynamic part contains production rules for the low-level non-terminals, which we will call primitives. These production rules are automatically built at runtime by querying the SPARQL endpoint of the RDF repository at hand.</p><p>This approach to organizing the structure of BNF grammar ensures the changes in the contents of RDF repositories will not require to rewrite the grammar.</p><p>In the following, we will refer to generating class disjointness axioms containing atomic expression such as DisjointClasses(Film, WrittenWork) or complex expression in the cases of relational operators, i.e., intersection and union, such as DisjointClasses(Film, ObjectIntersectionOf(Book,ObjectUnionOf(Comics, Musi-calWork))). We built the following pattern of the grammar structured for generating class disjointness axioms: This produces rules of the primitive Class, which will be filled by using SPARQL queries to extract the IRI of a class mentioned in the RDF store. An example representing a small excerpt of an RDF triple repository is the following: Encoding and Decoding Individual axioms are encoded as variable-length binary strings with numerical chromosomes. The binary string consists of a sequence of 8-bit words referred to as codons.</p><formula xml:id="formula_0">% Static part</formula><p>According to the structure of the above BNF grammar, chromosomes are then decoded into OWL 2 axioms in different OWL syntaxes through the mapping process according to the function: In the advantageous cases, axioms are generated before the end of the genome is reached; otherwise, a wrapping operator <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16]</ref> is applied and the reading of codons will continue from the beginning of the chromosome, until the maximum allowed number of wrapping events is reached. An unsuccessful mapping will happen if the threshold on the number of wrapping events is reached but the individual is still not completely mapped; in this case, the individual is assigned the lowest possible fitness. The production rule for ClassExpression is recursive and may lead to a large fan-out; to alleviate this problem and promote "reasonable" axioms, we increase the probability of obtaining a successful mapping to complex axiom expressions, we double the appearance probability of non-terminal ClassExpression. Rule (r4) in the grammar is modified to</p><formula xml:id="formula_1">(r4) ClassExpression := Class (0) | Class (1) | ObjectUnionOf (2) | ObjectIntersectionOf (3)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Initialization</head><p>In the beginning of the evolutionary process, a set of chromosomes, i.e., genotypic individuals, are randomly initialized once and for all. Each chromosome is a set of integers with the initialized length initlenChrom. Its length can be extended to maxlenChrom in the scope of the threshold of maxWrap in the wrapping process. The next step is the transformation of genotypes into phenotypic individuals, i.e., axioms according to grammar Gr, by means of the mapping process based on the input grammar called CreateNewAxiom() operator. The population of popSize class disjointness axioms is created by iterating popSize times CreateNewAxiom() operator described in Algorithm 2. mapping from input genotype gp to output phenotype of axiom individual A according to grammar Gr 6: return A</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Parent selection</head><p>Before executing the selection operator, the axioms in the populations are evaluated and ranked in descending order of their fitness. To combat the loss of fittest axioms as a result of the application of the variation operators, elitism selection is applied to copy the small proportion pElite of the best axioms into the next generation (line 7-8 of Algorithm 1). In the remaining part of the population, the elimination of duplicates is carried out to ensure only distinct individuals will be included in the candidate list for parent selection. The parent selection mechanism amounts to choosing the fittest individuals from this list for reproduction. Fig. <ref type="figure" target="#fig_3">1</ref>. illustrates the process of selecting potential candidate solutions for recombination, i.e., a list of parents. The top proportion pselectSize of distinct individuals in the candidate list is selected and it is replicated to maintain the size popSize of population. The list of parents is shuffled and the individuals are paired in order from the beginning to the end of the list. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Variation Operators</head><p>The purpose of these operators is to create new axioms from old ones. The standard genetic operators of crossover and mutation in the Evolutionary Algorithms (EA) are applied in the search space of genotypes. Well-formed individuals will then be generated syntactically from the new genotypes in the genotype-tophenotype mapping process.</p><p>Crossover A standard one-point crossover is employed whereby a single crossover point on the chromosomes of both parents is chosen randomly. The sets of codons Algorithm 3 -Crowding(parent1, parent2, offspring1, offspring2) Input: parent1, parent2, child 1, child 2 : a crowd of individual axioms; Output: A: ListWinners-a list containing two winners of individual axioms</p><formula xml:id="formula_2">1: d1 ← DISTANCE(parent1,child1) +DISTANCE (parent2,child2) d2 ← DISTANCE(parent1, child2) + DISTANCE(parent2, child1)</formula><p>in which DISTANCE(parent, child) -the number of distinct codons between parent and child.</p><formula xml:id="formula_3">2: if (d1 &gt;d2 ) ListWinners[0]← COMPARE(parent1,child1) ListWinners[1]← COMPARE(parent2,child2) else ListWinners[0]← COMPARE(parent1,child2) ListWinners[1]← COMPARE(parent2,child1)</formula><p>in which COMPARE(parent, child) -defines which individual in (parent,child) having higher fitness value. 3: return ListWinners beyond those points are exchanged between the two parents with probability pCross. The result of this exchange is two offspring genotypes. The mapping of these genotype into phenotypic axioms is performed by executing the Create-NewAxiom() operator (Algorithm 2) again with the offspring chromosomes as input.</p><p>Mutation The mutation is applied to the offspring genotypes of crossover with probability pMut. A selected individual undergoes single-point mutation, i.e. a codon is selected at random, and this codon is replaced with a new randomly generated codon.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Survival selection</head><p>In order to preserve population diversity, we used the Deterministic Crowding approach developed by Mahfoud <ref type="bibr" target="#b17">[18]</ref>. Each offspring competes with its most similar peers, based on a genotypic comparison, to be selected for inclusion in the population of the next generation. Algorithm 3 describes this approach in detail. Even though we are aware that computing distance at the phenotypic level would yield more accurate results, we chose to use genotypic distance because it is much faster and easier to compute.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Fitness Evaluation</head><p>As a consequence of the heterogeneous and collaborative character of the linked open data, some facts (instances) in the RDF repository may be missing or erroneous. This incompleteness and noise determines a sort of epistemic uncertainty in the evaluation of the quality of a candidate axiom. In order to properly capture this type of uncertainty, typical of an open world, which contrasts with the ontic uncertainty typical of random processes, we adopt an axiom scoring The cardinality of the sets of the facts in the RDF repository reflects the generality of each axiom. An axiom is all the more necessary as it is explicitly supported by facts, i.e., confirmations, and not contradicted by any fact, i.e., counterexamples, while it is the more possible as it is not contradicted by any fact. These numbers of confirmations and counterexamples are counted by executing corresponding SPARQL queries via an accessible SPARQL endpoint.</p><p>In principle, the fitness of axiom φ should be directly proportional to its necessity N (φ), its possibility Π(φ), and its support u φ , which is an indicator of its generality. In other words, what we are looking for is not only credible axioms, but also general ones. A definition of the fitness function that satisfies such requirement is</p><formula xml:id="formula_4">f (φ) = u φ • Π(φ) + N (φ) 2 ,<label>(7)</label></formula><p>which we adopted for our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiment &amp; Result</head><p>We applied our approach to mine the classes disjointness axioms relevant to the topic Work in DBpedia. The statistical data of classes and instances about this topic in DBpedia 2015-04 is given in Table <ref type="table" target="#tab_2">1</ref>.</p><p>All data used in this experiment is represented in the form of RDF triples, as explained in Section 3. In order to assess its ability to discover axioms, we ran the GE indicated in Section 4 by repeating the sample procedure of Algorithm 1 for each run with the same parameters indicated in Table <ref type="table" target="#tab_3">2</ref>. The chart in Fig. <ref type="figure" target="#fig_2">2</ref> illustrates the average diversity of the population of axioms over the generations of the evolutionary process. It shows how many different "species" of axioms are contained in the population, i.e., axioms that cover different aspects of the known facts. One of the remarkable points here is that there is a more rapid loss of diversity in the phenotype axioms compared with this decrease in the genotype ones. The use of Crowding method on genotypes instead of phenotypes can be the reason of this difference. Likewise, a set of codons of two parent chromosomes which are used for the mapping to phenotypes can fail to be swapped in the single-point crossover operator. From the chart in Fig. <ref type="figure">3</ref>, we can observe a gradual increase in the quality of discovered axioms over generations.</p><p>In order to evaluate the effectiveness of our method in discovering disjointness class axioms of the Work on RDF datasets of DBpedia, a benchmark of   class disjointness axioms about this topic was manually created, which we called Gold Standard. The process of creating the Gold Standard was carried out by knowledge engineers and consisted of two phases. In the first phase, the disjointness of the top-most classes to their siblings was assessed manually. Therefrom, two sibling classes being disjoint will infer automatically the disjointness of their corresponding pair of subclasses. This process is repeated in the same way on the next level of concepts. The second phase of Gold Standard creation consisted in manually annotating the disjointness for the not yet noted pairs of classes which did not belong to the cases given in the previous phase. The result of the completion of the Gold Standard is the disjointness evaluation between 1,891 pairs of distinct classes relevant to the chosen topic. Table <ref type="table" target="#tab_4">3</ref> summarizes the performance of our approach in discovering axioms with the parameters setting in Table <ref type="table" target="#tab_3">2</ref> over 20 runs. The precision and recall are computed by comparison to the Gold Standard. Although the main purpose in our research is to focus on exploring the more complex disjointness axioms which contain logical relationship-intersection and union expression, we also performed experiments to generate axioms involving atomic classes only, for comparison purpose. We carry out the comparison with the results of GoldMiner <ref type="bibr" target="#b9">[10]</ref> in generating class disjointness axioms about the topic Work. The precision and recall are computed by comparison to the Gold Standard. The results in Table <ref type="table" target="#tab_4">3</ref> confirm the high accuracy of our approach in discovering class disjointness axioms in the case of atomic expressions (Precision = 0.95 ± 0.02). Also, the recall value is higher than the value in GoldMiner. There are a number of class disjointness axioms generated in our experiments which are absent in the result of Gold-Miner. For example, there are no any axioms relevant to class Archive in the axioms generated by GoldMiner. In the case of more complex axioms, there is a smaller degree of precision (Precision = 0.867 ± 0.03). The reason may stem from the complexity in the content of generated axioms which is relevant to more different classes. We do not present the recall for the case of complex axioms, since the discovery process of this type of axioms cannot define how many of the complex axioms should have been generated. After 20 runs, from 10,000 candidate individual axioms, we got 5,728 qualified distinct complex axioms.</p><p>We performed an analysis of the discovered axioms and found some noticeable points. Almost all generated axioms have high fitness values with millions of support instances from the DBpedia dataset, which witness the generality of the discovered axioms. However, we found some deficiencies in determining the disjointness of classes. As in the case of axiom DisjointClasses(MovingImage, Ob-jectUnionOf(Article,ObjectUnionOf(Image, MusicalWork ))), 4,839,992 triples in DBpedia confirm that this class disjointness axiom is valid. However, according to the Gold Standard, these two classes should not be disjoint a priori. Indeed, the class MovingImage can be assessed as a subclass of Image, which makes the disjointness between class MovingImage and any complex class expression involving relational operator union of class Image altogether impossible. Another similar case is the axiom DisjointClasses(ObjectUnionOf(Article, Ob-jectUnionOf(ObjectUnionOf(ObjectUnionOf (ObjectUnionOf(TelevisionShow, Writ-tenWork), MusicalWork), Image), Film)), UndergroundJournal), having 5,037,468 triples in its support. However, according to the Gold Standard, these two classes should not be disjoint.</p><p>From the above examples we can infer that the main reason for such erroneous axioms may lie in the inconsistencies and errors in the DBpedia dataset. Therefore, a necessary direction to improve the quality of the knowledge base is to use the results of our mining algorithms to pinpoint errors and inconsistencies and thus aim at tighter constraints by excluding problematic triples from the RDF repository.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion and Future Work</head><p>We proposed an algorithm based on GE to discover class disjointness axioms from the DBpedia dataset relevant to the topic "Work". The experiment results have allowed us to evaluate the effectiveness of the model and analyze some of its shortcomings.</p><p>In future work, we will focus on three main directions:</p><p>1. improving the diversity of generated axioms by applying the crowding method at the level of phenotypes; 2. mining different types of axioms like identity axioms, equivalence axioms and relevant to broader topics; 3. enhancing the performance of the algorithm on parallel hardware in order to be able to carry out bigger data analytics.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>(</head><label></label><figDesc>r1) Axiom := ClassAxiom (r2) ClassAxiom := DisjointClasses (r3) DisjointClasses := 'DisjointClasses' '(' ClassExpression ' 'ClassExpression ')' (r4) ClassExpression :ObjectUnionOf := 'ObjectUnionOf' '(' ClassExpression ' ' ClassExpression ')' (r6) ObjectIntersectionOf := 'ObjectIntersectionOf' '(' ClassExpression' 'ClassExpression ')' % Dynamic part -Primitives (r7) Class := % production rules are constructed by using SPARQL queries</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Rule = Codon value modulo Number of Rules for the current terminal<ref type="bibr" target="#b0">(1)</ref> </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Algorithm 2 -</head><label>2</label><figDesc>CreateNewAxiom() Input: Chr : Chromosome -a set of integers; Gr : BNF grammar Output: A: a new axiom individual 1: maxlenChrom ← initlenChrom * maxWrap 2: ValCodon ← random(maxValCodon). 3: Set up Chr as input genotype gp used in mapping proccess to axiom individual A 4: while (Chr.length &lt;maxlenChrom) &amp;&amp; (incomplete mapping) do 5:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. An illustration of the parent selection mechanism.</figDesc><graphic coords="9,134.77,311.80,345.84,172.92" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 2 .Fig. 3 .</head><label>23</label><figDesc>Fig. 2. The diversity of axioms over generations</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Algorithm 1 -GE for discovering axioms from a set of RDF datasets Output: Pop: a set of axioms discovered based on Gr 1: Initialize a list of chromosomes L of length initlenChrom.Each codon value in chromosome are integer. 2: Create a population P of size popSize mapped from list of chromosomes L on grammar Gr by performing popSize times CreateNewAxiom() 3: Compute the fitness values for all axioms in Pop.</figDesc><table><row><cell cols="2">4: Initialize current generation number ( currentGeneration = 0 )</cell></row><row><cell cols="2">5: while( currentGeneration ¡ maxGenerations) do</cell></row><row><cell>6:</cell><cell>Sort Pop by descending fitness values</cell></row><row><cell>7:</cell><cell>Create a list of elite axioms listElites with the propotion pElite of the number</cell></row><row><cell></cell><cell>of the fittest axioms in Pop</cell></row><row><cell>8:</cell><cell>Add all axioms of listElites to a new population newPop</cell></row><row><cell>9:</cell><cell>Select the remaining part of population after elitism selection</cell></row><row><cell></cell><cell>Lr ← Pop\listElites</cell></row><row><cell>10:</cell><cell>Eliminate the duplicates in Lr</cell></row><row><cell></cell><cell>Lr ← Distinct (Lr )</cell></row><row><cell cols="2">11: Create a a list of axioms listCrossover used for crossover operation</cell></row><row><cell></cell><cell>with the propotion pselectSize of the number of</cell></row><row><cell></cell><cell>the fittest individuals in Lr</cell></row><row><cell>11:</cell><cell>Shuffle(listCrossover )</cell></row><row><cell>12:</cell><cell>for (i=0,1....listCrossover.length-2 ) do</cell></row><row><cell>10:</cell><cell>parent1 ← listCrossover [i]</cell></row><row><cell>13:</cell><cell>parent2 ← listCrossover [i+1]</cell></row><row><cell>14:</cell><cell>child1, child2 ← CROSSOVER(parent1,parent2 ) with the probability pCross</cell></row><row><cell>15:</cell><cell></cell></row></table><note><p>Input: T : RDF Triples data; Gr : BNF grammar; popSize: the size of the population; initlenChrom: the initialized length of chromosome ; maxWrap: the maximum number of wrapping; pElite: elitism propotion pselectSize: parent selection propotion; pCross: the probability of crossover; pMut: the probability of mutation;</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 .</head><label>1</label><figDesc>Statistical data in the topic Work in DBpedia</figDesc><table><row><cell>Total number of classes</cell><cell>62</cell></row><row><cell>Total number of classes having instances</cell><cell>53</cell></row><row><cell>Total number of instances</cell><cell>519,5019</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 .</head><label>2</label><figDesc>Input parameter values for GE</figDesc><table><row><cell>Parameter</cell><cell>Value</cell></row><row><cell>popSize</cell><cell>500</cell></row><row><cell cols="2">numGenerations 30</cell></row><row><cell>initlenChrom</cell><cell>20</cell></row><row><cell>maxWrap</cell><cell>2</cell></row><row><cell>pCross</cell><cell>80%</cell></row><row><cell>pMut</cell><cell>1%</cell></row><row><cell>pselectSize</cell><cell>70%</cell></row><row><cell>pElite</cell><cell>2%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 .</head><label>3</label><figDesc>Experimental results</figDesc><table><row><cell></cell><cell cols="2">Our approach</cell><cell>GoldMiner</cell></row><row><cell></cell><cell cols="3">Complex axioms Atomic axioms Atomic axioms</cell></row><row><cell>Precision (per run)</cell><cell>0.867 ± 0.03</cell><cell>0.95 ± 0.02</cell><cell>0.95</cell></row><row><cell>Recall (per run)</cell><cell>N/A</cell><cell>0.15 ± 0.017</cell><cell>0.38</cell></row><row><cell>Recall (over 20 runs)</cell><cell>N/A</cell><cell>0.54</cell><cell>0.38</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>http://linkeddata.org/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>https://www.w3.org/standards/semanticweb/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>https://www.w3.org/egov/wiki/Linked Open Data</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"><p>https://www.w3.org/RDF/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4"><p>https://www.w3.org/TR/rdf-sparql-query/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5"><p>https://wiki.dbpedia.org/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_6"><p>https://www.w3.org/TR/owl2-syntax/</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>heuristics based on possibility theory, proposed in <ref type="bibr" target="#b18">[19]</ref>, which is suitable for dealing with incomplete knowledge. This is a justified choice for assessing knowledge extracted from an RDF repository. We now provide a summary of this scoring heuristics.</p><p>Possibility theory <ref type="bibr" target="#b19">[20]</ref> is a mathematical theory of epistemic uncertainty. Given a finite universe of discourse Ω, whose elements ω ∈ Ω may be regarded as events, values of a variable, possible worlds, or states of affairs, a possibility distribution is a mapping π : Ω → [0, 1], which assigns to each ω a degree of possibility ranging from 0 (impossible, excluded) to 1 (completely possible, normal). A possibility distribution π for which there exists a completely possible state of affairs (∃ω ∈ Ω : π(ω) = 1) is said to be normalized.</p><p>A possibility distribution π induces a possibility measure and its dual necessity measure, denoted by Π and N respectively. Both measures apply to a set A ⊆ Ω (or to a formula φ, by way of the set of its models, A = {ω : ω |= φ}), and are usually defined as follows:</p><p>(2)</p><p>In other words, the possibility measure of A corresponds to the greatest of the possibilities associated to its elements; conversely, the necessity measure of A is equivalent to the impossibility of its complement Ā.</p><p>A generalization of the above definition can be obtained by replacing the min and the max operators with any dual pair of triangular norm and co-norm.</p><p>In the case of possibilistic axiom scoring, the basic principle for establishing the possibility of a formula φ should be that the absence of counterexamples to φ in the RDF repository means Π([φ]) = 1, i.e., that φ is completely possible. Let φ be an axiom that we wish to evaluate (i.e., a theory). The content of an axiom φ that we wish to evaluate is defined as a set of logical consequences</p><p>obtained through the instatiation of φ to the vocabulary of the RDF repository; the cardinality of content(φ) is finite and every formula ψ ∈ content(φ) may be readily tested by means of a SPARQL ASK query. Let us define u φ = content(φ) as the support of φ. Let then u + φ be the number of confirmations (basic statements ψ that are satisfied by the RDF repository) and u - φ the number of counterexamples (basic statements ψ that are falsified by the RDF repository).</p><p>The possibility measure Π(φ) and the necessity measure N (φ) of an axiom have been defined as follows in <ref type="bibr" target="#b18">[19]</ref>: for u φ &gt; 0,</p><p>(5)</p><p>, if Π(φ) = 1, 0 otherwise. <ref type="bibr" target="#b5">(6)</ref> </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">What Is an Ontology? Handbook on Ontologies</title>
		<author>
			<persName><forename type="first">N</forename><surname>Guarino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Oberle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Staab</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Ontology learning for the semantic web</title>
		<author>
			<persName><forename type="first">A</forename><surname>Maedche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Staab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Intelligent Systems</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="72" to="79" />
			<date type="published" when="2001-03">March 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Perspectives on Ontology Learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lehmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Völker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Studies on the Semantic Web</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<date type="published" when="2014">2014</date>
			<publisher>IOS Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A survey of ontology learning procedures</title>
		<author>
			<persName><forename type="first">L</forename><surname>Drumond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Girardi</surname></persName>
		</author>
		<ptr target="CEUR-WS.org" />
	</analytic>
	<monogr>
		<title level="m">WONTO. CEUR Workshop Proceedings</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="volume">427</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Article: A survey of ontology learning approaches</title>
		<author>
			<persName><forename type="first">M</forename><surname>Hazman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">R</forename><surname>El-Beltagy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rafea</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Applications</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="36" to="43" />
			<date type="published" when="2011-05">May 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Mid-ontology learning from linked data</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ichise</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">JIST. Lecture Notes in Computer Science</title>
		<imprint>
			<biblScope unit="volume">7185</biblScope>
			<biblScope unit="page" from="112" to="127" />
			<date type="published" when="2011">2011</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Ontology learning from open linked data and web snippets</title>
		<author>
			<persName><forename type="first">I</forename><surname>Tiddi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">B</forename><surname>Mustapha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Vanrompay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Aufaure</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">OTM Workshops</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">7567</biblScope>
			<biblScope unit="page" from="434" to="443" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">DC proposal: Ontology learning from noisy linked data</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Semantic Web Conference</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="volume">7032</biblScope>
			<biblScope unit="page" from="373" to="380" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Universal OWL axiom enrichment for large knowledge bases</title>
		<author>
			<persName><forename type="first">L</forename><surname>Bühmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lehmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">EKAW. Lecture Notes in Computer Science</title>
		<imprint>
			<biblScope unit="volume">7603</biblScope>
			<biblScope unit="page" from="57" to="71" />
			<date type="published" when="2012">2012</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Automatic acquisition of class disjointness</title>
		<author>
			<persName><forename type="first">J</forename><surname>Völker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Fleischhacker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Stuckenschmidt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Web Sem</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="124" to="139" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Dl-learner: Learning concepts in description logics</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lehmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="2639" to="2642" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning disjointness</title>
		<author>
			<persName><forename type="first">J</forename><surname>Völker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Vrandecic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sure</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hotho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">ESWC. Lecture Notes in Computer Science</title>
		<imprint>
			<biblScope unit="volume">4519</biblScope>
			<biblScope unit="page" from="175" to="189" />
			<date type="published" when="2007">2007</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Inductive learning of disjointness axioms</title>
		<author>
			<persName><forename type="first">D</forename><surname>Fleischhacker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Völker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">OTM Conferences</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="volume">7045</biblScope>
			<biblScope unit="page" from="680" to="697" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Pattern based knowledge base enrichment</title>
		<author>
			<persName><forename type="first">L</forename><surname>Bühmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lehmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Semantic Web Conference (1)</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">8218</biblScope>
			<biblScope unit="page" from="33" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Grammatical evolution</title>
		<author>
			<persName><forename type="first">M</forename><surname>O'neill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ryan</surname></persName>
		</author>
		<idno type="DOI">10.1109/4235.942529</idno>
		<ptr target="http://dx.doi.org/10.1109/4235.942529" />
	</analytic>
	<monogr>
		<title level="j">Trans. Evol. Comp</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="349" to="358" />
			<date type="published" when="2001-08">Aug 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Foundations in Grammatical Evolution for Dynamic Environments -Chapter 2 Grammatical Evolution</title>
		<author>
			<persName><forename type="first">I</forename><surname>Dempsey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>O'neill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Brabazon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Studies in Computational Intelligence</title>
		<imprint>
			<biblScope unit="volume">194</biblScope>
			<date type="published" when="2009">2009</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Grammatical evolution: Evolving programs for an arbitrary language</title>
		<author>
			<persName><forename type="first">C</forename><surname>Ryan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>O'neill</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">EuroGP. Lecture Notes in Computer Science</title>
		<imprint>
			<biblScope unit="volume">1391</biblScope>
			<biblScope unit="page" from="83" to="96" />
			<date type="published" when="1998">1998</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Crowding and preselection revisited</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">W</forename><surname>Mahfoud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">PPSN</title>
		<imprint>
			<publisher>Elsevier</publisher>
			<date type="published" when="1992">1992</date>
			<biblScope unit="page" from="27" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Testing OWL axioms against RDF facts: A possibilistic approach</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G B</forename><surname>Tettamanzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Faron-Zucker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">L</forename><surname>Gandon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">EKAW. Lecture Notes in Computer Science</title>
		<imprint>
			<biblScope unit="volume">8876</biblScope>
			<biblScope unit="page" from="519" to="530" />
			<date type="published" when="2014">2014</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Fuzzy sets as a basis for a theory of possibility</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Zadeh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Fuzzy Sets and Systems</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="3" to="28" />
			<date type="published" when="1978">1978</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
