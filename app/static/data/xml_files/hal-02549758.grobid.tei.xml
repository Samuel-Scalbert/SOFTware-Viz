<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Spark-based Cloud Data Analytics using Multi-Objective Optimization</title>
				<funder>
					<orgName type="full">China Scholarship Council</orgName>
					<orgName type="abbreviated">CSC</orgName>
				</funder>
				<funder ref="#_e3xqWgZ">
					<orgName type="full">European Research Council</orgName>
					<orgName type="abbreviated">ERC</orgName>
				</funder>
				<funder ref="#_Zj25QSa">
					<orgName type="full">National Science Foundation</orgName>
					<orgName type="abbreviated">NSF</orgName>
				</funder>
				<funder ref="#_fq7Gbk8">
					<orgName type="full">ARL</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Fei</forename><surname>Song</surname></persName>
							<email>fei.song@polytechnique.edu</email>
						</author>
						<author>
							<persName><forename type="first">Khaled</forename><surname>Zaouk</surname></persName>
							<email>khaled.zaouk@polytechnique.edu</email>
						</author>
						<author>
							<persName><forename type="first">Chenghao</forename><surname>Lyu</surname></persName>
							<email>‡chenghao@cs.umass.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">University of Massachusetts</orgName>
								<address>
									<settlement>Amherst</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Arnab</forename><surname>Sinha</surname></persName>
							<email>arnab.sinha@polytechnique.edu</email>
						</author>
						<author>
							<persName><forename type="first">Qi</forename><surname>Fan</surname></persName>
							<email>qi.fan@polytechnique.edu</email>
						</author>
						<author>
							<persName><forename type="first">Yanlei</forename><surname>Diao</surname></persName>
							<email>yanlei.diao@polytechnique.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">University of Massachusetts</orgName>
								<address>
									<settlement>Amherst</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Prashant</forename><surname>Shenoy</surname></persName>
							<email>shenoy@cs.umass.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">University of Massachusetts</orgName>
								<address>
									<settlement>Amherst</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Ecole Polytechnique</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Spark-based Cloud Data Analytics using Multi-Objective Optimization</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">2B193379EEE3FAD38EB52BEB81810083</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-04-12T14:54+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Data analytics in the cloud has become an integral part of enterprise businesses. Big data analytics systems, however, still lack the ability to take task objectives such as user performance goals and budgetary constraints and automatically configure an analytic job to achieve these objectives. This paper presents UDAO, a Spark-based Unified Data Analytics Optimizer that can automatically determine a cluster configuration with a suitable number of cores as well as other system parameters that best meet the task objectives. At a core of our work is a principled multi-objective optimization (MOO) approach that computes a Pareto optimal set of configurations to reveal tradeoffs between different objectives, recommends a new Spark configuration that best explores such tradeoffs, and employs novel optimizations to enable such recommendations within a few seconds. Detailed experiments using benchmark workloads show that our MOO techniques provide a 2-50x speedup over existing MOO methods, while offering good coverage of the Pareto frontier. Compared to Ottertune, a state-of-the-art performance tuning system, UDAO recommends Spark configurations that yield 26%-49% reduction of running time of the TPCx-BB benchmark while adapting to different user preferences on multiple objectives.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>As the volume of data generated by enterprises has continued to grow, big data analytics in the cloud has become commonplace for obtaining business insights from this voluminous data. Despite its wide adoption, current big data analytics systems such as Spark remain best effort in nature and typically lack the ability to take user objectives such as performance goals or cost constraints into account.</p><p>Determining an optimal hardware and software configuration for a big-data analytic task based on user-specified objectives is a complex task and one that is largely performed manually. Consider an enterprise user who wishes to run a mix of Spark analytic tasks in the cloud. First, she needs to choose the server hardware configuration from the set of available choices, e.g., from over 190 hardware configurations offered by Amazon's EC2 <ref type="bibr" target="#b0">[1]</ref>. These configurations differ in the number of cores, RAM size, availability of solid state disks, etc. After determining the hardware configuration, the user also needs to determine the software configuration by choosing various runtime parameters. For the Spark platform, these runtime parameters include the number of executors, cores per executor, memory per executor, parallelism (for reducestyle transformations), Rdd compression (boolean), Memory fraction (of heap space), to name a few.</p><p>The choice of a configuration is further complicated by the need to optimize multiple, possibly conflicting, user objectives.</p><p>Consider the following real-world use cases at data analytics companies and cloud providers (anonymized for confidentiality) that elaborate on these challenges and motivate our work:</p><p>Use Case 1 (Data-driven Business Users). A data-driven security company that runs thousands of cloud analytic tasks daily has two objectives: keep the latency low in order to quickly detect fraudulent behaviors and also reduce cloud costs that impose substantial operational expenses on the company. For cloud analytics, task latency can often be reduced by allocating more resources, but at the expense of higher cloud costs. Thus, the engineers face the challenge of deciding the cloud configuration and other runtime parameters that balance latency and cost.</p><p>Use Case 2 (Serverless Analytics). Cloud providers now offer databases and Spark for serverless computing <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b22">[23]</ref>. In this case, a database or Spark instance is turned off during idle periods, dynamically turned on when new queries arrive, and scaled up or down as the load changes over time. For example, a media company that uses a serverless offering to run a news site sees peak loads in the morning or as news stories break, and a lighter load at other times. The application specifies the minimum and maximum number of computing units (CUs) to service its workload across peak and off-peak periods; it prefers to minimize cost when the load is light and expects the cloud provider to dynamically scale CUs for the morning peak or breaking news. In this case, the cloud provider needs to balance between latency under different data loads and user cost, which directly depends on the number of CUs used.</p><p>Overall, choosing a configuration that balances multiple conflicting objectives is non-trivial-even expert engineers are often unable to choose between two cluster options for a single objective like latency <ref type="bibr" target="#b23">[24]</ref>, let alone choosing between dozens of cluster options for multiple competing objectives.</p><p>In this paper, we introduce a Spark-based Unified Data Analytics Optimizer (UDAO) that can automate the task of determining an optimal configuration for each Spark task based on multiple task objectives. Targeting cloud analytics, UDAO is designed with the following features:</p><p>First, UDAO aims to support a broad set of analytic tasks beyond SQL. Today, cloud analytics pipelines often mix SQL queries, ETL tasks based on SQL and UDFs, and machine learning (ML) tasks for deep analysis-this observation is revealed in our discussions with cloud providers, and is further supported by the recent development of the TPCx-BB benchmark <ref type="bibr" target="#b31">[32]</ref> that mixes SQL, UDF, and ML tasks in the same benchmark. UDAO unifies all of them in the general paradigm of dataflow programs and is implemented on top of Spark, a well-known unified analytics engine with both onpremise and serverless offerings in the cloud (e.g., <ref type="bibr" target="#b22">[23]</ref>).</p><p>Second, UDAO takes as input an analytic task in the form of a dataflow program and a set of objectives, and produces as output a configuration with a suitable number of cores as well as other runtime parameters that best meet the task objectives. At the core of UDAO is a principled multi-objective optimization (MOO) approach that takes multiple, possibly conflicting, objectives, computes a Paretooptimal set of configurations (i.e., not dominated by any other configuration in all objectives), and returns one from the set that best suits the objectives.</p><p>While MOO under numerical parameters has been studied in the optimization community <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b7">[8]</ref>, we address the MOO problem specifically for designing a cloud analytics optimizer-our problem setting poses systems challenges such as sufficient coverage of the Pareto set under stringent time constraints. More specifically, these challenges include:</p><p>1. Infinite parameter space: Since Spark runtime parameters mix numerical and categorial parameters, there are potentially infinite configurations, only a small fraction of which belong to the Pareto set-most configurations are dominated by some Pareto optimal configuration for all objectives. Hence, we will need to efficiently search through an infinite parameter space to find those Pareto optimal configurations.</p><p>2. Sufficient, consistent coverage of the Pareto Frontier: The Pareto set over the multi-objective space is also called the Pareto frontier. Since we aim to use the Pareto frontier to recommend a system configuration that best explores tradeoffs between different objectives, the frontier should provide sufficient coverage of the objective space. As we shall show in the paper, existing MOO methods such as Weighted Sum <ref type="bibr" target="#b18">[19]</ref> often fail to provide sufficient coverage. Further, the Pareto frontier must be consistent, i.e., the solutions computed with more CPU time should subsume the previous solutions. Randomized MOO solutions such as Evolutional methods <ref type="bibr" target="#b7">[8]</ref> cannot guarantee consistent Pareto frontiers and hence can lead to contradicting recommendations, as we reveal in evaluation, which is highly undesirable for a cloud optimizer.</p><p>3. Efficiency: Most notably, configurations must be recommended under stringent time requirements, e.g., within a few seconds, to minimize the delay of starting a recurring task, or invoking or scaling a serverless task. Such systems constraints fundamentally distinguish our MOO work from the theoretical work in the optimization community <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b7">[8]</ref>. Such time constraints are aggravated when the optimizer needs to call complex performance models of the objectives, e.g., to understand the (estimated) latency of a particular configuration. Recent modeling work tends to use models of high complexity, e.g., Gaussian Processes or Deep Neural Networks <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b37">[38]</ref>, and updates these models frequently as new training data becomes available. In this case, the MOO algorithm needs to recompute the Pareto frontier based on the updated model in order to recommend a new configuration that best suits the task objectives.</p><p>By way of designing our Spark-based optimizer, our paper makes the following contributions:</p><p>(1) We address the infinite search space issue by presenting a new approach for incrementally transforming a MOO problem to a set of constrained optimization (CO) problems, where each CO problem can be solved individually to return a Pareto optimal point.</p><p>(2) We address the coverage and efficiency challenges by designing practical Progressive Frontier (PF) algorithms to realize our approach. (i) Our first PF algorithm is designed to be incremental, i.e., gradually expanding the Pareto frontier as more computing time is invested, and uncertainty-aware, i.e., returning more points in regions of the frontier that lack sufficient information. (ii) To support complex learned models in recent work <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b37">[38]</ref>, we further develop an approximate PF algorithm that solves each CO problem efficiently for subdifferentiable models. (iii) We finally devise a parallel, approximate PF algorithm to further improve efficiency.</p><p>(3) We implement our algorithms into a Spark-based prototype. Evaluation results using batch and stream analytics benchmarks show that our approach produces a Pareto frontier within 2.5 seconds, and outperforms MOO methods including Weighted Sum <ref type="bibr" target="#b18">[19]</ref>, Normal Constraints <ref type="bibr" target="#b18">[19]</ref>, Evolutional methods <ref type="bibr" target="#b7">[8]</ref>, and multi-objective Bayesian optimization <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b4">[5]</ref> with 2-50x speedup, while offering better coverage over the frontier and enabling exploration of tradeoffs such as cost-latency or latency-throughput. When compared to Ottertune <ref type="bibr" target="#b34">[35]</ref>, a state-of-the-art performance tuning system, our approach recommends configurations that yield 26%-49% reduction of total running time of the TPCx-BB benchmark <ref type="bibr" target="#b31">[32]</ref> while adapting to different application preferences on multiple objectives and accommodating a broader set of models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. BACKGROUND AND SYSTEM DESIGN</head><p>In this section, we discuss requirements and constraints from real-world use cases that motivate our system design.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Use Cases and Requirements</head><p>We model an analytic task as a dataflow program (a directed graph of data collections flowing between operations), which is used as the programming model in big data systems such as Spark <ref type="bibr" target="#b36">[37]</ref>. We assume that each Spark task has user-or provider-specified objectives, referred to as task objectives, that need to be optimized during execution. To execute the task, the system needs to determine a cluster execution plan with runtime parameters instantiated. As stated before, these parameters control resource allocation (num. of cores, num. of executors, memory per executor), the degree of parallelism (for reduce-style transformations), granularity of scheduling, compression options, shuffling strategies, etc. An executing task using this plan is referred to as a job and the runtime parameters are collectively referred as the job configuration.</p><p>The goal of our Spark-based optimizer is: given a data flow program and a set of objectives, compute a job configuration that optimizes these objectives and adapt the configuration quickly if either the load or task objectives change.</p><p>To meet real world analytics needs, we present two concrete use cases with their requirements from our discussions with analytic and cloud companies, and design our optimizer (UDAO) accordingly to support these use cases.</p><p>1. Recurring user workloads. It is common for analytical users to issue repeated jobs in the form of daily or hourly batch jobs <ref type="bibr" target="#b37">[38]</ref> . Sometimes stream jobs can be repeated as well: under the lambda architecture, the batch layer runs to provide perfectly accurate analytical results, while the speed layer offers fast approximate analysis over live streams; the results of these two layers are combined to serve a model. As old data is periodically rolled into the batch job, the streaming job is restarted over new data with a clean state.</p><p>UDAO is designed to work for recurring workloads: for each scheduled job of a recurring task, once a user request is sent to UDAO, it recommends a configuration under a few seconds to improve job performance towards user-specified objectives.</p><p>2. Serverless analytics. As noted before, serverless analytics using databases (DBs) or Spark are becoming common in cloud computing. Each invocation of a serveless task by the cloud platform requires a configuration that meets multiple objectives-to provide low query latency to end-users while using the least computing units (CUs) for the expected load. Furthermore, auto scaling features imply that new configurations need to be computed quickly to react to load changes.</p><p>UDAO is designed to also support serverless workloads: whenever the cloud platform needs to launch a serverless analytic task or scale it to adapt to the load, the cloud platform sends a request to UDAO and within a few seconds receives a recommended configuration balancing latency and cost.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. System Overview</head><p>To meet stringent time constraints, we begin our system design by separating model learning and MOO into two asynchronous procedures: The time-consuming modeling process is performed offline by a model server whenever new training data becomes available. MOO runs online on-demand and uses the most recent model to compute a new configuration, with the delay of a few seconds. This approach distinguishes UDAO from DBMS performance tuning systems <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b38">[39]</ref> that couple modeling and optimization steps in an online iterative tuning session, which takes 15-45 mins to run for each query workload. The implications of this change are twofold: (i) It enables MOO to work with a range of models. More specifically, UDAO's MOO algorithm works with complex learned models represented by Gaussian Processes (GPs) or Deep Neural Networks (DNNs), besides simple closedform regression functions, whereas the optimization methods in <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b38">[39]</ref> work only for the specific model (GP or DNN) of choice. (ii) The model server can train a new model in the background as new training data becomes available. When MOO needs to be run for a given task, the model for this task may have been updated. Hence, the speed to compute a Pareto frontier based on the new model is a key performance goal.</p><p>Figure <ref type="figure" target="#fig_0">1</ref>(a) shows the design of UDAO based on this architectural change, with Spark as the underlying analytics engine. It takes as input a sequence of user or provider initiated requests, where each request specifies an analytic task and a set of objectives, (F 1 , . . . , F k ). Currently, UDAO offers a set of objectives for external requests to choose from, including: 1) average latency, for both batch and stream processing; 2) throughput for stream processing; 3) CPU utilization; 4) IO load; 5) network load; 6) resource cost in CPU cores; 7) resource cost in CPU-hour (latency x CPU cores); and 8) resource cost as a weighted combination of CPU-hour and IO cost (inspired by Severless DBs <ref type="bibr" target="#b1">[2]</ref>), while more options can be added in the future as application needs arise. Besides the chosen objectives, requests can optionally specify value constraints on these objectives,</p><formula xml:id="formula_0">F i ∈ [F L i , F U i ],</formula><p>as well as preferences on these objectives as a weight vector (w 1 , . . . , w k ), 0 ≤ w i ≤ 1 and</p><formula xml:id="formula_1">k i=1 w i = 1.</formula><p>Figure <ref type="figure" target="#fig_0">1</ref>(b) shows an example request, including the dataflow program of Q2 from the TPCx-BB <ref type="bibr" target="#b31">[32]</ref> benchmark, which mixes SQL with UDFs, and the application choice of latency and resource cost (CPU cores) as objectives.</p><p>UDAO handles each request as follows: If a Spark task runs for the first time or is specified with new objectives, no predictive models are available for these objectives yet. Its job will be to run with a default configuration x 1 . Our optimizer assumes that a separate model server will collect traces during job execution, including (i) system-level metrics, e.g., time measurements of different steps, bytes read and written, and fetch wait time from the Spark engine; (ii) observed values of task objectives such as latency and cost. The model server uses these traces offline asynchronously to compute task-specific models (Ψ 1 , . . . , Ψ k ), one for each chosen objective.</p><p>When the analytic task runs the next time, the multi objective optimization (MOO) module will contact the model server and retrieve the task-specific predictive models. Then it searches through the space of configurations and computes a set of Pareto-optimal configurations. Based on insights revealed in the Pareto frontier, the recommendation module chooses a new configuration, x 2 , that meets all user constraints and best explores the tradeoffs among different objectives. Future runs of the task will use this new configuration.</p><p>If the application decides to adjust the constraints on the objective (e.g., the service provider specifies a new bound [F L i , F U i ] to increase the throughput requirement when higher data rates occur) or adjust the weight factor (e.g., favoring latency more to cost in the coming hour), the optimizer can quickly return a new configuration from the computed Pareto frontier. As the model server continues to collect additional samples from recurring executions of this task as well as others, it may periodically update the task's predictive models. Upon the next run of the task, if updated models become available, the Pareto frontier will be recomputed by re-running the MOO and a new configuration will be recommended.</p><p>For the example task in Figure <ref type="figure" target="#fig_0">1</ref>(b), Figure <ref type="figure" target="#fig_0">1</ref>(c) shows the effect of optimization using UDAO against OtterTune <ref type="bibr" target="#b34">[35]</ref>, where the application first specified (0.5, 0.5) weights for latency and cost and then later (0.9, 0.1) weights to favor latency to cost. UDAO can achieve 43%-56% reduction in latency compared to OtterTune while adapting to user preferences (which will be detailed in our performance study).</p><p>Remarks on modeling choices. Since our focus in this paper is on MOO rather than modeling, we briefly discuss relevant modeling techniques here. As UDAO is designed for Spark workloads, various modeling options can be used to provide such models: (1) Handcrafted models: Domain knowledge and workload profiling were used to develop specific regression models for the Spark platform <ref type="bibr" target="#b35">[36]</ref>, where different hardware profiles can be collected to customize these models. Such models employ relatively simple function shapes (linear or low-degree polynomial) on a small set of resource parameters (e.g., the number of nodes). ( <ref type="formula" target="#formula_7">2</ref>) Learned models: Recent techniques can automatically learn predictive models, including function shapes, coefficients, etc. from runtime traces <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b38">[39]</ref>. These models tend to employ complex functions built on larger numbers of parameters. Section V discusses how UDAO incorporates these models using the model server.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. PROGRESSIVE FRONTIER APPROACH</head><p>We now present our Progressive Frontier framework for solving the MOO problem. Since our focus is on systems aspects of MOO-driven cloud analytics, we present key concepts in brief and refer the reader to <ref type="bibr" target="#b28">[29]</ref> for details and proofs.</p><p>Problem III.1. Multi-Objective Optimization (MOO).</p><formula xml:id="formula_2">arg min x f (x) =   F 1 (x) = Ψ 1 (x) ... F k (x) = Ψ k (x)  <label>(1)</label></formula><formula xml:id="formula_3">s.t. x ∈ Σ ⊆ R d F L i ≤ F i (x) ≤ F U i , i = 1, .</formula><p>.., k where x is the job configuration with d parameters, F i (x) generally denotes each of the k objectives as a function of x, and Ψ i (x) refers particularly to the predictive model derived for this objective. If an objective (e.g., throughput) favors larger values, we add the minus sign to the objective function to transform it to a minimization problem.</p><p>In general, multi-objective optimization (MOO) leads to a set of solutions rather than a single optimal solution. Definition III.1. Pareto Optimality: In the objective space</p><formula xml:id="formula_4">Φ ∈ R k , a point f Pareto-dominates another point f iff ∀i ∈ [1, k], f i ≤ f i and ∃j ∈ [1, k], f j &lt; f j . A point f * is Pareto</formula><p>Optimal iff there does not exist another point f that Paretodominates it. For an analytic task, the Pareto Set (Frontier) F includes all the Pareto optimal points in the objective space Φ, and is the solution to the MOO problem.</p><p>We further require the Pareto frontier to be computed with good coverage, high efficiency, and consistency. Among existing MOO methods, Weighted Sum (WS) <ref type="bibr" target="#b18">[19]</ref> is known to have poor coverage of the Pareto frontier <ref type="bibr" target="#b19">[20]</ref>. Normalized Constraints (NC) <ref type="bibr" target="#b20">[21]</ref> suffers from efficiency issues: it uses a pre-set parameter k to indicate the number of Pareto points desired but often returns fewer points than k; if more points are needed to cover the Pareto frontier, a large value k will be tried, by starting the computation from scratch. Evolutionary Methods (Evo) <ref type="bibr" target="#b7">[8]</ref> are randomized methods to approximately compute a frontier set, which suffer from the problem of inconsistency: The Pareto frontier built with k points can be inconsistent with that built with k points, as our evaluation results show. Multi-objective Bayesian Optimization (MOBO) extends the Bayesian approach to modeling an unknown function with an acquisition function for choosing the next point(s) to explore so that these new points are likely to be Pareto points. It suffers from inefficiency, taking a long time to return a decent Pareto set, as our evaluation results show.</p><p>To meet all the above requirements, we introduce a new approach, called Progressive Frontier. It incrementally transforms MOO to a series of constrained single-objective optimization problems, which can be solved individually.</p><p>Definition III.2. Uncertain Space: A Pareto optimal point is denoted as a reference point r i ∈ Φ if it achieves the minimum for objective</p><formula xml:id="formula_5">F i (i = 1, ..., k). A point f U ∈ Φ is a Utopia point iff for each j = 1, ..., k, f U j = min k i=1 {r i j }. A point f N ∈ Φ is a Nadir point iff for each j = 1, ..., k, f N j = max k i=1 {r i j }.</formula><p>Given a hyperrectangle formed by the Utopia Point f U and Nadir Point f N in the objective space, the Uncertain Space is defined as the volume of this hyperrectangle.</p><p>Next we describe a method to find one Pareto point by solving a single-objective constrained optimization problem.</p><p>Definition III.3. Middle Point Probe: Given a hyperrectangle formed by</p><formula xml:id="formula_6">f U = (f U 1 , . . . , f U k ) and f N = (f N 1 , . . . , f N k )</formula><p>, the middle point f M bounded by f U and f N is defined as the solution to Constrained Optimization (CO) of: where we can choose any i to be the objective to minimize.</p><formula xml:id="formula_7">x C = arg min x F i (x), subject to f U j ≤ F j (x) ≤ (f U j + f N j ) 2 , j ∈ [1, k].<label>(2)</label></formula><p>Fig. <ref type="figure">2</ref>(a) illustrates a middle point probe for our running example, TPCx-BB Q2 in Fig. <ref type="figure" target="#fig_3">1(b</ref>). Here, the 2D space is defined over F 1 (latency) and F 2 (cost in #cores), the Utopia point f U = (100, 8) denotes the hypothetical best performance (low latency using 8 cores), and the Nadir point f N = (300, 24) denotes the worst (high latency using 24 cores). The middle point probe generates a CO problem, C F1F2 : min F 1 (lat.) such that F 1 ∈ [100, 200] and F 2 (cost) in <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b15">16]</ref>, and returns a Pareto point, f M = (150, 16).</p><p>After finding the middle point f M , the sub-hyperrectangle enclosed by f M and f N , shaded in red, contains only points dominated by f M ; hence no Pareto points can exist there. The sub-hyperrectangle enclosed by f U and f M , shaded in blue, must be empty; otherwise f M cannot be Pareto optimal. This means that we can safely discard these two colored sub-hyperrectangles. The uncertain space is then reduced to two unshaded sub-hyperrectangles, enclosed by (U 1 , N 1 ) and (U 2 , N 2 ), respectively, which can be added to a queue. If we continue to take each sub-hyperrectangle from the queue and apply the Middle Point Probe iteratively, we can further reduce the uncertain space as shown in the unshaded region in Figure <ref type="figure">2(b)</ref>. Such an iterative procedure is called Iterative Middle Point Probes. This procedure can be extended naturally to hyperrectangles in k-dimensional objective space.</p><p>In general, we have the following result on the returned solution set of the Iterative Middle Point Probes procedure.</p><p>Proposition III.1. If we start the Iterative Middle Point Probes procedure from the initial Utopia and Nadir points, and let it terminate until the uncertain space becomes empty, then in the 2D case, our procedure guarantees to find all the Pareto points if they are finite. In high-dimensional cases, it is guaranteed to find a subset of Pareto optimal points.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. PRACTICAL PF ALGORITHMS</head><p>In this section, we present practical algorithms to implement our Progressive Frontier (PF) approach. Note that most MOO algorithms suffer from exponential complexity in the number of the objectives, k. This is because the number of nondominated points tends to grow quickly with k and the time complexity of computing the volume of dominated space grows super-polynomially with k <ref type="bibr" target="#b7">[8]</ref>. For this reason, the MOO literature refers to optimization with up to 3 objectives as multi-objective optimization, whereas optimization with more than 3 objectives is called many-objective optimization and handled using different methods such as preference modeling <ref type="bibr" target="#b7">[8]</ref> or fairness among different objectives <ref type="bibr" target="#b30">[31]</ref>.</p><p>Since we aim to develop a practical cloud optimizer, most of our use cases fall in the scope of multi-objective optimization. However, a key systems challenge is to compute the Pareto frontier in a few seconds, which hasn't been considered previously <ref type="bibr" target="#b7">[8]</ref>. To achieve our performance goal, we present a suite of techniques, including uncertainty-aware incremental computation, fast approximation, and parallel computation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. A Deterministic Sequential Algorithm</head><p>We first present a deterministic, sequential algorithm that implements the Progressive Frontier (PF) approach, referred to as PF-S. This algorithm has two key features:</p><p>(1) Incremental: It first constructs a Pareto frontier F with a small number of points and then expands F with more points as more time is invested. This feature is crucial because finding one Pareto point is already expensive due to being a mixed-integer nonlinear programming problem <ref type="bibr" target="#b10">[11]</ref>. Hence, one cannot expect the optimizer to find all Pareto points at once. Instead, it produces n 1 points first (e.g., those that can be computed within the first second), and then expands with additional n 2 points, afterwards n 3 points, and so on.</p><p>(2) Uncertainty-aware: The algorithm returns more points in the regions of the Pareto frontier that lack sufficient information. If the Pareto frontier includes many points, under a time constraint we can return only a subset of them. Further, in high-dimensional objective space, our PF approach can guarantee to find only a subset of Pareto points. In both cases, the uncertainty aware property means that the subset returned is likely to capture the major trend on the Pareto frontier.</p><p>To do so, we augment the Iterative Middle Point Probes method ( §III) by further deciding how to choose the best sub-hyperrectangle to probe next. We do so by defining a measure, the volume of uncertain space, to capture how much the current frontier F may deviate from the true yet unknown frontier F. This measure can be calculated from a related set of sub-hyperrectanges, which allows us to rank the subhyperrectangles that have not been probed. Among those, the sub-hyperrectangle with the largest volume will be chosen to probe next, thus reducing the uncertain space as fast as we can. Algorithm 1 shows the details, where the main steps are:</p><p>Init: Find the reference points by solving k single-objective optimization problems. Form the initial Utopia and Nadir (U 0 and N 0 ) points, and construct the first hyperrectangle. Prepare a priority queue in decreasing order of hyperrectangle volume, initialized with the first hyperrectangle.</p><p>Iterate: Pop a hyperrectangle from the priority queue. Apply the middle point probe to find a Pareto point, f M , in the current hyperrectangle, which is formed by U i and N i and has the largest volume among all the existing hyperrectangles. Divide the current hyperrectangle into 2 k sub-hyperrectangles, discard those that are dominated by f M , and calculate the volume of the others. Put them in to the priority queue.</p><p>Terminate: when we reach the desired number of solutions. Filter: Check the result set, and remove any point dominated by another one in the result set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Multi-Objective Gradient Descent</head><p>We next consider the subroutine, optimize(), that solves each constrained optimization problem (line 13 of Algorithm 1). As our objective functions are given by the models, Ψ i (x), i = 1 . . . k, returned from the model server, they are often non-linear and the variables in x can be integers and real numbers. This problem reduces to mixed-integer nonlinear programming (MINLP) and is NP-hard <ref type="bibr" target="#b10">[11]</ref>. There are no general solvers that work for every MINLP problem <ref type="bibr" target="#b21">[22]</ref>. Most of the MINLP solvers <ref type="bibr" target="#b21">[22]</ref> assume certain properties of the objective function, e.g., twice continuously differentiable (Bonmin <ref type="bibr" target="#b2">[3]</ref>) or factorable into the sumproduct of univariate functions (Couenne <ref type="bibr" target="#b3">[4]</ref>), which do not suit learned models such as Deep Neural Networks (DNNs). The most general MINLP solver, Knitro <ref type="bibr" target="#b13">[14]</ref>, supports complex models but runs very slowly, e.g., 42 <ref type="bibr" target="#b16">(17)</ref> minutes for solving a single oneobjective optimization problem based on a DNN (GP) model.</p><p>Therefore, we develop a new solver that uses a customized gradient descent (GD) approach to approximately solve constrained optimization (CO) problems for MOO (see Fig <ref type="figure">3(a)</ref>).</p><p>In the first step, we transform variables to prepare for optimization by following the common practice in machine learning: Let x be the original set of parameters, which can be categorical, integer, or continuous variables. If a variable is categorical, we use one-hot encoding to create dummy variables. For example, if x d takes values {a, b, c}, we create three boolean variables, As such, the CO problem deals only with continuous variables in [0,1], which we denote as x = x 1 , . . . , x D ∈ [0, 1]. After a solution is returned for the CO problem, we set the value for a categorical attribute based on the dummy variable with the highest value, and round the solution returned for a normalized integer variable to its closest integer.</p><p>Next, we focus on the CO problem. Our design of a Multi-Objective Gradient Descent (MOGD) solver uses carefullycrafted loss functions to guide gradient descent to find the minimum of a target objective while satisfying a variety of constraints, where both the target objective and constraints can be specified on complex models, e.g., DNNs and GPs.</p><p>1. Single objective optimization. As a base case, we consider single-objective optimization, minimize F 1 (x) = Ψ 1 (x). For optimization, we set the loss function simply as, L(x) = F 1 (x). Then starting from an initial configuration, x 0 , gradient descent (GD) will iteratively adjust the configuration to a sequence x 1 , . . . , x n in order to minimize the loss, until it converges to a local minimum or reaches a maximum of steps.</p><p>To increase the chance of hitting a better local minimum (closer to the global minimum), we use a multi-start method to try gradient descent from multiple initial points, and finally choose x * that gives the smallest value among these trials. Further, among GD variants (e.g, momentum, SGD) we use Adaptive Moment Estimation (Adam) <ref type="bibr" target="#b24">[25]</ref>, recommended as an overall best choice for achieving better local minima <ref type="bibr" target="#b24">[25]</ref>. To cope with the constraint, 0 ≤ x d ≤ 1, we restrict GD such that when it tries to push x d out of the range, we set x d to the boundary value. In future iterations, it can still adjust other variables, but not x d across its boundaries, to reduce the loss.</p><p>2. Constrained optimization. Next we consider a constrained optimization (CO) problem, as shown in Figure <ref type="figure">3(a)</ref>. WLOG, we treat F 1 as the target objective, and F j ∈ [F L j , F U j ], j = 1, . . . , k, as constraints, which are set by our middle point probe method (Eq. 2). To solve the CO problem, we seek to design a new loss function, L(x), such that by minimizing this loss, we can minimize F 1 (x) while at the same time satisfying all the constraints. Our proposed loss function is as follows:</p><formula xml:id="formula_8">L(x) = 1{0 ≤ F1 (x) ≤ 1} • F1 (x) 2 + k j=1 1{ Fj (x) &gt; 1 ∨ Fj (x) &lt; 0}[( Fj (x) - 1 2 ) 2 + P ]<label>(3)</label></formula><p>where</p><formula xml:id="formula_9">Fj (x) = Fj (x)-F L j F U j -F L j , j ∈ [1, k],</formula><p>denotes the normalized value of each objective. Since the range of each objective function F j (x) varies, we normalize each objective according to its upper and lower bounds, so that a valid objective value Fj (x) ∈ [0, 1]. Further, P is a constant for extra penalty.</p><p>Fig. <ref type="figure">3</ref>(c)-3(f) illustrate the loss function for the CO problem, C F1F2 : min F 1 (lat.) such that F 1 ∈ [100, 200] and F 2 (cost) in <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b15">16]</ref>, shown in the previous section for TPCx-BB Q2.</p><p>tion is returned for the CO problem, we set the categorical attribute based on the dummy varihe highest value, and round the solution for a integer variable to its closest integer.</p><formula xml:id="formula_10">x ⇤ = arg min x F 1 (x) [ 1 ] ect to F L 1  F 1 (x)  F U 1 [2]</formula><p>. . .</p><formula xml:id="formula_11">F L k  F k (x)  F U k 0  x i  1, i = 1, 2, . . . , D [3]</formula><p>bjective Gradient Descent (MOGD) Solver. cus on the CO problem depicted in Figure <ref type="figure">X(a)</ref>. of a Multi-Objective Gradient Descent (MOGD) carefully-crafted loss functions to guide gradient find the minimum of a target objective while satriety of constraints, while both the target objecnstraints are specified over complex models, e.g., s, GPs, or other regression functions. jective optimization. We begin our discussion e objective optimization, minimize F 1 (x) = 1 (x), rt <ref type="bibr" target="#b0">[1]</ref> in Figure X(a). To enable gradient descent, loss function simply as, L = F 1 (x). Let x n denfiguration computed after iteration n. Initially x 0 is the default configuration. We can use 1 (x n ) to compute the predicted value of the 1 (x n ) under the current configuration x n . We e loss L to estimate how well the configuration es (e.g., minimizes) the value of the objective. mpute the gradient of the loss function as r n x to adjust the configuration for the next iteration inimizes L. That is, we iteratively choose the nfiguration as x n+1 = x n ↵r n x . Currently, we aptive moment estimation SGD approach from pute the gradient of the loss function r n</p><p>x . The ess repeats iteratively until we find the optimal on x ⇤ that minimizes loss and yields the optimal e target objective F 1 (x ⇤ ). [This is obvious.] ned optimization. Then consider a constrained n (CO) problem, where F 1 as the target of opand constraints are F i 2 [F L j , F U j ], i = 1, . . . , k, de Parts <ref type="bibr" target="#b0">[1]</ref> and <ref type="bibr" target="#b1">[2]</ref> of Figure X(a). We can use rocess as above to address the CO problem, but </p><formula xml:id="formula_12">input, x output, f(x) -2 -1 0 1 2 -4 -2 0 2 4 (a) Prior input, x output, f(x) -3 -2 -1 0 1 2 3 -4 -2 0 2 4 (b) Posterior</formula><formula xml:id="formula_13">) = Cov(f (x), f(x 0 )).</formula><p>A GP is a distribution over functions with a special property: if we fix any vector of inputs (x1, . . . , xn), the output vector f = (f (x1), f(x2), . . . , f(xn)) has a multivariate Gaussian distribution. Specifically, f ⇠ N (m, K), where m is the vector (m(x1) . . . m(xn)) containing the mean function evaluated at all the inputs and K is a matrix of covariances Kij = k(xi, xj ) between all the input pairs.</p><p>The covariance function has a vital role. Recall that the idea was to approximate f by interpolating between its values at nearby points. The covariance function helps determine which points are "nearby". If two points are far away, then their function values should be only weakly related, i.e., their covariance should be near 0. On the other hand, if two points are nearby, then their covariance should be large in magnitude. We accomplish this by using a covariance function that depends on the distance between the input points.</p><p>In this work, we use standard choices for the mean and covariance functions. We choose the mean function m(x) = 0, which is a standard choice when we have no prior information about the UDF. For the covariance function, we use the squared exponential one, which in its simplest form is k(x, <ref type="figure"></ref>and<ref type="figure">2</ref> f and l are its parameters. The signal variance 2  f primarily determines the variance of the function value at individual points, i.e., x = x 0 . More important is the lengthscale l, which determines how rapidly the covariance decays as x and x 0 move farther apart. If l is small, the covariance decays rapidly, so sample functions from the result GP will have many small bumps; if l is large, then these functions will tend to be smoother.</p><formula xml:id="formula_14">x 0 ) = 2 f e 1 2l2 kx x 0 k 2 , where k•k is Euclidean distance,</formula><p>The key assumption made by GP modeling is that at any point x, the function value f (x) can be accurately predicted using the function values at nearby points. GPs are flexible to model different types of functions by using an appropriate covariance function <ref type="bibr" target="#b17">[18]</ref>. For instance, for smooth functions, squared-exponential covariance functions work well; for less smooth functions, Matern covariance functions work well (where smoothness is defined by "mean-squared differentiability"). In this paper, we focus on the common squaredexponential functions, which are shown experimentally to work well for the UDFs in our applications (see §6.4). In general, the user can choose a suitable covariance function based on the well-defined properties of UDFs, and plug it into our framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Inference for New Input Points</head><p>We next describe how to use a GP to predict the function outputs at new inputs. Denote the training data by X ⇤ = {x ⇤ i |i = 1, . . . , n} for the inputs and f ⇤ = {f ⇤ i |i = 1, . . . , n} for the function values. In this section, we assume that we are told a fixed set of m test inputs X = (x1, x2, ..., xm) at which we wish to predict the function values. Denote the unknown function values at the test points by f = (f1, f2, ..., fm). The vector (f ⇤ , f ) is a random vector because each fi:i=1...m is random, and by the definition of a GP, this vector simply has a multivariate Gaussian distribution. This distribution is:</p><formula xml:id="formula_15"> f ⇤ f ⇠ N 0,  K(X ⇤ , X ⇤ ) K(X ⇤ , X) K(X, X ⇤ ) K(X, X) ! ,<label>(1)</label></formula><p>where we have written the covariances as matrix with four blocks. The block K(X ⇤ , X) is an n⇥m matrix of the covariances between all training and test points, i.e., K(X ⇤ , X)ij = k(x ⇤ i , xj ). Similar notions are for K(X ⇤ , X ⇤ ), K(X, X), and K(X, X ⇤ ).</p><p>Now that we have a joint distribution, we can predict the unknown test outputs f by computing the conditional distribution of f given the training data and test inputs. Applying the standard formula for the conditional of a multivariate Gaussian yields:</p><formula xml:id="formula_16">f | X, X ⇤ , f ⇤ ⇠ N (m, ⌃), where<label>(2)</label></formula><formula xml:id="formula_17">m = K(X, X ⇤ )K(X ⇤ , X ⇤ ) 1 f ⇤ ⌃ = K(X, X) K(X, X ⇤ )K(X ⇤ , X ⇤ ) 1 K(X ⇤ , X)</formula><p>To interpret m intuitively, imagine that m = 1, i.e., we wish to predict only one output. Then K(X, X ⇤ )K(X ⇤ , X ⇤ ) 1 is an ndimensional vector, and the mean m We now consider the complexity of this inference step. Note that once the training data is collected, the inverse covariance matrix K(X ⇤ , X ⇤ ) 1 can be computed once, with a cost of O(n 3 ). Then given a test point x (or X has size 1), inference involves computing K(X, X ⇤ ) and multiplying matrices, which has a cost of O(n 2 ). The space complexity is also O(n 2 ), for storing these matrices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Learning the Hyperparameters</head><p>Typically, the covariance functions have some free parameters, called hyperparameters, such as the lengthscale l of the squaredexponential function. The hyperparameters determine how quickly the confidence estimates expand as test points move further from the training data. For example, in Fig. <ref type="figure" target="#fig_3">1(b)</ref>, if the lengthscale decreases, the spread of the function will increase, meaning that there is less confidence in the predictions.</p><p>We can learn the hyperparameters using the training data (see Chapter 5, <ref type="bibr" target="#b17">[18]</ref>). We adopt maximum likelihood estimation (MLE), a standard technique for this problem. Let ✓ be the vector of hyperparameters. The log likelihood function is L(✓) := log p(f ⇤ |X ⇤ , ✓) = log N (X ⇤ ; m, ⌃); here we use N to refer to the density of the Gaussian distribution, and m and ⌃ are defined in Eq. ( <ref type="formula" target="#formula_7">2</ref>). MLE solves for the value of ✓ that maximizes L(✓). We use gradient descent, a standard method for this task. Its complexity is O(n 3 ) due to the cost of inverting the matrix K(X ⇤ , X ⇤ ) 1 . Gradient descent requires many steps to compute the optimal ✓; thus, retraining often has a high cost for large numbers of training points. Note that when the training data X ⇤ changes, ✓ that maximizes the log likelihood L(✓) may also change. Thus, one would need to maximize the log likelihood to update the hyperparameters. In §5.3, we will discuss retraining strategies that aim to reduce this computation cost.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">UNCERTAINTY IN QUERY RESULTS</head><p>So far in our discussions of GPs, we have assumed that all the input values are known in advance. However, our work aims to compute UDFs on uncertain input. In this section, we describe how jective Gradient Descent er an important subroutine, optimize(), nstrained optimization problem (line 13 of all that our objective functions are given , i(x), i = 1 . . . k, where each model is ear and some variables among x can be inicted to a single objective, this problem is -integer nonlinear programming (MINLP) P-hard <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b14">15]</ref>. There is not one general t will work e↵ectively for every nonlinproblem <ref type="bibr" target="#b29">[30]</ref>. For example, many of the 1] fail to run because they assume certhe objective function F , e.g., twice coniable (Bonmin <ref type="bibr" target="#b4">[5]</ref>) or factorable into the ivariate functions (Couenne <ref type="bibr" target="#b6">[7]</ref>), which do d models, e.g., represented as Deep Neural . The most general MINLP solver, Knir learned models but very slowly, e.g., 42 g a single-objective optimization problem model is a DNN, or 17 minutes when the an Process (GP). Such a solution is too relaxed to continuous variables in [0, 1]. As such, the constrained optimization (CO) problem deals with continuous variables in [0,1], which we denote as x = x1, . . . , xD 2 [0, 1]. After a solution is returned for the CO problem, we set the value for a categorical attribute based on the dummy variable with the highest value, and round the solution for a normalized integer variable to its closest integer.</p><note type="other">Neural Networks Multi-Layer Perceptron</note><p>x ⇤ = arg min x F1(x) <ref type="bibr" target="#b0">[ 1 ]</ref> subject to</p><formula xml:id="formula_18">F L 1  F1(x)  F U 1 [2] F L 2  F2(x)  F U 2 [2]</formula><p>. . .</p><formula xml:id="formula_19">F L k  Fk(x)  F U k 0  xi  1, i = 1, 2, . . . , D [3]</formula><p>Multi-Objective Gradient Descent (MOGD) Solver. Next, we focus on the CO problem depicted in Figure <ref type="figure">X(a)</ref>. Our design of a Multi-Objective Gradient Descent (MOGD) solver uses carefully-crafted loss functions to guide gradient descent to find the minimum of a target objective while satisfying a variety of constraints, while both the target objective and constraints are specified over complex models, e.g., using DNNs, GPs, or other regression functions.</p><p>Single objective optimization. We begin our discussion with a single objective optimization, minimize F1(x) = 1(x), which is Part <ref type="bibr" target="#b0">[1]</ref> in Figure X(a). To enable gradient descent, we set the loss function simply as, L = F1(x). Let x n denote the configuration computed after iteration n. Initially n = 0 and x 0 is the default configuration. We can use the model 1(x n ) to compute the predicted value of the objective F1(x n ) under the current configuration x n . We then use the loss L to estimate how well the configuration x n optimizes (e.g., minimizes) the value of the objective. We then compute the gradient of the loss function as r n x and use it to adjust the configuration for the next iteration so that it minimizes L. That is, we iteratively choose the (n + 1) th configuration as x n+1 = x n ↵r n</p><p>x. Currently, we use the adaptive moment estimation SGD approach from <ref type="bibr" target="#b17">[18]</ref> to compute the gradient of the loss function r n</p><p>x. The above process repeats iteratively until we find the optimal configuration x ⇤ that minimizes loss and yields the optimal value of the target objective F1(x ⇤ ). . There is not one general rk e↵ectively for every nonlin- <ref type="bibr" target="#b29">[30]</ref>. For example, many of the run because they assume certive function F , e.g., twice connmin <ref type="bibr" target="#b4">[5]</ref>) or factorable into the nctions (Couenne <ref type="bibr" target="#b6">[7]</ref>), which do e.g., represented as Deep Neural st general MINLP solver, Knimodels but very slowly, e.g., 42 -objective optimization problem DNN, or 17 minutes when the ss (GP). Such a solution is too relaxed to continuous variables in [0, 1]. As such, the constrained optimization (CO) problem deals with continuous variables in [0,1], which we denote as x = x1, . . . , xD 2 [0, 1]. After a solution is returned for the CO problem, we set the value for a categorical attribute based on the dummy variable with the highest value, and round the solution for a normalized integer variable to its closest integer.</p><p>x ⇤ = arg min x F1(x) <ref type="bibr" target="#b0">[ 1 ]</ref> subject to</p><formula xml:id="formula_20">F L 1  F1(x)  F U 1 [2] F L 2  F2(x)  F U 2 [2] . . . F L k  Fk(x)  F U k 0  xi  1, i = 1, 2, . . . , D [3]</formula><p>Multi-Objective Gradient Descent (MOGD) Solver. Next, we focus on the CO problem depicted in Figure <ref type="figure">X(a)</ref>. Our design of a Multi-Objective Gradient Descent (MOGD) solver uses carefully-crafted loss functions to guide gradient descent to find the minimum of a target objective while satisfying a variety of constraints, while both the target objective and constraints are specified over complex models, e.g., using DNNs, GPs, or other regression functions.</p><p>Single objective optimization. We begin our discussion with a single objective optimization, minimize F1(x) = 1(x), which is Part <ref type="bibr" target="#b0">[1]</ref> in Figure <ref type="figure">X</ref>(a). To enable gradient descent, we set the loss function simply as, L = F1(x). Let x n denote the configuration computed after iteration n. Initially n = 0 and x 0 is the default configuration. We can use the model 1(x n ) to compute the predicted value of the objective F1(x n ) under the current configuration x n . We then use the loss L to estimate how well the configuration x n optimizes (e.g., minimizes) the value of the objective. We then compute the gradient of the loss function as r n x and use it to adjust the configuration for the next iteration so that it minimizes L. That is, we iteratively choose the (n + 1) th configuration as x n+1 = x n ↵r n</p><p>x. Currently, we use the adaptive moment estimation SGD approach from <ref type="bibr" target="#b17">[18]</ref> to compute the gradient of the loss function r n</p><p>x. The above process repeats iteratively until we find the optimal configuration x ⇤ that minimizes loss and yields the optimal value of the target objective F1(x ⇤ ). After a solution is returned for the CO problem, we set the value for a categorical attribute based on the dummy variable with the highest value, and round the solution for a normalized integer variable to its closest integer.</p><p>x ⇤ = arg min x F1(x) <ref type="bibr" target="#b0">[ 1 ]</ref> subject to</p><formula xml:id="formula_21">F L 1  F1(x)  F U 1 [2] F L 2  F2(x)  F U 2 [2]</formula><p>. . .</p><formula xml:id="formula_22">F L k  Fk(x)  F U k 0  xi  1, i = 1, 2, . . . , D [3]</formula><p>Multi-Objective Gradient Descent (MOGD) Solver. Next, we focus on the CO problem depicted in Figure <ref type="figure">X(a)</ref>. Our design of a Multi-Objective Gradient Descent (MOGD) solver uses carefully-crafted loss functions to guide gradient descent to find the minimum of a target objective while satisfying a variety of constraints, while both the target objective and constraints are specified over complex models, e.g., using DNNs, GPs, or other regression functions.</p><p>Single objective optimization. We begin our discussion with a single objective optimization, minimize F1(x) = 1 (x), which is Part <ref type="bibr" target="#b0">[1]</ref> in Figure <ref type="figure">X(a)</ref>. To enable gradient descent, we set the loss function simply as, L = F1(x). Let x n denote the configuration computed after iteration n. Initially n = 0 and x 0 is the default configuration. We can use the model 1 (x n ) to compute the predicted value of the objective F1(x n ) under the current configuration x n . We then use the loss L to estimate how well the configuration x n optimizes (e.g., minimizes) the value of the objective. We then compute the gradient of the loss function as r n x and use it to adjust the configuration for the next iteration so that it minimizes L. That is, we iteratively choose the (n + 1) th configuration as x n+1 = x n ↵r n</p><p>x . Currently, we use the adaptive moment estimation SGD approach from <ref type="bibr" target="#b17">[18]</ref> to compute the gradient of the loss function r n</p><p>x . The above process repeats iteratively until we find the optimal configuration x ⇤ that minimizes loss and yields the optimal value of the target objective F1(x ⇤ ). After a solution is returned for the CO problem, we set the value for a categorical attribute based on the dummy variable with the highest value, and round the solution for a normalized integer variable to its closest integer.</p><p>x ⇤ = arg min x F1(x) <ref type="bibr" target="#b0">[ 1 ]</ref> subject to</p><formula xml:id="formula_23">F L 1  F1(x)  F U 1 [2] F L 2  F2(x)  F U 2 [2]</formula><p>. . .</p><formula xml:id="formula_24">F L k  Fk(x)  F U k 0  xi  1, i = 1, 2, . . . , D [3]</formula><p>Multi-Objective Gradient Descent (MOGD) Solver. Next, we focus on the CO problem depicted in Figure <ref type="figure">X(a)</ref>. Our design of a Multi-Objective Gradient Descent (MOGD) solver uses carefully-crafted loss functions to guide gradient descent to find the minimum of a target objective while satisfying a variety of constraints, while both the target objective and constraints are specified over complex models, e.g., using DNNs, GPs, or other regression functions.</p><p>Single objective optimization. We begin our discussion with a single objective optimization, minimize F1(x) = 1 (x), which is Part <ref type="bibr" target="#b0">[1]</ref> in Figure <ref type="figure">X(a)</ref>. To enable gradient descent, we set the loss function simply as, L = F1(x). Let x n denote the configuration computed after iteration n. Initially n = 0 and x 0 is the default configuration. We can use the model 1 (x n ) to compute the predicted value of the objective F1(x n ) under the current configuration x n . We then use the loss L to estimate how well the configuration x n optimizes (e.g., minimizes) the value of the objective. We then compute the gradient of the loss function as r n x and use it to adjust the configuration for the next iteration so that it minimizes L. That is, we iteratively choose the (n + 1) th configuration as x n+1 = x n ↵r n</p><p>x . Currently, we use the adaptive moment estimation SGD approach from <ref type="bibr" target="#b17">[18]</ref> to compute the gradient of the loss function r n</p><p>x . The above process repeats iteratively until we find the optimal configuration x ⇤ that minimizes loss and yields the optimal value of the target objective F1(x ⇤ ). jective Gradient Descent er an important subroutine, optimize(), nstrained optimization problem (line 13 of all that our objective functions are given , i(x), i = 1 . . . k, where each model is ear and some variables among x can be inicted to a single objective, this problem is -integer nonlinear programming (MINLP) P-hard <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b14">15]</ref>. There is not one general at will work e↵ectively for every nonlinproblem <ref type="bibr" target="#b29">[30]</ref>. For example, many of the 1] fail to run because they assume certhe objective function F , e.g., twice coniable (Bonmin <ref type="bibr" target="#b4">[5]</ref>) or factorable into the ivariate functions (Couenne <ref type="bibr" target="#b6">[7]</ref>), which do d models, e.g., represented as Deep Neural . The most general MINLP solver, Knir learned models but very slowly, e.g., 42 g a single-objective optimization problem model is a DNN, or 17 minutes when the ian Process (GP). Such a solution is too and x c i , among which only one takes the value '1'. Afterwards all the variables are normalized to the range [0, 1], and boolean variables and normalized integer variables are relaxed to continuous variables in [0, 1]. As such, the constrained optimization (CO) problem deals with continuous variables in [0,1], which we denote as x = x1, . . . , xD 2 [0, 1]. After a solution is returned for the CO problem, we set the value for a categorical attribute based on the dummy variable with the highest value, and round the solution for a normalized integer variable to its closest integer.</p><p>x ⇤ = arg min x F1(x) <ref type="bibr" target="#b0">[ 1 ]</ref> subject to</p><formula xml:id="formula_25">F L 1  F1(x)  F U 1 [2] F L 2  F2(x)  F U 2 [2] . . . F L k  Fk(x)  F U k 0  xi  1, i = 1, 2, . .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>. , D [3]</head><p>Multi-Objective Gradient Descent (MOGD) Solver. Next, we focus on the CO problem depicted in Figure <ref type="figure">X(a)</ref>. Our design of a Multi-Objective Gradient Descent (MOGD) solver uses carefully-crafted loss functions to guide gradient descent to find the minimum of a target objective while satisfying a variety of constraints, while both the target objective and constraints are specified over complex models, e.g., using DNNs, GPs, or other regression functions.</p><p>Single objective optimization. We begin our discussion with a single objective optimization, minimize F1(x) = 1(x), which is Part <ref type="bibr" target="#b0">[1]</ref> in Figure X(a). To enable gradient descent, we set the loss function simply as, L = F1(x). Let x n denote the configuration computed after iteration n. Initially n = 0 and x 0 is the default configuration. We can use the model 1(x n ) to compute the predicted value of the objective F1(x n ) under the current configuration x n . We then use the loss L to estimate how well the configuration x n optimizes (e.g., minimizes) the value of the objective. We then compute the gradient of the loss function as r n x and use it to adjust the configuration for the next iteration so that it minimizes L. That is, we iteratively choose the (n + 1) th configuration as x n+1 = x n ↵r n</p><p>x. Currently, we use the adaptive moment estimation SGD approach from <ref type="bibr" target="#b17">[18]</ref> to compute the gradient of the loss function r n</p><p>x. The above process repeats iteratively until we find the optimal configuration x ⇤ that minimizes loss and yields the optimal value of the target objective F1(x ⇤ ). . There is not one general rk e↵ectively for every nonlin-30]. For example, many of the run because they assume certive function F , e.g., twice connmin <ref type="bibr" target="#b4">[5]</ref>) or factorable into the nctions (Couenne <ref type="bibr" target="#b6">[7]</ref>), which do e.g., represented as Deep Neural st general MINLP solver, Knimodels but very slowly, e.g., 42 -objective optimization problem DNN, or 17 minutes when the ss (GP). Such a solution is too and x c i , among which only one takes the value '1'. Afterwards all the variables are normalized to the range [0, 1], and boolean variables and normalized integer variables are relaxed to continuous variables in [0, 1]. As such, the constrained optimization (CO) problem deals with continuous variables in [0,1], which we denote as x = x1, . . . , xD 2 [0, 1]. After a solution is returned for the CO problem, we set the value for a categorical attribute based on the dummy variable with the highest value, and round the solution for a normalized integer variable to its closest integer.</p><p>x ⇤ = arg min x F1(x) <ref type="bibr" target="#b0">[ 1 ]</ref> subject to</p><formula xml:id="formula_26">F L 1  F1(x)  F U 1 [2] F L 2  F2(x)  F U 2 [2]</formula><p>. . .</p><formula xml:id="formula_27">F L k  Fk(x)  F U k 0  xi  1, i = 1, 2, . .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>. , D [3]</head><p>Multi-Objective Gradient Descent (MOGD) Solver. Next, we focus on the CO problem depicted in Figure <ref type="figure">X(a)</ref>. Our design of a Multi-Objective Gradient Descent (MOGD) solver uses carefully-crafted loss functions to guide gradient descent to find the minimum of a target objective while satisfying a variety of constraints, while both the target objective and constraints are specified over complex models, e.g., using DNNs, GPs, or other regression functions.</p><p>Single objective optimization. We begin our discussion with a single objective optimization, minimize F1(x) = 1(x), which is Part <ref type="bibr" target="#b0">[1]</ref> in Figure X(a). To enable gradient descent, we set the loss function simply as, L = F1(x). Let x n denote the configuration computed after iteration n. Initially n = 0 and x 0 is the default configuration. We can use the model 1(x n ) to compute the predicted value of the objective F1(x n ) under the current configuration x n . We then use the loss L to estimate how well the configuration x n optimizes (e.g., minimizes) the value of the objective. We then compute the gradient of the loss function as r n x and use it to adjust the configuration for the next iteration so that it minimizes L. That is, we iteratively choose the (n + 1) th configuration as x n+1 = x n ↵r n</p><p>x. Currently, we use the adaptive moment estimation SGD approach from <ref type="bibr" target="#b17">[18]</ref> to compute the gradient of the loss function r n</p><p>x. The above process repeats iteratively until we find the optimal configuration x ⇤ that minimizes loss and yields the optimal value of the target objective F1(x ⇤ ). [This is obvious.] Constrained optimization. Then consider a constrained optimization (CO) problem, where F1 as the target of optimization and constraints are Fi 2 [F L j , F U j ], i = 1, . . . , k, which include Parts <ref type="bibr" target="#b0">[1]</ref> and <ref type="bibr" target="#b1">[2]</ref> of Figure <ref type="figure">X(a)</ref>. We can use 8</p><p>In this work, we propose a novel solver that employs a customized gradient descent approach to approximately solve our constrained optimization problems involving multiple objective functions. The problem is illustrated in Figure <ref type="figure">??</ref>.</p><formula xml:id="formula_28">x ⇤ = arg min x F1(x) [ 1 ] subject to F L 1  F1(x)  F U 1 [2]</formula><p>. . . Next, we focus on the CO problem depicted in Figure X(a). Our design of a Multi-Objective Gradient Descent (MOGD) solver uses carefully-crafted loss functions to guide gradient descent to find the minimum of a target objective while satisfying a variety of constraints, where both the target objective and constraints can be specified over complex models, e.g., using DNNs, GPs, or other regression functions.</p><formula xml:id="formula_29">F L k  Fk(x)  F U k 0  xd  1, d = 1,</formula><p>Single objective optimization. As a base case, we consider singleobjective optimization, minimize F1(x) = 1(x), which is Part <ref type="bibr" target="#b0">[1]</ref> in Figure X(a). For optimization, we set the loss function simply as, L(x) = F1(x). Then starting from an initial configuration, x 0 , gradient descent will iteratively adjust the configuration to a sequence x 1 , . . . , x n in order to minimize the loss, until it reaches a local minimum or a maximum of steps allowed. To increase the chance of hitting a global minimum, we use a standard multi-start method to try gradient descent from multiple initial values of x, and finally choose x ⇤ that gives the smallest value among these trials.</p><p>Let x n denote the configuration computed after iteration n. Initially n = 0 and x 0 is the default configuration. We can use the model 1(x n ) to compute the predicted value of the objective F1(x n ) under the current configuration x n . We then use the loss L to estimate how well the configuration x n optimizes (e.g., minimizes) the value of the objective. We then compute the gradient of the loss function as r n</p><p>x and use it to adjust the configuration for the next iteration so that it minimizes L. That is, we iteratively choose the (n + 1) th configuration as x n+1 = x n ↵r n</p><p>x . Currently, we use the adaptive moment estimation SGD approach from <ref type="bibr" target="#b16">[17]</ref> to compute the gradient of the loss function r n</p><p>x . The above process repeats iteratively until we find the optimal configuration x ⇤ that minimizes loss and yields the optimal value of the target objective F1(x ⇤ ). [This is obvious.] Constrained optimization. Then consider a constrained optimization (CO) problem, where F1 as the target of optimization and constraints are Fi 2 [F L j , F U j ], i = 1, . . . , k, which include Parts [1] and <ref type="bibr" target="#b1">[2]</ref> of Figure <ref type="figure">X(a)</ref>. We can use the same process as above to address the CO problem, but with a different loss func scaled value of the objective as following:</p><formula xml:id="formula_30">loss (i) = k X j=1 [ { Fj (x) &gt; 1 _ Fj (x) &lt; 0}( Fj (x) + P ] + {0  Fi(x)  1} Fi(x) 2 where Fj (x) = F j (x) F L j F U j F L j</formula><p>, for j 2 [1, k], and P is const the range of each objective function Fj (x) varies, we fir ize each objective according to its upper and lower boun F L j = 0, F U j = 1, and a valid objective value Fj (x) The loss function includes two part. One is to push into its constraints region. If an objective Fj (x) cann the constraints ( Fj (x) &gt; 0 _ Fj (x) &lt; 1), it is goin tribute a loss according to its distance to the constraints push objectives satisfying the constraints, we further ass tra penalty P to stress their importances. The other par for the optimization target Fi. Once Fi(x) laid in the c region (0  Fi(x)  1), we generate a loss according to Therefore, the iterative forward and backward paths can objective values satisfying their constraints as well as mi target objective.</p><p>Supporting additional constraints. Two additional c 0  xi  1 and g(x)  0).</p><p>Handling model uncertainty. Fi(x) = E[Fi(x)]+↵•s</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Approximate and Parallel Algorith</head><p>Approximate Sequential Algorithm (PF-AS): Wh plement single-objective optimization (Line 2 of Algorit constrained optimization (Line 13) using the above proce obtain a new algorithm called PF Approximate Sequen that this leads to an approximate Pareto set for the MOO because each solution of a constrained optimization can timal as backpropogation is stuck at a local minima. In if the objective function is complex with many local mi hard for any algorithm to guarantee to find global minim</p><p>We finally present a parallel version of the approxim rithm, called PF-Approximate Parallel (PF-AP ). The m ence from the approximate sequential algorithm is that hyperrecetange we aim to explore at the moment, we p into a l k grid and for each grid cell, construct a constra mization (CO) problem using the the Middle Point Probe We send these l k CO problem to the DNN-based solve neously. Internally, the DNN solver will solve these pr parallel (using a multi-threaded implementation). Some o will not return any Pareto point and hence will be omitted of those cells that returns a Pareto point, the Pareto point cell into a set of sub-hyperrectangles that need to be f plored, and are added to a priority queue as before. After sub-hyperrectangle with the largest volume is removed queue. To explore it, we further partition it into l k cells a solver to solve their corresponding CO problems simul This process terminates when the queue becomes empty.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">AUTOMATIC SOLUTION SELEC</head><p>Our optimizer implements three strategies to recom job configurations from the computed Pareto frontier. Utopia Nearest (UN) strategy chooses the Pareto optimal est to the Utopia point by computing the Euclidean distan point in F to f U , and returns the point that minimizes th A variant is the Weighted Utopia Nearest (WUN) strate As a base case, we consider single-F1(x) = 1(x), which is Part tion, we set the loss function simrting from an initial configuration, vely adjust the configuration to a minimize the loss, until it reaches of steps allowed. To increase the um, we use a standard multi-start from multiple initial values of x, s the smallest value among these n computed after iteration n. Inifault configuration. We can use the predicted value of the objeconfiguration x n . We then use the configuration x n optimizes (e.g., ctive. We then compute the gradid use it to adjust the configuration inimizes L. That is, we iteratively ion as x n+1 = x n ↵r n x . Curnt estimation SGD approach from the loss function r n</p><p>x . The above we find the optimal configuration ds the optimal value of the target us.] n consider a constrained optimizathe target of optimization and con-1, . . . , k, which include Parts <ref type="bibr" target="#b0">[1]</ref> and <ref type="bibr" target="#b1">[2]</ref> of Figure <ref type="figure">X(a)</ref>. We can use the same process as above to address the CO problem, but with a different loss function and a scaled value of the objective as following:</p><formula xml:id="formula_31">loss (i) = k X j=1 [ { Fj (x) &gt; 1 _ Fj (x) &lt; 0}( Fj (x) 0.5) 2 + P ] + {0  Fi(x)  1} Fi(x) 2 (4)</formula><p>where Fj (x) =</p><formula xml:id="formula_32">Fj (x) F L j F U j F L j</formula><p>, for j 2 [1, k], and P is constant. Since the range of each objective function Fj (x) varies, we first normalize each objective according to its upper and lower bounds, so that F L j = 0, F U j = 1, and a valid objective value Fj (x) 2 [0, 1]. The loss function includes two part. One is to push objectives into its constraints region. If an objective Fj (x) cannot satisfy the constraints ( Fj (x) &gt; 0 _ Fj (x) &lt; 1), it is going to contribute a loss according to its distance to the constraints region. To push objectives satisfying the constraints, we further assign an extra penalty P to stress their importances. The other part of loss is for the optimization target Fi. Once Fi(x) laid in the constraints region (0  Fi(x)  1), we generate a loss according to its value. Therefore, the iterative forward and backward paths can push the objective values satisfying their constraints as well as minimize the target objective.</p><p>Supporting additional constraints. Two additional constraints: 0  xi  1 and g(x)  0).</p><p>Handling model uncertainty.</p><formula xml:id="formula_33">Fi(x) = E[Fi(x)]+↵•std[Fi(x)]</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Approximate and Parallel Algorithms</head><p>Approximate Sequential Algorithm (PF-AS): When we implement single-objective optimization (Line 2 of Algorithm 1) and constrained optimization (Line 13) using the above procedures, we obtain a new algorithm called PF Approximate Sequential. Note that this leads to an approximate Pareto set for the MOO problem because each solution of a constrained optimization can be suboptimal as backpropogation is stuck at a local minima. In practice, if the objective function is complex with many local minima, it is hard for any algorithm to guarantee to find global minima.</p><p>We finally present a parallel version of the approximate algorithm, called PF-Approximate Parallel (PF-AP ). The main difference from the approximate sequential algorithm is that given any hyperrecetange we aim to explore at the moment, we partition it into a l k grid and for each grid cell, construct a constrained optimization (CO) problem using the the Middle Point Probe (Eq. 4.2). We send these l k CO problem to the DNN-based solver simultaneously. Internally, the DNN solver will solve these problems in parallel (using a multi-threaded implementation). Some of the cells will not return any Pareto point and hence will be omitted. For each of those cells that returns a Pareto point, the Pareto point breaks the cell into a set of sub-hyperrectangles that need to be further explored, and are added to a priority queue as before. Afterwards, the sub-hyperrectangle with the largest volume is removed from the queue. To explore it, we further partition it into l k cells and ask the solver to solve their corresponding CO problems simultaneously. This process terminates when the queue becomes empty.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">AUTOMATIC SOLUTION SELECTION</head><p>Our optimizer implements three strategies to recommend new job configurations from the computed Pareto frontier. First, the Utopia Nearest (UN) strategy chooses the Pareto optimal point closest to the Utopia point by computing the Euclidean distance of each point in F to f U , and returns the point that minimizes the distance.  ions to guide gradient descent to jective while satisfying a variety rget objective and constraints can s, e.g., using DNNs, GPs, or other s a base case, we consider single-F1(x) = 1(x), which is Part ion, we set the loss function simting from an initial configuration, ely adjust the configuration to a minimize the loss, until it reaches of steps allowed. To increase the um, we use a standard multi-start rom multiple initial values of x, the smallest value among these n computed after iteration n. Inifault configuration. We can use the predicted value of the objecnfiguration x n . We then use the configuration x n optimizes (e.g., tive. We then compute the gradiuse it to adjust the configuration nimizes L. That is, we iteratively on as x n+1 = x n ↵r n x . Curnt estimation SGD approach from the loss function r n</p><p>x . The above e find the optimal configuration s the optimal value of the target s.] n consider a constrained optimizahe target of optimization and con-1, . . . , k, which include Parts <ref type="bibr" target="#b0">[1]</ref> and <ref type="bibr" target="#b1">[2]</ref> of Figure <ref type="figure">X(a)</ref>. We can use the same process as above to address the CO problem, but with a different loss function and a scaled value of the objective as following:</p><formula xml:id="formula_34">loss (i) = k X j=1 [ { Fj (x) &gt; 1 _ Fj (x) &lt; 0}( Fj (x) 0.5) 2 + P ] + {0  Fi(x)  1} Fi(x) 2 (4)</formula><p>where Fj (x) =</p><formula xml:id="formula_35">Fj (x) F L j F U j F L j</formula><p>, for j 2 [1, k], and P is constant. Since the range of each objective function Fj (x) varies, we first normalize each objective according to its upper and lower bounds, so that F L j = 0, F U j = 1, and a valid objective value Fj (x) 2 [0, 1]. The loss function includes two part. One is to push objectives into its constraints region. If an objective Fj (x) cannot satisfy the constraints ( Fj (x) &gt; 0 _ Fj (x) &lt; 1), it is going to contribute a loss according to its distance to the constraints region. To push objectives satisfying the constraints, we further assign an extra penalty P to stress their importances. The other part of loss is for the optimization target Fi. Once Fi(x) laid in the constraints region (0  Fi(x)  1), we generate a loss according to its value. Therefore, the iterative forward and backward paths can push the objective values satisfying their constraints as well as minimize the target objective.</p><p>Supporting additional constraints. Two additional constraints:</p><formula xml:id="formula_36">0  xi  1 and g(x)  0).</formula><p>Handling model uncertainty.</p><formula xml:id="formula_37">Fi(x) = E[Fi(x)]+↵•std[Fi(x)]</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Approximate and Parallel Algorithms</head><p>Approximate Sequential Algorithm (PF-AS): When we implement single-objective optimization (Line 2 of Algorithm 1) and constrained optimization (Line 13) using the above procedures, we obtain a new algorithm called PF Approximate Sequential. Note that this leads to an approximate Pareto set for the MOO problem because each solution of a constrained optimization can be suboptimal as backpropogation is stuck at a local minima. In practice, if the objective function is complex with many local minima, it is hard for any algorithm to guarantee to find global minima.</p><p>We finally present a parallel version of the approximate algorithm, called PF-Approximate Parallel (PF-AP ). The main difference from the approximate sequential algorithm is that given any hyperrecetange we aim to explore at the moment, we partition it into a l k grid and for each grid cell, construct a constrained optimization (CO) problem using the the Middle Point Probe (Eq. 4.2). We send these l k CO problem to the DNN-based solver simultaneously. Internally, the DNN solver will solve these problems in parallel (using a multi-threaded implementation). Some of the cells will not return any Pareto point and hence will be omitted. For each of those cells that returns a Pareto point, the Pareto point breaks the cell into a set of sub-hyperrectangles that need to be further explored, and are added to a priority queue as before. Afterwards, the sub-hyperrectangle with the largest volume is removed from the queue. To explore it, we further partition it into l k cells and ask the solver to solve their corresponding CO problems simultaneously. This process terminates when the queue becomes empty.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">AUTOMATIC SOLUTION SELECTION</head><p>Our optimizer implements three strategies to recommend new job configurations from the computed Pareto frontier. First, the Utopia Nearest (UN) strategy chooses the Pareto optimal point closest to the Utopia point by computing the Euclidean distance of each point in F to f U , and returns the point that minimizes the distance. 8 ovel solver that employs a cush to approximately solve our connvolving multiple objective funcn Figure ??.</p><formula xml:id="formula_38">(x) [ 1 ] x1  F U 1 [2]xd  F U k d = 1, 2, . . . , D [3]xD</formula><p>variables for optimization by folmachine learning: Let x be the h can be categorical, integer, or ble is categorical , we use oneriables. For example, if xd takes e boolean variables, x a d , x b d , and the value '1'. Afterwards all the nge [0, 1], and boolean variables s are relaxed to continuous varistrained optimization (CO) probles in [0,1], which we denote as a solution is returned for the CO categorical attribute based on the value, and round the solution for ts closest integer. lem depicted in Figure <ref type="figure">X(a)</ref>. Our adient Descent (MOGD) solver ons to guide gradient descent to jective while satisfying a variety rget objective and constraints can s, e.g., using DNNs, GPs, or other s a base case, we consider single-F1(x) = 1(x), which is Part ion, we set the loss function simting from an initial configuration, ely adjust the configuration to a minimize the loss, until it reaches of steps allowed. To increase the um, we use a standard multi-start rom multiple initial values of x, the smallest value among these n computed after iteration n. Inifault configuration. We can use the predicted value of the objecnfiguration x n . We then use the configuration x n optimizes (e.g., tive. We then compute the gradiuse it to adjust the configuration nimizes L. That is, we iteratively on as x n+1 = x n ↵r n x . Curnt estimation SGD approach from the loss function r n</p><p>x . The above e find the optimal configuration s the optimal value of the target s.] n consider a constrained optimizahe target of optimization and con-1, . . . , k, which include Parts <ref type="bibr" target="#b0">[1]</ref> and <ref type="bibr" target="#b1">[2]</ref> of Figure <ref type="figure">X(a)</ref>. We can use the same process as above to address the CO problem, but with a different loss function and a scaled value of the objective as following:</p><formula xml:id="formula_39">loss (i) = k X j=1 [ { Fj (x) &gt; 1 _ Fj (x) &lt; 0}( Fj (x) 0.5) 2 + P ] + {0  Fi(x)  1} Fi(x) 2 (4)</formula><p>where Fj (x) =</p><formula xml:id="formula_40">Fj (x) F L j F U j F L j</formula><p>, for j 2 [1, k], and P is constant. Since the range of each objective function Fj (x) varies, we first normalize each objective according to its upper and lower bounds, so that F L j = 0, F U j = 1, and a valid objective value Fj (x) 2 [0, 1]. The loss function includes two part. One is to push objectives into its constraints region. If an objective Fj (x) cannot satisfy the constraints ( Fj (x) &gt; 0 _ Fj (x) &lt; 1), it is going to contribute a loss according to its distance to the constraints region. To push objectives satisfying the constraints, we further assign an extra penalty P to stress their importances. The other part of loss is for the optimization target Fi. Once Fi(x) laid in the constraints region (0  Fi(x)  1), we generate a loss according to its value. Therefore, the iterative forward and backward paths can push the objective values satisfying their constraints as well as minimize the target objective.</p><p>Supporting additional constraints. Two additional constraints:</p><formula xml:id="formula_41">0  xi  1 and g(x)  0).</formula><p>Handling model uncertainty.</p><formula xml:id="formula_42">Fi(x) = E[Fi(x)]+↵•std[Fi(x)]</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Approximate and Parallel Algorithms</head><p>Approximate Sequential Algorithm (PF-AS): When we implement single-objective optimization (Line 2 of Algorithm 1) and constrained optimization (Line 13) using the above procedures, we obtain a new algorithm called PF Approximate Sequential. Note that this leads to an approximate Pareto set for the MOO problem because each solution of a constrained optimization can be suboptimal as backpropogation is stuck at a local minima. In practice, if the objective function is complex with many local minima, it is hard for any algorithm to guarantee to find global minima.</p><p>We finally present a parallel version of the approximate algorithm, called PF-Approximate Parallel (PF-AP ). The main difference from the approximate sequential algorithm is that given any hyperrecetange we aim to explore at the moment, we partition it into a l k grid and for each grid cell, construct a constrained optimization (CO) problem using the the Middle Point Probe (Eq. 4.2). We send these l k CO problem to the DNN-based solver simultaneously. Internally, the DNN solver will solve these problems in parallel (using a multi-threaded implementation). Some of the cells will not return any Pareto point and hence will be omitted. For each of those cells that returns a Pareto point, the Pareto point breaks the cell into a set of sub-hyperrectangles that need to be further explored, and are added to a priority queue as before. Afterwards, the sub-hyperrectangle with the largest volume is removed from the queue. To explore it, we further partition it into l k cells and ask the solver to solve their corresponding CO problems simultaneously. This process terminates when the queue becomes empty.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">AUTOMATIC SOLUTION SELECTION</head><p>Our optimizer implements three strategies to recommend new job configurations from the computed Pareto frontier. First, the Utopia Nearest (UN) strategy chooses the Pareto optimal point closest to the Utopia point by computing the Euclidean distance of each point in F to f U , and returns the point that minimizes the distance. , the first term of the loss penalizes a large value of F 1 , and hence minimizing the loss helps reduce F 1 . The second term of the loss aims to push the objective into its constraint region. If F 1 cannot meet the constraints ( Fj &gt; 0 ∨ Fj &lt; 1), it contributes a loss according to its distance from the valid region. The extra penalty, P , ensures that the loss for F 1 if it falls outside its valid region is much higher than that if F 1 lies in the region. Fig. <ref type="figure">3(c</ref>) shows the effect of these two terms on F 1 . In comparison, the loss for F 2 has only the second term, pushing it to meet its constraint, as shown in Fig. <ref type="figure">3(d)</ref>.</p><p>Fig. <ref type="figure">3</ref>(e)-3(f) further illustrate the loss, L(x), given that F 1 (x) and F 2 (x) are both functions of system parameters x. For simplicity, Fig. <ref type="figure">3</ref>(e) shows the loss, L, over univariate input x (#cores), assuming two simple models for F 1 (x) and F 2 (x). In practice, #cores is not a system parameter, but defined over two other parameters x 1 (#executors) and x 2 (#cores per executor). Fig. <ref type="figure">3</ref>(f) shows the loss over x 1 and x 2 . Given complex models on multiple parameters, the surface of L(x) quickly becomes complex. Nevertheless, the loss will guide GD such that by minimizing L, it is likely to find the x value that is an approximate solution to the CO problem.</p><p>Note that GD usually assumes the loss function L to be differentiable, but our loss function is not at specific points. However, we only require L to be subdifferentiable: for a point x that is not differentiable, we can choose a value between its left derivative and right derivative, called a subgradient. Machine learning libraries allow subgradients to be defined by the user program and can automatically handle common cases including our loss functions for DNNs and GPs.</p><p>3. Handling model uncertainty. We have several extensions of our MOGD solver. Most notably, we extend to support model uncertainty: when our objective functions use learned models, these models may not be accurate before sufficient training data is available. Hence, our optimization procedure takes into account model uncertainty when recommending an optimal solution. We leverage recent machine learning methods that support a regression task, F (x), with both expected value E[F (x)] and variance, Var[F (x)]; such methods include Gaussian Processes <ref type="bibr" target="#b26">[27]</ref>, with an example shown in Figure <ref type="figure">3</ref>(b), and Bayesian approximation for DNNs <ref type="bibr" target="#b8">[9]</ref>. Given E[F (x)] and Var[F (x)], we only need to replace each objective function, F j (x), with Fj (x) = E[F j (x)]+α•std[F j (x)], where α is a small positive constant. Here, Fj (x) offers a more conservative estimate of F j (x) for solving a minimization problem, given the model uncertainty. Then we use Fi (x) to build the loss function in Eq. 3 to solve the CO problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Approximate and Parallel PF Algorithms</head><p>Approximate Sequential (PF-AS): The PF Approximate Sequential algorithm (PF-AS) implements SO optimization (Line 2 of Algorithm 1) and constrained optimization (Line 13) using our MOGD solver. It yields an approximate Pareto set since the solution of each CO problem can be suboptimal. In fact, the SOTA MINLP solver, Knitro <ref type="bibr" target="#b13">[14]</ref>, also returns approximate solutions due to complex, non-convex properties of our objective functions, despite long running time.</p><p>Approximate Parallel (PF-AP ): We further propose a parallel algorithm (PF-AP ) that differs from the approximate sequential algorithm in that to probe a given hyperrecetange, we partition it into a l k grid and for each grid cell, construct a CO problem using the the Middle Point Probe (Eq. 2). We send these l k CO problems to our MOGD solver simultaneously. Internally, our solver will solve these problems in parallel (using multi-threading). Some of the cells will not return any Pareto point and hence will be discarded. For each of those cells that returns a Pareto point, the Pareto point breaks the cell into a set of sub-hyperrectangles that will be added to a priority queue for probing later. Inside the queue, the subhyperrectangle with the largest volume is removed, as before. We then further partition it into l k cells and ask the solver to solve their corresponding CO problems simultaneously. This process terminates when the queue becomes empty.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. UDAO IMPLEMENTATION</head><p>UDAO is built on top of Spark with three key modules: Model Server. UDAO's model server can take handcrafted subdifferentiable regression functions (e.g., <ref type="bibr" target="#b35">[36]</ref>) to model task objectives. In addition, it supports two automatic tools to learn models from runtime traces: (i) GP models from OtterTune <ref type="bibr" target="#b34">[35]</ref>: we chose OtterTune <ref type="bibr" target="#b34">[35]</ref> over other tools <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b38">[39]</ref> because it outperforms iTuned <ref type="bibr" target="#b6">[7]</ref>, another GP-based tool, due to the ability to map a new query against all past queries in model building, while CDBTune <ref type="bibr" target="#b38">[39]</ref> cannot return a regression function explicitly for each objective as required by our system. (2) Our custom DNN models <ref type="bibr" target="#b37">[38]</ref> can further extract workload encodings for blackbox programs using advanced autoencoders <ref type="bibr" target="#b37">[38]</ref> to improve prediction.</p><p>For the Spark platform, our model server takes a set of ∼40 parameters and a list of objectives such as latency, throughput, IO load, cost in CPU cores, and cost in CPU-hour (the full list was given in §II-B). The key steps in modeling include:</p><p>1) Training Data Collection: We distinguish online workloads, whose runs are invoked only by the user, from offline workloads, which the model server can sample intensively, e.g., an existing benchmark or some workloads offered by the user for sampling. We sample 100's configurations for each offline workload using (a) heuristic sampling based on Spark best practices and (b) Bayesian optimization <ref type="bibr" target="#b25">[26]</ref> for exploring configurations that are likely to minimize latency. In contrast, we create a small sample of configurations (of size 6 to 30) for each online workload to reflect the constraint that they usually do not have many configurations. Our training set includes runtime traces from both offline and online workloads.</p><p>2) Feature Engineering: We construct features from runtime traces by following standard ML steps: filtering features with a constant value; normalizing numerical features; one-hot encoding for categorical variables; and knob selection, for which we follow OtterTune's practice to select ∼10 most important knobs (parameters) by mixing results from a LASSO-based selection method and Spark recommendations.</p><p>3) Model Training: As we collect training data over months, we train the model periodically and checkpoint the best model weights. Inspired by industry practice <ref type="bibr" target="#b27">[28]</ref>, when receiving a large trace update (e.g., 5000 new traces), we retrain the model via hyper-parameter tuning; with a small trace update (e.g., 1000 new traces), we train incrementally by fine-tuning the model from the latest checkpoint. After training on our full TPCx-BB dataset, the largest DNN model has 4 hidden layers, each with 128 nodes and ReLU as activation function, with backpropogation ran by Adam <ref type="bibr" target="#b24">[25]</ref>. We observe the running time of retraining (incremental training) to be up to 6 hours using 5 servers (20 min. using one server), which are all background processes and do not affect MOO for online jobs.</p><p>Our model sever is implemented using PyTorch. The trained models interface with MOO through network sockets.</p><p>MOO. UDAO's MOO module is implemented in Java and invokes a solver for constrained optimization (CO). Our system supports several solvers including our MO-GD solver ( §IV-B) and the Knitro <ref type="bibr" target="#b13">[14]</ref> solver. To solve a single CO problem, Knitro with 16 threads takes 17 and 42 minutes to run on GP and DNN models, respectively. In contrast, MOGD with 16 threads takes 0.1-0.5 second while achieving the same or lower value of the target objective. Therefore, we use MOGD as the default solver in our sysyem.</p><p>Recommendation. Once a Pareto set is computed for a workload, our optimizer employs a range of strategies to recommend a new configuration from the set. We highlight the most effective strategies below and describe other recommendation strategies in our technical report <ref type="bibr" target="#b28">[29]</ref>.</p><p>First, the Utopia Nearest (UN) strategy chooses the Pareto point closest to the Utopia point f U , by computing the Euclidean distance of each point in the Pareto set F to f U and returning the point that minimizes the distance.</p><p>A variant is the Weighted Utopia Nearest (WUN) strategy, which uses a weight vector, w = (w 1 , ...w k ), to capture the importance among different objectives and is usually set based on the application preference. A further improvement is workload-aware WUN, motivated by our observation that expert knowledge about different objectives is available from the literature. For example, between latency and cost, it is known that it is beneficial to allocate more resources to large queries (e.g. join queries) but less so for small queries (e.g., selection queries). In this case, we use historical data to divide workloads into three categories, (low, medium, high), based on the observed latency under the default configuration. For long running workloads, we give more weight to latency than the cost, hence encouraging more cores to be allocated; for short running workloads, we give more weight to the cost, limiting the cores to be used. We encode such expert knowledge using internal weights, w I = (w I 1 , ..., w I k ), and application preference using external weights, w E = (w E 1 , ..., w E k ). The final weights are w = (w I 1 w E 1 , ..., w I k w E k ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. PERFORMANCE EVALUATION</head><p>In this section, we compare our MOO approach to popular MOO techniques <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b7">[8]</ref> and perform an endto-end comparison to a STOA performance tuning system, OtterTune <ref type="bibr" target="#b34">[35]</ref>. We use DNN models as the default for the MOO experiments as they are among the most time consuming models, and use GP models in the comparison to OtterTune.</p><p>Workloads. We used two benchmarks for evaluation. Batch Workloads: Our batch workloads use the TPCx-BB benchmark <ref type="bibr" target="#b31">[32]</ref> with a scale factor 100G. TPCx-BB includes 30 templates, including 14 SQL queries, 11 SQL with UDF, and 5 ML tasks, which we modified to run on Spark. We parameterized the 30 templates to create 258 workloads, with 58 reserved as offline workloads for intensive sampling and 200 as online workloads. We ran them under different configurations, totaling 24560 traces, each with 360 runtime (f) Uncertain space of all 258 jobs Fig. <ref type="figure">4</ref>. Comparative results on multi-objective optimization using 258 batch workloads metrics. These traces were used to train workload-specific models for latency, cost, etc. Feature selection resulted in 12 most important Spark parameters including the number of executors, number of cores per executor, memory per executor, shuffle compress, parallelism, etc. (see <ref type="bibr" target="#b28">[29]</ref> for the full list).</p><p>Streaming Workloads: We also created a streaming benchmark by extending a prior study <ref type="bibr" target="#b14">[15]</ref> on click stream analysis, including 5 SQL templates with UDFs and 1 ML template. We created 63 workloads from the templates via parameterization, and collected traces for training models for latency and throughput. MOO was run on the most important 10 knobs. Hardware. Our system was deployed on a cluster with 20 compute nodes. The compute nodes are CentOS based with 2xIntel Xeon Gold 6130 processors and 16 cores each, 768GB of memory, and RAID disks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Comparison to MOO Methods</head><p>We first compare our PF algorithms, PF-AS and PF-AP , to five MOO methods: Weighted Sum (WS) <ref type="bibr" target="#b18">[19]</ref>, Normalized Constraints (NC) <ref type="bibr" target="#b20">[21]</ref>, NSGA-II <ref type="bibr" target="#b5">[6]</ref> suggested as the most relevant Evolutionary (Evo) method <ref type="bibr" target="#b7">[8]</ref>, PESM from the Spearmint library <ref type="bibr" target="#b9">[10]</ref>, and qEHVI from BoTorch <ref type="bibr" target="#b4">[5]</ref>. The latter two are Multi-objective Bayesian Optimization (MOBO) methods (see §III). For each algorithm, we request it to generate increasingly more Pareto points <ref type="bibr" target="#b9">(10,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr">50,</ref><ref type="bibr">100,</ref><ref type="bibr">150,</ref><ref type="bibr">200)</ref>, which are called probes, as more computing time is invested, except qEHVI that is shown to have best runtime when calling for one point at a time.</p><p>Expt 1: Batch 2D. We start with the batch workloads where the objectives are latency and cost (in number of cores). As results across different jobs are consistent, we first show details using job 9. To compare PF-AS and PF-AP to WS and NC, Fig. <ref type="figure">4</ref>(a) shows the uncertain space (percentage of the total objective space that the algorithm is uncertain about) as more Pareto points are requested. Initially at 100%, it starts to reduce when the first Pareto set (up to 10 points) is produced. We observe that WS and NC take long to run, e.g., ∼47 seconds to produce the first Pareto set. In comparison, PF-AS and PF-AP reduce uncertain space more quickly, with the first Pareto set produced under 1 second by PF-AP . PF-AS does not work as well as PF-AP because as a sequential algorithm, its Pareto points found in the early stage have a severe impact on the later procedure, andone low-quality approximate result in the early stage may lead to overall low-quality Pareto frontier. In contrast, PF-AP is a parallel algorithm and hence one lowquality approximate result won't have as much impact.</p><p>Fig. <ref type="figure">4</ref>(b) shows the Pareto frontiers of WS and NC created after 47 seconds. WS is shown to have poor coverage of the Pareto frontier, e.g., returning only 3 points although 10 were requested. NC generates 8 points, still less than PF-AP shown in Fig. <ref type="figure">4</ref>(c), which produces 12 points using only 3.2 seconds.</p><p>We next compare PF-AP to PESM, qEHVI, and Evo in Fig. <ref type="figure">4(d)</ref>. qEHVI takes 48 seconds to generate the first Pareto set, while PESM takes 362 seconds to do so. This is consistent with common observations that Bayesian optimization can take long to run, hence not suitable for making online recommendations by a cloud optimizer. Although Evo runs faster than other prior MOO methods, it still fails to generate the first Pareto set until after 2.6 seconds. Fig. <ref type="figure">4</ref>(e) shows another issue of Evo: the Pareto frontiers generated over time are inconsistent. For example, the frontier generated with 30 probes indicates that if one aims at latency of 6 seconds, the cost is around 36 units. The frontier produced with 40 probes shows the cost to be as low as 20, while the frontier with 50 probes changes the cost to 28. Recommending configurations based on such inconsistent information is highly undesirable. Fig. <ref type="figure">4</ref>(f) summarizes the performance of 4 major methods for all 258 workloads, where the x axis is the elapsed time, and the y axis shows the uncertain space across all jobs, with the median depicted by an orange bar. All methods start with 100% uncertain space, and we show when each method starts to reduce until falling below 10% or reaching 100 seconds. PF-AP can generate Pareto sets under 1 second for all jobs, with a median of 8.8% uncertain space, and reduces the median to 5.9% after 2 seconds. Evo remains at 100% within 2 seconds and then achieves a median of 4.2% after 5 seconds. qEHVI can only generate Pareto sets for 54 jobs after 20s, and achieve only 69.4% median after 100s. NC can generate Pareto set for (f) Uncertain space of 63 jobs (3D) Fig. <ref type="figure">5</ref>. Comparative results on multi-objective optimization using 63 streaming workloads 30 jobs after 50s, and achieve 5.8% median after 100s.</p><p>Expt 2: Streaming 2D and 3D. We next use the streaming workload under 2 objectives, average latency (of output records) and throughput (the number of records per second), as well as under 3 objectives, further adding cost as the 3rd objective. As results for different jobs are similar, we illustrate them using job 54, while additional results are available in <ref type="bibr" target="#b28">[29]</ref>. Fig. <ref type="figure">5</ref>(a) and Fig. <ref type="figure">5</ref>(b) show that WS and NC again have poor coverage of the frontier (7 points only each), while Fig. <ref type="figure">5</ref>(c) shows that PF can better construct the frontier using less time. Evo again returns inconsistent Pareto frontiers as more probes are made, with plots shown in <ref type="bibr" target="#b28">[29]</ref>.</p><p>Regarding running time, Fig. <ref type="figure">5</ref>(d) confirms that WS, NC, and PESM take long, e.g., 42, 36, and 308 seconds, respectively, to return the first Pareto set, while PF-AP , Evo, and qEHVI are more efficient, taking 1.1s, 2.7s, 11.5s, respectively, to produce the first Pareto set. Fig. <ref type="figure">5</ref>(e) and 5(f) summarize runtime across 63 workloads for 2D and 3D cases. For 2D jobs, PF-AP is the first to generate Pareto sets, achieving a median of 6.5% under 2 seconds. Evo takes 5 seconds to generate first Pareto sets (which may change at a later time). Both qEHVI and NC need 50 seconds to reduce the median of uncertain space below 10%. The 3D results confirm the same order of the methods in efficiency, with PF-AP taking 2.5 seconds to reduce to 1.3% uncertain space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. End-to-End Comparison</head><p>Next we perform an end-to-end comparison of the configurations recommended by our MOO against Ottertune <ref type="bibr" target="#b34">[35]</ref>. In this study, since we consider the effect of models, we follow standard ML practice to separate workloads into training and test sets: For TPCx-BB, we created a random sample of 30 workloads, one from each template, as test workloads. We use the traces of other workloads as historical data to train predictive models for the test workloads. Similarly, for the stream benchmark we created a sample of 15 test workloads.</p><p>For each test workload, UDAO runs the PF algorithm to compute the Pareto set and then the Weighted Utopia Nearest (WUN) strategy ( §V) to recommend a configuration from the set. As Ottertune supports only single-objective (SO) optimization, we apply a weighted method <ref type="bibr" target="#b38">[39]</ref> that combines k objectives into a single objective, k i=1 w i Ψ i (x), with i w i = 1, and then call Ottertune to solve a SO problem. It is known from the theory of Weighted Sum (WS) <ref type="bibr" target="#b18">[19]</ref> that even if one tries many different values of w, the weighted method cannot find many diverse Pareto points, which is confirmed by the sparsity of the Pareto set in Fig. <ref type="figure">5</ref>(a) where WS tried different w values.</p><p>Expt 3: Accurate models. First, we assume learned models to be accurate and treat model-predicted values of objectives as true values, for any given configuration. For fair comparison, we use the GP models from Ottertune in both systems. Batch 2D. For 2D batch workloads, Fig. <ref type="figure">6</ref>(a) shows the performance of the recommended configurations by two systems when w=(0.5, 0.5), i.e., the application wants balanced results between latency and cost. Since TPCx-BB workloads have 2 orders of magnitude difference in latency, we normalize the latency of each workload (x-axis) by treating the slower one between PF-WUN and Ottertune as 100% and the faster as a value less than 100%. The number of cores (y-axis) allowed in this test is <ref type="bibr" target="#b3">[4,</ref><ref type="bibr">58]</ref>. For all 30 jobs, Ottertune recommends the smallest number of cores (4), favoring cost to latency. PF-WUN is adaptive to the application requirement of balanced objectives, using 2-4 more cores for each job to enable up to 26% reduction of latency. Fig. <ref type="figure">6</ref>(b) shows the results for w=(0.9, 0.1), indicating strong preference for low latency. For 19 out of 30 jobs, Ottertune still recommends 4 cores as it is the solution returned even using the 0.9 weight for latency. In contrast, PF-WUN is more adaptive, achieving lower latency than Ottertune for all 30 jobs with up to 61% reduction. Further, for 8 jobs, PF-WUN dominates Ottertune in both objectives, saving up to 33% of latency while using fewer cores-in this case, Ottertune's solution is not Pareto optimal.</p><p>Streaming 2D. For 15 stream jobs, Fig. <ref type="figure">6</ref>(c)-6(d) show that for w=(0.5, 0.5) the two systems mainly show tradeoffs, while for w=(0.9, 0.1), PF-WUN is more adaptive, achieving lower (a) Batch (0.5,0.5), latency vs cost, accurate models (b) Batch (0.9,0.1), latency vs cost, accurate models (c) Stream (0.5,0.5), latency vs throughput, accurate models (d) Stream (0.9,0.1), latency vs throughput, accurate models (e) Batch measured latency, inaccurate models (f) Batch (0.9,0.1), measured latency, inaccurate models (g) Performance improvement rate, Ottertune against Manual (h) Performance improvement rate, UDAO against Manual Fig. <ref type="figure">6</ref>. Comparative results to Ottertune on single-objective and multi-objective optimization latency for all jobs with up to 63% reduction of latency.</p><p>Expt 4: Inaccurate models. We next consider the case that learned models are not accurate (before enough training data are acquired). For a given objective, our MOGD solver uses the model variance to obtain a more conservative estimate for use in optimization. Further, for TPCx-BB our DNN model <ref type="bibr" target="#b37">[38]</ref> offers better latency estimates (20% error rate in Weighted Mean Absolute Percentage Error, where the percentage error is weighted by the objective value) than Ottertune's GP model (35% error rate). In this experiment we use our DNN model to demonstrate the flexibility of our optimizer, while Ottertune can only use its GP model. We consider two cost measures: cost1 in #cores, which is certain; cost2 as a weighted sum of CPU-hour and IO cost, which are both learned models. Thus, cost2 is subject to 15% error using our DNN model and 34% error using Ottertune's GP model.</p><p>Next we consider 2D optimization over latency and cost1. For w=(0.5, 0.5), we take recommendations from both systems and measure actual latency and cost on our cluster. Fig. <ref type="figure">6(e)</ref> shows the latency of top 12 long-running jobs. Since both systems use low numbers of cores, the cost plot is omitted here. Notably, to run the full TPCx-BB benchmark, UDAO outperforms Ottertune with 26% savings on running time and 3% less cost. For w=(0.9, 0.1), Ottertune's recommendations vary only slightly from (0.5, 0.5), with 6% reduction of total running time, while our recommendations lead to 35% reduction. As Fig. <ref type="figure">6</ref>(f) shows, UDAO outperforms Ottertune with 49% less total running time, and 48% increase of cost, which matches application's strong preference for latency. For the two long-running jobs <ref type="bibr" target="#b1">(2,</ref><ref type="bibr" target="#b29">30)</ref>, Ottertune reports better performance in prediction, but its model is way off and hence in actual running time it achieves much worse results. The prediction and actual latency are more similar in UDAO, enabling better optimization results. Results using latency and cost2 confirm the above observations and are left to <ref type="bibr" target="#b28">[29]</ref>.</p><p>Expt 5: Inaccuracy vs. optimization performance. To quantify the impact of model accuracy on optimization, we collect 120 configurations, recommended by UDAO and Ottertune each, from Expt 4 where optimization was ran using w=(0.5,0.5) or w=(0.9,0.1), cost1 or cost2. We measure actual latency of each configuration and report model accuracy using absolute percentage error (APE) weighted by the latency value. For optimization, we measure performance improvement rate (PIR) by comparing the recommended configuration against a manual configuration chosen by an expert engineer.</p><p>Fig. <ref type="figure">6</ref>(g)-6(h) show PIR over weighted APE. 1) The DNN model is more accurate than GP here, with the average error rate shown by a vertical green line. 2) Empirically, we do not observe DNN to be more susceptible to a long tail than GP, since our DNN model is regularized by the L2 loss and also considers variance when running MOGD. 3) When the model accuracy decreases, PIR can degrade to poor values in some cases (around -50%). Overall, Ottertune has more points (38 out of 120) that lead to PIR below 0%, performing worse than the Spark expert, than UDAO (16/120). This is because UDAO adapts better to user preferences for latency and recommends configurations that are more likely to improve PIR.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. RELATED WORK</head><p>Most relevant work was already discussed in previous sections. Below we survey a few broadly related areas. Multi-objective optimization for SQL <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b33">[34]</ref> enumerates a finite set of query plans built on relational algebra to select Pareto-optimal ones, which stands in contrast to our need for searching through an an infinite parameter space to find Pareto-optimal configurations. MOO approaches for workflow scheduling <ref type="bibr" target="#b12">[13]</ref> differ from our MOO in both the parameter space and the solution. Resource management. TEMPO <ref type="bibr" target="#b30">[31]</ref> addresses resource management for DBMSs in the MOO setting: when Service-Level Objectives (SLOs) cannot be all satisfied, it guarantees max-min fairness over SLOs; otherwise, it uses WS for returning a single solution. Morpheus <ref type="bibr" target="#b11">[12]</ref> addresses the tradeoff between cluster utilization and job performance predictability by codifying user expectations as SLOs and enforces them using scheduling methods. WiseDB <ref type="bibr" target="#b16">[17]</ref> manages cloud resources based on a decision tree trained on performance and cost features collected from minimum-cost schedules of sample workloads, while such schedules are not available in our case.</p><p>Recent performance tuning systems are limited to singleobjective optimization and cannot support complex MOO problems. First, platform-specific handcrafted models were developed by leveraging domain knowledge and workload profiling, and then used to solve a single-objective optimization problem <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b35">[36]</ref>. Among search-based methods, BestConfig <ref type="bibr" target="#b39">[40]</ref> searches for good configurations by dividing high-dimensional configuration space into subspaces based on samples, but it cold-starts each tuning request. ClassTune <ref type="bibr" target="#b40">[41]</ref> solves the optimization problem by classification, which cannot be easily extended to the MOO setting. Among learningbased methods, Ottertune <ref type="bibr" target="#b34">[35]</ref> can learn flexible models from data. It builds a predictive model for each query by leveraging similarities to past queries, and runs Gaussian Process exploration to minimize a single objective. CDBTune <ref type="bibr" target="#b38">[39]</ref> recommends the best configuration for optimizing the reward (fixed weighted sum of objectives) calculated by Deep RL. To the best of our knowledge, our work is the first to tackle MOObased performance tuning for big data systems like Spark. Learning-based query optimization. Recent work <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b29">[30]</ref> uses neural networks to match the structure of optimizerselected query plans and predicts cardinality, cost, or latency. Neo <ref type="bibr" target="#b15">[16]</ref> is a DNN-based query optimizer that bootstraps its optimization model from existing optimizers and then learns from incoming queries. Recent work <ref type="bibr" target="#b27">[28]</ref> integrates learned models into a traditional cost-based query optimizer. None of the above work considers MOO like in our work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VIII. CONCLUSIONS</head><p>We presented UDAO, a multi-objective optimizer that constructs Pareto-optimal job configurations for multiple task objectives, and recommends a new configuration to best meet them. Using batch and streaming workloads, we showed that UDAO outperforms existing MOO methods <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b9">[10]</ref> in both speed and coverage of the Pareto set, and outperforms Ottertune <ref type="bibr" target="#b34">[35]</ref> by a 26%-49% reduction in running time of the TPCx-BB benchmark, while adapting to different application preferences on multiple objectives. In future work, we plan to extend UDAO to support a pipeline of analytic tasks, and incorporate more complex and robust models.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Overview of UDAO, an unified data analytics optimizer built on top of Spark</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>F1</head><label></label><figDesc>multiple Pareto points. Fig.2. Uncertain space in 2D (latency, cost) space for TPCx-BB Q2. f U and f N are the Utopia and Nadir points, respectively. In (a), f M is the solution to the middle point probe. In (b), (f 1 , f 2 , f 3 ) represent the solutions of a series of middle point probes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>x a d , x b d , and x c d , among which only one takes the value '1'. Afterwards all the variables are normalized to the range [0, 1], and boolean variables and (normalized) integer variables are relaxed to continuous variables in [0, 1].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 1 :</head><label>1</label><figDesc>Example of GP regression. (a) prior functions, (b) posterior functions conditioning on training data returns the covariance between the function values at two input points, i.e., k(x, x 0</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>(x) is the dot product of this vector with the training values f ⇤ . So m(x) is simply a weighted average of the function values at the training points. A similar intuition holds when there is more than one test point, m &gt; 1. Fig. 1(b) illustrates the resulting GP after conditioning on training data. As observed, the posterior functions pass through the training points marked by the black dots. The sampled functions also show that the further a point is from the training points, the larger the variance is.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>8 / 20 …</head><label>820</label><figDesc>ize (LOW ER, U P P ER) {Single Objective LOWER and UPPER as constraints and op-bjective} computeBounds(plan1, . . . , plank) uteVolume(Utopia, Nadir) Nadir, volume) p() g.U topia .N adir topia + Nadir)/2 ptimize i (Utopia, Middle) {Constraint Opti--th objective} iddlei = generateSubRectangles(Utopia, Middle, Nadir) 2 rectangles, represented by each own Utopia angle in {rectangle} do rectangle.U topia ectangle.N adir computeVolume(Utopia, Nadir) opia, Nadir, volume) g) ({plan}){remove plan dominated by another e set}</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>[This is obvious.] Constrained optimization. Then consider a constrained optimization (CO) problem, where F1 as the target of optimization and constraints are Fi 2 [F L j , F U j ], i = 1, . . . , k, which include Parts [1] and [2] of Figure X(a). We can use 8 ER, U P P ER) {Single Objective and UPPER as constraints and op-Bounds(plan1, . . . , plank) e(Utopia, Nadir) ume) adir)/2 topia, Middle) {Constraint Opti-ve} SubRectangles(Utopia, Middle, Nadir) s, represented by each own Utopia ectangle} do U topia adir olume(Utopia, Nadir) r, volume) remove plan dominated by another Gradient Descent ortant subroutine, optimize(), optimization problem (line 13 of ur objective functions are given i = 1 . . . k, where each model is me variables among x can be insingle objective, this problem is nlinear programming (MINLP) , 15]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>[This is obvious.] Constrained optimization. Then consider a constrained optimization (CO) problem, where F1 as the target of optimization and constraints are Fi 2 [F L j , F U j ], i = 1, . . . , k, which include Parts [1] and [2] of Figure X(a). We can use 8 ed by hyperrectangle R) {Single Objective as constraints and op-1 , . . . , plank) dir) le) {Constraint Optiles(U topia, M iddle, N adir) d by each own Utopia a, N adir) dominated by another Descent routine, optimize(), n problem (line 13 of functions are given here each model is s among x can be intive, this problem is gramming (MINLP) e is not one general ly for every nonlinample, many of the e they assume cer-F , e.g., twice confactorable into the uenne [7]), which do nted as Deep Neural INLP solver, Knivery slowly, e.g., 42 ptimization problem 7 minutes when the ch a solution is too wards all the variables are normalized to the range [0, 1], and boolean variables and normalized integer variables are relaxed to continuous variables in [0, 1]. As such, the constrained optimization (CO) problem deals with continuous variables in [0,1], which we denote as x = x1, . . . , xD 2 [0, 1].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>[This is obvious.] Constrained optimization. Then consider a constrained optimization (CO) problem, where F1 as the target of optimization and constraints are Fi 2 [F L j , F U j ], i = 1, . . . , k, which include Parts [1] and [2] of Figure X(a). We can use 8 R) {Single Objective as constraints and op-1 , . . . , plank) dir) le) {Constraint Optiles(U topia, M iddle, N adir) d by each own Utopia a, N adir) dominated by another Descent routine, optimize(), n problem (line 13 of functions are given here each model is s among x can be intive, this problem is gramming (MINLP) e is not one general ly for every nonlinample, many of the e they assume cern F , e.g., twice confactorable into the uenne [7]), which do nted as Deep Neural MINLP solver, Knivery slowly, e.g., 42 ptimization problem 7 minutes when the ch a solution is too and boolean variables and normalized integer variables are relaxed to continuous variables in [0, 1]. As such, the constrained optimization (CO) problem deals with continuous variables in [0,1], which we denote as x = x1, . . . , xD 2 [0, 1].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>[This is obvious.] Constrained optimization. Then consider a constrained optimization (CO) problem, where F1 as the target of optimization and constraints are Fi 2 [F L j , F U j ], i = 1, . . . , k, which include Parts [1] and [2] of Figure X(a). We can use 8 ts: M is a priority queue sorted by hyperrectangle ize i (LOW ER, U P P ER) {Single Objective LOWER and UPPER as constraints and op-bjective} computeBounds(plan1, . . . , plank) uteVolume(Utopia, Nadir) Nadir, volume) p() g.U topia .N adir topia + Nadir)/2 ptimize i (Utopia, Middle) {Constraint Opti--th objective} iddlei = generateSubRectangles(Utopia, Middle, Nadir) 2 rectangles, represented by each own Utopia angle in {rectangle} do rectangle.U topia rectangle.N adir computeVolume(Utopia, Nadir) opia, Nadir, volume) g) M r({plan}){remove plan dominated by another e set}</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>[This is obvious.] Constrained optimization. Then consider a constrained optimization (CO) problem, where F1 as the target of optimization and constraints are Fi 2 [F L j , F U j ], i = 1, . . . , k, which include Parts [1] and [2] of Figure X(a). We can use 8 y queue sorted by hyperrectangle ER, U P P ER) {Single Objective nd UPPER as constraints and op-Bounds(plan1, . . . , plank) e(Utopia, Nadir) me) adir)/2 topia, Middle) {Constraint Opti-ve} SubRectangles(Utopia, Middle, Nadir) s, represented by each own Utopia ectangle} do U topia adir olume(Utopia, Nadir) r, volume) remove plan dominated by another Gradient Descent ortant subroutine, optimize(), optimization problem (line 13 of ur objective functions are given = 1 . . . k, where each model is me variables among x can be insingle objective, this problem is nlinear programming (MINLP) , 15]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head></head><label></label><figDesc>2, . . . , D<ref type="bibr" target="#b2">[3]</ref> In the first step, we transform variables for optimization by following the common practice in machine learning: Let x be the original set of parameters, which can be categorical, integer, or continuous variables. If a variable is categorical , we use onehot encoding to create dummy variables. For example, if xd takes values {a, b, c}, we create three boolean variables, x a d , x b d , and x c d , among which only one takes the value '1'. Afterwards all the variables are normalized to the range [0, 1], and boolean variables and (normalized) integer variables are relaxed to continuous variables in [0, 1]. As such, the constrained optimization (CO) problem deals with continuous variables in [0,1], which we denote as x = x1, . . . , xD 2 [0, 1]. After a solution is returned for the CO problem, we set the value for a categorical attribute based on the dummy variable with the highest value, and round the solution for a normalized integer variable to its closest integer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head></head><label></label><figDesc>employs a cusch to approximately solve our coninvolving multiple objective funcin Figure ??. 2, . . . , D [3]xD variables for optimization by folmachine learning: Let x be the h can be categorical, integer, or able is categorical , we use oneariables. For example, if xd takes e boolean variables, x a d , x b d , and the value '1'. Afterwards all the ange [0, 1], and boolean variables es are relaxed to continuous varistrained optimization (CO) probbles in [0,1], which we denote as a solution is returned for the CO categorical attribute based on the t value, and round the solution for its closest integer. blem depicted in Figure X(a). Our adient Descent (MOGD) solver ions to guide gradient descent to jective while satisfying a variety rget objective and constraints can s, e.g., using DNNs, GPs, or other</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>8 ovel</head><label>8</label><figDesc>solver that employs a cush to approximately solve our connvolving multiple objective funcn Figure??.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head></head><label></label><figDesc>2, . . . , D [3]xD variables for optimization by folmachine learning: Let x be the h can be categorical, integer, or ble is categorical , we use oneriables. For example, if xd takes e boolean variables, x a d , x b d , and the value '1'. Afterwards all the nge [0, 1], and boolean variables s are relaxed to continuous varistrained optimization (CO) probles in [0,1], which we denote as a solution is returned for the CO categorical attribute based on the value, and round the solution for ts closest integer. lem depicted in Figure X(a). Our adient Descent (MOGD) solver</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>8 ……Fig. 3 .</head><label>83</label><figDesc>Fig.3. Constrained optimization with k objectives, and an example CF 1 F 2 "min F1 (lat.) such that F1 ∈ [100, 200] and F2 (cost) in<ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b15">16]</ref>" Fig.3(c)-3(d) illustrate the breakdown of the loss terms for F 1 and F 2 . The loss for F 1 has two terms. When F 1 falls in the constraint region[100,200]  (or in the normalized form, 0 ≤ F1 ≤ 1), the first term of the loss penalizes a large value of F 1 , and hence minimizing the loss helps reduce F 1 . The second term of the loss aims to push the objective into its constraint region. If F 1 cannot meet the constraints ( Fj &gt; 0 ∨ Fj &lt; 1), it contributes a loss according to its distance from the valid region. The extra penalty, P , ensures that the loss for F 1 if it falls outside its valid region is much higher than that if F 1 lies in the region. Fig.3(c) shows the effect of these two terms on F 1 . In comparison, the loss for F 2 has only the second term, pushing it to meet its constraint, as shown in Fig.3(d).Fig.3(e)-3(f) further illustrate the loss, L(x), given that F 1 (x) and F 2 (x) are both functions of system parameters x. For simplicity, Fig.3(e) shows the loss, L, over univariate input x (#cores), assuming two simple models for F 1 (x) and F 2 (x). In practice, #cores is not a system parameter, but defined over two other parameters x 1 (#executors) and x 2 (#cores per executor). Fig.3(f) shows the loss over x 1 and x 2 . Given complex models on multiple parameters, the surface of L(x) quickly becomes complex. Nevertheless, the loss will guide GD such that by minimizing L, it is likely to find the x value that is an approximate solution to the CO problem.Note that GD usually assumes the loss function L to be differentiable, but our loss function is not at specific points. However, we only require L to be subdifferentiable: for a point x that is not differentiable, we can choose a value between its left derivative and right derivative, called a subgradient. Machine learning libraries allow subgradients to be defined by the user program and can automatically handle common cases including our loss functions for DNNs and GPs.3. Handling model uncertainty. We have several extensions of our MOGD solver. Most notably, we extend to support model uncertainty: when our objective functions use learned</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head></head><label></label><figDesc>(a) Frontier of WS (job 54, 3d) (b) Frontier of NC (job 54, 3d) (c) Frontier of PF (job 54, 3d)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>P Q ← φ {PQ is a priority queue sorted by hyperrectangle volume} 2: plan i ← optimize i (LOW ER, U P P ER) {Single Objective Optimizer takes LOWER and UPPER as constraints and optimizes on ith objective} 3: U topia, N adir ← computeBounds(plan 1 , . . . , plan k ) 4: volume ← computeVolume(U topia, N adir) 5: seg ← (U topia, N adir, volume)</figDesc><table><row><cell>Algorithm 1 Progressive Frontier-Sequential (PF-S)</cell></row><row><cell>Require: k lower bounds(LOWER): lower j , k upper bounds(UPPER): upper j , number of points: M 1: 6: P Q.put(seg) 7: count ← k 8: repeat 9: seg ← P Q.pop() 10: U topia ← seg.U topia; N adir ← seg.N adir 11: M iddle ← (U topia + N adir)/2 12: M iddle i ← optimize i (U topia, M iddle) {Constraint Optimization on i-th objective} 13: {plan} ← M iddle i 14: count+ = 1 15: {rectangle} = generateSubRectangles(U topia, M iddle, N adir) {return 2</cell></row></table><note><p>k -1 rectangles, represented by each own Utopia and Nadir} 16: for each rectangle in {rectangle} do 17: U topia ← rectangle.U topia; N adir ← rectangle.N adir 18: volume ← computeVolume(U topia, N adir) 19: seg ← (U topia, N adir, volume) 20: P Q.put(seg) 21: end for 22: until count &gt; M 23: output ← f ilter({plan}){filter dominated points}</p></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>ACKNOWLEDGMENTS</head><p>This work was partially supported by the <rs type="funder">European Research Council (ERC)</rs> <rs type="programName">Horizon 2020 research and innovation programme</rs> (grant <rs type="grantNumber">n725561</rs>), <rs type="funder">ARL</rs> grant <rs type="grantNumber">W911NF-17-2-0196</rs>, <rs type="funder">NSF</rs> grant <rs type="grantNumber">1908536</rs>, and <rs type="funder">China Scholarship Council (CSC)</rs>. We would like to thank <rs type="person">Wei Sheng</rs> for the discussion and help in the earlier phase of the project.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_e3xqWgZ">
					<idno type="grant-number">n725561</idno>
					<orgName type="program" subtype="full">Horizon 2020 research and innovation programme</orgName>
				</org>
				<org type="funding" xml:id="_fq7Gbk8">
					<idno type="grant-number">W911NF-17-2-0196</idno>
				</org>
				<org type="funding" xml:id="_Zj25QSa">
					<idno type="grant-number">1908536</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<ptr target="https://aws.amazon.com/ec2/instance-types/" />
		<title level="m">Amazon EC2 instances</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<ptr target="https://aws.amazon.com/rds/aurora/serverless/" />
		<title level="m">Aurora Serverless</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<ptr target="https://www.coin-or.org/Bonmin/" />
		<title level="m">Bomin: Basic open-source nonlinear mixed integer programming</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<ptr target="https://projects.coin-or.org/Couenne/" />
		<title level="m">Couenne: Convex over and under envelopes for nonlinear estimation</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Differentiable expected hypervolume improvement for parallel multi-objective Bayesian optimization</title>
		<author>
			<persName><forename type="first">S</forename><surname>Daulton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.05078</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A fast and elitist multiobjective genetic algorithm: Nsga-ii</title>
		<author>
			<persName><forename type="first">K</forename><surname>Deb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pratap</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trans. Evol. Comp</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="182" to="197" />
			<date type="published" when="2002-04">Apr. 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Tuning database configuration parameters with ituned</title>
		<author>
			<persName><forename type="first">S</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Thummala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Babu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PVLDB</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1246" to="1257" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A tutorial on multiobjective optimization: Fundamentals and evolutionary methods</title>
		<author>
			<persName><forename type="first">M</forename><surname>Emmerich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Natural Computing</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Dropout as a bayesian approximation: Representing model uncertainty in deep learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Predictive entropy search for multi-objective Bayesian optimization</title>
		<author>
			<persName><forename type="first">D</forename><surname>Hernández-Lobato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1492" to="1501" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">L</forename><surname>Iberti</surname></persName>
		</author>
		<ptr target="https://www.lix.polytechnique.fr/∼liberti/rairo18.pdf" />
		<title level="m">Undecidability and hardness in mixed-integer nonlinear programming</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Morpheus: Towards automated SLOs for enterprise clusters</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Jyothi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Curino</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">OSDI</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="117" to="134" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Schedule optimization for data processing flows on the cloud</title>
		<author>
			<persName><forename type="first">H</forename><surname>Kllapi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Sitaridi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGMOD</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="289" to="300" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<ptr target="https://www.artelys.com/docs/knitro/index.html" />
		<title level="m">Knitro user&apos;s manual</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Supporting scalable analytics with latency constraints</title>
		<author>
			<persName><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Diao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Shenoy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PVLDB</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1166" to="1177" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Neo: A learned query optimizer</title>
		<author>
			<persName><forename type="first">R</forename><surname>Marcus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Negi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. VLDB Endow</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1705" to="1718" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Wisedb: A learning-based workload management advisor for cloud databases</title>
		<author>
			<persName><forename type="first">R</forename><surname>Marcus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Papaemmanouil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PVLDB</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="780" to="791" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Plan-structured deep neural models for query performance prediction</title>
		<author>
			<persName><forename type="first">R</forename><surname>Marcus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Papaemmanouil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PVLDB</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1733" to="1746" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Survey of multi-objective optimization methods for engineering</title>
		<author>
			<persName><forename type="first">R</forename><surname>Marler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Structural &amp; Multidisciplinary Optimization</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">From dubious construction of objective functions to the application of physical programming</title>
		<author>
			<persName><forename type="first">A</forename><surname>Messac</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AIAA Journal</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">The normalized normal constraint method for generating the pareto frontier</title>
		<author>
			<persName><forename type="first">A</forename><surname>Messac</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Strl. &amp; Multidiscpl. Opt</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<ptr target="https://neos-guide.org/content/nonlinear-programming" />
		<title level="m">Neos guide: Nonlinear programming software</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<ptr target="https://www.oracle.com/big-data/data-flow/" />
		<title level="m">Oracle Cloud Data Flow</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Perforator: eloquent performance models for resource optimization</title>
		<author>
			<persName><forename type="first">K</forename><surname>Rajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kakadia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SoCC</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="415" to="427" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">An overview of gradient descent optimization algorithms</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ruder</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.04747</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Practical Bayesian Optimization of Machine Learning Algorithms</title>
		<author>
			<persName><forename type="first">J</forename><surname>Snoek</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<publisher>NIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A tutorial on gaussian process regression: Modeling, exploring, and exploiting functions</title>
		<author>
			<persName><forename type="first">E</forename><surname>Schulz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. of Math. Psych</title>
		<imprint>
			<biblScope unit="volume">85</biblScope>
			<biblScope unit="page" from="1" to="16" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Cost models for big data query processing: Learning, retrofitting, and our findings</title>
		<author>
			<persName><forename type="first">T</forename><surname>Siddiqui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jindal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGMOD</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="99" to="113" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<author>
			<persName><forename type="first">F</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zaouk</surname></persName>
		</author>
		<ptr target="http://scalla.cs.umass.edu/papers/udao2020.pdf" />
		<title level="m">Boosting cloud data analytics using multiobjective optimization</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">An end-to-end learning-based cost estimator</title>
		<author>
			<persName><forename type="first">J</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Sun</forename><forename type="middle">G</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. VLDB Endow</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="307" to="319" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Tempo: robust and self-tuning resource management in multi-tenant parallel databases</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Babu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PVLDB</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="720" to="731" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<ptr target="http://www.tpc.org/tpcx-bb/" />
		<title level="m">TPCx-BB benchmark for big data analytics</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Approximation schemes for many-objective query optimization</title>
		<author>
			<persName><forename type="first">I</forename><surname>Trummer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Koch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGMOD</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1299" to="1310" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">An incremental anytime algorithm for multiobjective query optimization</title>
		<author>
			<persName><forename type="first">I</forename><surname>Trummer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Koch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGMOD</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1941" to="1953" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Automatic database management system tuning through large-scale machine learning</title>
		<author>
			<persName><forename type="first">D</forename><surname>Van Aken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pavlo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGMOD</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Efficient performance prediction for large-scale advanced analytics</title>
		<author>
			<persName><forename type="first">S</forename><surname>Venkataraman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NSDI</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Resilient distributed datasets: a faulttolerant abstraction for in-memory cluster computing</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zaharia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chowdhury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NSDI</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">UDAO: A next-generation unified data analytics optimizer (vldb 2019 demo)</title>
		<author>
			<persName><forename type="first">K</forename><surname>Zaouk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PVLDB</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1934" to="1937" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">An end-to-end automatic cloud database tuning system using deep reinforcement learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGMOD</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="415" to="432" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">BestConfig: tapping the performance potential of systems via automatic configuration tuning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SoCC</title>
		<imprint>
			<biblScope unit="page" from="338" to="350" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">ClassyTune: A Performance Auto-Tuner for Systems in the Cloud</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans, on Cloud Computing</title>
		<imprint>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
