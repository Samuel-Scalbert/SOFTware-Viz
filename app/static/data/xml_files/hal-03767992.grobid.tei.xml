<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Statistical Claim Checking: StatCheck in Action (demonstration)</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Oana</forename><surname>Balalau</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Simon</forename><surname>Ebel</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Théo</forename><surname>Galizzi</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Ioana</forename><surname>Manolescu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Quentin</forename><surname>Massonnat</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Antoine</forename><surname>Deiana</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Emilie</forename><surname>Gautreau</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Antoine</forename><surname>Krempf</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Thomas</forename><surname>Pontillon</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Gérald</forename><surname>Roux</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Thomas</forename><surname>Pointillon</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Joanna</forename><surname>Yakin</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Inria</orgName>
								<orgName type="institution" key="instit2">Institut Polytechnique de Paris</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Info</orgName>
								<address>
									<country>France, Radio France France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Statistical Claim Checking: StatCheck in Action (demonstration)</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">5B45377416999D0E83FC9675D49A3C73</idno>
					<note type="submission">Submitted on 2 Sep 2022</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-04-12T14:54+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract xml:lang="fr">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>ou non, émanant des établissements d'enseignement et de recherche français ou étrangers, des laboratoires publics ou privés.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Professional journalism work has always involved verifying information with the help of trusted sources. In recent years, the proliferation of media in which public figures make statements, in particular online, has lead to an explosion in the amount of content that may need to be verified to distinguish accurate from inaccurate, and even potentially dangerous, information.</p><p>To help journalists deal with the deluge of information, computational fact-checking [CLL + 18, NCH + 21] emerges as a growing, multidisciplinary field. The main tasks of a fact-checking system are: identifying the claims made in an input document, finding the relevant evidence from a reference corpus, and (optionally) producing an automated verdict (is the claim true or false?). A reference corpus can be a knowledge graph [CSR + 15], Web sources such as Wikipedia [NCB19, YMW + 18], or relational tables [CWC + 20, HNM + 20, JTY + 19, KSPT20].</p><p>For fact-checks to be convincing, professional journalists prefer reference sources of high quality, carefully built by specialists. These include statistics produced by governmental and international organizations, such as INSEE (the French national statistics institute) and Eurostat (the equivalent EU office). Technically speaking, such statistics are multidimensional tables, where a fact is a number, characterized by one or more a dimensions, such as a geographical unit, time interval, and other categories such as "Education level". Such data sources are significantly more complex than relational tables, making their usage challenging. Consequently, despite the interest in such sources, few works have used them for automatic fact-checking <ref type="bibr" target="#b7">[CMT18,</ref><ref type="bibr" target="#b11">DCMT19]</ref>.</p><p>We propose to demonstrate StatCheck, a fact-checking system specialized in the French media. We developed StatCheck in a collaboration between computer science researchers and journalists at Radio France, the French national radio, with a daily audience recently estimated at 15.8M users<ref type="foot" target="#foot_1">2</ref> . StatCheck builds upon the open-source code base of <ref type="bibr" target="#b7">[CMT18,</ref><ref type="bibr" target="#b11">DCMT19]</ref>, which it generalizes into a first generic fact-checking pipeline based on multidimensional statistics. Different from [CWC + 20, HNM + 20, JTY + 19, KSPT20, CSR + 15, NCB19, YMW + 18], StatCheck also includes a claim detection step, which saves journalists' time by focusing their attention on the claims worth checking; our claim detection module significantly outperforms the only one we know of for French <ref type="bibr" target="#b11">[DCMT19]</ref>. The fact-checking journalist authors prefer to interpret the relationship between the known statistics and the statistic claim StatCheck identifies and include these interpretations in various news segments they author.</p><p>Outline Below, we present the actual organization of statistic databases, and the StatCheck architecture, in Section 2. Then, we explain how this architecture is instantiated over two different sources, INSEE and Eurostat, whose size and organization significantly varies, in Section 3; we ingest and index all the data to support efficient search over it, as described in Section 4. We find claim by subscribing to media sources, or allowing users to upload their own content (Section 5). We then describe the proposed demonstration scenarios (Section 6).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">FACT-CHECKING BASED ON MULTIDIMENSIONAL STATISTICS</head><p>A multidimensional dataset consists of a set of facts, each having one value along a set of dimensions. For instance, Figure <ref type="figure" target="#fig_0">1</ref> (top) represents three-dimensional datasets: French departments are on the horizontal axis, education levels on the vertical axis, while years are on the third (depth) axis. In each cell, the dataset could store the number of students in the respective department, level of study, and year. However, the actual Open Data statistics published by the government or international organizations are typically much more complex, as shown at the bottom of Figure <ref type="figure" target="#fig_0">1</ref>. First, to save space, dimension values may be encoded into short codes, e.g., "HI" for "High school", "MI" for "Middle school"; a decoding dictionary, associating a human-understandable term to each code, is published  this requires effort to interpret them correctly. Note also that there can be a hierarchy of headers, e.g., a dataset at the granularity of departments may also include region names, e.g., "Île-de-France" and "Grand Est", placed in the data files above, or close to, the region header cells. Third, datasets may contain partially aggregated results next to the cell-level data, illustrated by the orange box holding the sum of all facts for one region (Grand Est), one education level (elementary), and the three years. Fourth, for each dataset, there may exist a separate, textual description, which contains a title, e.g., "French student population", and other comments. Data representation in files. In practice, a multidimensional statistic dataset is published as a file, which can be CSV, a spreadsheet, etc. The dataset is laid out in a bidimensional format, with facts on each and as many lines as needed. If the data has more than two dimensions, which is often the case, this leads to row header cells encoding several dimensions and their values, such as "HI 2019", "MI 2019" etc., in the figure. The file may start with the column headers (yellow), then the encoded multidimensional row header cell "EL 2019" followed by the four cells corresponding to it, then a similar line for "MI 2019", a line for "HI 2019", followed by similar lines for 2020, then 2021, etc. Partially aggregated results are interspersed between such lines. Challenges and architecture. To exploit such datasets for factchecking, a set of challenges must be addressed. The useful information, e.g., "How many elementary school students were in the Île-de-France region in 2019?", is a number in a cell. To find such information, we must identify and store its relationships with human-understandable descriptions of its dimensions, such as "Education level: Elementary school". In this example, the question is asked at a granularity (region) more coarse than the granularity of the data. To find the answer, we must exploit the fact that Paris and Essonne are departments in the Île-de-France region. Further, statistic claims may use similar but different language, e.g., a claim may be made about "pupils in Île-de-France". Linguistic knowledge must be leveraged to connect the claim terminology with that of the dataset. Fine-granularity answers are preferred that is: if the answer consists of one or a few cells only, those should be returned to avoid users' efforts to search through facts in a file (a dataset can have millions of lines). Finally, speed at scale is essential to enable journalists to work efficiently.</p><p>To address these challenges, we have devised an architecture shown in Figure <ref type="figure" target="#fig_2">2</ref>. The modules in the lower row acquire reference datasets and analyze and index them. Those in the upper row acquire content to be fact-checked, extract claims, and identify,the most pertinent stored facts to use to check the claims.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">STATISTIC FACT DATABASE</head><p>We crawled the INSEE [www22b] and Eurostat [www22a] Web sites, extracting and storing their complete statistics as follows.</p><p>INSEE publishes each statistic report as an HTML page containing a description (title and comments) and statistic tables, in Excel or HTML. As of May 2022, there are 60,002 Excel files (each of which may contain several tables) and 58,849 HTML tables. The table organization varies significantly across the datasets; nested headers are frequent. The largest table has 50.885 lines. Following <ref type="bibr" target="#b6">[CMT17]</ref>, to capture all the elements of an INSEE dataset, we turn it into an RDF graph, where each data cell, header cell, and partial aggregate becomes an RDF node (URI). Further, each data cell or partial aggregate node is connected, through an RDF triple, to the cells corresponding to its closest header cells. Thus, the number of elementary school students in Paris in 2019 is connected to header cells labeled "Paris", respectively, "Elementary school 2019" -observe that we decoded "EL 2019" using the dictionary. Further, each header cell is connected through an RDF triple to its parent header cell. This allows us to quickly find out that the elementary school students in Paris in 2019 are also counted as being in the Île-de-France region. Finally, we create an RDF node per dataset, connected to all its header cells and the textual title and comments (each modeled as an RDF literal). The INSEE corpus lead to 7,362,538,629 RDF triples, including 22,366,376 header cells. We store them in Apache's Fuseki (with TDB2) RDF store.</p><p>Eurostat publishes 6,803 statistic tables, ranging from 2 lines to 37 million lines, and 580 dictionaries that, together, decode 243,083 statistical concepts codes into natural-language descriptions. Together, the data files total 414.908.786 lines. In Eurostat, dimension hierarchies are described in the dictionaries; we store them in memory. The statistic tables are simple TSV files; we keep them in this format, complemented by specialized indexes, as we explain below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">STATISTIC SEARCH</head><p>Given a keyword query 𝑄 = {𝑘 1 , 𝑘 2 , . . . , 𝑘 𝑛 }, such as "middle school pupils in Île-de-France in 2020", we find: the most relevant facts from our INSEE and Eurostat corpus; or, if a specific fact is not found, but some datasets appear related to the query, return those datasets. In general, there may be both fact-level and dataset-level answers; we return a ranked list based on their relevance.</p><p>We call metadata of a statistic dataset all the natural-language elements part of or associated with the dataset: its title, comments, and human-understandable versions of all its header values. We use L = {𝑇 , 𝐻, 𝐶} to denote the set of the locations in which a term can appear in metadata, respectively: the dataset title, a header, or a comment. The locations are important: (𝑖) since a term appearing in a title is more important than one appearing in a header (Section 4.1); (𝑖𝑖) to determine if a dataset matches some keywords headers of different dimensions -in which case the cell at the intersection of those dimensions likely has a very pertinent result (Section 4.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Dataset Indexing and Search</head><p>We split the metadata of each dataset 𝑑 into a set of tokens 𝑇 = {𝑡 , . . . , 𝑡 𝐷 }. For each token 𝑡, we identify, based on a Word2Vec model, the 50 tokens 𝑡 ′ closest semantically to 𝑡.</p><p>Next, for each appearance of a token 𝑡 in location 𝑙 within an INSEE dataset 𝑑, our term-location index 𝐼 𝑇 𝐿 stores: an entry of the form (𝑡, 𝑑, 𝑙), and 50 entries of the form (𝑡 ′ , 𝑑, 𝑙, 𝑑𝑖𝑠𝑡), for the 50 tokens closest to 𝑡. For instance, when 𝑡 is "school", 𝑡 ′ could be "teacher", "pupil", "student", etc. For fast access, 𝐼 𝑇 𝐿 is stored in the Redis in-memory key-value store.</p><p>The large size of Eurostat statistics prevents cell-or row-level metadata indexing, as the index might outgrow the memory. So instead, we index occurrences of statistical concept codes in datasets, as follows. Let 𝑐 be a Eurostat concept, e.g., "EL", appearing in dataset 𝑑 at a location 𝑙 ∈ L, and 𝑑 𝑐 be the decoding of 𝑐, e.g., "Elementary school" for "EL". Let 𝑇 𝑑𝑐 = {𝑡 1 , 𝑡 2 , . . . , 𝑡 𝑁 } be the tokens in 𝑑 𝑐 , and for 1≤𝑖≤𝑁 , let 𝑡 𝑗 𝑖 , for 1 ≤ 𝑗 ≤ 50, be the tokens closest to 𝑡 𝑖 . For each 𝑡 𝑖 ∈ 𝑇 𝑑𝑐 , we insert in the term-dataset index 𝐼 𝑇 , also stored in Redis: a (𝑡 𝑖 , 𝑑, 𝑙) entry; and, for every 𝑡 𝑗 𝑖 similar to 𝑡 𝑖 , an entry (𝑡 𝑗 𝑖 , 𝑑, 𝑙, 𝑑𝑖𝑠𝑡, 𝑡 𝑖 ), where 𝑑𝑖𝑠𝑡 is the distance between 𝑡 𝑖 and 𝑡 𝑗 𝑖 . Given the query 𝑄 = {𝑘 1 , . . . , 𝑘 𝑛 }, we search 𝐼 𝐶𝐿 and 𝐼 𝑇 for entries of the form (𝑘 𝑖 , 𝑑, 𝑙) or (𝑘 𝑖 , 𝑑, 𝑙, 𝑑𝑖𝑠𝑡, 𝑘 ′ 𝑖 ) for each 𝑘 𝑖 . We rank datasets based on the relevance score introduced in <ref type="bibr" target="#b7">[CMT18]</ref>. It leverages the word distances between the query keywords and the datasets' metadata and also reflects the locations where the keywords were found for each dataset. Among the retrieved datasets, we keep the 20 having the highest score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Data Cell Indexing and Search</head><p>Our next task is to extract results at the finest granularity level possible. Let 𝑑 be one of the most interesting datasets, and 𝐼 (𝑑) be the set of all index entries for the query 𝑄 and 𝑑. For our sample query 𝑄 and dataset in Figure <ref type="figure" target="#fig_0">1</ref>, 𝐼 (𝑑) contains:</p><p>• For "middle school", header (𝐻 ) entries for "Middle school" (exact), as well as for "High school" and "Elementary school" (similar); a title (𝑇 ) entry for "student" (similar); and a comment (𝐶) entry for "school" (similar); • For "pupils", 𝐻 , 𝑇 , and 𝐶 entries for the similar words above;</p><p>• For "Île-de-France", an exact 𝐻 entry, and two similar 𝐻 entries for "Paris" and "Essonne"; • For "2020", exact 𝐻 entries. If 𝐼 (𝑑) only features title (𝑇 ) or comment (𝐶) locations, then 𝑑 is pertinent as a whole, and no cell search is needed.</p><p>On the contrary, if 𝐼 (𝑑) has several header entries (having 𝑙 = 𝐻 ), matching two or more distinct query keywords (or close terms), this means that 𝑑 holds some fine-granularity results for the query. If 𝐼 (𝑑) holds an entry along each dataset dimension 𝑑, they designate exactly one cell, and we can directly return its value. Otherwise, the result is a collection of all the cells from 𝑑 characterized by the dimension values designated by the entries in 𝐼 (𝑑). In our example, we should return the cells for "MI 2019", "2020", and locations "Paris" and "Essonne", which belong to Île-de-France.</p><p>• If 𝑑 is an INSEE dataset, 𝐼 (𝑑) specifies exactly which rows and columns are concerned. Then, the cell is identified by asking a SPARQL query <ref type="bibr" target="#b7">[CMT18]</ref>, evaluated by Fuseki. • On the contrary, if 𝑑 is an Eurostat dataset, 𝐼 (𝑑) only specifies that "some row (column) headers match". Identifying the relevant cells requires more effort, as we explain below. An Eurostat file has at most a few dozen columns. To find the column referred to by an 𝐼 (𝑑) entry whose key is 𝑘, we search for 𝑘 in the first (header) line of 𝑑. To efficiently find the row even in a huge file, we created another index on all the Eurostat data files, inspired by the Adaptive Positional Map of [ABB + 15], storing the positions of the data rows in 𝑑 containing 𝑘 in their header. Knowing the rows and column indexes, we read the relevant row(s) from 𝑑, and extract from them the relevant data cell(s). On both INSEE and Eurostat, this takes from a few microseconds to 2.5s.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CLAIM DETECTION</head><p>A claim is a statement to be validated. The validation is achieved by finding related statements, called evidence, which back up or disprove the claim. In our work, the claims are detected in an input text, while the evidence is retrieved from a set of trusted sources, our reference datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Statistical Claim Detection</head><p>In <ref type="bibr" target="#b11">[DCMT19]</ref>, the authors introduced a statistical claim detection method that given an input set of statistical entities (e.g. chômage, coefficient budgétaire) and a sentence, it retrieves all the statistical statements of the form ⟨statistical entity, numerical value and unit, date⟩ present in the sentence. The statistical statement, if present, represents the statistical claim to be verified. The statistical entities and units are retrieved using exact string matching, while the date is extracted using HeidelTime [SG10], a time expression parser. If the parser finds no date, the posting timestamp is used. The initial statistical entity list is constructed from the reference datasets by taking groups of tokens from the headers of tables, we refer to <ref type="bibr" target="#b11">[DCMT19]</ref> for more details.</p><p>We improved the method presented in <ref type="bibr" target="#b11">[DCMT19]</ref> to optimize both the speed and quality of extractions. We refer to the two methods as OriginalStatClaim <ref type="bibr" target="#b11">[DCMT19]</ref> and StatClaim. We first performed a more careful match between the tokens of a sentence and our input statistical entities. Then, using the syntactic tree of the sentence and a lemmatizer, statistical entities are matched using their lemma, and are extended to contain the entire nominal group of the matched token. Numerical values are associated with units using both lemmas matching from our set of units and syntactic analysis. As in the original approach, if we retrieve a statistical statement of the form ⟨statistical entity, numerical value, and unit, date⟩, we have found a claim to verify. In the default setting of our algorithm, a claim should contain all three elements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Check-worthy Claim Detection</head><p>To complement the statistical claim detection model, we developed a model that is not conditioned on a set of initial statistical entities. Instead, the model classifies a sentence as check-worthy or not, where check-worthiness is defined as sentences containing factual claims that the general public will be interested in learning about their veracity <ref type="bibr" target="#b1">[AHLT20]</ref>. We leveraged the ClaimBuster dataset <ref type="bibr" target="#b1">[AHLT20]</ref>, containing check-worthy claims in English from the U.S. Presidential debates, to train a cross-lingual language model, XLM-R [CKG + 19], which can perform zero-shot classification on French sentences after having been trained on English data. The ClaimBuster dataset. ClaimBuster is a crowd-sourced dataset of 11𝐾 sentences from the 15 U.S. presidential elections debates from 1960 to 2016 that have been annotated. Each sentence is labeled as check-worthy or not; we use them to fine-tune the XLM-R model. such as the XNLI benchmark [CRL + 18], while remaining competitive on monolingual tasks. We used a weighted cross-entropy loss to account for the unbalanced ratio of labels. The dataset was split into train, dev, and test datasets with a ratio of 80%/%10%/10%. We fine-tune the model using a learning rate of 5 • 10 -5 , a batch size of 64, and the AdamW optimizer. Evaluation. To evaluate the performance of the different models on French data, we randomly sampled 200 French tweets and labeled them as check-worthy or not following the definition in <ref type="bibr" target="#b1">[AHLT20]</ref>. The Cohen Kappa score for inter-annotator agreement is 0.6, signifying moderate to substantial agreement. The results can be found in Table <ref type="table" target="#tab_0">1</ref>. The drop in precision on French data could be because we are evaluating on a small test dataset or because the tweets' format and vocabulary might differ from the ones in the training dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">DEMONSTRATION SCENARIOS</head><p>Our system is developed in Python and deployed on a Unix server. Its GUI is accessible via a Web server; Figure <ref type="figure" target="#fig_3">3</ref> illustrates it. Demonstration attendees will be able to: (𝑖) Ask queries in the statistic search interface, and inspect the results, at the level of cell, line, or column, together with their metadata from the original statistic site (INSEE or Eurostat); (𝑖𝑖) Visualize the analysis of incoming social media messages (as they arrive in real-time), in order to see the statistical mentions and claims deemed potentially checkworthy, identified in these messages; StatCheck also proposes candidate queries for the statistic search interface, as shown in Figure <ref type="figure" target="#fig_3">3</ref>. (𝑖𝑖𝑖) Select various options (restrict to numerical claims or not, include statements about the future or not, include first-person texts or not, etc.) and see their impact on the claim extraction output. (𝑖𝑣) Write their own text and/or suggest other content to be processed by our analysis pipeline (Section 5).</p><p>For the non-French speaking audience, we will use Google Translate for social media messages, and prepare examples where French and English statistic terms are sufficiently close, e.g., industrie, etc.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Multidimensional statistic data: conceptual view (top), structure of actual published dataset (bottom).</figDesc><graphic coords="3,70.70,179.85,204.22,123.09" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>with, or close to the data cells. Although not shown in the figure, dimension names are similarly encoded. Second, header cells, shown in yellow and green in the figure, may be mixed with data cells;</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: StatCheck architecture overview.</figDesc><graphic coords="4,128.34,83.68,353.08,120.62" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Screen captures of StatCheck' GUI. Top: statistic search interface with sample query results: data cells with row header in blue and column header in red; bottom: tweet analysis interface.unit, date⟩ present in the sentence. The statistical statement, if present, represents the statistical claim to be verified. The statistical entities and units are retrieved using exact string matching, while the date is extracted using HeidelTime [SG10], a time expression parser. If the parser finds no date, the posting timestamp is used. The initial statistical entity list is constructed from the reference datasets by taking groups of tokens from the headers of tables, we refer to<ref type="bibr" target="#b11">[DCMT19]</ref> for more details.We improved the method presented in<ref type="bibr" target="#b11">[DCMT19]</ref> to optimize both the speed and quality of extractions. We refer to the two methods as OriginalStatClaim<ref type="bibr" target="#b11">[DCMT19]</ref> and StatClaim. We first performed a more careful match between the tokens of a sentence and our input statistical entities. Then, using the syntactic tree of the sentence and a lemmatizer, statistical entities are matched using their lemma, and are extended to contain the entire nominal group of the matched token. Numerical values are associated with units using both lemmas matching from our set of units and syntactic analysis. As in the original approach, if we retrieve a statistical statement of the form ⟨statistical entity, numerical value, and unit, date⟩, we have found a claim to verify. In the default setting of our algorithm, a claim should contain all three elements.</figDesc><graphic coords="5,77.90,181.42,453.95,63.42" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Classification. The XLM-R model is a Transformer-based language model which achieves state-of-the-art results on multilingual tasks Evaluation of the fine-tuned XLM-R model.</figDesc><table><row><cell>Dataset</cell><cell cols="2">Precision Recall F1 score</cell></row><row><cell>ClaimBuster</cell><cell>0.883 0.848</cell><cell>0.865</cell></row><row><cell>French tweets</cell><cell>0.612 0.769</cell><cell>0.682</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>https://drive.google.com/drive/folders/16b202dFIVR3uSqmsnDQSQ7FkOsgJdZ6e? usp=sharing</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>https://www.radiofrance.com/professionnels/regie-publicitaire/actualite/lesaudiences-janvier-mars-2022</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Nodb: efficient query execution on raw data files</title>
		<author>
			<persName><forename type="first">Ioannis</forename><surname>Alagiannis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Renata</forename><surname>Borovica-Gajic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miguel</forename><surname>Branco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stratos</forename><surname>Idreos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anastasia</forename><surname>Ailamaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="112" to="121" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A Benchmark Dataset of Check-worthy Factual Claims</title>
		<author>
			<persName><forename type="first">Naeemul</forename><surname>Fatma Arslan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengkai</forename><surname>Hassan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><surname>Tremayne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">14th International AAAI Conference on Web and Social Media</title>
		<imprint>
			<publisher>AAAI</publisher>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName><surname>Ckg +</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Unsupervised cross-lingual representation learning at scale</title>
		<author>
			<persName><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kartikay</forename><surname>Khandelwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vishrav</forename><surname>Chaudhary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Wenzek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francisco</forename><surname>Guzmán</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.02116</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title/>
		<author>
			<orgName type="collaboration">CLL</orgName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A content management perspective on fact-checking</title>
		<author>
			<persName><forename type="first">Sylvie</forename><surname>Cazalens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philippe</forename><surname>Lamarre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Leblay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ioana</forename><surname>Manolescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Tannier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW (Companion</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="565" to="574" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Extracting linked data from statistic spreadsheets</title>
		<author>
			<persName><forename type="first">Tien</forename><surname>Duc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cao</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Ioana</forename><surname>Manolescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Tannier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Workshop on Semantic Big Data, International Workshop on Semantic Big Data</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Searching for Truth in a Database of Statistics</title>
		<author>
			<persName><forename type="first">Tien-Duc</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ioana</forename><surname>Manolescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Tannier</surname></persName>
		</author>
		<ptr target="https://gitlab.inria.fr/cedar/excel-search" />
	</analytic>
	<monogr>
		<title level="m">WebDB</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Xnli: Evaluating cross-lingual sentence representations</title>
		<author>
			<persName><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruty</forename><surname>Rinott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adina</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><forename type="middle">R</forename><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Computational fact checking from knowledge networks</title>
		<author>
			<persName><forename type="first">Gianluca</forename><surname>Ciampaglia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prashant</forename><surname>Shiralkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luis</forename><forename type="middle">M</forename><surname>Rocha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johan</forename><surname>Bollen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Filippo</forename><surname>Menczer Menczer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alessandro</forename><surname>Flammini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS ONE</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Tabfact : A large-scale dataset for table-based fact verification</title>
		<author>
			<persName><forename type="first">Wenhu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongmin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianshu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunkai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shiyang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiyou</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wang</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Extracting statistical mentions from textual claims to provide trusted content</title>
		<author>
			<persName><forename type="first">Tien</forename><surname>Duc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cao</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Ioana</forename><surname>Manolescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Tannier</surname></persName>
		</author>
		<ptr target="https://gitlab.inria.fr/cedar/statstical_mentions" />
	</analytic>
	<monogr>
		<title level="m">NLDB</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">TaPas: Weakly supervised table parsing via pre-training</title>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Herzig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Krzysztof</forename><surname>Pawel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Nowak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francesco</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Piccinno</surname></persName>
		</author>
		<author>
			<persName><surname>Eisenschlos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="4320" to="4333" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title/>
		<author>
			<persName><surname>Jty</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Verifying text summaries of relational data sets</title>
		<author>
			<persName><forename type="first">Saehan</forename><surname>Jo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Immanuel</forename><surname>Trummer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weicheng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuezhi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niyati</forename><surname>Mehta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGMOD, SIGMOD &apos;19</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="299" to="316" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Scrutinizer: Fact checking statistical claims</title>
		<author>
			<persName><forename type="first">Georgios</forename><surname>Karagiannis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammed</forename><surname>Saeed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paolo</forename><surname>Papotti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Immanuel</forename><surname>Trummer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. VLDB Endow</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2965" to="2968" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Combining fact extraction and verification with neural semantic matching networks</title>
		<author>
			<persName><forename type="first">Yixin</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haonan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Thirty-Third AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6859" to="6866" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Automated fact-checking for assisting human fact-checkers</title>
		<author>
			<persName><forename type="first">Preslav</forename><surname>Nakov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Corney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maram</forename><surname>Hasanain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Firoj</forename><surname>Alam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tamer</forename><surname>Elsayed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alberto</forename><surname>Barrón-Cedeño</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paolo</forename><surname>Papotti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaden</forename><surname>Shaar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Giovanni</forename><surname>Da</surname></persName>
		</author>
		<author>
			<persName><forename type="first">San</forename><surname>Martino</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="4551" to="4558" />
		</imprint>
	</monogr>
	<note type="report_type">Survey Track</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Heideltime: High quality rule-based extraction and normalization of temporal expressions</title>
		<author>
			<persName><forename type="first">Jannik</forename><surname>Strötgen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Gertz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int&apos;l. Workshop on Semantic Evaluation</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="321" to="324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">UCL machine reading group: Four factor framework for fact finding (HexaF)</title>
		<author>
			<persName><forename type="first">Takuma</forename><surname>Yoneda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Welbl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pontus</forename><surname>Stenetorp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">FEVER</title>
		<meeting><address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="97" to="102" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
