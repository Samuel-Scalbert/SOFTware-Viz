<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">De-icing federated SPARQL pipelines: a method for assessing the &quot;freshness&quot; of result sets</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Damien</forename><surname>Graux</surname></persName>
							<email>grauxd@tcd.ie</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Inria</orgName>
								<orgName type="institution" key="instit2">Université Côte d&apos;Azur</orgName>
								<orgName type="institution" key="instit3">CNRS</orgName>
								<address>
									<region>I3S</region>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">ADAPT SFI Centre</orgName>
								<orgName type="institution">Trinity College Dublin</orgName>
								<address>
									<country key="IE">Ireland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Fabrizio</forename><surname>Orlandi</surname></persName>
							<email>orlandif@tcd.ie</email>
							<idno type="ORCID">0000-0003-3392-3162</idno>
							<affiliation key="aff1">
								<orgName type="department">ADAPT SFI Centre</orgName>
								<orgName type="institution">Trinity College Dublin</orgName>
								<address>
									<country key="IE">Ireland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Declan</forename><surname>O'sullivan</surname></persName>
							<email>declan.osullivan@tcd.ie</email>
							<idno type="ORCID">0000-0003-1090-3548</idno>
							<affiliation key="aff1">
								<orgName type="department">ADAPT SFI Centre</orgName>
								<orgName type="institution">Trinity College Dublin</orgName>
								<address>
									<country key="IE">Ireland</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">De-icing federated SPARQL pipelines: a method for assessing the &quot;freshness&quot; of result sets</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">7509DDA571EE29CDBA464C39EE57CEE8</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-04-12T14:51+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In recent years, the ever-increasing number of available linkeddata endpoints has allowed the creation of complex data pipelines leveraging these massive amounts of information. One crucial challenge for federated pipeline designers is to know when to query the various sources they use in order to obtain fresher final results. In other words, they want to know when a data update on a specific source impacts their own final results. Unfortunately, the SPARQL standard does not provide them with a method to be aware of such updates; and therefore pipelines are regularly relaunched from scratch, often uselessly. To help them decide when to get fresher results, we propose a constructive method. Practically, it relies on digitally signing result sets from federated endpoints in order to create a specific query able to warn when, and explain why, the pipeline result set is outdated. In addition, as our solution is exclusively based on SPARQL 1.1 built-in functions, it is fully-compliant with all the endpoints.</p><p>2 https://www.w3.org/TR/sparql11-query/#func-hash 3 select * where{ values ?x {"ab" "ab"^^xsd:string} bind (md5(?x) as ?H)}</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>During the past decades, the number of linked open datasets has rapidly increased <ref type="foot" target="#foot_0">1</ref> . These datasets are structured following the W3C standard Resource Description Framework (RDF) <ref type="bibr" target="#b5">[6]</ref> and share knowledge on various domains, from the generalist ones such as DBpedia <ref type="bibr" target="#b0">[1]</ref> or WikiData <ref type="bibr" target="#b6">[7]</ref> to the most specialised ones, e.g. SemanGit <ref type="bibr" target="#b4">[5]</ref>. This abundance of open datasets and SPARQL <ref type="bibr" target="#b1">[2]</ref> endpoints led not only researchers but also businesses to integrate RDF graphs into their complex data pipelines. In particular, businesses are increasingly leveraging Semantic Web technologies to structure their own data and create value, sometimes integrating external Linked Data to enrich their analyses <ref type="bibr" target="#b3">[4]</ref>.</p><p>Benefiting from these two decades of developments made by the community, it is now possible to deploy Semantic Web pipelines and to adapt them to a wide range of needs and use-cases. Recent developments have been, for example, focused on distributed systems or on connecting Semantic Web data management systems together with non-RDF centric systems, paving the road to querying heterogeneous data. As a consequence of this increasing complexity of the usecases, the pipelines themselves are getting more complicated, and often rely on several distinct data sources using SPARQL federation techniques, in order to compute their final results.</p><p>Hence, as data available may change, these pipelines (or parts of them) are frequently re-run in order to get fresher results. However, lots of times they are re-run unnecessarily as datasets have not been updated in the meantime in ways that impact the result sets of the pipeline. All these operations are leading to a waste of computation power and loads on the network.</p><p>In this article, mainly dedicated to SPARQL practitioners and data pipeline designers, we present some possibilities provided by SPARQL 1.1 <ref type="bibr" target="#b1">[2]</ref> to hash query result sets with a focus on the case of SPARQL federation of sources with the SERVICE keyword. In particular, we will discuss how these methods can be used to optimise data pipelines avoiding expensive re-computation of results when data triples have not been updated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Hashing features in SPARQL</head><p>In this Section, we review several use-cases where the use of SPARQL hashing features becomes relevant. In particular, we focus on various methods to obtain a "signature" of a SPARQL result set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">SPARQL 1.1 capabilities</head><p>The SPARQL standard provides a large set of built-in functions, from ones dedicated to strings to specific ones about dates. These can be used by query designers to refine their result set. In particular, the standard offers a set of five hash functions 2 : MD5, SHA1, SHA256, SHA384 &amp; SHA512.</p><p>General signature of the hash functions: simple literal hash_function (simple literal arg) simple literal hash_function (xsd:string arg)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Example using MD5:</head><p>H = md5("ab") = md5("ab"^^xsd:string) H = "187ef4436122d1cc2f40dc2b92f0eba0" These functions accept either RDF literals or strings as argument and return the hash as a literal. In addition, a xsd:string or its corresponding literal should return the same result. In the 'MD5' example above, the hash value represents the result of a simple SPARQL query 3 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2.2</head><p>From hashing an RDF graph . . . Technically, the standard hash functions provide the query designer with useful tools to, for instance, check/sign a specific value within a filter field. Nevertheless, they could also be used for more complex tasks that simply checking a value. For example, in a more extreme case, they could be used over a complete RDF graph through a SPARQL endpoint: one can extract all the triples available with select * where {?s ?p ?o}, and then hash all of them, aggregated with a group concat function. This could look like so:</p><p>SELECT (SHA1(GROUP_CONCAT(?tripleStr ; separator=' \n'))) AS ?nTriples WHERE { ?s ?p ?o BIND(CONCAT(STR(?s), " ", STR(?p), " ", STR(?o)) AS ?tripleStr) }</p><p>In the previous query, the triples ?s ?p ?o are cast by element to a string (STR), and then concatenated to form a "triple". The recomposed list of triples is then grouped into one single string (GROUP CCONCAT) and finally hashed. Obviously, this "naive" approach has drawbacks. First, the result depends on the order of the triples returned by the triplestore: a workaround can be achieved adding e.g. ORDER BY ?s ?p ?o datatype(?o) lcase(lang(?o)). Second, this method has a scalability issue, as all the graph is loaded in-memory before the hash call. We therefore recommend this approach to sign small RDF graphs, e.g. ontologies or small result sets. Finally, this method does not address the complex case of blank node identification as e.g. { :a p o} and { :b p o} do not have the same hashes (see e.g. <ref type="bibr" target="#b2">[3]</ref> for algorithmic solutions).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">. . . To hashing SPARQL result sets</head><p>More pragmatically however, these hash functions can be used in order to sign SPARQL query result sets, typically, they allow users to compare different query results for the same SPARQL endpoint. As we know, on the same endpoint, the same query (without calls to functions like RAND or NOW) is supposed to return the same result set for the same dataset. A hash of the results could be computed by the endpoint and be compared with a previously obtained one. In case of a mismatch, the query (and the rest of the pipeline) could be run again. Assuming Q is the considered SPARQL select query, we propose the following steps to generate the query which computes the hash of the results of Q:</p><p>1. Extract and sort the list of distinguished variables V (if a * is given, the considered variables are the ones involved in the where); 2. Wrap Q in a select * query ordered by V; 3. Embed the obtained query in a select query computing the hash of the grouped concatenation of the cast (to string) distinguished variables.</p><p>To give an example, if we consider the query <ref type="foot" target="#foot_2">4</ref>  Using this method, a SPARQL pipeline designer can easily obtain a hash corresponding to a result set, and store it for computing a comparison later. She would therefore know if RDF data updates have been impacting, in the meanwhile, her results. On advantage of such a method lays in the place where the computations happens: the SPARQL endpoint which could thereby be in charge of having otpimisation techniques (such as result caching or sub-view updating) to retrieve the result sets faster.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Gaining in precision with the SERVICE keyword</head><p>SPARQL pipelines often rely on several data sources that are queried on-the-fly and whose sub-results are aggregated to form the final result set. Practically, the SPARQL standard provides the designers with a built-in function to call an external endpoint with specific conditions: the SERVICE keyword. This keyword is supposed to take as arguments a SPARQL endpoint URI<ref type="foot" target="#foot_3">5</ref> and a set of conditions similarly to the where clause.</p><p>Consequently, by the mean of SPARQL sub-queries it is possible to refine the previous method when the designer sets up federated queries. Indeed, each SERVICE call can be extracted and encapsulated following the steps presented previously. For example, considering SELECT ?s WHERE { ?s ?p ?o . SERVICE &lt;URI&gt; { ?s ?t ?u . }}, we can obtain a finer precision computing the following two hashing SPARQL queries: The first query (left-hand side) focuses on computing a hash of the results set obtained from the external source; and the second one hashes the complete result set obtained once the local and the external results are joined. More generally, one needs to generate one dedicated hashing query per SERVICE call plus a general hashing query for the main/top-level SPARQL query. By using this strategy, the query designer is then able to create her merkle tree<ref type="foot" target="#foot_4">6</ref> of her different sources' sub-results. Moreover, this method is fully compliant with the SPARQL standard as it uses exclusively built-in functions. Finally, this "incremental" approach allows the users to know which source has been updated in a way that might impact the general query.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Automation for complex federated SPARQL queries</head><p>The aforedescribed steps to generate the queries that obtain the hashes, splitting the query into pieces based on SERVICE, are easy to automate and allow users to know when to relaunch their pipelines. In order to help pipeline designers in practice, we serve our method at: https://dgraux.github.io/SPARQL-hash/ where our query converters can be used directly by developers to generate queries computing the hash of their result sets both for "regular" SPARQL queries and federated ones.</p><p>In the case of source federation, as the generation of several hashing queries (as presented in the previous section) is tedious and prone to committing errors, we also provide a generator dedicated to federated queries. Practically, our generator takes the SPARQL query Q and returns a new one Q hash which take care of computing all the necessary hashing sub-queries once run on the designers' SPARQL endpoint <ref type="foot" target="#foot_5">7</ref> . Specifically, Q hash's result is another SPARQL query Q check which is in charge of running<ref type="foot" target="#foot_6">8</ref> the various hashing sub-queries in order to compare the hashes with the previous ones obtained by Q hash. By construction, Q hash is supposed to be run only once and Q check should be the one -once obtained-used regularly to check if Q needs to be relaunched in order to update the designer's final result set. Furthermore, Q check is built in a way which allows the query designers to know why she should update her final result set; it actually explains which specific source is updated.</p><p>As Q check and more Q hash tend to be long, we refer the reader to the extensive set of examples on our website for details. Notwithstanding, for the sake of clarity, let's consider the following simplified query (Q): select * where { ... service uri {...}}. Let's now name Q serv and Q gene the sub-queries generating "123" and "abc": the hashes (following the methods proposed in Sections 2.3 &amp; 2.4) for the service part and for Q respectively. Therefore, Q check would have the following sketch: select ?res where { { Q_serv } # To compute a fresh hash for the service { Q_gene } # To compute a fresh hash for Q if (Q_serv="123"){ if (Q_gene="abc") {?res="No update"} else {?res="Local data has changed"} } else { ?res="External source has changed" } }</p><p>As presented above, Q check inspects the possible updates of hash results using a set of nested conditions. In particular, this set of conditions follows the exploration of a merkle tree starting from the bottom layer. Therefore, by structuring the query like so, an optimised SPARQL endpoint might be able to properly order its query plan in order to evaluate sequentially the various sub-queries and stop as soon as possible, saving computational resources.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusions</head><p>This article describes how to improve existing Semantic Web data pipelines with a SPARQL-based method that helps in identifying when query results have changed. In particular, it sheds lights on a possible strategy when dealing with complex federated queries. It allows to re-run pipelines only when interesting parts of the original datasets have been updated. By using SPARQL to compute the signature of the query results, it avoids large result sets to be sent over the network. Finally, it also helps the designers identify what are the external sources that have been updated. We hope this will inspire developers to use the hash functions provided by the standard.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>which extracts from DBpedia the current members of English-named Punk rock groups, Q= Its sorted list of distinguished variables would be ?bandName ?members. And to obtain a (MD5-)hash of the results of Q, we should run:</figDesc><table><row><cell cols="2">SELECT MD5(GROUP_CONCAT(CONCAT(STR(?bandName),STR(?members)); separator=' \n'))</cell></row><row><cell>as ?H WHERE {</cell><cell></cell></row><row><cell>SELECT * WHERE {</cell><cell># Collecting all the ordered results</cell></row><row><cell>SELECT ?members ?bandName WHERE {</cell><cell># The original query</cell></row><row><cell cols="2">?band dbo:genre dbr:Punk_rock . ?band dbp:currentMembers ?members.</cell></row><row><cell cols="2">?band foaf:name ?bandName FILTER(langMatches(lang(?bandName), "en")) }</cell></row><row><cell>} ORDER BY ?bandName ?members</cell><cell># Ordering by distinguished variables</cell></row><row><cell>}</cell><cell></cell></row><row><cell>SELECT ?members ?bandName WHERE {</cell><cell></cell></row><row><cell cols="2">?band dbo:genre dbr:Punk_rock . ?band dbp:currentMembers ?members .</cell></row><row><cell cols="2">?band foaf:name ?bandName FILTER(langMatches(lang(?bandName), "en")) }</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>From</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2010" xml:id="foot_1"><p>to 2020, the LOD-cloud has grown from 203 to 1 255 datasets, approximately: https://lod-cloud.net/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2"><p>The queries can be tested directly on DBpedia: the query Q and its Q hash . As of April 12 th 2021, Q hash returns ?H = "967d2c8c0a82038d8478d476fa41e14f".</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_3"><p>According to the SPARQL 1.1 standard, this SPARQL endpoint URI may also be a variable, e.g. "SERVICE ?url { ... }", requiring the endpoint to first retrieve the possible entities for this variable and then call the external endpoints applying the set of conditions. However, this case is currently out of our scope.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_4"><p>Following recursively the SERVICE calls.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_5"><p>i.e. the same endpoint as the one where Q is supposed to be run.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_6"><p>Like Q hash, Q check should be run on the endpoint where Q is supposed to be run.</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">DBpedia: A nucleus for a web of open data</title>
		<author>
			<persName><forename type="first">S</forename><surname>Auer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Bizer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Kobilarov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lehmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cyganiak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ives</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The semantic web</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Sparql 1.1 query language</title>
		<author>
			<persName><forename type="first">S</forename><surname>Harris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Seaborne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Prud'hommeaux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">W3C recommendation</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page">778</biblScope>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Canonical forms for isomorphic and equivalent RDF graphs: algorithms for leaning and labelling blank nodes</title>
		<author>
			<persName><forename type="first">A</forename><surname>Hogan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on the Web (TWEB)</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="62" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Knowledge graph-based legal search over german court cases</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Junior</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Orlandi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Graux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hossari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>O'sullivan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hartz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Dirschl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Semantic Web: ESWC 2020 Satellite Events. LNCS</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">12124</biblScope>
			<biblScope unit="page" from="293" to="297" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">SemanGit: A linked dataset from git</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">O</forename><surname>Kubitza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Böckmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Graux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Semantic Web Conference</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="215" to="228" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">RDF primer</title>
		<author>
			<persName><forename type="first">F</forename><surname>Manola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mcbride</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">W3C recommendation</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">1-107</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Wikidata: a free collaborative knowledgebase</title>
		<author>
			<persName><forename type="first">D</forename><surname>Vrandečić</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Krötzsch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="78" to="85" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
