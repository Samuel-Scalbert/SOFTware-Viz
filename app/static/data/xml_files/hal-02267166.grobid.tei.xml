<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Admissible Generalizations of Examples as Rules</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Philippe</forename><surname>Besnard</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Véronique</forename><surname>Masson</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">CNRS -IRIT France</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">Thomas Guyet Agrocampus Ouest</orgName>
								<orgName type="institution" key="instit2">IRISA</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Université de Rennes</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">IRISA</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Admissible Generalizations of Examples as Rules</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">D01A0923A6405D700DB43A5C93D7A0F9</idno>
					<idno type="DOI">10.1109/ICTAI.2019.00211</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-04-12T14:48+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Rule learning is a data analysis task that consists in extracting rules that generalize examples. This is achieved by a plethora of algorithms. Some generalizations make more sense for the data scientists, called here admissible generalizations. The purpose of this article is to show formal properties of admissible generalizations. A formalization for generalization of examples is proposed allowing the expression of rule admissibility. Some admissible generalizations are captured by preclosure and capping operators. Also, we are interested in selecting supersets of examples that induce such operators. We then define classes of selection functions. This formalization is more particularly developed for examples with numerical attributes. Classes of such functions are associated with notions of generalization and they are used to comment some results of the CN2 algorithm <ref type="bibr" target="#b4">[5]</ref>.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Generalizing a given set of examples is essential in many machine learning techniques such as rule learning or pattern mining. Particularly, rule learning is a data mining task that consists in generating disjunctive sets of rules from a dataset of examples labeled by a class identifier. We are focusing on propositional rule induction <ref type="bibr" target="#b7">[8]</ref> where rules have the form "IF Conditions THEN class-label". Rule learning consists in finding individual rules <ref type="bibr" target="#b18">[19]</ref>, each rule generalizes a subset of the dataset examples. Many algorithms achieve rule induction, from CN2 <ref type="bibr" target="#b4">[5]</ref>, Ripper <ref type="bibr" target="#b5">[6]</ref> to recent subgroup discovery algorithms <ref type="bibr" target="#b2">[3]</ref>. Usually each rule is evaluated by different measures using the number of positive and negative examples covered, i.e. generalized, by the rule. Numerous interestingness measures <ref type="bibr" target="#b10">[11]</ref> on rules have been proposed. Some heuristic measures guide the machine learning algorithms and some of them are used in a post-processing step to select final rules.</p><p>Rule learning algorithms received recent attention in the machine learning community due to their interpretability. Explainability and interpretability of machine learning results is a hot topic <ref type="bibr" target="#b6">[7]</ref>. The logical structure of a rule can be easily interpreted by users not familiar with machine learning or data mining concepts. We feel that generalization of examples by machine learning algorithms impacts interpretability, particularly when this generalization is counter-intuitive.</p><p>Table <ref type="table" target="#tab_0">I</ref> is an illustration of a dataset of house rental ads. Each row is a house rental ad and each column is an attribute. With a minimal coverage size of 2, CN2 extracts the following three rules predicting a value for the class-attribute C:  Generalization of a metric attribute leads to the difficult question of defining boundary values. The π CN 2   2   rule uses a value for A 2 attribute (i.e., 2.5) which is not in the original dataset. The choice of this boundary is motivated by statistical reasons: with an hypothesis of uniform distribution of numerical attribute, it minimizes the generalization error. One can notice that it is less intuitive (although equivalent) than the rule:</p><formula xml:id="formula_0">• π CN 2 1 : A 5 = Downtown → C = expensive • π CN 2 2 : A 2 &lt; 2.50 ∧ A 4 = Toulouse → C = low-priced • π CN</formula><formula xml:id="formula_1">A 2 ≤ 2 ∧ A 4 = Toulouse → C = low-priced.</formula><p>This small example illustrates that existing rule learning algorithms underestimate the effects that their underlying hypotheses about generalization can have on the value of extracted rules for a data scientist -some rules sometimes fail to capture an intuitive generalization of the examples.</p><p>We are wondering whether it is possible to highlight some general principles of intuitive generalization that would help to analyze or to qualify rules or rulesets extracted by rule learning algorithms. This means that we are interested in analyzing consequences of choices made by rule learning algorithm when generalizing examples.</p><p>There are different approaches to reach such an objective: scoring interestingness or quality measures <ref type="bibr" target="#b0">[1]</ref> or analyzing results on the light of subjective criteria <ref type="bibr" target="#b19">[20]</ref> (see related works for more details).</p><p>The purpose of this paper is to propose a topological formalization for generalization of examples which favours an analysis on the admissibility of the generalization by enabling to express different notions of admissibility. One objective is to make it possible to compare the outputs of rule learning algorithms with the theoretically admissible rules in order to shed light on some poorly interpretable outputs.</p><p>Importantly, our work is also original as it pays special attention to the values (mostly as boundaries) occurring in rules whereas work in the literature on improving rules or rulesets usually focus on the structure of rules or of rulesets, see e.g. <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b3">[4]</ref>. The contributions of this work are:</p><formula xml:id="formula_2">S π S π S π . . . . . . S ••• π ••• (C) (A 1 ) (A 2 ) (A 3 ) (A 4 ) (A 5 ) (A 6 )</formula><p>• we propose an abstract formalization of the rule learning process introducing the generalization of examples as a choice of a generalization rule π = S (for a set of examples S) among supersets of S, • we introduce the notion of admissibility, we derive two alternative versions of admissibility from Kuratowski's axioms and we give sufficient conditions on choice functions to induce admissible generalizations, • we instantiate admissibility in the specific case of metric attributes and we analyze some CN2 results in the light of our framework. Note that we do not have an immediate practical objective: our purpose is not to design a new rule learning algorithm but to set a general framework that may help to shed light on some aspects of existing, or future, rule learning algorithms. In particular, we illustrate that a well-known algorithm such as CN2 makes some counterintuitive choices of boundaries in rules with metric attributes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. GENERALIZATION OF DATA AS A RULE</head><p>This work focuses on the LearnOneRule step of the rule learning process <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b7">[8]</ref>. The LearnOneRule process is viewed as a two-step process, depicted in Figure <ref type="figure">1:</ref> 1) Some subsets of data are selected. In Figure <ref type="figure">1</ref>, φ selects possible subsets of data. 2) Each subset of data (both subset of columns and rows of the dataset) is assumed to be generalized by a single rule. A subset of data is generalized by a rule. For a data subset (A, S) (some examples restricted to some attributes) of the dataset, the idea of the "generalization" function f from (A, S) is to generate the rule π. Thus, a data subset is generalized by a single rule.</p><p>This work investigates the f function, i.e., how to generate a rule from a subset of data. We do not tackle the question of determining φ, i.e., how to select data subsets. It is assumed that rules are generated from all possible selected subsets. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Data and rules</head><p>Data consist of tuples of size n (for</p><formula xml:id="formula_3">n attributes A 1 , • • • , A n ).</formula><p>It is assumed that each tuple is assigned a class value. The range of an attribute A i , denoted Rng A i , may, or may not, be ordered. For Table <ref type="table" target="#tab_0">I</ref>, range of attributes and their structure are given in Table <ref type="table" target="#tab_2">II</ref>. Values of attributes are of various types: floats (e.g., A 1 ), integers (A 2 ), discrete values either structured (e.g., A 6 can be partially ordered), or unstructured (e.g., A 4 ). A rule learning algorithm elicits rules of the form:</p><formula xml:id="formula_4">A π(1) (x) ∈ v 1 ∧ • • • ∧ A π(k) (x) ∈ v k → C(x) ∈ v 0 ( * )</formula><p>where</p><formula xml:id="formula_5">1 ≤ k ≤ n, v i ⊆ Rng A π(i) for i = 1..k, v 0 ⊆ Rng C and {π(1), • • • , π(k)} ⊆ {1, • • • , n}.</formula><p>Such a rule expresses that for an item x, if the value of each attribute A π(i) is one within v i then the class value of x is one within v 0 .</p><p>The value v i is a subset of the range of the attribute A π(i) (or class C). So, v i can be a singleton subset {u} of the range Rng A π(i) of the attribute i.e.</p><formula xml:id="formula_6">A π(i) (x) ∈ v i is A π(i) (x) = u. Or, v i can be a finite subset {u 1 , • • • , u ip } of Rng A π(i) hence A π(i) (x) ∈ v i is just the disjunctive condition A π(i) (x) = u 1 or A π(i) (x) = u 2 or . . . or A π(i) (x) = u ip .</formula><p>Disjunctive conclusions are unusual in rule learning but they could be desired and they generalize the approach. Lastly, v i can be an arbitrary subset of the range Rng A π(i) . Structure over Rng A π(i) can be exploited, e.g. A π(i) (x) ≥ r is captured by setting v i to the interval [r, M ] (if M is the greatest element).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. General form of rules</head><p>We can identify a rule with a sequence of values for some attributes among A 1 , • • • , A n as well as C thus resulting in the general form for a rule π</p><formula xml:id="formula_7">π = A π(1) (x) ∈ v π 1 ∧ • • • ∧ A π(kπ) (x) ∈ v π kπ → C(x) ∈ v π 0 ( †) with 1 ≤ k π ≤ n, v π i ⊆ Rng A π(i) for i = 1..k π , v π 0 ⊆ Rng C and {π(1), • • • , π(k π )} ⊆ {1, • • • , n}.</formula><p>For the sake of simplicity, such a rule can be expressed as a member of</p><formula xml:id="formula_8">2 Rng C × 2 Rng A1 × • • • × 2 Rng An , i.e., a vector π = (v π 0 v π 1 v π 2 • • • v π n ) ( ‡)</formula><p>where</p><formula xml:id="formula_9">for i = 1..n, v π i = Rng A i if A i ∈ {A π(1) , • • • , A π(kπ) }. A tuple (x 1 , • • • , x n ) which is assigned the class value c is said to be covered by the rule π above if c ∈ v π 0 and all x i ∈ v π i (for i ∈ {π(1), • • • , π(k π )}</formula><p>). Notation: In the sequel, S i denotes the set of values that the attribute A i takes in the subset S of the data, i.e.,</p><formula xml:id="formula_10">S i def = {x i | (x 0 , x 1 , • • • , x n ) ∈ S}</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Admissible rule generation</head><p>This work focuses on finding one rule, generalizing a subset of the dataset, which makes sense for the data scientist. Learning one rule aims at extending the set of values actually taken by the examples. Theoretically, any extra value, not covered by a counter-example, would work. However, some extended sets make more sense than others: we call this notion rule admissibility.</p><p>We do not consider how to find such a subset of the dataset but, given such a subset, we investigate the question of what is an admissible generalization of these examples for a user. Please note that we don't provide a definition for admissibility. What we provide is a framework to express different notions of "admissibility". Indeed, it depends upon application and users. The way we propose to analyze rule learning algorithms is to confront practical results with these different notions.</p><p>There is a sense in which a data subset determines a single rule. It is the view that the only rule generated by S is</p><formula xml:id="formula_11">A 1 (x) ∈ S 1 ∧ • • • ∧ A n (x) ∈ S n → C(x) ∈ S 0 ( §)</formula><p>where X is the smallest rule admissible superset of X (for</p><formula xml:id="formula_12">X ⊆ Rng A i or X ⊆ Rng C).</formula><p>Please note that this requires the assumption that attributes are independent for the purpose of rule admissibility. That S is a data subset under φ (see Figure <ref type="figure">1</ref>) means that if S is to amount to a rule π then each tuple in S is covered by π. Therefore, such a rule ( §) is to be of the kind</p><formula xml:id="formula_13">A π(1) (x) ∈ v 1 ∧ • • • ∧ A π(k) (x) ∈ v k → C(x) ∈ v 0 ( * * ) where 1 ≤ k ≤ n, S π(i) ⊆ v i for i = 1, • • • , k and S 0 ⊆ v 0 (as usual, {π(1), • • • , π(k)} ⊆ {1, • • • , n}). What vector (v 0 v 1 v 2 • • • v n ) can count</formula><p>as a rule for the purpose of capturing S? Since ( * * ) is meant to capture S, we are looking for a vector π</p><formula xml:id="formula_14">≥ (S 0 S 1 S 2 • • • S n ) (i.e., S i ⊆ v π i for i = 0, • • • , n) where every v π i is rule admissible.</formula><p>Technically, the least 1 such π is the case that v i = S i for i = 0, • • • , n. As a rule, it does not fit. If S is to be viewed as a rule, the intuition is that a tuple close enough to some member(s) of S is expected to behave similarly to this member (or those members).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Properties of generalization as choice</head><p>The intuition we point out in the latter remark suggests that generalizing a set of values to a superset thereof amounts to applying a closure-like 2 operator • . For any attribute A i and data subset S, generalizing S i is identified with mapping S i to S i , with properties taken from the list of Kuratowski's axioms:</p><formula xml:id="formula_15">∅ = ∅ S ⊆ S ⊆ Rng A i S = S S ∪ S = S ∪ S (pre-closure)</formula><p>Actually, we downgrade Kuratowski's axioms as follows (the Appendix reminds the definitions of related operators)</p><formula xml:id="formula_16">S ⊆ S whenever S ⊆ S (closure) S = S whenever S ⊆ S ⊆ S (cumulation) S ∪ S ⊆ S whenever S ⊆ S (capping)</formula><p>We thus arrive at two classes of weaker operators that are worth exploring further: preclosure operators and capping operators, resp. realizing interpolation from single points and interpolation from pairs of points (with the view that rule generation encompasses interpolation of some kind).</p><p>This notion of rule admissibility is to be captured by a selection function, f , to fit the general view of Figure <ref type="figure">1</ref>. Such a function (from a special class) determines an appropriate superset of S i given some subsets of the powerset of Rng A i . The intuition here is that rule admissible subsets of the range Rng A i of an attribute A i can be characterized as choices from the powerset of Rng A i . Depending on what principles underly the actual choice, a different kind of closure embodies rule generation through the rule generalization principle.</p><p>The next theorems (with Rng A i generalized to a set Z) specify two classes of selection functions that induce a closurelike operator over a powerset: preclosure and capping.</p><p>Theorem 1 (Selection functions inducing a preclosure operator):</p><formula xml:id="formula_17">Let Z be a set such that f : 2 2 Z → 2 Z is a function satisfying the three conditions below for all X ⊆ 2 Z such that X is upward closed and all Y ⊆ 2 Z : 1. f (2 Z ) = ∅ 2. f (X ) ∈ X 3. f (X ∩ Y) = f (X ) ∪ f (Y) whenever min(X ∩ Y) = min X ∪ min Y The mapping • : 2 Z → 2 Z such that X def = f ({Y | X ⊆ Y ⊆ Z})</formula><p>is a preclosure operator on Z.</p><formula xml:id="formula_18">1 π ≤ π iff v π i ⊆ v π i for i = 0, • • • , n. 2 Closure-like operators are topological operators, cf Appendix.</formula><p>Theorem 2 (Selection functions inducing a capping operator): Let Z be a set, f : 2 2 Z → 2 Z be a function obeying the next two conditions for all X ⊆ 2 Z s.t. X ∈ X and all Y ⊆ 2 Z :</p><p>1</p><formula xml:id="formula_19">. f (X ) ∈ X , 2. if Y ⊆ X and ∃H ∈ Y, H ⊆ f (X ) then f (Y) ⊆ f (X ). The mapping • : 2 Z → 2 Z such that X def = f ({Y | X ⊆ Y ⊆ Z})</formula><p>is a capping operator on Z.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. GENERATION OF RULES WITH METRIC ATTRIBUTES</head><p>Back to the idea of rule generation as interpolation, we are considering the simplest case of rules with a collection of items that all take the value u for metric attribute A i and that all are in class c. A rule for this case is</p><formula xml:id="formula_20">A i (x) = u → C(x) = c</formula><p>i.e. c = f i (u) for some function f i . This rule is restrictive as it only applies for items that take exactly the value u for A i .</p><p>As items take a set of specific values {u 1 , u 2 , . . . , u n }, our idea is to generate rules that generalize them to an interval of values <ref type="bibr">[v, w]</ref>. The main issue is to determine classes of selection functions that yield intuitive intervals of values.</p><p>The first approach is to propose a neighborhood principle. Since c = f i (u), it seems rather reasonable to still expect the class to be c for all values close enough to u. Assuming a notion of neighborhood, a rule exemplifying this would be</p><formula xml:id="formula_21">A i (x) ∈ [u -r, u + r] → C(x) = c.</formula><p>This is developed in the next section where a class of selection functions is given that all induce a preclosure operator.</p><p>A drawback of the neighborhood approach is its predefined radius, r, which does not take into account the actual values distribution when it comes to finding intervals. Another section proposes a second approach that deals with interpolation from pairs (u, v) of values for an attribute A i . This captures the idea that an interval of values is made of elements that are close enough to each other. We show that this principle can be captured through a capping operator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Neighborhoods</head><p>As an application, consider neighborhoods for real-valued data (i.e., Rng A i ⊆ IR). For a datum u ∈ IR, we look at a generalization for u in the form of the neighborhood centered at u of radius r, for a given r &gt; 0. For r &gt; 0 ∈ IR, let n r : 2 IR → 2 IR be the function:</p><formula xml:id="formula_22">n r (X) def = ∅ if X = ∅ u∈X [u -r, u + r] otherwise</formula><p>It happens that n r is a preclosure operator, i.e., as presented in the Appendix, n r is a mapping c : 2 U → 2 U such that:</p><formula xml:id="formula_23">c(∅) = ∅ (null fixpoint) X ⊆ c(X) ⊆ U (extension) c(X ∪ Y ) = c(X) ∪ c(Y ) (preservation of binary unions)</formula><p>Besides this independent evidence, we get the same conclusion from Theorem 1, giving an actual selection function f . Now, a useful abbreviation is ↑ {X}</p><formula xml:id="formula_24">def = {Y | X ⊆ Y ⊆ Z}.</formula><p>For a subset S of IR, define f (↑ {S}) by:</p><formula xml:id="formula_25">f (↑ {S}) def = ∅ if S = ∅ x∈S [x -r, x + r] otherwise Extend f to all of 2 2 Z by taking f (X ) def = S ∈ min X f (↑ {S}).</formula><p>(Since X denotes a collection of subsets of Z, min X denotes those sets in X that have no proper subset in X .) Then, f satisfies conditions 1-3 of Theorem 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Intervals</head><p>We look now at generalization from S i as interpolation of the kind: If u ∈ S i and v ∈ S i such that the distance between u and v is smaller than some threshold then generalize u and v to all values (in the range of A i ) between u and v. We again follow the idea that generalizing S i amounts to applying some kind of closure operator • , giving S i . We start with considering closure operators (see the Appendix), i.e., for all S ⊆ Rng A i and S ⊆ Rng A i , the following holds:</p><formula xml:id="formula_26">S ⊆ S ⊆ Rng A i , S = S, S ⊆ S whenever S ⊆ S .</formula><p>Interestingly, for a data subset S, in order to determine the rule π induced by S, applying S = S means that if S i is rule admissible then it is enough to set v π i = S i and no further adjustment over π is needed regarding the attribute A i (further adjustements are likely for other attributes).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Example of • not being a closure operator</head><p>Imagine a principle that generalizes values (from IN) to small intervals over IN. For instance, let attribute A 2 be distance to townhall. Let the class attribute C be level of rent (understood as ranging over cheap, low-priced, . . . ). From the minimalistic S consisting of items 1 and 2 below:  <ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b8">9]</ref>. This is a counterexample to closure because (isotony) fails: S 2 ⊆ S 2 but v π 2 ⊆ v π 2 . π says that rents of flats in the vicinity of the town hall are low and so are rents of flats in a 7 to 9 km ring from the town hall but no example confirm this for flats in the range 3 to 7 km. We can regard [1,3] ∪ <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b8">9]</ref> as more admissible than the large <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b8">9]</ref>.</p><formula xml:id="formula_27">Distance . . . Level A 1 to</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Example of • not being a cumulation operator</head><p>Since • fails to be a closure operator, the next possibility is that • is a cumulation operator (every closure operator is a cumulation operator but the converse is untrue).</p><p>Again  <ref type="bibr">5.5, 6.9]</ref>. Again, the idea is that the gap between two consecutive values is to be turned into an interval unless the gap is much greater than most of the other gaps in the series: the gap from 5.0 to 5.5 has length .5 but all other gaps here (from 4.1 to 4.2, . . . , from 6.8 to 6.9) have length at most .2.</p><p>It is a counterexample to cumulation because</p><formula xml:id="formula_28">S 2 ⊆ S 2 ⊆ v π 2 but v π 2 ⊆ v π 2 (still, v π 2 ⊆ v π 2 )</formula><p>. All this suggests some kind of preservation principle:</p><p>If new items confirming a rule are added, generalization should not make the rule to be further generalized. It seems that such an approach to generalizing a set of values to a superset thereof amounts to applying a capping operator. The next section shows that such a view can be identified with using a selection function (from the special class specified in Theorem 2) to determine the appropriate superset eliciting the rule.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Capping operator: selection via power means</head><p>Let the special case that Z is totally ordered and f selects, among all supersets of S = x 1 , . . . , x m ,<ref type="foot" target="#foot_0">3</ref> the union of the intervals over Z that have both endpoints in S and length (denoted by l) bounded by a threshold value ∆(S) as follows</p><formula xml:id="formula_29">ϕ S (x j ) def = [x j , x j+1 ] if l([x j , x j+1 ]) ≤ ∆(S) [x j , x j ] else (including j = m)<label>(1)</label></formula><p>where ∆ is a function on the increasing sequences over Z, i.e. S → ∆(S) for every increasing sequence S. For all finite S ⊆ Z, define</p><formula xml:id="formula_30">f (↑ {S}) def = x∈S ϕ S (x)<label>(2)</label></formula><p>where the ϕ S : S → 2 Z functions can be required to satisfy, for all x ∈ S and all finite S ⊆ Z, the following constraints</p><formula xml:id="formula_31">(i) x ∈ ϕ S (x), (ii) S ⊆ S ⊆ ϕ S (S) ⇒ ϕ S (S ) ⊆ ϕ S (S).</formula><p>Extend f to all of 2 2 Z by taking</p><formula xml:id="formula_32">f (X ) def = X whenever X = ↑ {S} for all S ⊆ Z. (<label>3</label></formula><formula xml:id="formula_33">)</formula><p>Proposition 1: If ϕ satisfies conditions (i) and (ii) then f as defined by (2)-( <ref type="formula" target="#formula_32">3</ref>) enjoys conditions 1.-2. of Theorem 2.</p><p>We focus on intervals with endpoints in IR (hence S ⊆ IR) and length the absolute difference between both endpoints.</p><p>Proposition 2: Let ϕ be as in (1) with ∆ such that for all finite S and S , if</p><formula xml:id="formula_34">S ⊆ S ⊆ ϕ S (S) then ∆(S ) ≤ ∆(S). If ∆ is real-valued, if Z is IR and if l([x, y]) = |y -x| then ϕ satisfies (i)-(ii).</formula><p>An almost direct application of Theorem 2 then implies that functions defined as in (1) induce capping operators. The following instances for ∆ functions give selection functions, as per (1)-( <ref type="formula" target="#formula_32">3</ref>), generating admissible rules:</p><formula xml:id="formula_35">• Geometric mean: ∆(S) = m-1 i=1 (x i+1 -x i ) 1 m-1 • Higher power means: ∆(S) = p 1 m-1 m i=2 (x i -x i-1 ) p IV. RELATED WORK</formula><p>Rule learning algorithms are a class of machine learning algorithms mainly developed in the 90's <ref type="bibr" target="#b21">[22]</ref> that drew recent interest due to the interpretable nature of its outputs <ref type="bibr" target="#b7">[8]</ref>.</p><p>Prior works have proposed foundations for rule learning and many algorithms exist. A major reference is <ref type="bibr" target="#b7">[8]</ref> that broadly presents the concepts used in rule learning algorithms. It mainly focuses on practical aspects that enable the reader to understand a broad range of algorithms. Our framework aims at turning the rule learning formalization to a more conceptual level. We investigate in particular the LearnOneRule step of the rule learning process. A key issue in the LearnOneRule algorithm is how to evaluate and compare different rules <ref type="bibr" target="#b7">[8]</ref> using several measures such as precision, information gain, correlation, m-estimate, etc. The basic principle underlying these measures is a simultaneous optimization of consistency and coverage. This optimization addresses the way of choosing a subset of examples covered by a rule but does not give any information on the interest of a generalization from a data scientist viewpoint. Our framework allows to address admissibility of generalizations and thus to define classes of generalizations able to catch empirical concepts of neighborhood for example.</p><p>This notion of admissibility contributes to a formalization of the interpretativeness of the outputs of rule learning algorithms. Similar questions have been addressed in previous works. To the best of our knowledge, none of them addressed the problem of the choice of the values in the rules. They take into consideration the structure of the rules (e.g. their size) or the rule set <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b2">[3]</ref>. Instances of the GUHA method to mining association rule <ref type="bibr" target="#b11">[12]</ref> fall under our approach if conclusions of such rules are to play the role of classes.</p><p>In <ref type="bibr" target="#b0">[1]</ref>, the proposed framework is based on a score for rule quality measures. It does not use quality measures that select intuitive rules, but the most accurate ones. <ref type="bibr" target="#b14">[15]</ref> addressed the intuitiveness of rules through the effects of cognitive biases. They notice that a number of biases can be triggered by the lack of understanding of attributes or their values appearing in rules. In <ref type="bibr" target="#b8">[9]</ref>, the authors suggest that "longer explanations may be more convincing than shorter ones" (see <ref type="bibr" target="#b19">[20]</ref>, too) and evaluate this criterion using a crowd-sourcing study based on about 3.000 judgments.</p><p>Hence, one of our contributions is to relate generalization of examples to closure-like operators. Relationship with Formal Concept Analysis <ref type="bibr" target="#b9">[10]</ref> then comes to mind. In <ref type="bibr" target="#b13">[14]</ref>, the authors investigate the problem of mining numerical data with Formal Concept Analysis. This amounts to a way to generate some subsets of data. As we have shown that the operators at work In the upper histogram, the data distributions are simulated using uniform distributions. In the lower histogram, the data distributions are simulated using a mixture of two normal distributions per class. in generalizing by intervals are weaker than closure operators, no equivalence is expected. Even the idea that a subset (A, S) of the dataset is always a subset of a concept fails in general. For instance, let φ capture the idea of "contraries", in which case a selected subset of size two consists in two examples e 1 and e 2 such that A i (e 1 ) = A i (e 2 ) for all A i in A hence σ({e 1 , e 2 }) = ∅ which entails that no superset of {e 1 , e 2 } can be a concept with a non-empty set of attributes. In contrast, there exist selection functions that provide a subset of Rng A i as a generalization for {e 1 , e 2 }.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. ILLUSTRATION WITH CN2 RULES</head><p>We illustrate some behaviours of CN2 <ref type="bibr" target="#b4">[5]</ref> when facing artificial data distributions, to show that our proposed formalisation offers a framework for analyzing rule learning algorithms. We use a simulated dataset with a single numerical attribute, v, and two classes. The form of the generated rules is v ∈ [l, u] ⇒ C where [l, u] is an interval and C ∈ {blue, green}.</p><p>Remember, the abstract modeling of a rule learning process has two steps: selection of a subset of data and generalisation of this subset of data by a rule. This article is focused on the generalisation step. The simple case studies below make the assumption that each class of the dataset corresponds to a subset of examples to generalize. Then, our analysis assumes two subsets of data (and thus two rules): the subset of data labeled as blue and the ones labeled as green. These two subsets are for illustration purposes only, we make no claim that they are more sensible than other alternative subsets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. CN2 splits potentially interesting intervals</head><p>Here, we illustrate the fact that, despite the relative continuity of the attribute, the CN2 algorithm splits attribute intervals of a rule in some specific cases of overlapping values.</p><p>Figure <ref type="figure" target="#fig_0">2</ref> illustrates two data distributions for which we run the CN2 algorithm. The distribution of each class is dense from lower to higher value, i.e., the gaps between two consecutive examples of a class are bounded. It is desirable to have exactly one rule per class generalizing all examples. This would be the case if rule generalization were to follow the principles of neighborhoods or intervals applied on data subsets made of examples belonging to the same class. Yet, the CN2 algorithm splits the interval in several sub-intervals to improve its selection criteria based on accuracy.</p><p>The extracted rules in case of uniform distributions (top of Figure <ref type="figure" target="#fig_0">2</ref>), resp., for normal distributions (bottom of Figure <ref type="figure" target="#fig_0">2</ref>) are in the leftmost list, resp., in the rightmost list below:  This illustrates the more general situation that an approach insensitive to density of examples over the choice of boundaries amounts to a cumulation operator:</p><formula xml:id="formula_36">• v ∈ [-∞, 10.03] ⇒ blue • v ∈ [-∞, 0.96] ⇒ blue • v ∈ [12</formula><p>Theorem 3: Let Z be a set such that f : 2 2 Z → 2 Z is a function satisfying the two conditions below for all nonempty</p><formula xml:id="formula_37">X ⊆ 2 Z and Y ⊆ 2 Z : 1. f (X ) ∈ X , 2. f (X ∪ Y) ∈ X ⇒ f (X ∪ Y) = f (X ). The mapping • : 2 Z → 2 Z such that X def = f ({Y | X ⊆ Y ⊆ Z})</formula><p>is a cumulation operator on Z.  the same. Our experiments show that the gap length has no consequences on the rules. The very same rule is generated splitting examples by comparing their value with 0. We can conclude from this example that CN2 does not behave in the way described by preclosure operators.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Impact of example sparsity on boundaries choice</head><p>The two preceding experiments seem to show that rules are generated only from the extreme values of examples sets without considering the actual example distribution. Such a behaviour appears to be more constrained than the one of a capping operator. It amounts to a ∆ function that would not be decreasing. But these examples are specific cases of datasets with well-separated classes. In case of overlapping range of values (see Figure <ref type="figure" target="#fig_0">2</ref>), the rule choices that have been made depend on example distributions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSION AND PERSPECTIVES</head><p>Evaluation and comparison of rules in the rule learning task are currently based on optimization of consistency and coverage measures. In this article, we look at the notion of rule admissibility as the interest of a generalization from a data scientist viewpoint. We define a framework providing a formal approach to the generalization of examples in the attribute-value rule learning task and some foundations for the admissible generalizations of examples as rules. Our notion of admissibility is presented as a choice of one generalization among all supersets of examples. Distinguished notions of choice are shown to capture a closure-like operator (preclosure or capping). In the case of metric attributes, we offer actual selection functions that induce such operators.</p><p>These selection functions show how our framework supports the analysis of rule-learning evaluation. We have generated synthetic datasets to analyze the behaviour of CN2 in view of notions arising from our framework. Thus, we point out some counter-intuitive behaviours of this algorithm. Some novelty in our work lies with it focussing on the values occuring in rules, instead of, e.g., the structure of rules.</p><p>Since rule learning may involve non-numerical attributes, a short term perspective is to also give selection functions for attributes with weaker structure than enjoyed by numerical attributes (metric structure). Finally, this article does not address the issue of selection of data subsets. Future work is to focus on this part of the process to propose a more complete formal model for attribute-value rule learning.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>2 3:</head><label>2</label><figDesc>A 1 &gt; 36.00 ∧ A 3 = D → C = cheap § Corresponding author: Véronique Masson (veronique.masson@irisa.fr)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Data distributions for classes blue (in blue) and green (in green).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Balanced (on top) and unbalanced (on bottom) distributions of two classes examples. Classes are separated by a fixed distance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4</head><label>4</label><figDesc>Figure 4 illustrates two datasets whose example distributions differ by the gaps between consecutive examples of the same class. In the first case, these gaps are small (average gap of 1 unit) w.r.t. the gap between the two classes (3 units). In such a case, we expect to generate a rule that gather all examples in a single interval. It is actually what happens with CN2. In the second case, gaps between consecutive examples are larger (average gap of 10 units), but the behaviour of CN2 remains</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Two datasets centered on 0 (each bar is one example). The gap between consecutive examples is 1 unit at the top and 10 units at the bottom.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I DATASET</head><label>I</label><figDesc>OF HOUSE RENTAL ADS. BLANK CELLS ARE MISSING VALUES.</figDesc><table><row><cell></cell><cell>(C)</cell><cell cols="2">(A 1 ) (A 2 )</cell><cell>(A 3 )</cell><cell>(A 4 )</cell><cell>(A 5 )</cell><cell>(A 6 )</cell></row><row><cell></cell><cell cols="7">Price Area #Rooms Energy Town District Exposure</cell></row><row><cell cols="3">1 low-priced 45</cell><cell>2</cell><cell>D</cell><cell cols="2">Toulouse Minimes</cell></row><row><cell>2</cell><cell>cheap</cell><cell>75</cell><cell>4</cell><cell>D</cell><cell cols="2">Toulouse Rangueil</cell></row><row><cell cols="3">3 expensive 65</cell><cell>3</cell><cell></cell><cell cols="2">Toulouse Downtown</cell></row><row><cell cols="3">4 low-priced 32</cell><cell>2</cell><cell>D</cell><cell>Toulouse</cell><cell></cell><cell>SE</cell></row><row><cell cols="3">5 mid-priced 65</cell><cell>2</cell><cell>D</cell><cell>Rennes</cell><cell></cell><cell>SW</cell></row><row><cell cols="3">6 expensive 100</cell><cell>5</cell><cell>C</cell><cell cols="2">Rennes Downtown</cell></row><row><cell>7</cell><cell>cheap</cell><cell>40</cell><cell>2</cell><cell>D</cell><cell>Betton</cell><cell></cell><cell>S</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Fig.1. Abstract modeling of the LearnOneRule process (see text for details). Grey cells illustrate selected rows and columns that may generate πCN 2   </figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>f</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>φ</cell><cell>f</cell></row><row><cell></cell><cell>Price</cell><cell cols="3">Area #Rooms Energy</cell><cell>Town</cell><cell>District Exposure</cell></row><row><cell cols="3">1 low-priced 45</cell><cell>2</cell><cell>D</cell><cell cols="2">Toulouse Minimes</cell><cell>φ</cell></row><row><cell>2</cell><cell>cheap</cell><cell>75</cell><cell>4</cell><cell cols="3">D Toulouse Rangueil</cell><cell>φ</cell><cell>f</cell></row><row><cell cols="3">3 expensive 65</cell><cell>3</cell><cell></cell><cell cols="2">Toulouse Downtown</cell></row><row><cell cols="3">4 low-priced 32</cell><cell>2</cell><cell>D</cell><cell>Toulouse</cell><cell>SE</cell></row><row><cell cols="3">5 mid-priced 65</cell><cell>2</cell><cell>D</cell><cell>Rennes</cell><cell>SW</cell><cell>φ</cell></row><row><cell cols="3">6 expensive 100</cell><cell>5</cell><cell>C</cell><cell cols="2">Rennes Downtown</cell></row><row><cell>7</cell><cell>cheap</cell><cell>40</cell><cell>2</cell><cell>D</cell><cell>Betton</cell><cell>S</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>f</cell></row></table><note><p><p>2</p>.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE II RANGES</head><label>II</label><figDesc>FOR THE ATTRIBUTES OF THE DATASET FROM TABLE I.</figDesc><table><row><cell>Attr.</cell><cell>Range</cell><cell>Structure</cell></row><row><cell cols="3">A 0 {cheap, low-priced, mid-priced, expensive} total order</cell></row><row><cell>A 1</cell><cell>[1, 500]</cell><cell>metric</cell></row><row><cell>A 2</cell><cell>{1, 2, 3, 4, 5, 6}</cell><cell>metric</cell></row><row><cell>A 3</cell><cell>{A, B, C, D, E}</cell><cell>total order</cell></row><row><cell>A 4</cell><cell>{Toulouse, Rennes, Betton}</cell><cell></cell></row><row><cell>A 5</cell><cell>{Downtown, Rangueil, Minimes}</cell><cell></cell></row><row><cell>A 6</cell><cell>{S, N, W, E, SE, SW, N E, N W }</cell><cell>partial order</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>, think of some principle that generalizes values (from IN) to small intervals over IN. Here is a brief example (we use decimals of km to abbreviate hundreds of meters), with S consisting of items i 1 to i 9 below: Indeed, gaps between any two consecutive values among these nine values are of somewhat similar length and are turned into intervals. Now, from S consisting of items i 1 to i 20 below:</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>then the same principle can make S (items i 1 to i 20 ) to give a rule with v π 2 = [4.1, 5] ∪ [</cell></row><row><cell cols="2">Distance A 1 to townhall</cell><cell>. . .</cell><cell>Level of rent</cell></row><row><cell>item 1 . . .</cell><cell>4.1 km</cell><cell>. . .</cell><cell>cheap</cell></row><row><cell>item 2 . . .</cell><cell>4.4 km</cell><cell>. . .</cell><cell>cheap</cell></row><row><cell>item 3 . . .</cell><cell>4.8 km</cell><cell>. . .</cell><cell>cheap</cell></row><row><cell>item 4 . . .</cell><cell>5 km</cell><cell>. . .</cell><cell>cheap</cell></row><row><cell>item 5 . . .</cell><cell>5.5 km</cell><cell>. . .</cell><cell>cheap</cell></row><row><cell>item 6 . . .</cell><cell>5.8 km</cell><cell>. . .</cell><cell>cheap</cell></row><row><cell>item 7 . . .</cell><cell>6.1 km</cell><cell>. . .</cell><cell>cheap</cell></row><row><cell>item 8 . . .</cell><cell>6.6 km</cell><cell>. . .</cell><cell>cheap</cell></row><row><cell>item 9 . . .</cell><cell>6.9 km</cell><cell>. . .</cell><cell></cell></row><row><cell cols="2">Distance A 1 to townhall</cell><cell>. . .</cell><cell>Level of rent</cell></row><row><cell>item i 1 . . .</cell><cell>4.1 km</cell><cell>. . .</cell><cell>cheap</cell></row><row><cell>item i 2 . . .</cell><cell>4.2 km</cell><cell>. . .</cell><cell>cheap</cell></row><row><cell>item i 3 . . .</cell><cell>4.3 km</cell><cell>. . .</cell><cell>cheap</cell></row><row><cell>item i 4 . . .</cell><cell>4.4 km</cell><cell>. . .</cell><cell>cheap</cell></row><row><cell>item i 5 . . .</cell><cell>4.6 km</cell><cell>. . .</cell><cell>cheap</cell></row><row><cell>item i 6 . . .</cell><cell>4.7 km</cell><cell>. . .</cell><cell>cheap</cell></row><row><cell>item i 7 . . .</cell><cell>4.8 km</cell><cell>. . .</cell><cell>cheap</cell></row><row><cell>item i 8 . . .</cell><cell>6.6 km</cell><cell>. . .</cell><cell>cheap</cell></row><row><cell>item i 9 . . .</cell><cell>6.9 km</cell><cell>. . .</cell><cell>cheap</cell></row><row><cell>item i 10 . . .</cell><cell>4.1 km</cell><cell>. . .</cell><cell>cheap</cell></row><row><cell>item i 11 . . .</cell><cell>4.1 km</cell><cell>. . .</cell><cell>cheap</cell></row><row><cell>item i 12 . . .</cell><cell>4.4 km</cell><cell>. . .</cell><cell>cheap</cell></row><row><cell>item i 13 . . .</cell><cell>4.8 km</cell><cell>. . .</cell><cell>cheap</cell></row><row><cell>item i 14 . . .</cell><cell>5 km</cell><cell>. . .</cell><cell>cheap</cell></row><row><cell>item i 15 . . .</cell><cell>5.5 km</cell><cell>. . .</cell><cell>cheap</cell></row><row><cell>item i 16 . . .</cell><cell>5.8 km</cell><cell>. . .</cell><cell>cheap</cell></row><row><cell>item i 17 . . .</cell><cell>6.1 km</cell><cell>. . .</cell><cell>cheap</cell></row><row><cell>item i 18 . . .</cell><cell>6.6 km</cell><cell>. . .</cell><cell>cheap</cell></row><row><cell>item i 19 . . .</cell><cell>6.8 km</cell><cell>. . .</cell><cell>cheap</cell></row><row><cell>item i 20 . . .</cell><cell>6.9 km</cell><cell>. . .</cell><cell>cheap</cell></row></table><note><p><p>cheap</p>Then, such a principle could make S (the above 9 items) to generate a rule with v π 2 = [4.1, 6.9].</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>.73, 14.83] ⇒ blue • v ∈ [0.97, 2.57] ⇒ blue • v ∈ [10.65, 12.81] ⇒ green • v ∈ [3.09, 10.04] ⇒ blue • v ∈ [15.01, ∞] ⇒ green • v ∈ [3.50, 7.18] ⇒ green • v ∈ [11.55, 13.14] ⇒ green • v ∈ [13.15, ∞] ⇒ greenIn the case of the uniform distributions, the intervals of rules with different decision classes may overlap. CN2 thus allows for intervals occurring in different rules to overlap.B. Impact of example density on boundaries choiceFigure 3 illustrates the case of two (single-attribute) datasets whose class distributions are similar: uniform distribution with the same bounds, but different intensities. Example-classes are balanced in the first dataset but not in the second.</figDesc><table><row><cell>0 2 4 6 8 10 12</cell><cell>0</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell><cell>6</cell><cell>7</cell></row><row><cell>0 2 4 6 8 10 12 14 16</cell><cell>0</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell><cell>6</cell><cell>7</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_0"><p>Here, S is identified with its enumeration in increasing order since this simplifies the formulation in<ref type="bibr" target="#b0">(1)</ref>.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_1"><p>Capping originates in logic where it is called Restricted Cut as it captures the principle that intermediate conclusions can be freely removed from the premises with no loss among conclusions.</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Given a set U, a Kuratowki closure operator <ref type="bibr" target="#b15">[16]</ref> is a mapping c : 2 U → 2 U such that for all X ⊆ 2 U and all</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>preservation of binary unions)</head><p>A first direction to weaken Kuratowki closure operators is to drop (idempotence), resulting in preclosure operators</p><p>Another direction amounts to dropping (null fixpoint) and replacing (preservation of binary unions) by a weaker axiom with all this giving abstract closure operators</p><p>These, in turn, can be weakened (various axioms weaker than (isotony) are detailed in <ref type="bibr" target="#b17">[18]</ref>) to cumulation operators</p><p>which can themselves be weakened to capping operators 4</p><p>For cumulation and capping operators, (idempotence) holds as it is actually a consequence of the other two axioms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SKETCH OF PROOFS</head><p>Proof: [Theorem 1] (Preservation of binary unions) For X = ↑ {X} and Y = ↑ {Y }, the proviso for condition 3. is</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Evaluation of association rule quality measures through feature extraction</title>
		<author>
			<persName><forename type="first">L</forename><surname>José</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francis</forename><surname>Balcázar</surname></persName>
		</author>
		<author>
			<persName><surname>Dogbey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th International Symposium on Advances in Intelligent Data Analysis XII (IDA&apos;2013)</title>
		<editor>
			<persName><forename type="first">Allan</forename><surname>Tucker</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Frank</forename><surname>Höppner</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Arno</forename><surname>Siebes</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Stephen</forename><surname>Swift</surname></persName>
		</editor>
		<meeting>the 12th International Symposium on Advances in Intelligent Data Analysis XII (IDA&apos;2013)<address><addrLine>London, UK</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2013-10">October 2013</date>
			<biblScope unit="volume">8207</biblScope>
			<biblScope unit="page" from="68" to="79" />
		</imprint>
	</monogr>
	<note>Information Systems and Applications</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Hierarchical interestingness measures for association rules with generalization on both antecedent and consequent sides</title>
		<author>
			<persName><forename type="first">Fernando</forename><surname>Benites</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elena</forename><surname>Sapozhnikova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="page" from="197" to="203" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Identifying consistent statements about numerical data with dispersion-corrected subgroup discovery</title>
		<author>
			<persName><forename type="first">Mario</forename><surname>Boley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><forename type="middle">R</forename><surname>Goldsmith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luca</forename><forename type="middle">M</forename><surname>Ghiringhelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jilles</forename><surname>Vreeken</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Data Mining and Knowledge Discovery</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1391" to="1418" />
			<date type="published" when="2017-09">September 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">An interpretable classification rule mining algorithm</title>
		<author>
			<persName><forename type="first">Alberto</forename><surname>Cano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amelia</forename><surname>Zafra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastián</forename><surname>Ventura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Sciences</title>
		<imprint>
			<biblScope unit="volume">240</biblScope>
			<biblScope unit="page" from="1" to="20" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The CN2 induction algorithm</title>
		<author>
			<persName><forename type="first">Peter</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Niblett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="261" to="283" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Fast effective rule induction</title>
		<author>
			<persName><surname>William W Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th International Conference on Machine Learning (ICML&apos;1995)</title>
		<editor>
			<persName><forename type="first">Armand</forename><surname>Prieditis</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Stuart</forename><forename type="middle">J</forename><surname>Russell</surname></persName>
		</editor>
		<meeting>the 12th International Conference on Machine Learning (ICML&apos;1995)<address><addrLine>Tahoe City, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="1995-07">July 1995</date>
			<biblScope unit="page" from="115" to="123" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Explainable and Interpretable Models in Computer Vision and Machine Learning</title>
		<author>
			<persName><forename type="first">Sergio</forename><surname>Hugo Jair Escalante</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Isabelle</forename><surname>Escalera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yagmur</forename><surname>Baró</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Umut</forename><surname>Güçlütürk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marcel</forename><forename type="middle">A J</forename><surname>Güçlü</surname></persName>
		</author>
		<author>
			<persName><surname>Van Gerven</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Springer Series on Challenges in Machine Learning</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Foundations of Rule Learning</title>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Fürnkranz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dragan</forename><surname>Gamberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nada</forename><surname>Lavrač</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<publisher>Springer Science &amp; Business Media</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">On cognitive preferences and the plausability of rule-based models</title>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Fürnkranz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tomás</forename><surname>Kliegr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heiko</forename><surname>Paulheim</surname></persName>
		</author>
		<idno>CoRR, abs/1803.01316</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Formal Concept Analysis: Mathematical Foundations</title>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Ganter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rudolf</forename><surname>Wille</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<publisher>Springer Science &amp; Business Media</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Interestingness measures for data mining: A survey</title>
		<author>
			<persName><forename type="first">Liqiang</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Howard</forename><forename type="middle">J</forename><surname>Hamilton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The GUHA method and its meaning for data mining</title>
		<author>
			<persName><forename type="first">Petr</forename><surname>Hájek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Holena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Rauch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer System Science</title>
		<imprint>
			<biblScope unit="volume">76</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="34" to="48" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">BruteSuppression: a size reduction method for apriori rule sets</title>
		<author>
			<persName><forename type="first">Jon</forename><surname>Hills</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anthony</forename><forename type="middle">J</forename><surname>Bagnall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Beatriz</forename><surname>De La Iglesia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Graeme</forename><surname>Richards</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Intelligent Information Systems</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="431" to="454" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Revisiting numerical pattern mining with formal concept analysis</title>
		<author>
			<persName><forename type="first">Mehdi</forename><surname>Kaytoue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergei</forename><forename type="middle">O</forename><surname>Kuznetsov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amedeo</forename><surname>Napoli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd International Joint Conference on Artificial Intelligence (IJCAI&apos;2011)</title>
		<editor>
			<persName><forename type="first">Toby</forename><surname>Walsh</surname></persName>
		</editor>
		<meeting>the 22nd International Joint Conference on Artificial Intelligence (IJCAI&apos;2011)<address><addrLine>Barcelona, Catalonia, Spain</addrLine></address></meeting>
		<imprint>
			<publisher>IJCAI/AAAI Press</publisher>
			<date type="published" when="2011-07">July 2011</date>
			<biblScope unit="page" from="1342" to="1347" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">A review of possible effects of cognitive biases on interpretation of rule-based machine learning models</title>
		<author>
			<persName><forename type="first">Tomás</forename><surname>Kliegr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stepán</forename><surname>Bahník</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Fürnkranz</surname></persName>
		</author>
		<idno>CoRR, abs/1804.02969</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Topology</title>
		<author>
			<persName><forename type="first">Kazimierz</forename><surname>Kuratowski</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1966">1966</date>
			<publisher>Academic Press</publisher>
			<biblScope unit="volume">I</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Interpretable decision sets: A joint framework for description and prediction</title>
		<author>
			<persName><forename type="first">Himabindu</forename><surname>Lakkaraju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><forename type="middle">H</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD&apos;2016)</title>
		<editor>
			<persName><forename type="first">Balaji</forename><surname>Krishnapuram</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Mohak</forename><surname>Shah</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Alexander</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Charu</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Dou</forename><surname>Aggarwal</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Rajeev</forename><surname>Shen</surname></persName>
		</editor>
		<editor>
			<persName><surname>Rastogi</surname></persName>
		</editor>
		<meeting>the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD&apos;2016)<address><addrLine>San Francisco, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016-08">August 2016</date>
			<biblScope unit="page" from="1675" to="1684" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">General patterns in nonmonotonic reasoning</title>
		<author>
			<persName><forename type="first">David</forename><surname>Makinson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Handbook of Logic in Artificial Intelligence and Logic Programming</title>
		<editor>
			<persName><forename type="first">Donald</forename><surname>Nute</surname></persName>
		</editor>
		<meeting><address><addrLine>Oxford</addrLine></address></meeting>
		<imprint>
			<publisher>Clarendon Press</publisher>
			<date type="published" when="1994">1994</date>
			<biblScope unit="volume">III</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Generalization as search</title>
		<author>
			<persName><forename type="first">M</forename><surname>Tom</surname></persName>
		</author>
		<author>
			<persName><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="203" to="226" />
			<date type="published" when="1982">1982</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Shorter rules are better, aren&apos;t they?</title>
		<author>
			<persName><forename type="first">Julius</forename><surname>Stecher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frederik</forename><surname>Janssen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Fürnkranz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th International Conference on Discovery Science (DS&apos;2016)</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">Toon</forename><surname>Calders</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Michelangelo</forename><surname>Ceci</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Donato</forename><surname>Malerba</surname></persName>
		</editor>
		<meeting>the 19th International Conference on Discovery Science (DS&apos;2016)<address><addrLine>Bari, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016-10">October 2016</date>
			<biblScope unit="volume">9956</biblScope>
			<biblScope unit="page" from="279" to="294" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Bayesian rule sets for interpretable classification</title>
		<author>
			<persName><forename type="first">Tong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cynthia</forename><surname>Rudin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Finale</forename><surname>Doshi-Velez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yimin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erica</forename><surname>Klampfl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Perry</forename><surname>Macneille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">16th IEEE International Conference on Data Mining (ICDM&apos;2016)</title>
		<editor>
			<persName><forename type="first">Francesco</forename><surname>Bonchi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Josep</forename><surname>Domingo-Ferrer</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Ricardo</forename><forename type="middle">A</forename><surname>Baeza-Yates</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Zhi-Hua</forename><surname>Zhou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Xindong</forename><surname>Wu</surname></persName>
		</editor>
		<meeting><address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016-12">December 2016</date>
			<biblScope unit="page" from="1269" to="1274" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Data Mining: Practical Machine Learning Tools and Techniques</title>
		<author>
			<persName><forename type="first">Ian</forename><forename type="middle">H</forename><surname>Witten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eibe</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><forename type="middle">A</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">J</forename><surname>Pal</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>Morgan Kaufmann</publisher>
		</imprint>
	</monogr>
	<note>4th edition</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
