<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Explainable Artificial Intelligence</title>
				<funder>
					<orgName type="full">Alberta Machine Intelligence Institute</orgName>
				</funder>
				<funder>
					<orgName type="full">Pan Canadian AI Centres</orgName>
				</funder>
				<funder ref="#_Wza6SQH">
					<orgName type="full">Austrian Science Fund (FWF)</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Luca</forename><surname>Longo</surname></persName>
							<email>luca.longo@tudublin.ie</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Technological University Dublin</orgName>
								<address>
									<country key="IE">Ireland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Randy</forename><surname>Goebel</surname></persName>
							<email>rgoebel@ualberta.ca</email>
							<affiliation key="aff1">
								<orgName type="department">xAI-Lab</orgName>
								<orgName type="institution">Alberta Machine Intelligence Institute University of Alberta</orgName>
								<address>
									<settlement>Edmonton</settlement>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Peter</forename><surname>Kieseberg</surname></persName>
							<email>peter.kieseberg@fhstp.ac.at</email>
							<affiliation key="aff4">
								<orgName type="department">JRC Blockchains</orgName>
								<orgName type="institution">University of Applied Sciences St</orgName>
								<address>
									<settlement>Pölten</settlement>
									<country key="AT">Austria</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Andreas</forename><surname>Holzinger</surname></persName>
							<email>andreas.holzinger@medunigraz.at</email>
							<affiliation key="aff1">
								<orgName type="department">xAI-Lab</orgName>
								<orgName type="institution">Alberta Machine Intelligence Institute University of Alberta</orgName>
								<address>
									<settlement>Edmonton</settlement>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
							<affiliation key="aff5">
								<orgName type="department" key="dep1">Institute for Medical Informatics</orgName>
								<orgName type="department" key="dep2">Statistics &amp; Documentation</orgName>
								<orgName type="laboratory">Human-Centered AI Lab</orgName>
								<orgName type="institution">Medical University Graz</orgName>
								<address>
									<country key="AT">Austria</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">INRIA</orgName>
								<address>
									<settlement>Sophia Antipolis</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">Thales</orgName>
								<address>
									<settlement>Montreal</settlement>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Explainable Artificial Intelligence</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">772481AB365CC1487ACE82390757DA8D</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-04-12T14:52+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>concepts</term>
					<term>applications</term>
					<term>research challenges and visions Explainable Artificial Intelligence</term>
					<term>Machine Learning</term>
					<term>Explainability;</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The development of theory, frameworks and tools for Explainable AI (XAI) is a very active area of research these days, and articulating any kind of coherence on a vision and challenges is itself a challenge. At least two sometimes complementary and colliding threads have emerged. The first focuses on the development of pragmatic tools for increasing the transparency of automatically learned prediction models, as for instance by deep or reinforcement learning. The second is aimed at anticipating the negative impact of opaque models with the desire to regulate or control impactful consequences of incorrect predictions, especially in sensitive areas like medicine and law. The formulation of methods to augment the construction of predictive models with domain knowledge can provide support for producing human understandable explanations for predictions. This runs in parallel with AI regulatory concerns, like the European Union General Data Protection Regulation, which sets standards for the production of explanations from automated or semi-automated decision making. Despite the fact that all this research activity is the growing acknowledgement that the topic of explainability is essential, it is important to recall that it is also among the oldest fields of computer science. In fact, early AI was re-traceable, interpretable, thus understandable by and explainable to humans. The goal of this research is to articulate the big picture ideas and their role in advancing the development of XAI systems, to acknowledge their historical roots, and to emphasise the biggest challenges to moving forward.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Machine learning is often viewed as the technology belonging to the future in many application fields <ref type="bibr" target="#b46">[47]</ref>, ranging from pure commodities like recommender systems for music, to automatic diagnosis of cancer or control models for autonomous transportation. However, one fundamental issue lies within the realm of explainability <ref type="bibr" target="#b61">[62]</ref>. More precisely, most of the existing learning algorithms can often lead to robust and accurate models from data, but in application terms, they fail to provide end-users with descriptions on how they built them, or to produce convincing explanations for their predictions <ref type="bibr" target="#b6">[7]</ref>. In many sensitive applications, such as in medicine, law, and other sectors where the main workers are not computer scientists or engineers, the direct application of these learning algorithms and complex models, without human oversight, is currently inappropriate. The reasons are not only technical, like the accuracy of a model, its stability to decisions and susceptibility to attacks, but often arise from sociological concerns, practically settling on the issue of trust. In fact, one of the principal reasons to produce an explanation is to gain the trust of users <ref type="bibr" target="#b12">[13]</ref>.</p><p>Trust is the main way to enhance the confidence of users with a system <ref type="bibr" target="#b67">[68]</ref> as well as their comfort while using and governing it <ref type="bibr" target="#b41">[42]</ref>. Trust connects to ethics and the intensity of regulatory activities, as for instance the General Data Protection Regulation in the European Union, leads to many legal and even ethical questions: responsibility for safety, liability for malfunction, and tradeoffs therein must inform decision makers at the highest level. Many methods of explainability for data-driven models have emerged in the years, at a growing rate. On the one hand, a large body of work have focused on building post-hoc methods mainly aimed at wrapping fully trained models, often referred to black-boxes, with an explainability layer <ref type="bibr" target="#b37">[38]</ref>. A smaller body of research works, on the other hand, have concentrated on creating self-explainable and interpretable models by incorporating explainability mechanisms during their training, often referred to as the ante-hoc phase <ref type="bibr" target="#b6">[7]</ref>. Despite the fact that all this research activity is the growing acknowledgement of the topic of explainability <ref type="bibr" target="#b69">[70]</ref>, by now referred to as Explainable Artificial Intelligence (XAI) <ref type="bibr" target="#b55">[56]</ref>, it is important to recall that it is also among the oldest fields of computer science. In fact, early AI was retraceable, interpretable, thus understandable by and explainable to humans. For these reasons, many scholars have tried to review research works in the field <ref type="bibr" target="#b52">[53,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b73">74,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b2">3]</ref>. These reviews reveals the needs for a variety of kinds of explanation, for the identification of methods for explainability and their evaluation as well as the need to calibrate the tradeoffs in the degree or level of explanation appropriate for a broad spectrum of applications.</p><p>The goal of this research is to articulate the big picture ideas and their role in advancing the development of XAI systems, to acknowledge their historical roots, and to emphasise the biggest challenges to moving forward. The reminder of the paper focuses on relevant notions and concepts for explainability in section 2. It then continues in section 3 with descriptions on the applications of methods for XAI and on domains and areas in which these can have a significant impact.</p><p>A discussion on the research challenges surrounding XAI is presented in section 4. Eventually, recommendations and visions follow by presenting what we believe scholars should focus on in the development of future explainable AI systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Notions and related concepts</head><p>A serious challenge for any attempt to articulate the current concepts for XAI is that there is a very high volume of current activity, both on the research side <ref type="bibr" target="#b67">[68,</ref><ref type="bibr" target="#b73">74,</ref><ref type="bibr" target="#b21">22]</ref>, and in aggressive industrial developments, where any XAI functions can provide a market advantage to all for profit applications of AI <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b61">62]</ref>. In addition, there remains a lack of consensus on terminology, for example as noted within, there are a variety of definitions for the concept of interpretation, but little current connection to the formal history, that means in formal theories of explanation or causation <ref type="bibr" target="#b30">[31]</ref>. One recent paper <ref type="bibr" target="#b3">[4]</ref> provides an organizing framework based on comparing levels of explanation with levels of autonomous driving. The goal is to identify foundational XAI concepts like relationships to historical work on explanation, especially scientific ones, or the importance of interactive explanation as well as the challenge of their evaluation. Note further that the need for a complex system to provide explanations of activities, including predictions, is not limited to those with components created by machine learning (example in <ref type="bibr" target="#b54">[55]</ref>). Pragmatically, the abstract identification of a scientific explanation that enables an explainee to recreate an experiment or prediction can arise in very simple circumstances. For example, one can evaluate an explanation by simply noting whether it is sufficient to achieve an explainee's intended task. For example, in figure <ref type="figure" target="#fig_0">1</ref>, the pragmatic value of an Ikea visual assembly "explanation" is whether the assembler explainee can achieve the assembly using the diagram.</p><p>Overall and within this broad spectrum of ideas related to explanation, there is some focus on the foundational connection between explanation and that of abductive reasoning. For example, the historical notion of scientific explanation has been the subject of much debate in the community of science and philosophy <ref type="bibr" target="#b71">[72]</ref>. Some propose that a theory of explanation should include both scientific and other simpler forms of explanation. Consequently, it has been a common goal to formulate principles that can confirm an explanation as a scientific one. Aristotle is generally considered to be the first philosopher to articulate an opinion that knowledge becomes scientific when it tries to explain the causes of "why." His view urges that science should not only keep facts, but also describe them in an appropriate explanatory framework <ref type="bibr" target="#b14">[15]</ref>. In addition to this theoretical view, empiricists also maintain a belief that the components of ideas should be acquired from perceptions with which humans become familiar through sensory experience. The development of the principles of scientific explanation from this perspective prospered with the so-called Deductive-Nomological (DN) model that was described by Hempel in <ref type="bibr" target="#b23">[24]</ref><ref type="bibr" target="#b24">[25]</ref><ref type="bibr" target="#b25">[26]</ref>, and by Hempel and Oppenheim in <ref type="bibr" target="#b26">[27]</ref>.</p><p>There is a more pragmatic AI historical research thread that connects scientific explanation to AI implementations of abductive reasoning. One such thread, among many, begins with Pople in 1973 <ref type="bibr" target="#b60">[61]</ref>, <ref type="bibr">Poole et al. in 1987 [60]</ref>, Muggleton in 1991 <ref type="bibr" target="#b53">[54]</ref>, to <ref type="bibr">Evans et al. in 2018 [14]</ref>. Pople described an algorithm for abduction applied to medical diagnosis. Poole et al. provided an extension to first order logic which could subsume non-monotonic reasoning theories and also identify explanatory hypothesis for any application domain. Muggleton proposed a further refinement referred to as inductive logic programming where hypotheses are identified by inductive constraints within any logic, including higher-order logics. Finally, the adoption of this thread of reasoning have been generalised to explanation based on inductive logic programming by Evans et al. <ref type="bibr" target="#b13">[14]</ref>. This most recent work connects with information theoretic ideas used to compare differences in how to learn probability distributions that are modeled by machine learning methods.</p><p>Interpreting and explaining a model trained from data by employing a machine learning technique is not an easy task. A body of literature has focused on tackling this by attempting at defining the concept of interpretability. This has lead to the formation of many types of explanation, with several attributes and structures. For example, it seems to human nature to assign causal attribution of events <ref type="bibr" target="#b22">[23]</ref>, and we possess an innate psychological tendency to anthropomorphism. As a consequence, an AI-based system that purports to capture causal relations should be capable of providing a causal explanation of its inferential process (example in <ref type="bibr" target="#b56">[57]</ref>). Causality can been considered a fundamental attribute of explainability, especially when scientific explainability carries a responsibility to help the explainee reconstruct the inferential process leading to a prediction. Many have noted this role on how explanations should make the causal relationships between the inputs and the outputs of a model explicit <ref type="bibr" target="#b52">[53,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b30">31]</ref>.</p><p>Despite the fact that data-driven models are extremely good at discovering associations in the data, unfortunately they can not guarantee causality of these associations. The objective of significantly inferring causal relationships depends on prior knowledge, and very often some of the discovered associations might be completely unexpected, not interpretable nor explainable. As pointed by <ref type="bibr" target="#b0">[1]</ref>, the decisions taken considering the output of a model should be clearly explainable to support their justifiability. These explanations should allow the identification of potential flows both in a model, enhancing its transparency, the knowledge discovery process, supporting its controllability and improvement of its accuracy. Although the importance of explainability is clear, the definition of objective criteria to evaluate methods for XAI and validate their explanations is still lacking. Numerous notions underlying the effectiveness of explanations were identified from the fields of Philosophy, Psychology and Cognitive Science. These were related to the way humans define, generate, select, evaluate and present explanations <ref type="bibr" target="#b50">[51]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Applications and impact areas</head><p>Explainable artificial intelligence has produced many methods so far and it has been applied in many domains, with different expected impacts <ref type="bibr" target="#b20">[21]</ref>. In these applications, the production of explanations for black box predictions requires a companion method to extract or lift correlative structures from deep-learned models into vocabularies appropriate for user level explanations. Initial activities focused on deep learning image classification with explanations emerging as heat maps created on the basis of gaps in probability distributions between a learned model and an incorrect prediction <ref type="bibr" target="#b4">[5]</ref>. However, the field has become so diverse in methods, often determined by domain specific issues and attributes, that it is scarcely possible to get in-depth knowledge on the whole of it. Additionally, one major aspect though is the problem of explainable AI, where lot of problems have been emerged and illustrated in the literature, especially from not being able to provide explanations. While all of these topics require long and in-depth discussions and are certainly of significant importance for the future of several AI methods in many application domains, we want to focus on the benefits that can be reaped from explainability. This means not focusing on the issues of incomplete and imperfect technologies as a stopping point for applications, but discussing novel solutions provided by explainable AI. A discussion of some, partly prominent and partly surprising examples follows, with arguments on why a certain amount of explainability -as a reflection -is required for more advanced AI. There are many sectors that already have fully functional applications based on machine learning, but still serious problems in applying them exist. These are often caused by failing to be capable to explain how these methods work. In other words, it is known that they work, but the concrete results cannot be explained. Many of these applications either come from safety critical or personally sensitive domains, thus a lot of attention is put on explanations of the inferences of trained models, usually predictions or classifications.</p><p>Threat detection and triage -The detection of threats and efficient triage have been core topics in the area of IT-Security for at least the past three decades. This started with research in the area of code analysis and signature based AntiVirus-Software, moving towards automated decompilation and code analysis, as well as supporting the automated analysis of network monitoring information for triage. Currently, fully automated threat detection and triage is not available in real life systems due to the complexity of the task and the problem with false positives, even though several different approaches exist. These also include strategies that do not try to detect actual threats, but rather filtering out all known legit network travel and thus drastically reducing the amount of information requiring manual analysis <ref type="bibr" target="#b57">[58]</ref>. Still, a major problem without explainability lies in the opaque nature of these methods, thus not being able to fully understand their inner functioning and how an inference was reached. Explainability could greatly enhance the detection capabilities, especially since dynamic effects, such as changing user behavior, could be modelled and introduced earlier into the algorithms without generating a large set of false positives.</p><p>Explainable Object Detection -Object detection is usually performed from a large portfolio of artificial neural networks (ANN) architectures such as YOLO, trained on large amount of labelled data. In such contexts, explaining object detections is rather difficult if not impossible due to the high complexity of the hyperparameters (number of layers, filters, regularisers, optimizer, loss function) of the most accurate ANNs. Therefore, explanations of an object detection task are limited to features involved in the data and modeled in the form of saliency maps <ref type="bibr" target="#b10">[11]</ref> or at best to examples <ref type="bibr" target="#b40">[41]</ref>, or prototypes <ref type="bibr" target="#b35">[36]</ref>. They are the stateof-the-art approaches but explanations are limited by data frames feeding the ANNs. Industrial applications embedding object detection, such as obstacles detection for trains, do require human-like rational for ensuring the system can be guaranteed, even certified <ref type="bibr" target="#b39">[40]</ref>.</p><p>Protection against Adversarial ML -In adversarial machine learning, attackers try to manipulate the results of learning algorithms by inserting specifically crafted data in the learning process <ref type="bibr" target="#b32">[33]</ref>, in order to lead a model to learn erroneous things. Detection of such a manipulation is not trivial, especially in contexts with big data, where no model exists before the analysis phase. While there are several proposals on how to deal with this issue <ref type="bibr" target="#b15">[16]</ref>, some of them employ neural sub-networks for differentiating between malicious and benign input data like <ref type="bibr" target="#b49">[50]</ref>. In this specific circumstance, explainability would have a great impact as it will support the task of uncovering such a manipulation far more quickly, efficiently and without actually finding the examples that have been manipulated, thus greatly enhancing trust in machine learning inferences <ref type="bibr" target="#b31">[32]</ref>.</p><p>Open Source Intelligence (OSINT) -In Open Source Intelligence <ref type="bibr" target="#b18">[19]</ref>, information retrieval is purely reduced to openly available information, as contrary to Signals Intelligence (SIGINT). However, there are several major issues surrounding OSINT, especially referring to context, languages and the amount of information available. Similarly, another problem lies in deciding how much a source is trusted, and what level of impact news of sources shall have on the result of their aggregations. This is especially important when considering adversarial attacks against OSINT methods and systems <ref type="bibr" target="#b11">[12]</ref>. Explainability could provide means for detecting these attacks, with an impact on mitigating their influence. Furthermore, the information that an attack against an intelligent system was launched is also a valuable input from an intelligence perspective, so explainability might lead to additional valuable information. However, not all false information exists due to malice, especially when reporting very recent events: information particles might be wrong, misleading or simply unknown at the time of reporting. OSINT becomes especially complex in case of ongoing events, where facts change every minute, either due to knew intelligence, or simply because of changes in the event itself. Explainability would allow to estimate the effects of incorrect information particles on the overall machine learning outcomes, thus allowing, for instance, to give error margins on reported numbers.</p><p>Trustworthy (autonomous) medical agents -Several architectures for integrating machine learning into medical decision making have been devised in the past. These are based upon a doctor-in-the-loop approach whereby doctors act as input providers to machine learning algorithms. These can lead to suggestions related to diagnosis or treatment that can be subsequently reviewed by the doctors themselves, who, in turn, can provide feedback in a loop to further enhance modeling <ref type="bibr" target="#b34">[35]</ref>. Additionally, the mechanism can also introduces external knowledge to support decision making aimed at incorporating the latest findings in the underlying medical field.</p><p>Autonomous vehicles -While certainly being developed within machine learning, explainability would be beneficial for the area of autonomous vehicles, especially considering autonomous cars. In cases of car accidents, explanations can help trace the reasons why an autonomous vehicle behaved in a certain why and took certain actions. Consequently this can not only lead to safer vehicles, but it also can help solve issues in court faster, greatly enhancing trust towards these novel ML-based technologies and especially the resulting artifacts <ref type="bibr" target="#b19">[20]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Research challenges</head><p>A number of research challenges surrounding the development of methods for explainability exist, including technical, legal and practical challenges.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Technical challenges XAI systems evaluation</head><p>The comprehensive study of what explanation means from a sociological viewpoint <ref type="bibr" target="#b51">[52]</ref> begs a difficult issue that is both technical and non-technical: how does one evaluate the quality of an explanation? It is not a surprise that the quality or value of an explanation is at least partly determined by the receiver of an explanation, sometimes referred to as the "explainee". An easy way to frame the challenge of evaluating explanations, with respect to an explainee, arises from observing the history of the development of evaluation techniques from the field of data visualization <ref type="bibr" target="#b36">[37]</ref>. A simple example of "visual explanation" can frame the general evaluation problem for all explanations as follows. Consider the IKEA assembly diagram, rendered in Figure <ref type="figure" target="#fig_0">1</ref>. A simple list of requirements to assess explanation quality emerges from considering the IKEA assembly instructions as a visual explanation of how to assemble the piece of furniture. In this case, the visual explanation is intended to guide all explainees, and not just a single individual, to the successful assembly of the furniture item. One measure of quality is simply to test whether any individual explainee can use the visual explanation to complete the assembly. Another measure is about whether the visual explanation is clear and unambiguous, so that the assembly is time efficient. In the case of figure <ref type="figure" target="#fig_0">1</ref>, the sequencing of steps might be misinterpreted by an explainee, and that the simple use of circular arrows to indicate motion may also be ambiguous. Overall, and as anticipated in the general evaluation of explanation systems, one could design cognitive experiments to determine, over an experimental human cohort, which portions of the explanation clearly lead to correct inferences, and those which are more difficult to correctly understand. This means that XAI system requirements should include the need to produce an explicit representation of all the components in a way that supports the appropriate interpretation of the visual classification of components. One can generalize visual explanations to the full repertoire that might obtain for a general XAI system. This means a set of representations of the semantics of an underlying domain of application that can provide support to construct an explanation that is understood by a human explainee.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>XAI Interpretation</head><p>Even though XAI systems are supposed to expose the functioning of a learning technique as well as a set of justification of a model's inferences, it remains rather difficult for a human to interpret them. Explanations are not the final words of an intelligent system but rather the intermediate layer that requires knowledge expertise, context and common-sense characterization for appropriate and correct human interpretation and decision-making <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b65">66]</ref>. Semantics, knowledge graphs <ref type="bibr" target="#b38">[39]</ref> and their machine learning representations <ref type="bibr" target="#b5">[6]</ref> or similar technical advancements are interesting avenues to be considered for pushing the interpretation at the next right level of knowledge expertise. These might also include the addition of argumentative capabilities, as applied in <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b62">63]</ref> to produce rational and justifiable explanations <ref type="bibr" target="#b63">[64]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Legal challenges</head><p>While the theoretical ground work in AI stays on the very theoretical side and is thus typically considered to be not problematic from a legal point of view, the actual application of XAI methods in a certain domain can have serious legal implications. This is especially important when considering working with sensitive information. Here, it has yet to be researched whether the explainability related to a model might be used to infer information about individuals, for instance, by using it with slightly different data sets. This technique has been used in many variations in IT-Security, especially considering anonymized data sets or partially released sensitive information as a basis to gather more intelligence on the people involved <ref type="bibr" target="#b8">[9]</ref>. Similar attacks have already been proposed and carried out against machine learned models <ref type="bibr" target="#b66">[67]</ref> and these allowed to produce a great amount of information, hidden correlations and causalities that were used to infer sensitive information.</p><p>Concepts like federated machine learning are built on the notion of executing machine learning algorithms locally on sensitive data sets and then exchanging the resulting feature sets in order to be combined centrally. These are in contrast to more traditional approaches that collect all sensitive data centrally and then run the learning algorithms. One challenge for federated machine learning is to achieve model robustness but greatly focus on protective sensitive inferences. This justifies the need for more applicable anonymisation techniques, as many of the current methods are unsuitable for many application scenarios, either due to performance or quality issues <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b47">48]</ref>. In addition, other legal challenges exist such as the right to be forgotten <ref type="bibr" target="#b68">[69]</ref>. This 'reflects the claim of an individual to have certain data deleted so that third persons can no longer trace them'. This fair right is accompanied by technical difficulties ranging from the issue related to the deletion of entries in modern systems, to the problem of inferring information on individuals from aggregates and especially the removal of said individuals from the aggregation process.</p><p>Despite the aforementioned challenges, positive benefits can be brought by explainability to the area of machine learning and AI as a whole with respect to legal issues. While the issue of transparency, a key requirement in the General Data Protection Regulation (GDPR), can be a rather a hard issue to tackle, this could change with explainability providing detailed insight, where, when and to what extent personal data of a single individual was involved in a data analysis workflow <ref type="bibr" target="#b70">[71]</ref>. While this is currently not a binding requirement to provide that level of details <ref type="bibr" target="#b70">[71]</ref>, this could be a game changer regarding acceptance of AI, as well as increasing privacy protection in a data driven society. Furthermore, a significant problem currently tackled in machine learning is bias <ref type="bibr" target="#b72">[73]</ref>, especially since simple methods for tackling the issue have shown to be ineffective <ref type="bibr" target="#b33">[34]</ref>. Explainability could support this combat and thus provide a better legal standing for the results derived from data driven systems, especially when used for socioeconomic purposes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Practical challenges</head><p>One of the most crucial success factors of AI generally and XAI specifically, is to ensure effective human-AI interfaces to enable a usable and useful interaction between humans and AI <ref type="bibr" target="#b1">[2]</ref>. Such goals have been discussed in the HCI community for decades <ref type="bibr" target="#b9">[10]</ref>, but it was not really seen as important in the AI community. Now the needs and demands of XAI for 'explainable user interfaces' may finally stimulate to realise advanced human-centered concepts similar to the early visions of Vannevar Bush in 1945 <ref type="bibr" target="#b7">[8]</ref>. Here, the goal is to explore both the explainability side, that means the artificial explanation generated by machines, as well as the human side, that means the human understanding. In an ideal world, both machine explanations and human understanding would be identical, and congruent with the ground truth, which is defined for both machines and humans equally. However, in the real world we face two significant problems:</p><p>the ground truth cannot always be fully defined, as for instance when concerned with medical diagnoses <ref type="bibr" target="#b58">[59]</ref> when there is high uncertainty; human models such as scientific, world, problem solving models, are often based on causality, in the sense of Judea Pearl <ref type="bibr" target="#b56">[57]</ref>, which is very challenging as current machine learning does not incorporate them and simply follows pure correlation.</p><p>Practically speaking, current XAI methods mainly focus on highlighting inputrelevant parts, for example via heat-mapping, that significantly contributed to a certain output, or the most relevant features of a training data set that influenced the most the model accuracy. Unfortunately, they do not incorporate the notion of human model, and therefore there is a need to take also into account the concept of causability <ref type="bibr" target="#b29">[30]</ref>. In detail, in line with the concept of usability <ref type="bibr" target="#b27">[28]</ref>, causability is defined as 'the extent to which an explanation of a statement to a human expert achieves a specified level of causal understanding with effectiveness, efficiency and satisfaction in a specified context of use and the particular contextual understanding capabilities of a human'. Following this concept, it becomes possible to measure the quality of explanations in the same terms as usability (effectiveness, efficiency and satisfaction in a specified context of use), for example with a measurement scale <ref type="bibr" target="#b28">[29]</ref> 5 Recommendations and visions</p><p>Machine learning, as a solid research area within artificial intelligence, has undoubtedly impacted the field by providing scholars with a robust suite of methods for modeling complex, non-linear phenomena. With the growing body of work in the last decade on deep learning, this impact has significantly expanded to many applications areas. However, despite the widely acknowledged capability of machine and deep learning to allow scholars to induce accurate models from data and extract relevant patterns, accelerating scientific discovery <ref type="bibr" target="#b17">[18]</ref>, there is the problem of their interpretability and explainability. For this reason, the last few years have seen a growing body of work on research in methods aimed at explaining the inner functioning of data-driven models and the learning techniques used to induce them. Currently and generally recognised as a core area of AI, eXplainable Artificial Intelligence (XAI) has produced a plethora of methods for model interpretability and explainability. Hundred of scientific articles are published each month in many workshops, conferences and presented at symposium around the world. Some of them focus on wrapping trained models with explanatory layers, such as knowledge graphs <ref type="bibr" target="#b39">[40]</ref>. Other try to embed the concept of explainability during training, and some of them try to merge learning capabilities with symbolic reasoning <ref type="bibr" target="#b43">[44]</ref>. Explainability is a concept borrowed from psychology, since it is strictly connected to humans, that is difficult to operationalise. A precise formalisation of the construct of explainability is far from being a trivial task as multiple attributes can participate in its definition <ref type="bibr" target="#b62">[63]</ref>.</p><p>Similarly, the attributes might interact with each other, adding complexity in the definition of an objective measure of explainability <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b64">65]</ref>. For these reasons, the last few years have seen also a growing body of research on approaches for evaluating XAI methods. In other words, approaches that are more focused on the explanations generated by XAI solutions, their structure, efficiency, efficacy and impact on humans understanding.</p><p>The first recommendation to scholars willing to perform scientific research on explainable artificial intelligence and create XAI methods is to firstly focus on the structure of explanations, the attributes of explainability and the way they can influence humans. This links computer science with psychology. The second recommendation is to define the context of explanations, taking into consideration the underlying domain of application, who they will serve and how. Ultimately, explanations are effective when they help end-users to build a complete and correct mental representation of the inferential process of a given data-driven model. Work on this direction should also focus on which type of explanation can be provided to end-users, including textual, visual, numerical, rules-based or mixed solutions. This links computer science with the behavioural and social sciences. The third recommendation is to clearly define the scope of explanations. This might involve the creation of a method that provide end-users with a suite of local explanations for each input instance or the formation of a method that focuses more on generating explanations on a global level aimed at understanding a model as a whole. This links computer science to statistics and mathematics. The final recommendation is to involve humans, as ultimate users of XAI methods, within the loop of model creation, exploitation, as well as the enhancement of its interpretability and explainability. This can include the development of interactive interfaces that allow end-users to navigate through models, understanding their inner logic at a local or global level, for existing or new input instances. This links artificial intelligence with human-computer interaction.</p><p>The visions behind explainable artificial intelligence are certainly numerous. Probably the most important is the creation of models with high accuracy as well as high explainability. The trade-off between these two sides is well known, and usually, increments in one dimension means decrements in the other dimension. Creating interpretable and explainable models that are also highly accurate is the ideal scenario, but since this has been demonstrated to be a hard problem with currents methods of learning and explainability, further research is needed. One possible solution is the creation of models that are fully transparent at all stages of model formation, exploitation and exploration and that are capable of providing local and global explanations. This leads to another vision, which is the use of methods that embed learning capabilities and symbolic reasoning. The former is aimed at generating models and representations with high accuracy for predictive and forecasting purposes, while the latter to explain these representations in highly interpretable natural language terms, aligned to the way human understand and reason.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. An IKEA visual explanation for furniture assembly</figDesc><graphic coords="8,152.06,115.83,311.19,89.10" type="bitmap" /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p><rs type="person">R.Goebel</rs> would like to acknowledge the support of the <rs type="funder">Alberta Machine Intelligence Institute</rs>, which is one of the three <rs type="funder">Pan Canadian AI Centres</rs>. <rs type="person">A.Holzinger</rs> would like to acknowledge the support of the <rs type="funder">Austrian Science Fund (FWF)</rs>, Project: <rs type="grantNumber">P-32554</rs> "<rs type="projectName">Explainable AI -A reference model of explainable Artificial Intelligence for the Medical Domain</rs>"</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_Wza6SQH">
					<idno type="grant-number">P-32554</idno>
					<orgName type="project" subtype="full">Explainable AI -A reference model of explainable Artificial Intelligence for the Medical Domain</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Peeking inside the black-box: A survey on explainable artificial intelligence (xai)</title>
		<author>
			<persName><forename type="first">Amina</forename><surname>Adadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammed</forename><surname>Berrada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="52138" to="52160" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Guidelines for human-ai interaction</title>
		<author>
			<persName><forename type="first">Saleema</forename><surname>Amershi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Weld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mihaela</forename><surname>Vorvoreanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Fourney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Besmira</forename><surname>Nushi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Penny</forename><surname>Collisson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jina</forename><surname>Suh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shamsi</forename><surname>Iqbal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><forename type="middle">N</forename><surname>Bennett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kori</forename><surname>Inkpen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems</title>
		<meeting>the 2019 CHI Conference on Human Factors in Computing Systems</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Evaluating recurrent neural network explanations</title>
		<author>
			<persName><forename type="first">Leila</forename><surname>Arras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ahmed</forename><surname>Osman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Klaus-Robert</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wojciech</forename><surname>Samek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP</title>
		<meeting>the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="113" to="126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">A multi-component framework for the analysis and design of explainable artificial intelligence</title>
		<author>
			<persName><forename type="first">S</forename><surname>Atakishiyev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Babiker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Farruque</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Goebel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M-Y.</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Motallebi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Rabelo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Syed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">R</forename><surname>Zaïane</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.01908v1[cs.AI</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">An introduction to deep visual explanation</title>
		<author>
			<persName><forename type="first">Housam</forename><surname>Khalifa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bashier</forename><surname>Babiker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Randy</forename><surname>Goebel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS 2017 -Workshop Interpreting, Explaining and Visualizing Deep Learning</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Knowledge graph embeddings and explainable AI</title>
		<author>
			<persName><forename type="first">Federico</forename><surname>Bianchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gaetano</forename><surname>Rossiello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luca</forename><surname>Costabello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matteo</forename><surname>Palmonari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pasquale</forename><surname>Minervini</surname></persName>
		</author>
		<idno>CoRR, abs/2004.14843</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Explanation and justification in machine learning: A survey</title>
		<author>
			<persName><forename type="first">Or</forename><surname>Biran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Courtenay</forename><surname>Cotton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI 2017 Workshop on Explainable Artificial Intelligence (XAI)</title>
		<meeting><address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>International Joint Conferences on Artificial Intelligence, Inc</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="8" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">As we may think</title>
		<author>
			<persName><forename type="first">Vannevar</forename><surname>Bush</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Atlantic Monthly</title>
		<imprint>
			<biblScope unit="volume">176</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="101" to="108" />
			<date type="published" when="1945">1945</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Collective data-sanitization for preventing sensitive information inference attacks in social networks</title>
		<author>
			<persName><forename type="first">Zhipeng</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zaobo</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yingshu</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Dependable and Secure Computing</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="577" to="590" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">The psychology of Human-Computer Interaction</title>
		<author>
			<persName><forename type="first">K</forename><surname>Stuart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">P</forename><surname>Card</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><surname>Moran</surname></persName>
		</author>
		<author>
			<persName><surname>Newell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1983">1983</date>
			<publisher>Erlbaum</publisher>
			<pubPlace>Hillsdale (NJ)</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Interpreting neural network classifications with variational dropout saliency maps</title>
		<author>
			<persName><forename type="first">Chun-Hao</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elliot</forename><surname>Creager</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anna</forename><surname>Goldenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Duvenaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Intelligent systems design for malware classification under adversarial conditions</title>
		<author>
			<persName><forename type="first">M</forename><surname>Sean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nathaniel</forename><forename type="middle">D</forename><surname>Devine</surname></persName>
		</author>
		<author>
			<persName><surname>Bastian</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.03149</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">The role of trust in automation reliance</title>
		<author>
			<persName><forename type="first">Mary</forename><forename type="middle">T</forename><surname>Dzindolet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><forename type="middle">A</forename><surname>Peterson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Regina</forename><forename type="middle">A</forename><surname>Pomranky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linda</forename><forename type="middle">G</forename><surname>Pierce</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hall</forename><forename type="middle">P</forename><surname>Beck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of human-computer studies</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="697" to="718" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning explanatory rules from noisy data</title>
		<author>
			<persName><forename type="first">Richard</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><surname>Greffenstette</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="page" from="1" to="64" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Aristotle on causality</title>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Falcon</surname></persName>
		</author>
		<ptr target="https://plato.stanford.edu)" />
	</analytic>
	<monogr>
		<title level="s">Stanford Encyclopedia of Philosophy</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">Reuben</forename><surname>Feinman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saurabh</forename><surname>Ryan R Curtin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">B</forename><surname>Shintre</surname></persName>
		</author>
		<author>
			<persName><surname>Gardner</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.00410</idno>
		<title level="m">Detecting adversarial samples from artifacts</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Explainable planning</title>
		<author>
			<persName><forename type="first">Maria</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Derek</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniele</forename><surname>Magazzeni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI 2017 Workshop on Explainable Artificial Intelligence (XAI)</title>
		<meeting><address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>International Joint Conferences on Artificial Intelligence, Inc</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="24" to="30" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Amplify scientific discovery with artificial intelligence</title>
		<author>
			<persName><forename type="first">Yolanda</forename><surname>Gil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Greaves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Hendler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haym</forename><surname>Hirsh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">346</biblScope>
			<biblScope unit="issue">6206</biblScope>
			<biblScope unit="page" from="171" to="172" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Intelligence in the internet age: The emergence and evolution of open source intelligence (osint)</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Glassman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min</forename><forename type="middle">Ju</forename><surname>Kang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computers in Human Behavior</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="673" to="682" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Trustworthy versus explainable ai in autonomous vessels</title>
		<author>
			<persName><forename type="first">Jon</forename><surname>Arne Glomsrud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">André</forename><surname>Ødegårdstuen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Asun</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Lera</forename><surname>St</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clair</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Øyvind</forename><surname>Smogeli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Seminar on Safety and Security of Autonomous Vessels (ISSAV) and European STAMP Workshop and Conference (ESWC)</title>
		<meeting>the International Seminar on Safety and Security of Autonomous Vessels (ISSAV) and European STAMP Workshop and Conference (ESWC)</meeting>
		<imprint>
			<publisher>Sciendo</publisher>
			<date type="published" when="2019">2019. 2020</date>
			<biblScope unit="page" from="37" to="47" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Explainable ai: the new 42?</title>
		<author>
			<persName><forename type="first">Randy</forename><surname>Goebel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ajay</forename><surname>Chander</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katharina</forename><surname>Holzinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Freddy</forename><surname>Lecue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeynep</forename><surname>Akata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simone</forename><surname>Stumpf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Kieseberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Holzinger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Cross-Domain Conference for Machine Learning and Knowledge Extraction</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="295" to="303" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A survey of methods for explaining black box models</title>
		<author>
			<persName><forename type="first">Riccardo</forename><surname>Guidotti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anna</forename><surname>Monreale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Salvatore</forename><surname>Ruggieri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Franco</forename><surname>Turini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fosca</forename><surname>Giannotti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dino</forename><surname>Pedreschi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM computing surveys (CSUR)</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1" to="93" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Designing explainability of an artificial intelligence system</title>
		<author>
			<persName><forename type="first">Taehyun</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sangwon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sangyeon</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Technology, Mind, and Society</title>
		<meeting>the Technology, Mind, and Society<address><addrLine>Washington, District of Columbia, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">The function of general laws in history</title>
		<author>
			<persName><forename type="first">G</forename><surname>Carl</surname></persName>
		</author>
		<author>
			<persName><surname>Hempel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Philosophy</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="35" to="48" />
			<date type="published" when="1942">1942</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">The theoretician&apos;s dilemma: A study in the logic of theory construction</title>
		<author>
			<persName><forename type="first">G</forename><surname>Carl</surname></persName>
		</author>
		<author>
			<persName><surname>Hempel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Minnesota Studies in the Philosophy of Science</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="173" to="226" />
			<date type="published" when="1958">1958</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<author>
			<persName><forename type="first">G</forename><surname>Carl</surname></persName>
		</author>
		<author>
			<persName><surname>Hempel</surname></persName>
		</author>
		<title level="m">Aspects of scientific explanation</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Free Press</publisher>
			<date type="published" when="1965">1965</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Studies in the logic of explanation</title>
		<author>
			<persName><forename type="first">G</forename><surname>Carl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Hempel</surname></persName>
		</author>
		<author>
			<persName><surname>Oppenheim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Philosophy of science</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="135" to="175" />
			<date type="published" when="1948">1948</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Usability engineering methods for software developers</title>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Holzinger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="71" to="74" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Measuring the quality of explanations: The system causability scale (scs). comparing human and machine explanations</title>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Holzinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andre</forename><surname>Carrington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heimo</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">KI -Künstliche Intelligenz (German Journal of Artificial intelligence), Special Issue on Interactive Machine Learning</title>
		<editor>
			<persName><forename type="first">Kristian</forename><surname>Kersting</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2020">2020</date>
			<pubPlace>TU Darmstadt</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Causability and explainability of artificial intelligence in medicine</title>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Holzinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georg</forename><surname>Langs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Helmut</forename><surname>Denk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kurt</forename><surname>Zatloukal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heimo</forename><surname>Mueller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Causability and explainability of artificial intelligence in medicine</title>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Holzinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georg</forename><surname>Langs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Helmut</forename><surname>Denk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kurt</forename><surname>Zatloukal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heimo</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">e1312</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Can we trust machine learning results? artificial intelligence in safety-critical decision support</title>
		<author>
			<persName><forename type="first">Katharina</forename><surname>Holzinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Klaus</forename><surname>Mak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Kieseberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Holzinger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ERCIM NEWS</title>
		<imprint>
			<biblScope unit="volume">112</biblScope>
			<biblScope unit="page" from="42" to="43" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Adversarial machine learning</title>
		<author>
			<persName><forename type="first">Ling</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anthony</forename><forename type="middle">D</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Blaine</forename><surname>Nelson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Ip Rubinstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Doug</forename><surname>Tygar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 4th ACM workshop on Security and artificial intelligence</title>
		<meeting>the 4th ACM workshop on Security and artificial intelligence</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="43" to="58" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Fairnessaware classifier with prejudice remover regularizer</title>
		<author>
			<persName><forename type="first">Toshihiro</forename><surname>Kamishima</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shotaro</forename><surname>Akaho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hideki</forename><surname>Asoh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Sakuma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Joint European Conference on Machine Learning and Knowledge Discovery in Databases</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="35" to="50" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A tamper-proof audit and control system for the doctor in the loop</title>
		<author>
			<persName><forename type="first">Peter</forename><surname>Kieseberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernd</forename><surname>Malle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Frühwirt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edgar</forename><surname>Weippl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Holzinger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Brain informatics</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="269" to="279" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Examples are not enough, learn to criticize! criticism for interpretability</title>
		<author>
			<persName><forename type="first">Been</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oluwasanmi</forename><surname>Koyejo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rajiv</forename><surname>Khanna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems 2016</title>
		<meeting><address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">December 5-10, 2016. 2016</date>
			<biblScope unit="page" from="2280" to="2288" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Empirical studies in information visualization: Seven scenarios</title>
		<author>
			<persName><forename type="first">Heidi</forename><surname>Lam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Enrico</forename><surname>Bertini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Petra</forename><surname>Isenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Catherine</forename><surname>Plaisant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sheelagh</forename><surname>Carpendale</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Graphics and Visual Computing</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1520" to="1536" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">The dangers of post-hoc interpretability: Unjustified counterfactual explanations</title>
		<author>
			<persName><forename type="first">Thibault</forename><surname>Laugel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marie-Jeanne</forename><surname>Lesot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christophe</forename><surname>Marsala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Renard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marcin</forename><surname>Detyniecki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence</title>
		<meeting>the Twenty-Eighth International Joint Conference on Artificial Intelligence<address><addrLine>Macao, China</addrLine></address></meeting>
		<imprint>
			<publisher>International Joint Conferences on Artificial Intelligence Organization</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2801" to="2807" />
		</imprint>
	</monogr>
	<note>IJCAI)</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">On the role of knowledge graphs in explainable AI</title>
		<author>
			<persName><forename type="first">Freddy</forename><surname>Lécué</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Semantic Web</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="41" to="51" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Feeding machine learning with knowledge graphs for explainable object detection</title>
		<author>
			<persName><forename type="first">Freddy</forename><surname>Lécué</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tanguy</forename><surname>Pommellet</surname></persName>
		</author>
		<ptr target="CEUR-WS.org" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ISWC 2019 Satellite Tracks (Posters &amp; Demonstrations, Industry, and Outrageous Ideas) co-located with 18th International Semantic Web Conference (ISWC 2019)</title>
		<title level="s">CEUR Workshop Proceedings</title>
		<editor>
			<persName><forename type="first">Mari</forename></persName>
		</editor>
		<editor>
			<persName><forename type="first">Carmen</forename><surname>Suárez-Figueroa</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Gong</forename><surname>Cheng</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Anna</forename><forename type="middle">Lisa</forename><surname>Gentile</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Christophe</forename><surname>Guéret</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><forename type="middle">Maria</forename><surname>Keet</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Abraham</forename><surname>Bernstein</surname></persName>
		</editor>
		<meeting>the ISWC 2019 Satellite Tracks (Posters &amp; Demonstrations, Industry, and Outrageous Ideas) co-located with 18th International Semantic Web Conference (ISWC 2019)<address><addrLine>Auckland, New Zealand</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">October 26-30, 2019. 2019</date>
			<biblScope unit="volume">2456</biblScope>
			<biblScope unit="page" from="277" to="280" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Deep learning for casebased reasoning through prototypes: A neural network that explains its predictions</title>
		<author>
			<persName><forename type="first">Oscar</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chaofan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cynthia</forename><surname>Rudin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence</title>
		<meeting>the Thirty-Second AAAI Conference on Artificial Intelligence<address><addrLine>New Orleans, Louisiana, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">February 2-7, 2018. 2018</date>
			<biblScope unit="page" from="3530" to="3537" />
		</imprint>
	</monogr>
	<note>AAAI-18)</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">The mythos of model interpretability</title>
		<author>
			<persName><surname>Zachary C Lipton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="36" to="43" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Argumentation for knowledge representation, conflict resolution, defeasible inference and its integration with machine learning</title>
		<author>
			<persName><forename type="first">Luca</forename><surname>Longo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning for Health Informatics</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="183" to="208" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Defeasible reasoning and argument-based systems in medical fields: An informal overview</title>
		<author>
			<persName><forename type="first">Luca</forename><surname>Longo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierpaolo</forename><surname>Dondio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 27th International Symposium on Computer-Based Medical Systems</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014">2014. 2014</date>
			<biblScope unit="page" from="376" to="381" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Argumentation theory for decision support in health-care: a comparison with machine learning</title>
		<author>
			<persName><forename type="first">Luca</forename><surname>Longo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucy</forename><surname>Hederman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Brain and Health Informatics</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="168" to="180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Argumentation theory in health care</title>
		<author>
			<persName><forename type="first">Luca</forename><surname>Longo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bridget</forename><surname>Kane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucy</forename><surname>Hederman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer-Based Medical Systems (CBMS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012">2012. 2012</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
	<note>25th International Symposium on</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">The forthcoming artificial intelligence (ai) revolution: Its impact on society and firms</title>
		<author>
			<persName><forename type="first">Spyros</forename><surname>Makridakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Futures</title>
		<imprint>
			<biblScope unit="volume">90</biblScope>
			<biblScope unit="page" from="46" to="60" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Do not disturb? classifier behavior on perturbed datasets</title>
		<author>
			<persName><forename type="first">Bernd</forename><surname>Malle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Kieseberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Holzinger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Cross-Domain Conference for Machine Learning and Knowledge Extraction</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="155" to="173" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">The right to be forgotten: towards machine learning on perturbed knowledge bases</title>
		<author>
			<persName><forename type="first">Bernd</forename><surname>Malle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Kieseberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edgar</forename><surname>Weippl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Holzinger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ternational Conference on Availability, Reliability, and Security</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="251" to="266" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<author>
			<persName><forename type="first">Jan</forename><surname>Hendrik Metzen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Genewein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Volker</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bastian</forename><surname>Bischoff</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.04267</idno>
		<title level="m">On detecting adversarial perturbations</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Explanation in artificial intelligence: insights from the social sciences</title>
		<author>
			<persName><forename type="first">Tim</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">267</biblScope>
			<biblScope unit="page" from="1" to="38" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Explanation in Artificial Intelligence: Insights from the Social Sciences</title>
		<author>
			<persName><forename type="first">Tim</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">267</biblScope>
			<biblScope unit="page" from="1" to="38" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Explainable ai: Beware of inmates running the asylum or: How i learnt to stop worrying and love the social and behavioural sciences</title>
		<author>
			<persName><forename type="first">Tim</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piers</forename><surname>Howe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liz</forename><surname>Sonenberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI Workshop on Explainable AI (XAI)</title>
		<meeting><address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>International Joint Conferences on Artificial Intelligence, Inc</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="36" to="42" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Inductive logic programming</title>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Muggleton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">New Generation Computing</title>
		<imprint>
			<biblScope unit="page" from="295" to="318" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">A systematic review and taxonomy of explanations in decision support and recommender systems</title>
		<author>
			<persName><forename type="first">Ingrid</forename><surname>Nunes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dietmar</forename><surname>Jannach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">User Modeling and User-Adapted Interaction</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="393" to="444" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">The pragmatic turn in explainable artificial intelligence (xai)</title>
		<author>
			<persName><forename type="first">Andrés</forename><surname>Páez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Minds and Machines</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="1" to="19" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<author>
			<persName><forename type="first">Judea</forename><surname>Pearl</surname></persName>
		</author>
		<title level="m">Causality: Models, Reasoning, and Inference (2nd Edition)</title>
		<meeting><address><addrLine>Cambridge</addrLine></address></meeting>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Behavioural comparison of systems for anomaly detection</title>
		<author>
			<persName><forename type="first">Martin</forename><surname>Pirker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Kochberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Schwandter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th International Conference on Availability, Reliability and Security</title>
		<meeting>the 13th International Conference on Availability, Reliability and Security</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Towards a deeper understanding of how a pathologist makes a diagnosis: Visualization of the diagnostic process in histopathology</title>
		<author>
			<persName><forename type="first">Birgit</forename><surname>Pohn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michaela</forename><surname>Kargl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Reihs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Holzinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kurt</forename><surname>Zatloukal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heimo</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Symposium on Computers and Communications (ISCC 2019)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Theorist: A logical reasoning system for defaults and diagnosis. The Knowledge Frontier</title>
		<author>
			<persName><forename type="first">David</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Randy</forename><surname>Goebel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Romas</forename><surname>Aleliunas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Symbolic Computation (Artificial Intelligence)</title>
		<imprint>
			<date type="published" when="1987">1987</date>
			<biblScope unit="page" from="331" to="352" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">On the mechanization of abductive logic</title>
		<author>
			<persName><forename type="first">Harry</forename><surname>Pople</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI&apos;73: Proceedings of the 3rd International Joint Conference on Artificial Intelligence</title>
		<imprint>
			<biblScope unit="page" from="147" to="152" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Asking &quot;why&quot; in ai: Explainability of intelligent systems-perspectives and challenges</title>
		<author>
			<persName><forename type="first">Alun</forename><surname>Preece</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Intelligent Systems in Accounting, Finance and Management</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="63" to="72" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Inferential models of mental workload with defeasible argumentation and non-monotonic fuzzy reasoning: a comparative study</title>
		<author>
			<persName><forename type="first">Lucas</forename><surname>Rizzo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luca</forename><surname>Longo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd Workshop on Advances In Argumentation In Artificial Intelligence, co-located with XVII International Conference of the Italian Association for Artificial Intelligence, AI 3 @AI*IA 2018</title>
		<meeting>the 2nd Workshop on Advances In Argumentation In Artificial Intelligence, co-located with XVII International Conference of the Italian Association for Artificial Intelligence, AI 3 @AI*IA 2018<address><addrLine>Trento, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-11">November 2018. 2018</date>
			<biblScope unit="page" from="11" to="26" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">A qualitative investigation of the explainability of defeasible argumentation and non-monotonic fuzzy reasoning</title>
		<author>
			<persName><forename type="first">Lucas</forename><surname>Rizzo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luca</forename><surname>Longo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings for the 26th AIAI Irish Conference on Artificial Intelligence and Cognitive Science Trinity College</title>
		<meeting>for the 26th AIAI Irish Conference on Artificial Intelligence and Cognitive Science Trinity College<address><addrLine>Dublin, Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">December 6-7th, 2018. 2018</date>
			<biblScope unit="page" from="138" to="149" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">An empirical evaluation of the inferential capacity of defeasible argumentation, non-monotonic fuzzy reasoning and expert systems</title>
		<author>
			<persName><forename type="first">Lucas</forename><surname>Rizzo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luca</forename><surname>Longo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Expert Systems with Applications</title>
		<imprint>
			<biblScope unit="volume">147</biblScope>
			<biblScope unit="page">113220</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">A comparative study of defeasible argumentation and non-monotonic fuzzy reasoning for elderly survival prediction using biomarkers</title>
		<author>
			<persName><forename type="first">Lucas</forename><surname>Rizzo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ljiljana</forename><surname>Majnaric</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luca</forename><surname>Longo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference of the Italian Association for Artificial Intelligence</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="197" to="209" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Membership inference attacks against machine learning models</title>
		<author>
			<persName><forename type="first">Reza</forename><surname>Shokri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Stronati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Congzheng</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vitaly</forename><surname>Shmatikov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Symposium on Security and Privacy (SP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="3" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">A survey of explanations in recommender systems</title>
		<author>
			<persName><forename type="first">Nava</forename><surname>Tintarev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Judith</forename><surname>Masthoff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 23rd international conference on data engineering workshop</title>
		<meeting><address><addrLine>Istanbul, Turkey</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="801" to="810" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Humans forget, machines remember: Artificial intelligence and the right to be forgotten</title>
		<author>
			<persName><forename type="first">Eduard</forename><forename type="middle">Fosch</forename><surname>Villaronga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Kieseberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tiffany</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Law &amp; Security Review</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="304" to="313" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<title level="m" type="main">Explainable artificial intelligence: a systematic review</title>
		<author>
			<persName><forename type="first">Giulia</forename><surname>Vilone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luca</forename><surname>Longo</surname></persName>
		</author>
		<idno>CoRR, abs/2006.00093</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Transparent, explainable, and accountable ai for robotics</title>
		<author>
			<persName><forename type="first">Sandra</forename><surname>Wachter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brent</forename><surname>Mittelstadt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luciano</forename><surname>Floridi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science Robotics</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Scientific explanation</title>
		<author>
			<persName><forename type="first">James</forename><surname>Woodward</surname></persName>
		</author>
		<ptr target="https://plato.stanford.edu)" />
	</analytic>
	<monogr>
		<title level="s">Stanford Encyclopedia of Philosophy</title>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Ethical implications of bias in machine learning</title>
		<author>
			<persName><forename type="first">Adrienne</forename><surname>Yapo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><surname>Weiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Hawaii International Conference on System Sciences</title>
		<meeting>the 51st Hawaii International Conference on System Sciences</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>HICCS 2018</note>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Visual interpretability for deep learning: a survey</title>
		<author>
			<persName><forename type="first">Quan-Shi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Song-Chun</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers of Information Technology &amp; Electronic Engineering</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="27" to="39" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
