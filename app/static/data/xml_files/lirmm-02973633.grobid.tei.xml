<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="fr">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">BestNeighbor: Efficient Evaluation of kNN Queries on Large Time Series Databases</title>
				<funder>
					<orgName type="full">INRIA international</orgName>
				</funder>
				<funder ref="#_zNHGKAz">
					<orgName type="full">European Union</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Oleksandra</forename><surname>Levchenko</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Boyan</forename><surname>Kolev</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Djamel-Edine</forename><forename type="middle">Edine</forename><surname>Yagoubi</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Reza</forename><surname>Akbarinia</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Florent</forename><surname>Masseglia</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Themis</forename><surname>Palpanas</surname></persName>
							<email>themis@mi.parisdescartes.fr</email>
						</author>
						<author>
							<persName><forename type="first">Patrick</forename><surname>Valduriez</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Dennis</forename><surname>Shasha</surname></persName>
							<email>shasha@cs.nyu.edu</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="laboratory">LIRMM</orgName>
								<orgName type="institution" key="instit1">Inria</orgName>
								<orgName type="institution" key="instit2">University of Montpellier</orgName>
								<orgName type="institution" key="instit3">CNRS</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Themis Palpanas Université de Paris</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">Dep. of Computer Sc</orgName>
								<orgName type="institution">New York University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">BestNeighbor: Efficient Evaluation of kNN Queries on Large Time Series Databases</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="MD5">33C7E6A891B22322FBAD8C0E1F410FDC</idno>
					<idno type="DOI">10.1007/s10115-020-01518-4</idno>
					<note type="submission">Submitted on 17 Dec 2020 Received: date / Accepted: date</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.0" ident="GROBID" when="2024-04-12T14:49+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Time Series</term>
					<term>Parallel Indexing</term>
					<term>Distributed Querying</term>
					<term>Fourier Coefficients</term>
					<term>Frequency Analysis</term>
					<term>Filtering</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>niveau recherche, publiés ou non, émanant des établissements d'enseignement et de recherche français ou étrangers, des laboratoires publics ou privés.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="fr">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Nowadays people are able to monitor various indicators for their personal activities (e.g., through smart-meters or smart-plugs for electricity or water consumption), or professional activities (e.g., through the sensors installed on plants by farmers). Sensor technology is also improving over time and the number of sensors is increasing, e.g., in finance and seismic studies. This results in the production of large and complex data, usually in the form of time series (or TS in short) <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b29">30]</ref> that challenge knowledge discovery.</p><p>With such complex and massive sets of time series, fast and accurate similarity search is critical to performing many data mining tasks like Shapelets, Motifs Discovery, Classification or Clustering <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b51">52]</ref>.</p><p>In order to improve the performance of such similarity queries, indexing <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11]</ref> has been successfully used in a variety of settings and applications <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b27">28]</ref>.</p><p>In this work, we focus on two distributed time series indexing methods of quite different natures -a hash-based and a tree-based one.</p><p>The hash-based method ParSketch follows a locality-sensitive hashing (LSH) strategy that uses a number of grid structures to hash similar items to the same buckets with high probability. ParSketch processes a query time series by performing a number of hash lookups to identify candidate neighbors, which are then verified by explicit distance computation.</p><p>By contrast, the tree-based method DPiSAX builds an index tree using multiresolution symbolic representations of time series items, so that leaf nodes are represented by high resolution symbols. Thus, candidate neighbors for a query are found by traversing the tree down to the leaf node whose symbolic representation is closest to the representation of the query.</p><p>The two methods are distributed, which allows us to examine their abilities to scale to a large number of time series and operate in a distributed data processing framework, such as Apache Spark. However, the different nature of the two methods leads to different partitioning approaches and method-specific parallelization of the query processing.</p><p>The partitioning of DPiSAX uses the same multiresolution property as centralized iSAX and is based on representations of time series items at some basic resolution. This means that a single query is first quickly routed to one of the partitions using that basic resolution, where the rest of the search down the tree is done locally at the partition. So, each query is performed serially, at one partition of the index.</p><p>In ParSketch, the input dataset is simply split into disjoint subsets of equal sizes, each assigned to a partition, where local grid structures are built. This means that a single query is broadcast to all partitions, where candidate subsets are searched locally. So, the processing of one query makes hash lookups in all partitions of the index, but is done in parallel.</p><p>The major objective of both methods is to do fast approximate search with high quality results. So, one of our goals is to evaluate the quality of the results in relationship to the response time. Moreover, DPiSAX enables exact search as a second step, where it uses the lower bounding properties of the symbolic representation to prune candidates based on the current best-so-far neighbors.</p><p>Our experiments show that these pruning capabilities are sensitive to the frequency spectrum of the input dataset. DPiSAX pruning seems to be quite efficient when energy is concentrated in the first few Fourier coefficients (which is the case of random walks). Exact DPiSAX is slow (and approximate DPiSAX is inaccurate) when energy is spread across the frequency spectrum (the extreme case of which is white noise), yielding nearly all available time series as candidates.</p><p>By contrast, ParSketch is less sensitive to the energy distribution across Fourier coefficients. This can be explained by the different dimensionality reduction techniques used by the two methods: ParSketch is based on random projection, while DPiSAX uses piecewise aggregate approximation (PAA) as a first step to reduce dimensionality. If there are enough high frequency components, the time series will oscillate substantially within a piece, but the approximation (which is based on the mean within the piece) will not be sensitive to such oscillations.</p><p>This paper compares four parallel methods for solving nearest neighbor queries: (i) parallel linear search in which we compare the query time series directly with every time series in the database, (ii) Exact distributed iSAX (to be described below), (iii) Approximate distributed iSAX, (iv) and the random sketch approach ParSketch. Methods (ii, iii, and iv) all require indexes so perform best when there are many queries against an unchanging database.</p><p>The rest of this paper is organized as follows. In Section 2, we define the problem we address in the paper and present the related background. In Section 3 and Section 4, we describe the details of our parallel index construction and query processing algorithms. Section 5 introduces our approach to suggest an indexing method by analyzing the frequency characteristics of the time series. In Section 6, we present a detailed experimental evaluation for comparing our approaches. In Section 7, we discuss the related work. Finally, we conclude in 8.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Problem Definition and Background</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Time series and kNN query</head><p>A time series X is a sequence of values X = {x 1 , ..., x n }. We assume that every time series has a value at every timestamp t = 1, 2, ..., n. The length of X is denoted by |X|. Figure <ref type="figure">1a</ref> shows a time series of length 16, which will be used as running example throughout this paper. (Note that while time series are our main use case, any database and queries on sequences of numerical values can use the tools described in this paper.)</p><p>Given two time series (or vectors) of real numbers, X = {x 1 , ..., x n } and Y = {y 1 , ..., y m } such that n = m, the Euclidean distance between X and Y is defined as <ref type="bibr" target="#b12">[13]</ref>:</p><formula xml:id="formula_0">ED(X, Y ) = n i=1 (x i -y i ) 2 .</formula><p>The Euclidean distance is one of the main similarity measurement methods used in time series analysis. In this work, we assume that the distance between the time series is measured by using the Euclidean function.</p><p>The problem of similarity queries is one of the most important problems in time series analysis and mining. We construe "similarity" to mean finding the k nearest neighbors (k-NN) of a query. Definition 1 (Exact k nearest neighbors) Given a query time series Q and a set of time series D, let R = ExactkN N (Q, D) be the set of k nearest neighbors of Q from D. Let ED(X, Y ) be the Euclidean distance between the points X and Y , then the set R is defined as follows:</p><p>(</p><formula xml:id="formula_1">R ⊆ D) ∧ (|R| = k) ∧ (∀a ∈ R, ∀b ∈ (D -R), ED(a, Q) ≤ ED(b, Q))</formula><p>Definition 2 (Approximate k nearest neighbors) Given a set of time series D, a query time series Q, and &gt; 0. We say that</p><formula xml:id="formula_2">R = AppkN N (Q, D) is the approximate k nearest neighbors of Q from D, if ED(a, Q) ≤ (1 + )ED(b, Q),</formula><p>where a is the k th nearest neighbor from R and b is the true k th nearest neighbor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Parallel iSAX-based kNN Search</head><p>In this section, we first describe the iSAX representation <ref type="bibr" target="#b41">[42]</ref>, and then present DPiSAX, our parallel iSAX-based solution for kNN search over large time series datasets. Fig. <ref type="figure">1</ref>: A time series X is discretized by obtaining a PAA representation and then using predetermined break-points to map the PAA coefficients into SAX symbols. Here, the symbols are given in binary notation, where 00 is the first symbol, 01 is the second symbol, etc. The time series of Figure <ref type="figure">1a</ref> in the representation of Figure <ref type="figure">1d</ref> is [fourth, third, second, second] (which becomes <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr">01</ref>, 01] in binary). The representation of that time series in Figure <ref type="figure">1c</ref> becomes [1 2 , 1 2 , 01 4 , 0 2 ], where 1 ( 2) means that 1 is the selected symbol among 2 possible choices, 01 is the selected symbol among 4 possible choices, etc.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">iSax Representation</head><p>For very large time series databases, it is important to estimate the distance between two time series very quickly. There are several techniques, providing lower bounds by segmenting time series. One popular method, is called indexable Symbolic Aggregate approXimation (iSAX) representation <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b42">43]</ref>. The iSAX representation is used to represent time series in our index. The iSAX representation extends the SAX representation <ref type="bibr" target="#b25">[26]</ref>. This latter representation is based on the PAA representation <ref type="bibr" target="#b24">[25]</ref> which allows for dimensionality reduction while providing an important lower bounding property. The idea of PAA is to have a fixed segment size, and minimize dimensionality by using the mean values of each segment. Example 1 gives an illustration of PAA.</p><p>Example 1 Figure <ref type="figure">1b</ref> shows the PAA representation of X, the time series of Figure <ref type="figure">1a</ref>. The representation is composed of w = |X|/l values, where l is the segment size. For each segment, the set of values is replaced with their mean. The length of the final representation w is the number of segments (and, usually, w &lt;&lt; |X|).</p><p>The SAX representation takes as input the reduced time series obtained using PAA. It discretizes this representation into a predefined set of symbols, with a given cardinality, where a symbol is a binary number. The cardinality of a symbol is the number of possible distinct values it can take. Example 2 gives an illustration of the SAX representation.</p><p>Example 2 In Figure <ref type="figure">1c</ref>, we have converted the time series X to SAX representation with size 4, and cardinality 4 using the PAA representation shown in Figure <ref type="figure">1b</ref>. We denote SAX(X) = <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr">01,</ref><ref type="bibr">01]</ref>.</p><p>The iSAX representation uses a variable cardinality for each symbol of SAX representation, each symbol is accompanied by a number that denotes its cardinality. We defined the iSAX representation of time series X as iSAX(X) and we call it the iSAX word of the time series X. For example, the iSAX word shown in Figure <ref type="figure">1d</ref> can be written as iSAX(X) = [1 2 , 1 2 , 01 4 , 0 2 ].</p><p>The lower bounding approximation of the Euclidean distance for iSAX representation iSAX(X) = {x 1 , ..., x w } and iSAX(Y ) = {y 1 , ..., y w } of two time series X and Y is defined as <ref type="bibr" target="#b41">[42]</ref>:</p><formula xml:id="formula_3">M IN DIST (iSAX(X), iSAX(Y )) = √ n w w i=1 (dist(x i , y i )) 2</formula><p>, where the function dist(x i , y i ) is the distance between two iSAX symbols x i and y i . The lower bounding condition is formulated as:</p><formula xml:id="formula_4">M IN DIST (iSAX(X), iSAX(Y )) ≤ ED(X, Y )</formula><p>Using a variable cardinality allows the iSAX representation to be indexable. We can build a tree index as follows. Given a cardinality b, an iSAX word length w and leaf capacity th, we produce a set of b w children for the root node, insert the time series to their corresponding leaf, and gradually split the leaves by increasing the cardinality by one character if the number of time series in a leaf node rises above the given threshold th. The root node has 2 2 children while each child node forms a binary sub-tree. There are three types of nodes: root node, internal node (N2, N5, N6, N7) and terminal node or leaf node (N3, N4, N8, N9, N10, N11, N12, N13). Each leaf node links to a disk file that contains the corresponding time series (up to th time series).</p><p>Note that previous studies have shown that the iSAX index is robust with respect to the choice of parameters (word length, cardinality, leaf threshold) <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b50">51]</ref>. Moreover, it can also be used to answer queries with the Dynamic Time Warping (DTW) distance, through the use of the corresponding lower bounding envelope <ref type="bibr" target="#b21">[22]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">DPiSAX</head><p>Here, we describe DPiSAX (Distributed Partitioned iSAX) <ref type="bibr" target="#b47">[48]</ref>, our parallel solution for constructing a parallel iSAX-based index over large sets of time series by making the most of the parallel environment and carefully distributing the workload.</p><p>DPiSAX is based on a sampling phase that allows anticipating the distribution of time series among the computing nodes. Such anticipation is mandatory for efficient query processing, since it will allow, later on, to decide which partition contains the time series that actually correspond to the query. DPiSAX splits the full dataset for distribution into partitions using the partition table constructed at the sampling stage. Then each worker builds an independent iSAX index on its partition, with the iSAX representations having the highest possible cardinalities. Alongside an efficient node splitting policy, it allows to preserve index tree's load balance and to improve query performance, hence taking full advantage of time series indexing in distributed environments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Index construction</head><p>The index construction by DPiSAX proceeds in two steps.</p><p>-Partitioning. The first stage is done as follows. Given a desired number of partitions P and a time series dataset D, the algorithm takes a sample S of size L time series from D using stratified sampling, and emits its iSAX words SW s = {iSAX(ts i ), i = 1, ..., L}, initially assigned to a single partition. At each iteration, DPiSAX divides the sample by splitting the biggest partition into two sub-partitions, until the number of partitions reaches P , using the same splitting policy, as defined for iSAX tree index construction. As a result, the binary tree is converted to a partition table, where each leaf-node represents a partition, described with its iSAX word, and will become a root node of each sub-tree of the distributed index. -Parallel sub-index creation. On the next stage, the input dataset D, is mapped to iSAX representation DW s = {iSAX(ts i ), i = 1, ..., N } with the highest possible cardinalities of each symbol. Then, the database partitions are distributed among the available workers. Each worker builds locally its iSAX index sub-tree on the given partition of time series and stores it on HDFS in JSON format. Alongside, each leaf node (terminal node) is stored to HDFS as a file with corresponding time series ids and their iSAX representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Query processing</head><p>Given a collection of queries Q, in the form of time series, and the index constructed in the previous section for a database D, we consider the problem of finding time series in D that are similar to Q, according to the definitions of approximate k-NN and exact k-NN searches:</p><p>-Approximate search: Given a batch of queries Q, DPiSAX approximate search starts by obtaining the iSAX representations of all queries time series using the highest possible cardinalities. The master identifies the target partition for each query by checking the query's iSAX representation with the iSAX word of each entry in the partition table and sends the query to the worker in charge of the target partition. On each local index (sub-tree), the approximate search is done by traversing the local index to the terminal node that has the same iSAX representation as the query. The target terminal node contains at least one and at most th iSAX words, where th is the leaf threshold.</p><p>On the next stage of search a parallel computation of the Euclidean distance is performed in order to obtain the k nearest neighbors among all the candidates for a queries in a batch, retrieved from terminal nodes. -Exact search:</p><p>The exact search uses the approximate search result AKNN for a batch of queries Q as best-so-far k nearest neighbors. This result is broadcast among the workers in order to examine the index sub-trees that may contain the time series that are probably more similar to Q than those of AKNN. The lower bound distance M IN DIST is computed for each node on the sub-tree to determine nodes with probable closer candidates. If the lower bound of a node is already higher than the current best-so-far, the node and its sub-tree are excluded from the search. The k nearest neighbors are found by computing direct correlations on the candidates subset for each query in parallel.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Parallel Sketch-based kNN Search</head><p>This section presents ParSketch, our sketch-based parallel solution for indexing and similarity search over big time series datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">The Sketch Approach</head><p>The sketch approach, as developed by Kushilevitz et al. <ref type="bibr" target="#b23">[24]</ref>, Indyk et al. <ref type="bibr" target="#b16">[17]</ref>, and Achlioptas <ref type="bibr" target="#b0">[1]</ref>, provides a very nice guarantee: with high probability a random mapping taking b points in R m to points in (R d ) 2b+1 (the (2b+1)-fold crossproduct of R d with itself) approximately preserves distances (with higher fidelity the larger b is).</p><p>In our version of this idea, given a point (a time series or a window of a time series) t ∈ R m , we compute its dot product with N random vectors r i ∈ {1, -1} m . This results in N inner products called the sketch (or random projection) of t i . Specifically, sketch</p><formula xml:id="formula_5">(t i ) = (t i • r 1 , t i • r 2 , ..., t i • r N ).</formula><p>We compute sketches for t 1 , ..., t b using the same random vectors r 1 , ..., r N .</p><p>The theoretical underpinning for the utilization of sketches is given by the Johnson-Lindenstrauss lemma <ref type="bibr" target="#b19">[20]</ref>.</p><formula xml:id="formula_6">Lemma 1 Given a collection C of m time series, for any two time series -→ x , -→ y ∈ C, if &lt; 1/2 and n = 9logm 2 , then (1 -) ≤ -→ s ( -→ x ) --→ s ( -→ y ) 2 -→ x --→ y ≤ (1 + ) holds with probability 1/2, where -→ s ( -→ x ) is the sketch of -→ x of at least n dimensions.</formula><p>The Johnson-Lindenstrauss lemma implies that the distance sketch(t i )sketch(t j ) is a good appproximation of t i -t j provided the dimensionality of the sketches (r) is large enough. Specifically, if sketch(t i ) -sketch(t j ) &lt; sketch(t k ) -sketch(t m ) , then it's likely that t i -t j &lt; t k -t m , because the ratio between the sketch distance and the real distance is close to one.</p><p>A sketch of a time series t is a vector of dot products: element i of the sketch is the dot product between t and the ith random vector. Thus the full sketch contains as many dot products as there are random vectors.</p><p>The data structure consists of a set of grids. Each grid maintains the sketch values corresponding to the dot products between a specific set of random vectors and all time series. Let |g| be the number of random vectors assigned to each grid, and N be the total number of random vectors, then the total number of grids is b = N/|g|. (We make sure that |g| divides N .) The distance between two time series in different grids may differ. We consider two time series similar if they are similar in a given (large) fraction of grids.</p><p>Example 4 Let's consider two time series t 1 =(2, 2, 5, 2, 6, 5) and t 2 =(2, 1, 6, 5, 5, 6). Suppose that we have generated four random vectors as follows :</p><formula xml:id="formula_7">r 1 =(1, -1, 1, -1, 1, 1), r 2 =(1, 1, 1, -1, -1, 1), r 3 =(-1, 1, 1, 1, -1, 1) and r 4 =(1, 1, 1, -1, 1, 1)</formula><p>. Then the sketches of t 1 and t 2 , i.e. the inner products computed as described above, are Fig. <ref type="figure">3</ref>: Two series (s 1 and s 2 ) may be similar in some dimensions (here, illustrated by Grid 1 ) and dissimilar in other dimensions (Grid 2 ). The higher the similarity between t 1 and t 2 , the larger the fraction of grids in which the series are close.</p><p>respectively s 1 =(14, 6, 6, 18) and s 2 = <ref type="bibr" target="#b12">(13,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b14">15)</ref>. In this example, we create two grids, Grid 1 and Grid 2 , as depicted in figure <ref type="figure">3</ref>. Grid 1 is built according to the sketches calculated with respect to vectors r 1 and r 2 (where t 1 has sketch values 14 and 6 and t 2 has sketch values 13 and 5). In other words, Grid 1 captures the values of the sketches of t 1 and t 2 on the first two dimensions (vectors). Grid 2 is built according to vectors r 3 and r 4 (where t 1 has sketch values 6 and 18 and t 2 has sketch values 11 and 15). Thus, Grid 2 captures the values of the sketches on the last two dimensions. We observe that t 1 and t 2 are close to one another in Grid 1 . On the other hand, t 1 and t 2 are far apart in Grid 2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Partitioning Sketch Vectors</head><p>Multi-dimensional search structures don't work well for more than four dimensions in practice <ref type="bibr" target="#b39">[40]</ref>. For this reason, as indicated in Example 4, we adopt a first algorithmic framework that partitions each sketch vector into subvectors and builds grid structures for the subvectors as follows:</p><p>-Partition each sketch vector s of size N into groups of some size |g|.</p><p>-The ith group of each sketch vector s is placed in the ith grid structure (of dimension |g|). -If two sketch vectors s and s are within distance c × d in more than a given fraction f of the groups, then the corresponding time series are candidate highly correlated time series and should be checked exactly.</p><p>For example, if each sketch vector is of length N = 40, we might partition each one into ten groups of size |g| = 4. This would yield 10 grid structures, where time series items are assigned to grid cells, so that close items are grouped in the same grid cells. Suppose that the fraction f is 90%, then a time series t is considered as similar to a database time series t , if they are similar (assigned to the same cell) in at least nine grids.</p><p>Grid granularity can be adjusted to control the tradeoff between efficiency and accuracy. Coarser grids have larger grid cells (i.e. more time series assigned to the same cell), which leads to a larger number of candidates to process (slower execution), but lower probability to miss a true positive (higher accuracy). The grid granularity is defined through the parameter grid size that specifies the number of cells per grid dimension. At the cell assignment step, grids are divided into cells in a way that results in a uniform distribution of items across grid cells. This is supported by a sampling phase that infers the distribution and defines the cell borders along each dimension of each grid.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">ParSketch</head><p>Our ParSketch solution takes full advantage of parallel data processing, both while constructing indexes and querying them for time series similarity search.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Index construction</head><p>Time-series index construction on the input dataset D within distributed data processing frameworks proceeds as follows:</p><p>1) A random transformation matrix R (composed of N random vectors) is generated, where each element r i,j ∈ R is a random value in {1, -1}. This matrix will be used to compute the sketch (of size N ) of each time series t ∈ D, by computing the dot product of t with R. Once generated, R is broadcast to all worker nodes.</p><p>2) ParSketch strives to place an approximately equal number of time series within each cell of each grid. The borders of these variable sized cells are defined by a breakpoint table. This is done by sampling: ParSketch takes a sample S of size L time series from the input dataset D. Based on random projection of S with the random transformation matrix R, ParSketch defines the cell breakpoints B G for each dimension at each grid G, considering the parameter grid size and the distribution of values at each of the N sketch dimensions. The resulting breakpoints table is also broadcast to worker nodes.</p><p>3) At the sketch computation stage the dot product of time series t, where t ∈ D, with the random transformation matrix R results in a vector of much lower dimension: s j = t j × R. The input dataset D is partitioned horizontally, so that each time series t ∈ D is entirely handled at the same worker node. Then, at each node, sketch vectors over D are built locally and then split into equal subvectors of given size. Each subvector corresponds to a grid. Thus, each sketch is assigned to a grid cell in each of the grids G using the breakpoints B G , defined at the sampling stage.</p><p>4) We use a cluster of relational database instances, previously created and distributed at the nodes, to persist the indexed time series. One database instance stores the indexing of one partition of D into a relation with the structure (grid id, cell id, t id), i.e., the cell assignment at each grid of a given time series t with identifier t id. Thus, the entire contents of a particular cell in a particular grid is spread across index nodes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Query processing</head><p>Given a collection of queries Q, in the form of distributed time series dataset, and a previously constructed index on a dataset D, we consider the problem of finding time series that are similar to Q in D. We perform such a search in the following steps: 1) Similarly to the grid construction, at each node, sketches over Q are computed in parallel, using the same random matrix R. Sketches are then split into subvectors in parallel and assigned to grid cells at each grid G, using the same breakpoints sequences B G . The resulting cell assignments of all queries are then broadcast to worker nodes.</p><p>2) Each node checks the cell assignments of all queries against its partition of the index. Then, the full list of candidates for each query is retrieved in parallel by all worker nodes from their database instances. If a subvector of a query time series q lands in the same grid cell as a database time series t, then t is a possible match for q. Two time series (a query and a database one whose sketch is stored in the grids) are considered to be similar if they are assigned to the same grid cell in a large user-tunable fraction of grids. The candidates that meet these conditions are filtered in a SELECT statement to each database, thus efficiently reducing the data to be processed at early stages of the search. For instance, the entire filtering of candidates for a given query q is pushed down to the databases and done through the parallel execution of the aforementioned SELECT statement, which selects all time series t i collocated with q in grid cells, then groups by t i and counts, in order to finally emit those t i that are collocated with q in at least the desired fraction f of grids.</p><p>3) Because sketching is approximate, each candidate match between a query q and data vector t is checked by performing direct similarity computation. The kNN results of a query q are the k candidates that have the highest similarity to it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Selecting the Best Indexing Approach based on Frequency Coefficients</head><p>To choose the best indexing and querying approach between DPiSAX and ParSketch for a given dataset, we have developed a tool called BestNeighbor. It makes its decision based on the Fourier coefficients derived from the discrete Fourier Transform of the data. We start from the observation that the absolute value of the i th coefficient of the discrete Fourier transform of a time series represents the amplitude of a wave signal component whose full cycle repeats i times through the entire series (we also refer to this signal as the i th frequency component). Intuitively (and in fact), this relates to how the series should be divided into PAA segments for iSAX. In particular, having fewer than i segments would fail to capture useful piecewise summarization of the i th frequency component, because one PAA segment would correspond to more than one full cycle of the component. However, there is a cost to having too many segments because the iSAX data structure might then grow significantly.</p><p>Experiments back this up. DPiSAX performs extremely well in terms of both quality and response time when the data resembles a random walk, even when segment sizes are large, with most of the energy in the low Fourier coefficients. One might call that the iSAX-friendly regime. As the energy spreads to higher Fourier coefficients, iSAX pruning becomes less effective for large segment sizes, leading to a higher time burden for exact iSAX and lower precision for approximate iSAX. In those regimes, the sketch approach offers quite high precision (over 90% across both simulated and real data sets) and a time performance far below that of exact iSAX. That would be the iSAX-unfriendly regime.</p><p>For a given number of segments, the frequency composition of a dataset has a big impact on performance. For example, Figure <ref type="figure" target="#fig_3">4</ref> shows two time series of stock prices whose spectrum concentrates energy in the first 4 frequency components. The PAA of the 8 segments take values from a quite wide range, which leads to a good variety of iSAX representations of time series with similar spectral characteristics. By contrast, the two seismic series on Figure <ref type="figure" target="#fig_4">5</ref> are characterized by higher frequencies, up to 53. One can easily notice that the mean of each segment tends to be close to 0 (assuming z-normalization), which significantly restricts the possible iSAX grammar for that class of time series. This inevitably leads to poor pruning of candidates, as the iSAX representations of any two (even very distant) time series tend to show significant similarity.</p><p>To analyze the frequency spectrum of a time series dataset D for iSAXfriendliness, we use the following strategy:</p><p>1. Take a sample S of the input dataset D. 2. For each series t in S, apply the Fast Fourier Transform (FFT). The energy distribution of t is a vector E(t) of energy components, where each element E i (t) is the square of the absolute value of the i th Fourier coefficient, i.e. proportional to the energy of the i th frequency component. 3. The averages of each energy component over all series constitute the mean energy distribution E(S) for the entire sample S, which we consider a good enough approximation of the energy distribution E(D) of the input dataset D. 4. Sort E(D) in descending order and take the highest energy components HEC(D) that hold p% of the total energy. The parameter p% is configurable by the user. By default, it is set to 80%. The least and greatest frequency indexes in HEC(D), l D and g D respectively, indicate the range of Fourier coefficients that concentrates the majority of the energy.</p><p>Low values of g D , meaning that high frequencies do not have significant energy impact, are generally favorable for iSAX. By contrast, when g D is greater than the number of segments, the problem depicted at Figure <ref type="figure" target="#fig_4">5</ref> occurs. In such cases, increasing the number of segments can help increase the diversity of iSAX symbols, hence improve the pruning to some reasonable values.</p><p>Our experiments (detailed in the next section) on different synthetic datasets with diverse bandwidths (g D ), varying the number of iSAX segments, show that the pruning ratio (the fraction of the dataset filtered out by approximate iSAX to ease the exact search) stays close to zero until the number of segments reaches some minseg(g D ), where it seems to take off and start making iSAX exact search useful. To approximate minseg(g D ), we made a simple regression analysis of the results from our measurements, arriving at the following formula (detailed explanations presented in Section 6.3):</p><formula xml:id="formula_8">minseg(g D ) = (7g D -20)<label>4</label></formula><p>For a reasonable upper bound on the number of segments, we assessed the pruning ratio, setting the number of segments to 2g D , inspired by the Nyquist sampling rate, which by definition is twice the highest frequency of a signal (and, as Nyquist showed, is high enough to reconstruct the signal). This generally leads to a very good pruning, hence fast exact search. However, for higher values of g D , this entails slow indexing and requires more space for storing the index, since the size of the index structure grows with the number of segments. In such cases, the user may choose a number of segments between minseg(g D ) and 2g D , to optimize the tradeoff between indexing time and exact search time.</p><p>BestNeighbor does a Fourier analysis of any dataset to determine where most of the power is. If there is substantial power at least up to the 30 th coefficient, then the dataset is in an "iSAX-unfriendly" regime, so using ParSketch instead would be recommended. Otherwise, the tool outputs the recommended range of values for the number of segments that may lead to good iSAX indexing and exact search times.</p><p>The BestNeighbor tool finds the highest energy components that concentrate the bulk of the energy. In the current version of BestNeighbor and based on our experiments, we set the parameter p% to 80%, i.e., to choose the components that hold 80% of the total energy. This works well in our experiments. However, if this default value is not appropriate for the user's dataset, he/she can change the parameter accordingly. Testing on samples of the data may help set this parameter, though we suspect the default will work well.</p><p>The time complexity for the analysis of frequency spectrum is O(a * b log b), where a -size of input sample dataset, b -length of 1 time series from dataset. To determine the type of dataset in terms of frequency composition and to get recommendations on which indexing algorithm to use, the BestNeighbor tool requires relatively small input. In our experiments we used dataset samples of size up to 1000 time series (a ≤ 1000), 200-256 points each (b ≤ 256). Hence dataset analysis time complexity will be insignificant in the full cycle of indexing large time series data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Performance Evaluation</head><p>In this section, we report experimental results for comparing the quality and the performance of DPiSAX and ParSketch for indexing different time series datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Setup</head><p>Our experiments were conducted on a cluster<ref type="foot" target="#foot_0">1</ref> of 16 compute nodes each having two 8 core Intel Xeon E5-2630 v3 CPUs, 128 GB RAM, 2x558GB capacity storage per node. The cluster is running under Hadoop version 2.7, Spark v. 2.4 and PostgreSQL v. 9.4 as a relational database system.</p><p>Compared approaches. We evaluate the performance of four kNN search approaches: 1) ParSketch; 2) Exact version of DpiSAx; 3) Approximate version of DPiSAX; 4) Parallel Linear Search (PLS), which is a parallel version of the UCR Suite fast sequential search (with all applicable optimizations in our context: no computation of square root, and early abandoning) <ref type="bibr" target="#b33">[34]</ref>.</p><p>We performed our performance evaluation using different synthetic and real datasets.</p><p>Synthetic datasets. For the purpose of experimentation, we generated several synthetic data sets:</p><p>1. Random Walk input dataset: whose sizes/volumes vary between 50 million and 500 million time series each of size 256 time points. At each time point, a random walk generator cumulatively adds to the value of the previous time point a random number drawn from a Gaussian distribution N(0,1). 2. Random dataset: contains 200 million time series each again of length 256, representing "white noise". Each point is randomly drawn from a Gaussian distribution N(0,1). 3. Fourier slice data of the form F x,y . This is done by first generating the frequency spectra of the time series and then applying an inverse Fourier transform plus some additional white noise to generate the series themselves. The spectrum generation assigns high amplitudes to x subsequent frequency components, starting from the y th one, and low ones for the rest. Randomness is applied to the phase shifts of all frequencies, so that the generated set of series can enjoy a good variety. In our notation, when y is not explicitly specified, y = 0 is assumed.</p><p>Real datasets. In our experiments, we used several real datasets representing various domains such as seismology, finance, astronomy, neuroscience and image processing.</p><p>1. The seismic dataset, Seismic, was obtained from the IRIS Seismic Data Access archive <ref type="bibr" target="#b18">[19]</ref>. It contains seismic instrument recording from thousands of stations worldwide and consists of 40 million data series of 200 values each. 2. The two finance datasets, StockP and StockR, are based on historical finance data downloaded using the Yahoo Finance API<ref type="foot" target="#foot_1">2</ref> that contains end-of-day quotes for over 40000 stock symbols for the period from Jan 2010 to Mar 2018. After preprocessing, the StockP dataset contains 72 million series of 200 end-of-day quotes. Each series represents a sub-period of 200 consecutive days for a particular stock symbol. The StockR dataset contains the price returns (fractional change of price from one day to another) of the quotes from StockP. 3. The astronomy dataset, Astro, represents celestial objects and was obtained from <ref type="bibr" target="#b44">[45]</ref>. The dataset consists of 104 million data series of size 256. 4. The neuroscience dataset, SALD, obtained from <ref type="bibr" target="#b37">[38]</ref> represents MRI data, including 209 million data series of size 128. 5. The image processing dataset, Deep1B, retrieved from <ref type="bibr" target="#b35">[36]</ref>, contains 279 million Deep1B vectors of size 96 extracted from the last layers of a convolutional neural network.</p><p>Measures. For each dataset and for each method (approximate iSAX, exact iSAX, and ParSketch), we measure the pruning ratio (when available), the precision, and the time in seconds. Pruning is the fraction of the database that are discarded by the iSAX index. Precision is defined to be correlation of the k th time series found by a particular method divided by the k th closest time series found by direct computation of correlation.</p><p>Queries. We evaluated the performance of the tested methods with three types of query workloads, batches of 10, 100 and 1000 queries. In our experiments, the default number of queries is 100.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Results</head><p>Using different synthetic datasets, Figures <ref type="figure">6</ref> and<ref type="figure">7</ref> show the query performance results<ref type="foot" target="#foot_2">3</ref> for different approaches while setting k to 1 and 10 respectively. In both figures, we see the query response time and precision of ParSketch, and exact/approximate versions of DPiSAX. We can see, for instance, that on the Random Walk dataset, DPiSAX approximate search is fast and takes 40 seconds (yellow bar) for a precision of 98% (green bar), while exact gives 100% precision (as expected for exact search) and is nearly as fast (52 seconds) as the pruning (green bar) is high -98%. The query takes 78 seconds with parSketch, where the precision is 99.99%. These numbers may be compared, for instance, with those reported for the Random F 5,4 dataset, where approximate takes 75 seconds for a precision of 94%, but exact search with zero pruning increases in time significantly, to 1179 seconds. For this dataset, parSketch returns result in 117 second for a precision of 99.85%. Figure <ref type="figure">8</ref> shows the performance results using real datasets. As shown, DPiSAX performance varies across the datasets. The stock price dataset is like a random walk with most of the energy of its time series in the low Fourier coefficients. For that dataset, approximate DPiSAX is fast (52 seconds, orange bar) and quite accurate (94%, blue bar) and exact DPiSAX is almost as fast (126 seconds) and perfectly accurate. This is not true for the other data sets.</p><p>We studied the effect of Fourier coefficients on the quality of the kNN results by DPiSAX and ParSketch, in particular for k = 1 and k = 10. Table <ref type="table">1</ref> shows the results sorted in ascending order by g D that indicates the greatest of the Fourier coefficients that concentrate the majority of the time series energy. These results were measured through a setup with 8 PAA segments for the iSAX words. As seen, low values of g D indicate that the data set will work well on iSAX (iSAXfriendliness), because it returns high quality results with lower response time than that of ParSketch.</p><p>As the number of high energy components increases, the iSAX pruning ratio significantly decreases. The reason is that in the low frequency case, different time series will acquire different PAA symbols. High frequencies within a PAA segment are not captured within the mean calculation of the PAA calculation, so very different time series will acquire the same PAA symbol. This reduces the efficiency of the index.</p><p>Furthermore, we studied the effect of varying the iSAX word length w (number of segments) on the quality of iSAX results, on various synthetic datasets with different frequency characteristics. The results are presented in Table <ref type="table">2</ref>, where, for each particular dataset D and its highest important Fourier coefficient g D , Table <ref type="table">1</ref>: Frequency analysis of all datasets (real and synthetic). The range [l D ..g D ] specifies the slice of Fourier coefficients that collectively account for at least 80% of the energy. iSAX pruning and quality of iSAX and sketches are displayed for both top-1 and top-10 nearest neighbors queries. The table is sorted by g D , low values of which are favorable for efficient iSAX pruning and therefore the quality of approximate iSAX, which is observed in both top-1 and top-10 cases. Sketch quality is only slightly sensitive to the frequency characterization. the rows corresponding to w = minseg(g D ) and w = 2g D are highlighted. The findings show that pruning is ineffective when w &lt; minseg(g D ). Increasing w helps improve the quality, which becomes particularly high when w reaches 2g D . However, raising w to 60 or more makes the indexing very slow, even slower than linear search for 100 queries. For this reason, we consider a dataset D to be in an "iSAX-unfriendly" regime, if its highest frequency g D is greater than 30.</p><p>The first rows of the table show that iSAX pruning can be good even with very few segments (as few as 2). However, too few segments may lead to partitioning limitations that result in very few partitions, hence underexploiting the parallelization capabilities of the index. This explains the large indexing times for these cases. So, for efficient indexing, a minimum of 8 segments is recommended.</p><p>These results confirm our claims in Section 5, that:</p><p>1. low values of g D (meaning that high frequencies do not have significant impact) are favorable for iSAX; 2. pruning is reasonable when the number of iSAX segments w is at least minseg(g D ); 3. setting w to 2g D leads to effective pruning; 4. iSAX exact search gets very expensive when g D &gt; 30; in such cases, ParSketch is recommended as a better approximate solution.</p><p>Readers interested in seeing more comparison results over different datasets are invited to visit the following web page: http://imitates.gforge.inria.fr/. In addition to performance results, the visitors can visually see randomly chosen Table <ref type="table">2</ref>: iSAX PAA segments parametrization. We vary the number of segments w for certain datasets and study the tradeoffs of DPiSAX performance in terms of quality and time of index construction and query answering. The rows corresponding to w = minseg(g D ) and w = 2g D are highlighted. Based on these results, the tool BestNeighbor (described in section 5) does an analysis of any given dataset to determine if it is in an "iSAX-friendly" regime. If so, the tool outputs a recommended range of values for the number of segments for a given dataset, which corresponds to the highlighted rows in Table <ref type="table">2</ref> (and could be used directly to tune DPiSAX parameter --wordLength). That may lead to efficient iSAX indexing and search time. If the dataset is determined to be "iSAX-unfriendly" -the tool recommends using ParSketch. time series queries (up to 1000 for each dataset) and their k nearest neighbor time series.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Defining minseg(g D )</head><p>As introduced in Section 5 and later validated through the experiments, for low values of w, the pruning ratio stays equal or close to zero (i.e., no useful pruning from the iSAX data structure). The result is that exact iSAX search resolves to a linear search (comparing the query time series to each item in the database) with the added overhead of pointlessly traversing the entire iSAX index tree to attempt pruning. For this reason, we consider pruning to be "reasonable" if it filters out at least a small fraction of the database. At some point as we increase w the pruning ratio takes off.</p><p>The minseg(g D ) formula helps determine the minimal number of segments (word length) w for iSAX to achieve a reasonable pruning. To define the formula, we first utilized an empirical approach to build a training set for a regression model to approximate the pruning ratio as a function of g D (the greatest high energy frequency of a dataset D) and w. Out of our collection of synthetic datasets, each characterized by its g D , we took the measured pruning ratio for various w among the values {2, 4, 8, 16, 32, 48, 60}.</p><p>For each particular dataset (and g D ), we have selected a representative range of w values that lead to a range of pruning ratios, i.e., keeping at most one training example with zero pruning ratio and at most one with high pruning (≥ 0.95). The training set is in fact a subset of Table <ref type="table">2</ref>, where the 24 training examples are marked with ( * ) in the "pruning ratio" column.</p><p>Then, we explored different setups for linear regression using the pruning ratio as target and combinations of g D , w, 1 g D , 1 w , g D w , w g D as features. The most significant regression coefficients were attributed to the features 1 w and g D w . The linear regression on those features showed the satisfactory R 2 score of 0.85, leading to the following approximation of pruning ratio P approx (coefficients rounded to 1 decimal place for simplicity):</p><formula xml:id="formula_9">P approx = 3.4 1 w -1.2 g D w + 1.2</formula><p>Further, we aim at finding a threshold C, such that P approx ≥ C holds for all training examples, for which the measured pruning ratio is "reasonable", and does not hold for those where the pruning ratio is "close to zero". We determined that when C = 0.5, this condition is satisfied. Solving the inequality P approx ≥ 0.5 leads to w ≥ minseg(g D ), where:</p><formula xml:id="formula_10">minseg(g D ) = 1.75g D -5</formula><p>After defining minseg(g D ) as above (note that 2.0g D would be the Nyquist rate), we measured the iSAX pruning for all datasets, setting the number of segments to minseg(g D ) and 2.0g D , as shown in Table <ref type="table">2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Related Work</head><p>In the context of time series data mining, several techniques have been developed and applied to time series data, e.g., clustering, classification, outlier detection, pattern identification, motif discovery, and others. The idea of indexing time series is relevant to all these techniques. Note that, even though several databases have been developed for the management of time series (such as Informix Time Series, InfluxDB, OpenTSDB, and DalmatinerDB based on RIAK), they do not include similarity search indexes, focusing instead on (temporal) SQL-like query workloads. Thus, they cannot efficiently support similarity search queries, which is the focus of our study.</p><p>Indexes often make the response time of lookup operations sublinear in the database size. Relational systems have mostly been supported by hash structures, B-trees, and multidimensional structures such as R-trees, with bit vectors playing a supporting role. Such structures work well for lookups, but only adequately for similarity queries. The problem of indexing time series using centralized solutions has been widely studied in the literature, e.g., <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b6">7]</ref>. For instance, in <ref type="bibr" target="#b2">[3]</ref>, Assent et al. propose the TS-tree (time series tree), an index structure for efficient retrieval and similarity search over time series. The TS-tree provides compact summaries of subtrees, thus reducing the search space very effectively. To ensure high fanout, which in turn results in small and efficient trees, index entries are quantized and dimensionally reduced.</p><p>In <ref type="bibr" target="#b3">[4]</ref>, Cai et al. use Chebyshev polynomials as a basis for dealing with the problem of approximating and indexing d-dimensional trajectories and time series. They show that the Euclidean distance between two d-dimensional trajectories is lower bounded by the weighted Euclidean distance between the two vectors of Chebyshev coefficients, and use this fact to create their index.</p><p>In <ref type="bibr" target="#b13">[14]</ref>, Faloutsos et al. use R*-trees to locate multi dimensional sequences in a collection of time series. The idea is to map a large time series sequence into a set of multi-dimensional rectangles, and then index the rectangles using an R*-tree. Our work is able to use the simplest possible multi-dimensional structure, the grid structure, because our problem is simpler as we will see.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">iSAX-based Indexes</head><p>In <ref type="bibr" target="#b43">[44]</ref>, Shieh and Keogh propose a multiresolution symbolic representation called indexable Symbolic Aggregate approXimation (iSAX) which is based on the SAX representation. The advantage of iSAX over SAX is that it allows the comparison of words with different cardinalities, and even different cardinalities within a single word. iSAX can be used to create efficient indices over very large databases. Several works have then built upon the iSAX representation, including iSAX2+ <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6]</ref>, Adaptive Data Series Index (ADS+) <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b50">51]</ref>, Compact and Contiguous Sequence Infrastructure (Coconut) <ref type="bibr" target="#b22">[23]</ref>, Parallel Index for Sequences (ParIS) <ref type="bibr" target="#b32">[33]</ref>, and Ultra Compact Index for Variable-Length Similarity Search (ULISSE) <ref type="bibr" target="#b27">[28]</ref>. A recent study is comparing the performance of several different time series indexes <ref type="bibr" target="#b9">[10]</ref>.</p><p>The iSAX2+ index <ref type="bibr" target="#b5">[6]</ref> was specifically designed for very large collections of time series, proposing new mechanisms and corresponding algorithms for efficient bulk loading and node splitting. The authors described algorithms for efficient handling of the raw time series data during the bulk loading process, by using a technique that uses main memory buffers to group and route similar time series together down the tree, performing the insertion in a lazy manner. In <ref type="bibr" target="#b49">[50]</ref>, instead of building the complete iSAX2+ index over the complete dataset and querying only later, Zoumpatianos et al. propose to adaptively build parts of the index, only for the parts of the data on which the users issue queries. Coconut <ref type="bibr" target="#b22">[23]</ref> proposed a compact and contiguous data layout that is based on sortable iSAX representations, while ULISSE <ref type="bibr" target="#b27">[28]</ref> focused on the problem of supporting similarity search queries on sequences of variable-length. All these indexes have been developed for a centralized environment, and cannot scale up to very high volumes of time series. The ParIS index <ref type="bibr" target="#b32">[33]</ref> was recently proposed for taking advantage of the modern hardware parallelization opportunities within a single compute node. ParIS describes techniques that use the Single Instruction Multiple Data (SIMD) instructions, as well as the multi-core and multi-socket architectures, for parallel index creation and query answering. As such, ParIS is complementary to our DPiSAX approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Sketch-based Solutions</head><p>Our sketch-based method takes advantage of random vectors. The basic idea is to multiply each time series (or in a sliding window context, each window of a time series) with a set of random vectors. The result of that operation is a sketch for each time series consisting of the distance (or similarity) of the time series to each random vector. Then two time series can be compared by comparing sketches.</p><p>Note that the sketch approach we advocate is a kind of Locality Sensitive Hashing <ref type="bibr" target="#b14">[15]</ref>, by which similar items are hashed to the same buckets with high probability. In particular, the sketch approach is similar in spirit to SimHash <ref type="bibr" target="#b7">[8]</ref>, in which the vectors of data items are hashed based on their angles with random vectors.</p><p>Random projection has been widely used in the literature for dimensionality reduction. For example, in <ref type="bibr" target="#b38">[39]</ref>, Schneider et al. propose scalable density-based clustering algorithms using random projections. Their clustering algorithms achieve significant speedup compared to equivalent density-based techniques, with good clustering quality in Euclidean space. Wilkinson et al. <ref type="bibr" target="#b46">[47]</ref> introduce a classifier designed to address the curse of dimensionality and exponential complexity by using random projections. Their classifier organizes a set of random projections into a decision list used for scoring new data points.</p><p>In <ref type="bibr" target="#b17">[18]</ref>, random projection is used for discovering representative trends over time series, e.g., finding average trend that is the subsequence whose total distance to all other subsequences is the smallest. The authors propose approximate algorithms that work on a pool of sketches made from the original time series. In <ref type="bibr" target="#b8">[9]</ref>, Dasgupta takes advantage of random projections for learning high-dimensional Gaussian mixture models. He proposes an efficient algorithm that returns with high probability the true centers of the Gaussians, given a precision threshold specified by the user.</p><p>In ParSketch, we use random projection for creating an efficient index over large time series datasets. We parallelize the sketch approach both at index creation time and at query processing time. Experiments show excellent and nearly linear gains in performance.</p><p>On the whole, the main limitation of the existing solutions in the literature is that they do not scale to very big databases containing billions of time series. For example, with the state of the art iSax-based solution <ref type="bibr" target="#b4">[5]</ref>, indexing a database of one billion time series takes several days, while our parallel solutions allow us to index it in some hours.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusions</head><p>We have studied the performance and quality characteristics of the two state-ofthe-art distributed methods for indexing massive time series databases, DPiSAX and ParSketch. DPiSAX excels when most of the energy in a database of time series lies in the low order Fourier coefficients. Otherwise, the DPiSAX pruning ratio suffers and therefore both accuracy of approximate DPiSAX and time for exact DPiSAX. In that case, ParSketch exhibits much better accuracy, thanks to its insensitivity to energy distribution across Fourier coefficients. These results hold across simulated and real datasets.</p><p>We have introduced a utility that estimates DPiSAX-friendliness based on a Fourier analysis of a sample of a given time series database to suggest an indexing method to use.</p><p>For purposes of reproducibility, our datasets and code (including for the random generators) are available here: http://imitates.gforge.inria.fr/code. html. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>iSAX representation of X, with 4 segments and different cardinalities<ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b11">12,</ref> 014, 02].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :Example 3</head><label>23</label><figDesc>Fig. 2: Example of iSAX Index</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 :</head><label>4</label><figDesc>Fig. 4: Two normalized time series of finance stock prices (and their PAA representations) with substantial power at lower frequency components. The iSAX symbols corresponding to these PAA values can distinguish different time series from one another.</figDesc><graphic coords="12,57.78,89.45,357.12,133.92" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 :</head><label>5</label><figDesc>Fig.5: Two quite different time series (normalized) from the seismic dataset (and their PAA representations) with substantial power at higher frequency components. The PAA values are all close to 0, masking the differences.</figDesc><graphic coords="13,57.78,89.45,357.12,133.92" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Themis</head><label></label><figDesc>Palpanas is Senior Member of the French University Institute (IUF), director of the Data Intelligence Institute of Paris (diiP), and Professor of computer science at the University of Paris. He is the author of 9 US patents (3 implemented in world-leading commercial data management products) and 2 French patents. He is the recipient of 3 Best Paper awards, and the IBM Shared University Research (SUR) Award. He is serving as Editor in Chief for BDR Journal, Associate Editor for PVLDB 2019 and TKDE journal, and Editorial Advisory Board member for IS journal. Dennis E. Shasha is a Professor of Computer Science at the Courant Institute of New York University. His research interests include data science, pattern recognition, database tuning, biological computing, wireless communication, and concurrent verification. Shasha is an ACM Fellow and the recipient of an Inria Internaional Chair. He is co-editor in chief of Information Systems, and is or has been the puzzle columnist for CACM, Dr. Dobb's Journal, and Scientific American. Patrick Valduriez is a senior researcher at Inria, working on distributed data management. He has been associate editor of major journals such as VLDBJ and DAPD and has served as PC chair or general chair of major conferences such as EDBT, SIGMOD and VLDB. He obtained the best paper award at VLDB 2000. He was the recipient of the 1993 IBM scientific prize in Computer Science in France and the 2014 Innovation Award from Inria -French Academy of Science. He is an ACM Fellow.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Synthetic datasets kNN=1 comparison. For the top-1 nearest neighbor query, the relative quality and time performance depends strongly on the dataset. When the lowest 5 Fourier coefficients contain most of the energy of the time series iSAX pruning works very well so both approximate iSAX gives high precision and exact iSAX is very fast. As the energy spreads to higher Fourier coefficients, iSAX pruning deteriorates with the result that approximate iSAX loses in precision and exact iSAX increases in time. Sketch time and precision stay about the same for all frequency distributions, though also degrades slightly as higher Fourier coefficients acquire more energy. Synthetic datasets kNN=10 comparison. For the top-10 nearest neighbor query, the results are qualitatively similar to those for top-1 queries: iSAX works well when most of the power is in the lower Fourier coefficients, but less well when energy spreads to higher Fourier coefficients. Sketch time and precision vary much less. Comparison of Real Datasets: Stock Prices datasets (a random walk distribution model) have most of their power in the first few coefficients. The other datasets have power in multiple coefficients. For those cases, iSAX pruning works less well and so the quality of approximate iSAX suffers and the time of exact iSAX increases. ParSketch is slower than exact iSAX in the Random Walk dataset but is otherwise faster.</figDesc><table><row><cell></cell><cell>Pruning Precision top10 Precision top1</cell><cell>Query response time top1 (sec) Query response time (sec)</cell><cell></cell></row><row><cell></cell><cell>100% 100%</cell><cell>1400 3000</cell><cell></cell></row><row><cell></cell><cell>80% 80% 90% 90%</cell><cell>2500 1200</cell><cell></cell></row><row><cell>Pruning / Precision, % Precision, %</cell><cell>30% 40% 50% 60% 70% 30% 40% 50% 60% 70%</cell><cell>400 600 800 1000 1000 1500 2000</cell><cell>Execution Time, sec Execution Time, sec</cell></row><row><cell></cell><cell>10% 10% 20% 20%</cell><cell>200 500</cell><cell></cell></row><row><cell cols="3">0 Fig. 6: 0 0% Approx Exact Sketch Approx Exact Sketch Approx Exact Sketch Approx Exact Sketch Approx Exact Sketch Approx Exact Sketch Random Walk Random F 5,0 Random F 5,2 Random F 5,4 Random F 5,8 Random F 50 200 400 600 800 1000 1200 1400 0% 10% 20% 30% 40% 50% 60% 70% 80% 90% 100% Approx Exact Sketch Approx Exact Sketch Approx Exact Sketch Approx Exact Sketch Approx Exact Sketch Approx Exact Sketch Random Walk Random F 5,0 Random F 5,2 Random F 5,4 Random F 5,8 Random F 50 Pruning / Precision, % Pruning Precision top10 Query response time top10 (sec) 0% Approx Exact Sketch Approx Exact Sketch Approx Exact Sketch Approx Exact Sketch Approx Exact Sketch Approx Exact Sketch Stock P (57Gb) Stock R (57Gb) Seismic (32Gb) Astro (100 Gb) SALD (100Gb) Deep1B (100Gb) Fig. 7: 0 Fig. 8:</cell><cell>Execution Time, sec</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Pruning ratio measures for particular combinations of g D and w that were used as training examples for the regression model to define minseg(g D ).</figDesc><table><row><cell>dataset D</cell><cell>greatest</cell><cell>high energy</cell><cell>freq. g D</cell><cell>number of</cell><cell>segments w</cell><cell>index</cell><cell>construction</cell><cell>time (s)</cell><cell>pruning</cell><cell>top 10</cell><cell>quality</cell><cell>top 10</cell><cell>approximate</cell><cell>search</cell><cell>time (s)</cell><cell>exact</cell><cell>search</cell><cell>time (s)</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">6900</cell><cell cols="2">0.611 ( * )</cell><cell cols="2">0.982</cell><cell></cell><cell cols="2">51</cell><cell cols="3">808</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>4</cell><cell></cell><cell cols="3">747</cell><cell cols="2">0.851 ( * )</cell><cell cols="2">0.941</cell><cell></cell><cell cols="2">45</cell><cell></cell><cell cols="2">254</cell></row><row><cell>Random Walk</cell><cell></cell><cell>4</cell><cell></cell><cell cols="2">8 16</cell><cell cols="3">751 850</cell><cell cols="2">0.961 ( * ) 0.988</cell><cell cols="2">0.945 0.943</cell><cell></cell><cell cols="2">31 31</cell><cell cols="3">102 69</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">32</cell><cell cols="3">1117</cell><cell cols="2">0.994</cell><cell cols="2">0.927</cell><cell></cell><cell cols="2">28</cell><cell></cell><cell cols="2">30</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">48</cell><cell cols="3">1387</cell><cell cols="2">0.996</cell><cell cols="2">0.935</cell><cell></cell><cell cols="2">32</cell><cell></cell><cell cols="2">26</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">60</cell><cell cols="3">1548</cell><cell cols="2">0.996</cell><cell cols="2">0.930</cell><cell></cell><cell cols="2">28</cell><cell></cell><cell cols="2">26</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">4</cell><cell cols="3">649</cell><cell cols="2">0.351 ( * )</cell><cell cols="2">0.905</cell><cell cols="3">104</cell><cell cols="3">986</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>8</cell><cell></cell><cell cols="3">541</cell><cell cols="2">0.910 ( * )</cell><cell cols="2">0.952</cell><cell></cell><cell cols="2">23</cell><cell></cell><cell cols="2">151</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">10</cell><cell cols="3">538</cell><cell cols="2">0.969</cell><cell cols="2">0.962</cell><cell></cell><cell cols="2">21</cell><cell></cell><cell cols="2">84</cell></row><row><cell>Random F5.0</cell><cell></cell><cell>5</cell><cell></cell><cell cols="2">16</cell><cell cols="3">640</cell><cell cols="2">0.987 ( * )</cell><cell cols="2">0.961</cell><cell></cell><cell cols="2">32</cell><cell></cell><cell cols="2">66</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">32</cell><cell cols="3">890</cell><cell cols="2">0.993</cell><cell cols="2">0.965</cell><cell></cell><cell cols="2">37</cell><cell></cell><cell cols="2">67</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">48</cell><cell cols="3">1157</cell><cell cols="2">0.994</cell><cell cols="2">0.962</cell><cell></cell><cell cols="2">37</cell><cell></cell><cell cols="2">65</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">60</cell><cell cols="3">1320</cell><cell cols="2">0.995</cell><cell cols="2">0.965</cell><cell></cell><cell cols="2">32</cell><cell></cell><cell cols="2">56</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>4</cell><cell></cell><cell cols="3">824</cell><cell cols="2">0.000 ( * )</cell><cell cols="2">0.856</cell><cell cols="3">179</cell><cell cols="3">1698</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">8</cell><cell cols="3">566</cell><cell cols="2">0.389 ( * )</cell><cell cols="2">0.928</cell><cell></cell><cell cols="2">38</cell><cell cols="3">755</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">14</cell><cell cols="3">590</cell><cell cols="2">0.958</cell><cell cols="2">0.960</cell><cell></cell><cell cols="2">21</cell><cell></cell><cell cols="2">72</cell></row><row><cell>Random F5.2</cell><cell></cell><cell>7</cell><cell></cell><cell cols="2">16</cell><cell cols="3">644</cell><cell cols="2">0.975 ( * )</cell><cell cols="2">0.962</cell><cell></cell><cell cols="2">39</cell><cell></cell><cell cols="2">87</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">32</cell><cell cols="3">881</cell><cell cols="2">0.992</cell><cell cols="2">0.963</cell><cell></cell><cell cols="2">21</cell><cell></cell><cell cols="2">52</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">48</cell><cell cols="3">1137</cell><cell cols="2">0.994</cell><cell cols="2">0.966</cell><cell></cell><cell cols="2">21</cell><cell></cell><cell cols="2">49</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">60</cell><cell cols="3">1295</cell><cell cols="2">0.994</cell><cell cols="2">0.961</cell><cell></cell><cell cols="2">21</cell><cell></cell><cell cols="2">48</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>4</cell><cell></cell><cell cols="3">936</cell><cell cols="2">0.000</cell><cell cols="2">0.843</cell><cell cols="3">204</cell><cell cols="3">1827</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>8</cell><cell></cell><cell cols="3">560</cell><cell cols="2">0.000 ( * )</cell><cell cols="2">0.890</cell><cell></cell><cell cols="2">54</cell><cell cols="3">1179</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">11</cell><cell cols="3">594</cell><cell cols="2">0.315</cell><cell cols="2">0.933</cell><cell></cell><cell cols="2">34</cell><cell cols="3">808</cell></row><row><cell>Random F5.4</cell><cell></cell><cell>9</cell><cell></cell><cell cols="2">16</cell><cell cols="3">838</cell><cell cols="2">0.890 ( * )</cell><cell cols="2">0.959</cell><cell></cell><cell cols="2">24</cell><cell></cell><cell cols="2">174</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">18</cell><cell cols="3">654</cell><cell cols="2">0.954</cell><cell cols="2">0.966</cell><cell></cell><cell cols="2">37</cell><cell cols="3">112</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">32</cell><cell cols="3">825</cell><cell cols="2">0.990 ( * )</cell><cell cols="2">0.967</cell><cell></cell><cell cols="2">27</cell><cell></cell><cell cols="2">62</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">48</cell><cell cols="3">1058</cell><cell cols="2">0.993</cell><cell cols="2">0.964</cell><cell></cell><cell cols="2">35</cell><cell></cell><cell cols="2">65</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">60</cell><cell cols="3">1308</cell><cell cols="2">0.995</cell><cell cols="2">0.965</cell><cell></cell><cell cols="2">21</cell><cell></cell><cell cols="2">47</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>4</cell><cell></cell><cell cols="3">680</cell><cell cols="2">0.001</cell><cell cols="2">0.685</cell><cell></cell><cell cols="2">99</cell><cell cols="3">1519</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>8</cell><cell></cell><cell cols="3">548</cell><cell cols="2">0.094 ( * )</cell><cell cols="2">0.778</cell><cell></cell><cell cols="2">28</cell><cell cols="3">1070</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">13</cell><cell cols="3">586</cell><cell cols="2">0.694</cell><cell cols="2">0.847</cell><cell></cell><cell cols="2">23</cell><cell cols="3">384</cell></row><row><cell>Random F10</cell><cell></cell><cell>10</cell><cell></cell><cell cols="2">16</cell><cell cols="3">636</cell><cell cols="2">0.851 ( * )</cell><cell cols="2">0.857</cell><cell></cell><cell cols="2">27</cell><cell></cell><cell cols="2">222</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">20</cell><cell cols="3">659</cell><cell cols="2">0.961</cell><cell cols="2">0.878</cell><cell></cell><cell cols="2">29</cell><cell></cell><cell cols="2">95</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">32</cell><cell cols="3">867</cell><cell cols="2">0.987 ( * )</cell><cell cols="2">0.860</cell><cell></cell><cell cols="2">30</cell><cell></cell><cell cols="2">69</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">48</cell><cell cols="3">1131</cell><cell cols="2">0.993</cell><cell cols="2">0.869</cell><cell></cell><cell cols="2">26</cell><cell></cell><cell cols="2">56</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">60</cell><cell cols="3">1286</cell><cell cols="2">0.996</cell><cell cols="2">0.877</cell><cell></cell><cell cols="2">25</cell><cell></cell><cell cols="2">50</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>4</cell><cell></cell><cell cols="3">630</cell><cell cols="2">0.000</cell><cell cols="2">0.845</cell><cell cols="3">1245</cell><cell cols="3">1715</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>8</cell><cell></cell><cell cols="3">762</cell><cell cols="2">0.000 ( * )</cell><cell cols="2">0.904</cell><cell></cell><cell cols="2">43</cell><cell cols="3">1218</cell></row><row><cell>Random F5.8</cell><cell></cell><cell>13</cell><cell></cell><cell cols="2">16</cell><cell cols="3">667</cell><cell cols="2">0.090 ( * )</cell><cell cols="2">0.947</cell><cell></cell><cell cols="2">41</cell><cell cols="3">1073</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">18</cell><cell cols="3">713</cell><cell cols="2">0.406</cell><cell cols="2">0.950</cell><cell></cell><cell cols="2">38</cell><cell cols="3">723</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">26</cell><cell cols="3">789</cell><cell cols="2">0.944</cell><cell cols="2">0.964</cell><cell></cell><cell cols="2">40</cell><cell cols="3">137</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">32</cell><cell cols="3">890</cell><cell cols="2">0.979 ( * )</cell><cell cols="2">0.965</cell><cell></cell><cell cols="2">30</cell><cell></cell><cell cols="2">78</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>8</cell><cell></cell><cell cols="3">551</cell><cell cols="2">0.000</cell><cell cols="2">0.661</cell><cell></cell><cell cols="2">33</cell><cell cols="3">1180</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">16</cell><cell cols="3">624</cell><cell cols="2">0.002 ( * )</cell><cell cols="2">0.743</cell><cell></cell><cell cols="2">43</cell><cell cols="3">1130</cell></row><row><cell>Random F20</cell><cell></cell><cell>20</cell><cell></cell><cell cols="2">32</cell><cell cols="3">884</cell><cell cols="2">0.559 ( * )</cell><cell cols="2">0.768</cell><cell></cell><cell cols="2">21</cell><cell cols="3">537</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">40</cell><cell cols="3">1017</cell><cell cols="2">0.816</cell><cell cols="2">0.769</cell><cell></cell><cell cols="2">21</cell><cell cols="3">248</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">48</cell><cell cols="3">1076</cell><cell cols="2">0.897 ( * )</cell><cell cols="2">0.751</cell><cell></cell><cell cols="2">32</cell><cell></cell><cell cols="2">175</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">60</cell><cell cols="3">1213</cell><cell cols="2">0.950 ( * )</cell><cell cols="2">0.751</cell><cell></cell><cell cols="2">37</cell><cell></cell><cell cols="2">114</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>8</cell><cell></cell><cell cols="3">564</cell><cell cols="2">0.000</cell><cell cols="2">0.585</cell><cell></cell><cell cols="2">35</cell><cell cols="3">1162</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">16</cell><cell cols="3">676</cell><cell cols="2">0.000</cell><cell cols="2">0.68</cell><cell></cell><cell cols="2">23</cell><cell cols="3">1135</cell></row><row><cell>Random F30</cell><cell></cell><cell>30</cell><cell></cell><cell cols="2">32</cell><cell cols="3">866</cell><cell cols="2">0.003 ( * )</cell><cell cols="2">0.706</cell><cell></cell><cell cols="2">22</cell><cell cols="3">1119</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">48</cell><cell cols="3">1137</cell><cell cols="2">0.256</cell><cell cols="2">0.699</cell><cell></cell><cell cols="2">22</cell><cell cols="3">843</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">60</cell><cell cols="3">1289</cell><cell cols="2">0.590 ( * )</cell><cell cols="2">0.697</cell><cell></cell><cell cols="2">24</cell><cell cols="3">497</cell></row><row><cell>( * )</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>http://www.grid5000.fr</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>http://finance.yahoo.com</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>Indexing time responses are not reported here since they mainly depend on the cluster size.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>The research leading to these results has received funding from the <rs type="funder">European Union</rs>'s <rs type="programName">Horizon 2020</rs>, under grant agreement No. <rs type="grantNumber">732051</rs>. <rs type="person">Shasha</rs> was supported by an <rs type="funder">INRIA international</rs> chair.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_zNHGKAz">
					<idno type="grant-number">732051</idno>
					<orgName type="program" subtype="full">Horizon 2020</orgName>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Florent Masseglia is a scientific researcher in computer science at Inria since 2002. He works in Montpellier, in the Zenith team of Inria, on the analysis of very large scientific data. These data, derived from observations, experiments and simulation are indeed complex, often very large, and are at the heart of important issues better understand the studied domains (agronomy, biology, medicine). http://www.florent-masseglia.info</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Database-friendly random projections: Johnson-lindenstrauss with binary coins</title>
		<author>
			<persName><forename type="first">Dimitris</forename><surname>Achlioptas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computer and System Sciences</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="671" to="687" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The ts-tree: Efficient time series search and retrieval</title>
		<author>
			<persName><forename type="first">Ira</forename><surname>Assent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ralph</forename><surname>Krieger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Farzad</forename><surname>Afschari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Seidl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EDBT Conf</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="252" to="263" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The ts-tree: efficient time series search and retrieval</title>
		<author>
			<persName><forename type="first">Ira</forename><surname>Assent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ralph</forename><surname>Krieger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Farzad</forename><surname>Afschari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Seidl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Extending Database Technology (EDBT)</title>
		<meeting>the International Conference on Extending Database Technology (EDBT)</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="252" to="263" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Indexing spatio-temporal trajectories with chebyshev polynomials</title>
		<author>
			<persName><forename type="first">Yuhan</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raymond</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Management of Data (SIG-MOD)</title>
		<meeting>the International Conference on Management of Data (SIG-MOD)</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="599" to="610" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">isax 2.0: Indexing and mining one billion time series</title>
		<author>
			<persName><forename type="first">A</forename><surname>Camerra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Palpanas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shieh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Keogh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2010 IEEE International Conference on Data Mining, ICDM &apos;10</title>
		<meeting>the 2010 IEEE International Conference on Data Mining, ICDM &apos;10</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="58" to="67" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Beyond one billion time series: indexing and mining very large time series collections with iSAX2+</title>
		<author>
			<persName><forename type="first">A</forename><surname>Camerra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shieh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Palpanas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Rakthanmanon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">J</forename><surname>Keogh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowledge and Information Systems (KAIS)</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="123" to="151" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Beyond one billion time series: indexing and mining very large time series collections with i SAX2+</title>
		<author>
			<persName><forename type="first">Alessandro</forename><surname>Camerra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jin</forename><surname>Shieh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Themis</forename><surname>Palpanas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thanawin</forename><surname>Rakthanmanon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eamonn</forename><forename type="middle">J</forename><surname>Keogh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowl. Inf. Syst</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="123" to="151" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Similarity estimation techniques from rounding algorithms</title>
		<author>
			<persName><forename type="first">Moses</forename><forename type="middle">S</forename><surname>Charikar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thiry-fourth Annual ACM Symposium on Theory of Computing (STOC)</title>
		<meeting>the Thiry-fourth Annual ACM Symposium on Theory of Computing (STOC)</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="380" to="388" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning mixtures of gaussians</title>
		<author>
			<persName><forename type="first">Sanjoy</forename><surname>Dasgupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th Annual Symposium on Foundations of Computer Science, FOCS &apos;99</title>
		<meeting>the 40th Annual Symposium on Foundations of Computer Science, FOCS &apos;99</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page">634</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The lernaean hydra of data series similarity search: An experimental evaluation of the state of the art</title>
		<author>
			<persName><forename type="first">Karima</forename><surname>Echihabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kostas</forename><surname>Zoumpatianos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Themis</forename><surname>Palpanas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Houda</forename><surname>Benbrahim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PVLDB</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="112" to="127" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Return of the Lernaean Hydra: Experimental Evaluation of Data Series Approximate Similarity Search</title>
		<author>
			<persName><forename type="first">Karima</forename><surname>Echihabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kostas</forename><surname>Zoumpatianos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Themis</forename><surname>Palpanas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Houda</forename><surname>Benbrahim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PVLDB</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Time-series data mining</title>
		<author>
			<persName><forename type="first">Philippe</forename><surname>Esling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlos</forename><surname>Agon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Comput. Surv</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2012-12">December 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Fast subsequence matching in time-series databases</title>
		<author>
			<persName><forename type="first">Christos</forename><surname>Faloutsos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ranganathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yannis</forename><surname>Manolopoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIGMOD Rec</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="419" to="429" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Fast subsequence matching in time-series databases</title>
		<author>
			<persName><forename type="first">Christos</forename><surname>Faloutsos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ranganathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yannis</forename><surname>Manolopoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Management of Data (SIGMOD)</title>
		<meeting>the International Conference on Management of Data (SIGMOD)</meeting>
		<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="419" to="429" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Similarity search in high dimensions via hashing</title>
		<author>
			<persName><forename type="first">Aristides</forename><surname>Gionis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Indyk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rajeev</forename><surname>Motwani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Very Large Data Bases (VLDB)</title>
		<meeting>the International Conference on Very Large Data Bases (VLDB)</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="518" to="529" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Computational intelligence challenges and applications on large-scale astronomical time series databases</title>
		<author>
			<persName><forename type="first">Pablo</forename><surname>Huijse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pablo</forename><forename type="middle">A</forename><surname>Estévez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pavlos</forename><surname>Protopapas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jose</forename><forename type="middle">C</forename><surname>Principe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pablo</forename><surname>Zegers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Comp. Int. Mag</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="27" to="39" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Stable distributions, pseudorandom generators, embeddings and data stream computation</title>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Indyk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">41st Annual Symposium on Foundations of Computer Science (FOCS)</title>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="189" to="197" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Identifying representative trends in massive time series data sets using sketches</title>
		<author>
			<persName><forename type="first">Piotr</forename><surname>Indyk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Koudas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Muthukrishnan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Very Large Data Bases (VLDB)</title>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="363" to="372" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">R I</forename></persName>
		</author>
		<ptr target="http://ds.iris.edu/data/access/" />
		<title level="m">Seismology with Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note>Seismic data access</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Extensions of Lipschitz mappings into a Hilbert space</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">B</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lindenstrauss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference in Modern Analysis and Probability</title>
		<title level="s">Contemporary Mathematics</title>
		<imprint>
			<date type="published" when="1984">1984</date>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="189" to="206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Time-series active search for quick retrieval of audio and video</title>
		<author>
			<persName><forename type="first">Kunio</forename><surname>Kashino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gavin</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hiroshi</forename><surname>Murase</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP</title>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Exact indexing of dynamic time warping</title>
		<author>
			<persName><forename type="first">J</forename><surname>Eamonn</surname></persName>
		</author>
		<author>
			<persName><surname>Keogh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">VLDB</title>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Coconut: A scalable bottom-up approach for building data series indexes</title>
		<author>
			<persName><forename type="first">Haridimos</forename><surname>Kondylakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niv</forename><surname>Dayan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kostas</forename><surname>Zoumpatianos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Themis</forename><surname>Palpanas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PVLDB</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Efficient search for approximate nearest neighbor in high dimensional spaces</title>
		<author>
			<persName><forename type="first">Eyal</forename><surname>Kushilevitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rafail</forename><surname>Ostrovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuval</forename><surname>Rabani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirtieth Annual ACM Symposium on Theory of Computing (STOC)</title>
		<meeting>the Thirtieth Annual ACM Symposium on Theory of Computing (STOC)</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="614" to="623" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A symbolic representation of time series, with implications for streaming algorithms</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Keogh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lonardi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Chiu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGMOD</title>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Experiencing sax: A novel symbolic representation of time series</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Keogh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lonardi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Data Min. Knowl. Discov</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="107" to="144" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">ULISSE: ultra compact index for variable-length similarity search in data series</title>
		<author>
			<persName><forename type="first">Michele</forename><surname>Linardi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Themis</forename><surname>Palpanas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDE</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Scalable, variable-length similarity search in data series: The ulisse approach</title>
		<author>
			<persName><forename type="first">Michele</forename><surname>Linardi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Themis</forename><surname>Palpanas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PVLDB</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Matrix profile X: VALMOD -scalable discovery of variable-length motifs in data series</title>
		<author>
			<persName><forename type="first">Michele</forename><surname>Linardi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Themis</forename><surname>Palpanas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eamonn</forename><forename type="middle">J</forename><surname>Keogh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGMOD</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">VALMOD: A suite for easy and exact detection of variable length motifs in data series</title>
		<author>
			<persName><forename type="first">Michele</forename><surname>Linardi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Themis</forename><surname>Palpanas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eamonn</forename><forename type="middle">J</forename><surname>Keogh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGMOD</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Data series management: The road to big sequence analytics</title>
		<author>
			<persName><forename type="first">Themis</forename><surname>Palpanas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIGMOD Record</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="47" to="52" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Evolution of a Data Series Index</title>
		<author>
			<persName><forename type="first">Themis</forename><surname>Palpanas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1197">1197. 2020</date>
			<publisher>CCIS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">ParIS: The Next Destination for Fast Data Series Indexing and Query Answering</title>
		<author>
			<persName><forename type="first">Botao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Themis</forename><surname>Palpanas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Panagiota</forename><surname>Fatourou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE BigData</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Searching and mining trillions of time series subsequences under dynamic time warping</title>
		<author>
			<persName><forename type="first">T</forename><surname>Rakthanmanon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Campana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mueen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Batista</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Westover</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zakaria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Keogh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Practical data prediction for real-world wireless sensor networks</title>
		<author>
			<persName><forename type="first">Usman</forename><surname>Raza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alessandro</forename><surname>Camerra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amy</forename><forename type="middle">L</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Themis</forename><surname>Palpanas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gian</forename><forename type="middle">Pietro</forename><surname>Picco</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Knowl. Data Eng</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note>accepted for publication</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>Vision</surname></persName>
		</author>
		<ptr target="http://sites.skoltech.ru/compvision/noimi" />
		<title level="m">Deep billion-scale indexing</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Long-term variability of agn at hard x-rays</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">H</forename><surname>Baumgartner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Astronomy &amp; Astrophysics</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>University</surname></persName>
		</author>
		<ptr target="http://fcon_1000.projects.nitrc.org/indi/retro/sald.html?utm_source=newsletter&amp;utm_medium=email&amp;utm_content=SeeData&amp;utm_campaign=indi-1" />
		<title level="m">Southwest university adult lifespan dataset</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Scalable density-based clustering with quality guarantees using random projections</title>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michail</forename><surname>Vlachos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Data Min. Knowl. Discov</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="972" to="1005" />
			<date type="published" when="2017-07">July 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">High Performance Discovery in Time series, Techniques and Case Studies</title>
		<author>
			<persName><forename type="first">D</forename><surname>Shasha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004">2004</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Tuning time series queries in finance: Case studies and recommendations</title>
		<author>
			<persName><forename type="first">Dennis</forename><surname>Shasha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Data Eng. Bull</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="40" to="46" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">isax: Indexing and mining terabyte sized time series</title>
		<author>
			<persName><forename type="first">J</forename><surname>Shieh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Keogh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD &apos;08</title>
		<meeting>the 14th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD &apos;08</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="623" to="631" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">isax: Disk-aware mining and indexing of massive time series datasets</title>
		<author>
			<persName><forename type="first">J</forename><surname>Shieh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Keogh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">DMKD</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="24" to="57" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">iSAX: Indexing and mining terabyte sized time series</title>
		<author>
			<persName><forename type="first">Jin</forename><surname>Shieh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eamonn</forename><surname>Keogh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Knowledge Discovery and Data Mining (SIGKDD)</title>
		<meeting>the International Conference on Knowledge Discovery and Data Mining (SIGKDD)</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="623" to="631" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Long-term variability of agn at hard x-rays</title>
		<author>
			<persName><forename type="first">S</forename><surname>Soldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Beckmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">H</forename><surname>Baumgartner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Ponti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">R</forename><surname>Shrader</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lubi´nski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">A</forename><surname>Krimm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Mattana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tueller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">A&amp;A</title>
		<imprint>
			<biblScope unit="volume">563</biblScope>
			<biblScope unit="page">57</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">A data-adaptive and dynamic segmentation index for whole matching on time series</title>
		<author>
			<persName><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Jian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Sheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PVLDB</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="793" to="804" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Chirp: A new classifier based on composite hypercubes on iterated random projections</title>
		<author>
			<persName><forename type="first">Leland</forename><surname>Wilkinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anushka</forename><surname>Anand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dang</forename><surname>Nhon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tuan</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Knowledge Discovery and Data Mining (KDD)</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="6" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Dpisax: Massively distributed partitioned isax</title>
		<author>
			<persName><forename type="first">Reza</forename><surname>Djamel Edine Yagoubi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Florent</forename><surname>Akbarinia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Themis</forename><surname>Masseglia</surname></persName>
		</author>
		<author>
			<persName><surname>Palpanas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDM</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1135" to="1140" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Time series shapelets: a new primitive for data mining</title>
		<author>
			<persName><forename type="first">Lexiang</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eamonn</forename><forename type="middle">J</forename><surname>Keogh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Indexing for interactive exploration of big data series</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zoumpatianos</surname></persName>
		</author>
		<author>
			<persName><surname>Idreos</surname></persName>
		</author>
		<author>
			<persName><surname>Palpanas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Management of Data (SIGMOD), SIGMOD &apos;14</title>
		<meeting>the International Conference on Management of Data (SIGMOD), SIGMOD &apos;14</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1555" to="1566" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">ADS: the adaptive data series index</title>
		<author>
			<persName><forename type="first">Kostas</forename><surname>Zoumpatianos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stratos</forename><surname>Idreos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Themis</forename><surname>Palpanas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">VLDB J</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="843" to="866" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Data series management: Fulfilling the need for big sequence analytics</title>
		<author>
			<persName><forename type="first">Kostas</forename><surname>Zoumpatianos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Themis</forename><surname>Palpanas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDE</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
