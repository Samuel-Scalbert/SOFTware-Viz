{"application": "software-mentions", "version": "0.8.0", "date": "2024-10-07T11:53+0000", "md5": "D865872433705D6646757010CBA18D87", "mentions": [{"type": "software", "software-type": "software", "software-name": {"rawForm": "SEM", "normalizedForm": "SEM", "offsetStart": 0, "offsetEnd": 3}, "version": {"rawForm": "1", "normalizedForm": "1", "offsetStart": 41, "offsetEnd": 42}, "context": "SEM uses Wapiti (Lavergne et al., 2010) v1.5.0 as linear-chain CRFs implementation. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.6196155548095703}, "created": {"value": false, "score": 1.430511474609375e-06}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9997407793998718}, "created": {"value": true, "score": 0.9384669661521912}, "shared": {"value": false, "score": 1.9073486328125e-06}}, "references": [{"label": "(Lavergne et al., 2010)", "normalizedForm": "Lavergne et al., 2010", "refKey": 16, "offsetStart": 15323, "offsetEnd": 15346}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "SEM", "normalizedForm": "SEM", "offsetStart": 0, "offsetEnd": 3}, "version": {"rawForm": "1", "normalizedForm": "1"}, "context": "SEM uses the following features for NER:", "mentionContextAttributes": {"used": {"value": false, "score": 0.00027245283126831055}, "created": {"value": false, "score": 3.325939178466797e-05}, "shared": {"value": false, "score": 1.0728836059570312e-06}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9997407793998718}, "created": {"value": true, "score": 0.9384669661521912}, "shared": {"value": false, "score": 1.9073486328125e-06}}, "references": [{"label": "(Lavergne et al., 2010)", "normalizedForm": "Lavergne et al., 2010", "refKey": 16}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "SEM", "normalizedForm": "SEM", "offsetStart": 0, "offsetEnd": 3}, "version": {"rawForm": "1", "normalizedForm": "1"}, "context": "SEM also uses an NE mention broadcasting post-processing (mentions found at least once are used as a gazetteer to tag unlabeled mentions), but we did not observe any improvement using this post-processing on the best hyperparameters on the development set.", "mentionContextAttributes": {"used": {"value": false, "score": 0.3915473222732544}, "created": {"value": false, "score": 1.3589859008789062e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9997407793998718}, "created": {"value": true, "score": 0.9384669661521912}, "shared": {"value": false, "score": 1.9073486328125e-06}}, "references": [{"label": "(Lavergne et al., 2010)", "normalizedForm": "Lavergne et al., 2010", "refKey": 16}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "CamemBERT", "normalizedForm": "CamemBERT", "offsetStart": 0, "offsetEnd": 9}, "context": "CamemBERT Embeddings: Adding the CamemBERT embeddings always increases the performance of the model LSTM based models. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.0002587437629699707}, "created": {"value": false, "score": 7.748603820800781e-06}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999847412109375}, "created": {"value": false, "score": 0.014398396015167236}, "shared": {"value": false, "score": 1.9073486328125e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "CamemBERT", "normalizedForm": "CamemBERT", "offsetStart": 0, "offsetEnd": 9}, "context": "CamemBERT + FrELMo: Contrary to the results given in Strakov\u00e1 et al. (2019), adding ELMo to CamemBERT did not have a positive impact on the performances of the models. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9995717406272888}, "created": {"value": false, "score": 3.4570693969726562e-06}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999847412109375}, "created": {"value": false, "score": 0.014398396015167236}, "shared": {"value": false, "score": 1.9073486328125e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "CamemBERT", "normalizedForm": "CamemBERT", "offsetStart": 0, "offsetEnd": 9}, "context": "CamemBERT models are transformer-based models based on an architecture similar to that of RoBERTa (Liu et al., 2019), an improvement over the widely used and successful BERT model (Devlin et al., 2019).", "mentionContextAttributes": {"used": {"value": false, "score": 0.0002079606056213379}, "created": {"value": false, "score": 0.0003565549850463867}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999847412109375}, "created": {"value": false, "score": 0.014398396015167236}, "shared": {"value": false, "score": 1.9073486328125e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "CamemBERT", "normalizedForm": "CamemBERT", "offsetStart": 4, "offsetEnd": 13}, "context": "The CamemBERT models we use in our experiments differ in multiple ways:", "mentionContextAttributes": {"used": {"value": false, "score": 0.008569777011871338}, "created": {"value": false, "score": 3.790855407714844e-05}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999847412109375}, "created": {"value": false, "score": 0.014398396015167236}, "shared": {"value": false, "score": 1.9073486328125e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "FrELMo", "normalizedForm": "FrELMo", "offsetStart": 6, "offsetEnd": 12}, "context": ", the FrELMo contextual language model obtained by training the ELMo architecture on the OSCAR large-coverage Common-Crawlbased corpus developed by Ortiz Su\u00e1rez et al. (2019),", "mentionContextAttributes": {"used": {"value": false, "score": 0.49602842330932617}, "created": {"value": false, "score": 0.08631420135498047}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9995717406272888}, "created": {"value": false, "score": 0.08631420135498047}, "shared": {"value": false, "score": 5.960464477539062e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "SEM", "normalizedForm": "SEM", "offsetStart": 8, "offsetEnd": 11}, "version": {"rawForm": "1", "normalizedForm": "1"}, "context": "We used SEM (Dupont, 2017) as our strong baseline because, to the best of our knowledge, it was the previous state-of-the-art for named entity recognition on the FTB-NE corpus.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9991868138313293}, "created": {"value": false, "score": 0.0012322664260864258}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9997407793998718}, "created": {"value": true, "score": 0.9384669661521912}, "shared": {"value": false, "score": 1.9073486328125e-06}}, "references": [{"label": "(Lavergne et al., 2010)", "normalizedForm": "Lavergne et al., 2010", "refKey": 16}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "FrELMo", "normalizedForm": "FrELMo", "offsetStart": 12, "offsetEnd": 18}, "context": "CamemBERT + FrELMo: Contrary to the results given in Strakov\u00e1 et al. (2019), adding ELMo to CamemBERT did not have a positive impact on the performances of the models.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9995717406272888}, "created": {"value": false, "score": 3.4570693969726562e-06}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9995717406272888}, "created": {"value": false, "score": 0.08631420135498047}, "shared": {"value": false, "score": 5.960464477539062e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "CamemBERT", "normalizedForm": "CamemBERT", "offsetStart": 15, "offsetEnd": 24}, "context": "In fact adding CamemBERT embeddings increases the original scores far more than ELMo embeddings does, so much so that the state-of-the-art model is the LSTM + CRF + FastText + CamemBERT OSCAR-BASE-SWM .", "mentionContextAttributes": {"used": {"value": false, "score": 0.002403080463409424}, "created": {"value": false, "score": 2.2649765014648438e-06}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999847412109375}, "created": {"value": false, "score": 0.014398396015167236}, "shared": {"value": false, "score": 1.9073486328125e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "SEM", "normalizedForm": "SEM", "offsetStart": 19, "offsetEnd": 22}, "version": {"rawForm": "1", "normalizedForm": "1"}, "context": "We trained our own SEM model by using SEM features on gold tokenization and optimized L1 and L2 penalties on the development set.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9705885052680969}, "created": {"value": true, "score": 0.9384669661521912}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9997407793998718}, "created": {"value": true, "score": 0.9384669661521912}, "shared": {"value": false, "score": 1.9073486328125e-06}}, "references": [{"label": "(Lavergne et al., 2010)", "normalizedForm": "Lavergne et al., 2010", "refKey": 16}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "FastText", "normalizedForm": "FastText", "offsetStart": 29, "offsetEnd": 37}, "context": "In particular, by using both FastText embeddings (Bojanowski et al., 2017) and one of the versions of the CamemBERT French neural contextual language model (Martin et al., 2019) within an LSTM-CRF architecture, we can reach an F1-score of 90.25, a 6.5point improvement over the previously state-of-the-art system SEM (Dupont, 2017).", "mentionContextAttributes": {"used": {"value": true, "score": 0.6253992915153503}, "created": {"value": false, "score": 0.006717801094055176}, "shared": {"value": false, "score": 1.9073486328125e-06}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9493878483772278}, "created": {"value": false, "score": 0.006717801094055176}, "shared": {"value": false, "score": 1.9073486328125e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "CamemBERT", "normalizedForm": "CamemBERT", "offsetStart": 33, "offsetEnd": 42}, "context": "CamemBERT Embeddings: Adding the CamemBERT embeddings always increases the performance of the model LSTM based models.", "mentionContextAttributes": {"used": {"value": false, "score": 0.0002587437629699707}, "created": {"value": false, "score": 7.748603820800781e-06}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999847412109375}, "created": {"value": false, "score": 0.014398396015167236}, "shared": {"value": false, "score": 1.9073486328125e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "FastText", "normalizedForm": "FastText", "offsetStart": 36, "offsetEnd": 44}, "context": "For instance, in Table 1, the model FastText + CamemBERT OSCAR-BASE-WWM under the header \"LSTM-CRF + embeddings corresponds to a model using the LSTM-CRF architecture and, as embeddings, the concatenation of FastText embeddings, the output of the CamemBERT \"BASE\" model trained on OSCAR with a whole-word masking objective, and the output of the FrELMo language model.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9493878483772278}, "created": {"value": false, "score": 2.2649765014648438e-06}, "shared": {"value": false, "score": 5.960464477539062e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9493878483772278}, "created": {"value": false, "score": 0.006717801094055176}, "shared": {"value": false, "score": 1.9073486328125e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "SEM", "normalizedForm": "SEM", "offsetStart": 37, "offsetEnd": 40}, "version": {"rawForm": "1", "normalizedForm": "1"}, "context": "Word Embeddings: Results obtained by SEM and by our neural models are shown in table 1.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9997407793998718}, "created": {"value": false, "score": 0.00011408329010009766}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9997407793998718}, "created": {"value": true, "score": 0.9384669661521912}, "shared": {"value": false, "score": 1.9073486328125e-06}}, "references": [{"label": "(Lavergne et al., 2010)", "normalizedForm": "Lavergne et al., 2010", "refKey": 16}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "SEM", "normalizedForm": "SEM", "offsetStart": 38, "offsetEnd": 41}, "version": {"rawForm": "1", "normalizedForm": "1"}, "context": "We trained our own SEM model by using SEM features on gold tokenization and optimized L1 and L2 penalties on the development set.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9705885052680969}, "created": {"value": true, "score": 0.9384669661521912}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9997407793998718}, "created": {"value": true, "score": 0.9384669661521912}, "shared": {"value": false, "score": 1.9073486328125e-06}}, "references": [{"label": "(Lavergne et al., 2010)", "normalizedForm": "Lavergne et al., 2010", "refKey": 16}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "CamemBERT", "normalizedForm": "CamemBERT", "offsetStart": 38, "offsetEnd": 47}, "context": "However, BERT-like architectures like CamemBERT rely on a fixed vocabulary of explicitly predefined size obtained by an algorithm that splits rarer words into subwords, which are part of the vocabulary together with more frequent words. ", "mentionContextAttributes": {"used": {"value": false, "score": 5.8650970458984375e-05}, "created": {"value": false, "score": 0.0033678412437438965}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999847412109375}, "created": {"value": false, "score": 0.014398396015167236}, "shared": {"value": false, "score": 1.9073486328125e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "CamemBERT", "normalizedForm": "CamemBERT", "offsetStart": 47, "offsetEnd": 56}, "context": "For instance, in Table 1, the model FastText + CamemBERT OSCAR-BASE-WWM under the header \"LSTM-CRF + embeddings corresponds to a model using the LSTM-CRF architecture and, as embeddings, the concatenation of FastText embeddings, the output of the CamemBERT \"BASE\" model trained on OSCAR with a whole-word masking objective, and the output of the FrELMo language model.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9493878483772278}, "created": {"value": false, "score": 2.2649765014648438e-06}, "shared": {"value": false, "score": 5.960464477539062e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999847412109375}, "created": {"value": false, "score": 0.014398396015167236}, "shared": {"value": false, "score": 1.9073486328125e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "CamemBERT", "normalizedForm": "CamemBERT", "offsetStart": 59, "offsetEnd": 68}, "context": "\u2022 Masking strategy: the objective function used to train a CamemBERT model is a masked language model objective. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.0011110901832580566}, "created": {"value": false, "score": 0.00029081106185913086}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999847412109375}, "created": {"value": false, "score": 0.014398396015167236}, "shared": {"value": false, "score": 1.9073486328125e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "FastText", "normalizedForm": "FastText", "offsetStart": 64, "offsetEnd": 72}, "context": "They use zero to three of the following vector representations: FastText non-contextual embeddings (Bojanowski et al., 2017) and one of multiple CamemBERT language models (Martin et al., 2019).", "mentionContextAttributes": {"used": {"value": false, "score": 0.3672880530357361}, "created": {"value": false, "score": 7.987022399902344e-06}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9493878483772278}, "created": {"value": false, "score": 0.006717801094055176}, "shared": {"value": false, "score": 1.9073486328125e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "CamemBERT", "normalizedForm": "CamemBERT", "offsetStart": 64, "offsetEnd": 73}, "context": "However, as opposed to adding ELMo, the difference with/without CamemBERT is equally considerable for both the LSTM-seq2seq and LSTM-CRF. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.12456673383712769}, "created": {"value": false, "score": 1.2755393981933594e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999847412109375}, "created": {"value": false, "score": 0.014398396015167236}, "shared": {"value": false, "score": 1.9073486328125e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "CamemBERT", "normalizedForm": "CamemBERT", "offsetStart": 76, "offsetEnd": 85}, "context": "We will use various pre-trained embeddings in said architectures: fastText, CamemBERT (a French BERT-like model) and FrELMo (a French ELMo model) embeddings.", "mentionContextAttributes": {"used": {"value": false, "score": 0.3331691026687622}, "created": {"value": false, "score": 0.014398396015167236}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999847412109375}, "created": {"value": false, "score": 0.014398396015167236}, "shared": {"value": false, "score": 1.9073486328125e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "SEM", "normalizedForm": "SEM", "offsetStart": 79, "offsetEnd": 82}, "version": {"rawForm": "1", "normalizedForm": "1"}, "context": "The model yielded an F1-score of 0.7564, which makes it a weaker baseline than SEM.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9676939845085144}, "created": {"value": false, "score": 4.088878631591797e-05}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9997407793998718}, "created": {"value": true, "score": 0.9384669661521912}, "shared": {"value": false, "score": 1.9073486328125e-06}}, "references": [{"label": "(Lavergne et al., 2010)", "normalizedForm": "Lavergne et al., 2010", "refKey": 16}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "mBERT", "normalizedForm": "mBERT", "offsetStart": 80, "offsetEnd": 85}, "context": "For comparison purposes, we also display the results of an experiment using the mBERT multilingual BERT model trained on the Wikpiedias for over 100 languages.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9999650716781616}, "created": {"value": false, "score": 0.0008652806282043457}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999650716781616}, "created": {"value": false, "score": 0.0008652806282043457}, "shared": {"value": false, "score": 1.1920928955078125e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "CamemBERT", "normalizedForm": "CamemBERT", "offsetStart": 92, "offsetEnd": 101}, "context": "CamemBERT + FrELMo: Contrary to the results given in Strakov\u00e1 et al. (2019), adding ELMo to CamemBERT did not have a positive impact on the performances of the models. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9995717406272888}, "created": {"value": false, "score": 3.4570693969726562e-06}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999847412109375}, "created": {"value": false, "score": 0.014398396015167236}, "shared": {"value": false, "score": 1.9073486328125e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "CamemBERT", "normalizedForm": "CamemBERT", "offsetStart": 98, "offsetEnd": 107}, "context": "Our hypothesis for these results is that, contrary to Strakov\u00e1 et al. (2019), we trained ELMo and CamemBERT on the same corpus. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9999847412109375}, "created": {"value": false, "score": 0.0016308426856994629}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999847412109375}, "created": {"value": false, "score": 0.014398396015167236}, "shared": {"value": false, "score": 1.9073486328125e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "CamemBERT", "normalizedForm": "CamemBERT", "offsetStart": 98, "offsetEnd": 107}, "context": "We think that, in our case, ELMo either does not bring any new information or even interfere with CamemBERT.", "mentionContextAttributes": {"used": {"value": false, "score": 0.0008386373519897461}, "created": {"value": false, "score": 0.006231784820556641}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999847412109375}, "created": {"value": false, "score": 0.014398396015167236}, "shared": {"value": false, "score": 1.9073486328125e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "CamemBERT", "normalizedForm": "CamemBERT", "offsetStart": 106, "offsetEnd": 115}, "context": "In particular, by using both FastText embeddings (Bojanowski et al., 2017) and one of the versions of the CamemBERT French neural contextual language model (Martin et al., 2019) within an LSTM-CRF architecture, we can reach an F1-score of 90.25, a 6.5point improvement over the previously state-of-the-art system SEM (Dupont, 2017).", "mentionContextAttributes": {"used": {"value": true, "score": 0.6253992915153503}, "created": {"value": false, "score": 0.006717801094055176}, "shared": {"value": false, "score": 1.9073486328125e-06}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999847412109375}, "created": {"value": false, "score": 0.014398396015167236}, "shared": {"value": false, "score": 1.9073486328125e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "FrELMo", "normalizedForm": "FrELMo", "offsetStart": 117, "offsetEnd": 123}, "context": "We will use various pre-trained embeddings in said architectures: fastText, CamemBERT (a French BERT-like model) and FrELMo (a French ELMo model) embeddings.", "mentionContextAttributes": {"used": {"value": false, "score": 0.3331691026687622}, "created": {"value": false, "score": 0.014398396015167236}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9995717406272888}, "created": {"value": false, "score": 0.08631420135498047}, "shared": {"value": false, "score": 5.960464477539062e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "SEM", "normalizedForm": "SEM", "offsetStart": 126, "offsetEnd": 129}, "version": {"rawForm": "1", "normalizedForm": "1"}, "context": "First important result that should be noted is that LSTM+CRF and LSTM+seq2seq models have similar performances to that of the SEM (CRF) baseline when they are not augmented with any kind of embeddings.", "mentionContextAttributes": {"used": {"value": false, "score": 0.010336697101593018}, "created": {"value": false, "score": 3.5643577575683594e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9997407793998718}, "created": {"value": true, "score": 0.9384669661521912}, "shared": {"value": false, "score": 1.9073486328125e-06}}, "references": [{"label": "(Lavergne et al., 2010)", "normalizedForm": "Lavergne et al., 2010", "refKey": 16}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "SEM", "normalizedForm": "SEM", "offsetStart": 135, "offsetEnd": 138}, "version": {"rawForm": "1", "normalizedForm": "1"}, "context": "Our best neural model reaches an F1-score which is 6.55 points higher (a 40% error reduction) than the strong baseline provided by the SEM system.", "mentionContextAttributes": {"used": {"value": false, "score": 0.006792843341827393}, "created": {"value": false, "score": 0.0005748271942138672}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9997407793998718}, "created": {"value": true, "score": 0.9384669661521912}, "shared": {"value": false, "score": 1.9073486328125e-06}}, "references": [{"label": "(Lavergne et al., 2010)", "normalizedForm": "Lavergne et al., 2010", "refKey": 16}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "CamemBERT", "normalizedForm": "CamemBERT", "offsetStart": 145, "offsetEnd": 154}, "context": "They use zero to three of the following vector representations: FastText non-contextual embeddings (Bojanowski et al., 2017) and one of multiple CamemBERT language models (Martin et al., 2019).", "mentionContextAttributes": {"used": {"value": false, "score": 0.3672868609428406}, "created": {"value": false, "score": 7.987022399902344e-06}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999847412109375}, "created": {"value": false, "score": 0.014398396015167236}, "shared": {"value": false, "score": 1.9073486328125e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "FastText", "normalizedForm": "FastText", "offsetStart": 165, "offsetEnd": 173}, "context": "In fact adding CamemBERT embeddings increases the original scores far more than ELMo embeddings does, so much so that the state-of-the-art model is the LSTM + CRF + FastText + CamemBERT OSCAR-BASE-SWM .", "mentionContextAttributes": {"used": {"value": false, "score": 0.002403080463409424}, "created": {"value": false, "score": 2.2649765014648438e-06}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9493878483772278}, "created": {"value": false, "score": 0.006717801094055176}, "shared": {"value": false, "score": 1.9073486328125e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "CamemBERT", "normalizedForm": "CamemBERT", "offsetStart": 176, "offsetEnd": 185}, "context": "In fact adding CamemBERT embeddings increases the original scores far more than ELMo embeddings does, so much so that the state-of-the-art model is the LSTM + CRF + FastText + CamemBERT OSCAR-BASE-SWM .", "mentionContextAttributes": {"used": {"value": false, "score": 0.002403080463409424}, "created": {"value": false, "score": 2.2649765014648438e-06}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999847412109375}, "created": {"value": false, "score": 0.014398396015167236}, "shared": {"value": false, "score": 1.9073486328125e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "FastText", "normalizedForm": "FastText", "offsetStart": 208, "offsetEnd": 216}, "context": "For instance, in Table 1, the model FastText + CamemBERT OSCAR-BASE-WWM under the header \"LSTM-CRF + embeddings corresponds to a model using the LSTM-CRF architecture and, as embeddings, the concatenation of FastText embeddings, the output of the CamemBERT \"BASE\" model trained on OSCAR with a whole-word masking objective, and the output of the FrELMo language model.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9493875503540039}, "created": {"value": false, "score": 2.2649765014648438e-06}, "shared": {"value": false, "score": 5.960464477539062e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9493878483772278}, "created": {"value": false, "score": 0.006717801094055176}, "shared": {"value": false, "score": 1.9073486328125e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "SEM", "normalizedForm": "SEM", "offsetStart": 242, "offsetEnd": 245}, "version": {"rawForm": "1", "normalizedForm": "1"}, "context": "The raw results of this experiment are given in table 2. We can see that the shuffled splits result in improvements on all metrics, the improvement in F1-score on the test set ranging from 4.04 to 5.75 (or 25% to 35% error reduction) for our SEM baseline, and from 1.73 to 3.21 (or 18% to 30% error reduction) for our LSTM-CRF architectures, reaching scores comparable to the English state-ofthe-art.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9995365142822266}, "created": {"value": false, "score": 4.887580871582031e-06}, "shared": {"value": false, "score": 1.3113021850585938e-06}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9997407793998718}, "created": {"value": true, "score": 0.9384669661521912}, "shared": {"value": false, "score": 1.9073486328125e-06}}, "references": [{"label": "(Lavergne et al., 2010)", "normalizedForm": "Lavergne et al., 2010", "refKey": 16}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "CamemBERT", "normalizedForm": "CamemBERT", "offsetStart": 247, "offsetEnd": 256}, "context": "For instance, in Table 1, the model FastText + CamemBERT OSCAR-BASE-WWM under the header \"LSTM-CRF + embeddings corresponds to a model using the LSTM-CRF architecture and, as embeddings, the concatenation of FastText embeddings, the output of the CamemBERT \"BASE\" model trained on OSCAR with a whole-word masking objective, and the output of the FrELMo language model.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9493875503540039}, "created": {"value": false, "score": 2.2649765014648438e-06}, "shared": {"value": false, "score": 5.960464477539062e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999847412109375}, "created": {"value": false, "score": 0.014398396015167236}, "shared": {"value": false, "score": 1.9073486328125e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "SEM", "normalizedForm": "SEM", "offsetStart": 313, "offsetEnd": 316}, "version": {"rawForm": "1", "normalizedForm": "1"}, "context": "In particular, by using both FastText embeddings (Bojanowski et al., 2017) and one of the versions of the CamemBERT French neural contextual language model (Martin et al., 2019) within an LSTM-CRF architecture, we can reach an F1-score of 90.25, a 6.5point improvement over the previously state-of-the-art system SEM (Dupont, 2017).", "mentionContextAttributes": {"used": {"value": true, "score": 0.6253995299339294}, "created": {"value": false, "score": 0.006717801094055176}, "shared": {"value": false, "score": 1.9073486328125e-06}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9997407793998718}, "created": {"value": true, "score": 0.9384669661521912}, "shared": {"value": false, "score": 1.9073486328125e-06}}, "references": [{"label": "(Dupont, 2017)", "normalizedForm": "Dupont, 2017", "refKey": 10, "offsetStart": 6149, "offsetEnd": 6163}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "FrELMo", "normalizedForm": "FrELMo", "offsetStart": 346, "offsetEnd": 352}, "context": "For instance, in Table 1, the model FastText + CamemBERT OSCAR-BASE-WWM under the header \"LSTM-CRF + embeddings corresponds to a model using the LSTM-CRF architecture and, as embeddings, the concatenation of FastText embeddings, the output of the CamemBERT \"BASE\" model trained on OSCAR with a whole-word masking objective, and the output of the FrELMo language model.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9493875503540039}, "created": {"value": false, "score": 2.2649765014648438e-06}, "shared": {"value": false, "score": 5.960464477539062e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9995717406272888}, "created": {"value": false, "score": 0.08631420135498047}, "shared": {"value": false, "score": 5.960464477539062e-07}}}], "references": [{"refKey": 16, "tei": "<biblStruct xml:id=\"b16\">\n\t<analytic>\n\t\t<title level=\"a\" type=\"main\">Practical very large scale CRFs</title>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">T</forename><surname>Lavergne</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">O</forename><surname>Capp\u00e9</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">F</forename><surname>Yvon</surname></persName>\n\t\t</author>\n\t</analytic>\n\t<monogr>\n\t\t<title level=\"m\">Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics</title>\n\t\t<meeting>the 48th Annual Meeting of the Association for Computational Linguistics</meeting>\n\t\t<imprint>\n\t\t\t<publisher>Association for Computational Linguistics</publisher>\n\t\t\t<date>2010</date>\n\t\t\t<biblScope unit=\"page\">513</biblScope>\n\t\t</imprint>\n\t</monogr>\n</biblStruct>\n"}, {"refKey": 10, "tei": "<biblStruct xml:id=\"b10\">\n\t<analytic>\n\t\t<title level=\"a\" type=\"main\">Exploration de traits pour la reconnaissance d&apos;entit\u00e9s nomm\u00e9es du fran\u00e7ais par apprentissage automatique</title>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Y</forename><surname>Dupont</surname></persName>\n\t\t</author>\n\t</analytic>\n\t<monogr>\n\t\t<title level=\"m\">24e Conf&apos;erence sur le Traitement Automatique des Langues Naturelles (TALN)</title>\n\t\t<imprint>\n\t\t\t<date>2017</date>\n\t\t\t<biblScope unit=\"page\">42</biblScope>\n\t\t</imprint>\n\t</monogr>\n</biblStruct>\n"}], "runtime": 34651, "id": "2f2bc4191e5a446f089671c9ea5bcc14b9bfe712", "metadata": {"id": "2f2bc4191e5a446f089671c9ea5bcc14b9bfe712"}, "original_file_path": "../../datalake/Samuel/SOFTware-Sync/downloads/xml/hal-02617950.grobid.tei.xml", "file_name": "hal-02617950.grobid.tei.xml"}