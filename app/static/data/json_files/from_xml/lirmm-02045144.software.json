{"application": "software-mentions", "version": "0.8.0", "date": "2024-10-07T12:04+0000", "md5": "D529C893C944C057DA655737E394EAFA", "mentions": [{"type": "software", "software-type": "software", "software-name": {"rawForm": "Spark", "normalizedForm": "Spark", "offsetStart": 0, "offsetEnd": 5}, "context": "Spark can be deployed on shared-nothing clusters, i.e. clusters of commodity computers with no sharing of either disk or memory among computers. ", "mentionContextAttributes": {"used": {"value": false, "score": 3.4928321838378906e-05}, "created": {"value": false, "score": 0.0005665421485900879}, "shared": {"value": false, "score": 4.76837158203125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9997071623802185}, "created": {"value": true, "score": 0.9999213218688965}, "shared": {"value": false, "score": 0.00011897087097167969}}, "references": [{"label": "[3]", "normalizedForm": "[3]", "refKey": 3}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Spark", "normalizedForm": "Spark", "offsetStart": 0, "offsetEnd": 5}, "context": "Spark provides an important abstraction, called resilient distributed dataset (RDD), which is a read-only and fault-tolerant collection of data elements (represented as key-value pairs) partitioned across the nodes of a shared-nothing cluster. ", "mentionContextAttributes": {"used": {"value": false, "score": 2.9087066650390625e-05}, "created": {"value": false, "score": 0.016987502574920654}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9997071623802185}, "created": {"value": true, "score": 0.9999213218688965}, "shared": {"value": false, "score": 0.00011897087097167969}}, "references": [{"label": "[3]", "normalizedForm": "[3]", "refKey": 3}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Spark", "normalizedForm": "Spark", "offsetStart": 0, "offsetEnd": 5}, "context": "Spark provides a functional-style programming interface with various operations to execute a user-provided function in parallel on an RDD. ", "mentionContextAttributes": {"used": {"value": false, "score": 4.172325134277344e-05}, "created": {"value": false, "score": 0.0009360909461975098}, "shared": {"value": false, "score": 1.5497207641601562e-06}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9997071623802185}, "created": {"value": true, "score": 0.9999213218688965}, "shared": {"value": false, "score": 0.00011897087097167969}}, "references": [{"label": "[3]", "normalizedForm": "[3]", "refKey": 3}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Spark", "normalizedForm": "Spark", "offsetStart": 0, "offsetEnd": 5}, "context": "Spark data can be stored in the Hadoop Distributed File System (HDFS) [41], a popular open source file system inspired by Google File System [22].", "mentionContextAttributes": {"used": {"value": false, "score": 0.0076784491539001465}, "created": {"value": false, "score": 0.00010371208190917969}, "shared": {"value": false, "score": 0.00011897087097167969}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9997071623802185}, "created": {"value": true, "score": 0.9999213218688965}, "shared": {"value": false, "score": 0.00011897087097167969}}, "references": [{"label": "[3]", "normalizedForm": "[3]", "refKey": 3}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Slice", "normalizedForm": "Slice", "offsetStart": 0, "offsetEnd": 5}, "version": {"rawForm": "201", "normalizedForm": "201"}, "context": "Slice 201 in Section 7) as the region to calculate PDFs.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9999680519104004}, "created": {"value": false, "score": 7.510185241699219e-06}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999680519104004}, "created": {"value": false, "score": 0.10225743055343628}, "shared": {"value": false, "score": 2.0265579223632812e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Spark", "normalizedForm": "Spark", "offsetStart": 0, "offsetEnd": 5}, "context": "Spark and HDFS are deployed over the nodes of the cluster.", "mentionContextAttributes": {"used": {"value": false, "score": 0.15886086225509644}, "created": {"value": false, "score": 6.258487701416016e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9997071623802185}, "created": {"value": true, "score": 0.9999213218688965}, "shared": {"value": false, "score": 0.00011897087097167969}}, "references": [{"label": "[3]", "normalizedForm": "[3]", "refKey": 3}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Slice", "normalizedForm": "Slice", "offsetStart": 0, "offsetEnd": 5}, "version": {"rawForm": "201", "normalizedForm": "201"}, "context": "Slice 0, to generate decision tree model and use the model to calculate PDFs of the points in other slices, e.g.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9648339748382568}, "created": {"value": false, "score": 1.9550323486328125e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999680519104004}, "created": {"value": false, "score": 0.10225743055343628}, "shared": {"value": false, "score": 2.0265579223632812e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Slice", "normalizedForm": "Slice", "offsetStart": 0, "offsetEnd": 5}, "version": {"rawForm": "201", "normalizedForm": "201"}, "context": "Slice 201.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9359207153320312}, "created": {"value": false, "score": 0.00025707483291625977}, "shared": {"value": false, "score": 2.0265579223632812e-06}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999680519104004}, "created": {"value": false, "score": 0.10225743055343628}, "shared": {"value": false, "score": 2.0265579223632812e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Slice", "normalizedForm": "Slice", "offsetStart": 0, "offsetEnd": 5}, "version": {"rawForm": "201", "normalizedForm": "201"}, "context": "Slice 201.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9359207153320312}, "created": {"value": false, "score": 0.00025707483291625977}, "shared": {"value": false, "score": 2.0265579223632812e-06}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999680519104004}, "created": {"value": false, "score": 0.10225743055343628}, "shared": {"value": false, "score": 2.0265579223632812e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "RawData", "normalizedForm": "RawData", "offsetStart": 0, "offsetEnd": 7}, "context": "RawData \u2190 windowData \u222a RawData 18: end while end considered in T ypes, the longer the execution time of Algorithm 3 is. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.8075495362281799}, "created": {"value": false, "score": 2.5033950805664062e-06}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999734163284302}, "created": {"value": false, "score": 1.7404556274414062e-05}, "shared": {"value": false, "score": 5.960464477539062e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "RawData", "normalizedForm": "RawData", "offsetStart": 0, "offsetEnd": 7}, "context": "RawData \u2190 rd \u222a RawData 13: end for 14: P oints \u2190 selectByGrouping(RawData) 15: allT ypes \u2190 \u2205 16: for each p \u2208 P oints do 17:", "mentionContextAttributes": {"used": {"value": true, "score": 0.9909045100212097}, "created": {"value": false, "score": 1.1920928955078125e-06}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999734163284302}, "created": {"value": false, "score": 1.7404556274414062e-05}, "shared": {"value": false, "score": 5.960464477539062e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Spark", "normalizedForm": "Spark", "offsetStart": 0, "offsetEnd": 10}, "context": "Spark [49] is an Apache open-source data processing framework. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.00038892030715942383}, "created": {"value": false, "score": 2.1457672119140625e-05}, "shared": {"value": false, "score": 1.3113021850585938e-06}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9997071623802185}, "created": {"value": true, "score": 0.9999213218688965}, "shared": {"value": false, "score": 0.00011897087097167969}}, "references": [{"label": "[3]", "normalizedForm": "[3]", "refKey": 3}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "windowData", "normalizedForm": "windowData", "offsetStart": 0, "offsetEnd": 10}, "context": "windowData \u2190 rd \u222a windowData 15:", "mentionContextAttributes": {"used": {"value": true, "score": 0.7321558594703674}, "created": {"value": false, "score": 2.6226043701171875e-06}, "shared": {"value": false, "score": 2.2649765014648438e-06}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999157190322876}, "created": {"value": false, "score": 5.4836273193359375e-06}, "shared": {"value": false, "score": 2.2649765014648438e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Spark", "normalizedForm": "Spark", "offsetStart": 3, "offsetEnd": 8}, "context": "In Spark, there are two types of operations: transformations, which create a new dataset from an existing one ( e.g. ", "mentionContextAttributes": {"used": {"value": false, "score": 5.340576171875e-05}, "created": {"value": false, "score": 0.0003007650375366211}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9997071623802185}, "created": {"value": true, "score": 0.9999213218688965}, "shared": {"value": false, "score": 0.00011897087097167969}}, "references": [{"label": "[3]", "normalizedForm": "[3]", "refKey": 3}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Spark", "normalizedForm": "Spark", "offsetStart": 5, "offsetEnd": 10}, "context": "In a Spark cluster, a master node is used to coordinate job execution while worker nodes are in charge of executing the parallel operations.", "mentionContextAttributes": {"used": {"value": false, "score": 0.0011205077171325684}, "created": {"value": false, "score": 0.0004101395606994629}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9997071623802185}, "created": {"value": true, "score": 0.9999213218688965}, "shared": {"value": false, "score": 0.00011897087097167969}}, "references": [{"label": "[3]", "normalizedForm": "[3]", "refKey": 3}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "windowData", "normalizedForm": "windowData", "offsetStart": 6, "offsetEnd": 16}, "context": "Cache(windowData) 17:", "mentionContextAttributes": {"used": {"value": true, "score": 0.9999157190322876}, "created": {"value": false, "score": 5.4836273193359375e-06}, "shared": {"value": false, "score": 1.1920928955078125e-06}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999157190322876}, "created": {"value": false, "score": 5.4836273193359375e-06}, "shared": {"value": false, "score": 2.2649765014648438e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Spark", "normalizedForm": "Spark", "offsetStart": 7, "offsetEnd": 12}, "context": "We use Spark as our execution environment for computing PDFs. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.015403211116790771}, "created": {"value": false, "score": 0.032768309116363525}, "shared": {"value": false, "score": 4.76837158203125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9997071623802185}, "created": {"value": true, "score": 0.9999213218688965}, "shared": {"value": false, "score": 0.00011897087097167969}}, "references": [{"label": "[3]", "normalizedForm": "[3]", "refKey": 3}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "program", "normalizedForm": "program", "offsetStart": 7, "offsetEnd": 14}, "context": "A Java program is called in the Map function to read the data at a specific position instead of loading all the data.", "mentionContextAttributes": {"used": {"value": false, "score": 0.00084686279296875}, "created": {"value": false, "score": 0.22242999076843262}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999203681945801}, "created": {"value": true, "score": 0.9999256134033203}, "shared": {"value": false, "score": 4.76837158203125e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "windowData", "normalizedForm": "windowData", "offsetStart": 10, "offsetEnd": 20}, "context": "RawData \u2190 windowData \u222a RawData 18: end while end considered in T ypes, the longer the execution time of Algorithm 3 is. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.8075495362281799}, "created": {"value": false, "score": 2.5033950805664062e-06}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999157190322876}, "created": {"value": false, "score": 5.4836273193359375e-06}, "shared": {"value": false, "score": 2.2649765014648438e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "MapReduce", "normalizedForm": "MapReduce", "offsetStart": 14, "offsetEnd": 23}, "context": "Compared with MapReduce, it improves the ease of use with the Scala language (a functional extension of Java) and a rich set of operators (Map, Reduce, Filter, Join, Aggregate, Count, etc.). ", "mentionContextAttributes": {"used": {"value": false, "score": 7.772445678710938e-05}, "created": {"value": false, "score": 0.0003584623336791992}, "shared": {"value": false, "score": 5.960464477539062e-07}}, "documentContextAttributes": {"used": {"value": false, "score": 7.772445678710938e-05}, "created": {"value": false, "score": 0.00041109323501586914}, "shared": {"value": false, "score": 5.960464477539062e-07}}, "references": [{"label": "[16]", "normalizedForm": "[16]", "refKey": 16}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "GLDEX", "normalizedForm": "GLDEX", "offsetStart": 15, "offsetEnd": 20}, "context": "R provides the GLDEX library that supports the Generalized Lambda Distribution (GLD) family of PDFs. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.0014507174491882324}, "created": {"value": false, "score": 0.00015556812286376953}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "documentContextAttributes": {"used": {"value": false, "score": 0.0014507174491882324}, "created": {"value": false, "score": 0.00015556812286376953}, "shared": {"value": false, "score": 3.5762786865234375e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Scala", "normalizedForm": "Scala", "offsetStart": 15, "offsetEnd": 20}, "context": "We execute the Scala program with Grouping (with 4 -types and without ML prediction) for two windows while each window is composed of different numbers of lines. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.002305924892425537}, "created": {"value": false, "score": 0.005843997001647949}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9950926303863525}, "created": {"value": true, "score": 0.9999256134033203}, "shared": {"value": false, "score": 5.960464477539062e-07}}}, {"type": "software", "software-type": "implicit", "software-name": {"rawForm": "program", "normalizedForm": "program", "offsetStart": 15, "offsetEnd": 22}, "context": "We developed a program written in Scala, which realizes the functionality of different methods to compute PDFs. ", "mentionContextAttributes": {"used": {"value": false, "score": 2.658367156982422e-05}, "created": {"value": true, "score": 0.9999256134033203}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999203681945801}, "created": {"value": true, "score": 0.9999256134033203}, "shared": {"value": false, "score": 4.76837158203125e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "RawData", "normalizedForm": "RawData", "offsetStart": 15, "offsetEnd": 22}, "context": "RawData \u2190 rd \u222a RawData 13: end for 14: P oints \u2190 selectByGrouping(RawData) 15: allT ypes \u2190 \u2205 16: for each p \u2208 P oints do 17:", "mentionContextAttributes": {"used": {"value": true, "score": 0.9909045100212097}, "created": {"value": false, "score": 1.1920928955078125e-06}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999734163284302}, "created": {"value": false, "score": 1.7404556274414062e-05}, "shared": {"value": false, "score": 5.960464477539062e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "program", "normalizedForm": "program", "offsetStart": 15, "offsetEnd": 22}, "context": "We execute the program for the points of the first 6 lines in Slice 201 using Baseline, Grouping, Reuse, ML and the combination of these methods with ML.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9730685353279114}, "created": {"value": false, "score": 8.678436279296875e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999203681945801}, "created": {"value": true, "score": 0.9999256134033203}, "shared": {"value": false, "score": 4.76837158203125e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "MapReduce", "normalizedForm": "MapReduce", "offsetStart": 15, "offsetEnd": 24}, "context": "It extends the MapReduce model [16] for two important classes of analytics applications: iterative processing (machine learning, graph processing) and interactive data mining (with R, Excel or Python). ", "mentionContextAttributes": {"used": {"value": false, "score": 4.088878631591797e-05}, "created": {"value": false, "score": 0.00041109323501586914}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": false, "score": 7.772445678710938e-05}, "created": {"value": false, "score": 0.00041109323501586914}, "shared": {"value": false, "score": 5.960464477539062e-07}}, "references": [{"label": "[16]", "normalizedForm": "[16]", "refKey": 16, "offsetStart": 14771, "offsetEnd": 14775}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Slice", "normalizedForm": "Slice", "offsetStart": 17, "offsetEnd": 22}, "version": {"rawForm": "201", "normalizedForm": "201"}, "context": "Execution of One Slice In this section, we compare the performance of different methods to execute the program for the points of the whole slice, i.e.", "mentionContextAttributes": {"used": {"value": false, "score": 0.3056541681289673}, "created": {"value": false, "score": 2.968311309814453e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999680519104004}, "created": {"value": false, "score": 0.10225743055343628}, "shared": {"value": false, "score": 2.0265579223632812e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Apache", "normalizedForm": "Apache", "offsetStart": 17, "offsetEnd": 23}, "context": "Spark [49] is an Apache open-source data processing framework. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.00038892030715942383}, "created": {"value": false, "score": 2.1457672119140625e-05}, "shared": {"value": false, "score": 1.3113021850585938e-06}}, "documentContextAttributes": {"used": {"value": false, "score": 0.00038892030715942383}, "created": {"value": false, "score": 2.1457672119140625e-05}, "shared": {"value": false, "score": 1.3113021850585938e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "windowData", "normalizedForm": "windowData", "offsetStart": 18, "offsetEnd": 28}, "context": "windowData \u2190 rd \u222a windowData 15:", "mentionContextAttributes": {"used": {"value": true, "score": 0.7321558594703674}, "created": {"value": false, "score": 2.6226043701171875e-06}, "shared": {"value": false, "score": 2.2649765014648438e-06}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999157190322876}, "created": {"value": false, "score": 5.4836273193359375e-06}, "shared": {"value": false, "score": 2.2649765014648438e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "program", "normalizedForm": "program", "offsetStart": 20, "offsetEnd": 27}, "context": "Now, we execute the program for all points in Slice 201.", "mentionContextAttributes": {"used": {"value": false, "score": 0.015969157218933105}, "created": {"value": false, "score": 0.10225743055343628}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999203681945801}, "created": {"value": true, "score": 0.9999256134033203}, "shared": {"value": false, "score": 4.76837158203125e-07}}}, {"type": "software", "software-type": "environment", "software-name": {"rawForm": "MATLAB", "normalizedForm": "MATLAB", "offsetStart": 21, "offsetEnd": 27}, "context": "Then, we execute the MATLAB programs of the seismic benchmark to generate a spatial dataset. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9705975651741028}, "created": {"value": true, "score": 0.8872366547584534}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9705975651741028}, "created": {"value": true, "score": 0.8872366547584534}, "shared": {"value": false, "score": 1.0728836059570312e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "program", "normalizedForm": "program", "offsetStart": 21, "offsetEnd": 28}, "context": "We execute the Scala program with Grouping (with 4 -types and without ML prediction) for two windows while each window is composed of different numbers of lines.", "mentionContextAttributes": {"used": {"value": false, "score": 0.002305924892425537}, "created": {"value": false, "score": 0.005843997001647949}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999203681945801}, "created": {"value": true, "score": 0.9999256134033203}, "shared": {"value": false, "score": 4.76837158203125e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Scala", "normalizedForm": "Scala", "offsetStart": 22, "offsetEnd": 27}, "context": "First, we execute the Scala program (for computing PDFs with different methods) on a small workload (6 lines and 3006 points). ", "mentionContextAttributes": {"used": {"value": false, "score": 0.01750272512435913}, "created": {"value": false, "score": 0.26194846630096436}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9950926303863525}, "created": {"value": true, "score": 0.9999256134033203}, "shared": {"value": false, "score": 5.960464477539062e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "RawData", "normalizedForm": "RawData", "offsetStart": 22, "offsetEnd": 29}, "context": "type \u2190 predict(model, RawData) 18:", "mentionContextAttributes": {"used": {"value": true, "score": 0.9973390698432922}, "created": {"value": false, "score": 3.933906555175781e-06}, "shared": {"value": false, "score": 5.960464477539062e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999734163284302}, "created": {"value": false, "score": 1.7404556274414062e-05}, "shared": {"value": false, "score": 5.960464477539062e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "program", "normalizedForm": "program", "offsetStart": 22, "offsetEnd": 29}, "context": "First, we execute the program for the points of the first 2 lines in Slice 201 in a cluster of 30 nodes.", "mentionContextAttributes": {"used": {"value": true, "score": 0.8204200267791748}, "created": {"value": false, "score": 0.019007980823516846}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999203681945801}, "created": {"value": true, "score": 0.9999256134033203}, "shared": {"value": false, "score": 4.76837158203125e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Slice", "normalizedForm": "Slice", "offsetStart": 23, "offsetEnd": 28}, "version": {"rawForm": "201", "normalizedForm": "201"}, "context": "We use the same slice (Slice 201 because it has interesting information) in all the experiments.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9973152279853821}, "created": {"value": false, "score": 6.9141387939453125e-06}, "shared": {"value": false, "score": 4.76837158203125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999680519104004}, "created": {"value": false, "score": 0.10225743055343628}, "shared": {"value": false, "score": 2.0265579223632812e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "RawData", "normalizedForm": "RawData", "offsetStart": 23, "offsetEnd": 30}, "context": "RawData \u2190 windowData \u222a RawData 18: end while end considered in T ypes, the longer the execution time of Algorithm 3 is. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.8075495362281799}, "created": {"value": false, "score": 2.5033950805664062e-06}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999734163284302}, "created": {"value": false, "score": 1.7404556274414062e-05}, "shared": {"value": false, "score": 5.960464477539062e-07}}}, {"type": "software", "software-type": "environment", "software-name": {"rawForm": "MATLAB", "normalizedForm": "MATLAB", "offsetStart": 24, "offsetEnd": 30}, "context": "This benchmark includes MATLAB models for seismic wave propagation. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.00015270709991455078}, "created": {"value": false, "score": 6.99758529663086e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9705975651741028}, "created": {"value": true, "score": 0.8872366547584534}, "shared": {"value": false, "score": 1.0728836059570312e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "RawData", "normalizedForm": "RawData", "offsetStart": 25, "offsetEnd": 32}, "context": "P oints \u2190 Select(window, RawData) 7:", "mentionContextAttributes": {"used": {"value": true, "score": 0.9876948595046997}, "created": {"value": false, "score": 4.410743713378906e-06}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999734163284302}, "created": {"value": false, "score": 1.7404556274414062e-05}, "shared": {"value": false, "score": 5.960464477539062e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Spark", "normalizedForm": "Spark", "offsetStart": 26, "offsetEnd": 31}, "context": "The main reason to choose Spark is that, unlike MPI, it makes it easy to parallelize application code in a cluster. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.0024656057357788086}, "created": {"value": false, "score": 0.0002930760383605957}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9997071623802185}, "created": {"value": true, "score": 0.9999213218688965}, "shared": {"value": false, "score": 0.00011897087097167969}}, "references": [{"label": "[3]", "normalizedForm": "[3]", "refKey": 3}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Spark", "normalizedForm": "Spark", "offsetStart": 26, "offsetEnd": 31}, "context": "In this paper, we exploit Spark MLlib [3], a scalable machine learning (ML) library that can handle big data [28] in memory by exploiting Spark RDDs and Spark operations.", "mentionContextAttributes": {"used": {"value": false, "score": 0.03300833702087402}, "created": {"value": true, "score": 0.9990726709365845}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9997071623802185}, "created": {"value": true, "score": 0.9999213218688965}, "shared": {"value": false, "score": 0.00011897087097167969}}, "references": [{"label": "[3]", "normalizedForm": "[3]", "refKey": 3, "offsetStart": 17081, "offsetEnd": 17084}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "program", "normalizedForm": "program", "offsetStart": 28, "offsetEnd": 35}, "context": "First, we execute the Scala program (for computing PDFs with different methods) on a small workload (6 lines and 3006 points).", "mentionContextAttributes": {"used": {"value": false, "score": 0.01750272512435913}, "created": {"value": false, "score": 0.26194846630096436}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999203681945801}, "created": {"value": true, "score": 0.9999256134033203}, "shared": {"value": false, "score": 4.76837158203125e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Slice", "normalizedForm": "Slice", "offsetStart": 29, "offsetEnd": 34}, "version": {"rawForm": "201", "normalizedForm": "201"}, "context": "Then, the average error E of Slice i can be calculated according to Equation 6.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9668933153152466}, "created": {"value": false, "score": 3.421306610107422e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999680519104004}, "created": {"value": false, "score": 0.10225743055343628}, "shared": {"value": false, "score": 2.0265579223632812e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "RawData", "normalizedForm": "RawData", "offsetStart": 29, "offsetEnd": 36}, "context": "d \u2190 getObservationV alues(p, RawData) 9:", "mentionContextAttributes": {"used": {"value": true, "score": 0.9646069407463074}, "created": {"value": false, "score": 1.1920928955078125e-06}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999734163284302}, "created": {"value": false, "score": 1.7404556274414062e-05}, "shared": {"value": false, "score": 5.960464477539062e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "program", "normalizedForm": "program", "offsetStart": 31, "offsetEnd": 38}, "context": "The fitdistr function in the R program can only fit the parameters corresponding to a specific distribution type to calculate the PDF based on a set of observation values.", "mentionContextAttributes": {"used": {"value": false, "score": 0.31551897525787354}, "created": {"value": false, "score": 1.2993812561035156e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999203681945801}, "created": {"value": true, "score": 0.9999256134033203}, "shared": {"value": false, "score": 4.76837158203125e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Hadoop", "normalizedForm": "Hadoop", "offsetStart": 32, "offsetEnd": 38}, "context": "Spark data can be stored in the Hadoop Distributed File System (HDFS) [41], a popular open source file system inspired by Google File System [22]. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.0076784491539001465}, "created": {"value": false, "score": 0.00010371208190917969}, "shared": {"value": false, "score": 0.00011897087097167969}}, "documentContextAttributes": {"used": {"value": false, "score": 0.0076784491539001465}, "created": {"value": false, "score": 0.00010371208190917969}, "shared": {"value": false, "score": 0.00011897087097167969}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Scala", "normalizedForm": "Scala", "offsetStart": 34, "offsetEnd": 39}, "context": "We developed a program written in Scala, which realizes the functionality of different methods to compute PDFs. ", "mentionContextAttributes": {"used": {"value": false, "score": 2.658367156982422e-05}, "created": {"value": true, "score": 0.9999256134033203}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9950926303863525}, "created": {"value": true, "score": 0.9999256134033203}, "shared": {"value": false, "score": 5.960464477539062e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Spark", "normalizedForm": "Spark", "offsetStart": 37, "offsetEnd": 42}, "context": "An HDFS file can be represented as a Spark RDD and processed in parallel.", "mentionContextAttributes": {"used": {"value": false, "score": 0.0004189610481262207}, "created": {"value": false, "score": 2.372264862060547e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9997071623802185}, "created": {"value": true, "score": 0.9999213218688965}, "shared": {"value": false, "score": 0.00011897087097167969}}, "references": [{"label": "[3]", "normalizedForm": "[3]", "refKey": 3}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "program", "normalizedForm": "program", "offsetStart": 39, "offsetEnd": 46}, "context": "This can be achieved by executing an R program.", "mentionContextAttributes": {"used": {"value": false, "score": 0.000756382942199707}, "created": {"value": false, "score": 0.10658669471740723}, "shared": {"value": false, "score": 4.76837158203125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999203681945801}, "created": {"value": true, "score": 0.9999256134033203}, "shared": {"value": false, "score": 4.76837158203125e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Spark", "normalizedForm": "Spark", "offsetStart": 40, "offsetEnd": 45}, "context": "Section 3 introduces some background on Spark.", "mentionContextAttributes": {"used": {"value": false, "score": 0.00017303228378295898}, "created": {"value": false, "score": 0.006186485290527344}, "shared": {"value": false, "score": 5.960464477539062e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9997071623802185}, "created": {"value": true, "score": 0.9999213218688965}, "shared": {"value": false, "score": 0.00011897087097167969}}, "references": [{"label": "[3]", "normalizedForm": "[3]", "refKey": 3}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Spark", "normalizedForm": "Spark", "offsetStart": 40, "offsetEnd": 45}, "context": "We proposed a parallel solution using a Spark cluster with three new methods: Grouping, ML and Sampling.", "mentionContextAttributes": {"used": {"value": false, "score": 0.00012969970703125}, "created": {"value": true, "score": 0.999091386795044}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9997071623802185}, "created": {"value": true, "score": 0.9999213218688965}, "shared": {"value": false, "score": 0.00011897087097167969}}, "references": [{"label": "[3]", "normalizedForm": "[3]", "refKey": 3}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "UQlab", "normalizedForm": "UQlab", "offsetStart": 41, "offsetEnd": 46}, "context": "To generate the spatial data, we use the UQlab framework [32] to produce 16 values as the input parameters, i.e. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9996551275253296}, "created": {"value": false, "score": 6.079673767089844e-06}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9996551275253296}, "created": {"value": false, "score": 6.079673767089844e-06}, "shared": {"value": false, "score": 1.1920928955078125e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "program", "normalizedForm": "program", "offsetStart": 41, "offsetEnd": 48}, "context": "Finally, the output data of the external program is transformed to key-value pairs and stored in RDDs by the same Map operation, which executes the external program.", "mentionContextAttributes": {"used": {"value": false, "score": 0.29698705673217773}, "created": {"value": false, "score": 5.543231964111328e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999203681945801}, "created": {"value": true, "score": 0.9999256134033203}, "shared": {"value": false, "score": 4.76837158203125e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Scala", "normalizedForm": "Scala", "offsetStart": 44, "offsetEnd": 49}, "context": "To find an optimal window size, we test the Scala program on a small workload (with a small number of points) with different window sizes, and then use the optimal size for the PDF computation of all the points in the slice.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9950926303863525}, "created": {"value": false, "score": 9.34600830078125e-05}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9950926303863525}, "created": {"value": true, "score": 0.9999256134033203}, "shared": {"value": false, "score": 5.960464477539062e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Spark", "normalizedForm": "Spark", "offsetStart": 46, "offsetEnd": 51}, "context": "In addition to deploying PDF computation over Spark, we propose three new methods to efficiently compute PDFs: data grouping, ML prediction and sampling. ", "mentionContextAttributes": {"used": {"value": false, "score": 7.224082946777344e-05}, "created": {"value": true, "score": 0.9996557235717773}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9997071623802185}, "created": {"value": true, "score": 0.9999213218688965}, "shared": {"value": false, "score": 0.00011897087097167969}}, "references": [{"label": "[3]", "normalizedForm": "[3]", "refKey": 3}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Slice", "normalizedForm": "Slice", "offsetStart": 46, "offsetEnd": 51}, "version": {"rawForm": "201", "normalizedForm": "201", "offsetStart": 52, "offsetEnd": 55}, "context": "Now, we execute the program for all points in Slice 201. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.015969157218933105}, "created": {"value": false, "score": 0.10225743055343628}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999680519104004}, "created": {"value": false, "score": 0.10225743055343628}, "shared": {"value": false, "score": 2.0265579223632812e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Spark", "normalizedForm": "Spark", "offsetStart": 46, "offsetEnd": 51}, "context": "This process is realized in a Map function in Spark.", "mentionContextAttributes": {"used": {"value": false, "score": 0.00026792287826538086}, "created": {"value": false, "score": 0.09914207458496094}, "shared": {"value": false, "score": 4.76837158203125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9997071623802185}, "created": {"value": true, "score": 0.9999213218688965}, "shared": {"value": false, "score": 0.00011897087097167969}}, "references": [{"label": "[3]", "normalizedForm": "[3]", "refKey": 3}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "program", "normalizedForm": "program", "offsetStart": 46, "offsetEnd": 53}, "context": "This is expected since we avoid calling the R program to compute PDFs and use a decision tree model to predict the distribution type of each point.", "mentionContextAttributes": {"used": {"value": false, "score": 0.009374499320983887}, "created": {"value": false, "score": 0.3147410750389099}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999203681945801}, "created": {"value": true, "score": 0.9999256134033203}, "shared": {"value": false, "score": 4.76837158203125e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Scala", "normalizedForm": "Scala", "offsetStart": 47, "offsetEnd": 52}, "context": "Once the method to compute PDFs is chosen, the Scala program is executed as a job within Spark.", "mentionContextAttributes": {"used": {"value": false, "score": 0.0013257265090942383}, "created": {"value": false, "score": 0.005249083042144775}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9950926303863525}, "created": {"value": true, "score": 0.9999256134033203}, "shared": {"value": false, "score": 5.960464477539062e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "MPI", "normalizedForm": "MPI", "offsetStart": 48, "offsetEnd": 51}, "context": "The main reason to choose Spark is that, unlike MPI, it makes it easy to parallelize application code in a cluster. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.0024656057357788086}, "created": {"value": false, "score": 0.0002930760383605957}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "documentContextAttributes": {"used": {"value": false, "score": 0.0024656057357788086}, "created": {"value": false, "score": 0.0002930760383605957}, "shared": {"value": false, "score": 3.5762786865234375e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Slice", "normalizedForm": "Slice", "offsetStart": 48, "offsetEnd": 53}, "version": {"rawForm": "201", "normalizedForm": "201"}, "context": "Based on multiple datasets, each point p x,y in Slice i corresponds to a set of values V = {v 1 , v 2 , ..., v n } while v k is the observation value corresponding to the point p x,y in Slice i in d k \u2208 DS and n is the number of observation values in the set (the same for Equations 1, 2 and 5).", "mentionContextAttributes": {"used": {"value": true, "score": 0.9975537657737732}, "created": {"value": false, "score": 3.5762786865234375e-06}, "shared": {"value": false, "score": 1.6689300537109375e-06}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999680519104004}, "created": {"value": false, "score": 0.10225743055343628}, "shared": {"value": false, "score": 2.0265579223632812e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Spark", "normalizedForm": "Spark", "offsetStart": 50, "offsetEnd": 55}, "context": "Finally, the loaded data is cached in memory in a Spark RDD (Line 16).", "mentionContextAttributes": {"used": {"value": true, "score": 0.723479688167572}, "created": {"value": false, "score": 6.628036499023438e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9997071623802185}, "created": {"value": true, "score": 0.9999213218688965}, "shared": {"value": false, "score": 0.00011897087097167969}}, "references": [{"label": "[3]", "normalizedForm": "[3]", "refKey": 3}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "program", "normalizedForm": "program", "offsetStart": 50, "offsetEnd": 57}, "context": "To find an optimal window size, we test the Scala program on a small workload (with a small number of points) with different window sizes, and then use the optimal size for the PDF computation of all the points in the slice.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9950926303863525}, "created": {"value": false, "score": 9.34600830078125e-05}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999203681945801}, "created": {"value": true, "score": 0.9999256134033203}, "shared": {"value": false, "score": 4.76837158203125e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Spark", "normalizedForm": "Spark", "offsetStart": 53, "offsetEnd": 58}, "context": "We use two cluster testbeds, each with NFS, HDFS and Spark deployed.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9986380934715271}, "created": {"value": false, "score": 3.0159950256347656e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9997071623802185}, "created": {"value": true, "score": 0.9999213218688965}, "shared": {"value": false, "score": 0.00011897087097167969}}, "references": [{"label": "[3]", "normalizedForm": "[3]", "refKey": 3}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "program", "normalizedForm": "program", "offsetStart": 53, "offsetEnd": 60}, "context": "Once the method to compute PDFs is chosen, the Scala program is executed as a job within Spark.", "mentionContextAttributes": {"used": {"value": false, "score": 0.0013257265090942383}, "created": {"value": false, "score": 0.005249083042144775}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999203681945801}, "created": {"value": true, "score": 0.9999256134033203}, "shared": {"value": false, "score": 4.76837158203125e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Spark", "normalizedForm": "Spark", "offsetStart": 55, "offsetEnd": 60}, "context": "This loading process is realized by a Map operation in Spark, which is fully parallel.", "mentionContextAttributes": {"used": {"value": false, "score": 0.004447519779205322}, "created": {"value": false, "score": 0.11294102668762207}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9997071623802185}, "created": {"value": true, "score": 0.9999213218688965}, "shared": {"value": false, "score": 0.00011897087097167969}}, "references": [{"label": "[3]", "normalizedForm": "[3]", "refKey": 3}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "program", "normalizedForm": "program", "offsetStart": 56, "offsetEnd": 63}, "context": "Since the PDF computation is implemented by an external program (in R), we call it within the Map operation for the parallel processing of PDFs.", "mentionContextAttributes": {"used": {"value": false, "score": 0.0536157488822937}, "created": {"value": false, "score": 0.31956642866134644}, "shared": {"value": false, "score": 4.76837158203125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999203681945801}, "created": {"value": true, "score": 0.9999256134033203}, "shared": {"value": false, "score": 4.76837158203125e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Spark", "normalizedForm": "Spark", "offsetStart": 59, "offsetEnd": 64}, "context": "During execution, some intermediate data that is stored in Spark RDDs can be cached using the Spark Cache operation, which stores RDD data in main memory.", "mentionContextAttributes": {"used": {"value": false, "score": 0.017237603664398193}, "created": {"value": false, "score": 4.124641418457031e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9997071623802185}, "created": {"value": true, "score": 0.9999213218688965}, "shared": {"value": false, "score": 0.00011897087097167969}}, "references": [{"label": "[3]", "normalizedForm": "[3]", "refKey": 3}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Spark", "normalizedForm": "Spark", "offsetStart": 60, "offsetEnd": 65}, "context": "Section 5 presents our architecture for computing PDFs with Spark, with its main functions.", "mentionContextAttributes": {"used": {"value": false, "score": 0.00019401311874389648}, "created": {"value": true, "score": 0.9987782835960388}, "shared": {"value": false, "score": 8.344650268554688e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9997071623802185}, "created": {"value": true, "score": 0.9999213218688965}, "shared": {"value": false, "score": 0.00011897087097167969}}, "references": [{"label": "[3]", "normalizedForm": "[3]", "refKey": 3}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Spark", "normalizedForm": "Spark", "offsetStart": 60, "offsetEnd": 65}, "context": "To validate our solution, we implemented these methods in a Spark cluster and performed extensive experiments on two different clusters (with 6 and 64 nodes) using big spatial data ranging from hundreds of GB to several TB.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9997071623802185}, "created": {"value": true, "score": 0.5761113166809082}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9997071623802185}, "created": {"value": true, "score": 0.9999213218688965}, "shared": {"value": false, "score": 0.00011897087097167969}}, "references": [{"label": "[3]", "normalizedForm": "[3]", "refKey": 3}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Spark", "normalizedForm": "Spark", "offsetStart": 61, "offsetEnd": 66}, "context": "The grouping can be realized by the aggregation operation in Spark.", "mentionContextAttributes": {"used": {"value": false, "score": 0.0008006691932678223}, "created": {"value": false, "score": 0.0010706782341003418}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9997071623802185}, "created": {"value": true, "score": 0.9999213218688965}, "shared": {"value": false, "score": 0.00011897087097167969}}, "references": [{"label": "[3]", "normalizedForm": "[3]", "refKey": 3}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Slice", "normalizedForm": "Slice", "offsetStart": 61, "offsetEnd": 66}, "version": {"rawForm": "201", "normalizedForm": "201"}, "context": "In addition, we can find that the correlation information in Slice 0 can always be used in our targeted slice with a few acceptable extra errors as shown in Section 7. According to Equation 5 end", "mentionContextAttributes": {"used": {"value": false, "score": 0.021724581718444824}, "created": {"value": false, "score": 1.2874603271484375e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999680519104004}, "created": {"value": false, "score": 0.10225743055343628}, "shared": {"value": false, "score": 2.0265579223632812e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Scala", "normalizedForm": "Scala", "offsetStart": 62, "offsetEnd": 67}, "context": "Compared with MapReduce, it improves the ease of use with the Scala language (a functional extension of Java) and a rich set of operators (Map, Reduce, Filter, Join, Aggregate, Count, etc.). ", "mentionContextAttributes": {"used": {"value": false, "score": 7.772445678710938e-05}, "created": {"value": false, "score": 0.0003584623336791992}, "shared": {"value": false, "score": 5.960464477539062e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9950926303863525}, "created": {"value": true, "score": 0.9999256134033203}, "shared": {"value": false, "score": 5.960464477539062e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Slice", "normalizedForm": "Slice", "offsetStart": 62, "offsetEnd": 67}, "version": {"rawForm": "201", "normalizedForm": "201"}, "context": "We execute the program for the points of the first 6 lines in Slice 201 using Baseline, Grouping, Reuse, ML and the combination of these methods with ML.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9730685353279114}, "created": {"value": false, "score": 8.678436279296875e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999680519104004}, "created": {"value": false, "score": 0.10225743055343628}, "shared": {"value": false, "score": 2.0265579223632812e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Spark", "normalizedForm": "Spark", "offsetStart": 63, "offsetEnd": 68}, "context": "The grouping can be realized using an Aggregation operation in Spark.", "mentionContextAttributes": {"used": {"value": false, "score": 0.2696933150291443}, "created": {"value": false, "score": 0.0001220703125}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9997071623802185}, "created": {"value": true, "score": 0.9999213218688965}, "shared": {"value": false, "score": 0.00011897087097167969}}, "references": [{"label": "[3]", "normalizedForm": "[3]", "refKey": 3}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Spark", "normalizedForm": "Spark", "offsetStart": 65, "offsetEnd": 70}, "context": "With the input data stored in NFS, the NFS server is outside the Spark/HDFS cluster and takes care of file management services, including transferring the data that is read to the cluster nodes.", "mentionContextAttributes": {"used": {"value": false, "score": 0.14970064163208008}, "created": {"value": false, "score": 3.325939178466797e-05}, "shared": {"value": false, "score": 1.6689300537109375e-06}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9997071623802185}, "created": {"value": true, "score": 0.9999213218688965}, "shared": {"value": false, "score": 0.00011897087097167969}}, "references": [{"label": "[3]", "normalizedForm": "[3]", "refKey": 3}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Spark", "normalizedForm": "Spark", "offsetStart": 66, "offsetEnd": 71}, "context": "Note that the tumbling window is different from the data chunk in Spark.", "mentionContextAttributes": {"used": {"value": false, "score": 0.19988799095153809}, "created": {"value": false, "score": 2.1576881408691406e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9997071623802185}, "created": {"value": true, "score": 0.9999213218688965}, "shared": {"value": false, "score": 0.00011897087097167969}}, "references": [{"label": "[3]", "normalizedForm": "[3]", "refKey": 3}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "RawData", "normalizedForm": "RawData", "offsetStart": 66, "offsetEnd": 73}, "context": "RawData \u2190 rd \u222a RawData 13: end for 14: P oints \u2190 selectByGrouping(RawData) 15: allT ypes \u2190 \u2205 16: for each p \u2208 P oints do 17:", "mentionContextAttributes": {"used": {"value": true, "score": 0.9909045100212097}, "created": {"value": false, "score": 1.1920928955078125e-06}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999734163284302}, "created": {"value": false, "score": 1.7404556274414062e-05}, "shared": {"value": false, "score": 5.960464477539062e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Spark", "normalizedForm": "Spark", "offsetStart": 67, "offsetEnd": 72}, "context": "The basic processing of PDFs consists of data loading, from NFS to Spark RDDs, followed by PDF computation using Spark.", "mentionContextAttributes": {"used": {"value": false, "score": 0.00732654333114624}, "created": {"value": false, "score": 3.147125244140625e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9997071623802185}, "created": {"value": true, "score": 0.9999213218688965}, "shared": {"value": false, "score": 0.00011897087097167969}}, "references": [{"label": "[3]", "normalizedForm": "[3]", "refKey": 3}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Slice", "normalizedForm": "Slice", "offsetStart": 68, "offsetEnd": 73}, "version": {"rawForm": "201", "normalizedForm": "201"}, "context": "And the average mean (\u00b5 i ) and standard deviation (\u03c3 i ) values of Slice i can be calculated according to Equations 3 and 4, respectively.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9909405708312988}, "created": {"value": false, "score": 1.0848045349121094e-05}, "shared": {"value": false, "score": 4.76837158203125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999680519104004}, "created": {"value": false, "score": 0.10225743055343628}, "shared": {"value": false, "score": 2.0265579223632812e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Slice", "normalizedForm": "Slice", "offsetStart": 69, "offsetEnd": 74}, "version": {"rawForm": "201", "normalizedForm": "201"}, "context": "First, we execute the program for the points of the first 2 lines in Slice 201 in a cluster of 30 nodes.", "mentionContextAttributes": {"used": {"value": true, "score": 0.8204200267791748}, "created": {"value": false, "score": 0.019007980823516846}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999680519104004}, "created": {"value": false, "score": 0.10225743055343628}, "shared": {"value": false, "score": 2.0265579223632812e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "program", "normalizedForm": "program", "offsetStart": 72, "offsetEnd": 79}, "context": "We take 25 lines as the window size for PDF computation and execute the program with different methods for the whole slice, i.e. 11 windows of points in Slice 201.", "mentionContextAttributes": {"used": {"value": true, "score": 0.6930989027023315}, "created": {"value": false, "score": 3.981590270996094e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999203681945801}, "created": {"value": true, "score": 0.9999256134033203}, "shared": {"value": false, "score": 4.76837158203125e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Spark", "normalizedForm": "Spark", "offsetStart": 77, "offsetEnd": 82}, "context": "The infrastructure layer provides the basic execution environment, including Spark, HDFS and Network File System (NFS) in a cluster.", "mentionContextAttributes": {"used": {"value": false, "score": 0.003946900367736816}, "created": {"value": false, "score": 0.0007081031799316406}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9997071623802185}, "created": {"value": true, "score": 0.9999213218688965}, "shared": {"value": false, "score": 0.00011897087097167969}}, "references": [{"label": "[3]", "normalizedForm": "[3]", "refKey": 3}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Spark", "normalizedForm": "Spark", "offsetStart": 78, "offsetEnd": 83}, "context": "The loop of Lines 8-10 can be executed in parallel using the Map operation in Spark. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.18020057678222656}, "created": {"value": false, "score": 1.2755393981933594e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9997071623802185}, "created": {"value": true, "score": 0.9999213218688965}, "shared": {"value": false, "score": 0.00011897087097167969}}, "references": [{"label": "[3]", "normalizedForm": "[3]", "refKey": 3}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Spark", "normalizedForm": "Spark", "offsetStart": 87, "offsetEnd": 92}, "context": "For instance, we can use k-means [33], which is already implemented in parallel within Spark MLlib.", "mentionContextAttributes": {"used": {"value": false, "score": 0.0010513663291931152}, "created": {"value": false, "score": 0.0008811354637145996}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9997071623802185}, "created": {"value": true, "score": 0.9999213218688965}, "shared": {"value": false, "score": 0.00011897087097167969}}, "references": [{"label": "[3]", "normalizedForm": "[3]", "refKey": 3}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Spark", "normalizedForm": "Spark", "offsetStart": 87, "offsetEnd": 96}, "context": "In this paper, we propose a new solution to efficiently compute PDFs in parallel using Spark [49], a popular in-memory big data processing framework (see [29] for a survey on big data systems). ", "mentionContextAttributes": {"used": {"value": false, "score": 6.413459777832031e-05}, "created": {"value": true, "score": 0.9999028444290161}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9997071623802185}, "created": {"value": true, "score": 0.9999213218688965}, "shared": {"value": false, "score": 0.00011897087097167969}}, "references": [{"label": "[3]", "normalizedForm": "[3]", "refKey": 3}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "RawData", "normalizedForm": "RawData", "offsetStart": 88, "offsetEnd": 95}, "context": "T ypesP ercentage \u2190 pct \u222a T ypesP ercentage 24: end for 25: (\u00b5, \u03c3) \u2190 averageCalculation(RawData) end", "mentionContextAttributes": {"used": {"value": true, "score": 0.9999734163284302}, "created": {"value": false, "score": 3.2186508178710938e-06}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999734163284302}, "created": {"value": false, "score": 1.7404556274414062e-05}, "shared": {"value": false, "score": 5.960464477539062e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Spark", "normalizedForm": "Spark", "offsetStart": 89, "offsetEnd": 94}, "context": "Once the method to compute PDFs is chosen, the Scala program is executed as a job within Spark.", "mentionContextAttributes": {"used": {"value": false, "score": 0.0013257265090942383}, "created": {"value": false, "score": 0.005249083042144775}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9997071623802185}, "created": {"value": true, "score": 0.9999213218688965}, "shared": {"value": false, "score": 0.00011897087097167969}}, "references": [{"label": "[3]", "normalizedForm": "[3]", "refKey": 3}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Spark", "normalizedForm": "Spark", "offsetStart": 92, "offsetEnd": 97}, "context": "In this paper, we propose a new solution to efficiently compute such PDFs in parallel using Spark, with three methods: data grouping, machine learning prediction and sampling.", "mentionContextAttributes": {"used": {"value": false, "score": 5.4717063903808594e-05}, "created": {"value": true, "score": 0.9999213218688965}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9997071623802185}, "created": {"value": true, "score": 0.9999213218688965}, "shared": {"value": false, "score": 0.00011897087097167969}}, "references": [{"label": "[3]", "normalizedForm": "[3]", "refKey": 3}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Slice", "normalizedForm": "Slice", "offsetStart": 93, "offsetEnd": 98}, "version": {"rawForm": "201", "normalizedForm": "201"}, "context": "In addition, we also need to compute the statistical parameters of slices in order to choose Slice i mentioned in the first subproblem.", "mentionContextAttributes": {"used": {"value": true, "score": 0.8153159618377686}, "created": {"value": false, "score": 0.00032532215118408203}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999680519104004}, "created": {"value": false, "score": 0.10225743055343628}, "shared": {"value": false, "score": 2.0265579223632812e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Spark", "normalizedForm": "Spark", "offsetStart": 94, "offsetEnd": 99}, "context": "-A scalable approach and architecture to compute PDFs of QOIs in large spatial datasets using Spark; -Three new methods to reduce the time of computing PDFs, i.e. data grouping, ML prediction and sampling; -An extensive experimental evaluation based on the implementation of the methods in a Spark/HDFS cluster and big datasets ranging from 235 GB to 2.4 TB. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.0021881461143493652}, "created": {"value": false, "score": 0.14419466257095337}, "shared": {"value": false, "score": 5.960464477539062e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9997071623802185}, "created": {"value": true, "score": 0.9999213218688965}, "shared": {"value": false, "score": 0.00011897087097167969}}, "references": [{"label": "[3]", "normalizedForm": "[3]", "refKey": 3}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Spark", "normalizedForm": "Spark", "offsetStart": 94, "offsetEnd": 99}, "context": "During execution, some intermediate data that is stored in Spark RDDs can be cached using the Spark Cache operation, which stores RDD data in main memory. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.017237603664398193}, "created": {"value": false, "score": 4.124641418457031e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9997071623802185}, "created": {"value": true, "score": 0.9999213218688965}, "shared": {"value": false, "score": 0.00011897087097167969}}, "references": [{"label": "[3]", "normalizedForm": "[3]", "refKey": 3}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "RawData", "normalizedForm": "RawData", "offsetStart": 103, "offsetEnd": 110}, "context": "Input: DS: a set of datasets corresponding to a spatial cube area; i: the ith slice to analyze Output: RawData: mean, standard deviation and the original dataset of each point in the cube area 1: RawData \u2190 \u2205 2: while RawData does not contain all the points in slice i do 3: windowData \u2190 \u2205 4:", "mentionContextAttributes": {"used": {"value": true, "score": 0.7704876661300659}, "created": {"value": false, "score": 2.86102294921875e-06}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999734163284302}, "created": {"value": false, "score": 1.7404556274414062e-05}, "shared": {"value": false, "score": 5.960464477539062e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "RawData", "normalizedForm": "RawData", "offsetStart": 103, "offsetEnd": 110}, "context": "the percentage of distribution types of the points in the slice 1: P oints \u2190 Sample(slice i , rate) 2: RawData \u2190 \u2205 3: for each p \u2208 points do 4:", "mentionContextAttributes": {"used": {"value": true, "score": 0.9990683197975159}, "created": {"value": false, "score": 4.887580871582031e-06}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999734163284302}, "created": {"value": false, "score": 1.7404556274414062e-05}, "shared": {"value": false, "score": 5.960464477539062e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "program", "normalizedForm": "program", "offsetStart": 103, "offsetEnd": 110}, "context": "Execution of One Slice In this section, we compare the performance of different methods to execute the program for the points of the whole slice, i.e.", "mentionContextAttributes": {"used": {"value": false, "score": 0.3056541681289673}, "created": {"value": false, "score": 2.968311309814453e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999203681945801}, "created": {"value": true, "score": 0.9999256134033203}, "shared": {"value": false, "score": 4.76837158203125e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Spark", "normalizedForm": "Spark", "offsetStart": 113, "offsetEnd": 118}, "context": "The basic processing of PDFs consists of data loading, from NFS to Spark RDDs, followed by PDF computation using Spark.", "mentionContextAttributes": {"used": {"value": false, "score": 0.00732654333114624}, "created": {"value": false, "score": 3.147125244140625e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9997071623802185}, "created": {"value": true, "score": 0.9999213218688965}, "shared": {"value": false, "score": 0.00011897087097167969}}, "references": [{"label": "[3]", "normalizedForm": "[3]", "refKey": 3}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Scala", "normalizedForm": "Scala", "offsetStart": 117, "offsetEnd": 122}, "context": "The architecture is deployed in the execution environment and the optimization methods are directly implemented in a Scala program, thus facilitating the use of the three new methods. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.003164827823638916}, "created": {"value": true, "score": 0.6812536716461182}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9950926303863525}, "created": {"value": true, "score": 0.9999256134033203}, "shared": {"value": false, "score": 5.960464477539062e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Google File System", "normalizedForm": "Google File System", "offsetStart": 122, "offsetEnd": 144}, "context": "Spark data can be stored in the Hadoop Distributed File System (HDFS) [41], a popular open source file system inspired by Google File System [22]. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.0076784491539001465}, "created": {"value": false, "score": 0.00010371208190917969}, "shared": {"value": false, "score": 0.00011897087097167969}}, "documentContextAttributes": {"used": {"value": false, "score": 0.0076784491539001465}, "created": {"value": false, "score": 0.00010371208190917969}, "shared": {"value": false, "score": 0.00011897087097167969}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Scala", "normalizedForm": "Scala", "offsetStart": 123, "offsetEnd": 128}, "context": "In addition, an average error of the PDF of the points in the slice is calculated and shown as the result of executing the Scala program. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.8920292258262634}, "created": {"value": false, "score": 4.9591064453125e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9950926303863525}, "created": {"value": true, "score": 0.9999256134033203}, "shared": {"value": false, "score": 5.960464477539062e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "program", "normalizedForm": "program", "offsetStart": 123, "offsetEnd": 130}, "context": "The architecture is deployed in the execution environment and the optimization methods are directly implemented in a Scala program, thus facilitating the use of the three new methods.", "mentionContextAttributes": {"used": {"value": false, "score": 0.003164827823638916}, "created": {"value": true, "score": 0.6812536716461182}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999203681945801}, "created": {"value": true, "score": 0.9999256134033203}, "shared": {"value": false, "score": 4.76837158203125e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Spark", "normalizedForm": "Spark", "offsetStart": 124, "offsetEnd": 129}, "context": "An alternative solution would have been to store the input data in HDFS, which would lead to have HDFS tasks competing with Spark tasks for resource usage on the same cluster nodes.", "mentionContextAttributes": {"used": {"value": false, "score": 0.010111987590789795}, "created": {"value": false, "score": 0.000254213809967041}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9997071623802185}, "created": {"value": true, "score": 0.9999213218688965}, "shared": {"value": false, "score": 0.00011897087097167969}}, "references": [{"label": "[3]", "normalizedForm": "[3]", "refKey": 3}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Spark", "normalizedForm": "Spark", "offsetStart": 124, "offsetEnd": 129}, "context": "The hyperparameters are tuned at the very beginning, which takes 18 minutes for 4 -types and 22 minutes for 10 -types using Spark on a workstation with 8 CPU cores and 32 GB of RAM.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9899632334709167}, "created": {"value": false, "score": 1.2040138244628906e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9997071623802185}, "created": {"value": true, "score": 0.9999213218688965}, "shared": {"value": false, "score": 0.00011897087097167969}}, "references": [{"label": "[3]", "normalizedForm": "[3]", "refKey": 3}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Slice", "normalizedForm": "Slice", "offsetStart": 125, "offsetEnd": 130}, "version": {"rawForm": "201", "normalizedForm": "201"}, "context": "We take the relationship between the set of mean and standard deviation values and the distribution types of 25000 points in Slice 0 in the previously generated output data of Set1 to tune the hyperparameters of the decision tree.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9995984435081482}, "created": {"value": false, "score": 8.106231689453125e-06}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999680519104004}, "created": {"value": false, "score": 0.10225743055343628}, "shared": {"value": false, "score": 2.0265579223632812e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Spark", "normalizedForm": "Spark", "offsetStart": 126, "offsetEnd": 131}, "context": "For each point in the window, Line 8 gets the set of corresponding observation values, which can be automatically realized by Spark as the RawData is stored in RDD with the id of a point as key and the set of observation values as value. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.05096721649169922}, "created": {"value": false, "score": 1.7404556274414062e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9997071623802185}, "created": {"value": true, "score": 0.9999213218688965}, "shared": {"value": false, "score": 0.00011897087097167969}}, "references": [{"label": "[3]", "normalizedForm": "[3]", "refKey": 3}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "program", "normalizedForm": "program", "offsetStart": 129, "offsetEnd": 136}, "context": "In addition, an average error of the PDF of the points in the slice is calculated and shown as the result of executing the Scala program.", "mentionContextAttributes": {"used": {"value": true, "score": 0.8920292258262634}, "created": {"value": false, "score": 4.9591064453125e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999203681945801}, "created": {"value": true, "score": 0.9999256134033203}, "shared": {"value": false, "score": 4.76837158203125e-07}}}, {"type": "software", "software-type": "environment", "software-name": {"rawForm": "MATLAB", "normalizedForm": "MATLAB", "offsetStart": 131, "offsetEnd": 137}, "context": "Solutions for computing PDFs are now available in popular programming environments for numerical and statistical analysis, such as MATLAB and R. MATLAB provides libraries [31] [36] that use a baseline method for PDF fitting. ", "mentionContextAttributes": {"used": {"value": false, "score": 5.0187110900878906e-05}, "created": {"value": false, "score": 9.822845458984375e-05}, "shared": {"value": false, "score": 1.0728836059570312e-06}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9705975651741028}, "created": {"value": true, "score": 0.8872366547584534}, "shared": {"value": false, "score": 1.0728836059570312e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Slice", "normalizedForm": "Slice", "offsetStart": 133, "offsetEnd": 138}, "version": {"rawForm": "201", "normalizedForm": "201"}, "context": "We take the relationship between the combination of mean and standard deviation values and the distribution types of 25000 points in Slice 0 as previously generated data to build the decision tree model.", "mentionContextAttributes": {"used": {"value": true, "score": 0.993441104888916}, "created": {"value": false, "score": 5.4955482482910156e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999680519104004}, "created": {"value": false, "score": 0.10225743055343628}, "shared": {"value": false, "score": 2.0265579223632812e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Spark", "normalizedForm": "Spark", "offsetStart": 138, "offsetEnd": 143}, "context": "In this paper, we exploit Spark MLlib [3], a scalable machine learning (ML) library that can handle big data [28] in memory by exploiting Spark RDDs and Spark operations.", "mentionContextAttributes": {"used": {"value": false, "score": 0.03300833702087402}, "created": {"value": true, "score": 0.9990726709365845}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9997071623802185}, "created": {"value": true, "score": 0.9999213218688965}, "shared": {"value": false, "score": 0.00011897087097167969}}, "references": [{"label": "[3]", "normalizedForm": "[3]", "refKey": 3}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "RawData", "normalizedForm": "RawData", "offsetStart": 139, "offsetEnd": 146}, "context": "For each point in the window, Line 8 gets the set of corresponding observation values, which can be automatically realized by Spark as the RawData is stored in RDD with the id of a point as key and the set of observation values as value. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.05096721649169922}, "created": {"value": false, "score": 1.7404556274414062e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999734163284302}, "created": {"value": false, "score": 1.7404556274414062e-05}, "shared": {"value": false, "score": 5.960464477539062e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Slice", "normalizedForm": "Slice", "offsetStart": 140, "offsetEnd": 145}, "version": {"rawForm": "201", "normalizedForm": "201"}, "context": "Again, we take the relationship between the combination of mean and standard deviation values and the distribution types of 25000 points in Slice 0 as the previously generated data for the decision tree model.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9989370703697205}, "created": {"value": false, "score": 6.031990051269531e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999680519104004}, "created": {"value": false, "score": 0.10225743055343628}, "shared": {"value": false, "score": 2.0265579223632812e-06}}}, {"type": "software", "software-type": "environment", "software-name": {"rawForm": "MATLAB", "normalizedForm": "MATLAB", "offsetStart": 145, "offsetEnd": 151}, "context": "Solutions for computing PDFs are now available in popular programming environments for numerical and statistical analysis, such as MATLAB and R. MATLAB provides libraries [31] [36] that use a baseline method for PDF fitting. ", "mentionContextAttributes": {"used": {"value": false, "score": 5.0187110900878906e-05}, "created": {"value": false, "score": 9.822845458984375e-05}, "shared": {"value": false, "score": 1.0728836059570312e-06}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9705975651741028}, "created": {"value": true, "score": 0.8872366547584534}, "shared": {"value": false, "score": 1.0728836059570312e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Spark", "normalizedForm": "Spark", "offsetStart": 151, "offsetEnd": 156}, "context": "Then, the information in the cached files is retrieved and stored as intermediate data in RDDs, which can be again cached using the Cache operation of Spark.", "mentionContextAttributes": {"used": {"value": false, "score": 0.21983057260513306}, "created": {"value": false, "score": 2.6345252990722656e-05}, "shared": {"value": false, "score": 4.76837158203125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9997071623802185}, "created": {"value": true, "score": 0.9999213218688965}, "shared": {"value": false, "score": 0.00011897087097167969}}, "references": [{"label": "[3]", "normalizedForm": "[3]", "refKey": 3}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Spark", "normalizedForm": "Spark", "offsetStart": 153, "offsetEnd": 158}, "context": "In this paper, we exploit Spark MLlib [3], a scalable machine learning (ML) library that can handle big data [28] in memory by exploiting Spark RDDs and Spark operations.", "mentionContextAttributes": {"used": {"value": false, "score": 0.03300833702087402}, "created": {"value": true, "score": 0.9990726709365845}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9997071623802185}, "created": {"value": true, "score": 0.9999213218688965}, "shared": {"value": false, "score": 0.00011897087097167969}}, "references": [{"label": "[3]", "normalizedForm": "[3]", "refKey": 3}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Slice", "normalizedForm": "Slice", "offsetStart": 153, "offsetEnd": 158}, "version": {"rawForm": "201", "normalizedForm": "201"}, "context": "We take 25 lines as the window size for PDF computation and execute the program with different methods for the whole slice, i.e. 11 windows of points in Slice 201.", "mentionContextAttributes": {"used": {"value": true, "score": 0.6930989027023315}, "created": {"value": false, "score": 3.981590270996094e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999680519104004}, "created": {"value": false, "score": 0.10225743055343628}, "shared": {"value": false, "score": 2.0265579223632812e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Spark", "normalizedForm": "Spark", "offsetStart": 153, "offsetEnd": 158}, "context": "This is because using more nodes increases data transfers to send the mean and standard deviation values and the distribution type from each node to the Spark master node in Spark cluster to compute the distribution percentage.", "mentionContextAttributes": {"used": {"value": false, "score": 0.12696558237075806}, "created": {"value": false, "score": 1.4781951904296875e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9997071623802185}, "created": {"value": true, "score": 0.9999213218688965}, "shared": {"value": false, "score": 0.00011897087097167969}}, "references": [{"label": "[3]", "normalizedForm": "[3]", "refKey": 3}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Spark", "normalizedForm": "Spark", "offsetStart": 155, "offsetEnd": 160}, "context": "To decide whether to use data grouping, the ideal solution would be to have a cost model, but it is difficult to estimate the time to transfer data within Spark.", "mentionContextAttributes": {"used": {"value": false, "score": 0.0030884146690368652}, "created": {"value": false, "score": 0.0004857778549194336}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9997071623802185}, "created": {"value": true, "score": 0.9999213218688965}, "shared": {"value": false, "score": 0.00011897087097167969}}, "references": [{"label": "[3]", "normalizedForm": "[3]", "refKey": 3}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "program", "normalizedForm": "program", "offsetStart": 157, "offsetEnd": 164}, "context": "Finally, the output data of the external program is transformed to key-value pairs and stored in RDDs by the same Map operation, which executes the external program.", "mentionContextAttributes": {"used": {"value": false, "score": 0.29698705673217773}, "created": {"value": false, "score": 5.543231964111328e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999203681945801}, "created": {"value": true, "score": 0.9999256134033203}, "shared": {"value": false, "score": 4.76837158203125e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "program", "normalizedForm": "program", "offsetStart": 161, "offsetEnd": 168}, "context": "In order to calculate the PDF of a point, we use the fitdistr function, which implements MLE and NM method (a commonly used and robust numerical method) in an R program. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9999203681945801}, "created": {"value": false, "score": 0.0002529025077819824}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999203681945801}, "created": {"value": true, "score": 0.9999256134033203}, "shared": {"value": false, "score": 4.76837158203125e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Spark", "normalizedForm": "Spark", "offsetStart": 168, "offsetEnd": 173}, "context": "skipBytes (Skips and discards a specified number of bytes in the current file input stream), may not work correctly during the parallel execution of a Map operation in Spark [23], we call an external Java program in the Map operation to retrieve observation values of a point from different spatial datasets and pre-process values.", "mentionContextAttributes": {"used": {"value": false, "score": 0.004340112209320068}, "created": {"value": false, "score": 1.9550323486328125e-05}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9997071623802185}, "created": {"value": true, "score": 0.9999213218688965}, "shared": {"value": false, "score": 0.00011897087097167969}}, "references": [{"label": "[23]", "normalizedForm": "[23]", "refKey": 23, "offsetStart": 35331, "offsetEnd": 35335}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Spark", "normalizedForm": "Spark", "offsetStart": 174, "offsetEnd": 179}, "context": "This is because using more nodes increases data transfers to send the mean and standard deviation values and the distribution type from each node to the Spark master node in Spark cluster to compute the distribution percentage.", "mentionContextAttributes": {"used": {"value": false, "score": 0.12696564197540283}, "created": {"value": false, "score": 1.4781951904296875e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9997071623802185}, "created": {"value": true, "score": 0.9999213218688965}, "shared": {"value": false, "score": 0.00011897087097167969}}, "references": [{"label": "[3]", "normalizedForm": "[3]", "refKey": 3}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Excel", "normalizedForm": "Excel", "offsetStart": 184, "offsetEnd": 189}, "context": "It extends the MapReduce model [16] for two important classes of analytics applications: iterative processing (machine learning, graph processing) and interactive data mining (with R, Excel or Python). ", "mentionContextAttributes": {"used": {"value": false, "score": 4.088878631591797e-05}, "created": {"value": false, "score": 0.00041109323501586914}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": false, "score": 4.088878631591797e-05}, "created": {"value": false, "score": 0.00041109323501586914}, "shared": {"value": false, "score": 2.384185791015625e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Slice", "normalizedForm": "Slice", "offsetStart": 186, "offsetEnd": 191}, "version": {"rawForm": "201", "normalizedForm": "201"}, "context": "Based on multiple datasets, each point p x,y in Slice i corresponds to a set of values V = {v 1 , v 2 , ..., v n } while v k is the observation value corresponding to the point p x,y in Slice i in d k \u2208 DS and n is the number of observation values in the set (the same for Equations 1, 2 and 5).", "mentionContextAttributes": {"used": {"value": true, "score": 0.9975537657737732}, "created": {"value": false, "score": 3.5762786865234375e-06}, "shared": {"value": false, "score": 1.6689300537109375e-06}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999680519104004}, "created": {"value": false, "score": 0.10225743055343628}, "shared": {"value": false, "score": 2.0265579223632812e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Python", "normalizedForm": "Python", "offsetStart": 193, "offsetEnd": 199}, "context": "It extends the MapReduce model [16] for two important classes of analytics applications: iterative processing (machine learning, graph processing) and interactive data mining (with R, Excel or Python). ", "mentionContextAttributes": {"used": {"value": false, "score": 4.088878631591797e-05}, "created": {"value": false, "score": 0.00041109323501586914}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": false, "score": 4.088878631591797e-05}, "created": {"value": false, "score": 0.00041109323501586914}, "shared": {"value": false, "score": 2.384185791015625e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "RawData", "normalizedForm": "RawData", "offsetStart": 196, "offsetEnd": 203}, "context": "Input: DS: a set of datasets corresponding to a spatial cube area; i: the ith slice to analyze Output: RawData: mean, standard deviation and the original dataset of each point in the cube area 1: RawData \u2190 \u2205 2: while RawData does not contain all the points in slice i do 3: windowData \u2190 \u2205 4:", "mentionContextAttributes": {"used": {"value": true, "score": 0.770488440990448}, "created": {"value": false, "score": 2.86102294921875e-06}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999734163284302}, "created": {"value": false, "score": 1.7404556274414062e-05}, "shared": {"value": false, "score": 5.960464477539062e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "program", "normalizedForm": "program", "offsetStart": 205, "offsetEnd": 212}, "context": "skipBytes (Skips and discards a specified number of bytes in the current file input stream), may not work correctly during the parallel execution of a Map operation in Spark [23], we call an external Java program in the Map operation to retrieve observation values of a point from different spatial datasets and pre-process values.", "mentionContextAttributes": {"used": {"value": false, "score": 0.004340112209320068}, "created": {"value": false, "score": 1.9550323486328125e-05}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999203681945801}, "created": {"value": true, "score": 0.9999256134033203}, "shared": {"value": false, "score": 4.76837158203125e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "RawData", "normalizedForm": "RawData", "offsetStart": 217, "offsetEnd": 224}, "context": "Input: DS: a set of datasets corresponding to a spatial cube area; i: the ith slice to analyze Output: RawData: mean, standard deviation and the original dataset of each point in the cube area 1: RawData \u2190 \u2205 2: while RawData does not contain all the points in slice i do 3: windowData \u2190 \u2205 4:", "mentionContextAttributes": {"used": {"value": true, "score": 0.770488440990448}, "created": {"value": false, "score": 2.86102294921875e-06}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999734163284302}, "created": {"value": false, "score": 1.7404556274414062e-05}, "shared": {"value": false, "score": 5.960464477539062e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "windowData", "normalizedForm": "windowData", "offsetStart": 274, "offsetEnd": 284}, "context": "Input: DS: a set of datasets corresponding to a spatial cube area; i: the ith slice to analyze Output: RawData: mean, standard deviation and the original dataset of each point in the cube area 1: RawData \u2190 \u2205 2: while RawData does not contain all the points in slice i do 3: windowData \u2190 \u2205 4:", "mentionContextAttributes": {"used": {"value": true, "score": 0.770488440990448}, "created": {"value": false, "score": 2.86102294921875e-06}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999157190322876}, "created": {"value": false, "score": 5.4836273193359375e-06}, "shared": {"value": false, "score": 2.2649765014648438e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Spark", "normalizedForm": "Spark", "offsetStart": 292, "offsetEnd": 297}, "context": "-A scalable approach and architecture to compute PDFs of QOIs in large spatial datasets using Spark; -Three new methods to reduce the time of computing PDFs, i.e. data grouping, ML prediction and sampling; -An extensive experimental evaluation based on the implementation of the methods in a Spark/HDFS cluster and big datasets ranging from 235 GB to 2.4 TB.", "mentionContextAttributes": {"used": {"value": false, "score": 0.0021881461143493652}, "created": {"value": false, "score": 0.1441946029663086}, "shared": {"value": false, "score": 5.960464477539062e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9997071623802185}, "created": {"value": true, "score": 0.9999213218688965}, "shared": {"value": false, "score": 0.00011897087097167969}}, "references": [{"label": "[3]", "normalizedForm": "[3]", "refKey": 3}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "RawData", "normalizedForm": "RawData", "offsetStart": 330, "offsetEnd": 337}, "context": "Input: DS: a set of spatial datasets corresponding to a spatial cube area; i: the ith slice to analyze; T ypes: a set of distribution types Output: P DF : the PDF of all the points in the ith slice of the cube area; E: the average error between the PDF and the observation values of all the points in the ith slice 1: P DF \u2190 \u2205 2: RawData \u2190 loadData(DS, i) 3: while not all points in slice i are processed do 4:", "mentionContextAttributes": {"used": {"value": true, "score": 0.9996931552886963}, "created": {"value": false, "score": 1.6689300537109375e-06}, "shared": {"value": false, "score": 5.960464477539062e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999734163284302}, "created": {"value": false, "score": 1.7404556274414062e-05}, "shared": {"value": false, "score": 5.960464477539062e-07}}}], "references": [{"refKey": 1, "tei": "<biblStruct xml:id=\"b1\">\n\t<monogr>\n\t\t<title level=\"m\">Fitdistr Function in R language</title>\n\t\t<imprint/>\n\t</monogr>\n</biblStruct>\n"}, {"refKey": 3, "tei": "<biblStruct xml:id=\"b3\">\n\t<monogr>\n\t\t<title/>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Spark</forename><surname>Mlib</surname></persName>\n\t\t</author>\n\t\t<imprint/>\n\t</monogr>\n</biblStruct>\n"}, {"refKey": 16, "tei": "<biblStruct xml:id=\"b16\">\n\t<analytic>\n\t\t<title level=\"a\" type=\"main\">MapReduce</title>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Jeffrey</forename><surname>Dean</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Sanjay</forename><surname>Ghemawat</surname></persName>\n\t\t</author>\n\t\t<idno type=\"DOI\">10.1145/1327452.1327492</idno>\n\t</analytic>\n\t<monogr>\n\t\t<title level=\"j\">Communications of the ACM</title>\n\t\t<title level=\"j\" type=\"abbrev\">Commun. ACM</title>\n\t\t<idno type=\"ISSN\">0001-0782</idno>\n\t\t<idno type=\"ISSNe\">1557-7317</idno>\n\t\t<imprint>\n\t\t\t<biblScope unit=\"volume\">51</biblScope>\n\t\t\t<biblScope unit=\"issue\">1</biblScope>\n\t\t\t<biblScope unit=\"page\" from=\"107\" to=\"113\" />\n\t\t\t<date type=\"published\" when=\"2008-01\">2004</date>\n\t\t\t<publisher>Association for Computing Machinery (ACM)</publisher>\n\t\t</imprint>\n\t</monogr>\n</biblStruct>\n"}, {"refKey": 23, "tei": "<biblStruct xml:id=\"b23\">\n\t<monogr>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">E</forename><forename type=\"middle\">R</forename><surname>Harold</surname></persName>\n\t\t</author>\n\t\t<title level=\"m\">Java I/O: Tips and Techniques for Putting I/O to Work</title>\n\t\t<imprint>\n\t\t\t<date>2006</date>\n\t\t\t<biblScope unit=\"page\">132</biblScope>\n\t\t</imprint>\n\t</monogr>\n</biblStruct>\n"}], "runtime": 20741, "id": "13088e3b2d942e4d19ccf409fac331b609a75b20", "metadata": {"id": "13088e3b2d942e4d19ccf409fac331b609a75b20"}, "original_file_path": "../../datalake/Samuel/SOFTware-Sync/downloads/xml/lirmm-02045144.grobid.tei.xml", "file_name": "lirmm-02045144.grobid.tei.xml"}