{"application": "software-mentions", "version": "0.8.0", "date": "2024-10-07T11:57+0000", "md5": "BB79D5658B7CC97CBD7D4D4B8818701D", "mentions": [{"type": "software", "software-type": "software", "software-name": {"rawForm": "Spark", "normalizedForm": "Spark", "offsetStart": 0, "offsetEnd": 5}, "version": {"rawForm": "2.1.1", "normalizedForm": "2.1.1"}, "context": "Spark affords a much faster data process in contrast to transferring it through needless Hadoop MapReduce mechanisms. ", "mentionContextAttributes": {"used": {"value": false, "score": 4.2438507080078125e-05}, "created": {"value": false, "score": 4.6253204345703125e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9997394680976868}, "created": {"value": true, "score": 0.9999246597290039}, "shared": {"value": false, "score": 4.76837158203125e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Hadoop", "normalizedForm": "Hadoop", "offsetStart": 0, "offsetEnd": 6}, "context": "Hadoop is based on simple programming paradigms that allow a highly scalable and reliable parallel processing of high-dimensional data sets. ", "mentionContextAttributes": {"used": {"value": false, "score": 3.826618194580078e-05}, "created": {"value": true, "score": 0.7193017601966858}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "documentContextAttributes": {"used": {"value": false, "score": 0.02491825819015503}, "created": {"value": true, "score": 0.7193017601966858}, "shared": {"value": false, "score": 1.3113021850585938e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "MapReduce", "normalizedForm": "MapReduce", "offsetStart": 0, "offsetEnd": 9}, "context": "MapReduce is the core of the Hadoop framework. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.0005880594253540039}, "created": {"value": false, "score": 0.017611801624298096}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9987735152244568}, "created": {"value": true, "score": 0.9997852444648743}, "shared": {"value": false, "score": 1.0728836059570312e-06}}, "references": [{"label": "[40,", "normalizedForm": "[40", "refKey": 46}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Apache Spark", "normalizedForm": "Apache Spark", "offsetStart": 0, "offsetEnd": 12}, "context": "Apache Spark is characterized by its capability of improving the system's effectiveness-which is achieved via the use of intensive memory-,its efficiency, and its high transparency for users. ", "mentionContextAttributes": {"used": {"value": false, "score": 9.262561798095703e-05}, "created": {"value": false, "score": 0.008869409561157227}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9897136688232422}, "created": {"value": true, "score": 0.999842643737793}, "shared": {"value": false, "score": 0.00041347742080688477}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Apache Storm", "normalizedForm": "Apache Storm", "offsetStart": 0, "offsetEnd": 14}, "context": "Apache Storm 6 and Apache Samza7 are among the most popular stream processing frameworks.", "mentionContextAttributes": {"used": {"value": false, "score": 0.00020688772201538086}, "created": {"value": false, "score": 5.1856040954589844e-05}, "shared": {"value": false, "score": 1.7881393432617188e-06}}, "documentContextAttributes": {"used": {"value": false, "score": 0.00020688772201538086}, "created": {"value": false, "score": 5.1856040954589844e-05}, "shared": {"value": false, "score": 1.7881393432617188e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "MapReduce", "normalizedForm": "MapReduce", "offsetStart": 0, "offsetEnd": 14}, "context": "MapReduce [10] is one of the most popular processing techniques and program models for distributed computing to deal with big data. ", "mentionContextAttributes": {"used": {"value": false, "score": 5.1140785217285156e-05}, "created": {"value": false, "score": 5.1975250244140625e-05}, "shared": {"value": false, "score": 7.152557373046875e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9987735152244568}, "created": {"value": true, "score": 0.9997852444648743}, "shared": {"value": false, "score": 1.0728836059570312e-06}}, "references": [{"label": "[40,", "normalizedForm": "[40", "refKey": 46}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "MapReduce", "normalizedForm": "MapReduce", "offsetStart": 4, "offsetEnd": 13}, "context": "The MapReduce paradigm is composed of two main tasks/phases, namely the map phase and the reduce phase. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.000733792781829834}, "created": {"value": false, "score": 0.000588536262512207}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9987735152244568}, "created": {"value": true, "score": 0.9997852444648743}, "shared": {"value": false, "score": 1.0728836059570312e-06}}, "references": [{"label": "[40,", "normalizedForm": "[40", "refKey": 46}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Spark", "normalizedForm": "Spark", "offsetStart": 8, "offsetEnd": 13}, "version": {"rawForm": "2.1.1", "normalizedForm": "2.1.1"}, "context": "Indeed, Spark has a number of high-level libraries for stream processing, machine learning and graph processing, e.g., MLlib [18]. ", "mentionContextAttributes": {"used": {"value": false, "score": 4.5418739318847656e-05}, "created": {"value": false, "score": 0.0001456737518310547}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9997394680976868}, "created": {"value": true, "score": 0.9999246597290039}, "shared": {"value": false, "score": 4.76837158203125e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Hadoop", "normalizedForm": "Hadoop", "offsetStart": 13, "offsetEnd": 19}, "context": "Technically, Hadoop works on top of the Hadoop distributed file system (HDFS) which duplicates the input data files in various storage machines (nodes). ", "mentionContextAttributes": {"used": {"value": false, "score": 0.00993955135345459}, "created": {"value": false, "score": 0.0006103515625}, "shared": {"value": false, "score": 1.0728836059570312e-06}}, "documentContextAttributes": {"used": {"value": false, "score": 0.02491825819015503}, "created": {"value": true, "score": 0.7193017601966858}, "shared": {"value": false, "score": 1.3113021850585938e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "MapReduce", "normalizedForm": "MapReduce", "offsetStart": 16, "offsetEnd": 25}, "context": "After that, the MapReduce paradigm assembles all the intermediate (key , value ) pairs by key via the shuffling phase. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.01972264051437378}, "created": {"value": false, "score": 5.125999450683594e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9987735152244568}, "created": {"value": true, "score": 0.9997852444648743}, "shared": {"value": false, "score": 1.0728836059570312e-06}}, "references": [{"label": "[40,", "normalizedForm": "[40", "refKey": 46}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "MapReduce", "normalizedForm": "MapReduce", "offsetStart": 17, "offsetEnd": 26}, "context": "Technically, the MapReduce paradigm is based on a specific data structure which is the (key, value) pair. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.0017981529235839844}, "created": {"value": false, "score": 0.0011636614799499512}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9987735152244568}, "created": {"value": true, "score": 0.9997852444648743}, "shared": {"value": false, "score": 1.0728836059570312e-06}}, "references": [{"label": "[40,", "normalizedForm": "[40", "refKey": 46}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "MapReduce", "normalizedForm": "MapReduce", "offsetStart": 18, "offsetEnd": 27}, "context": "Based on the same MapReduce paradigm, the Spark framework could offer an immediate 10 times increase in the system's performance. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9504634737968445}, "created": {"value": false, "score": 0.0002790093421936035}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9987735152244568}, "created": {"value": true, "score": 0.9997852444648743}, "shared": {"value": false, "score": 1.0728836059570312e-06}}, "references": [{"label": "[40,", "normalizedForm": "[40", "refKey": 46}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Google", "normalizedForm": "Google", "offsetStart": 19, "offsetEnd": 25}, "context": "It was proposed by Google in 2004 and designed to easily scale data processing over multiple computing nodes.", "mentionContextAttributes": {"used": {"value": false, "score": 6.890296936035156e-05}, "created": {"value": true, "score": 0.9994778037071228}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": false, "score": 0.0001481771469116211}, "created": {"value": true, "score": 0.9994778037071228}, "shared": {"value": false, "score": 1.0728836059570312e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Apache Samza", "normalizedForm": "Apache Samza", "offsetStart": 19, "offsetEnd": 32}, "context": "Apache Storm 6 and Apache Samza7 are among the most popular stream processing frameworks.", "mentionContextAttributes": {"used": {"value": false, "score": 0.00020688772201538086}, "created": {"value": false, "score": 5.1856040954589844e-05}, "shared": {"value": false, "score": 1.7881393432617188e-06}}, "documentContextAttributes": {"used": {"value": false, "score": 0.00020688772201538086}, "created": {"value": false, "score": 5.1856040954589844e-05}, "shared": {"value": false, "score": 1.7881393432617188e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "MapReduce", "normalizedForm": "MapReduce", "offsetStart": 24, "offsetEnd": 33}, "context": "A representation of the MapReduce framework is given in Fig. 1.", "mentionContextAttributes": {"used": {"value": false, "score": 0.10219144821166992}, "created": {"value": false, "score": 0.000110626220703125}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9987735152244568}, "created": {"value": true, "score": 0.9997852444648743}, "shared": {"value": false, "score": 1.0728836059570312e-06}}, "references": [{"label": "[40,", "normalizedForm": "[40", "refKey": 46}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Apache Spark", "normalizedForm": "Apache Spark", "offsetStart": 25, "offsetEnd": 37}, "context": "To deal with this issue, Apache Spark comes into play. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.00012624263763427734}, "created": {"value": false, "score": 0.3174266815185547}, "shared": {"value": false, "score": 7.152557373046875e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9897136688232422}, "created": {"value": true, "score": 0.999842643737793}, "shared": {"value": false, "score": 0.00041347742080688477}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "MapReduce", "normalizedForm": "MapReduce", "offsetStart": 26, "offsetEnd": 35}, "context": "Fig. 1 The process of the MapReduce framework", "mentionContextAttributes": {"used": {"value": true, "score": 0.9987735152244568}, "created": {"value": false, "score": 4.029273986816406e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9987735152244568}, "created": {"value": true, "score": 0.9997852444648743}, "shared": {"value": false, "score": 1.0728836059570312e-06}}, "references": [{"label": "[40,", "normalizedForm": "[40", "refKey": 46}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "MapReduce", "normalizedForm": "MapReduce", "offsetStart": 27, "offsetEnd": 36}, "context": "The programming details of MapReduce as well as its basic concepts will be given in Sect. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.0008325576782226562}, "created": {"value": false, "score": 0.14166337251663208}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9987735152244568}, "created": {"value": true, "score": 0.9997852444648743}, "shared": {"value": false, "score": 1.0728836059570312e-06}}, "references": [{"label": "[40,", "normalizedForm": "[40", "refKey": 46}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Hadoop", "normalizedForm": "Hadoop", "offsetStart": 29, "offsetEnd": 35}, "context": "MapReduce is the core of the Hadoop framework. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.0005880594253540039}, "created": {"value": false, "score": 0.017611801624298096}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "documentContextAttributes": {"used": {"value": false, "score": 0.02491825819015503}, "created": {"value": true, "score": 0.7193017601966858}, "shared": {"value": false, "score": 1.3113021850585938e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Hadoop", "normalizedForm": "Hadoop", "offsetStart": 36, "offsetEnd": 42}, "context": "More precisely and in comparison to Hadoop, in Hadoop MapReduce multiple jobs would be adjusted together to build a data pipeline. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.02491825819015503}, "created": {"value": false, "score": 0.00010013580322265625}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": false, "score": 0.02491825819015503}, "created": {"value": true, "score": 0.7193017601966858}, "shared": {"value": false, "score": 1.3113021850585938e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "MapReduce", "normalizedForm": "MapReduce", "offsetStart": 36, "offsetEnd": 45}, "context": "We, also, give a description of the MapReduce paradigm.", "mentionContextAttributes": {"used": {"value": false, "score": 0.0005491375923156738}, "created": {"value": true, "score": 0.9960718154907227}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9987735152244568}, "created": {"value": true, "score": 0.9997852444648743}, "shared": {"value": false, "score": 1.0728836059570312e-06}}, "references": [{"label": "[40,", "normalizedForm": "[40", "refKey": 46}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "MapReduce", "normalizedForm": "MapReduce", "offsetStart": 39, "offsetEnd": 48}, "context": "Among the possible alternatives is the MapReduce paradigm [10] which was introduced by Google and which offers a robust and efficient framework to deal with big data analysis.", "mentionContextAttributes": {"used": {"value": false, "score": 0.0001481771469116211}, "created": {"value": true, "score": 0.6761071681976318}, "shared": {"value": false, "score": 1.0728836059570312e-06}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9987735152244568}, "created": {"value": true, "score": 0.9997852444648743}, "shared": {"value": false, "score": 1.0728836059570312e-06}}, "references": [{"label": "[10]", "normalizedForm": "[10]", "refKey": 17, "offsetStart": 12921, "offsetEnd": 12925}, {"label": "[10]", "normalizedForm": "[10]", "refKey": 17, "offsetStart": 12921, "offsetEnd": 12925}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Hadoop", "normalizedForm": "Hadoop", "offsetStart": 40, "offsetEnd": 46}, "context": "Technically, Hadoop works on top of the Hadoop distributed file system (HDFS) which duplicates the input data files in various storage machines (nodes). ", "mentionContextAttributes": {"used": {"value": false, "score": 0.00993955135345459}, "created": {"value": false, "score": 0.0006103515625}, "shared": {"value": false, "score": 1.0728836059570312e-06}}, "documentContextAttributes": {"used": {"value": false, "score": 0.02491825819015503}, "created": {"value": true, "score": 0.7193017601966858}, "shared": {"value": false, "score": 1.3113021850585938e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Apache Spark", "normalizedForm": "Apache Spark", "offsetStart": 40, "offsetEnd": 52}, "context": "In this conducted research, we focus on Apache Spark. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.000826418399810791}, "created": {"value": true, "score": 0.999652624130249}, "shared": {"value": false, "score": 5.960464477539062e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9897136688232422}, "created": {"value": true, "score": 0.999842643737793}, "shared": {"value": false, "score": 0.00041347742080688477}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Spark", "normalizedForm": "Spark", "offsetStart": 42, "offsetEnd": 47}, "version": {"rawForm": "2.1.1", "normalizedForm": "2.1.1"}, "context": "Based on the same MapReduce paradigm, the Spark framework could offer an immediate 10 times increase in the system's performance.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9504634737968445}, "created": {"value": false, "score": 0.0002790093421936035}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9997394680976868}, "created": {"value": true, "score": 0.9999246597290039}, "shared": {"value": false, "score": 4.76837158203125e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Apache", "normalizedForm": "Apache", "offsetStart": 42, "offsetEnd": 48}, "context": "By applying Algorithm 4 and based on both Apache Spark splits, the output is the following:", "mentionContextAttributes": {"used": {"value": true, "score": 0.9997394680976868}, "created": {"value": false, "score": 2.396106719970703e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9997394680976868}, "created": {"value": false, "score": 2.396106719970703e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Grid", "normalizedForm": "Grid", "offsetStart": 44, "offsetEnd": 48}, "context": "Our experiments for Sp-RST are performed on Grid5000, 16 a large-scale testbed for experimentdriven research.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9960038065910339}, "created": {"value": true, "score": 0.8938215374946594}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9960038065910339}, "created": {"value": true, "score": 0.8938215374946594}, "shared": {"value": false, "score": 2.384185791015625e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Apache Spark", "normalizedForm": "Apache Spark", "offsetStart": 47, "offsetEnd": 59}, "context": "In this paper, we mainly focused on the use of Apache Spark.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9897136688232422}, "created": {"value": true, "score": 0.999842643737793}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9897136688232422}, "created": {"value": true, "score": 0.999842643737793}, "shared": {"value": false, "score": 0.00041347742080688477}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Apache Spark", "normalizedForm": "Apache Spark", "offsetStart": 47, "offsetEnd": 59}, "context": "Sp-RST has a distributed architecture based on Apache Spark for a distributed and in-memory computation task. ", "mentionContextAttributes": {"used": {"value": false, "score": 5.3763389587402344e-05}, "created": {"value": true, "score": 0.7123439908027649}, "shared": {"value": false, "score": 0.00041347742080688477}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9897136688232422}, "created": {"value": true, "score": 0.999842643737793}, "shared": {"value": false, "score": 0.00041347742080688477}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Hadoop MapReduce", "normalizedForm": "Hadoop MapReduce", "offsetStart": 47, "offsetEnd": 63}, "context": "More precisely and in comparison to Hadoop, in Hadoop MapReduce multiple jobs would be adjusted together to build a data pipeline. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.02491825819015503}, "created": {"value": false, "score": 0.00010013580322265625}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": false, "score": 0.02491825819015503}, "created": {"value": false, "score": 0.00010013580322265625}, "shared": {"value": false, "score": 2.384185791015625e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Spark", "normalizedForm": "Spark", "offsetStart": 49, "offsetEnd": 54}, "version": {"rawForm": "2.1.1", "normalizedForm": "2.1.1"}, "context": "Adding to this specificity, the key concept that Spark offers is a resilient distributed data set (RDD), which is a set of elements that are distributed across the nodes of the used cluster that can be operated on in a parallel way. ", "mentionContextAttributes": {"used": {"value": false, "score": 3.8504600524902344e-05}, "created": {"value": false, "score": 0.05113327503204346}, "shared": {"value": false, "score": 1.430511474609375e-06}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9997394680976868}, "created": {"value": true, "score": 0.9999246597290039}, "shared": {"value": false, "score": 4.76837158203125e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Spark", "normalizedForm": "Spark", "offsetStart": 49, "offsetEnd": 54}, "version": {"rawForm": "2.1.1", "normalizedForm": "2.1.1"}, "context": "By applying Algorithm 4 and based on both Apache Spark splits, the output is the following:", "mentionContextAttributes": {"used": {"value": true, "score": 0.9997394680976868}, "created": {"value": false, "score": 2.396106719970703e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9997394680976868}, "created": {"value": true, "score": 0.9999246597290039}, "shared": {"value": false, "score": 4.76837158203125e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Weka", "normalizedForm": "Weka", "offsetStart": 53, "offsetEnd": 57}, "version": {"rawForm": "3.8.2", "normalizedForm": "3.8.2", "offsetStart": 58, "offsetEnd": 63}, "context": "Moreover, we use the Naive Bayes implementation from Weka 3.8.2. 15  The Sp-RST algorithm is implemented in Scala 2.11 within the Spark 2.1.1 framework. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9769062399864197}, "created": {"value": false, "score": 2.682209014892578e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999898672103882}, "created": {"value": false, "score": 3.838539123535156e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Grid", "normalizedForm": "Grid", "offsetStart": 55, "offsetEnd": 59}, "context": "We run all settings on 1, 2, 4, 8, 16, and 32 nodes on Grid5000. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9810555577278137}, "created": {"value": false, "score": 0.0018569231033325195}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9960038065910339}, "created": {"value": true, "score": 0.8938215374946594}, "shared": {"value": false, "score": 2.384185791015625e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Scala", "normalizedForm": "Scala", "offsetStart": 56, "offsetEnd": 61}, "version": {"rawForm": "2.11", "normalizedForm": "2.11"}, "context": "Based on a distributed implementation design using both Scala and the Apache Spark framework [36], our proposed distributed algorithm copes with the RST computational inefficiencies and its restriction to be only applied to non-large data sets. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.00012946128845214844}, "created": {"value": true, "score": 0.8960633277893066}, "shared": {"value": false, "score": 5.960464477539062e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9769062399864197}, "created": {"value": true, "score": 0.8960633277893066}, "shared": {"value": false, "score": 5.960464477539062e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "MapReduce", "normalizedForm": "MapReduce", "offsetStart": 60, "offsetEnd": 69}, "context": "In this process, and in every level of that built pipeline, MapReduce will have to read the data from the disk and then write it back to the disk again. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.00032722949981689453}, "created": {"value": false, "score": 0.0001843571662902832}, "shared": {"value": false, "score": 5.960464477539062e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9987735152244568}, "created": {"value": true, "score": 0.9997852444648743}, "shared": {"value": false, "score": 1.0728836059570312e-06}}, "references": [{"label": "[40,", "normalizedForm": "[40", "refKey": 46}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Apache Spark", "normalizedForm": "Apache Spark", "offsetStart": 70, "offsetEnd": 82}, "context": "Based on a distributed implementation design using both Scala and the Apache Spark framework [36], our proposed distributed algorithm copes with the RST computational inefficiencies and its restriction to be only applied to non-large data sets. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.00012946128845214844}, "created": {"value": true, "score": 0.8960633277893066}, "shared": {"value": false, "score": 5.960464477539062e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9897136688232422}, "created": {"value": true, "score": 0.999842643737793}, "shared": {"value": false, "score": 0.00041347742080688477}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "MapReduce", "normalizedForm": "MapReduce", "offsetStart": 72, "offsetEnd": 81}, "context": "Specifically, the evolutionary algorithms were implemented based on the MapReduce paradigm to obtain subsets of features from big data sets. 2 These include a generic implementation of greedy information theoretic feature selection methods 3 which are based on the common theoretic framework presented in [29], and an improved implementation of the classical minimum Redundancy and Maximum Relevance feature selection method [29].", "mentionContextAttributes": {"used": {"value": false, "score": 0.027913928031921387}, "created": {"value": false, "score": 0.08878648281097412}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9987735152244568}, "created": {"value": true, "score": 0.9997852444648743}, "shared": {"value": false, "score": 1.0728836059570312e-06}}, "references": [{"label": "[40,", "normalizedForm": "[40", "refKey": 46}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Apache Spark", "normalizedForm": "Apache Spark", "offsetStart": 73, "offsetEnd": 85}, "context": "Based on these assumptions, the following partitions and splits based on Apache Spark are obtained (Tables 4,5, 6, 7).", "mentionContextAttributes": {"used": {"value": true, "score": 0.9887616038322449}, "created": {"value": false, "score": 5.602836608886719e-06}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9897136688232422}, "created": {"value": true, "score": 0.999842643737793}, "shared": {"value": false, "score": 0.00041347742080688477}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Apache Spark", "normalizedForm": "Apache Spark", "offsetStart": 74, "offsetEnd": 87}, "context": "Among the well-known streaming processing parallel frameworks, we mention Apache Spark8 and Apache Flink. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.00023370981216430664}, "created": {"value": false, "score": 0.051341474056243896}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9897136688232422}, "created": {"value": true, "score": 0.999842643737793}, "shared": {"value": false, "score": 0.00041347742080688477}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "AMPLab", "normalizedForm": "AMPLab", "offsetStart": 81, "offsetEnd": 87}, "publisher": {"rawForm": "UC Berkeley", "normalizedForm": "UC Berkeley", "offsetStart": 69, "offsetEnd": 80}, "context": "The distributed open source framework was initially developed in the UC Berkeley AMPLab for big data processing. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.00627666711807251}, "created": {"value": true, "score": 0.9842686653137207}, "shared": {"value": false, "score": 1.3589859008789062e-05}}, "documentContextAttributes": {"used": {"value": false, "score": 0.00627666711807251}, "created": {"value": true, "score": 0.9842686653137207}, "shared": {"value": false, "score": 1.3589859008789062e-05}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Hadoop", "normalizedForm": "Hadoop", "offsetStart": 84, "offsetEnd": 90}, "context": "This paradigm offers an intensive scalability over a large number of nodes within a Hadoop cluster.", "mentionContextAttributes": {"used": {"value": false, "score": 3.218650817871094e-05}, "created": {"value": false, "score": 0.022421836853027344}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": false, "score": 0.02491825819015503}, "created": {"value": true, "score": 0.7193017601966858}, "shared": {"value": false, "score": 1.3113021850585938e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Google", "normalizedForm": "Google", "offsetStart": 87, "offsetEnd": 93}, "context": "Among the possible alternatives is the MapReduce paradigm [10] which was introduced by Google and which offers a robust and efficient framework to deal with big data analysis. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.0001481771469116211}, "created": {"value": true, "score": 0.6761071681976318}, "shared": {"value": false, "score": 1.0728836059570312e-06}}, "documentContextAttributes": {"used": {"value": false, "score": 0.0001481771469116211}, "created": {"value": true, "score": 0.9994778037071228}, "shared": {"value": false, "score": 1.0728836059570312e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "RandomForestClassifier", "normalizedForm": "RandomForestClassifier", "offsetStart": 87, "offsetEnd": 109}, "context": "Subset selection: 13 http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.00022989511489868164}, "created": {"value": false, "score": 1.1205673217773438e-05}, "shared": {"value": true, "score": 0.7448496222496033}}, "documentContextAttributes": {"used": {"value": false, "score": 0.00022989511489868164}, "created": {"value": false, "score": 1.1205673217773438e-05}, "shared": {"value": true, "score": 0.7448496222496033}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Hadoop MapReduce", "normalizedForm": "Hadoop MapReduce", "offsetStart": 89, "offsetEnd": 105}, "context": "Spark affords a much faster data process in contrast to transferring it through needless Hadoop MapReduce mechanisms. ", "mentionContextAttributes": {"used": {"value": false, "score": 4.2438507080078125e-05}, "created": {"value": false, "score": 4.6253204345703125e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": false, "score": 0.02491825819015503}, "created": {"value": false, "score": 0.00010013580322265625}, "shared": {"value": false, "score": 2.384185791015625e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Apache Flink", "normalizedForm": "Apache Flink", "offsetStart": 92, "offsetEnd": 104}, "context": "Among the well-known streaming processing parallel frameworks, we mention Apache Spark8 and Apache Flink. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.00023370981216430664}, "created": {"value": false, "score": 0.051341474056243896}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": false, "score": 0.00023370981216430664}, "created": {"value": false, "score": 0.051341474056243896}, "shared": {"value": false, "score": 2.384185791015625e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Spark", "normalizedForm": "Spark", "offsetStart": 99, "offsetEnd": 104}, "version": {"rawForm": "2.1.1", "normalizedForm": "2.1.1"}, "context": "Our work, which is an extension of [8], is based on a distributed partitioning procedure, within a Spark/MapReduce paradigm, that makes our proposed solution scalable and effective in dealing with big data.", "mentionContextAttributes": {"used": {"value": false, "score": 5.0067901611328125e-05}, "created": {"value": true, "score": 0.9997852444648743}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9997394680976868}, "created": {"value": true, "score": 0.9999246597290039}, "shared": {"value": false, "score": 4.76837158203125e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "MapReduce", "normalizedForm": "MapReduce", "offsetStart": 103, "offsetEnd": 112}, "context": "Recently, a set of new and more flexible paradigms have been proposed aiming at extending the standard MapReduce approach, mainly Apache Spark1  [36] which has been applied with success over a number of data mining and machine learning real-world problems [36]. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.00015556812286376953}, "created": {"value": true, "score": 0.6206890940666199}, "shared": {"value": false, "score": 7.152557373046875e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9987735152244568}, "created": {"value": true, "score": 0.9997852444648743}, "shared": {"value": false, "score": 1.0728836059570312e-06}}, "references": [{"label": "[40,", "normalizedForm": "[40", "refKey": 46}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "MapReduce", "normalizedForm": "MapReduce", "offsetStart": 105, "offsetEnd": 114}, "context": "Our work, which is an extension of [8], is based on a distributed partitioning procedure, within a Spark/MapReduce paradigm, that makes our proposed solution scalable and effective in dealing with big data.", "mentionContextAttributes": {"used": {"value": false, "score": 5.0067901611328125e-05}, "created": {"value": true, "score": 0.9997852444648743}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9987735152244568}, "created": {"value": true, "score": 0.9997852444648743}, "shared": {"value": false, "score": 1.0728836059570312e-06}}, "references": [{"label": "[40,", "normalizedForm": "[40", "refKey": 46}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Scala", "normalizedForm": "Scala", "offsetStart": 108, "offsetEnd": 113}, "version": {"rawForm": "2.11", "normalizedForm": "2.11", "offsetStart": 114, "offsetEnd": 118}, "context": "Moreover, we use the Naive Bayes implementation from Weka 3.8.2. 15  The Sp-RST algorithm is implemented in Scala 2.11 within the Spark 2.1.1 framework. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9769062399864197}, "created": {"value": false, "score": 2.682209014892578e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9769062399864197}, "created": {"value": true, "score": 0.8960633277893066}, "shared": {"value": false, "score": 5.960464477539062e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Hadoop", "normalizedForm": "Hadoop", "offsetStart": 110, "offsetEnd": 116}, "context": "Among the well-known open-source distributed processing frameworks dedicated for batch processing, we mention Hadoop. ", "mentionContextAttributes": {"used": {"value": false, "score": 5.447864532470703e-05}, "created": {"value": false, "score": 0.2757152318954468}, "shared": {"value": false, "score": 1.3113021850585938e-06}}, "documentContextAttributes": {"used": {"value": false, "score": 0.02491825819015503}, "created": {"value": true, "score": 0.7193017601966858}, "shared": {"value": false, "score": 1.3113021850585938e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "MapReduce", "normalizedForm": "MapReduce", "offsetStart": 116, "offsetEnd": 125}, "context": "Several recent works have been concentrated on parallelizing and distributing machine learning techniques using the MapReduce paradigm [40,43,44]. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.00015056133270263672}, "created": {"value": false, "score": 0.386715292930603}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9987735152244568}, "created": {"value": true, "score": 0.9997852444648743}, "shared": {"value": false, "score": 1.0728836059570312e-06}}, "references": [{"label": "[40,", "normalizedForm": "[40", "refKey": 46, "offsetStart": 13174, "offsetEnd": 13178}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Spark", "normalizedForm": "Spark", "offsetStart": 130, "offsetEnd": 135}, "version": {"rawForm": "2.1.1", "normalizedForm": "2.1.1", "offsetStart": 136, "offsetEnd": 141}, "context": "Moreover, we use the Naive Bayes implementation from Weka 3.8.2. 15  The Sp-RST algorithm is implemented in Scala 2.11 within the Spark 2.1.1 framework. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9769062399864197}, "created": {"value": false, "score": 2.682209014892578e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9997394680976868}, "created": {"value": true, "score": 0.9999246597290039}, "shared": {"value": false, "score": 4.76837158203125e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Apache Spark", "normalizedForm": "Apache Spark", "offsetStart": 130, "offsetEnd": 149}, "context": "Recently, a set of new and more flexible paradigms have been proposed aiming at extending the standard MapReduce approach, mainly Apache Spark1  [36] which has been applied with success over a number of data mining and machine learning real-world problems [36]. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.00015556812286376953}, "created": {"value": true, "score": 0.6206890940666199}, "shared": {"value": false, "score": 7.152557373046875e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9897136688232422}, "created": {"value": true, "score": 0.999842643737793}, "shared": {"value": false, "score": 0.00041347742080688477}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Apache Spark", "normalizedForm": "Apache Spark", "offsetStart": 144, "offsetEnd": 156}, "context": "With the aim of choosing the most relevant and pertinent subset of features, a variety of feature reduction techniques were proposed within the Apache Spark framework to deal with big data in a distributed way. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.0037513375282287598}, "created": {"value": false, "score": 0.30193811655044556}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9897136688232422}, "created": {"value": true, "score": 0.999842643737793}, "shared": {"value": false, "score": 0.00041347742080688477}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Weka", "normalizedForm": "Weka", "offsetStart": 155, "offsetEnd": 159}, "version": {"rawForm": "3.8.2", "normalizedForm": "3.8.2", "offsetStart": 160, "offsetEnd": 165}, "context": "For sum squares ratio, we have used the version implemented in Smile; 17 while for the other three techniques, we have used the implementation provided in Weka 3.8.2.1", "mentionContextAttributes": {"used": {"value": true, "score": 0.9999898672103882}, "created": {"value": false, "score": 3.838539123535156e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999898672103882}, "created": {"value": false, "score": 3.838539123535156e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "MapReduce", "normalizedForm": "MapReduce", "offsetStart": 155, "offsetEnd": 164}, "context": "For example, some of these distributed methods adopt some evolutionary algorithms, such as the work proposed in [12], where authors defined a hierarchical MapReduce implementation of a parallel genetic algorithm for determining the minimum rough set reduct, i.e., the set of the selected features. ", "mentionContextAttributes": {"used": {"value": false, "score": 9.512901306152344e-05}, "created": {"value": true, "score": 0.6800314784049988}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9987735152244568}, "created": {"value": true, "score": 0.9997852444648743}, "shared": {"value": false, "score": 1.0728836059570312e-06}}, "references": [{"label": "[40,", "normalizedForm": "[40", "refKey": 46}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Spark", "normalizedForm": "Spark", "offsetStart": 166, "offsetEnd": 171}, "version": {"rawForm": "2.1.1", "normalizedForm": "2.1.1"}, "context": "In this paper, we propose a scalable and effective rough set theory-based approach for large-scale data pre-processing, specifically for feature selection, under the Spark framework.", "mentionContextAttributes": {"used": {"value": false, "score": 5.4001808166503906e-05}, "created": {"value": true, "score": 0.9999246597290039}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9997394680976868}, "created": {"value": true, "score": 0.9999246597290039}, "shared": {"value": false, "score": 4.76837158203125e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Apache", "normalizedForm": "Apache", "offsetStart": 184, "offsetEnd": 190}, "context": "Based on the first partition m = 1, and by applying Algorithm 3, which aims to generate all the AllComb (C r ) possible combinations of the C r set of attributes, the output from both Apache Spark splits is the following:", "mentionContextAttributes": {"used": {"value": true, "score": 0.9912745356559753}, "created": {"value": false, "score": 1.4543533325195312e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9997394680976868}, "created": {"value": false, "score": 2.396106719970703e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Spark", "normalizedForm": "Spark", "offsetStart": 191, "offsetEnd": 196}, "version": {"rawForm": "2.1.1", "normalizedForm": "2.1.1"}, "context": "Based on the first partition m = 1, and by applying Algorithm 3, which aims to generate all the AllComb (C r ) possible combinations of the C r set of attributes, the output from both Apache Spark splits is the following:", "mentionContextAttributes": {"used": {"value": true, "score": 0.9912745356559753}, "created": {"value": false, "score": 1.4543533325195312e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9997394680976868}, "created": {"value": true, "score": 0.9999246597290039}, "shared": {"value": false, "score": 4.76837158203125e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Windows-10", "normalizedForm": "Windows-10", "offsetStart": 193, "offsetEnd": 203}, "context": "Since the study does not require a scalable version of the two classifiers, these experiments are run on a standard laptop configuration with Intel(R) Core(TM) i7-7500U CPU, 16 GB RAM, 64-bit, Windows-10.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9430841207504272}, "created": {"value": false, "score": 0.00015926361083984375}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9430841207504272}, "created": {"value": false, "score": 0.00015926361083984375}, "shared": {"value": false, "score": 2.384185791015625e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "VectorSlicer", "normalizedForm": "VectorSlicer", "offsetStart": 240, "offsetEnd": 252}, "context": "Among these are several feature extraction methods such as nn-gram, principal component analysis, discrete cosine transform, tokenizer, Polynomi-alExpansion, ElementwiseProduct, etc., and very few feature selection techniques which are the VectorSlicer, the RFormula and the ChiSqSelector. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.00016880035400390625}, "created": {"value": false, "score": 4.76837158203125e-06}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "documentContextAttributes": {"used": {"value": false, "score": 0.00016880035400390625}, "created": {"value": false, "score": 4.76837158203125e-06}, "shared": {"value": false, "score": 3.5762786865234375e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Apache Spark", "normalizedForm": "Apache Spark", "offsetStart": 258, "offsetEnd": 270}, "context": "The choice of this specific framework to design our proposed algorithm based on rough sets for big data feature selection is essentially based on several reasons which are as follows: (1) to offer a general solution based on a hybrid parallel framework, (2) Apache Spark provides high-speed benefits with a trade-off in the usage of high memory, (3) Spark is one of the well-known and certified distributed frameworks and also a mature hybrid system specifically when comparing it to some other frameworks in the market. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.0016308426856994629}, "created": {"value": false, "score": 0.2415788769721985}, "shared": {"value": false, "score": 4.76837158203125e-06}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9897136688232422}, "created": {"value": true, "score": 0.999842643737793}, "shared": {"value": false, "score": 0.00041347742080688477}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "ChiSqSelector", "normalizedForm": "ChiSqSelector", "offsetStart": 275, "offsetEnd": 288}, "context": "Among these are several feature extraction methods such as nn-gram, principal component analysis, discrete cosine transform, tokenizer, Polynomi-alExpansion, ElementwiseProduct, etc., and very few feature selection techniques which are the VectorSlicer, the RFormula and the ChiSqSelector. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.00016880035400390625}, "created": {"value": false, "score": 4.76837158203125e-06}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "documentContextAttributes": {"used": {"value": false, "score": 0.00016880035400390625}, "created": {"value": false, "score": 4.76837158203125e-06}, "shared": {"value": false, "score": 3.5762786865234375e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Spark", "normalizedForm": "Spark", "offsetStart": 350, "offsetEnd": 355}, "version": {"rawForm": "2.1.1", "normalizedForm": "2.1.1"}, "context": "The choice of this specific framework to design our proposed algorithm based on rough sets for big data feature selection is essentially based on several reasons which are as follows: (1) to offer a general solution based on a hybrid parallel framework, (2) Apache Spark provides high-speed benefits with a trade-off in the usage of high memory, (3) Spark is one of the well-known and certified distributed frameworks and also a mature hybrid system specifically when comparing it to some other frameworks in the market.", "mentionContextAttributes": {"used": {"value": false, "score": 0.0016308426856994629}, "created": {"value": false, "score": 0.24157947301864624}, "shared": {"value": false, "score": 4.76837158203125e-06}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9997394680976868}, "created": {"value": true, "score": 0.9999246597290039}, "shared": {"value": false, "score": 4.76837158203125e-06}}}], "references": [{"refKey": 46, "tei": "<biblStruct xml:id=\"b46\">\n\t<analytic>\n\t\t<title level=\"a\" type=\"main\">Discovering outlying aspects in large datasets</title>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Nguyen</forename><forename type=\"middle\">Xuan</forename><surname>Vinh</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Jeffrey</forename><surname>Chan</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Simone</forename><surname>Romano</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">James</forename><surname>Bailey</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Christopher</forename><surname>Leckie</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Kotagiri</forename><surname>Ramamohanarao</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Jian</forename><surname>Pei</surname></persName>\n\t\t</author>\n\t\t<idno type=\"DOI\">10.1007/s10618-016-0453-2</idno>\n\t</analytic>\n\t<monogr>\n\t\t<title level=\"j\">Data Mining and Knowledge Discovery</title>\n\t\t<title level=\"j\" type=\"abbrev\">Data Min Knowl Disc</title>\n\t\t<idno type=\"ISSN\">1384-5810</idno>\n\t\t<idno type=\"ISSNe\">1573-756X</idno>\n\t\t<imprint>\n\t\t\t<biblScope unit=\"volume\">30</biblScope>\n\t\t\t<biblScope unit=\"issue\">6</biblScope>\n\t\t\t<biblScope unit=\"page\" from=\"1520\" to=\"1555\" />\n\t\t\t<date type=\"published\" when=\"2016-02-09\">2016</date>\n\t\t\t<publisher>Springer Science and Business Media LLC</publisher>\n\t\t</imprint>\n\t</monogr>\n</biblStruct>\n"}, {"refKey": 17, "tei": "<biblStruct xml:id=\"b17\">\n\t<analytic>\n\t\t<title level=\"a\" type=\"main\">MapReduce</title>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Jeffrey</forename><surname>Dean</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Sanjay</forename><surname>Ghemawat</surname></persName>\n\t\t</author>\n\t\t<idno type=\"DOI\">10.1145/1629175.1629198</idno>\n\t</analytic>\n\t<monogr>\n\t\t<title level=\"j\">Communications of the ACM</title>\n\t\t<title level=\"j\" type=\"abbrev\">Commun. ACM</title>\n\t\t<idno type=\"ISSN\">0001-0782</idno>\n\t\t<idno type=\"ISSNe\">1557-7317</idno>\n\t\t<imprint>\n\t\t\t<biblScope unit=\"volume\">53</biblScope>\n\t\t\t<biblScope unit=\"issue\">1</biblScope>\n\t\t\t<biblScope unit=\"page\" from=\"72\" to=\"77\" />\n\t\t\t<date type=\"published\" when=\"2010-01\">2010</date>\n\t\t\t<publisher>Association for Computing Machinery (ACM)</publisher>\n\t\t</imprint>\n\t</monogr>\n</biblStruct>\n"}], "runtime": 17626, "id": "ea237ea780fc0f81af375130ca16773556967d65", "metadata": {"id": "ea237ea780fc0f81af375130ca16773556967d65"}, "original_file_path": "../../datalake/Samuel/SOFTware-Sync/downloads/xml/hal-04456307.grobid.tei.xml", "file_name": "hal-04456307.grobid.tei.xml"}