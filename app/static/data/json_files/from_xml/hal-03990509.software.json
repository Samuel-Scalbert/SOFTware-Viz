{"application": "software-mentions", "version": "0.8.0", "date": "2024-10-07T11:50+0000", "md5": "8699A0AFD750FCC2D27C0103C74D679C", "mentions": [{"type": "software", "software-type": "software", "software-name": {"rawForm": "LightGBM", "normalizedForm": "LightGBM", "offsetStart": 4, "offsetEnd": 12}, "context": "For LightGBM, a \"square root of the square root of the inverse class proportion\" correction was selected.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9997714161872864}, "created": {"value": false, "score": 3.9458274841308594e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9998453855514526}, "created": {"value": false, "score": 0.08952409029006958}, "shared": {"value": false, "score": 1.1920928955078125e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "PyTorch", "normalizedForm": "PyTorch", "offsetStart": 8, "offsetEnd": 37}, "context": "We used PyTorch (Paszke et al., 2019) to implement the neural models. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9998812675476074}, "created": {"value": false, "score": 0.005120754241943359}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9998812675476074}, "created": {"value": false, "score": 0.005120754241943359}, "shared": {"value": false, "score": 2.384185791015625e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "spaCy", "normalizedForm": "spaCy", "offsetStart": 12, "offsetEnd": 17}, "context": "We used the spaCy POS-tagger and considered POS unigrams, bigrams and trigrams that occur at least 10 times in the training dataset. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9999842643737793}, "created": {"value": false, "score": 4.172325134277344e-06}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999842643737793}, "created": {"value": false, "score": 2.276897430419922e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "LightGBM", "normalizedForm": "LightGBM", "offsetStart": 13, "offsetEnd": 21}, "context": "We relied on LightGBM, an ensemble of decision trees trained with gradient boosting (Ke et al., 2017).", "mentionContextAttributes": {"used": {"value": true, "score": 0.9998328685760498}, "created": {"value": false, "score": 0.08952409029006958}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9998453855514526}, "created": {"value": false, "score": 0.08952409029006958}, "shared": {"value": false, "score": 1.1920928955078125e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "LightGBM", "normalizedForm": "LightGBM", "offsetStart": 23, "offsetEnd": 31}, "context": "Feature analysis using LightGBM: Using the best performing model, Table 5 shows the role of each feature set in the prediction task. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9998453855514526}, "created": {"value": false, "score": 1.0013580322265625e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9998453855514526}, "created": {"value": false, "score": 0.08952409029006958}, "shared": {"value": false, "score": 1.1920928955078125e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "AdamW", "normalizedForm": "AdamW", "offsetStart": 33, "offsetEnd": 38}, "context": "Neural models were trained using AdamW as an optimizer (Loshchilov and Hutter, 2018), and used a reduced feature vector, obtained with the application of PCA (d init = 1800; d = 100 ; 99.8 % of the information is conserved). ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9999227523803711}, "created": {"value": false, "score": 3.2186508178710938e-06}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999227523803711}, "created": {"value": false, "score": 3.2186508178710938e-06}, "shared": {"value": false, "score": 1.1920928955078125e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Glove", "normalizedForm": "Glove", "offsetStart": 40, "offsetEnd": 45}, "context": "We reimplemented the Attention-CNN with Glove (Pennington et al., 2014) 300-D words embeddings as the vector representation. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9999475479125977}, "created": {"value": false, "score": 0.0023525357246398926}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999475479125977}, "created": {"value": false, "score": 0.0023525357246398926}, "shared": {"value": false, "score": 2.384185791015625e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "LightGBM", "normalizedForm": "LightGBM", "offsetStart": 45, "offsetEnd": 53}, "context": "As demonstrated by (Lundberg and Lee, 2017), LightGBM internal feature importance scores are inconsistent with both the model behavior and human intuition, so we instead used a model-agnostic tool.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9997548460960388}, "created": {"value": false, "score": 5.507469177246094e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9998453855514526}, "created": {"value": false, "score": 0.08952409029006958}, "shared": {"value": false, "score": 1.1920928955078125e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "LightGBM", "normalizedForm": "LightGBM", "offsetStart": 52, "offsetEnd": 60}, "context": "Best performance (F1score of 79.0) is obtained with LightGBM leveraging almost all the features.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9779738783836365}, "created": {"value": false, "score": 1.049041748046875e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9998453855514526}, "created": {"value": false, "score": 0.08952409029006958}, "shared": {"value": false, "score": 1.1920928955078125e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "nltk", "normalizedForm": "nltk", "offsetStart": 66, "offsetEnd": 70}, "context": "We used the lemma of the words for unigrams and bigrams using the nltk lemmatizer (Loper, 2002) and selected unigrams and bigrams that occurred in the training dataset at least fifty times. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9999872446060181}, "created": {"value": false, "score": 2.9802322387695312e-06}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999872446060181}, "created": {"value": false, "score": 2.9802322387695312e-06}, "shared": {"value": false, "score": 1.1920928955078125e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "spaCy", "normalizedForm": "spaCy", "offsetStart": 75, "offsetEnd": 80}, "context": "Note that there is strong redundancy between some features of LIWC and the spaCy POS tagger that both produce a \"Pronoun\" category, using a lexicon in the first case, and a neural inference in the second.", "mentionContextAttributes": {"used": {"value": false, "score": 0.0007803440093994141}, "created": {"value": false, "score": 2.276897430419922e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999842643737793}, "created": {"value": false, "score": 2.276897430419922e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "LightGBM", "normalizedForm": "LightGBM", "offsetStart": 151, "offsetEnd": 159}, "context": "First, and perhaps surprisingly, we notice that the use of \"Knowledge-Driven\" features based on rules built from linguistic knowledge of hedges in the LightGBM model outperforms the use of pre-trained embeddings within a fine-tuned BERT model (79.0 vs. 70.6),", "mentionContextAttributes": {"used": {"value": false, "score": 0.05949968099594116}, "created": {"value": false, "score": 0.0006652474403381348}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9998453855514526}, "created": {"value": false, "score": 0.08952409029006958}, "shared": {"value": false, "score": 1.1920928955078125e-07}}}], "references": [], "runtime": 6150, "id": "e1fe8e56a4aaba212a359100a1b38275821839e5", "metadata": {"id": "e1fe8e56a4aaba212a359100a1b38275821839e5"}, "original_file_path": "../../datalake/Samuel/SOFTware-Sync/downloads/xml/hal-03990509.grobid.tei.xml", "file_name": "hal-03990509.grobid.tei.xml"}