{"application": "software-mentions", "version": "0.8.0", "date": "2024-10-07T11:49+0000", "md5": "14C5F1B97BD7E4DDA9D41822454784D6", "mentions": [{"type": "software", "software-type": "software", "software-name": {"rawForm": "FlauBERT", "normalizedForm": "FlauBERT", "offsetStart": 0, "offsetEnd": 8}, "context": "FlauBERT model with different thresholds ensures high precision, hence, being confident that all the ads labeled about a specific policy are correct.", "mentionContextAttributes": {"used": {"value": false, "score": 0.00013327598571777344}, "created": {"value": false, "score": 9.28640365600586e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999920129776001}, "created": {"value": false, "score": 0.006425082683563232}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "references": [{"label": "[29]", "normalizedForm": "[29]", "refKey": 29}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "CamemBERT", "normalizedForm": "CamemBERT", "offsetStart": 0, "offsetEnd": 9}, "context": "CamemBERT is trained on 138 Gb of textual data crawled from the web, and FlauBERT is trained with 71 Gb of data from diverse sources, including crawled data and Wikipedia.", "mentionContextAttributes": {"used": {"value": false, "score": 0.045939743518829346}, "created": {"value": false, "score": 7.855892181396484e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9986611604690552}, "created": {"value": false, "score": 0.006425082683563232}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "references": [{"label": "[34]", "normalizedForm": "[34]", "refKey": 34}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "FlauBERT", "normalizedForm": "FlauBERT", "offsetStart": 13, "offsetEnd": 21}, "context": "As expected, FlauBERT and CamemBERT outperform SVMs and Random Forests by a large margin, and obtain F 1 scores over 0.65. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.05230492353439331}, "created": {"value": false, "score": 8.940696716308594e-06}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999920129776001}, "created": {"value": false, "score": 0.006425082683563232}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "references": [{"label": "[29]", "normalizedForm": "[29]", "refKey": 29}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "FlauBERT", "normalizedForm": "FlauBERT", "offsetStart": 15, "offsetEnd": 23}, "context": "We applied the FlauBERT model for the analysis with different thresholds on the 76 067 political ads we collected from Meta's Ad Library that ran between 1 January 2022 and 15 June 2022.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9999920129776001}, "created": {"value": false, "score": 4.506111145019531e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999920129776001}, "created": {"value": false, "score": 0.006425082683563232}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "references": [{"label": "[29]", "normalizedForm": "[29]", "refKey": 29}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "CamemBERT", "normalizedForm": "CamemBERT", "offsetStart": 26, "offsetEnd": 35}, "context": "As expected, FlauBERT and CamemBERT outperform SVMs and Random Forests by a large margin, and obtain F 1 scores over 0.65. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.05230492353439331}, "created": {"value": false, "score": 8.940696716308594e-06}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9986611604690552}, "created": {"value": false, "score": 0.006425082683563232}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "references": [{"label": "[34]", "normalizedForm": "[34]", "refKey": 34}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "FlauBERT", "normalizedForm": "FlauBERT", "offsetStart": 34, "offsetEnd": 42}, "context": "In what follows, we settle on the FlauBERT-based classifier, that slightly outperformed the CamemBERT-based classifier.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9986611604690552}, "created": {"value": false, "score": 0.006425082683563232}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999920129776001}, "created": {"value": false, "score": 0.006425082683563232}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "references": [{"label": "[29]", "normalizedForm": "[29]", "refKey": 29}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "FlauBERT", "normalizedForm": "FlauBERT", "offsetStart": 40, "offsetEnd": 48}, "context": "A) presents the precision and recall of FlauBERT with 14 policy categories using different thresholds.", "mentionContextAttributes": {"used": {"value": false, "score": 0.01722407341003418}, "created": {"value": false, "score": 0.00015592575073242188}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999920129776001}, "created": {"value": false, "score": 0.006425082683563232}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "references": [{"label": "[29]", "normalizedForm": "[29]", "refKey": 29}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "FlauBERT", "normalizedForm": "FlauBERT", "offsetStart": 58, "offsetEnd": 66}, "context": "Table 5 illustrates the precision, recall and F1 score of FlauBERT across the 9 policy categories.", "mentionContextAttributes": {"used": {"value": true, "score": 0.989028811454773}, "created": {"value": false, "score": 1.9550323486328125e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999920129776001}, "created": {"value": false, "score": 0.006425082683563232}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "references": [{"label": "[29]", "normalizedForm": "[29]", "refKey": 29}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "FlauBERT", "normalizedForm": "FlauBERT", "offsetStart": 66, "offsetEnd": 74}, "context": "We first train the four models we used (i.e., SVM, Random Forest, FlauBERT and CamemBERT) on VM dataset and we report the accuracy of the best configuration of each classifier, as selected by 10-fold cross validation. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9940606951713562}, "created": {"value": false, "score": 0.0001843571662902832}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999920129776001}, "created": {"value": false, "score": 0.006425082683563232}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "references": [{"label": "[29]", "normalizedForm": "[29]", "refKey": 29}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "FlauBERT", "normalizedForm": "FlauBERT", "offsetStart": 73, "offsetEnd": 81}, "context": "CamemBERT is trained on 138 Gb of textual data crawled from the web, and FlauBERT is trained with 71 Gb of data from diverse sources, including crawled data and Wikipedia.", "mentionContextAttributes": {"used": {"value": false, "score": 0.045939743518829346}, "created": {"value": false, "score": 7.855892181396484e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999920129776001}, "created": {"value": false, "score": 0.006425082683563232}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "references": [{"label": "[29]", "normalizedForm": "[29]", "refKey": 29}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "CamemBERT", "normalizedForm": "CamemBERT", "offsetStart": 79, "offsetEnd": 88}, "context": "We first train the four models we used (i.e., SVM, Random Forest, FlauBERT and CamemBERT) on VM dataset and we report the accuracy of the best configuration of each classifier, as selected by 10-fold cross validation. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9940606951713562}, "created": {"value": false, "score": 0.0001843571662902832}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9986611604690552}, "created": {"value": false, "score": 0.006425082683563232}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "references": [{"label": "[34]", "normalizedForm": "[34]", "refKey": 34}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "CamemBERT", "normalizedForm": "CamemBERT", "offsetStart": 87, "offsetEnd": 96}, "context": "For French, the language we are dealing with, there exist two main pre-trained models: CamemBERT [34] and FlauBERT [29].", "mentionContextAttributes": {"used": {"value": false, "score": 0.00034224987030029297}, "created": {"value": false, "score": 0.000253140926361084}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9986611604690552}, "created": {"value": false, "score": 0.006425082683563232}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "references": [{"label": "[34]", "normalizedForm": "[34]", "refKey": 34, "offsetStart": 24514, "offsetEnd": 24518}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "FlauBERT", "normalizedForm": "FlauBERT", "offsetStart": 90, "offsetEnd": 98}, "context": "We investigate whether modelling annotator disagreement helps in our case by training the FlauBERT-based classifier on DISTRIB dataset.", "mentionContextAttributes": {"used": {"value": false, "score": 0.025270462036132812}, "created": {"value": false, "score": 0.003383815288543701}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999920129776001}, "created": {"value": false, "score": 0.006425082683563232}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "references": [{"label": "[29]", "normalizedForm": "[29]", "refKey": 29}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "CamemBERT", "normalizedForm": "CamemBERT", "offsetStart": 92, "offsetEnd": 101}, "context": "In what follows, we settle on the FlauBERT-based classifier, that slightly outperformed the CamemBERT-based classifier.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9986611604690552}, "created": {"value": false, "score": 0.006425082683563232}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9986611604690552}, "created": {"value": false, "score": 0.006425082683563232}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "references": [{"label": "[34]", "normalizedForm": "[34]", "refKey": 34}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "FlauBERT", "normalizedForm": "FlauBERT", "offsetStart": 106, "offsetEnd": 114}, "context": "For French, the language we are dealing with, there exist two main pre-trained models: CamemBERT [34] and FlauBERT [29].", "mentionContextAttributes": {"used": {"value": false, "score": 0.00034224987030029297}, "created": {"value": false, "score": 0.000253140926361084}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999920129776001}, "created": {"value": false, "score": 0.006425082683563232}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "references": [{"label": "[29]", "normalizedForm": "[29]", "refKey": 29, "offsetStart": 24532, "offsetEnd": 24536}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "FlauBERT", "normalizedForm": "FlauBERT", "offsetStart": 153, "offsetEnd": 161}, "context": "We present the results of the models trained on CAP dataset in Table 4. Unfortunately, the hypothesis turned out wrong: when trained on CAP dataset, the FlauBERT-based classifier only achieved 0.13 F 1 .", "mentionContextAttributes": {"used": {"value": true, "score": 0.9999713897705078}, "created": {"value": false, "score": 0.0034084320068359375}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999920129776001}, "created": {"value": false, "score": 0.006425082683563232}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "references": [{"label": "[29]", "normalizedForm": "[29]", "refKey": 29}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "FlauBERT", "normalizedForm": "FlauBERT", "offsetStart": 167, "offsetEnd": 175}, "context": "For better convergence, we used a linear-decreasing learning rate during optimisation and a batch size of 8. Our implementation uses the transformers library [27] for FlauBERT and CamemBERT pre-trained models.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9948223829269409}, "created": {"value": false, "score": 0.0024284720420837402}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999920129776001}, "created": {"value": false, "score": 0.006425082683563232}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "references": [{"label": "[29]", "normalizedForm": "[29]", "refKey": 29}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "CamemBERT", "normalizedForm": "CamemBERT", "offsetStart": 180, "offsetEnd": 189}, "context": "For better convergence, we used a linear-decreasing learning rate during optimisation and a batch size of 8. Our implementation uses the transformers library [27] for FlauBERT and CamemBERT pre-trained models.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9948223829269409}, "created": {"value": false, "score": 0.0024284720420837402}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9986611604690552}, "created": {"value": false, "score": 0.006425082683563232}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "references": [{"label": "[34]", "normalizedForm": "[34]", "refKey": 34}]}], "references": [{"refKey": 29, "tei": "<biblStruct xml:id=\"b29\">\n\t<analytic>\n\t\t<title level=\"a\" type=\"main\">FlauBERT: Unsupervised language model pre-training for French</title>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">H</forename><surname>Le</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">L</forename><surname>Vial</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">J</forename><surname>Frej</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">V</forename><surname>Segonne</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">M</forename><surname>Coavoux</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">B</forename><surname>Lecouteux</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">A</forename><surname>Allauzen</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">B</forename><surname>Crabb\u00e9</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">L</forename><surname>Besacier</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">D</forename><surname>Schwab</surname></persName>\n\t\t</author>\n\t</analytic>\n\t<monogr>\n\t\t<title level=\"m\">Proceedings of the Twelfth Language Resources and Evaluation Conference</title>\n\t\t<meeting>the Twelfth Language Resources and Evaluation Conference</meeting>\n\t\t<imprint>\n\t\t\t<publisher>European Language Resources Association</publisher>\n\t\t\t<date>2020</date>\n\t\t</imprint>\n\t</monogr>\n</biblStruct>\n"}, {"refKey": 34, "tei": "<biblStruct xml:id=\"b34\">\n\t<analytic>\n\t\t<title level=\"a\" type=\"main\">CamemBERT: a Tasty French Language Model</title>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Louis</forename><surname>Martin</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Benjamin</forename><surname>Muller</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Pedro</forename><forename type=\"middle\">Javier</forename><surname>Ortiz Su\u00e1rez</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Yoann</forename><surname>Dupont</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Laurent</forename><surname>Romary</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">\u00c9ric</forename><surname>De La Clergerie</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Djam\u00e9</forename><surname>Seddah</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Beno\u00eet</forename><surname>Sagot</surname></persName>\n\t\t</author>\n\t\t<idno type=\"DOI\">10.18653/v1/2020.acl-main.645</idno>\n\t</analytic>\n\t<monogr>\n\t\t<title level=\"m\">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>\n\t\t<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>\n\t\t<imprint>\n\t\t\t<publisher>Association for Computational Linguistics</publisher>\n\t\t\t<date type=\"published\" when=\"2020\">2020</date>\n\t\t</imprint>\n\t</monogr>\n</biblStruct>\n"}], "runtime": 12050, "id": "06e9ae8dd41d4e770806ab5f7928c75b5c5bd63e", "metadata": {"id": "06e9ae8dd41d4e770806ab5f7928c75b5c5bd63e"}, "original_file_path": "../../datalake/Samuel/SOFTware-Sync/downloads/xml/hal-04129915.grobid.tei.xml", "file_name": "hal-04129915.grobid.tei.xml"}