{"application": "software-mentions", "version": "0.8.0", "date": "2024-10-07T11:39+0000", "md5": "A3800CC474E92E495F6E24B08094F528", "mentions": [{"type": "software", "software-type": "software", "software-name": {"rawForm": "MANTa", "normalizedForm": "MANTa", "offsetStart": 0, "offsetEnd": 5}, "context": "MANTa is a differentiable tokenizer trained end-to-end with the language model.", "mentionContextAttributes": {"used": {"value": false, "score": 2.9325485229492188e-05}, "created": {"value": false, "score": 0.001113593578338623}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999856948852539}, "created": {"value": true, "score": 0.9999426603317261}, "shared": {"value": false, "score": 4.76837158203125e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "MANTa", "normalizedForm": "MANTa", "offsetStart": 0, "offsetEnd": 5}, "context": "MANTa can be divided in three different parts:", "mentionContextAttributes": {"used": {"value": false, "score": 0.001110851764678955}, "created": {"value": false, "score": 7.295608520507812e-05}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999856948852539}, "created": {"value": true, "score": 0.9999426603317261}, "shared": {"value": false, "score": 4.76837158203125e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "MANTa", "normalizedForm": "MANTa", "offsetStart": 0, "offsetEnd": 5}, "context": "MANTa learns to downsample input sequences so that no information is lost through truncation, but also converges towards a sharp segmentation.", "mentionContextAttributes": {"used": {"value": false, "score": 0.0006462931632995605}, "created": {"value": false, "score": 6.079673767089844e-06}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999856948852539}, "created": {"value": true, "score": 0.9999426603317261}, "shared": {"value": false, "score": 4.76837158203125e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "MANTa", "normalizedForm": "MANTa", "offsetStart": 0, "offsetEnd": 5}, "context": "MANTa Base ) tokenizer and embedding module and a T5 Small (resp.", "mentionContextAttributes": {"used": {"value": false, "score": 0.365861713886261}, "created": {"value": false, "score": 9.179115295410156e-06}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999856948852539}, "created": {"value": true, "score": 0.9999426603317261}, "shared": {"value": false, "score": 4.76837158203125e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "MANTa", "normalizedForm": "MANTa", "offsetStart": 0, "offsetEnd": 5}, "context": "MANTa-LM is approximately 4 times faster than Byte-level T5 Small , and 5 times faster than ByT5 Small , which can be explained by the reduced sequence length we use in the encoder-decoder model.", "mentionContextAttributes": {"used": {"value": false, "score": 0.0479586124420166}, "created": {"value": false, "score": 5.6862831115722656e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999856948852539}, "created": {"value": true, "score": 0.9999426603317261}, "shared": {"value": false, "score": 4.76837158203125e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "MANTa", "normalizedForm": "MANTa", "offsetStart": 0, "offsetEnd": 5}, "context": "MANTa-LM is only 2.3 times slower than T5 Small which furthermore benefits from already tokenized sequences at training time.", "mentionContextAttributes": {"used": {"value": false, "score": 0.0004630088806152344}, "created": {"value": false, "score": 1.811981201171875e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999856948852539}, "created": {"value": true, "score": 0.9999426603317261}, "shared": {"value": false, "score": 4.76837158203125e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "MANTa", "normalizedForm": "MANTa", "offsetStart": 6, "offsetEnd": 11}, "context": "Since MANTa is not static and can be finetuned on non-standard data, we expect it should be able to learn to be more robust to variation/noise compared to a subword tokenizer paired with a LM.", "mentionContextAttributes": {"used": {"value": false, "score": 2.9802322387695312e-05}, "created": {"value": false, "score": 0.00014543533325195312}, "shared": {"value": false, "score": 4.76837158203125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999856948852539}, "created": {"value": true, "score": 0.9999426603317261}, "shared": {"value": false, "score": 4.76837158203125e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "CANINE", "normalizedForm": "CANINE", "offsetStart": 6, "offsetEnd": 12}, "context": "While CANINE downsamples sequences using a fixed rate after byte contextualization and Charformer's GBST (Gradient Based Subword Tokenizer) pools representations created using various downsampling rates, MANTa only applies downsampling right before the LM to limit the length of block sequences. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.004469335079193115}, "created": {"value": false, "score": 1.5497207641601562e-06}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": false, "score": 0.004469335079193115}, "created": {"value": false, "score": 0.00037157535552978516}, "shared": {"value": false, "score": 2.384185791015625e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "MANTa", "normalizedForm": "MANTa", "offsetStart": 9, "offsetEnd": 14}, "context": "Overall, MANTa-LM exhibits a performance slightly below Charformer but stays within a small margin on average (1.1 points below).", "mentionContextAttributes": {"used": {"value": false, "score": 0.0026983022689819336}, "created": {"value": false, "score": 2.9206275939941406e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999856948852539}, "created": {"value": true, "score": 0.9999426603317261}, "shared": {"value": false, "score": 4.76837158203125e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "MANTa", "normalizedForm": "MANTa", "offsetStart": 9, "offsetEnd": 14}, "context": "Although MANTa can help alleviate some of the inherent issues accompanying subword tokenizers, it also suffers some flaws that we believe could be addressed in future work.", "mentionContextAttributes": {"used": {"value": false, "score": 5.9604644775390625e-05}, "created": {"value": false, "score": 0.0006462931632995605}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999856948852539}, "created": {"value": true, "score": 0.9999426603317261}, "shared": {"value": false, "score": 4.76837158203125e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "MANTa", "normalizedForm": "MANTa", "offsetStart": 11, "offsetEnd": 16}, "context": "\u2022 We train MANTa-LM on English data and we evaluate its robustness to synthetic and natural variation and its ability to adapt to new domains compared to byte-level models.", "mentionContextAttributes": {"used": {"value": false, "score": 0.31220024824142456}, "created": {"value": true, "score": 0.9989950060844421}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999856948852539}, "created": {"value": true, "score": 0.9999426603317261}, "shared": {"value": false, "score": 4.76837158203125e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "MANTa", "normalizedForm": "MANTa", "offsetStart": 11, "offsetEnd": 16}, "context": "We believe MANTa could be used in the multilingual setting to ensure a more balanced segmentation between languages.", "mentionContextAttributes": {"used": {"value": false, "score": 0.0008484125137329102}, "created": {"value": false, "score": 0.011593163013458252}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999856948852539}, "created": {"value": true, "score": 0.9999426603317261}, "shared": {"value": false, "score": 4.76837158203125e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "MANTa", "normalizedForm": "MANTa", "offsetStart": 12, "offsetEnd": 17}, "context": "We finetune MANTa-LM on MEDNLI (Romanov and Shivade, 2018), a dataset consisting of 14,049 sentence pairs extracted from clinical notes.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9357391595840454}, "created": {"value": false, "score": 0.00022727251052856445}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999856948852539}, "created": {"value": true, "score": 0.9999426603317261}, "shared": {"value": false, "score": 4.76837158203125e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "MANTa", "normalizedForm": "MANTa", "offsetStart": 13, "offsetEnd": 18}, "context": "We find that MANTa improves robustness to character perturbations and out-of-domain data.", "mentionContextAttributes": {"used": {"value": false, "score": 0.0002308487892150879}, "created": {"value": false, "score": 0.013696730136871338}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999856948852539}, "created": {"value": true, "score": 0.9999426603317261}, "shared": {"value": false, "score": 4.76837158203125e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "MANTa", "normalizedForm": "MANTa", "offsetStart": 13, "offsetEnd": 18}, "context": "We show that MANTa-LM is robust to noisy text data and able to adapt to new domains while being significantly faster than byte-level models.", "mentionContextAttributes": {"used": {"value": false, "score": 0.00030791759490966797}, "created": {"value": false, "score": 0.33796584606170654}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999856948852539}, "created": {"value": true, "score": 0.9999426603317261}, "shared": {"value": false, "score": 4.76837158203125e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "MANTa", "normalizedForm": "MANTa", "offsetStart": 13, "offsetEnd": 18}, "context": "As expected, MANTa is less prone to over-segmentation of unknown words like named entities.", "mentionContextAttributes": {"used": {"value": false, "score": 5.352497100830078e-05}, "created": {"value": false, "score": 0.0005090236663818359}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999856948852539}, "created": {"value": true, "score": 0.9999426603317261}, "shared": {"value": false, "score": 4.76837158203125e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "MANTa", "normalizedForm": "MANTa", "offsetStart": 14, "offsetEnd": 19}, "context": "Details about MANTa hyperparameters can be found in Appendix B.", "mentionContextAttributes": {"used": {"value": false, "score": 5.626678466796875e-05}, "created": {"value": false, "score": 3.0875205993652344e-05}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999856948852539}, "created": {"value": true, "score": 0.9999426603317261}, "shared": {"value": false, "score": 4.76837158203125e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "MANTa", "normalizedForm": "MANTa", "offsetStart": 14, "offsetEnd": 19}, "context": "We found that MANTa is not keen to produce subword level segmentations.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9860155582427979}, "created": {"value": false, "score": 0.0009047985076904297}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999856948852539}, "created": {"value": true, "score": 0.9999426603317261}, "shared": {"value": false, "score": 4.76837158203125e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "MANTa", "normalizedForm": "MANTa", "offsetStart": 15, "offsetEnd": 20}, "context": "Interestingly, MANTa learns a simple but explainable segmentation using only the LM objective while effectively reducing the length of byte sequences.", "mentionContextAttributes": {"used": {"value": false, "score": 6.306171417236328e-05}, "created": {"value": false, "score": 7.557868957519531e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999856948852539}, "created": {"value": true, "score": 0.9999426603317261}, "shared": {"value": false, "score": 4.76837158203125e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "MANTa", "normalizedForm": "MANTa", "offsetStart": 15, "offsetEnd": 20}, "context": "\u2022 We introduce MANTa, a gradient-based tokenization and pooling module that can learn jointly with an encoder-decoder LM;", "mentionContextAttributes": {"used": {"value": false, "score": 7.319450378417969e-05}, "created": {"value": true, "score": 0.9999297857284546}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999856948852539}, "created": {"value": true, "score": 0.9999426603317261}, "shared": {"value": false, "score": 4.76837158203125e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "MANTa", "normalizedForm": "MANTa", "offsetStart": 18, "offsetEnd": 23}, "context": "We then show that MANTa performs comparably to other models on the general-domain GLUE benchmark.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9878011345863342}, "created": {"value": false, "score": 0.0018407702445983887}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999856948852539}, "created": {"value": true, "score": 0.9999426603317261}, "shared": {"value": false, "score": 4.76837158203125e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "MANTa", "normalizedForm": "MANTa", "offsetStart": 18, "offsetEnd": 23}, "context": "We also introduce MANTa-LM, a Transformer encoder-decoder that incorporates MANTa and that is trained end-to-end.", "mentionContextAttributes": {"used": {"value": false, "score": 5.1975250244140625e-05}, "created": {"value": true, "score": 0.9999161958694458}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999856948852539}, "created": {"value": true, "score": 0.9999426603317261}, "shared": {"value": false, "score": 4.76837158203125e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "MANTa", "normalizedForm": "MANTa", "offsetStart": 18, "offsetEnd": 23}, "context": "We then show that MANTa-LM is more robust when applied to noisy or out-ofdomain data than models using static subword tokenizers.", "mentionContextAttributes": {"used": {"value": false, "score": 0.4206221103668213}, "created": {"value": false, "score": 0.0022282004356384277}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999856948852539}, "created": {"value": true, "score": 0.9999426603317261}, "shared": {"value": false, "score": 4.76837158203125e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "CANINE", "normalizedForm": "CANINE", "offsetStart": 20, "offsetEnd": 26}, "context": "Indeed, contrary to CANINE and Charformer, MANTa disentangles the segmentation of blocks from their representations, allowing to study each part separately.", "mentionContextAttributes": {"used": {"value": false, "score": 0.004262566566467285}, "created": {"value": false, "score": 3.2186508178710938e-06}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": false, "score": 0.004469335079193115}, "created": {"value": false, "score": 0.00037157535552978516}, "shared": {"value": false, "score": 2.384185791015625e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "MANTa", "normalizedForm": "MANTa", "offsetStart": 22, "offsetEnd": 27}, "context": "Each of them stacks a MANTa Small (resp.", "mentionContextAttributes": {"used": {"value": false, "score": 0.08468097448348999}, "created": {"value": false, "score": 2.7298927307128906e-05}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999856948852539}, "created": {"value": true, "score": 0.9999426603317261}, "shared": {"value": false, "score": 4.76837158203125e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "MANTa", "normalizedForm": "MANTa", "offsetStart": 25, "offsetEnd": 30}, "context": "In this work, we propose MANTa, a Module for Adaptive Neural TokenizAtion.", "mentionContextAttributes": {"used": {"value": false, "score": 5.6862831115722656e-05}, "created": {"value": true, "score": 0.9999357461929321}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999856948852539}, "created": {"value": true, "score": 0.9999426603317261}, "shared": {"value": false, "score": 4.76837158203125e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "MANTa", "normalizedForm": "MANTa", "offsetStart": 25, "offsetEnd": 30}, "context": "In this work, we present MANTa, a fully differentiable module that learns to segment input byte sequences into blocks of arbitrary lengths, and constructs a robust representation for these blocks.", "mentionContextAttributes": {"used": {"value": false, "score": 3.2067298889160156e-05}, "created": {"value": true, "score": 0.9999426603317261}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999856948852539}, "created": {"value": true, "score": 0.9999426603317261}, "shared": {"value": false, "score": 4.76837158203125e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "MANTa", "normalizedForm": "MANTa", "offsetStart": 26, "offsetEnd": 31}, "context": "Nonetheless, we note that MANTa-LM reaches a better performance than its subword tokenization counterpart T5.", "mentionContextAttributes": {"used": {"value": false, "score": 0.01023191213607788}, "created": {"value": false, "score": 0.0001475811004638672}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999856948852539}, "created": {"value": true, "score": 0.9999426603317261}, "shared": {"value": false, "score": 4.76837158203125e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "MANTa", "normalizedForm": "MANTa", "offsetStart": 26, "offsetEnd": 31}, "context": "This is essential to make MANTa-LM work.", "mentionContextAttributes": {"used": {"value": false, "score": 0.00018268823623657227}, "created": {"value": false, "score": 0.12631696462631226}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999856948852539}, "created": {"value": true, "score": 0.9999426603317261}, "shared": {"value": false, "score": 4.76837158203125e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "MANTa", "normalizedForm": "MANTa", "offsetStart": 27, "offsetEnd": 32}, "context": "In this work, we introduce MANTa, a gradient-based tokenizer and embedding module.", "mentionContextAttributes": {"used": {"value": false, "score": 3.445148468017578e-05}, "created": {"value": true, "score": 0.9999376535415649}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999856948852539}, "created": {"value": true, "score": 0.9999426603317261}, "shared": {"value": false, "score": 4.76837158203125e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "MANTa", "normalizedForm": "MANTa", "offsetStart": 29, "offsetEnd": 34}, "context": "Training We train T5 Small , MANTa-LM Small , and MANTa-LM Base for 65k steps with a batch size of 1024.", "mentionContextAttributes": {"used": {"value": false, "score": 0.0013774633407592773}, "created": {"value": false, "score": 0.0009494423866271973}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999856948852539}, "created": {"value": true, "score": 0.9999426603317261}, "shared": {"value": false, "score": 4.76837158203125e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "MANTa", "normalizedForm": "MANTa", "offsetStart": 29, "offsetEnd": 34}, "context": "We also found that a trained MANTa produced spiked separation probabilities, meaning that it converged towards a \"hard\" segmentation.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9999760389328003}, "created": {"value": false, "score": 5.841255187988281e-06}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999856948852539}, "created": {"value": true, "score": 0.9999426603317261}, "shared": {"value": false, "score": 4.76837158203125e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Charformer", "normalizedForm": "Charformer", "offsetStart": 31, "offsetEnd": 41}, "context": "Indeed, contrary to CANINE and Charformer, MANTa disentangles the segmentation of blocks from their representations, allowing to study each part separately.", "mentionContextAttributes": {"used": {"value": false, "score": 0.004262566566467285}, "created": {"value": false, "score": 3.2186508178710938e-06}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9676150679588318}, "created": {"value": false, "score": 0.2409656047821045}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "references": [{"label": "(Tay et al., 2021)", "normalizedForm": "Tay et al., 2021", "refKey": 23}, {"label": "(Tay et al., 2021)", "normalizedForm": "Tay et al., 2021", "refKey": 23}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "MANTa", "normalizedForm": "MANTa", "offsetStart": 33, "offsetEnd": 38}, "context": "Figure 4: A detailed view of the MANTa module described in section 3.1.", "mentionContextAttributes": {"used": {"value": false, "score": 0.028626322746276855}, "created": {"value": false, "score": 7.283687591552734e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999856948852539}, "created": {"value": true, "score": 0.9999426603317261}, "shared": {"value": false, "score": 4.76837158203125e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "MANTa", "normalizedForm": "MANTa", "offsetStart": 38, "offsetEnd": 43}, "context": "By removing this static bottleneck in MANTa-LM, we hope that it should be able to adapt more easily to new domains.", "mentionContextAttributes": {"used": {"value": false, "score": 0.00029581785202026367}, "created": {"value": false, "score": 0.46009427309036255}, "shared": {"value": false, "score": 4.76837158203125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999856948852539}, "created": {"value": true, "score": 0.9999426603317261}, "shared": {"value": false, "score": 4.76837158203125e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "MANTa", "normalizedForm": "MANTa", "offsetStart": 39, "offsetEnd": 44}, "context": "We include here the scores obtained by MANTa-LM on the GLUE test sets for reproducibility and future comparisons.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9998971223831177}, "created": {"value": false, "score": 0.0014247894287109375}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999856948852539}, "created": {"value": true, "score": 0.9999426603317261}, "shared": {"value": false, "score": 4.76837158203125e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "MANTa", "normalizedForm": "MANTa", "offsetStart": 42, "offsetEnd": 47}, "context": "To evaluate this hypothesis, we study how MANTa-LM behaves on both naturally occurring text variation and multiple levels of synthetic noise.", "mentionContextAttributes": {"used": {"value": true, "score": 0.7615897059440613}, "created": {"value": false, "score": 0.008267521858215332}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999856948852539}, "created": {"value": true, "score": 0.9999426603317261}, "shared": {"value": false, "score": 4.76837158203125e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "MANTa", "normalizedForm": "MANTa", "offsetStart": 43, "offsetEnd": 48}, "context": "On the contrary, in the TRAIN-DEV setting, MANTa-LM can be finetuned as well as ByT5 for all levels of noise, while the performance of T5 quickly degrades.", "mentionContextAttributes": {"used": {"value": false, "score": 0.0008023381233215332}, "created": {"value": false, "score": 3.4689903259277344e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999856948852539}, "created": {"value": true, "score": 0.9999426603317261}, "shared": {"value": false, "score": 4.76837158203125e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "MANTa", "normalizedForm": "MANTa", "offsetStart": 43, "offsetEnd": 48}, "context": "In terms of speed, we compare our model to MANTa-LM Small to T5 Small counterparts: one that is trained at the classical subword-level, and one trained at byte-level, hence using sequences that are roughly 4 times longer.", "mentionContextAttributes": {"used": {"value": false, "score": 0.07085329294204712}, "created": {"value": false, "score": 0.00022369623184204102}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999856948852539}, "created": {"value": true, "score": 0.9999426603317261}, "shared": {"value": false, "score": 4.76837158203125e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "MANTa", "normalizedForm": "MANTa", "offsetStart": 43, "offsetEnd": 48}, "context": "Indeed, contrary to CANINE and Charformer, MANTa disentangles the segmentation of blocks from their representations, allowing to study each part separately.", "mentionContextAttributes": {"used": {"value": false, "score": 0.004262566566467285}, "created": {"value": false, "score": 3.2186508178710938e-06}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999856948852539}, "created": {"value": true, "score": 0.9999426603317261}, "shared": {"value": false, "score": 4.76837158203125e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "MANTa", "normalizedForm": "MANTa", "offsetStart": 50, "offsetEnd": 55}, "context": "Training We train T5 Small , MANTa-LM Small , and MANTa-LM Base for 65k steps with a batch size of 1024.", "mentionContextAttributes": {"used": {"value": false, "score": 0.0013774633407592773}, "created": {"value": false, "score": 0.0009494423866271973}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999856948852539}, "created": {"value": true, "score": 0.9999426603317261}, "shared": {"value": false, "score": 4.76837158203125e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "MANTa", "normalizedForm": "MANTa", "offsetStart": 51, "offsetEnd": 56}, "context": "Once we obtain block embeddings, the final step in MANTa consists in truncating sequences to a length 4 times smaller than the original byte sequence, as described in Section 3.1.3.", "mentionContextAttributes": {"used": {"value": false, "score": 0.007280588150024414}, "created": {"value": false, "score": 0.00010526180267333984}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999856948852539}, "created": {"value": true, "score": 0.9999426603317261}, "shared": {"value": false, "score": 4.76837158203125e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "MANTa", "normalizedForm": "MANTa", "offsetStart": 56, "offsetEnd": 61}, "context": "Hyperparameters We pre-train two versions of our model: MANTa-LM Small and MANTa-LM Base .", "mentionContextAttributes": {"used": {"value": false, "score": 0.00013148784637451172}, "created": {"value": false, "score": 0.45698583126068115}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999856948852539}, "created": {"value": true, "score": 0.9999426603317261}, "shared": {"value": false, "score": 4.76837158203125e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Charformer", "normalizedForm": "Charformer", "offsetStart": 56, "offsetEnd": 66}, "context": "Overall, MANTa-LM exhibits a performance slightly below Charformer but stays within a small margin on average (1.1 points below).", "mentionContextAttributes": {"used": {"value": false, "score": 0.0026983022689819336}, "created": {"value": false, "score": 2.9206275939941406e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9676150679588318}, "created": {"value": false, "score": 0.2409656047821045}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "references": [{"label": "(Tay et al., 2021)", "normalizedForm": "Tay et al., 2021", "refKey": 23}, {"label": "(Tay et al., 2021)", "normalizedForm": "Tay et al., 2021", "refKey": 23}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "MANTa", "normalizedForm": "MANTa", "offsetStart": 60, "offsetEnd": 65}, "context": "Caching greatly lowers the speed and memory requirements of MANTa, allowing to save L B -1 element-wise products.", "mentionContextAttributes": {"used": {"value": false, "score": 0.00018256902694702148}, "created": {"value": false, "score": 0.00012481212615966797}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999856948852539}, "created": {"value": true, "score": 0.9999426603317261}, "shared": {"value": false, "score": 4.76837158203125e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "MANTa", "normalizedForm": "MANTa", "offsetStart": 67, "offsetEnd": 72}, "context": "For each byte, we retrieve the expected block position produced by MANTa and approximate it with the closest integer to mimic hard tokenization.", "mentionContextAttributes": {"used": {"value": true, "score": 0.998123824596405}, "created": {"value": false, "score": 8.821487426757812e-06}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999856948852539}, "created": {"value": true, "score": 0.9999426603317261}, "shared": {"value": false, "score": 4.76837158203125e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "MANTa", "normalizedForm": "MANTa", "offsetStart": 75, "offsetEnd": 80}, "context": "Hyperparameters We pre-train two versions of our model: MANTa-LM Small and MANTa-LM Base .", "mentionContextAttributes": {"used": {"value": false, "score": 0.00013148784637451172}, "created": {"value": false, "score": 0.45698583126068115}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999856948852539}, "created": {"value": true, "score": 0.9999426603317261}, "shared": {"value": false, "score": 4.76837158203125e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "MANTa", "normalizedForm": "MANTa", "offsetStart": 76, "offsetEnd": 81}, "context": "We also introduce MANTa-LM, a Transformer encoder-decoder that incorporates MANTa and that is trained end-to-end.", "mentionContextAttributes": {"used": {"value": false, "score": 5.1975250244140625e-05}, "created": {"value": true, "score": 0.9999161958694458}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999856948852539}, "created": {"value": true, "score": 0.9999426603317261}, "shared": {"value": false, "score": 4.76837158203125e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Charformer", "normalizedForm": "Charformer", "offsetStart": 87, "offsetEnd": 97}, "context": "While CANINE downsamples sequences using a fixed rate after byte contextualization and Charformer's GBST (Gradient Based Subword Tokenizer) pools representations created using various downsampling rates, MANTa only applies downsampling right before the LM to limit the length of block sequences.", "mentionContextAttributes": {"used": {"value": false, "score": 0.004469335079193115}, "created": {"value": false, "score": 1.5497207641601562e-06}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9676150679588318}, "created": {"value": false, "score": 0.2409656047821045}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "references": [{"label": "(Tay et al., 2021)", "normalizedForm": "Tay et al., 2021", "refKey": 23}, {"label": "(Tay et al., 2021)", "normalizedForm": "Tay et al., 2021", "refKey": 23}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "MANTa", "normalizedForm": "MANTa", "offsetStart": 95, "offsetEnd": 100}, "context": "We train this module jointly with an encoderdecoder LM on a span denoising objective to obtain MANTa-LM.", "mentionContextAttributes": {"used": {"value": true, "score": 0.8099154829978943}, "created": {"value": true, "score": 0.9865140318870544}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999856948852539}, "created": {"value": true, "score": 0.9999426603317261}, "shared": {"value": false, "score": 4.76837158203125e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "MANTa", "normalizedForm": "MANTa", "offsetStart": 99, "offsetEnd": 104}, "context": "Constrained by limited computational resources, we were unable to assess their exact importance on MANTa's performance.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9999856948852539}, "created": {"value": false, "score": 0.0011678338050842285}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999856948852539}, "created": {"value": true, "score": 0.9999426603317261}, "shared": {"value": false, "score": 4.76837158203125e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Charformer", "normalizedForm": "Charformer", "offsetStart": 102, "offsetEnd": 112}, "context": "In addition, it enables a better comparison with other tokenizer-free models trained using it such as Charformer.", "mentionContextAttributes": {"used": {"value": false, "score": 3.814697265625e-05}, "created": {"value": false, "score": 0.0004150271415710449}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9676150679588318}, "created": {"value": false, "score": 0.2409656047821045}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "references": [{"label": "(Tay et al., 2021)", "normalizedForm": "Tay et al., 2021", "refKey": 23}, {"label": "(Tay et al., 2021)", "normalizedForm": "Tay et al., 2021", "refKey": 23}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "MANTa", "normalizedForm": "MANTa", "offsetStart": 105, "offsetEnd": 110}, "context": "By learning a soft, adaptive segmentation of input sequences jointly with the LM pre-training objective, MANTa-LM produces byte-based representations with sequence lengths similar to those produced by static subword tokenizers. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.00016391277313232422}, "created": {"value": false, "score": 0.00015604496002197266}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999856948852539}, "created": {"value": true, "score": 0.9999426603317261}, "shared": {"value": false, "score": 4.76837158203125e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Charformer", "normalizedForm": "Charformer", "offsetStart": 114, "offsetEnd": 124}, "context": "The development sets are used in the main body to allow a fair comparison, as the test scores are not reported in Charformer (Tay et al., 2021) and CharBERT (Ma et al., 2020)", "mentionContextAttributes": {"used": {"value": true, "score": 0.9676150679588318}, "created": {"value": false, "score": 3.7670135498046875e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9676150679588318}, "created": {"value": false, "score": 0.2409656047821045}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "references": [{"label": "(Tay et al., 2021)", "normalizedForm": "Tay et al., 2021", "refKey": 23, "offsetStart": 31296, "offsetEnd": 31314}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "CANINE", "normalizedForm": "CANINE", "offsetStart": 116, "offsetEnd": 143}, "context": "We employ a radically different downsampling approach compared to other gradient-based tokenization methods such as CANINE (Clark et al., 2022) or Charformer (Tay et al., 2021). ", "mentionContextAttributes": {"used": {"value": false, "score": 0.00013518333435058594}, "created": {"value": false, "score": 0.00037157535552978516}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": false, "score": 0.004469335079193115}, "created": {"value": false, "score": 0.00037157535552978516}, "shared": {"value": false, "score": 2.384185791015625e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Charformer", "normalizedForm": "Charformer", "offsetStart": 127, "offsetEnd": 137}, "context": "On the other hand, constructing sentence representations with byte-level information helps and our model is more accurate than Charformer. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.00012373924255371094}, "created": {"value": false, "score": 0.2409656047821045}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9676150679588318}, "created": {"value": false, "score": 0.2409656047821045}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "references": [{"label": "(Tay et al., 2021)", "normalizedForm": "Tay et al., 2021", "refKey": 23}, {"label": "(Tay et al., 2021)", "normalizedForm": "Tay et al., 2021", "refKey": 23}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "MANTa", "normalizedForm": "MANTa", "offsetStart": 143, "offsetEnd": 148}, "context": "We also argue that our method yields more explainable pooled representations as the segmentation can be explicitly derived from the outputs of MANTa.", "mentionContextAttributes": {"used": {"value": false, "score": 0.00023692846298217773}, "created": {"value": false, "score": 0.001002490520477295}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999856948852539}, "created": {"value": true, "score": 0.9999426603317261}, "shared": {"value": false, "score": 4.76837158203125e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Charformer", "normalizedForm": "Charformer", "offsetStart": 147, "offsetEnd": 157}, "context": "We employ a radically different downsampling approach compared to other gradient-based tokenization methods such as CANINE (Clark et al., 2022) or Charformer (Tay et al., 2021).", "mentionContextAttributes": {"used": {"value": false, "score": 0.00013518333435058594}, "created": {"value": false, "score": 0.00037157535552978516}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9676150679588318}, "created": {"value": false, "score": 0.2409656047821045}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "references": [{"label": "(Tay et al., 2021)", "normalizedForm": "Tay et al., 2021", "refKey": 23, "offsetStart": 25264, "offsetEnd": 25282}, {"label": "(Tay et al., 2021)", "normalizedForm": "Tay et al., 2021", "refKey": 23, "offsetStart": 25264, "offsetEnd": 25282}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "MANTa", "normalizedForm": "MANTa", "offsetStart": 148, "offsetEnd": 153}, "context": "To avoid prohibitive computation costs and ensure fairness in terms of available resources between models, we limit its training time to the one of MANTa-LM Small .", "mentionContextAttributes": {"used": {"value": false, "score": 0.007356226444244385}, "created": {"value": false, "score": 0.0002040266990661621}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999856948852539}, "created": {"value": true, "score": 0.9999426603317261}, "shared": {"value": false, "score": 4.76837158203125e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "WordPiece", "normalizedForm": "WordPiece", "offsetStart": 175, "offsetEnd": 202}, "context": "Non-neural subword-level tokenization methods have dominated in the last few years as the default way to encode textual data, the most used being BPE (Sennrich et al., 2016), WordPiece (Wu et al., 2016) and Unigram (Kudo, 2018). ", "mentionContextAttributes": {"used": {"value": false, "score": 0.0011322498321533203}, "created": {"value": false, "score": 0.00021725893020629883}, "shared": {"value": false, "score": 1.0728836059570312e-06}}, "documentContextAttributes": {"used": {"value": false, "score": 0.0011322498321533203}, "created": {"value": false, "score": 0.00021725893020629883}, "shared": {"value": false, "score": 1.0728836059570312e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "CharacterBERT", "normalizedForm": "CharacterBERT", "offsetStart": 196, "offsetEnd": 209}, "context": "Although we notice a significant drop in perfor-mance compared to the encoder models trained by (El Boukkouri et al., 2020), we believe this drop may be due to the different pretraining data used-CharacterBERT uses splits of Wikipedia, which may be helpful to learn some technical terms related to the clinical domain-, and the different model sizes-CharacterBERT uses all of its parameters to encode example, while we keep half of the parameters in the decoder. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9799827337265015}, "created": {"value": false, "score": 2.9802322387695312e-06}, "shared": {"value": false, "score": 5.960464477539062e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9799827337265015}, "created": {"value": false, "score": 2.9802322387695312e-06}, "shared": {"value": false, "score": 5.960464477539062e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "MANTa", "normalizedForm": "MANTa", "offsetStart": 204, "offsetEnd": 209}, "context": "While CANINE downsamples sequences using a fixed rate after byte contextualization and Charformer's GBST (Gradient Based Subword Tokenizer) pools representations created using various downsampling rates, MANTa only applies downsampling right before the LM to limit the length of block sequences.", "mentionContextAttributes": {"used": {"value": false, "score": 0.004469335079193115}, "created": {"value": false, "score": 1.5497207641601562e-06}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999856948852539}, "created": {"value": true, "score": 0.9999426603317261}, "shared": {"value": false, "score": 4.76837158203125e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "CharacterBERT", "normalizedForm": "CharacterBERT", "offsetStart": 350, "offsetEnd": 363}, "context": "Although we notice a significant drop in perfor-mance compared to the encoder models trained by (El Boukkouri et al., 2020), we believe this drop may be due to the different pretraining data used-CharacterBERT uses splits of Wikipedia, which may be helpful to learn some technical terms related to the clinical domain-, and the different model sizes-CharacterBERT uses all of its parameters to encode example, while we keep half of the parameters in the decoder. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9799827337265015}, "created": {"value": false, "score": 2.9802322387695312e-06}, "shared": {"value": false, "score": 5.960464477539062e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9799827337265015}, "created": {"value": false, "score": 2.9802322387695312e-06}, "shared": {"value": false, "score": 5.960464477539062e-07}}}], "references": [{"refKey": 23, "tei": "<biblStruct xml:id=\"b23\">\n\t<analytic>\n\t\t<title level=\"a\" type=\"main\">Charformer: Fast character transformers via gradientbased subword tokenization</title>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Yi</forename><surname>Tay</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Sebastian</forename><surname>Vinh Q Tran</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Jai</forename><surname>Ruder</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Hyung</forename><forename type=\"middle\">Won</forename><surname>Gupta</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Dara</forename><surname>Chung</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Zhen</forename><surname>Bahri</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Simon</forename><surname>Qin</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Cong</forename><surname>Baumgartner</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Donald</forename><surname>Yu</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><surname>Metzler</surname></persName>\n\t\t</author>\n\t</analytic>\n\t<monogr>\n\t\t<title level=\"m\">International Conference on Learning Representations</title>\n\t\t<imprint>\n\t\t\t<date>2021</date>\n\t\t</imprint>\n\t</monogr>\n</biblStruct>\n"}], "runtime": 14551, "id": "b78c0c2928a75dc4af9069e02a3a809e50d57461", "metadata": {"id": "b78c0c2928a75dc4af9069e02a3a809e50d57461"}, "original_file_path": "../../datalake/Samuel/SOFTware-Sync/downloads/xml/hal-03844262.grobid.tei.xml", "file_name": "hal-03844262.grobid.tei.xml"}