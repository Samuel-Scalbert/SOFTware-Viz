{"application": "software-mentions", "version": "0.8.0", "date": "2024-10-07T12:45+0000", "md5": "252F3A199E6EB2AE0D2A081FF727FA7B", "mentions": [{"type": "software", "software-type": "software", "software-name": {"rawForm": "HatEval", "normalizedForm": "HatEval", "offsetStart": 0, "offsetEnd": 7}, "context": "HatEval corpus. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.00799340009689331}, "created": {"value": false, "score": 0.0008909106254577637}, "shared": {"value": false, "score": 7.152557373046875e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999874830245972}, "created": {"value": true, "score": 0.9849298596382141}, "shared": {"value": false, "score": 7.152557373046875e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "HatEval", "normalizedForm": "HatEval", "offsetStart": 6, "offsetEnd": 13}, "context": "As in HatEval the classes are almost balanced, there is no bias due to imbalanced classes.", "mentionContextAttributes": {"used": {"value": false, "score": 0.0028458833694458008}, "created": {"value": false, "score": 1.5735626220703125e-05}, "shared": {"value": false, "score": 4.76837158203125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999874830245972}, "created": {"value": true, "score": 0.9849298596382141}, "shared": {"value": false, "score": 7.152557373046875e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "HatEval", "normalizedForm": "HatEval", "offsetStart": 7, "offsetEnd": 14}, "context": "In the HatEval corpus, the annotation of a tweet is a binary value indicating if HS is occurring against women or immigrants. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.045877814292907715}, "created": {"value": false, "score": 3.8504600524902344e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999874830245972}, "created": {"value": true, "score": 0.9849298596382141}, "shared": {"value": false, "score": 7.152557373046875e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "HatEval", "normalizedForm": "HatEval", "offsetStart": 7, "offsetEnd": 14}, "context": "On the HatEval, the proposed system classifies better non-hateful tweets than the baseline system.", "mentionContextAttributes": {"used": {"value": false, "score": 0.0001925826072692871}, "created": {"value": false, "score": 0.028949737548828125}, "shared": {"value": false, "score": 5.960464477539062e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999874830245972}, "created": {"value": true, "score": 0.9849298596382141}, "shared": {"value": false, "score": 7.152557373046875e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "HatEval", "normalizedForm": "HatEval", "offsetStart": 11, "offsetEnd": 18}, "context": "As for the HatEval corpus, we use a small part of training as the validation part.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9722750186920166}, "created": {"value": false, "score": 1.5139579772949219e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999874830245972}, "created": {"value": true, "score": 0.9849298596382141}, "shared": {"value": false, "score": 7.152557373046875e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "HatEval", "normalizedForm": "HatEval", "offsetStart": 16, "offsetEnd": 23}, "context": "Furthermore, on HatEval corpus, the proposed system with MWEall categories and BERT embedding significantly outperformed the state-of-the-art system FERMI ranked first at the SemEval2019 shared task 5.", "mentionContextAttributes": {"used": {"value": true, "score": 0.981577455997467}, "created": {"value": false, "score": 4.673004150390625e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999874830245972}, "created": {"value": true, "score": 0.9849298596382141}, "shared": {"value": false, "score": 7.152557373046875e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "HatEval", "normalizedForm": "HatEval", "offsetStart": 17, "offsetEnd": 24}, "context": "For instance, on HatEval, MWEall with BERT embedding configuration achieves the best result with 66.8% of macro-F1. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.3947022557258606}, "created": {"value": false, "score": 1.1086463928222656e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999874830245972}, "created": {"value": true, "score": 0.9849298596382141}, "shared": {"value": false, "score": 7.152557373046875e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "google", "normalizedForm": "google", "offsetStart": 18, "offsetEnd": 24}, "context": "https://tfhub.dev/google/universal-sentence-encoder-large/3", "mentionContextAttributes": {"used": {"value": false, "score": 0.0001748204231262207}, "created": {"value": false, "score": 5.841255187988281e-06}, "shared": {"value": true, "score": 0.7254491448402405}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9859564304351807}, "created": {"value": false, "score": 7.3909759521484375e-06}, "shared": {"value": true, "score": 0.7254491448402405}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "MWEall", "normalizedForm": "MWEall", "offsetStart": 26, "offsetEnd": 32}, "context": "For instance, on HatEval, MWEall with BERT embedding configuration achieves the best result with 66.8% of macro-F1. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.3947022557258606}, "created": {"value": false, "score": 1.1086463928222656e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.999962568283081}, "created": {"value": false, "score": 0.04459911584854126}, "shared": {"value": false, "score": 1.1920928955078125e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "HatEval", "normalizedForm": "HatEval", "offsetStart": 27, "offsetEnd": 34}, "context": "In the training set of the HatEval corpus our parser, described in section 2, annotated 4257 MWEs.", "mentionContextAttributes": {"used": {"value": true, "score": 0.999946117401123}, "created": {"value": false, "score": 0.0029630661010742188}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999874830245972}, "created": {"value": true, "score": 0.9849298596382141}, "shared": {"value": false, "score": 7.152557373046875e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "HatEval", "normalizedForm": "HatEval", "offsetStart": 33, "offsetEnd": 40}, "context": "Table 2 displays the macro-F1 on HatEval and Founta test sets. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.5299389958381653}, "created": {"value": false, "score": 3.3020973205566406e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999874830245972}, "created": {"value": true, "score": 0.9849298596382141}, "shared": {"value": false, "score": 7.152557373046875e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "HatEval", "normalizedForm": "HatEval", "offsetStart": 33, "offsetEnd": 40}, "context": "We observe that about 25% of the HatEval training tweets contain at least one MWE and so the presence of MWE can influence the HSD performance.", "mentionContextAttributes": {"used": {"value": false, "score": 0.45499300956726074}, "created": {"value": false, "score": 1.4424324035644531e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999874830245972}, "created": {"value": true, "score": 0.9849298596382141}, "shared": {"value": false, "score": 7.152557373046875e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "MWEall", "normalizedForm": "MWEall", "offsetStart": 34, "offsetEnd": 40}, "context": "Finally, the proposed system with MWEall and BERT embedding for HatEval outperforms the state-of-the-art system FERMI submitted at HatEval competition (SemEval task 5): 66.8% for our system versus 65% for FERMI of macro-F1 [8].", "mentionContextAttributes": {"used": {"value": false, "score": 0.007944941520690918}, "created": {"value": false, "score": 4.0531158447265625e-06}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.999962568283081}, "created": {"value": false, "score": 0.04459911584854126}, "shared": {"value": false, "score": 1.1920928955078125e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "MWEall", "normalizedForm": "MWEall", "offsetStart": 40, "offsetEnd": 46}, "context": "With BERT embedding it is better to use MWEall categories.", "mentionContextAttributes": {"used": {"value": false, "score": 0.0222625732421875}, "created": {"value": false, "score": 1.0013580322265625e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.999962568283081}, "created": {"value": false, "score": 0.04459911584854126}, "shared": {"value": false, "score": 1.1920928955078125e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "HatEval", "normalizedForm": "HatEval", "offsetStart": 44, "offsetEnd": 51}, "context": "We apply the same pre-processing as for the HatEval corpus.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9972301125526428}, "created": {"value": false, "score": 0.00013077259063720703}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999874830245972}, "created": {"value": true, "score": 0.9849298596382141}, "shared": {"value": false, "score": 7.152557373046875e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "HatEval", "normalizedForm": "HatEval", "offsetStart": 46, "offsetEnd": 53}, "context": "We study the influence of MWE features on the HatEval corpus, and we use the Founta corpus to confirm our results.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9999874830245972}, "created": {"value": false, "score": 0.00030171871185302734}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999874830245972}, "created": {"value": true, "score": 0.9849298596382141}, "shared": {"value": false, "score": 7.152557373046875e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "HatEval", "normalizedForm": "HatEval", "offsetStart": 49, "offsetEnd": 56}, "context": "The results were validated on two tweet corpora: HatEval and Founta.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9999798536300659}, "created": {"value": false, "score": 6.198883056640625e-06}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999874830245972}, "created": {"value": true, "score": 0.9849298596382141}, "shared": {"value": false, "score": 7.152557373046875e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "MWEall", "normalizedForm": "MWEall", "offsetStart": 57, "offsetEnd": 63}, "context": "Furthermore, on HatEval corpus, the proposed system with MWEall categories and BERT embedding significantly outperformed the state-of-the-art system FERMI ranked first at the SemEval2019 shared task 5.", "mentionContextAttributes": {"used": {"value": true, "score": 0.981577455997467}, "created": {"value": false, "score": 4.673004150390625e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.999962568283081}, "created": {"value": false, "score": 0.04459911584854126}, "shared": {"value": false, "score": 1.1920928955078125e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "HatEval", "normalizedForm": "HatEval", "offsetStart": 64, "offsetEnd": 71}, "context": "Finally, the proposed system with MWEall and BERT embedding for HatEval outperforms the state-of-the-art system FERMI submitted at HatEval competition (SemEval task 5): 66.8% for our system versus 65% for FERMI of macro-F1 [8].", "mentionContextAttributes": {"used": {"value": false, "score": 0.007944941520690918}, "created": {"value": false, "score": 4.0531158447265625e-06}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999874830245972}, "created": {"value": true, "score": 0.9849298596382141}, "shared": {"value": false, "score": 7.152557373046875e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "google", "normalizedForm": "google", "offsetStart": 64, "offsetEnd": 72}, "context": "For the USE embedding, we use the pre-trained model provided by google 3 (space dimension is 512) without fine-tuning.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9859564304351807}, "created": {"value": false, "score": 7.3909759521484375e-06}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9859564304351807}, "created": {"value": false, "score": 7.3909759521484375e-06}, "shared": {"value": true, "score": 0.7254491448402405}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "MWEall", "normalizedForm": "MWEall", "offsetStart": 72, "offsetEnd": 78}, "context": "We carried out experiments with the different groups of MWE categories: MWEall, including all MWE categories, and the combination of VMWE5 and MWE5.", "mentionContextAttributes": {"used": {"value": true, "score": 0.999962568283081}, "created": {"value": false, "score": 0.04459911584854126}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.999962568283081}, "created": {"value": false, "score": 0.04459911584854126}, "shared": {"value": false, "score": 1.1920928955078125e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "MWEall", "normalizedForm": "MWEall", "offsetStart": 73, "offsetEnd": 79}, "context": "Using the three-branch neural network with only VMWE5 or MWE5 instead of MWEall seems to be interesting only for word2vec embedding.", "mentionContextAttributes": {"used": {"value": false, "score": 0.000879824161529541}, "created": {"value": false, "score": 5.078315734863281e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.999962568283081}, "created": {"value": false, "score": 0.04459911584854126}, "shared": {"value": false, "score": 1.1920928955078125e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "HatEval", "normalizedForm": "HatEval", "offsetStart": 80, "offsetEnd": 87}, "context": "Table 1 shows MWEs that appear only in hateful or non-hateful tweets or both in HatEval training part.", "mentionContextAttributes": {"used": {"value": false, "score": 0.4694749712944031}, "created": {"value": false, "score": 1.1205673217773438e-05}, "shared": {"value": false, "score": 4.76837158203125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999874830245972}, "created": {"value": true, "score": 0.9849298596382141}, "shared": {"value": false, "score": 7.152557373046875e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "HatEval", "normalizedForm": "HatEval", "offsetStart": 82, "offsetEnd": 89}, "context": "We observe that some MWE categories, as symbol and interjection, do not appear in HatEval training set.", "mentionContextAttributes": {"used": {"value": false, "score": 0.011044681072235107}, "created": {"value": false, "score": 5.614757537841797e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999874830245972}, "created": {"value": true, "score": 0.9849298596382141}, "shared": {"value": false, "score": 7.152557373046875e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "HatEval", "normalizedForm": "HatEval", "offsetStart": 86, "offsetEnd": 93}, "context": "We think that the balance between the classes plays an important role: in the case of HatEval corpus, the classes are balanced, in the case of Founta, the classes are unbalanced.", "mentionContextAttributes": {"used": {"value": false, "score": 0.0023404359817504883}, "created": {"value": false, "score": 7.534027099609375e-05}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999874830245972}, "created": {"value": true, "score": 0.9849298596382141}, "shared": {"value": false, "score": 7.152557373046875e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "MWEall", "normalizedForm": "MWEall", "offsetStart": 96, "offsetEnd": 102}, "context": "We compare the confusion matrices of two systems: the baseline system and the proposed one with MWEall and BERT embeddings.", "mentionContextAttributes": {"used": {"value": false, "score": 0.0003813505172729492}, "created": {"value": false, "score": 0.005844712257385254}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.999962568283081}, "created": {"value": false, "score": 0.04459911584854126}, "shared": {"value": false, "score": 1.1920928955078125e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "HatEval", "normalizedForm": "HatEval", "offsetStart": 100, "offsetEnd": 107}, "context": "Our baseline system without MWE features, called USE in Table 2, achieves a 65.7% macro-F1 score on HatEval test set.", "mentionContextAttributes": {"used": {"value": false, "score": 7.128715515136719e-05}, "created": {"value": true, "score": 0.6601331830024719}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999874830245972}, "created": {"value": true, "score": 0.9849298596382141}, "shared": {"value": false, "score": 7.152557373046875e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "MWEall", "normalizedForm": "MWEall", "offsetStart": 105, "offsetEnd": 111}, "context": "To analyze further MWE features, we experiment with different groups of MWE categories: VMWE5, MWE5, and MWEall.", "mentionContextAttributes": {"used": {"value": true, "score": 0.8204280734062195}, "created": {"value": false, "score": 0.000869452953338623}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.999962568283081}, "created": {"value": false, "score": 0.04459911584854126}, "shared": {"value": false, "score": 1.1920928955078125e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "HatEval", "normalizedForm": "HatEval", "offsetStart": 121, "offsetEnd": 128}, "context": "The goal of our experiments is to study the impact of MWEs on automatic hate speech detection for two different corpora: HatEval and Founta.", "mentionContextAttributes": {"used": {"value": false, "score": 0.01011127233505249}, "created": {"value": true, "score": 0.9849298596382141}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999874830245972}, "created": {"value": true, "score": 0.9849298596382141}, "shared": {"value": false, "score": 7.152557373046875e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "HatEval", "normalizedForm": "HatEval", "offsetStart": 131, "offsetEnd": 138}, "context": "Finally, the proposed system with MWEall and BERT embedding for HatEval outperforms the state-of-the-art system FERMI submitted at HatEval competition (SemEval task 5): 66.8% for our system versus 65% for FERMI of macro-F1 [8].", "mentionContextAttributes": {"used": {"value": false, "score": 0.007944941520690918}, "created": {"value": false, "score": 4.0531158447265625e-06}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999874830245972}, "created": {"value": true, "score": 0.9849298596382141}, "shared": {"value": false, "score": 7.152557373046875e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "HatEval", "normalizedForm": "HatEval", "offsetStart": 134, "offsetEnd": 141}, "context": "We use two tweets corpora to show that our approach is domain-independent: the English corpus of SemEval2019 task 5 subTask A (called HatEval in the following) [1] and Founta corpora [5].", "mentionContextAttributes": {"used": {"value": true, "score": 0.6937289834022522}, "created": {"value": false, "score": 0.00020486116409301758}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999874830245972}, "created": {"value": true, "score": 0.9849298596382141}, "shared": {"value": false, "score": 7.152557373046875e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "HatEval", "normalizedForm": "HatEval", "offsetStart": 143, "offsetEnd": 150}, "context": "To perform a deeper analysis, we focus our observations on only the tweets from the test sets containing at least one MWE: 758 tweets from the HatEval test set and 3508 tweets from the Founta test set.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9999140501022339}, "created": {"value": false, "score": 8.893013000488281e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999874830245972}, "created": {"value": true, "score": 0.9849298596382141}, "shared": {"value": false, "score": 7.152557373046875e-07}}}], "references": [], "runtime": 11081, "id": "03272b8729ee14e90d238dc7a8a8630960f2d115", "metadata": {"id": "03272b8729ee14e90d238dc7a8a8630960f2d115"}, "original_file_path": "../../datalake/Samuel/SOFTware-Sync/downloads/xml/hal-03231047.grobid.tei.xml", "file_name": "hal-03231047.grobid.tei.xml"}