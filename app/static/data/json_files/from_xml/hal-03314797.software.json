{"application": "software-mentions", "version": "0.8.0", "date": "2024-10-07T12:02+0000", "md5": "8F94403E1134D3263F3A74BCC338D6C8", "mentions": [{"type": "software", "software-type": "software", "software-name": {"rawForm": "RoBERTa", "normalizedForm": "RoBERTa", "offsetStart": 0, "offsetEnd": 7}, "context": "RoBERTa with argumentation features can outperform results on \"Thought-terminating Cliches\".", "mentionContextAttributes": {"used": {"value": false, "score": 0.0002773404121398926}, "created": {"value": false, "score": 5.7220458984375e-06}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9972279667854309}, "created": {"value": false, "score": 0.44860905408859253}, "shared": {"value": false, "score": 3.5762786865234375e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "SemEval", "normalizedForm": "SemEval", "offsetStart": 3, "offsetEnd": 10}, "context": "In SemEval'20 T11 dataset, 14 propaganda techniques are annotated. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.47565966844558716}, "created": {"value": false, "score": 2.1457672119140625e-06}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9958643913269043}, "created": {"value": false, "score": 0.2004706859588623}, "shared": {"value": false, "score": 1.0728836059570312e-05}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "SemEval", "normalizedForm": "SemEval", "offsetStart": 4, "offsetEnd": 11}, "context": "For SemEval'20-T11, we do not have the scores from the challenge (the binary task was not proposed), but we compare the obtained results with the replicated architectures of SOTA models.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9958643913269043}, "created": {"value": false, "score": 1.9431114196777344e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9958643913269043}, "created": {"value": false, "score": 0.2004706859588623}, "shared": {"value": false, "score": 1.0728836059570312e-05}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "RoBERTa", "normalizedForm": "RoBERTa", "offsetStart": 4, "offsetEnd": 11}, "context": "For RoBERTa, we take roberta-base pre-trained model with learning rate of 2e-5 with \u03b1 of 0.5.", "mentionContextAttributes": {"used": {"value": true, "score": 0.7706490159034729}, "created": {"value": false, "score": 2.181529998779297e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9972279667854309}, "created": {"value": false, "score": 0.44860905408859253}, "shared": {"value": false, "score": 3.5762786865234375e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "SemEval", "normalizedForm": "SemEval", "offsetStart": 24, "offsetEnd": 31}, "context": "As a follow up, in 2020 SemEval proposed a shared task (T11) (Da San Martino et al., 2020a) reducing the number of propaganda categories with respect to NLP4IF'19, and proposing a more restrictive evaluation scheme. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.000496983528137207}, "created": {"value": false, "score": 0.2004706859588623}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9958643913269043}, "created": {"value": false, "score": 0.2004706859588623}, "shared": {"value": false, "score": 1.0728836059570312e-05}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "SemEval", "normalizedForm": "SemEval", "offsetStart": 34, "offsetEnd": 41}, "context": "The systems that took part in the SemEval 2020 Challenge -Task 11 represent the most recent approaches to identify propaganda techniques based on given propagandist spans.", "mentionContextAttributes": {"used": {"value": true, "score": 0.679127037525177}, "created": {"value": false, "score": 0.002300560474395752}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9958643913269043}, "created": {"value": false, "score": 0.2004706859588623}, "shared": {"value": false, "score": 1.0728836059570312e-05}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "RoBERTa", "normalizedForm": "RoBERTa", "offsetStart": 43, "offsetEnd": 50}, "context": "For all the tested architectures (BERT and RoBERTa), we use the same type of transformer model to produce logits (L) regarding the sentence-level and span-level individually.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9721179604530334}, "created": {"value": false, "score": 2.6345252990722656e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9972279667854309}, "created": {"value": false, "score": 0.44860905408859253}, "shared": {"value": false, "score": 3.5762786865234375e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "RoBERTa", "normalizedForm": "RoBERTa", "offsetStart": 51, "offsetEnd": 58}, "context": "The winning team (Jurkiewicz et al., 2020) applies RoBERTa (robertalarge) with pre-trained model. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9792000651359558}, "created": {"value": false, "score": 7.152557373046875e-06}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9972279667854309}, "created": {"value": false, "score": 0.44860905408859253}, "shared": {"value": false, "score": 3.5762786865234375e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "SemEval", "normalizedForm": "SemEval", "offsetStart": 56, "offsetEnd": 63}, "context": "As mentioned before, the gold labels of the test set of SemEval'20 T11 are not available, but it is possible to submit a system run to the challenge website and to obtain the evaluation score. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.08947908878326416}, "created": {"value": false, "score": 6.508827209472656e-05}, "shared": {"value": false, "score": 1.0728836059570312e-05}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9958643913269043}, "created": {"value": false, "score": 0.2004706859588623}, "shared": {"value": false, "score": 1.0728836059570312e-05}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "AdamW", "normalizedForm": "AdamW", "offsetStart": 62, "offsetEnd": 67}, "context": "We fine-tune the BERT model with a learning rate of 5e-5, and AdamW optimizer.", "mentionContextAttributes": {"used": {"value": false, "score": 0.01438683271408081}, "created": {"value": false, "score": 0.0005432963371276855}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9972279667854309}, "created": {"value": false, "score": 0.0005432963371276855}, "shared": {"value": false, "score": 3.5762786865234375e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "AdamW", "normalizedForm": "AdamW", "offsetStart": 66, "offsetEnd": 71}, "context": "Firstly, the BERT model is used with learning rate of 0.001, with AdamW optimizer. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.7458696961402893}, "created": {"value": false, "score": 1.9669532775878906e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9972279667854309}, "created": {"value": false, "score": 0.0005432963371276855}, "shared": {"value": false, "score": 3.5762786865234375e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "RoBERTa", "normalizedForm": "RoBERTa", "offsetStart": 78, "offsetEnd": 85}, "context": "We investigate different architectures of recent language models (i.e., BERT, RoBERTa), combining them with a rich set of linguistic features ranging from sentiment and emotion to argumentation features, to rhetorical stylistic ones.", "mentionContextAttributes": {"used": {"value": false, "score": 0.0003032684326171875}, "created": {"value": false, "score": 0.44860905408859253}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9972279667854309}, "created": {"value": false, "score": 0.44860905408859253}, "shared": {"value": false, "score": 3.5762786865234375e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "General Inquirer", "normalizedForm": "General Inquirer", "offsetStart": 90, "offsetEnd": 106}, "context": "To analyze the writing style of the messages, we apply the dictionary-based mapping tool \"General Inquirer (v. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9999537467956543}, "created": {"value": false, "score": 3.826618194580078e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999537467956543}, "created": {"value": false, "score": 3.826618194580078e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "RoBERTa", "normalizedForm": "RoBERTa", "offsetStart": 96, "offsetEnd": 103}, "context": "The training set is increased with silver annotation based on gold annotation, and then another RoBERTa model is stacked on top to output the predictions.", "mentionContextAttributes": {"used": {"value": true, "score": 0.991411566734314}, "created": {"value": false, "score": 0.0005429387092590332}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9972279667854309}, "created": {"value": false, "score": 0.44860905408859253}, "shared": {"value": false, "score": 3.5762786865234375e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "SemEval", "normalizedForm": "SemEval", "offsetStart": 101, "offsetEnd": 108}, "context": "To evaluate the proposed approach, we rely on these two standard benchmarks, i.e., the NLP4IF'19 and SemEval'20 datasets.", "mentionContextAttributes": {"used": {"value": false, "score": 0.11501967906951904}, "created": {"value": false, "score": 0.03893136978149414}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9958643913269043}, "created": {"value": false, "score": 0.2004706859588623}, "shared": {"value": false, "score": 1.0728836059570312e-05}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "RoBERTa", "normalizedForm": "RoBERTa", "offsetStart": 161, "offsetEnd": 168}, "context": "We use a pre-trained model base-uncased with a learning rate of 3e-5 for BERT transformer, and a pre-trained model roberta-base with a learning rate of 2e-5 for RoBERTa transformers (hyperparameters: dropout of 0.1 with the max length of 128, batch size of 16 with AdamW optimizer and CrossEntropy loss function).", "mentionContextAttributes": {"used": {"value": true, "score": 0.9972279667854309}, "created": {"value": false, "score": 2.1457672119140625e-06}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9972279667854309}, "created": {"value": false, "score": 0.44860905408859253}, "shared": {"value": false, "score": 3.5762786865234375e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "AdamW", "normalizedForm": "AdamW", "offsetStart": 265, "offsetEnd": 270}, "context": "We use a pre-trained model base-uncased with a learning rate of 3e-5 for BERT transformer, and a pre-trained model roberta-base with a learning rate of 2e-5 for RoBERTa transformers (hyperparameters: dropout of 0.1 with the max length of 128, batch size of 16 with AdamW optimizer and CrossEntropy loss function).", "mentionContextAttributes": {"used": {"value": true, "score": 0.9972279667854309}, "created": {"value": false, "score": 2.1457672119140625e-06}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9972279667854309}, "created": {"value": false, "score": 0.0005432963371276855}, "shared": {"value": false, "score": 3.5762786865234375e-07}}}], "references": [], "runtime": 17880, "id": "c684d3389e55e3b8c9cddd9cebdb837a4f89ef2c", "metadata": {"id": "c684d3389e55e3b8c9cddd9cebdb837a4f89ef2c"}, "original_file_path": "../../datalake/Samuel/SOFTware-Sync/downloads/xml/hal-03314797.grobid.tei.xml", "file_name": "hal-03314797.grobid.tei.xml"}