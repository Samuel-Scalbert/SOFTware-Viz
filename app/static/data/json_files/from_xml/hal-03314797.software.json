{"application": "software-mentions", "version": "0.8.0", "date": "2024-04-12T16:56+0000", "md5": "176BE4B0F5496FC72589AB4FBC7F3C23", "mentions": [{"type": "software", "software-type": "software", "software-name": {"rawForm": "RoBERTa", "normalizedForm": "RoBERTa", "offsetStart": 0, "offsetEnd": 7}, "context": "RoBERTa with argumentation features can outperform results on \"Thought-terminating Cliches\".", "mentionContextAttributes": {"used": {"value": false, "score": 0.0002772808074951172}, "created": {"value": false, "score": 5.781650543212891e-06}, "shared": {"value": false, "score": 4.172325134277344e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9972279667854309}, "created": {"value": false, "score": 0.44861453771591187}, "shared": {"value": false, "score": 4.172325134277344e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "RoBERTa", "normalizedForm": "RoBERTa", "offsetStart": 4, "offsetEnd": 11}, "context": "For RoBERTa, we take roberta-base pre-trained model with learning rate of 2e-5 with \u03b1 of 0.5.", "mentionContextAttributes": {"used": {"value": true, "score": 0.7706480622291565}, "created": {"value": false, "score": 2.187490463256836e-05}, "shared": {"value": false, "score": 1.7881393432617188e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9972279667854309}, "created": {"value": false, "score": 0.44861453771591187}, "shared": {"value": false, "score": 4.172325134277344e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "RoBERTa", "normalizedForm": "RoBERTa", "offsetStart": 43, "offsetEnd": 50}, "context": "For all the tested architectures (BERT and RoBERTa), we use the same type of transformer model to produce logits (L) regarding the sentence-level and span-level individually.", "mentionContextAttributes": {"used": {"value": true, "score": 0.97211754322052}, "created": {"value": false, "score": 2.6404857635498047e-05}, "shared": {"value": false, "score": 2.980232238769531e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9972279667854309}, "created": {"value": false, "score": 0.44861453771591187}, "shared": {"value": false, "score": 4.172325134277344e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "RoBERTa", "normalizedForm": "RoBERTa", "offsetStart": 51, "offsetEnd": 58}, "context": "The winning team (Jurkiewicz et al., 2020) applies RoBERTa (robertalarge) with pre-trained model. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9791997075080872}, "created": {"value": false, "score": 7.212162017822266e-06}, "shared": {"value": false, "score": 2.980232238769531e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9972279667854309}, "created": {"value": false, "score": 0.44861453771591187}, "shared": {"value": false, "score": 4.172325134277344e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "AdamW", "normalizedForm": "AdamW", "offsetStart": 62, "offsetEnd": 67}, "context": "We fine-tune the BERT model with a learning rate of 5e-5, and AdamW optimizer.", "mentionContextAttributes": {"used": {"value": false, "score": 0.01438683271408081}, "created": {"value": false, "score": 0.0005432367324829102}, "shared": {"value": false, "score": 1.7881393432617188e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9972279667854309}, "created": {"value": false, "score": 0.0005432367324829102}, "shared": {"value": false, "score": 3.5762786865234375e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "AdamW", "normalizedForm": "AdamW", "offsetStart": 66, "offsetEnd": 71}, "context": "Firstly, the BERT model is used with learning rate of 0.001, with AdamW optimizer. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.7458742260932922}, "created": {"value": false, "score": 1.9729137420654297e-05}, "shared": {"value": false, "score": 2.980232238769531e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9972279667854309}, "created": {"value": false, "score": 0.0005432367324829102}, "shared": {"value": false, "score": 3.5762786865234375e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "RoBERTa", "normalizedForm": "RoBERTa", "offsetStart": 78, "offsetEnd": 85}, "context": "We investigate different architectures of recent language models (i.e., BERT, RoBERTa), combining them with a rich set of linguistic features ranging from sentiment and emotion to argumentation features, to rhetorical stylistic ones.", "mentionContextAttributes": {"used": {"value": false, "score": 0.0003032684326171875}, "created": {"value": false, "score": 0.44861453771591187}, "shared": {"value": false, "score": 4.172325134277344e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9972279667854309}, "created": {"value": false, "score": 0.44861453771591187}, "shared": {"value": false, "score": 4.172325134277344e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "General Inquirer", "normalizedForm": "General Inquirer", "offsetStart": 90, "offsetEnd": 106}, "context": "To analyze the writing style of the messages, we apply the dictionary-based mapping tool \"General Inquirer (v. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9999536871910095}, "created": {"value": false, "score": 3.832578659057617e-05}, "shared": {"value": false, "score": 1.7881393432617188e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999536871910095}, "created": {"value": false, "score": 3.832578659057617e-05}, "shared": {"value": false, "score": 1.7881393432617188e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "RoBERTa", "normalizedForm": "RoBERTa", "offsetStart": 96, "offsetEnd": 103}, "context": "The training set is increased with silver annotation based on gold annotation, and then another RoBERTa model is stacked on top to output the predictions.", "mentionContextAttributes": {"used": {"value": true, "score": 0.991411566734314}, "created": {"value": false, "score": 0.0005429387092590332}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9972279667854309}, "created": {"value": false, "score": 0.44861453771591187}, "shared": {"value": false, "score": 4.172325134277344e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "RoBERTa", "normalizedForm": "RoBERTa", "offsetStart": 161, "offsetEnd": 168}, "context": "We use a pre-trained model base-uncased with a learning rate of 3e-5 for BERT transformer, and a pre-trained model roberta-base with a learning rate of 2e-5 for RoBERTa transformers (hyperparameters: dropout of 0.1 with the max length of 128, batch size of 16 with AdamW optimizer and CrossEntropy loss function).", "mentionContextAttributes": {"used": {"value": true, "score": 0.9972279667854309}, "created": {"value": false, "score": 2.1457672119140625e-06}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9972279667854309}, "created": {"value": false, "score": 0.44861453771591187}, "shared": {"value": false, "score": 4.172325134277344e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "AdamW", "normalizedForm": "AdamW", "offsetStart": 265, "offsetEnd": 270}, "context": "We use a pre-trained model base-uncased with a learning rate of 3e-5 for BERT transformer, and a pre-trained model roberta-base with a learning rate of 2e-5 for RoBERTa transformers (hyperparameters: dropout of 0.1 with the max length of 128, batch size of 16 with AdamW optimizer and CrossEntropy loss function).", "mentionContextAttributes": {"used": {"value": true, "score": 0.9972279667854309}, "created": {"value": false, "score": 2.1457672119140625e-06}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9972279667854309}, "created": {"value": false, "score": 0.0005432367324829102}, "shared": {"value": false, "score": 3.5762786865234375e-07}}}], "references": [], "runtime": 87258, "id": "972f70a1e52e9db834fe73225f99f77cd919d8cd", "metadata": {"id": "972f70a1e52e9db834fe73225f99f77cd919d8cd"}, "original_file_path": "../../datalake/Samuel/SOFTware-Sync/data/xml_not_sofctied/hal-03314797.grobid.tei.xml", "file_name": "hal-03314797.grobid.tei.xml"}