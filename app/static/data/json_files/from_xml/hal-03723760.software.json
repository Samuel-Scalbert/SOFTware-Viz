{"application": "software-mentions", "version": "0.8.0", "date": "2024-10-07T11:48+0000", "md5": "4D288084D342C9FF6B7F0D386D182AFF", "mentions": [{"type": "software", "software-type": "software", "software-name": {"rawForm": "MultiFiT", "normalizedForm": "MultiFiT", "offsetStart": 0, "offsetEnd": 13}, "context": "MultiFiT [29] is a training procedure involving LASER and a monolingual model to perform a cross-lingual transfer. ", "mentionContextAttributes": {"used": {"value": false, "score": 6.73532485961914e-05}, "created": {"value": false, "score": 1.7523765563964844e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": false, "score": 6.73532485961914e-05}, "created": {"value": false, "score": 1.7523765563964844e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "AWESOME", "normalizedForm": "AWESOME", "offsetStart": 3, "offsetEnd": 10}, "context": "e) AWESOME: [32] which is mBERT fine-tuned on a variety of self-supervised objectives and supervised objectives on a parallel corpus to improve word-level alignment for extracting pairs of translated words in parallel sentences: MLM, TLM but also objectives on the consistency of the produced alignment.", "mentionContextAttributes": {"used": {"value": false, "score": 0.001995265483856201}, "created": {"value": false, "score": 7.152557373046875e-06}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999899864196777}, "created": {"value": false, "score": 0.2533044219017029}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "references": [{"label": "[32]", "normalizedForm": "[32]", "refKey": 32, "offsetStart": 17679, "offsetEnd": 17683}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "FastAlign", "normalizedForm": "FastAlign", "offsetStart": 9, "offsetEnd": 18}, "context": "[5] used FastAlign to compare the similarity of translated pairs of words and random ones with mBERT. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9999910593032837}, "created": {"value": false, "score": 1.9073486328125e-06}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999910593032837}, "created": {"value": true, "score": 0.9061416387557983}, "shared": {"value": false, "score": 5.960464477539062e-07}}, "references": [{"label": "[5]", "normalizedForm": "[5]", "refKey": 5}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "mBERT", "normalizedForm": "mBERT", "offsetStart": 12, "offsetEnd": 17}, "context": "Results for mBERT and five language pairs are shown in Fig. 6. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.8390638828277588}, "created": {"value": false, "score": 4.1961669921875e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999910593032837}, "created": {"value": false, "score": 0.253304660320282}, "shared": {"value": false, "score": 5.960464477539062e-07}}, "references": [{"label": "[6]", "normalizedForm": "[6]", "refKey": 6}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "FastText", "normalizedForm": "FastText", "offsetStart": 12, "offsetEnd": 20}, "context": "Monolingual FastText embeddings [26] aligned with RCSLS [24] are used as a baseline.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9430691003799438}, "created": {"value": false, "score": 1.1920928955078125e-06}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999198913574219}, "created": {"value": false, "score": 5.2094459533691406e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "mBERT", "normalizedForm": "mBERT", "offsetStart": 13, "offsetEnd": 18}, "context": "For example, mBERT, which uses 12 Transformer blocks, will produce 13 representations for a word, the 0th one for the initial embedding and the 1st to 12th for the output of each Transformer block.", "mentionContextAttributes": {"used": {"value": false, "score": 0.0004729032516479492}, "created": {"value": false, "score": 3.921985626220703e-05}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999910593032837}, "created": {"value": false, "score": 0.253304660320282}, "shared": {"value": false, "score": 5.960464477539062e-07}}, "references": [{"label": "[6]", "normalizedForm": "[6]", "refKey": 6}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "mBERT", "normalizedForm": "mBERT", "offsetStart": 18, "offsetEnd": 23}, "context": "It was shown that mBERT builds representations that enable the extraction of corresponding words in a pair of translated sentences with fair accuracy, and AWESOME [32] was proposed to fine-tune mBERT and improve this word-level matching. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.002938210964202881}, "created": {"value": false, "score": 0.253304660320282}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999910593032837}, "created": {"value": false, "score": 0.253304660320282}, "shared": {"value": false, "score": 5.960464477539062e-07}}, "references": [{"label": "[6]", "normalizedForm": "[6]", "refKey": 6}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "mBERT", "normalizedForm": "mBERT", "offsetStart": 18, "offsetEnd": 23}, "context": "The best layer of mBERT gives the best results with respect to all other models. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.017019152641296387}, "created": {"value": false, "score": 1.4424324035644531e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999910593032837}, "created": {"value": false, "score": 0.253304660320282}, "shared": {"value": false, "score": 5.960464477539062e-07}}, "references": [{"label": "[6]", "normalizedForm": "[6]", "refKey": 6}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "mBERT", "normalizedForm": "mBERT", "offsetStart": 18, "offsetEnd": 23}, "context": "For example, when mBERT is finetuned on a classification task in English, it gives competitive results when evaluated in French.", "mentionContextAttributes": {"used": {"value": false, "score": 0.0007058382034301758}, "created": {"value": false, "score": 8.344650268554688e-06}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999910593032837}, "created": {"value": false, "score": 0.253304660320282}, "shared": {"value": false, "score": 5.960464477539062e-07}}, "references": [{"label": "[6]", "normalizedForm": "[6]", "refKey": 6}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "FastAlign", "normalizedForm": "FastAlign", "offsetStart": 19, "offsetEnd": 28}, "context": "This confirms that FastAlign generates too many mistakes to make an accurate evaluation of the multilingual alignment produced by a model.", "mentionContextAttributes": {"used": {"value": false, "score": 0.020863831043243408}, "created": {"value": false, "score": 6.389617919921875e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999910593032837}, "created": {"value": true, "score": 0.9061416387557983}, "shared": {"value": false, "score": 5.960464477539062e-07}}, "references": [{"label": "[5]", "normalizedForm": "[5]", "refKey": 5}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "FastAlign", "normalizedForm": "FastAlign", "offsetStart": 21, "offsetEnd": 30}, "context": "But because they use FastAlign, they are considering many unrelated pairs as translations. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.004305720329284668}, "created": {"value": false, "score": 0.00013387203216552734}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999910593032837}, "created": {"value": true, "score": 0.9061416387557983}, "shared": {"value": false, "score": 5.960464477539062e-07}}, "references": [{"label": "[5]", "normalizedForm": "[5]", "refKey": 5}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "FastAlign", "normalizedForm": "FastAlign", "offsetStart": 24, "offsetEnd": 33}, "context": "We show that tools like FastAlign make too many alignment mistakes to be able to conclude about the quality of the multilingual alignment produced by models like mBERT. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.0006496310234069824}, "created": {"value": false, "score": 0.03039640188217163}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999910593032837}, "created": {"value": true, "score": 0.9061416387557983}, "shared": {"value": false, "score": 5.960464477539062e-07}}, "references": [{"label": "[5]", "normalizedForm": "[5]", "refKey": 5}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "mBERT", "normalizedForm": "mBERT", "offsetStart": 26, "offsetEnd": 31}, "context": "We compare six models: a) mBERT: [6] pre-trained on Wikipedia in the 104 most frequent languages with two objectives: (1) Masked language modeling (MLM); predicting randomly masked out words and (2) next sentence prediction (NSP) determining whether two sentences are consecutive only using the representation of the [CLS] token (Fig. 1). ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9996110796928406}, "created": {"value": false, "score": 3.4570693969726562e-06}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999910593032837}, "created": {"value": false, "score": 0.253304660320282}, "shared": {"value": false, "score": 5.960464477539062e-07}}, "references": [{"label": "[6]", "normalizedForm": "[6]", "refKey": 6, "offsetStart": 16448, "offsetEnd": 16451}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "mBERT", "normalizedForm": "mBERT", "offsetStart": 26, "offsetEnd": 31}, "context": "e) AWESOME: [32] which is mBERT fine-tuned on a variety of self-supervised objectives and supervised objectives on a parallel corpus to improve word-level alignment for extracting pairs of translated words in parallel sentences: MLM, TLM but also objectives on the consistency of the produced alignment.", "mentionContextAttributes": {"used": {"value": false, "score": 0.001995265483856201}, "created": {"value": false, "score": 7.152557373046875e-06}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999910593032837}, "created": {"value": false, "score": 0.253304660320282}, "shared": {"value": false, "score": 5.960464477539062e-07}}, "references": [{"label": "[6]", "normalizedForm": "[6]", "refKey": 6}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "mBERT", "normalizedForm": "mBERT", "offsetStart": 39, "offsetEnd": 44}, "context": "A multilingual version of BERT, called mBERT, was proposed and is the same architecture pretrained for the same task but on 104 monolingual corpora of distinct languages. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.0002480149269104004}, "created": {"value": false, "score": 0.03718382120132446}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999910593032837}, "created": {"value": false, "score": 0.253304660320282}, "shared": {"value": false, "score": 5.960464477539062e-07}}, "references": [{"label": "[6]", "normalizedForm": "[6]", "refKey": 6}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "mBERT", "normalizedForm": "mBERT", "offsetStart": 39, "offsetEnd": 44}, "context": "We observe that for a few deep layers, mBERT produces representations that are better aligned than multilingual word embeddings.", "mentionContextAttributes": {"used": {"value": false, "score": 8.106231689453125e-05}, "created": {"value": false, "score": 0.0005291104316711426}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999910593032837}, "created": {"value": false, "score": 0.253304660320282}, "shared": {"value": false, "score": 5.960464477539062e-07}}, "references": [{"label": "[6]", "normalizedForm": "[6]", "refKey": 6}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "MUSE", "normalizedForm": "MUSE", "offsetStart": 43, "offsetEnd": 51}, "context": "And For the bilingual dictionaries, we use MUSE [23]. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9294020533561707}, "created": {"value": false, "score": 4.410743713378906e-06}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9294020533561707}, "created": {"value": false, "score": 4.410743713378906e-06}, "shared": {"value": false, "score": 2.384185791015625e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "mBERT", "normalizedForm": "mBERT", "offsetStart": 44, "offsetEnd": 49}, "context": "Multilingual Transformer-based models, like mBERT [6] or XLM-R [16], are the focus of our paper.", "mentionContextAttributes": {"used": {"value": false, "score": 9.78708267211914e-05}, "created": {"value": false, "score": 0.0296022891998291}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999910593032837}, "created": {"value": false, "score": 0.253304660320282}, "shared": {"value": false, "score": 5.960464477539062e-07}}, "references": [{"label": "[6]", "normalizedForm": "[6]", "refKey": 6, "offsetStart": 3188, "offsetEnd": 3191}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "FastText", "normalizedForm": "FastText", "offsetStart": 45, "offsetEnd": 53}, "context": "To avoid favoring contextualized models over FastText aligned embedding we chose to sample distinct pairs of words.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9999198913574219}, "created": {"value": false, "score": 5.2094459533691406e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999198913574219}, "created": {"value": false, "score": 5.2094459533691406e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "mBERT", "normalizedForm": "mBERT", "offsetStart": 52, "offsetEnd": 57}, "context": "Our results show that Transformer-based models like mBERT produce contextualized representations of words that are well aligned across languages, particularly in the deeper layers, despite those models having only been trained on monolingual objectives. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.21970081329345703}, "created": {"value": false, "score": 0.052955806255340576}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999910593032837}, "created": {"value": false, "score": 0.253304660320282}, "shared": {"value": false, "score": 5.960464477539062e-07}}, "references": [{"label": "[6]", "normalizedForm": "[6]", "refKey": 6}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "mBERT", "normalizedForm": "mBERT", "offsetStart": 53, "offsetEnd": 58}, "context": "However, it could also be explained by the fact that mBERT is trained solely on Wikipedia whereas models like XLM-R are trained on the CommonCrawl corpus which might contain texts that are less comparable across languages. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.17547786235809326}, "created": {"value": false, "score": 1.990795135498047e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999910593032837}, "created": {"value": false, "score": 0.253304660320282}, "shared": {"value": false, "score": 5.960464477539062e-07}}, "references": [{"label": "[6]", "normalizedForm": "[6]", "refKey": 6}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "mBERT", "normalizedForm": "mBERT", "offsetStart": 53, "offsetEnd": 58}, "context": "It must also be noted that alignment of the averaged mBERT representation suffers less from the typological distance between languages than the CLS representation or aligned FastText.", "mentionContextAttributes": {"used": {"value": true, "score": 0.683159589767456}, "created": {"value": false, "score": 9.298324584960938e-06}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999910593032837}, "created": {"value": false, "score": 0.253304660320282}, "shared": {"value": false, "score": 5.960464477539062e-07}}, "references": [{"label": "[6]", "normalizedForm": "[6]", "refKey": 6}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "mBERT", "normalizedForm": "mBERT", "offsetStart": 57, "offsetEnd": 62}, "context": "Fig. 5 shows those distributions for the eighth layer of mBERT.", "mentionContextAttributes": {"used": {"value": true, "score": 0.996981680393219}, "created": {"value": false, "score": 7.271766662597656e-06}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999910593032837}, "created": {"value": false, "score": 0.253304660320282}, "shared": {"value": false, "score": 5.960464477539062e-07}}, "references": [{"label": "[6]", "normalizedForm": "[6]", "refKey": 6}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "mBERT", "normalizedForm": "mBERT", "offsetStart": 59, "offsetEnd": 64}, "context": "Despite not being pre-trained explicitly on parallel text, mBERT was shown to have surprisingly good cross-lingual transfer abilities [2], [19].", "mentionContextAttributes": {"used": {"value": true, "score": 0.9961432814598083}, "created": {"value": false, "score": 6.67572021484375e-06}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999910593032837}, "created": {"value": false, "score": 0.253304660320282}, "shared": {"value": false, "score": 5.960464477539062e-07}}, "references": [{"label": "[6]", "normalizedForm": "[6]", "refKey": 6}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "mBERT", "normalizedForm": "mBERT", "offsetStart": 60, "offsetEnd": 65}, "context": "We empirically verified for all the layers of three models (mBERT, XLM-R, AWESOME) on three language pairs and for 10 different sampling of pairs of words that it gives equivalent results: we observed a strong correlation with a 0.86 Spearman rank correlation (p-value < 0.01).", "mentionContextAttributes": {"used": {"value": true, "score": 0.9999899864196777}, "created": {"value": false, "score": 7.867813110351562e-06}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999910593032837}, "created": {"value": false, "score": 0.253304660320282}, "shared": {"value": false, "score": 5.960464477539062e-07}}, "references": [{"label": "[6]", "normalizedForm": "[6]", "refKey": 6}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "FastAlign", "normalizedForm": "FastAlign", "offsetStart": 62, "offsetEnd": 71}, "context": "With its right and wrong extracted pairs, the distribution of FastAlign pairs (in blue)  overlaps more with the distribution of random pairs (in purple) than the distribution of pairs extracted with our methods (in yellow). ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9720359444618225}, "created": {"value": false, "score": 1.0013580322265625e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999910593032837}, "created": {"value": true, "score": 0.9061416387557983}, "shared": {"value": false, "score": 5.960464477539062e-07}}, "references": [{"label": "[5]", "normalizedForm": "[5]", "refKey": 5}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "mBERT", "normalizedForm": "mBERT", "offsetStart": 67, "offsetEnd": 72}, "context": "the similarity between the contextualized representations built by mBERT for those words was compared with the similarity between representations of random words. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9999872446060181}, "created": {"value": false, "score": 4.124641418457031e-05}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999910593032837}, "created": {"value": false, "score": 0.253304660320282}, "shared": {"value": false, "score": 5.960464477539062e-07}}, "references": [{"label": "[6]", "normalizedForm": "[6]", "refKey": 6}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "FastAlign", "normalizedForm": "FastAlign", "offsetStart": 72, "offsetEnd": 81}, "context": "Then, we demonstrate that our method provides better pair of words than FastAlign thanks to a lesser number of carefully selected pairs. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.08163326978683472}, "created": {"value": false, "score": 0.012088775634765625}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999910593032837}, "created": {"value": true, "score": 0.9061416387557983}, "shared": {"value": false, "score": 5.960464477539062e-07}}, "references": [{"label": "[5]", "normalizedForm": "[5]", "refKey": 5}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "mBERT", "normalizedForm": "mBERT", "offsetStart": 73, "offsetEnd": 78}, "context": "Despite the surprisingly efficient cross-lingual transfer of models like mBERT, there is no consensus on whether those multilingual models learn universal multilingual patterns, as a recent literature review states [20].", "mentionContextAttributes": {"used": {"value": false, "score": 0.00010669231414794922}, "created": {"value": false, "score": 0.00022739171981811523}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999910593032837}, "created": {"value": false, "score": 0.253304660320282}, "shared": {"value": false, "score": 5.960464477539062e-07}}, "references": [{"label": "[6]", "normalizedForm": "[6]", "refKey": 6}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "mBERT", "normalizedForm": "mBERT", "offsetStart": 74, "offsetEnd": 79}, "context": "The former found aligned representations, while the latter concluded that mBERT \"is not an interlangua\".", "mentionContextAttributes": {"used": {"value": true, "score": 0.9994527697563171}, "created": {"value": false, "score": 6.9141387939453125e-06}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999910593032837}, "created": {"value": false, "score": 0.253304660320282}, "shared": {"value": false, "score": 5.960464477539062e-07}}, "references": [{"label": "[6]", "normalizedForm": "[6]", "refKey": 6}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "AWESOME", "normalizedForm": "AWESOME", "offsetStart": 74, "offsetEnd": 81}, "context": "We empirically verified for all the layers of three models (mBERT, XLM-R, AWESOME) on three language pairs and for 10 different sampling of pairs of words that it gives equivalent results: we observed a strong correlation with a 0.86 Spearman rank correlation (p-value < 0.01).", "mentionContextAttributes": {"used": {"value": true, "score": 0.9999899864196777}, "created": {"value": false, "score": 7.867813110351562e-06}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999899864196777}, "created": {"value": false, "score": 0.2533044219017029}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "references": [{"label": "[32]", "normalizedForm": "[32]", "refKey": 32}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "FastAlign", "normalizedForm": "FastAlign", "offsetStart": 76, "offsetEnd": 88}, "context": "Table II shows the proportion of accurate pairs extracted by our method and FastAlign [4]. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9963428378105164}, "created": {"value": false, "score": 1.609325408935547e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999910593032837}, "created": {"value": true, "score": 0.9061416387557983}, "shared": {"value": false, "score": 5.960464477539062e-07}}, "references": [{"label": "[5]", "normalizedForm": "[5]", "refKey": 5}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "FastAlign", "normalizedForm": "FastAlign", "offsetStart": 81, "offsetEnd": 90}, "context": "It demonstrates that our method extracts proportionally more accurate pairs than FastAlign, although it provides fewer pairs in quantity to be fair. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.0012560486793518066}, "created": {"value": false, "score": 0.0003954768180847168}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999910593032837}, "created": {"value": true, "score": 0.9061416387557983}, "shared": {"value": false, "score": 5.960464477539062e-07}}, "references": [{"label": "[5]", "normalizedForm": "[5]", "refKey": 5}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "mBERT", "normalizedForm": "mBERT", "offsetStart": 87, "offsetEnd": 92}, "context": "The very last layers also give worse results than layers 8 to 10. [19] have shown that mBERT representations hold language-specific information at each layer. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.5835275053977966}, "created": {"value": false, "score": 4.410743713378906e-06}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999910593032837}, "created": {"value": false, "score": 0.253304660320282}, "shared": {"value": false, "score": 5.960464477539062e-07}}, "references": [{"label": "[6]", "normalizedForm": "[6]", "refKey": 6}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "FastAlign", "normalizedForm": "FastAlign", "offsetStart": 89, "offsetEnd": 98}, "context": "And instead of trying to extract all possible word pairs with a word-alignment tool like FastAlign, we extract only those that we are certain of.", "mentionContextAttributes": {"used": {"value": false, "score": 0.0023012757301330566}, "created": {"value": false, "score": 0.0026521682739257812}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999910593032837}, "created": {"value": true, "score": 0.9061416387557983}, "shared": {"value": false, "score": 5.960464477539062e-07}}, "references": [{"label": "[5]", "normalizedForm": "[5]", "refKey": 5}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "FastAlign", "normalizedForm": "FastAlign", "offsetStart": 90, "offsetEnd": 102}, "context": "The proposed method relies on a bilingual dictionary instead of a probabilistic tool like FastAlign [4], as in [5], to retrieve more accurate word pairs in parallel texts. ", "mentionContextAttributes": {"used": {"value": false, "score": 8.678436279296875e-05}, "created": {"value": false, "score": 0.02481144666671753}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999910593032837}, "created": {"value": true, "score": 0.9061416387557983}, "shared": {"value": false, "score": 5.960464477539062e-07}}, "references": [{"label": "[5]", "normalizedForm": "[5]", "refKey": 5, "offsetStart": 1934, "offsetEnd": 1937}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "FastAlign", "normalizedForm": "FastAlign", "offsetStart": 93, "offsetEnd": 105}, "context": "In another work [5], translated pairs of words were extracted from translated sentences with FastAlign [4]. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.999981164932251}, "created": {"value": false, "score": 4.1961669921875e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999910593032837}, "created": {"value": true, "score": 0.9061416387557983}, "shared": {"value": false, "score": 5.960464477539062e-07}}, "references": [{"label": "[5]", "normalizedForm": "[5]", "refKey": 5}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "mBERT", "normalizedForm": "mBERT", "offsetStart": 95, "offsetEnd": 100}, "context": "[5] used FastAlign to compare the similarity of translated pairs of words and random ones with mBERT.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9999910593032837}, "created": {"value": false, "score": 1.9073486328125e-06}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999910593032837}, "created": {"value": false, "score": 0.253304660320282}, "shared": {"value": false, "score": 5.960464477539062e-07}}, "references": [{"label": "[6]", "normalizedForm": "[6]", "refKey": 6}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "FastText", "normalizedForm": "FastText", "offsetStart": 112, "offsetEnd": 120}, "context": "In our experiments, a specific version of those aligned word embeddings will be used as a baseline: monolingual FastText embeddings [26] aligned with RCSLS [24].", "mentionContextAttributes": {"used": {"value": false, "score": 0.012350380420684814}, "created": {"value": false, "score": 2.181529998779297e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999198913574219}, "created": {"value": false, "score": 5.2094459533691406e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "FastAlign", "normalizedForm": "FastAlign", "offsetStart": 115, "offsetEnd": 124}, "context": "For 10 000 sentences from WMT19 for the English-German pair: our method extracts 50 590 word pairs and 190 665 for FastAlign.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9044030904769897}, "created": {"value": false, "score": 2.7418136596679688e-06}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999910593032837}, "created": {"value": true, "score": 0.9061416387557983}, "shared": {"value": false, "score": 5.960464477539062e-07}}, "references": [{"label": "[5]", "normalizedForm": "[5]", "refKey": 5}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "mBERT", "normalizedForm": "mBERT", "offsetStart": 122, "offsetEnd": 127}, "context": "To build the representation of each contextualized word, the whole sentences are passed to the evaluated model (typically mBERT) and the contextualized representations of the words from the extracted word pairs are kept.", "mentionContextAttributes": {"used": {"value": true, "score": 0.996723473072052}, "created": {"value": false, "score": 4.5299530029296875e-05}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999910593032837}, "created": {"value": false, "score": 0.253304660320282}, "shared": {"value": false, "score": 5.960464477539062e-07}}, "references": [{"label": "[6]", "normalizedForm": "[6]", "refKey": 6}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "mBERT", "normalizedForm": "mBERT", "offsetStart": 125, "offsetEnd": 130}, "context": "It also goes against the hypothesis made by several papers [2], [3], [19], that shared vocabulary is what allows models like mBERT to align representations without having been exposed to parallel texts. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.003645777702331543}, "created": {"value": false, "score": 0.00025957822799682617}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999910593032837}, "created": {"value": false, "score": 0.253304660320282}, "shared": {"value": false, "score": 5.960464477539062e-07}}, "references": [{"label": "[6]", "normalizedForm": "[6]", "refKey": 6}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "mBERT", "normalizedForm": "mBERT", "offsetStart": 128, "offsetEnd": 133}, "context": "Finally, the same models are evaluated for the strong alignment retrieval criterion S strong defined in Equation 3. Results for mBERT are reported on Fig. 7 and results for all models are reported on Table IV.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9981693029403687}, "created": {"value": false, "score": 2.86102294921875e-06}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999910593032837}, "created": {"value": false, "score": 0.253304660320282}, "shared": {"value": false, "score": 5.960464477539062e-07}}, "references": [{"label": "[6]", "normalizedForm": "[6]", "refKey": 6}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "FastAlign", "normalizedForm": "FastAlign", "offsetStart": 130, "offsetEnd": 139}, "context": "When measuring that similarity on the last layer and plotting the sampled distribution, they observe that the pairs obtained with FastAlign give a very broad distribution that overlaps a lot with random pairs, which leads them to conclude that word-level representations built by mBERT are not well aligned across languages and it motivates them to propose a method to realign representation after pre-training.", "mentionContextAttributes": {"used": {"value": false, "score": 0.4026362895965576}, "created": {"value": false, "score": 0.0004898309707641602}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999910593032837}, "created": {"value": true, "score": 0.9061416387557983}, "shared": {"value": false, "score": 5.960464477539062e-07}}, "references": [{"label": "[5]", "normalizedForm": "[5]", "refKey": 5}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "mBERT", "normalizedForm": "mBERT", "offsetStart": 142, "offsetEnd": 147}, "context": "Before reporting our results on word-level alignment, we investigate the contradiction between [2] and [3] on sentencelevel alignment for the mBERT model.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9968944787979126}, "created": {"value": false, "score": 8.869171142578125e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999910593032837}, "created": {"value": false, "score": 0.253304660320282}, "shared": {"value": false, "score": 5.960464477539062e-07}}, "references": [{"label": "[6]", "normalizedForm": "[6]", "refKey": 6}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "AWESOME", "normalizedForm": "AWESOME", "offsetStart": 147, "offsetEnd": 154}, "context": "It is also to be noted that the TLM objective on parallel texts proposed by the XLM model seems to make the alignment worse but it is also used in AWESOME.", "mentionContextAttributes": {"used": {"value": false, "score": 0.006352245807647705}, "created": {"value": false, "score": 9.357929229736328e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999899864196777}, "created": {"value": false, "score": 0.2533044219017029}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "references": [{"label": "[32]", "normalizedForm": "[32]", "refKey": 32}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "mBERT", "normalizedForm": "mBERT", "offsetStart": 155, "offsetEnd": 160}, "context": "Sentence representations can give different results according to the chosen method, but it seems that whatever it is, sentence representations produced by mBERT are relatively aligned across languages. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.0009888410568237305}, "created": {"value": false, "score": 1.990795135498047e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999910593032837}, "created": {"value": false, "score": 0.253304660320282}, "shared": {"value": false, "score": 5.960464477539062e-07}}, "references": [{"label": "[6]", "normalizedForm": "[6]", "refKey": 6}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "mBERT", "normalizedForm": "mBERT", "offsetStart": 155, "offsetEnd": 160}, "context": "Nevertheless, we have demonstrated that there is a word-level strong alignment in most multilingual Transformer-based language models, even for those like mBERT and XLM-R which have no explicit information about the language in input and haven't been pre-trained on parallel texts.", "mentionContextAttributes": {"used": {"value": false, "score": 0.0007061958312988281}, "created": {"value": false, "score": 0.05412435531616211}, "shared": {"value": false, "score": 5.960464477539062e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999910593032837}, "created": {"value": false, "score": 0.253304660320282}, "shared": {"value": false, "score": 5.960464477539062e-07}}, "references": [{"label": "[6]", "normalizedForm": "[6]", "refKey": 6}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "AWESOME", "normalizedForm": "AWESOME", "offsetStart": 155, "offsetEnd": 167}, "context": "It was shown that mBERT builds representations that enable the extraction of corresponding words in a pair of translated sentences with fair accuracy, and AWESOME [32] was proposed to fine-tune mBERT and improve this word-level matching. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.00293809175491333}, "created": {"value": false, "score": 0.2533044219017029}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999899864196777}, "created": {"value": false, "score": 0.2533044219017029}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "references": [{"label": "[32]", "normalizedForm": "[32]", "refKey": 32}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "mBERT", "normalizedForm": "mBERT", "offsetStart": 161, "offsetEnd": 166}, "context": "Additionally to its training on parallel data, its input embedding is added to a language embedding indicating the language of the sentence, whereas models like mBERT and XLM-R have no input information about the language.", "mentionContextAttributes": {"used": {"value": false, "score": 0.00021821260452270508}, "created": {"value": false, "score": 7.200241088867188e-05}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999910593032837}, "created": {"value": false, "score": 0.253304660320282}, "shared": {"value": false, "score": 5.960464477539062e-07}}, "references": [{"label": "[6]", "normalizedForm": "[6]", "refKey": 6}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "mBERT", "normalizedForm": "mBERT", "offsetStart": 162, "offsetEnd": 167}, "context": "We show that tools like FastAlign make too many alignment mistakes to be able to conclude about the quality of the multilingual alignment produced by models like mBERT. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.0006496310234069824}, "created": {"value": false, "score": 0.030395925045013428}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999910593032837}, "created": {"value": false, "score": 0.253304660320282}, "shared": {"value": false, "score": 5.960464477539062e-07}}, "references": [{"label": "[6]", "normalizedForm": "[6]", "refKey": 6}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "mBERT", "normalizedForm": "mBERT", "offsetStart": 171, "offsetEnd": 176}, "context": "Indeed, even if the sentence-level alignment was guaranteed, does it necessarily mean that there is a word-level multilingual alignment in the representations produced by mBERT and others? the CLS representation is independent of the token representation. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.7346662282943726}, "created": {"value": false, "score": 3.540515899658203e-05}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999910593032837}, "created": {"value": false, "score": 0.253304660320282}, "shared": {"value": false, "score": 5.960464477539062e-07}}, "references": [{"label": "[6]", "normalizedForm": "[6]", "refKey": 6}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "FastText", "normalizedForm": "FastText", "offsetStart": 174, "offsetEnd": 182}, "context": "It must also be noted that alignment of the averaged mBERT representation suffers less from the typological distance between languages than the CLS representation or aligned FastText.", "mentionContextAttributes": {"used": {"value": true, "score": 0.6831609010696411}, "created": {"value": false, "score": 9.298324584960938e-06}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999198913574219}, "created": {"value": false, "score": 5.2094459533691406e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "MultiFit", "normalizedForm": "MultiFit", "offsetStart": 186, "offsetEnd": 194}, "context": "However, we will focus here on Transformer-based models which are pre-trained with fewer parallel texts and are not built specifically for sentence-level downstream tasks like LASER and MultiFit.", "mentionContextAttributes": {"used": {"value": false, "score": 4.208087921142578e-05}, "created": {"value": true, "score": 0.9720321297645569}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": false, "score": 4.208087921142578e-05}, "created": {"value": true, "score": 0.9720321297645569}, "shared": {"value": false, "score": 2.384185791015625e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "mBERT", "normalizedForm": "mBERT", "offsetStart": 194, "offsetEnd": 199}, "context": "It was shown that mBERT builds representations that enable the extraction of corresponding words in a pair of translated sentences with fair accuracy, and AWESOME [32] was proposed to fine-tune mBERT and improve this word-level matching. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.00293809175491333}, "created": {"value": false, "score": 0.2533044219017029}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999910593032837}, "created": {"value": false, "score": 0.253304660320282}, "shared": {"value": false, "score": 5.960464477539062e-07}}, "references": [{"label": "[6]", "normalizedForm": "[6]", "refKey": 6}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "FastAlign", "normalizedForm": "FastAlign", "offsetStart": 219, "offsetEnd": 231}, "context": "Our contribution is (1) to propose a method for extracting pairs of translated words in context; (2) to show that the proposed method extracts more accurate pairs than others [5] which rely on word-alignment tools like FastAlign [4]; and (3) to reveal that the alignment produced by most Transformer-based multilingual models is competitive with other representations.", "mentionContextAttributes": {"used": {"value": false, "score": 0.00043517351150512695}, "created": {"value": true, "score": 0.9061416387557983}, "shared": {"value": false, "score": 5.960464477539062e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999910593032837}, "created": {"value": true, "score": 0.9061416387557983}, "shared": {"value": false, "score": 5.960464477539062e-07}}, "references": [{"label": "[5]", "normalizedForm": "[5]", "refKey": 5}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "FastText", "normalizedForm": "FastText", "offsetStart": 250, "offsetEnd": 258}, "context": "By comparing results for weak and strong alignment, it was also shown that multilingual Transformers perform far better on a more challenging evaluation than cross-lingual embeddings which were built with explicit cross-lingual training signal, like FastText embeddings aligned with RCSLS but also deeper models trained with TLM objective such as XLM-15.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9905917048454285}, "created": {"value": false, "score": 8.344650268554688e-06}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999198913574219}, "created": {"value": false, "score": 5.2094459533691406e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "mBERT", "normalizedForm": "mBERT", "offsetStart": 280, "offsetEnd": 285}, "context": "When measuring that similarity on the last layer and plotting the sampled distribution, they observe that the pairs obtained with FastAlign give a very broad distribution that overlaps a lot with random pairs, which leads them to conclude that word-level representations built by mBERT are not well aligned across languages and it motivates them to propose a method to realign representation after pre-training.", "mentionContextAttributes": {"used": {"value": false, "score": 0.4026362895965576}, "created": {"value": false, "score": 0.0004898309707641602}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999910593032837}, "created": {"value": false, "score": 0.253304660320282}, "shared": {"value": false, "score": 5.960464477539062e-07}}, "references": [{"label": "[6]", "normalizedForm": "[6]", "refKey": 6}]}], "references": [{"refKey": 32, "tei": "<biblStruct xml:id=\"b32\">\n\t<analytic>\n\t\t<title level=\"a\" type=\"main\">Word Alignment by Fine-tuning Embeddings on Parallel Corpora</title>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Zi-Yi</forename><surname>Dou</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Graham</forename><surname>Neubig</surname></persName>\n\t\t</author>\n\t\t<idno type=\"DOI\">10.18653/v1/2021.eacl-main.181</idno>\n\t\t<idno>abs/2101.08231</idno>\n\t</analytic>\n\t<monogr>\n\t\t<title level=\"m\">Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume</title>\n\t\t<meeting>the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume</meeting>\n\t\t<imprint>\n\t\t\t<publisher>Association for Computational Linguistics</publisher>\n\t\t\t<date type=\"published\" when=\"2021\">2021</date>\n\t\t</imprint>\n\t</monogr>\n</biblStruct>\n"}, {"refKey": 5, "tei": "<biblStruct xml:id=\"b5\">\n\t<analytic>\n\t\t<title level=\"a\" type=\"main\">Inducing Language-Agnostic Multilingual Representations</title>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Wei</forename><surname>Zhao</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Steffen</forename><surname>Eger</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Johannes</forename><surname>Bjerva</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Isabelle</forename><surname>Augenstein</surname></persName>\n\t\t</author>\n\t\t<idno type=\"DOI\">10.18653/v1/2021.starsem-1.22</idno>\n\t\t<idno>abs/2008.09112</idno>\n\t</analytic>\n\t<monogr>\n\t\t<title level=\"m\">Proceedings of *SEM 2021: The Tenth Joint Conference on Lexical and Computational Semantics</title>\n\t\t<meeting>*SEM 2021: The Tenth Joint Conference on Lexical and Computational Semantics</meeting>\n\t\t<imprint>\n\t\t\t<publisher>Association for Computational Linguistics</publisher>\n\t\t\t<date type=\"published\" when=\"2021\">2020</date>\n\t\t</imprint>\n\t</monogr>\n</biblStruct>\n"}, {"refKey": 6, "tei": "<biblStruct xml:id=\"b6\">\n\t<analytic>\n\t\t<title level=\"a\" type=\"main\">BERT: Pretraining of deep bidirectional transformers for language understanding</title>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">J</forename><surname>Devlin</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">M.-W</forename><surname>Chang</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">K</forename><surname>Lee</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">K</forename><surname>Toutanova</surname></persName>\n\t\t</author>\n\t</analytic>\n\t<monogr>\n\t\t<title level=\"m\">Proceedings of NAACL 2019</title>\n\t\t<meeting>NAACL 2019</meeting>\n\t\t<imprint>\n\t\t\t<date>Jun. 2019</date>\n\t\t</imprint>\n\t</monogr>\n</biblStruct>\n"}], "runtime": 16262, "id": "3c2446120b45868e33eee95dd01c85c6df081584", "metadata": {"id": "3c2446120b45868e33eee95dd01c85c6df081584"}, "original_file_path": "../../datalake/Samuel/SOFTware-Sync/downloads/xml/hal-03723760.grobid.tei.xml", "file_name": "hal-03723760.grobid.tei.xml"}