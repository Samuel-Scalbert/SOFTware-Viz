{"application": "software-mentions", "version": "0.8.0", "date": "2024-10-07T12:46+0000", "md5": "A6671D9E41E323829FF08BDEE34FA008", "mentions": [{"type": "software", "software-type": "software", "software-name": {"rawForm": "BERT", "normalizedForm": "BERT", "offsetStart": 0, "offsetEnd": 4}, "context": "BERT [6], which stands for Bidirectional Encoder Representations from Transformers, is pretrained to generate bidirectional representations of the words, taking into account the semantics by considering both left and right directions of the text.", "mentionContextAttributes": {"used": {"value": false, "score": 0.0005702376365661621}, "created": {"value": false, "score": 3.2782554626464844e-05}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999899864196777}, "created": {"value": true, "score": 0.9998382329940796}, "shared": {"value": false, "score": 1.0728836059570312e-06}}, "references": [{"label": "[6]", "normalizedForm": "[6]", "refKey": 6, "offsetStart": 3819, "offsetEnd": 3822}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "SciBERT", "normalizedForm": "SciBERT", "offsetStart": 0, "offsetEnd": 7}, "context": "SciBERT emerged as the highest performing model when the fine-tuning is done using a Bi-LSTM layer, without need of extra features. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.0009192228317260742}, "created": {"value": false, "score": 0.00028389692306518555}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999836683273315}, "created": {"value": false, "score": 0.00028389692306518555}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "references": [{"label": "[8]", "normalizedForm": "[8]", "refKey": 8}, {"label": "[8]", "normalizedForm": "[8]", "refKey": 8}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "SciBERT", "normalizedForm": "SciBERT", "offsetStart": 0, "offsetEnd": 7}, "context": "SciBERT presented the highest performance when the fine-tuning is done using a Bi-LSTM classifier without adding any extra-features. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9996670484542847}, "created": {"value": false, "score": 2.586841583251953e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999836683273315}, "created": {"value": false, "score": 0.00028389692306518555}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "references": [{"label": "[8]", "normalizedForm": "[8]", "refKey": 8}, {"label": "[8]", "normalizedForm": "[8]", "refKey": 8}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "SciBERT", "normalizedForm": "SciBERT", "offsetStart": 0, "offsetEnd": 7}, "context": "SciBERT and PubMedBERT, were pretrained from scratch, meaning that they use a unique vocabulary on the pretraining corpus and include embeddings that are specific for in-domain words.", "mentionContextAttributes": {"used": {"value": true, "score": 0.989198625087738}, "created": {"value": false, "score": 7.033348083496094e-06}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999836683273315}, "created": {"value": false, "score": 0.00028389692306518555}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "references": [{"label": "[8]", "normalizedForm": "[8]", "refKey": 8}, {"label": "[8]", "normalizedForm": "[8]", "refKey": 8}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "BioBERT", "normalizedForm": "BioBERT", "offsetStart": 0, "offsetEnd": 7}, "context": "BioBERT and BioMedRoBERTa were pretrained starting from the BERT checkpoints, which means that the vocabularies are built with general-domain texts (similar to BERT) as well as the initialization of the embeddings.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9985925555229187}, "created": {"value": false, "score": 8.58306884765625e-06}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999116659164429}, "created": {"value": false, "score": 5.3882598876953125e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "references": [{"label": "[7]", "normalizedForm": "[7]", "refKey": 7}, {"label": "[8]", "normalizedForm": "[8]", "refKey": 8}, {"label": "[7]", "normalizedForm": "[7]", "refKey": 7}, {"label": "[8]", "normalizedForm": "[8]", "refKey": 8}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "BioBERT", "normalizedForm": "BioBERT", "offsetStart": 0, "offsetEnd": 7}, "context": "BioBERT used the smallest vocabulary for its pretraining, while BioMedRoBERTa used the largest in comparison to the rest of the models.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9999116659164429}, "created": {"value": false, "score": 3.2186508178710938e-06}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999116659164429}, "created": {"value": false, "score": 5.3882598876953125e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "references": [{"label": "[7]", "normalizedForm": "[7]", "refKey": 7}, {"label": "[8]", "normalizedForm": "[8]", "refKey": 8}, {"label": "[7]", "normalizedForm": "[7]", "refKey": 7}, {"label": "[8]", "normalizedForm": "[8]", "refKey": 8}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "PubMedBERT", "normalizedForm": "PubMedBERT", "offsetStart": 0, "offsetEnd": 10}, "url": {"rawForm": "org/ modules/nltk/stem/snowball", "normalizedForm": "org/ modules/nltk/stem/snowball"}, "context": "PubMedBERT, a model pretrained from scratch using biomedical data, achieved the second best performance, being below SciBERT by 4 % when the training is done for 30 epochs, using Bi-LSTM as classifier and no adding extra-features (which was the best overall result of SciBERT). ", "mentionContextAttributes": {"used": {"value": true, "score": 0.943794310092926}, "created": {"value": false, "score": 3.933906555175781e-06}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999229907989502}, "created": {"value": false, "score": 8.988380432128906e-05}, "shared": {"value": false, "score": 1.430511474609375e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "BioBERT", "normalizedForm": "BioBERT", "offsetStart": 0, "offsetEnd": 11}, "context": "BioBERT [7] and BioMedRoBERTa [21] are some examples of BERT variants pretrained in the biomedical domain.", "mentionContextAttributes": {"used": {"value": false, "score": 0.0009292364120483398}, "created": {"value": false, "score": 3.814697265625e-06}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999116659164429}, "created": {"value": false, "score": 5.3882598876953125e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "references": [{"label": "[7]", "normalizedForm": "[7]", "refKey": 7}, {"label": "[8]", "normalizedForm": "[8]", "refKey": 8}, {"label": "[7]", "normalizedForm": "[7]", "refKey": 7}, {"label": "[8]", "normalizedForm": "[8]", "refKey": 8}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "DeepEventMine", "normalizedForm": "DeepEventMine", "offsetStart": 0, "offsetEnd": 18}, "context": "DeepEventMine [16] is an end-to-end system for event extraction that consists on four main modules; BERT model, trigger and entity detection and classification, relation extraction and event identification. ", "mentionContextAttributes": {"used": {"value": false, "score": 8.237361907958984e-05}, "created": {"value": false, "score": 0.0002709031105041504}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": false, "score": 8.237361907958984e-05}, "created": {"value": false, "score": 0.0002709031105041504}, "shared": {"value": false, "score": 1.1920928955078125e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "BERT", "normalizedForm": "BERT", "offsetStart": 3, "offsetEnd": 7}, "context": "In BERT, the sequence of input tokens (words or sub-words) is constituted with initial vectors that are the combination of the token embeddings, the (token) position embeddings and the segment embeddings (text segment to which the token corresponds) through element-wise summation.", "mentionContextAttributes": {"used": {"value": false, "score": 0.18751364946365356}, "created": {"value": false, "score": 4.744529724121094e-05}, "shared": {"value": false, "score": 1.0728836059570312e-06}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999899864196777}, "created": {"value": true, "score": 0.9998382329940796}, "shared": {"value": false, "score": 1.0728836059570312e-06}}, "references": [{"label": "[6]", "normalizedForm": "[6]", "refKey": 6}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "PubMedBERT", "normalizedForm": "PubMedBERT", "offsetStart": 5, "offsetEnd": 15}, "url": {"rawForm": "org/ modules/nltk/stem/snowball", "normalizedForm": "org/ modules/nltk/stem/snowball", "offsetStart": 75, "offsetEnd": 106}, "context": "When PubMedBERT used Bi-LSTM as classifier, the results 2 https://www.nltk.org/ modules/nltk/stem/snowball.html 3 https://spacy.io/ ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9999229907989502}, "created": {"value": false, "score": 4.172325134277344e-06}, "shared": {"value": false, "score": 1.430511474609375e-06}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999229907989502}, "created": {"value": false, "score": 8.988380432128906e-05}, "shared": {"value": false, "score": 1.430511474609375e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "BERT", "normalizedForm": "BERT", "offsetStart": 6, "offsetEnd": 10}, "context": "These BERT variants have been chosen for comparison because they share the same BERT architecture but have previously been pretrained using different data in the biomedical and/or general domain [7]- [9].", "mentionContextAttributes": {"used": {"value": true, "score": 0.8960372805595398}, "created": {"value": false, "score": 3.2067298889160156e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999899864196777}, "created": {"value": true, "score": 0.9998382329940796}, "shared": {"value": false, "score": 1.0728836059570312e-06}}, "references": [{"label": "[6]", "normalizedForm": "[6]", "refKey": 6}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "PubMedBERT", "normalizedForm": "PubMedBERT", "offsetStart": 12, "offsetEnd": 22}, "url": {"rawForm": "org/ modules/nltk/stem/snowball", "normalizedForm": "org/ modules/nltk/stem/snowball"}, "context": "SciBERT and PubMedBERT, were pretrained from scratch, meaning that they use a unique vocabulary on the pretraining corpus and include embeddings that are specific for in-domain words.", "mentionContextAttributes": {"used": {"value": true, "score": 0.989198625087738}, "created": {"value": false, "score": 7.033348083496094e-06}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999229907989502}, "created": {"value": false, "score": 8.988380432128906e-05}, "shared": {"value": false, "score": 1.430511474609375e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "BioMedRoBERTa", "normalizedForm": "BioMedRoBERTa", "offsetStart": 12, "offsetEnd": 25}, "context": "BioBERT and BioMedRoBERTa were pretrained starting from the BERT checkpoints, which means that the vocabularies are built with general-domain texts (similar to BERT) as well as the initialization of the embeddings.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9985925555229187}, "created": {"value": false, "score": 8.58306884765625e-06}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999116659164429}, "created": {"value": false, "score": 5.3882598876953125e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "BERT", "normalizedForm": "BERT", "offsetStart": 14, "offsetEnd": 18}, "context": "Following the BERT principle, other transformer models have been developed being pretrained with data from specific domains, e.g.", "mentionContextAttributes": {"used": {"value": false, "score": 0.0001398324966430664}, "created": {"value": false, "score": 0.4338177442550659}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999899864196777}, "created": {"value": true, "score": 0.9998382329940796}, "shared": {"value": false, "score": 1.0728836059570312e-06}}, "references": [{"label": "[6]", "normalizedForm": "[6]", "refKey": 6}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "BERT", "normalizedForm": "BERT", "offsetStart": 15, "offsetEnd": 19}, "context": "In the case of BERT, which was trained from scratch using data from the general domain, it presented lower results than PubMedBERT by around 5 %.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9995325803756714}, "created": {"value": false, "score": 2.384185791015625e-06}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999899864196777}, "created": {"value": true, "score": 0.9998382329940796}, "shared": {"value": false, "score": 1.0728836059570312e-06}}, "references": [{"label": "[6]", "normalizedForm": "[6]", "refKey": 6}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "BioMedRoBERTa", "normalizedForm": "BioMedRoBERTa", "offsetStart": 16, "offsetEnd": 34}, "context": "BioBERT [7] and BioMedRoBERTa [21] are some examples of BERT variants pretrained in the biomedical domain.", "mentionContextAttributes": {"used": {"value": false, "score": 0.0009292364120483398}, "created": {"value": false, "score": 3.814697265625e-06}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999116659164429}, "created": {"value": false, "score": 5.3882598876953125e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "BERT", "normalizedForm": "BERT", "offsetStart": 18, "offsetEnd": 22}, "context": "For this purpose, BERT, BioBERT, SciBERT, Pub-MedBERT, and BioMedRoBERTa are fine-tuned using two different classifiers, a linear layer and a Bidirectional Long Short Term Memory (Bi-LSTM) layer, to detect biomedical event triggers. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.07567298412322998}, "created": {"value": false, "score": 5.3882598876953125e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999899864196777}, "created": {"value": true, "score": 0.9998382329940796}, "shared": {"value": false, "score": 1.0728836059570312e-06}}, "references": [{"label": "[6]", "normalizedForm": "[6]", "refKey": 6}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "BERT", "normalizedForm": "BERT", "offsetStart": 20, "offsetEnd": 24}, "context": "The models used are BERT and four of its variants, who have achieved state-of-theart performance in different NLP tasks without requiring major architectural modifications according to the specific tasks.", "mentionContextAttributes": {"used": {"value": false, "score": 0.03834468126296997}, "created": {"value": false, "score": 0.00031816959381103516}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999899864196777}, "created": {"value": true, "score": 0.9998382329940796}, "shared": {"value": false, "score": 1.0728836059570312e-06}}, "references": [{"label": "[6]", "normalizedForm": "[6]", "refKey": 6}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "BERT", "normalizedForm": "BERT", "offsetStart": 20, "offsetEnd": 24}, "context": "The best results of BERT were obtained using a linear classifier and not adding extra features, noticing that the results of BERT+POS and BERT+stem were slightly lower and very similar between each other.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9999899864196777}, "created": {"value": false, "score": 7.867813110351562e-06}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999899864196777}, "created": {"value": true, "score": 0.9998382329940796}, "shared": {"value": false, "score": 1.0728836059570312e-06}}, "references": [{"label": "[6]", "normalizedForm": "[6]", "refKey": 6}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "BERT", "normalizedForm": "BERT", "offsetStart": 22, "offsetEnd": 26}, "context": "A. Transformer Model: BERT BERT [6] is a contextualized word representation model based on a masked language model pretrained with bidirectional transformers [7].", "mentionContextAttributes": {"used": {"value": false, "score": 7.95125961303711e-05}, "created": {"value": false, "score": 1.3470649719238281e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999899864196777}, "created": {"value": true, "score": 0.9998382329940796}, "shared": {"value": false, "score": 1.0728836059570312e-06}}, "references": [{"label": "[6]", "normalizedForm": "[6]", "refKey": 6, "offsetStart": 10363, "offsetEnd": 10366}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "BERT", "normalizedForm": "BERT", "offsetStart": 23, "offsetEnd": 27}, "context": "From this pretraining, BERT can be fine-tuned by including additional layers on top of the model to solve new specific tasks.", "mentionContextAttributes": {"used": {"value": false, "score": 5.364418029785156e-05}, "created": {"value": false, "score": 0.0007520318031311035}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999899864196777}, "created": {"value": true, "score": 0.9998382329940796}, "shared": {"value": false, "score": 1.0728836059570312e-06}}, "references": [{"label": "[6]", "normalizedForm": "[6]", "refKey": 6}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "BERT", "normalizedForm": "BERT", "offsetStart": 23, "offsetEnd": 27}, "context": "The transformer model, BERT [6], and four BERT variants pretrained in the biomedical domain, BioBERT [7], SciBERT [8], PubMedBERT [20], and BioMedRoBERTa [21], are used and compared for the detection of event triggers.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9660391807556152}, "created": {"value": false, "score": 5.9604644775390625e-06}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999899864196777}, "created": {"value": true, "score": 0.9998382329940796}, "shared": {"value": false, "score": 1.0728836059570312e-06}}, "references": [{"label": "[6]", "normalizedForm": "[6]", "refKey": 6, "offsetStart": 14220, "offsetEnd": 14223}, {"label": "[6]", "normalizedForm": "[6]", "refKey": 6, "offsetStart": 14220, "offsetEnd": 14223}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "SciBERT", "normalizedForm": "SciBERT", "offsetStart": 23, "offsetEnd": 30}, "context": "First, we observe that SciBERT, which was pretrained from scratch using biomedical and general data, obtained the best results for each number of epochs and overall, in P, R and F1. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9999836683273315}, "created": {"value": false, "score": 7.271766662597656e-06}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999836683273315}, "created": {"value": false, "score": 0.00028389692306518555}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "references": [{"label": "[8]", "normalizedForm": "[8]", "refKey": 8}, {"label": "[8]", "normalizedForm": "[8]", "refKey": 8}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "BERT", "normalizedForm": "BERT", "offsetStart": 24, "offsetEnd": 28}, "context": "In this work we compare BERT and four of its variants pretrained in the biomedical domain for the detection of biomedical event triggers to analyze their performance and identify which model is the most appropriate to address this task.", "mentionContextAttributes": {"used": {"value": false, "score": 0.10326075553894043}, "created": {"value": false, "score": 0.17881929874420166}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999899864196777}, "created": {"value": true, "score": 0.9998382329940796}, "shared": {"value": false, "score": 1.0728836059570312e-06}}, "references": [{"label": "[6]", "normalizedForm": "[6]", "refKey": 6}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "BioBERT", "normalizedForm": "BioBERT", "offsetStart": 24, "offsetEnd": 31}, "context": "For this purpose, BERT, BioBERT, SciBERT, Pub-MedBERT, and BioMedRoBERTa are fine-tuned using two different classifiers, a linear layer and a Bidirectional Long Short Term Memory (Bi-LSTM) layer, to detect biomedical event triggers. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.07567298412322998}, "created": {"value": false, "score": 5.3882598876953125e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999116659164429}, "created": {"value": false, "score": 5.3882598876953125e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "references": [{"label": "[7]", "normalizedForm": "[7]", "refKey": 7}, {"label": "[8]", "normalizedForm": "[8]", "refKey": 8}, {"label": "[7]", "normalizedForm": "[7]", "refKey": 7}, {"label": "[8]", "normalizedForm": "[8]", "refKey": 8}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "BERT", "normalizedForm": "BERT", "offsetStart": 25, "offsetEnd": 29}, "context": "For each of the modules, BERT is used as base model and a linear layer is added.", "mentionContextAttributes": {"used": {"value": false, "score": 0.1906760334968567}, "created": {"value": false, "score": 7.009506225585938e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999899864196777}, "created": {"value": true, "score": 0.9998382329940796}, "shared": {"value": false, "score": 1.0728836059570312e-06}}, "references": [{"label": "[6]", "normalizedForm": "[6]", "refKey": 6}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "BERT", "normalizedForm": "BERT", "offsetStart": 25, "offsetEnd": 29}, "context": "In this work, we analyze BERT and four of its variants for biomedical event detection using corpora of different biomedical subdomains.", "mentionContextAttributes": {"used": {"value": false, "score": 0.002253234386444092}, "created": {"value": true, "score": 0.9883823990821838}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999899864196777}, "created": {"value": true, "score": 0.9998382329940796}, "shared": {"value": false, "score": 1.0728836059570312e-06}}, "references": [{"label": "[6]", "normalizedForm": "[6]", "refKey": 6}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "BERT", "normalizedForm": "BERT", "offsetStart": 27, "offsetEnd": 31}, "context": "A. Transformer Model: BERT BERT [6] is a contextualized word representation model based on a masked language model pretrained with bidirectional transformers [7].", "mentionContextAttributes": {"used": {"value": false, "score": 7.95125961303711e-05}, "created": {"value": false, "score": 1.3470649719238281e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999899864196777}, "created": {"value": true, "score": 0.9998382329940796}, "shared": {"value": false, "score": 1.0728836059570312e-06}}, "references": [{"label": "[6]", "normalizedForm": "[6]", "refKey": 6, "offsetStart": 10363, "offsetEnd": 10366}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "BERT", "normalizedForm": "BERT", "offsetStart": 33, "offsetEnd": 37}, "context": "B. Portelli et al. [17] compared BERT and five of its variants for the identification of Adverse Drugs and Events (ADEs).", "mentionContextAttributes": {"used": {"value": true, "score": 0.9999064207077026}, "created": {"value": false, "score": 4.887580871582031e-06}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999899864196777}, "created": {"value": true, "score": 0.9998382329940796}, "shared": {"value": false, "score": 1.0728836059570312e-06}}, "references": [{"label": "[6]", "normalizedForm": "[6]", "refKey": 6}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "SciBERT", "normalizedForm": "SciBERT", "offsetStart": 33, "offsetEnd": 40}, "context": "For this purpose, BERT, BioBERT, SciBERT, Pub-MedBERT, and BioMedRoBERTa are fine-tuned using two different classifiers, a linear layer and a Bidirectional Long Short Term Memory (Bi-LSTM) layer, to detect biomedical event triggers. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.07567298412322998}, "created": {"value": false, "score": 5.3882598876953125e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999836683273315}, "created": {"value": false, "score": 0.00028389692306518555}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "references": [{"label": "[8]", "normalizedForm": "[8]", "refKey": 8}, {"label": "[8]", "normalizedForm": "[8]", "refKey": 8}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "PyTorch", "normalizedForm": "PyTorch", "offsetStart": 34, "offsetEnd": 41}, "context": "All the experiments are done with PyTorch, using the Transformers 4 library and the models were taken from Hugging Face 5 . ", "mentionContextAttributes": {"used": {"value": true, "score": 0.999739944934845}, "created": {"value": false, "score": 5.030632019042969e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.999739944934845}, "created": {"value": false, "score": 5.030632019042969e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "BERT", "normalizedForm": "BERT", "offsetStart": 39, "offsetEnd": 43}, "context": "Furthermore, a series of variants from BERT have been developed for specific domains by being trained on large corpus with the same context, such as the biomedical domain.", "mentionContextAttributes": {"used": {"value": false, "score": 5.257129669189453e-05}, "created": {"value": false, "score": 0.29649436473846436}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999899864196777}, "created": {"value": true, "score": 0.9998382329940796}, "shared": {"value": false, "score": 1.0728836059570312e-06}}, "references": [{"label": "[6]", "normalizedForm": "[6]", "refKey": 6}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "BERT", "normalizedForm": "BERT", "offsetStart": 42, "offsetEnd": 46}, "context": "In this work we propose the comparison of BERT and four of its variants for the detection of biomedical events to evaluate and analyze the differences in their performance.", "mentionContextAttributes": {"used": {"value": false, "score": 0.00013911724090576172}, "created": {"value": true, "score": 0.9998382329940796}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999899864196777}, "created": {"value": true, "score": 0.9998382329940796}, "shared": {"value": false, "score": 1.0728836059570312e-06}}, "references": [{"label": "[6]", "normalizedForm": "[6]", "refKey": 6}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "BERT", "normalizedForm": "BERT", "offsetStart": 42, "offsetEnd": 46}, "context": "The transformer model, BERT [6], and four BERT variants pretrained in the biomedical domain, BioBERT [7], SciBERT [8], PubMedBERT [20], and BioMedRoBERTa [21], are used and compared for the detection of event triggers.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9660391807556152}, "created": {"value": false, "score": 5.9604644775390625e-06}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999899864196777}, "created": {"value": true, "score": 0.9998382329940796}, "shared": {"value": false, "score": 1.0728836059570312e-06}}, "references": [{"label": "[6]", "normalizedForm": "[6]", "refKey": 6}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Pub-MedBERT", "normalizedForm": "Pub-MedBERT", "offsetStart": 42, "offsetEnd": 53}, "context": "For this purpose, BERT, BioBERT, SciBERT, Pub-MedBERT, and BioMedRoBERTa are fine-tuned using two different classifiers, a linear layer and a Bidirectional Long Short Term Memory (Bi-LSTM) layer, to detect biomedical event triggers. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.07567298412322998}, "created": {"value": false, "score": 5.3882598876953125e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": false, "score": 0.07567298412322998}, "created": {"value": false, "score": 5.3882598876953125e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "SciBERT", "normalizedForm": "SciBERT", "offsetStart": 44, "offsetEnd": 51}, "context": "Fig. 3 shows the performance of fine-tuning SciBERT during 30 epochs using a Bi-LSTM classifier on the seven datasets separately.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9994121789932251}, "created": {"value": false, "score": 5.0067901611328125e-06}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999836683273315}, "created": {"value": false, "score": 0.00028389692306518555}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "references": [{"label": "[8]", "normalizedForm": "[8]", "refKey": 8}, {"label": "[8]", "normalizedForm": "[8]", "refKey": 8}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "spanBERT", "normalizedForm": "spanBERT", "offsetStart": 46, "offsetEnd": 54}, "context": "They showed that span-based pretraining, from spanBERT, provides an improvement in the recognition of ADEs, and that the pretraining of the models in the specific domain is particularly useful in comparison to train the models from scratch. A. Ramponi et al. [18] developed BEESL, a neural network model based on a sequence labeling system for the extraction of events. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.00023442506790161133}, "created": {"value": true, "score": 0.8193156123161316}, "shared": {"value": false, "score": 5.960464477539062e-07}}, "documentContextAttributes": {"used": {"value": false, "score": 0.00023442506790161133}, "created": {"value": true, "score": 0.8193156123161316}, "shared": {"value": false, "score": 5.960464477539062e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "PubMedBERT", "normalizedForm": "PubMedBERT", "offsetStart": 47, "offsetEnd": 57}, "url": {"rawForm": "org/ modules/nltk/stem/snowball", "normalizedForm": "org/ modules/nltk/stem/snowball"}, "context": "These three last transformer models, SciB-ERT, PubMedBERT and BERT, presented some similarities in that they were trained from scratch, used very comparable text sizes for their pretraining and had similar vocabulary sizes.", "mentionContextAttributes": {"used": {"value": false, "score": 0.09230715036392212}, "created": {"value": false, "score": 8.988380432128906e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999229907989502}, "created": {"value": false, "score": 8.988380432128906e-05}, "shared": {"value": false, "score": 1.430511474609375e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "SciBERT", "normalizedForm": "SciBERT", "offsetStart": 51, "offsetEnd": 58}, "context": "In Fig. 4 it is observed the effect of fine-tuning SciBERT over 30 epochs using a Bi-LSTM classifier without adding extra-features by cumulatively adding each corpus one by one.", "mentionContextAttributes": {"used": {"value": false, "score": 0.016741931438446045}, "created": {"value": false, "score": 4.8279762268066406e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999836683273315}, "created": {"value": false, "score": 0.00028389692306518555}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "references": [{"label": "[8]", "normalizedForm": "[8]", "refKey": 8}, {"label": "[8]", "normalizedForm": "[8]", "refKey": 8}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "BERT", "normalizedForm": "BERT", "offsetStart": 56, "offsetEnd": 60}, "context": "BioBERT [7] and BioMedRoBERTa [21] are some examples of BERT variants pretrained in the biomedical domain.", "mentionContextAttributes": {"used": {"value": false, "score": 0.0009292364120483398}, "created": {"value": false, "score": 3.814697265625e-06}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999899864196777}, "created": {"value": true, "score": 0.9998382329940796}, "shared": {"value": false, "score": 1.0728836059570312e-06}}, "references": [{"label": "[6]", "normalizedForm": "[6]", "refKey": 6}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "SciBERT", "normalizedForm": "SciBERT", "offsetStart": 57, "offsetEnd": 64}, "context": "When the fine tuning was done using a linear classifier, SciBERT+POS achieved the best results, having a difference of around 10 % to when the lexical feature (SciBERT+stem) is added. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9999474287033081}, "created": {"value": false, "score": 5.602836608886719e-06}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999836683273315}, "created": {"value": false, "score": 0.00028389692306518555}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "references": [{"label": "[8]", "normalizedForm": "[8]", "refKey": 8}, {"label": "[8]", "normalizedForm": "[8]", "refKey": 8}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "BioBERT", "normalizedForm": "BioBERT", "offsetStart": 57, "offsetEnd": 64}, "context": "The two models that presented the lowest performance are BioBERT and BioMedRoBERTa, both pretrained from the BERT weights, using biomedical and, biomedical and general data, respectively, presenting the largest text sizes of all the models.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9997931122779846}, "created": {"value": false, "score": 1.2278556823730469e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999116659164429}, "created": {"value": false, "score": 5.3882598876953125e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "references": [{"label": "[7]", "normalizedForm": "[7]", "refKey": 7}, {"label": "[8]", "normalizedForm": "[8]", "refKey": 8}, {"label": "[7]", "normalizedForm": "[7]", "refKey": 7}, {"label": "[8]", "normalizedForm": "[8]", "refKey": 8}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "BioMedRoBERTa", "normalizedForm": "BioMedRoBERTa", "offsetStart": 59, "offsetEnd": 72}, "context": "For this purpose, BERT, BioBERT, SciBERT, Pub-MedBERT, and BioMedRoBERTa are fine-tuned using two different classifiers, a linear layer and a Bidirectional Long Short Term Memory (Bi-LSTM) layer, to detect biomedical event triggers. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.07567298412322998}, "created": {"value": false, "score": 5.3882598876953125e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999116659164429}, "created": {"value": false, "score": 5.3882598876953125e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "BERT", "normalizedForm": "BERT", "offsetStart": 60, "offsetEnd": 64}, "context": "BioBERT and BioMedRoBERTa were pretrained starting from the BERT checkpoints, which means that the vocabularies are built with general-domain texts (similar to BERT) as well as the initialization of the embeddings.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9985925555229187}, "created": {"value": false, "score": 8.58306884765625e-06}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999899864196777}, "created": {"value": true, "score": 0.9998382329940796}, "shared": {"value": false, "score": 1.0728836059570312e-06}}, "references": [{"label": "[6]", "normalizedForm": "[6]", "refKey": 6}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "BERT", "normalizedForm": "BERT", "offsetStart": 62, "offsetEnd": 66}, "context": "These three last transformer models, SciB-ERT, PubMedBERT and BERT, presented some similarities in that they were trained from scratch, used very comparable text sizes for their pretraining and had similar vocabulary sizes.", "mentionContextAttributes": {"used": {"value": false, "score": 0.09230715036392212}, "created": {"value": false, "score": 8.988380432128906e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999899864196777}, "created": {"value": true, "score": 0.9998382329940796}, "shared": {"value": false, "score": 1.0728836059570312e-06}}, "references": [{"label": "[6]", "normalizedForm": "[6]", "refKey": 6}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "BioMedRoBERTa", "normalizedForm": "BioMedRoBERTa", "offsetStart": 64, "offsetEnd": 77}, "context": "BioBERT used the smallest vocabulary for its pretraining, while BioMedRoBERTa used the largest in comparison to the rest of the models.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9999116659164429}, "created": {"value": false, "score": 3.2186508178710938e-06}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999116659164429}, "created": {"value": false, "score": 5.3882598876953125e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "BioMedRoBERTa", "normalizedForm": "BioMedRoBERTa", "offsetStart": 69, "offsetEnd": 82}, "context": "The two models that presented the lowest performance are BioBERT and BioMedRoBERTa, both pretrained from the BERT weights, using biomedical and, biomedical and general data, respectively, presenting the largest text sizes of all the models.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9997931122779846}, "created": {"value": false, "score": 1.2278556823730469e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999116659164429}, "created": {"value": false, "score": 5.3882598876953125e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "BERT", "normalizedForm": "BERT", "offsetStart": 70, "offsetEnd": 74}, "context": "The transformer models are trained using the original parameters from BERT, presenting a dropout probability for the attention heads and hidden layers of 0.1, a hidden size of 768, an initializer range of 0.02, a max position embeddings of 512 and an intermediate size of 3,072.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9997299313545227}, "created": {"value": false, "score": 4.458427429199219e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999899864196777}, "created": {"value": true, "score": 0.9998382329940796}, "shared": {"value": false, "score": 1.0728836059570312e-06}}, "references": [{"label": "[6]", "normalizedForm": "[6]", "refKey": 6}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "SciBERT", "normalizedForm": "SciBERT", "offsetStart": 76, "offsetEnd": 83}, "context": "When the training was done for more than 10 epochs, the performance between SciBERT+POS (syntactic feature) and SciBERT+stem (lexical feature) was very similar. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.999737560749054}, "created": {"value": false, "score": 1.2636184692382812e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999836683273315}, "created": {"value": false, "score": 0.00028389692306518555}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "references": [{"label": "[8]", "normalizedForm": "[8]", "refKey": 8}, {"label": "[8]", "normalizedForm": "[8]", "refKey": 8}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "BERT", "normalizedForm": "BERT", "offsetStart": 80, "offsetEnd": 84}, "context": "These BERT variants have been chosen for comparison because they share the same BERT architecture but have previously been pretrained using different data in the biomedical and/or general domain [7]- [9].", "mentionContextAttributes": {"used": {"value": true, "score": 0.8960372805595398}, "created": {"value": false, "score": 3.2067298889160156e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999899864196777}, "created": {"value": true, "score": 0.9998382329940796}, "shared": {"value": false, "score": 1.0728836059570312e-06}}, "references": [{"label": "[6]", "normalizedForm": "[6]", "refKey": 6}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "BERT", "normalizedForm": "BERT", "offsetStart": 86, "offsetEnd": 90}, "context": "The system converts the event structures into a format of sequence labeling, and uses BERT as language model.", "mentionContextAttributes": {"used": {"value": false, "score": 0.00031065940856933594}, "created": {"value": false, "score": 0.0007901191711425781}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999899864196777}, "created": {"value": true, "score": 0.9998382329940796}, "shared": {"value": false, "score": 1.0728836059570312e-06}}, "references": [{"label": "[6]", "normalizedForm": "[6]", "refKey": 6}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "BERT", "normalizedForm": "BERT", "offsetStart": 93, "offsetEnd": 97}, "context": "Various downstream text mining tasks can be performed by making minimal modifications to the BERT architecture through a process of fine-tuning.", "mentionContextAttributes": {"used": {"value": false, "score": 0.0002135634422302246}, "created": {"value": false, "score": 0.002641737461090088}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999899864196777}, "created": {"value": true, "score": 0.9998382329940796}, "shared": {"value": false, "score": 1.0728836059570312e-06}}, "references": [{"label": "[6]", "normalizedForm": "[6]", "refKey": 6}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "BioBERT", "normalizedForm": "BioBERT", "offsetStart": 93, "offsetEnd": 100}, "context": "The transformer model, BERT [6], and four BERT variants pretrained in the biomedical domain, BioBERT [7], SciBERT [8], PubMedBERT [20], and BioMedRoBERTa [21], are used and compared for the detection of event triggers.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9660391807556152}, "created": {"value": false, "score": 5.9604644775390625e-06}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999116659164429}, "created": {"value": false, "score": 5.3882598876953125e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "references": [{"label": "[7]", "normalizedForm": "[7]", "refKey": 7, "offsetStart": 14293, "offsetEnd": 14296}, {"label": "[8]", "normalizedForm": "[8]", "refKey": 8, "offsetStart": 14306, "offsetEnd": 14309}, {"label": "[7]", "normalizedForm": "[7]", "refKey": 7, "offsetStart": 14293, "offsetEnd": 14296}, {"label": "[8]", "normalizedForm": "[8]", "refKey": 8, "offsetStart": 14306, "offsetEnd": 14309}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "BERT", "normalizedForm": "BERT", "offsetStart": 100, "offsetEnd": 104}, "context": "DeepEventMine [16] is an end-to-end system for event extraction that consists on four main modules; BERT model, trigger and entity detection and classification, relation extraction and event identification.", "mentionContextAttributes": {"used": {"value": false, "score": 8.237361907958984e-05}, "created": {"value": false, "score": 0.0002709031105041504}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999899864196777}, "created": {"value": true, "score": 0.9998382329940796}, "shared": {"value": false, "score": 1.0728836059570312e-06}}, "references": [{"label": "[6]", "normalizedForm": "[6]", "refKey": 6}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "BERT", "normalizedForm": "BERT", "offsetStart": 103, "offsetEnd": 107}, "context": "In addition, the context of the different biomedical subdomains may also affect the performance, since BERT and its variants compute embeddings considering the semantics.", "mentionContextAttributes": {"used": {"value": false, "score": 0.000750124454498291}, "created": {"value": false, "score": 2.7179718017578125e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999899864196777}, "created": {"value": true, "score": 0.9998382329940796}, "shared": {"value": false, "score": 1.0728836059570312e-06}}, "references": [{"label": "[6]", "normalizedForm": "[6]", "refKey": 6}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "SciBERT", "normalizedForm": "SciBERT", "offsetStart": 106, "offsetEnd": 113}, "context": "The transformer model, BERT [6], and four BERT variants pretrained in the biomedical domain, BioBERT [7], SciBERT [8], PubMedBERT [20], and BioMedRoBERTa [21], are used and compared for the detection of event triggers.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9660391807556152}, "created": {"value": false, "score": 5.9604644775390625e-06}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999836683273315}, "created": {"value": false, "score": 0.00028389692306518555}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "references": [{"label": "[8]", "normalizedForm": "[8]", "refKey": 8, "offsetStart": 14306, "offsetEnd": 14309}, {"label": "[8]", "normalizedForm": "[8]", "refKey": 8, "offsetStart": 14306, "offsetEnd": 14309}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "BERT", "normalizedForm": "BERT", "offsetStart": 109, "offsetEnd": 113}, "context": "The two models that presented the lowest performance are BioBERT and BioMedRoBERTa, both pretrained from the BERT weights, using biomedical and, biomedical and general data, respectively, presenting the largest text sizes of all the models.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9997931122779846}, "created": {"value": false, "score": 1.2278556823730469e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999899864196777}, "created": {"value": true, "score": 0.9998382329940796}, "shared": {"value": false, "score": 1.0728836059570312e-06}}, "references": [{"label": "[6]", "normalizedForm": "[6]", "refKey": 6}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "SciBERT", "normalizedForm": "SciBERT", "offsetStart": 112, "offsetEnd": 119}, "context": "When the training was done for more than 10 epochs, the performance between SciBERT+POS (syntactic feature) and SciBERT+stem (lexical feature) was very similar. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.999737560749054}, "created": {"value": false, "score": 1.2636184692382812e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999836683273315}, "created": {"value": false, "score": 0.00028389692306518555}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "references": [{"label": "[8]", "normalizedForm": "[8]", "refKey": 8}, {"label": "[8]", "normalizedForm": "[8]", "refKey": 8}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "SciBERT", "normalizedForm": "SciBERT", "offsetStart": 117, "offsetEnd": 124}, "context": "PubMedBERT, a model pretrained from scratch using biomedical data, achieved the second best performance, being below SciBERT by 4 % when the training is done for 30 epochs, using Bi-LSTM as classifier and no adding extra-features (which was the best overall result of SciBERT). ", "mentionContextAttributes": {"used": {"value": true, "score": 0.943794310092926}, "created": {"value": false, "score": 3.933906555175781e-06}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999836683273315}, "created": {"value": false, "score": 0.00028389692306518555}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "references": [{"label": "[8]", "normalizedForm": "[8]", "refKey": 8}, {"label": "[8]", "normalizedForm": "[8]", "refKey": 8}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "SciBERT", "normalizedForm": "SciBERT", "offsetStart": 117, "offsetEnd": 124}, "context": "By comparing the performance of the models and by adding a lexical and syntactic features, we found that fine-tuning SciBERT during 30 epochs using a Bi-LSTM classifier is the best strategy to detect biomedical events, especially if the additional features are not included. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9638322591781616}, "created": {"value": false, "score": 2.3484230041503906e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999836683273315}, "created": {"value": false, "score": 0.00028389692306518555}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "references": [{"label": "[8]", "normalizedForm": "[8]", "refKey": 8}, {"label": "[8]", "normalizedForm": "[8]", "refKey": 8}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "PubMedBERT", "normalizedForm": "PubMedBERT", "offsetStart": 119, "offsetEnd": 133}, "url": {"rawForm": "org/ modules/nltk/stem/snowball", "normalizedForm": "org/ modules/nltk/stem/snowball"}, "context": "The transformer model, BERT [6], and four BERT variants pretrained in the biomedical domain, BioBERT [7], SciBERT [8], PubMedBERT [20], and BioMedRoBERTa [21], are used and compared for the detection of event triggers. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9660391807556152}, "created": {"value": false, "score": 5.9604644775390625e-06}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999229907989502}, "created": {"value": false, "score": 8.988380432128906e-05}, "shared": {"value": false, "score": 1.430511474609375e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "PubMedBERT", "normalizedForm": "PubMedBERT", "offsetStart": 120, "offsetEnd": 130}, "url": {"rawForm": "org/ modules/nltk/stem/snowball", "normalizedForm": "org/ modules/nltk/stem/snowball"}, "context": "In the case of BERT, which was trained from scratch using data from the general domain, it presented lower results than PubMedBERT by around 5 %. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9995325803756714}, "created": {"value": false, "score": 2.384185791015625e-06}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999229907989502}, "created": {"value": false, "score": 8.988380432128906e-05}, "shared": {"value": false, "score": 1.430511474609375e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "BERT", "normalizedForm": "BERT", "offsetStart": 125, "offsetEnd": 129}, "context": "The best results of BERT were obtained using a linear classifier and not adding extra features, noticing that the results of BERT+POS and BERT+stem were slightly lower and very similar between each other.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9999899864196777}, "created": {"value": false, "score": 7.867813110351562e-06}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999899864196777}, "created": {"value": true, "score": 0.9998382329940796}, "shared": {"value": false, "score": 1.0728836059570312e-06}}, "references": [{"label": "[6]", "normalizedForm": "[6]", "refKey": 6}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "BERT", "normalizedForm": "BERT", "offsetStart": 126, "offsetEnd": 130}, "context": "Each sentence is further split into words by spaces and then, each word into sub-words or tokens following the setting of the BERT tokenization.", "mentionContextAttributes": {"used": {"value": true, "score": 0.7657299041748047}, "created": {"value": false, "score": 7.545948028564453e-05}, "shared": {"value": false, "score": 7.152557373046875e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999899864196777}, "created": {"value": true, "score": 0.9998382329940796}, "shared": {"value": false, "score": 1.0728836059570312e-06}}, "references": [{"label": "[6]", "normalizedForm": "[6]", "refKey": 6}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "BERT", "normalizedForm": "BERT", "offsetStart": 138, "offsetEnd": 142}, "context": "The best results of BERT were obtained using a linear classifier and not adding extra features, noticing that the results of BERT+POS and BERT+stem were slightly lower and very similar between each other.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9999899864196777}, "created": {"value": false, "score": 7.867813110351562e-06}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999899864196777}, "created": {"value": true, "score": 0.9998382329940796}, "shared": {"value": false, "score": 1.0728836059570312e-06}}, "references": [{"label": "[6]", "normalizedForm": "[6]", "refKey": 6}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "BioMedRoBERTa", "normalizedForm": "BioMedRoBERTa", "offsetStart": 140, "offsetEnd": 157}, "context": "The transformer model, BERT [6], and four BERT variants pretrained in the biomedical domain, BioBERT [7], SciBERT [8], PubMedBERT [20], and BioMedRoBERTa [21], are used and compared for the detection of event triggers. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9660394191741943}, "created": {"value": false, "score": 5.9604644775390625e-06}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999116659164429}, "created": {"value": false, "score": 5.3882598876953125e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "spaCy", "normalizedForm": "spaCy", "offsetStart": 141, "offsetEnd": 146}, "version": {"rawForm": "3", "normalizedForm": "3", "offsetStart": 147, "offsetEnd": 148}, "context": "For this work, the stems of the words are obtained using the 'Snowball Stemmer' module from NLTK-3.4.5 2 , while the POS were obtained using spaCy-3.0.0 3 , using 'en core web sm', a pipeline developed for biomedical data. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9999350309371948}, "created": {"value": false, "score": 0.00045186281204223633}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999350309371948}, "created": {"value": false, "score": 0.00045186281204223633}, "shared": {"value": false, "score": 2.384185791015625e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "BERT", "normalizedForm": "BERT", "offsetStart": 160, "offsetEnd": 164}, "context": "BioBERT and BioMedRoBERTa were pretrained starting from the BERT checkpoints, which means that the vocabularies are built with general-domain texts (similar to BERT) as well as the initialization of the embeddings.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9985925555229187}, "created": {"value": false, "score": 8.58306884765625e-06}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999899864196777}, "created": {"value": true, "score": 0.9998382329940796}, "shared": {"value": false, "score": 1.0728836059570312e-06}}, "references": [{"label": "[6]", "normalizedForm": "[6]", "refKey": 6}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "SciBERT", "normalizedForm": "SciBERT", "offsetStart": 160, "offsetEnd": 167}, "context": "When the fine tuning was done using a linear classifier, SciBERT+POS achieved the best results, having a difference of around 10 % to when the lexical feature (SciBERT+stem) is added.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9999474287033081}, "created": {"value": false, "score": 5.602836608886719e-06}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999836683273315}, "created": {"value": false, "score": 0.00028389692306518555}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "references": [{"label": "[8]", "normalizedForm": "[8]", "refKey": 8}, {"label": "[8]", "normalizedForm": "[8]", "refKey": 8}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "SciBERT", "normalizedForm": "SciBERT", "offsetStart": 268, "offsetEnd": 275}, "context": "PubMedBERT, a model pretrained from scratch using biomedical data, achieved the second best performance, being below SciBERT by 4 % when the training is done for 30 epochs, using Bi-LSTM as classifier and no adding extra-features (which was the best overall result of SciBERT). ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9437940716743469}, "created": {"value": false, "score": 3.933906555175781e-06}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999836683273315}, "created": {"value": false, "score": 0.00028389692306518555}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "references": [{"label": "[8]", "normalizedForm": "[8]", "refKey": 8}, {"label": "[8]", "normalizedForm": "[8]", "refKey": 8}]}], "references": [{"refKey": 6, "tei": "<biblStruct xml:id=\"b6\">\n\t<analytic>\n\t\t<title level=\"a\" type=\"main\"></title>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Jacob</forename><surname>Devlin</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Ming-Wei</forename><surname>Chang</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Kenton</forename><surname>Lee</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Kristina</forename><surname>Toutanova</surname></persName>\n\t\t</author>\n\t\t<idno type=\"DOI\">10.18653/v1/n19-1423</idno>\n\t\t<idno>arXiv:1810.04805</idno>\n\t</analytic>\n\t<monogr>\n\t\t<title level=\"m\">Proceedings of the 2019 Conference of the North</title>\n\t\t<meeting>the 2019 Conference of the North</meeting>\n\t\t<imprint>\n\t\t\t<publisher>Association for Computational Linguistics</publisher>\n\t\t\t<date type=\"published\" when=\"2019\">2018</date>\n\t\t</imprint>\n\t</monogr>\n</biblStruct>\n"}, {"refKey": 8, "tei": "<biblStruct xml:id=\"b8\">\n\t<analytic>\n\t\t<title level=\"a\" type=\"main\">SciBERT: A Pretrained Language Model for Scientific Text</title>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Iz</forename><surname>Beltagy</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Kyle</forename><surname>Lo</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Arman</forename><surname>Cohan</surname></persName>\n\t\t</author>\n\t\t<idno type=\"DOI\">10.18653/v1/d19-1371</idno>\n\t\t<idno>arXiv:1903.10676</idno>\n\t</analytic>\n\t<monogr>\n\t\t<title level=\"m\">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>\n\t\t<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</meeting>\n\t\t<imprint>\n\t\t\t<publisher>Association for Computational Linguistics</publisher>\n\t\t\t<date type=\"published\" when=\"2019\">2019</date>\n\t\t</imprint>\n\t</monogr>\n</biblStruct>\n"}, {"refKey": 7, "tei": "<biblStruct xml:id=\"b7\">\n\t<analytic>\n\t\t<title level=\"a\" type=\"main\">BioBERT: a pre-trained biomedical language representation model for biomedical text mining</title>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Jinhyuk</forename><surname>Lee</surname></persName>\n\t\t\t<idno type=\"ORCID\">0000-0003-4972-239X</idno>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Wonjin</forename><surname>Yoon</surname></persName>\n\t\t\t<idno type=\"ORCID\">0000-0002-6435-548X</idno>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Sungdong</forename><surname>Kim</surname></persName>\n\t\t\t<idno type=\"ORCID\">0000-0002-0240-6210</idno>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Donghyeon</forename><surname>Kim</surname></persName>\n\t\t\t<idno type=\"ORCID\">0000-0002-8224-8354</idno>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Sunkyu</forename><surname>Kim</surname></persName>\n\t\t\t<idno type=\"ORCID\">0000-0002-0240-6210</idno>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Chan</forename><forename type=\"middle\">Ho</forename><surname>So</surname></persName>\n\t\t\t<idno type=\"ORCID\">0000-0001-7633-1074</idno>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Jaewoo</forename><surname>Kang</surname></persName>\n\t\t\t<idno type=\"ORCID\">0000-0001-6798-9106</idno>\n\t\t</author>\n\t\t<idno type=\"DOI\">10.1093/bioinformatics/btz682</idno>\n\t</analytic>\n\t<monogr>\n\t\t<title level=\"j\">Bioinformatics</title>\n\t\t<idno type=\"ISSN\">1367-4803</idno>\n\t\t<idno type=\"ISSNe\">1367-4811</idno>\n\t\t<imprint>\n\t\t\t<biblScope unit=\"volume\">36</biblScope>\n\t\t\t<biblScope unit=\"issue\">4</biblScope>\n\t\t\t<biblScope unit=\"page\" from=\"1234\" to=\"1240\" />\n\t\t\t<date type=\"published\" when=\"2019-09-10\">2020</date>\n\t\t\t<publisher>Oxford University Press (OUP)</publisher>\n\t\t</imprint>\n\t</monogr>\n</biblStruct>\n"}], "runtime": 19265, "id": "ce213ca9a797b5e8347139b25494b595f2d04440", "metadata": {"id": "ce213ca9a797b5e8347139b25494b595f2d04440"}, "original_file_path": "../../datalake/Samuel/SOFTware-Sync/downloads/xml/hal-04197974.grobid.tei.xml", "file_name": "hal-04197974.grobid.tei.xml"}