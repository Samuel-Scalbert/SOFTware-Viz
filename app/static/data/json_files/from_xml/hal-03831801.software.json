{"application": "software-mentions", "version": "0.8.0", "date": "2024-10-07T11:47+0000", "md5": "115F130ED85D687E021F6499A907C024", "mentions": [{"type": "software", "software-type": "software", "software-name": {"rawForm": "HuBERT", "normalizedForm": "HuBERT", "offsetStart": 0, "offsetEnd": 6}, "context": "HuBERT is a selfsupervised model trained on the task of masked prediction of continuous audio signals, similarly to BERT (Devlin et al., 2019). ", "mentionContextAttributes": {"used": {"value": false, "score": 4.4226646423339844e-05}, "created": {"value": false, "score": 0.012139737606048584}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999951124191284}, "created": {"value": false, "score": 0.0972023606300354}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "references": [{"label": "(Hsu et al., 2021)", "normalizedForm": "Hsu et al., 2021", "refKey": 17}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "LibriSpeech", "normalizedForm": "LibriSpeech", "offsetStart": 3, "offsetEnd": 14}, "context": "As LibriSpeech consists of non-expressive samples, we treat them as \"neutral\". ", "mentionContextAttributes": {"used": {"value": false, "score": 0.0005603432655334473}, "created": {"value": false, "score": 0.0005784034729003906}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9990035891532898}, "created": {"value": false, "score": 0.32181453704833984}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "references": [{"label": "(Panayotov et al., 2015)", "normalizedForm": "Panayotov et al., 2015", "refKey": 41}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "CrowdMOS", "normalizedForm": "CrowdMOS", "offsetStart": 4, "offsetEnd": 12}, "context": "The CrowdMOS package (Ribeiro et al., 2011) was used in all subjective experiments with the recommended recipes for outliers removal. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9999498128890991}, "created": {"value": false, "score": 7.271766662597656e-06}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999498128890991}, "created": {"value": false, "score": 7.271766662597656e-06}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "references": [{"label": "(Ribeiro et al., 2011)", "normalizedForm": "Ribeiro et al., 2011", "refKey": 51, "offsetStart": 23494, "offsetEnd": 23516}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "HuBERT", "normalizedForm": "HuBERT", "offsetStart": 6, "offsetEnd": 12}, "context": "Since HuBERT outputs continuous representations, an additional kmeans step is needed in order to quantize these representations into a discrete unit sequence denoted by z c = (z 1 c , . . . ", "mentionContextAttributes": {"used": {"value": false, "score": 0.01874256134033203}, "created": {"value": false, "score": 3.0994415283203125e-06}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999951124191284}, "created": {"value": false, "score": 0.0972023606300354}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "references": [{"label": "(Hsu et al., 2021)", "normalizedForm": "Hsu et al., 2021", "refKey": 17}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Tacotron", "normalizedForm": "Tacotron", "offsetStart": 6, "offsetEnd": 14}, "version": {"rawForm": "2                    . 1", "normalizedForm": "2 . 1"}, "context": "While Tacotron2 and Seq2seq-EVC succeed in conveying the target emotion, they produce less natural expressive speech utterances, which is reflected in lower MOS.", "mentionContextAttributes": {"used": {"value": false, "score": 0.0005674958229064941}, "created": {"value": false, "score": 1.5497207641601562e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9988890290260315}, "created": {"value": true, "score": 0.6764569878578186}, "shared": {"value": false, "score": 4.76837158203125e-07}}, "references": [{"label": "(Shen et al., 2018)", "normalizedForm": "Shen et al., 2018", "refKey": 58}, {"label": "(Shen et al., 2018)", "normalizedForm": "Shen et al., 2018", "refKey": 58}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "HuBERT", "normalizedForm": "HuBERT", "offsetStart": 7, "offsetEnd": 13}, "context": "We use HuBERT for the phonetic-content units as it was shown to better disentangle between speech content and both speaker and prosody compared to other SSL-based models (Polyak et al., 2021).", "mentionContextAttributes": {"used": {"value": true, "score": 0.9784975647926331}, "created": {"value": false, "score": 0.00010287761688232422}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999951124191284}, "created": {"value": false, "score": 0.0972023606300354}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "references": [{"label": "(Hsu et al., 2021)", "normalizedForm": "Hsu et al., 2021", "refKey": 17}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Tacotron", "normalizedForm": "Tacotron", "offsetStart": 13, "offsetEnd": 21}, "version": {"rawForm": "2", "normalizedForm": "2", "offsetStart": 21, "offsetEnd": 22}, "context": "The input to Tacotron2 is the ground-truth text representing the speech content. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9174422025680542}, "created": {"value": false, "score": 0.00048291683197021484}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9988890290260315}, "created": {"value": true, "score": 0.6764569878578186}, "shared": {"value": false, "score": 4.76837158203125e-07}}, "references": [{"label": "(Shen et al., 2018)", "normalizedForm": "Shen et al., 2018", "refKey": 58}, {"label": "(Shen et al., 2018)", "normalizedForm": "Shen et al., 2018", "refKey": 58}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Tacotron", "normalizedForm": "Tacotron", "offsetStart": 14, "offsetEnd": 22}, "version": {"rawForm": "2                    . 1", "normalizedForm": "2 . 1"}, "context": "We modify the Tacotron2 architecture by adding a Global-Style-Token (Skerry-Ryan et al., 2018b) to control for the target emotion.", "mentionContextAttributes": {"used": {"value": false, "score": 0.008894860744476318}, "created": {"value": true, "score": 0.6764569878578186}, "shared": {"value": false, "score": 4.76837158203125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9988890290260315}, "created": {"value": true, "score": 0.6764569878578186}, "shared": {"value": false, "score": 4.76837158203125e-07}}, "references": [{"label": "(Shen et al., 2018)", "normalizedForm": "Shen et al., 2018", "refKey": 58}, {"label": "(Shen et al., 2018)", "normalizedForm": "Shen et al., 2018", "refKey": 58}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Tacotron", "normalizedForm": "Tacotron", "offsetStart": 16, "offsetEnd": 24}, "version": {"rawForm": "2                    . 1", "normalizedForm": "2 . 1"}, "context": "Seq2seq-EVC and Tacotron2 were first pre-trained on VCTK (Yamagishi et al., 2019).", "mentionContextAttributes": {"used": {"value": true, "score": 0.9988890290260315}, "created": {"value": false, "score": 1.2874603271484375e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9988890290260315}, "created": {"value": true, "score": 0.6764569878578186}, "shared": {"value": false, "score": 4.76837158203125e-07}}, "references": [{"label": "(Shen et al., 2018)", "normalizedForm": "Shen et al., 2018", "refKey": 58}, {"label": "(Shen et al., 2018)", "normalizedForm": "Shen et al., 2018", "refKey": 58}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Tacotron", "normalizedForm": "Tacotron", "offsetStart": 24, "offsetEnd": 32}, "version": {"rawForm": "2                    . 1", "normalizedForm": "2 . 1"}, "context": "Unlike our method, both Tacotron2 and Seq2seq-EVC are text-based systems hence they attempt to learn an alignment (an attention map) between text inputs and audio targets.", "mentionContextAttributes": {"used": {"value": false, "score": 3.4809112548828125e-05}, "created": {"value": false, "score": 0.06637871265411377}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9988890290260315}, "created": {"value": true, "score": 0.6764569878578186}, "shared": {"value": false, "score": 4.76837158203125e-07}}, "references": [{"label": "(Shen et al., 2018)", "normalizedForm": "Shen et al., 2018", "refKey": 58}, {"label": "(Shen et al., 2018)", "normalizedForm": "Shen et al., 2018", "refKey": 58}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "HuBERT", "normalizedForm": "HuBERT", "offsetStart": 29, "offsetEnd": 35}, "context": "The content encoder E c is a HuBERT model (Hsu et al., 2021) pre-trained on the LibriSpeech corpus (Panayotov et al., 2015).", "mentionContextAttributes": {"used": {"value": true, "score": 0.975818932056427}, "created": {"value": false, "score": 1.990795135498047e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999951124191284}, "created": {"value": false, "score": 0.0972023606300354}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "references": [{"label": "(Hsu et al., 2021)", "normalizedForm": "Hsu et al., 2021", "refKey": 17, "offsetStart": 11227, "offsetEnd": 11245}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "LibriSpeech", "normalizedForm": "LibriSpeech", "offsetStart": 34, "offsetEnd": 45}, "context": "For pre-training, we use a mix of LibriSpeech, Blizzard2013 (Chalamandaris et al., 2013) and EmoV (Adigwe et al., 2018), and stop after 3M update steps.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9717690944671631}, "created": {"value": false, "score": 1.811981201171875e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9990035891532898}, "created": {"value": false, "score": 0.32181453704833984}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "references": [{"label": "(Panayotov et al., 2015)", "normalizedForm": "Panayotov et al., 2015", "refKey": 41}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "HuBERT", "normalizedForm": "HuBERT", "offsetStart": 43, "offsetEnd": 49}, "context": "Yang et al. (2021) found that intermediate HuBERT representations obtained from different layers have an impact on the downstream task at hand.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9997318387031555}, "created": {"value": false, "score": 9.059906005859375e-06}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999951124191284}, "created": {"value": false, "score": 0.0972023606300354}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "references": [{"label": "(Hsu et al., 2021)", "normalizedForm": "Hsu et al., 2021", "refKey": 17}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "HuBERT", "normalizedForm": "HuBERT", "offsetStart": 52, "offsetEnd": 58}, "context": "We extracted representations from the 9 th layer of HuBERT model and set k = 200. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9999951124191284}, "created": {"value": false, "score": 7.271766662597656e-06}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999951124191284}, "created": {"value": false, "score": 0.0972023606300354}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "references": [{"label": "(Hsu et al., 2021)", "normalizedForm": "Hsu et al., 2021", "refKey": 17}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "HuBERT", "normalizedForm": "HuBERT", "offsetStart": 52, "offsetEnd": 58}, "context": "For emotion conversion, we find that using the 9 th HuBERT layer and 200 tokens performs best.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9534863829612732}, "created": {"value": false, "score": 5.841255187988281e-06}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999951124191284}, "created": {"value": false, "score": 0.0972023606300354}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "references": [{"label": "(Hsu et al., 2021)", "normalizedForm": "Hsu et al., 2021", "refKey": 17}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "LibriSpeech", "normalizedForm": "LibriSpeech", "offsetStart": 58, "offsetEnd": 69}, "context": "To that end, we input our system with recordings from the LibriSpeech dataset. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9990035891532898}, "created": {"value": false, "score": 0.32181453704833984}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9990035891532898}, "created": {"value": false, "score": 0.32181453704833984}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "references": [{"label": "(Panayotov et al., 2015)", "normalizedForm": "Panayotov et al., 2015", "refKey": 41}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "LibriSpeech", "normalizedForm": "LibriSpeech", "offsetStart": 61, "offsetEnd": 72}, "context": "We use a BASE wav2vec 2.0 phoneme detection model trained on LibriSpeech-960h with CTC loss from scratch.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9975894689559937}, "created": {"value": false, "score": 1.7881393432617188e-06}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9990035891532898}, "created": {"value": false, "score": 0.32181453704833984}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "references": [{"label": "(Panayotov et al., 2015)", "normalizedForm": "Panayotov et al., 2015", "refKey": 41}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "LibriSpeech", "normalizedForm": "LibriSpeech", "offsetStart": 61, "offsetEnd": 72}, "context": "We use a BASE wav2vec 2.0 phoneme detection model trained on LibriSpeech-960h with CTC loss from scratch.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9975894689559937}, "created": {"value": false, "score": 1.7881393432617188e-06}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9990035891532898}, "created": {"value": false, "score": 0.32181453704833984}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "references": [{"label": "(Panayotov et al., 2015)", "normalizedForm": "Panayotov et al., 2015", "refKey": 41}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Tacotron", "normalizedForm": "Tacotron", "offsetStart": 68, "offsetEnd": 76}, "version": {"rawForm": "2                    . 1", "normalizedForm": "2 . 1", "offsetStart": 76, "offsetEnd": 101}, "context": "We also evaluate an expressive Text-to-Speech (TTS) system based on Tacotron2 (Shen et al., 2018). 1  For the text-based approach we use the Tacotron2 and Seq2seq-EVC models. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.794068455696106}, "created": {"value": false, "score": 0.0043604373931884766}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9988890290260315}, "created": {"value": true, "score": 0.6764569878578186}, "shared": {"value": false, "score": 4.76837158203125e-07}}, "references": [{"label": "(Shen et al., 2018)", "normalizedForm": "Shen et al., 2018", "refKey": 58, "offsetStart": 22053, "offsetEnd": 22072}, {"label": "(Shen et al., 2018)", "normalizedForm": "Shen et al., 2018", "refKey": 58, "offsetStart": 22053, "offsetEnd": 22072}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "LibriSpeech", "normalizedForm": "LibriSpeech", "offsetStart": 80, "offsetEnd": 91}, "context": "The content encoder E c is a HuBERT model (Hsu et al., 2021) pre-trained on the LibriSpeech corpus (Panayotov et al., 2015).", "mentionContextAttributes": {"used": {"value": true, "score": 0.975818932056427}, "created": {"value": false, "score": 1.990795135498047e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9990035891532898}, "created": {"value": false, "score": 0.32181453704833984}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "references": [{"label": "(Panayotov et al., 2015)", "normalizedForm": "Panayotov et al., 2015", "refKey": 41, "offsetStart": 11284, "offsetEnd": 11308}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "HuBERT", "normalizedForm": "HuBERT", "offsetStart": 118, "offsetEnd": 124}, "context": "To better understand the effect of these architectural configurations in our setting, we experimented with extracting HuBERT features from layers 6 and 9, using 100 and 200 clusters for the k-means postprocessing step. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9996381998062134}, "created": {"value": false, "score": 0.0972023606300354}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999951124191284}, "created": {"value": false, "score": 0.0972023606300354}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "references": [{"label": "(Hsu et al., 2021)", "normalizedForm": "Hsu et al., 2021", "refKey": 17}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "HuBERT", "normalizedForm": "HuBERT", "offsetStart": 132, "offsetEnd": 138}, "context": "To represent speech phonetic-content we extract a discrete representation of the audio signal using a pre-trained SSL model, namely HuBERT. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9923691749572754}, "created": {"value": false, "score": 0.00028842687606811523}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999951124191284}, "created": {"value": false, "score": 0.0972023606300354}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "references": [{"label": "(Hsu et al., 2021)", "normalizedForm": "Hsu et al., 2021", "refKey": 17}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Tacotron", "normalizedForm": "Tacotron", "offsetStart": 141, "offsetEnd": 149}, "version": {"rawForm": "2                    . 1", "normalizedForm": "2 . 1"}, "context": "We also evaluate an expressive Text-to-Speech (TTS) system based on Tacotron2 (Shen et al., 2018). 1  For the text-based approach we use the Tacotron2 and Seq2seq-EVC models.", "mentionContextAttributes": {"used": {"value": true, "score": 0.794068455696106}, "created": {"value": false, "score": 0.0043604373931884766}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9988890290260315}, "created": {"value": true, "score": 0.6764569878578186}, "shared": {"value": false, "score": 4.76837158203125e-07}}, "references": [{"label": "(Shen et al., 2018)", "normalizedForm": "Shen et al., 2018", "refKey": 58}, {"label": "(Shen et al., 2018)", "normalizedForm": "Shen et al., 2018", "refKey": 58}]}], "references": [{"refKey": 17, "tei": "<biblStruct xml:id=\"b17\">\n\t<analytic>\n\t\t<title level=\"a\" type=\"main\">Hubert: How Much Can a Bad Teacher Benefit ASR Pre-Training?</title>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Wei-Ning</forename><surname>Hsu</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Yao-Hung Hubert</forename><surname>Tsai</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Benjamin</forename><surname>Bolte</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Ruslan</forename><surname>Salakhutdinov</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Abdelrahman</forename><surname>Mohamed</surname></persName>\n\t\t</author>\n\t\t<idno type=\"DOI\">10.1109/icassp39728.2021.9414460</idno>\n\t</analytic>\n\t<monogr>\n\t\t<title level=\"m\">ICASSP 2021 - 2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>\n\t\t<imprint>\n\t\t\t<publisher>IEEE</publisher>\n\t\t\t<date type=\"published\" when=\"2021-06-06\">2021</date>\n\t\t\t<biblScope unit=\"page\">6537</biblScope>\n\t\t</imprint>\n\t</monogr>\n</biblStruct>\n"}, {"refKey": 41, "tei": "<biblStruct xml:id=\"b41\">\n\t<analytic>\n\t\t<title level=\"a\" type=\"main\">Librispeech: An ASR corpus based on public domain audio books</title>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Vassil</forename><surname>Panayotov</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Guoguo</forename><surname>Chen</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Daniel</forename><surname>Povey</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Sanjeev</forename><surname>Khudanpur</surname></persName>\n\t\t</author>\n\t\t<idno type=\"DOI\">10.1109/icassp.2015.7178964</idno>\n\t</analytic>\n\t<monogr>\n\t\t<title level=\"m\">2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>\n\t\t<imprint>\n\t\t\t<publisher>IEEE</publisher>\n\t\t\t<date type=\"published\" when=\"2015-04\">2015</date>\n\t\t\t<biblScope unit=\"page\">5210</biblScope>\n\t\t</imprint>\n\t</monogr>\n</biblStruct>\n"}, {"refKey": 51, "tei": "<biblStruct xml:id=\"b51\">\n\t<analytic>\n\t\t<title level=\"a\" type=\"main\">CROWDMOS: An approach for crowdsourcing mean opinion score studies</title>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Flavio</forename><surname>Ribeiro</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Dinei</forename><surname>Florencio</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Cha</forename><surname>Zhang</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Michael</forename><surname>Seltzer</surname></persName>\n\t\t</author>\n\t\t<idno type=\"DOI\">10.1109/icassp.2011.5946971</idno>\n\t</analytic>\n\t<monogr>\n\t\t<title level=\"m\">2011 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>\n\t\t<imprint>\n\t\t\t<publisher>IEEE</publisher>\n\t\t\t<date type=\"published\" when=\"2011-05\">2011</date>\n\t\t\t<biblScope unit=\"page\" from=\"2416\" to=\"2419\" />\n\t\t</imprint>\n\t</monogr>\n</biblStruct>\n"}, {"refKey": 58, "tei": "<biblStruct xml:id=\"b58\">\n\t<analytic>\n\t\t<title level=\"a\" type=\"main\">Natural TTS Synthesis by Conditioning Wavenet on MEL Spectrogram Predictions</title>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Jonathan</forename><surname>Shen</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Ruoming</forename><surname>Pang</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Ron</forename><forename type=\"middle\">J</forename><surname>Weiss</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Mike</forename><surname>Schuster</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Navdeep</forename><surname>Jaitly</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Zongheng</forename><surname>Yang</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Zhifeng</forename><surname>Chen</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Yu</forename><surname>Zhang</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Yuxuan</forename><surname>Wang</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Rj</forename><surname>Skerrv-Ryan</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Rif</forename><forename type=\"middle\">A</forename><surname>Saurous</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Yannis</forename><surname>Agiomvrgiannakis</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Yonghui</forename><surname>Wu</surname></persName>\n\t\t</author>\n\t\t<idno type=\"DOI\">10.1109/icassp.2018.8461368</idno>\n\t</analytic>\n\t<monogr>\n\t\t<title level=\"m\">2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>\n\t\t<imprint>\n\t\t\t<publisher>IEEE</publisher>\n\t\t\t<date type=\"published\" when=\"2018-04\">2018</date>\n\t\t\t<biblScope unit=\"page\">4783</biblScope>\n\t\t</imprint>\n\t</monogr>\n</biblStruct>\n"}], "runtime": 16837, "id": "fee8eaa9ff3b672d1239e6ac381e303e8298978d", "metadata": {"id": "fee8eaa9ff3b672d1239e6ac381e303e8298978d"}, "original_file_path": "../../datalake/Samuel/SOFTware-Sync/downloads/xml/hal-03831801.grobid.tei.xml", "file_name": "hal-03831801.grobid.tei.xml"}