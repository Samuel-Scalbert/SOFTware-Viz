{"application": "software-mentions", "version": "0.8.0", "date": "2024-10-07T11:55+0000", "md5": "AEE9B463F1550C6832E36A597B7E48F8", "mentions": [{"type": "software", "software-type": "software", "software-name": {"rawForm": "CamemBERT", "normalizedForm": "CamemBERT", "offsetStart": 0, "offsetEnd": 9}, "context": "CamemBERT was trained on a freely available, automatically web-crawled corpus called OSCAR (Ortiz Su\u00e1rez et al., 2019;Ortiz Su\u00e1rez et al., 2020), while FlauBERT was trained on a mix of webcrawled data and manually curated (some of which is not freely available) contemporary French corpora.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9874798655509949}, "created": {"value": false, "score": 0.00017470121383666992}, "shared": {"value": false, "score": 3.6954879760742188e-06}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9997442364692688}, "created": {"value": false, "score": 0.00027507543563842773}, "shared": {"value": false, "score": 3.6954879760742188e-06}}, "references": [{"label": "(Martin et al., 2020)", "normalizedForm": "Martin et al., 2020", "refKey": 22}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "AlemBERT", "normalizedForm": "AlemBERT", "offsetStart": 7, "offsetEnd": 15}, "context": "With D'AlemBERT, we showed that it is possible to successfully train a transformer-based language model for historical French with even less data than originally shown in previous works (Martin et al., 2020).", "mentionContextAttributes": {"used": {"value": false, "score": 0.0005435347557067871}, "created": {"value": true, "score": 0.9801771640777588}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9998594522476196}, "created": {"value": true, "score": 0.9998880624771118}, "shared": {"value": false, "score": 8.821487426757812e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "CamemBERT", "normalizedForm": "CamemBERT", "offsetStart": 7, "offsetEnd": 16}, "context": "As for CamemBERT, we can see that it consistently scores lower than both D'AlemBERT and Pie Extended.", "mentionContextAttributes": {"used": {"value": false, "score": 0.05376839637756348}, "created": {"value": false, "score": 3.528594970703125e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9997442364692688}, "created": {"value": false, "score": 0.00027507543563842773}, "shared": {"value": false, "score": 3.6954879760742188e-06}}, "references": [{"label": "(Martin et al., 2020)", "normalizedForm": "Martin et al., 2020", "refKey": 22}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "RoBERTa", "normalizedForm": "RoBERTa", "offsetStart": 11, "offsetEnd": 18}, "context": "We use the RoBERTa implementation in the Zelda Rose library, 3 and again, in the same way as Liu et al. (2019) our learning rate is warmed up for 10k steps up to a peak value of 0.0003 instead of the original 0.0001 used by the original implementation of RoBERTa (Liu et al., 2019), as our model diverged with the 0.0001 value. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9965435862541199}, "created": {"value": false, "score": 5.4836273193359375e-06}, "shared": {"value": false, "score": 1.0728836059570312e-06}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9965435862541199}, "created": {"value": true, "score": 0.9998880624771118}, "shared": {"value": false, "score": 1.0728836059570312e-06}}, "references": [{"label": "(Liu et al., 2019)", "normalizedForm": "Liu et al., 2019", "refKey": 20}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "RoBERTa", "normalizedForm": "RoBERTa", "offsetStart": 11, "offsetEnd": 18}, "context": "Similar to RoBERTa (Liu et al., 2019) we segment the input text data into subword units using Byte-Pair encoding (BPE) (Sennrich et al., 2016) in the implementation proposed by (Radford et al., 2019) that uses bytes instead of unicode characters as the base subword units.", "mentionContextAttributes": {"used": {"value": false, "score": 0.0011668801307678223}, "created": {"value": false, "score": 0.0015027523040771484}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9965435862541199}, "created": {"value": true, "score": 0.9998880624771118}, "shared": {"value": false, "score": 1.0728836059570312e-06}}, "references": [{"label": "(Liu et al., 2019)", "normalizedForm": "Liu et al., 2019", "refKey": 20, "offsetStart": 11582, "offsetEnd": 11600}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "AlemBERT", "normalizedForm": "AlemBERT", "offsetStart": 13, "offsetEnd": 21}, "context": "We release D'AlemBERT and the open-sourced subpart of the FREEMmax corpus.", "mentionContextAttributes": {"used": {"value": false, "score": 0.33711904287338257}, "created": {"value": true, "score": 0.9942939877510071}, "shared": {"value": false, "score": 8.821487426757812e-06}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9998594522476196}, "created": {"value": true, "score": 0.9998880624771118}, "shared": {"value": false, "score": 8.821487426757812e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "AlemBERT", "normalizedForm": "AlemBERT", "offsetStart": 14, "offsetEnd": 22}, "context": "Transformer D'AlemBERT uses the exact same architecture as RoBERTa, which is a multi-layer bidirectional Transformer (Vaswani et al., 2017)  Pretraining Objective We train our model on the Masked Language Modelling (MLM) task as proposed by RoBERTa's authors (Liu et al., 2019): given an input text sequence composed of N tokens x 1 , ..., x N , we select 15% of tokens for possible replacement.", "mentionContextAttributes": {"used": {"value": false, "score": 0.06264525651931763}, "created": {"value": false, "score": 0.00012493133544921875}, "shared": {"value": false, "score": 9.5367431640625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9998594522476196}, "created": {"value": true, "score": 0.9998880624771118}, "shared": {"value": false, "score": 8.821487426757812e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "AlemBERT", "normalizedForm": "AlemBERT", "offsetStart": 15, "offsetEnd": 23}, "context": "To fine-tune D'AlemBERT for POS tagging, we follow the same approach as Schweter and Akbik (2020) with some modifications: we append a linear layer of size 256 that takes as input the last hidden representation of the <s> special token and the mean of the last hidden representation of the subword units of each token (token as defined for FREEM LPM ), that is, we use a \"mean\" subword pooling strategy. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.7965133190155029}, "created": {"value": false, "score": 4.172325134277344e-06}, "shared": {"value": false, "score": 7.152557373046875e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9998594522476196}, "created": {"value": true, "score": 0.9998880624771118}, "shared": {"value": false, "score": 8.821487426757812e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "AlemBERT", "normalizedForm": "AlemBERT", "offsetStart": 15, "offsetEnd": 23}, "context": "We fine-tune D'AlemBERT with a learning rate of 0.000005 for a total of 10 epochs. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.6727880239486694}, "created": {"value": false, "score": 0.0002993345260620117}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9998594522476196}, "created": {"value": true, "score": 0.9998880624771118}, "shared": {"value": false, "score": 8.821487426757812e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "AlemBERT", "normalizedForm": "AlemBERT", "offsetStart": 18, "offsetEnd": 26}, "context": "We can see that D'AlemBERT consistently outperforms Pie Extended and CamemBERT in both the normalised and original versions of our out-of-domain testing data and for all different periods by a considerable margin. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.03225970268249512}, "created": {"value": false, "score": 1.3709068298339844e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9998594522476196}, "created": {"value": true, "score": 0.9998880624771118}, "shared": {"value": false, "score": 8.821487426757812e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "CamemBERT", "normalizedForm": "CamemBERT", "offsetStart": 18, "offsetEnd": 27}, "context": "We also fine-tune CamemBERT using the exact same hyperparameters as the ones we use for D'AlemBERT. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9179113507270813}, "created": {"value": false, "score": 1.049041748046875e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9997442364692688}, "created": {"value": false, "score": 0.00027507543563842773}, "shared": {"value": false, "score": 3.6954879760742188e-06}}, "references": [{"label": "(Martin et al., 2020)", "normalizedForm": "Martin et al., 2020", "refKey": 22}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "RoBERTa", "normalizedForm": "RoBERTa", "offsetStart": 21, "offsetEnd": 28}, "context": "Again, following the RoBERTa approach, we dynamically mask tokens instead of fixing them statically for the whole dataset during preprocessing.", "mentionContextAttributes": {"used": {"value": false, "score": 0.0012103915214538574}, "created": {"value": false, "score": 0.0022445321083068848}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9965435862541199}, "created": {"value": true, "score": 0.9998880624771118}, "shared": {"value": false, "score": 1.0728836059570312e-06}}, "references": [{"label": "(Liu et al., 2019)", "normalizedForm": "Liu et al., 2019", "refKey": 20}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "AlemBERT", "normalizedForm": "AlemBERT", "offsetStart": 21, "offsetEnd": 29}, "context": "This suggests that D'AlemBERT can generalise more effectively to non-normalised data than the more traditional architecture used by Pie Extended. ", "mentionContextAttributes": {"used": {"value": false, "score": 4.839897155761719e-05}, "created": {"value": false, "score": 0.00021845102310180664}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9998594522476196}, "created": {"value": true, "score": 0.9998880624771118}, "shared": {"value": false, "score": 8.821487426757812e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "AlemBERT", "normalizedForm": "AlemBERT", "offsetStart": 22, "offsetEnd": 30}, "context": "We release both the D'AlemBERT model and a subset of the FREEM max dataset that we were allowed by the original authors to open-source .", "mentionContextAttributes": {"used": {"value": false, "score": 0.024114787578582764}, "created": {"value": true, "score": 0.5764997601509094}, "shared": {"value": false, "score": 4.76837158203125e-06}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9998594522476196}, "created": {"value": true, "score": 0.9998880624771118}, "shared": {"value": false, "score": 8.821487426757812e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "AlemBERT", "normalizedForm": "AlemBERT", "offsetStart": 27, "offsetEnd": 35}, "context": "In order to evaluate our D'AlemBERT model, we finetune it for POS tagging on the FREEM LPM corpus.", "mentionContextAttributes": {"used": {"value": false, "score": 0.12263298034667969}, "created": {"value": true, "score": 0.5598669052124023}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9998594522476196}, "created": {"value": true, "score": 0.9998880624771118}, "shared": {"value": false, "score": 8.821487426757812e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "AlemBERT", "normalizedForm": "AlemBERT", "offsetStart": 32, "offsetEnd": 40}, "context": "We evaluate the usefulness of D'AlemBERT by fine-tuning it on a part-of-speech tagging task, outperforming previous work on the test set.", "mentionContextAttributes": {"used": {"value": false, "score": 0.13155311346054077}, "created": {"value": false, "score": 0.00030481815338134766}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9998594522476196}, "created": {"value": true, "score": 0.9998880624771118}, "shared": {"value": false, "score": 8.821487426757812e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "AlemBERT", "normalizedForm": "AlemBERT", "offsetStart": 37, "offsetEnd": 45}, "context": "This result actually suggests that D'AlemBERT might be able to do effective transfer learning from the 18 th c., 19 th c. and 20 th c. data to the 16 th c. and 17 th c. data.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9970141649246216}, "created": {"value": false, "score": 8.106231689453125e-06}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9998594522476196}, "created": {"value": true, "score": 0.9998880624771118}, "shared": {"value": false, "score": 8.821487426757812e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Zelda Rose library", "normalizedForm": "Zelda Rose library", "offsetStart": 41, "offsetEnd": 59}, "context": "We use the RoBERTa implementation in the Zelda Rose library, 3 and again, in the same way as Liu et al. (2019) our learning rate is warmed up for 10k steps up to a peak value of 0.0003 instead of the original 0.0001 used by the original implementation of RoBERTa (Liu et al., 2019), as our model diverged with the 0.0001 value. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9965435862541199}, "created": {"value": false, "score": 5.4836273193359375e-06}, "shared": {"value": false, "score": 1.0728836059570312e-06}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9965435862541199}, "created": {"value": false, "score": 5.4836273193359375e-06}, "shared": {"value": false, "score": 1.0728836059570312e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "AlemBERT", "normalizedForm": "AlemBERT", "offsetStart": 53, "offsetEnd": 61}, "context": "These results also show the impressive capacity of D'AlemBERT to quickly generalise to diverse set of states of language, as well as its capacity to transfer knowledge from the FREEM max corpus into this task.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9998594522476196}, "created": {"value": false, "score": 0.0009872913360595703}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9998594522476196}, "created": {"value": true, "score": 0.9998880624771118}, "shared": {"value": false, "score": 8.821487426757812e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "AlemBERT", "normalizedForm": "AlemBERT", "offsetStart": 54, "offsetEnd": 62}, "context": "The following paper will address the development of D'AlemBERT, a neural language model in a complex setting, defined here as the state of language with scarce heterogeneous resources.", "mentionContextAttributes": {"used": {"value": false, "score": 9.548664093017578e-05}, "created": {"value": true, "score": 0.9997172951698303}, "shared": {"value": false, "score": 4.76837158203125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9998594522476196}, "created": {"value": true, "score": 0.9998880624771118}, "shared": {"value": false, "score": 8.821487426757812e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "RoBERTa", "normalizedForm": "RoBERTa", "offsetStart": 59, "offsetEnd": 66}, "context": "Transformer D'AlemBERT uses the exact same architecture as RoBERTa, which is a multi-layer bidirectional Transformer (Vaswani et al., 2017)  Pretraining Objective We train our model on the Masked Language Modelling (MLM) task as proposed by RoBERTa's authors (Liu et al., 2019): given an input text sequence composed of N tokens x 1 , ..., x N , we select 15% of tokens for possible replacement.", "mentionContextAttributes": {"used": {"value": false, "score": 0.06264525651931763}, "created": {"value": false, "score": 0.00012493133544921875}, "shared": {"value": false, "score": 9.5367431640625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9965435862541199}, "created": {"value": true, "score": 0.9998880624771118}, "shared": {"value": false, "score": 1.0728836059570312e-06}}, "references": [{"label": "(Liu et al., 2019)", "normalizedForm": "Liu et al., 2019", "refKey": 20}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "AlemBERT", "normalizedForm": "AlemBERT", "offsetStart": 60, "offsetEnd": 68}, "context": "We present the FREEMmax corpus of Early Modern French and D'AlemBERT, a RoBERTa-based language model trained on FREEMmax.", "mentionContextAttributes": {"used": {"value": false, "score": 0.0006742477416992188}, "created": {"value": true, "score": 0.9998880624771118}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9998594522476196}, "created": {"value": true, "score": 0.9998880624771118}, "shared": {"value": false, "score": 8.821487426757812e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "AlemBERT", "normalizedForm": "AlemBERT", "offsetStart": 67, "offsetEnd": 75}, "context": "We can also see that on average the differ-ence in score between D'AlemBERT and Pie Extended is greater for the original split than the normalised one.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9121402502059937}, "created": {"value": false, "score": 7.3909759521484375e-06}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9998594522476196}, "created": {"value": true, "score": 0.9998880624771118}, "shared": {"value": false, "score": 8.821487426757812e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "CamemBERT", "normalizedForm": "CamemBERT", "offsetStart": 69, "offsetEnd": 78}, "context": "We can see that D'AlemBERT consistently outperforms Pie Extended and CamemBERT in both the normalised and original versions of our out-of-domain testing data and for all different periods by a considerable margin. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.03225970268249512}, "created": {"value": false, "score": 1.3709068298339844e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9997442364692688}, "created": {"value": false, "score": 0.00027507543563842773}, "shared": {"value": false, "score": 3.6954879760742188e-06}}, "references": [{"label": "(Martin et al., 2020)", "normalizedForm": "Martin et al., 2020", "refKey": 22}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "RoBERTa", "normalizedForm": "RoBERTa", "offsetStart": 72, "offsetEnd": 79}, "context": "We present the FREEMmax corpus of Early Modern French and D'AlemBERT, a RoBERTa-based language model trained on FREEMmax.", "mentionContextAttributes": {"used": {"value": false, "score": 0.0006742477416992188}, "created": {"value": true, "score": 0.9998880624771118}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9965435862541199}, "created": {"value": true, "score": 0.9998880624771118}, "shared": {"value": false, "score": 1.0728836059570312e-06}}, "references": [{"label": "(Liu et al., 2019)", "normalizedForm": "Liu et al., 2019", "refKey": 20}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "AlemBERT", "normalizedForm": "AlemBERT", "offsetStart": 75, "offsetEnd": 83}, "context": "As for CamemBERT, we can see that it consistently scores lower than both D'AlemBERT and Pie Extended.", "mentionContextAttributes": {"used": {"value": false, "score": 0.05376839637756348}, "created": {"value": false, "score": 3.528594970703125e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9998594522476196}, "created": {"value": true, "score": 0.9998880624771118}, "shared": {"value": false, "score": 8.821487426757812e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "AlemBERT", "normalizedForm": "AlemBERT", "offsetStart": 81, "offsetEnd": 89}, "context": "For our future work, we hope that will be able to study the application of our D'AlemBERT model to other NLP tasks such as text normalisation, named entity recognition and even document structuring, where we hope to more extensively study the transfer learning capabilities of our approach.", "mentionContextAttributes": {"used": {"value": false, "score": 0.0007476210594177246}, "created": {"value": true, "score": 0.999581515789032}, "shared": {"value": false, "score": 1.9073486328125e-06}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9998594522476196}, "created": {"value": true, "score": 0.9998880624771118}, "shared": {"value": false, "score": 8.821487426757812e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "AlemBERT", "normalizedForm": "AlemBERT", "offsetStart": 84, "offsetEnd": 92}, "context": "We report the energy consumption and carbon emissions of both the pre-training of D'AlemBERT and its evaluation.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9998465776443481}, "created": {"value": false, "score": 0.0002613663673400879}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9998594522476196}, "created": {"value": true, "score": 0.9998880624771118}, "shared": {"value": false, "score": 8.821487426757812e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "AlemBERT", "normalizedForm": "AlemBERT", "offsetStart": 90, "offsetEnd": 98}, "context": "We also fine-tune CamemBERT using the exact same hyperparameters as the ones we use for D'AlemBERT. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9179113507270813}, "created": {"value": false, "score": 1.049041748046875e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9998594522476196}, "created": {"value": true, "score": 0.9998880624771118}, "shared": {"value": false, "score": 8.821487426757812e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "CamemBERT", "normalizedForm": "CamemBERT", "offsetStart": 96, "offsetEnd": 105}, "context": "Concerning language modelling in French, two main models are available for contemporary French, CamemBERT (Martin et al., 2020) and FlauBERT (Le et al., 2020).", "mentionContextAttributes": {"used": {"value": false, "score": 4.124641418457031e-05}, "created": {"value": false, "score": 0.00027507543563842773}, "shared": {"value": false, "score": 4.76837158203125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9997442364692688}, "created": {"value": false, "score": 0.00027507543563842773}, "shared": {"value": false, "score": 3.6954879760742188e-06}}, "references": [{"label": "(Martin et al., 2020)", "normalizedForm": "Martin et al., 2020", "refKey": 22, "offsetStart": 5165, "offsetEnd": 5186}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "AlemBERT", "normalizedForm": "AlemBERT", "offsetStart": 101, "offsetEnd": 109}, "context": "In this paper we presented the manually curated FREEM max corpus of Early Modern French as well as D'AlemBERT, a RoBERTa-based language model trained on FREEM max .", "mentionContextAttributes": {"used": {"value": false, "score": 0.00031173229217529297}, "created": {"value": true, "score": 0.9996016621589661}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9998594522476196}, "created": {"value": true, "score": 0.9998880624771118}, "shared": {"value": false, "score": 8.821487426757812e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "RoBERTa", "normalizedForm": "RoBERTa", "offsetStart": 106, "offsetEnd": 113}, "context": "We hypothesise that this is either due to the smaller size of FREEM max (compared to the corpora used for RoBERTa or CamemBERT) or to our large batch size. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.897496223449707}, "created": {"value": false, "score": 0.00022369623184204102}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9965435862541199}, "created": {"value": true, "score": 0.9998880624771118}, "shared": {"value": false, "score": 1.0728836059570312e-06}}, "references": [{"label": "(Liu et al., 2019)", "normalizedForm": "Liu et al., 2019", "refKey": 20}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "RoBERTa", "normalizedForm": "RoBERTa", "offsetStart": 113, "offsetEnd": 120}, "context": "In this paper we presented the manually curated FREEM max corpus of Early Modern French as well as D'AlemBERT, a RoBERTa-based language model trained on FREEM max .", "mentionContextAttributes": {"used": {"value": false, "score": 0.00031173229217529297}, "created": {"value": true, "score": 0.9996016621589661}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9965435862541199}, "created": {"value": true, "score": 0.9998880624771118}, "shared": {"value": false, "score": 1.0728836059570312e-06}}, "references": [{"label": "(Liu et al., 2019)", "normalizedForm": "Liu et al., 2019", "refKey": 20}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "CamemBERT", "normalizedForm": "CamemBERT", "offsetStart": 117, "offsetEnd": 126}, "context": "We hypothesise that this is either due to the smaller size of FREEM max (compared to the corpora used for RoBERTa or CamemBERT) or to our large batch size. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.8974972367286682}, "created": {"value": false, "score": 0.00022369623184204102}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9997442364692688}, "created": {"value": false, "score": 0.00027507543563842773}, "shared": {"value": false, "score": 3.6954879760742188e-06}}, "references": [{"label": "(Martin et al., 2020)", "normalizedForm": "Martin et al., 2020", "refKey": 22}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "CamemBERT", "normalizedForm": "CamemBERT", "offsetStart": 117, "offsetEnd": 126}, "context": "We use the scores previously reported by Cl\u00e9rice (2020) using Pie Extended as our baseline as well as the fine-tuned CamemBERT that serves as a second baseline as well as a rough estimation of how much knowledge can D'AlemBERT transfer from the FREEM max into this task. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9957140684127808}, "created": {"value": false, "score": 6.61611557006836e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9997442364692688}, "created": {"value": false, "score": 0.00027507543563842773}, "shared": {"value": false, "score": 3.6954879760742188e-06}}, "references": [{"label": "(Martin et al., 2020)", "normalizedForm": "Martin et al., 2020", "refKey": 22}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "AlemBERT", "normalizedForm": "AlemBERT", "offsetStart": 120, "offsetEnd": 128}, "context": "In this section, we describe the pretraining data, architecture, training objective and optimisation setup we use for D'AlemBERT, our new neural language model for Early Modern French.", "mentionContextAttributes": {"used": {"value": false, "score": 0.0001799464225769043}, "created": {"value": true, "score": 0.9986885190010071}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9998594522476196}, "created": {"value": true, "score": 0.9998880624771118}, "shared": {"value": false, "score": 8.821487426757812e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "CamemBERT", "normalizedForm": "CamemBERT", "offsetStart": 141, "offsetEnd": 150}, "context": "The obtained results are also a testament to the importance of the pretraining data, specially taking in account that the pretraining set of CamemBERT is more than 100 times bigger than that of D'AlemBERT.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9997442364692688}, "created": {"value": false, "score": 5.054473876953125e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9997442364692688}, "created": {"value": false, "score": 0.00027507543563842773}, "shared": {"value": false, "score": 3.6954879760742188e-06}}, "references": [{"label": "(Martin et al., 2020)", "normalizedForm": "Martin et al., 2020", "refKey": 22}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "AlemBERT", "normalizedForm": "AlemBERT", "offsetStart": 196, "offsetEnd": 204}, "context": "The obtained results are also a testament to the importance of the pretraining data, specially taking in account that the pretraining set of CamemBERT is more than 100 times bigger than that of D'AlemBERT.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9997442364692688}, "created": {"value": false, "score": 5.054473876953125e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9998594522476196}, "created": {"value": true, "score": 0.9998880624771118}, "shared": {"value": false, "score": 8.821487426757812e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "AlemBERT", "normalizedForm": "AlemBERT", "offsetStart": 218, "offsetEnd": 226}, "context": "We use the scores previously reported by Cl\u00e9rice (2020) using Pie Extended as our baseline as well as the fine-tuned CamemBERT that serves as a second baseline as well as a rough estimation of how much knowledge can D'AlemBERT transfer from the FREEM max into this task. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9957140684127808}, "created": {"value": false, "score": 6.61611557006836e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9998594522476196}, "created": {"value": true, "score": 0.9998880624771118}, "shared": {"value": false, "score": 8.821487426757812e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "RoBERTa", "normalizedForm": "RoBERTa", "offsetStart": 241, "offsetEnd": 248}, "context": "Transformer D'AlemBERT uses the exact same architecture as RoBERTa, which is a multi-layer bidirectional Transformer (Vaswani et al., 2017)  Pretraining Objective We train our model on the Masked Language Modelling (MLM) task as proposed by RoBERTa's authors (Liu et al., 2019): given an input text sequence composed of N tokens x 1 , ..., x N , we select 15% of tokens for possible replacement.", "mentionContextAttributes": {"used": {"value": false, "score": 0.0626455545425415}, "created": {"value": false, "score": 0.00012493133544921875}, "shared": {"value": false, "score": 9.5367431640625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9965435862541199}, "created": {"value": true, "score": 0.9998880624771118}, "shared": {"value": false, "score": 1.0728836059570312e-06}}, "references": [{"label": "(Liu et al., 2019)", "normalizedForm": "Liu et al., 2019", "refKey": 20}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "RoBERTa", "normalizedForm": "RoBERTa", "offsetStart": 255, "offsetEnd": 280}, "context": "We use the RoBERTa implementation in the Zelda Rose library, 3 and again, in the same way as Liu et al. (2019) our learning rate is warmed up for 10k steps up to a peak value of 0.0003 instead of the original 0.0001 used by the original implementation of RoBERTa (Liu et al., 2019), as our model diverged with the 0.0001 value. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9965435862541199}, "created": {"value": false, "score": 5.4836273193359375e-06}, "shared": {"value": false, "score": 1.0728836059570312e-06}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9965435862541199}, "created": {"value": true, "score": 0.9998880624771118}, "shared": {"value": false, "score": 1.0728836059570312e-06}}, "references": [{"label": "(Liu et al., 2019)", "normalizedForm": "Liu et al., 2019", "refKey": 20}]}], "references": [{"refKey": 22, "tei": "<biblStruct xml:id=\"b22\">\n\t<analytic>\n\t\t<title level=\"a\" type=\"main\">CamemBERT: a Tasty French Language Model</title>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Louis</forename><surname>Martin</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Benjamin</forename><surname>Muller</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Pedro</forename><forename type=\"middle\">Javier</forename><surname>Ortiz Su\u00e1rez</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Yoann</forename><surname>Dupont</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Laurent</forename><surname>Romary</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">\u00c9ric</forename><surname>De La Clergerie</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Djam\u00e9</forename><surname>Seddah</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Beno\u00eet</forename><surname>Sagot</surname></persName>\n\t\t</author>\n\t\t<idno type=\"DOI\">10.18653/v1/2020.acl-main.645</idno>\n\t</analytic>\n\t<monogr>\n\t\t<title level=\"m\">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>\n\t\t<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>\n\t\t<imprint>\n\t\t\t<publisher>Association for Computational Linguistics</publisher>\n\t\t\t<date type=\"published\" when=\"2020\">2020. July</date>\n\t\t\t<biblScope unit=\"page\">7219</biblScope>\n\t\t</imprint>\n\t</monogr>\n</biblStruct>\n"}, {"refKey": 20, "tei": "<biblStruct xml:id=\"b20\">\n\t<monogr>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Y</forename><surname>Liu</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">M</forename><surname>Ott</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">N</forename><surname>Goyal</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">J</forename><surname>Du</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">M</forename><surname>Joshi</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">D</forename><surname>Chen</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">O</forename><surname>Levy</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">M</forename><surname>Lewis</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">L</forename><surname>Zettlemoyer</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">V</forename><surname>Stoyanov</surname></persName>\n\t\t</author>\n\t\t<idno>arXiv:1907.11692</idno>\n\t\t<title level=\"m\">RoBERTa: A Robustly Optimized BERT Pretraining Approach</title>\n\t\t<imprint>\n\t\t\t<date>2019. July</date>\n\t\t</imprint>\n\t</monogr>\n</biblStruct>\n"}], "runtime": 16406, "id": "4961dbadda4e1c167a5d9bbc4caca0eadaab593f", "metadata": {"id": "4961dbadda4e1c167a5d9bbc4caca0eadaab593f"}, "original_file_path": "../../datalake/Samuel/SOFTware-Sync/downloads/xml/hal-03596653.grobid.tei.xml", "file_name": "hal-03596653.grobid.tei.xml"}