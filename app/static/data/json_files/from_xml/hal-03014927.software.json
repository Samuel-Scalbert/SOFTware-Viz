{"application": "software-mentions", "version": "0.8.0", "date": "2024-10-07T12:08+0000", "md5": "E425D9D33ACF7C3CB97E2B952BBCFD2B", "mentions": [{"type": "software", "software-type": "software", "software-name": {"rawForm": "Grew", "normalizedForm": "Grew", "offsetStart": 0, "offsetEnd": 4}, "context": "Grew-match All along the annotation phase, the latest version of the annotated corpora (on a git repository) was searchable online via the Grew-match querying tool.20 Grew-match is a generic graphmatching tool which was adapted to take into account the MWE annotations, by adding MWE-specific graph nodes and arcs, as shown in Figure 1: each MWE gives rise to a fake \"token\" node, heading arcs to all the components of the MWE.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9162024855613708}, "created": {"value": false, "score": 3.814697265625e-05}, "shared": {"value": false, "score": 5.0187110900878906e-05}}, "documentContextAttributes": {"used": {"value": true, "score": 0.99748694896698}, "created": {"value": false, "score": 3.814697265625e-05}, "shared": {"value": false, "score": 5.0187110900878906e-05}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Grew-match", "normalizedForm": "Grew-match", "offsetStart": 0, "offsetEnd": 10}, "context": "Grew-match is a generic graphmatching tool which was adapted to take into account the MWE annotations, by adding MWE-specific graph nodes and arcs, as shown in Figure 1: each MWE gives rise to a fake \"token\" node, heading arcs to all the components of the MWE. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.000156402587890625}, "created": {"value": false, "score": 0.12823039293289185}, "shared": {"value": false, "score": 4.76837158203125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999593496322632}, "created": {"value": false, "score": 0.12823039293289185}, "shared": {"value": false, "score": 4.76837158203125e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "PARSEME", "normalizedForm": "PARSEME", "offsetStart": 6, "offsetEnd": 13}, "context": "-tab \"PARSEME\".", "mentionContextAttributes": {"used": {"value": false, "score": 0.013737320899963379}, "created": {"value": false, "score": 6.830692291259766e-05}, "shared": {"value": false, "score": 4.76837158203125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9782657027244568}, "created": {"value": false, "score": 0.4336351156234741}, "shared": {"value": true, "score": 0.9802738428115845}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "PARSEME", "normalizedForm": "PARSEME", "offsetStart": 25, "offsetEnd": 32}, "context": "The contributions of the PARSEME shared task 1.2 can be summarised as: (1) the creation and enhancement of VMWE-annotated corpora including three new languages, (2) an evaluation methodology to split the corpora ensuring the representativity of the target phenomenon, and (3) encouraging results hinting at improvements on the identification of unseen VMWEs. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.020050346851348877}, "created": {"value": false, "score": 0.08829039335250854}, "shared": {"value": false, "score": 7.152557373046875e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9782657027244568}, "created": {"value": false, "score": 0.4336351156234741}, "shared": {"value": true, "score": 0.9802738428115845}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "PARSEME", "normalizedForm": "PARSEME", "offsetStart": 25, "offsetEnd": 32}, "context": "Previous editions of the PARSEME shared task focused on the identification of verbal MWEs (VMWEs), because of their challenging traits: complex structure, discontinuities, variability, ambiguity, etc. (Savary et al., 2017).", "mentionContextAttributes": {"used": {"value": false, "score": 0.31777071952819824}, "created": {"value": false, "score": 0.0006421208381652832}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9782657027244568}, "created": {"value": false, "score": 0.4336351156234741}, "shared": {"value": true, "score": 0.9802738428115845}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Grew-match", "normalizedForm": "Grew-match", "offsetStart": 25, "offsetEnd": 35}, "context": "Language teams thus used Grew-match to identify potential errors   and inconsistencies, e.g., the VMWE in Figure 1 would be retrieved by searching for VMWEs lacking a verbal component (in this case, the MWE annotation is correct whereas the POS of cut is incorrect).", "mentionContextAttributes": {"used": {"value": true, "score": 0.9999593496322632}, "created": {"value": false, "score": 1.1324882507324219e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999593496322632}, "created": {"value": false, "score": 0.12823039293289185}, "shared": {"value": false, "score": 4.76837158203125e-07}}}, {"type": "software", "software-type": "implicit", "software-name": {"rawForm": "script", "normalizedForm": "script", "offsetStart": 30, "offsetEnd": 36}, "context": "Evaluation Tools We adopt the script and metrics developed in edition 1.1 and described in detail by Ramisch et al. (2018). ", "mentionContextAttributes": {"used": {"value": false, "score": 0.08173680305480957}, "created": {"value": false, "score": 0.3528773784637451}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": false, "score": 0.08173680305480957}, "created": {"value": false, "score": 0.3528773784637451}, "shared": {"value": false, "score": 2.384185791015625e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "PARSEME", "normalizedForm": "PARSEME", "offsetStart": 38, "offsetEnd": 45}, "context": "This work was supported by the IC1207 PARSEME COST action and project PARSEME-FR (ANR-14-CERA-0001).", "mentionContextAttributes": {"used": {"value": true, "score": 0.941505491733551}, "created": {"value": false, "score": 0.012165665626525879}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9782657027244568}, "created": {"value": false, "score": 0.4336351156234741}, "shared": {"value": true, "score": 0.9802738428115845}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "PARSEME", "normalizedForm": "PARSEME", "offsetStart": 39, "offsetEnd": 46}, "context": "7 Chinese is the first language in the PARSEME collection in which word boundaries are not spelled out in running text.", "mentionContextAttributes": {"used": {"value": false, "score": 0.03598010540008545}, "created": {"value": false, "score": 5.7578086853027344e-05}, "shared": {"value": false, "score": 4.76837158203125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9782657027244568}, "created": {"value": false, "score": 0.4336351156234741}, "shared": {"value": true, "score": 0.9802738428115845}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "MTLB-STRUCT", "normalizedForm": "MTLB-STRUCT", "offsetStart": 43, "offsetEnd": 54}, "context": "In the open track, the best F1 obtained by MTLB-STRUCT (38.53) is by over 10 points higher the corresponding best score in the edition 1.1 (28.46, by SHOMA). ", "mentionContextAttributes": {"used": {"value": false, "score": 0.4322168231010437}, "created": {"value": false, "score": 2.1696090698242188e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": false, "score": 0.4322168231010437}, "created": {"value": false, "score": 2.1696090698242188e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "PARSEME", "normalizedForm": "PARSEME", "offsetStart": 56, "offsetEnd": 63}, "context": "These two findings motivated the current edition of the PARSEME shared task focusing on the identification of unseen VMWEs.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9782657027244568}, "created": {"value": false, "score": 0.017501473426818848}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9782657027244568}, "created": {"value": false, "score": 0.4336351156234741}, "shared": {"value": true, "score": 0.9802738428115845}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "CoNLL-U", "normalizedForm": "CoNLL-U", "offsetStart": 57, "offsetEnd": 64}, "context": "They were released to participants in an instance of the CoNLL-U Plus format14 called CUPT. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9999299049377441}, "created": {"value": false, "score": 0.0005623698234558105}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999299049377441}, "created": {"value": false, "score": 0.0005623698234558105}, "shared": {"value": false, "score": 1.430511474609375e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "UDPipe", "normalizedForm": "UDPipe", "offsetStart": 65, "offsetEnd": 71}, "context": "For all languages except Italian, the raw corpus was parsed with UDPipe (Straka and Strakov\u00e1, 2017) using models trained on UD treebanks (2.0, 2.4 or 2.5). ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9999488592147827}, "created": {"value": false, "score": 5.4836273193359375e-06}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999488592147827}, "created": {"value": false, "score": 5.4836273193359375e-06}, "shared": {"value": false, "score": 5.960464477539062e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "PARSEME", "normalizedForm": "PARSEME", "offsetStart": 70, "offsetEnd": 77}, "context": "This work was supported by the IC1207 PARSEME COST action and project PARSEME-FR (ANR-14-CERA-0001).", "mentionContextAttributes": {"used": {"value": true, "score": 0.941505491733551}, "created": {"value": false, "score": 0.012165665626525879}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9782657027244568}, "created": {"value": false, "score": 0.4336351156234741}, "shared": {"value": true, "score": 0.9802738428115845}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "PARSEME", "normalizedForm": "PARSEME", "offsetStart": 73, "offsetEnd": 80}, "context": "2 The remainder of this paper describes the design of edition 1.2 of the PARSEME shared task, and summarises its outcomes.3", "mentionContextAttributes": {"used": {"value": false, "score": 0.044421255588531494}, "created": {"value": false, "score": 0.4336351156234741}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9782657027244568}, "created": {"value": false, "score": 0.4336351156234741}, "shared": {"value": true, "score": 0.9802738428115845}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "PARSEME", "normalizedForm": "PARSEME", "offsetStart": 77, "offsetEnd": 84}, "context": "and described at http://gitlab.com/parseme/corpora/wikis/Raw-corpora-for-the-PARSEME-1.2-shared-task", "mentionContextAttributes": {"used": {"value": false, "score": 0.01615971326828003}, "created": {"value": false, "score": 0.004916012287139893}, "shared": {"value": true, "score": 0.9802738428115845}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9782657027244568}, "created": {"value": false, "score": 0.4336351156234741}, "shared": {"value": true, "score": 0.9802738428115845}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "PARSEME", "normalizedForm": "PARSEME", "offsetStart": 128, "offsetEnd": 135}, "context": "Progress on this task was especially motivated by shared tasks such as DiMSUM (Schneider et al., 2016), and two editions of the PARSEME shared tasks, edition 1.0 in 2017 (Savary et al., 2017), and edition 1.1 in 2018 (Ramisch et al., 2018).", "mentionContextAttributes": {"used": {"value": true, "score": 0.8845371603965759}, "created": {"value": false, "score": 0.0001385211944580078}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9782657027244568}, "created": {"value": false, "score": 0.4336351156234741}, "shared": {"value": true, "score": 0.9802738428115845}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Grew", "normalizedForm": "Grew", "offsetStart": 139, "offsetEnd": 143}, "context": "Grew-match All along the annotation phase, the latest version of the annotated corpora (on a git repository) was searchable online via the Grew-match querying tool.2", "mentionContextAttributes": {"used": {"value": true, "score": 0.99748694896698}, "created": {"value": false, "score": 3.123283386230469e-05}, "shared": {"value": false, "score": 1.0251998901367188e-05}}, "documentContextAttributes": {"used": {"value": true, "score": 0.99748694896698}, "created": {"value": false, "score": 3.814697265625e-05}, "shared": {"value": false, "score": 5.0187110900878906e-05}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "SHOMA", "normalizedForm": "SHOMA", "offsetStart": 150, "offsetEnd": 155}, "context": "In the open track, the best F1 obtained by MTLB-STRUCT (38.53) is by over 10 points higher the corresponding best score in the edition 1.1 (28.46, by SHOMA). ", "mentionContextAttributes": {"used": {"value": false, "score": 0.4322168231010437}, "created": {"value": false, "score": 2.1696090698242188e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": false, "score": 0.4322168231010437}, "created": {"value": false, "score": 5.7220458984375e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "CoNLL-U", "normalizedForm": "CoNLL-U", "offsetStart": 200, "offsetEnd": 207}, "context": "As described in more detail by Ramisch et al. (2018), it is a TAB-separated textual format with one token per line and 11 columns: the first 10 correspond to morpho-syntactic information identical to CoNLL-U such as the token's LEMMA and UPOS tags, and the 11th column contains the VMWE annotations in the form of numerical indices and a category label. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.31958264112472534}, "created": {"value": false, "score": 5.9604644775390625e-06}, "shared": {"value": false, "score": 1.430511474609375e-06}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999299049377441}, "created": {"value": false, "score": 0.0005623698234558105}, "shared": {"value": false, "score": 1.430511474609375e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "SHOMA", "normalizedForm": "SHOMA", "offsetStart": 212, "offsetEnd": 217}, "context": "Results are not comparable across editions due to different corpora, but for languages with similar number of annotated total and unseen VMWEs, some systems reach higher unseen F1 scores than the best 1.1 system SHOMA (e.g. in German, French, and Hindi).", "mentionContextAttributes": {"used": {"value": false, "score": 0.009010732173919678}, "created": {"value": false, "score": 5.7220458984375e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": false, "score": 0.4322168231010437}, "created": {"value": false, "score": 5.7220458984375e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "UDPipe", "normalizedForm": "UDPipe", "offsetStart": 337, "offsetEnd": 343}, "context": "Quality All 14 languages now benefit from morphosyntactic tagsets compatible with UD version 2. The tokenisation, lemmatisation, and morphosyntactic layers contain manual annotations for some languages (Chinese, French, Irish, Italian, Swedish, partly German, Greek, Polish and Portuguese) and automatic ones for the others (mostly with UDPipe11 trained on UD version 2.5).", "mentionContextAttributes": {"used": {"value": false, "score": 0.0291936993598938}, "created": {"value": false, "score": 4.887580871582031e-06}, "shared": {"value": false, "score": 5.960464477539062e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999488592147827}, "created": {"value": false, "score": 5.4836273193359375e-06}, "shared": {"value": false, "score": 5.960464477539062e-07}}}], "references": [], "runtime": 5194, "id": "fa29889cc2d07a4798ffbbed2a0eb860d6a8949b", "metadata": {"id": "fa29889cc2d07a4798ffbbed2a0eb860d6a8949b"}, "original_file_path": "../../datalake/Samuel/SOFTware-Sync/downloads/xml/hal-03014927.grobid.tei.xml", "file_name": "hal-03014927.grobid.tei.xml"}