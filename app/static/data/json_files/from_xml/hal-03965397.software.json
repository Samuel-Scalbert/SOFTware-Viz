{"application": "software-mentions", "version": "0.8.0", "date": "2024-10-07T11:46+0000", "md5": "E3EFB4DB72C0CA19DD67EC5BF2435D7A", "mentions": [{"type": "software", "software-type": "software", "software-name": {"rawForm": "BERTalsem", "normalizedForm": "BERTalsem", "offsetStart": 0, "offsetEnd": 9}, "context": "BERTalsem rescoring model In this section, we recall the architecture of the BERTalsem model from (Fohr and Illina, 2021), used as the starting point for the current work.", "mentionContextAttributes": {"used": {"value": false, "score": 0.02920585870742798}, "created": {"value": false, "score": 0.002281785011291504}, "shared": {"value": false, "score": 5.960464477539062e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.999982476234436}, "created": {"value": true, "score": 0.9935000538825989}, "shared": {"value": false, "score": 9.5367431640625e-07}}, "references": [{"label": "(Fohr and Illina, 2021)", "normalizedForm": "Fohr and Illina, 2021", "refKey": 4}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "BERTalsem", "normalizedForm": "BERTalsem", "offsetStart": 0, "offsetEnd": 9}, "context": "BERTalsem rescoring model.", "mentionContextAttributes": {"used": {"value": false, "score": 0.001008450984954834}, "created": {"value": false, "score": 5.53131103515625e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.999982476234436}, "created": {"value": true, "score": 0.9935000538825989}, "shared": {"value": false, "score": 9.5367431640625e-07}}, "references": [{"label": "(Fohr and Illina, 2021)", "normalizedForm": "Fohr and Illina, 2021", "refKey": 4}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "BERTalsem", "normalizedForm": "BERTalsem", "offsetStart": 6, "offsetEnd": 15}, "context": "For P-BERTalsem rescoring model, to avoid the overflow of the number of the BERT input tokens, we use at most M last words from the previous sentence (M=30).", "mentionContextAttributes": {"used": {"value": true, "score": 0.885818600654602}, "created": {"value": false, "score": 6.198883056640625e-06}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.999982476234436}, "created": {"value": true, "score": 0.9935000538825989}, "shared": {"value": false, "score": 9.5367431640625e-07}}, "references": [{"label": "(Fohr and Illina, 2021)", "normalizedForm": "Fohr and Illina, 2021", "refKey": 4}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "BERTalsem", "normalizedForm": "BERTalsem", "offsetStart": 12, "offsetEnd": 21}, "context": "Comparing P-BERTalsem with BERTalsem model (Fohr and Illina, 2021) (without the previous sentence information, lines 8 and 9 versus line 4), we see that the integration of the previous sentence information helps in the selection of the best hypothesis.", "mentionContextAttributes": {"used": {"value": false, "score": 0.17970341444015503}, "created": {"value": false, "score": 8.702278137207031e-06}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.999982476234436}, "created": {"value": true, "score": 0.9935000538825989}, "shared": {"value": false, "score": 9.5367431640625e-07}}, "references": [{"label": "(Fohr and Illina, 2021)", "normalizedForm": "Fohr and Illina, 2021", "refKey": 4}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "BERTalsem", "normalizedForm": "BERTalsem", "offsetStart": 13, "offsetEnd": 22}, "context": "The proposed BERTalsem-fg displays a similar performance as BERTalsem (lines 5, 6 versus line 4).", "mentionContextAttributes": {"used": {"value": false, "score": 0.00018852949142456055}, "created": {"value": false, "score": 0.00041049718856811523}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.999982476234436}, "created": {"value": true, "score": 0.9935000538825989}, "shared": {"value": false, "score": 9.5367431640625e-07}}, "references": [{"label": "(Fohr and Illina, 2021)", "normalizedForm": "Fohr and Illina, 2021", "refKey": 4}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "BERTalsem", "normalizedForm": "BERTalsem", "offsetStart": 16, "offsetEnd": 25}, "context": "Compared to the BERTalsem, we added the words from the previously recognized sentences to each hypothesis of a hypothesis pair.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9998013377189636}, "created": {"value": false, "score": 0.00047004222869873047}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.999982476234436}, "created": {"value": true, "score": 0.9935000538825989}, "shared": {"value": false, "score": 9.5367431640625e-07}}, "references": [{"label": "(Fohr and Illina, 2021)", "normalizedForm": "Fohr and Illina, 2021", "refKey": 4}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "BERTalsem", "normalizedForm": "BERTalsem", "offsetStart": 27, "offsetEnd": 36}, "context": "We iterate the training of BERTalsem as follows: during the first epoch, the layer weights of the BERT model are frozen, and during the following epochs all BERT weights are updated.", "mentionContextAttributes": {"used": {"value": false, "score": 0.044471144676208496}, "created": {"value": false, "score": 0.0003510713577270508}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.999982476234436}, "created": {"value": true, "score": 0.9935000538825989}, "shared": {"value": false, "score": 9.5367431640625e-07}}, "references": [{"label": "(Fohr and Illina, 2021)", "normalizedForm": "Fohr and Illina, 2021", "refKey": 4}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "BERTalsem", "normalizedForm": "BERTalsem", "offsetStart": 27, "offsetEnd": 36}, "context": "By studying the results of BERTalsem (line 4), we see that the conclusions given by Fohr and Illina (2021) are still valid when the noisy acoustic model is used: the BERTalsem provides consistent WER reduction compared to the baseline model with GPT-2 rescoring (line 3).", "mentionContextAttributes": {"used": {"value": true, "score": 0.9348284006118774}, "created": {"value": false, "score": 2.86102294921875e-06}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.999982476234436}, "created": {"value": true, "score": 0.9935000538825989}, "shared": {"value": false, "score": 9.5367431640625e-07}}, "references": [{"label": "(Fohr and Illina, 2021)", "normalizedForm": "Fohr and Illina, 2021", "refKey": 4}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "BERTalsem", "normalizedForm": "BERTalsem", "offsetStart": 27, "offsetEnd": 36}, "context": "Comparing P-BERTalsem with BERTalsem model (Fohr and Illina, 2021) (without the previous sentence information, lines 8 and 9 versus line 4), we see that the integration of the previous sentence information helps in the selection of the best hypothesis.", "mentionContextAttributes": {"used": {"value": false, "score": 0.17970341444015503}, "created": {"value": false, "score": 8.702278137207031e-06}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.999982476234436}, "created": {"value": true, "score": 0.9935000538825989}, "shared": {"value": false, "score": 9.5367431640625e-07}}, "references": [{"label": "(Fohr and Illina, 2021)", "normalizedForm": "Fohr and Illina, 2021", "refKey": 4, "offsetStart": 20705, "offsetEnd": 20728}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "BERTalsem", "normalizedForm": "BERTalsem", "offsetStart": 27, "offsetEnd": 36}, "context": "Analysing the results of P-BERTalsem, we observe that the model corrects syntactic and semantic errors, compared to BERTalsem (lines 8 and 9 versus line 4).", "mentionContextAttributes": {"used": {"value": true, "score": 0.9998082518577576}, "created": {"value": false, "score": 5.4836273193359375e-06}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.999982476234436}, "created": {"value": true, "score": 0.9935000538825989}, "shared": {"value": false, "score": 9.5367431640625e-07}}, "references": [{"label": "(Fohr and Illina, 2021)", "normalizedForm": "Fohr and Illina, 2021", "refKey": 4}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "BERTalsem", "normalizedForm": "BERTalsem", "offsetStart": 28, "offsetEnd": 37}, "context": "The objective is to provide BERTalsem model with fine-grained information (at the word token level and not just at the sentence level as in BERTalsem).", "mentionContextAttributes": {"used": {"value": false, "score": 0.0001462697982788086}, "created": {"value": false, "score": 0.004530549049377441}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.999982476234436}, "created": {"value": true, "score": 0.9935000538825989}, "shared": {"value": false, "score": 9.5367431640625e-07}}, "references": [{"label": "(Fohr and Illina, 2021)", "normalizedForm": "Fohr and Illina, 2021", "refKey": 4}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "BERTalsem", "normalizedForm": "BERTalsem", "offsetStart": 28, "offsetEnd": 37}, "context": "( 1) with Psem (h) given by BERTalsem and Plm (h) given by the GPT-2.", "mentionContextAttributes": {"used": {"value": true, "score": 0.999982476234436}, "created": {"value": false, "score": 2.1457672119140625e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.999982476234436}, "created": {"value": true, "score": 0.9935000538825989}, "shared": {"value": false, "score": 9.5367431640625e-07}}, "references": [{"label": "(Fohr and Illina, 2021)", "normalizedForm": "Fohr and Illina, 2021", "refKey": 4}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "BERTalsem", "normalizedForm": "BERTalsem", "offsetStart": 29, "offsetEnd": 38}, "context": "DNN rescoring model (denoted BERTalsem) computes SI, associated with each pair of hypotheses.", "mentionContextAttributes": {"used": {"value": false, "score": 0.13689643144607544}, "created": {"value": false, "score": 3.337860107421875e-06}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.999982476234436}, "created": {"value": true, "score": 0.9935000538825989}, "shared": {"value": false, "score": 9.5367431640625e-07}}, "references": [{"label": "(Fohr and Illina, 2021)", "normalizedForm": "Fohr and Illina, 2021", "refKey": 4}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "BERTalsem", "normalizedForm": "BERTalsem", "offsetStart": 29, "offsetEnd": 38}, "context": "Fine grained rescoring model BERTalsem-fg This section presents the first rescoring method proposed in this paper.", "mentionContextAttributes": {"used": {"value": false, "score": 0.00012743473052978516}, "created": {"value": false, "score": 0.06361150741577148}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.999982476234436}, "created": {"value": true, "score": 0.9935000538825989}, "shared": {"value": false, "score": 9.5367431640625e-07}}, "references": [{"label": "(Fohr and Illina, 2021)", "normalizedForm": "Fohr and Illina, 2021", "refKey": 4}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "BERTalsem", "normalizedForm": "BERTalsem", "offsetStart": 30, "offsetEnd": 39}, "context": "Fine-grained rescoring model: BERTalsem-fg The BERTalsem-fg shows an improvement over the baseline system with GPT-2 rescoring (line 6 versus line 3, the significance is indicated by \"*\" in Table 1).", "mentionContextAttributes": {"used": {"value": false, "score": 0.0026284456253051758}, "created": {"value": false, "score": 1.633167266845703e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.999982476234436}, "created": {"value": true, "score": 0.9935000538825989}, "shared": {"value": false, "score": 9.5367431640625e-07}}, "references": [{"label": "(Fohr and Illina, 2021)", "normalizedForm": "Fohr and Illina, 2021", "refKey": 4}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "BERTalsem", "normalizedForm": "BERTalsem", "offsetStart": 32, "offsetEnd": 41}, "context": "We call this model fine-grained BERTalsem-fg.", "mentionContextAttributes": {"used": {"value": false, "score": 4.458427429199219e-05}, "created": {"value": true, "score": 0.9935000538825989}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.999982476234436}, "created": {"value": true, "score": 0.9935000538825989}, "shared": {"value": false, "score": 9.5367431640625e-07}}, "references": [{"label": "(Fohr and Illina, 2021)", "normalizedForm": "Fohr and Illina, 2021", "refKey": 4}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "BERTalsem", "normalizedForm": "BERTalsem", "offsetStart": 33, "offsetEnd": 42}, "context": "In conclusion, the best system P-BERTalsem gives between 1% and 3% relative WER reduction compared to BERTalsem rescoring model (Fohr and Illina, 2021) (lines 8, 9 versus line 4).", "mentionContextAttributes": {"used": {"value": false, "score": 0.015627920627593994}, "created": {"value": false, "score": 9.059906005859375e-06}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.999982476234436}, "created": {"value": true, "score": 0.9935000538825989}, "shared": {"value": false, "score": 9.5367431640625e-07}}, "references": [{"label": "(Fohr and Illina, 2021)", "normalizedForm": "Fohr and Illina, 2021", "refKey": 4}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "BERTalsem", "normalizedForm": "BERTalsem", "offsetStart": 42, "offsetEnd": 51}, "context": "The semantic model is not used (\u03b3=0); (e) BERTalsem with GPT-2 resc (Fohr and Illina, 2021) is performed to compare the Table 1.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9917984008789062}, "created": {"value": false, "score": 2.6226043701171875e-06}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.999982476234436}, "created": {"value": true, "score": 0.9935000538825989}, "shared": {"value": false, "score": 9.5367431640625e-07}}, "references": [{"label": "(Fohr and Illina, 2021)", "normalizedForm": "Fohr and Illina, 2021", "refKey": 4}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "BERTalsem", "normalizedForm": "BERTalsem", "offsetStart": 47, "offsetEnd": 56}, "context": "Fine-grained rescoring model: BERTalsem-fg The BERTalsem-fg shows an improvement over the baseline system with GPT-2 rescoring (line 6 versus line 3, the significance is indicated by \"*\" in Table 1).", "mentionContextAttributes": {"used": {"value": false, "score": 0.0026284456253051758}, "created": {"value": false, "score": 1.633167266845703e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.999982476234436}, "created": {"value": true, "score": 0.9935000538825989}, "shared": {"value": false, "score": 9.5367431640625e-07}}, "references": [{"label": "(Fohr and Illina, 2021)", "normalizedForm": "Fohr and Illina, 2021", "refKey": 4}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "BERTalsem", "normalizedForm": "BERTalsem", "offsetStart": 48, "offsetEnd": 57}, "context": "The lines 8 and 9 display the results for our P-BERTalsem model.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9990474581718445}, "created": {"value": false, "score": 0.000440061092376709}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.999982476234436}, "created": {"value": true, "score": 0.9935000538825989}, "shared": {"value": false, "score": 9.5367431640625e-07}}, "references": [{"label": "(Fohr and Illina, 2021)", "normalizedForm": "Fohr and Illina, 2021", "refKey": 4}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "OpenAI", "normalizedForm": "OpenAI", "offsetStart": 49, "offsetEnd": 55}, "context": "The model has 117M parameters and was trained by OpenAI on 40GB of Internet text. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9978622794151306}, "created": {"value": false, "score": 0.0022614002227783203}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9978622794151306}, "created": {"value": false, "score": 0.0022614002227783203}, "shared": {"value": false, "score": 2.384185791015625e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "BERTalsem", "normalizedForm": "BERTalsem", "offsetStart": 55, "offsetEnd": 64}, "context": "\"~\" denotes significantly different result compared to BERTalsem with GPT-2 resc.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9999305009841919}, "created": {"value": false, "score": 3.337860107421875e-06}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.999982476234436}, "created": {"value": true, "score": 0.9935000538825989}, "shared": {"value": false, "score": 9.5367431640625e-07}}, "references": [{"label": "(Fohr and Illina, 2021)", "normalizedForm": "Fohr and Illina, 2021", "refKey": 4}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "BERTalsem", "normalizedForm": "BERTalsem", "offsetStart": 55, "offsetEnd": 64}, "context": "Here is one example of a semantic error corrected by P-BERTalsem model for 5dB noisy condition, test set: ref: I got to lhasa that i understood the face behind the statistics you hear about six thousand sacred monuments\u2026 hyp1: I got to loss that i understood the face behind these statistics you hear about six thousand sacred monuments\u2026 hyp2: I got to lhasa that i understood the face behind these statistics you hear about six thousand sacred monuments\u2026", "mentionContextAttributes": {"used": {"value": true, "score": 0.9999048709869385}, "created": {"value": false, "score": 2.0265579223632812e-06}, "shared": {"value": false, "score": 4.76837158203125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.999982476234436}, "created": {"value": true, "score": 0.9935000538825989}, "shared": {"value": false, "score": 9.5367431640625e-07}}, "references": [{"label": "(Fohr and Illina, 2021)", "normalizedForm": "Fohr and Illina, 2021", "refKey": 4}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "BERTalsem", "normalizedForm": "BERTalsem", "offsetStart": 60, "offsetEnd": 69}, "context": "Configurations with CNN and bi-LSTM models are shown; (b) P-BERTalsem with GPT-2 gives the results for the approach taking into account the previous sentence; (c) P-BERTalsem-fg with GPT-2: the combined model gives no additional improvement compared to P-BERTalsem with GPT-2 and the results are not presented in this paper.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9964569211006165}, "created": {"value": false, "score": 1.6689300537109375e-06}, "shared": {"value": false, "score": 7.152557373046875e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.999982476234436}, "created": {"value": true, "score": 0.9935000538825989}, "shared": {"value": false, "score": 9.5367431640625e-07}}, "references": [{"label": "(Fohr and Illina, 2021)", "normalizedForm": "Fohr and Illina, 2021", "refKey": 4}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "BERTalsem", "normalizedForm": "BERTalsem", "offsetStart": 60, "offsetEnd": 69}, "context": "The proposed BERTalsem-fg displays a similar performance as BERTalsem (lines 5, 6 versus line 4).", "mentionContextAttributes": {"used": {"value": false, "score": 0.00018852949142456055}, "created": {"value": false, "score": 0.00041049718856811523}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.999982476234436}, "created": {"value": true, "score": 0.9935000538825989}, "shared": {"value": false, "score": 9.5367431640625e-07}}, "references": [{"label": "(Fohr and Illina, 2021)", "normalizedForm": "Fohr and Illina, 2021", "refKey": 4}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "BERTalsem", "normalizedForm": "BERTalsem", "offsetStart": 63, "offsetEnd": 72}, "context": "Concerning the acoustic and language model information part of BERTalsem, we modify the language model probabilities by replacing them with the conditional probabilities Plm(hi | prev_sent) and Plm(hj | prev_sent).", "mentionContextAttributes": {"used": {"value": false, "score": 0.4502498507499695}, "created": {"value": false, "score": 3.62396240234375e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.999982476234436}, "created": {"value": true, "score": 0.9935000538825989}, "shared": {"value": false, "score": 9.5367431640625e-07}}, "references": [{"label": "(Fohr and Illina, 2021)", "normalizedForm": "Fohr and Illina, 2021", "refKey": 4}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Kaldi", "normalizedForm": "Kaldi", "offsetStart": 65, "offsetEnd": 70}, "context": "Speech recognition system Our recognition system is based on the Kaldi speech recognition toolbox (Povey et al., 2011). ", "mentionContextAttributes": {"used": {"value": false, "score": 0.48916566371917725}, "created": {"value": false, "score": 0.007120788097381592}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": false, "score": 0.48916566371917725}, "created": {"value": false, "score": 0.007120788097381592}, "shared": {"value": false, "score": 2.384185791015625e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "BERTalsem", "normalizedForm": "BERTalsem", "offsetStart": 66, "offsetEnd": 75}, "context": "The second hypothesis is selected as the sentence recognized by P-BERTalsem because the previous sentence contains the word \"Tibet\".", "mentionContextAttributes": {"used": {"value": true, "score": 0.9377396106719971}, "created": {"value": false, "score": 0.00037360191345214844}, "shared": {"value": false, "score": 9.5367431640625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.999982476234436}, "created": {"value": true, "score": 0.9935000538825989}, "shared": {"value": false, "score": 9.5367431640625e-07}}, "references": [{"label": "(Fohr and Illina, 2021)", "normalizedForm": "Fohr and Illina, 2021", "refKey": 4}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "BERTalsem", "normalizedForm": "BERTalsem", "offsetStart": 77, "offsetEnd": 86}, "context": "BERTalsem rescoring model In this section, we recall the architecture of the BERTalsem model from (Fohr and Illina, 2021), used as the starting point for the current work.", "mentionContextAttributes": {"used": {"value": false, "score": 0.02920585870742798}, "created": {"value": false, "score": 0.002281785011291504}, "shared": {"value": false, "score": 5.960464477539062e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.999982476234436}, "created": {"value": true, "score": 0.9935000538825989}, "shared": {"value": false, "score": 9.5367431640625e-07}}, "references": [{"label": "(Fohr and Illina, 2021)", "normalizedForm": "Fohr and Illina, 2021", "refKey": 4}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "BERTalsem", "normalizedForm": "BERTalsem", "offsetStart": 78, "offsetEnd": 87}, "context": "configuration (line 4) transformer-based models, proposed in this paper, with BERTalsem proposed by Fohr and Illina (2021).", "mentionContextAttributes": {"used": {"value": false, "score": 0.0007256865501403809}, "created": {"value": true, "score": 0.8456586003303528}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.999982476234436}, "created": {"value": true, "score": 0.9935000538825989}, "shared": {"value": false, "score": 9.5367431640625e-07}}, "references": [{"label": "(Fohr and Illina, 2021)", "normalizedForm": "Fohr and Illina, 2021", "refKey": 4}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "BERTalsem", "normalizedForm": "BERTalsem", "offsetStart": 84, "offsetEnd": 93}, "context": "The proposed rescoring model using the previously recognized sentences is denoted P-BERTalsem.", "mentionContextAttributes": {"used": {"value": false, "score": 0.00045377016067504883}, "created": {"value": false, "score": 0.18929237127304077}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.999982476234436}, "created": {"value": true, "score": 0.9935000538825989}, "shared": {"value": false, "score": 9.5367431640625e-07}}, "references": [{"label": "(Fohr and Illina, 2021)", "normalizedForm": "Fohr and Illina, 2021", "refKey": 4}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "BERTalsem", "normalizedForm": "BERTalsem", "offsetStart": 88, "offsetEnd": 97}, "context": "For the rescoring models proposed in this article, we studied three configurations: (a) BERTalsem-fg with GPT-2 represents rescoring using BERTalsem-fg.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9566235542297363}, "created": {"value": false, "score": 0.009108006954193115}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.999982476234436}, "created": {"value": true, "score": 0.9935000538825989}, "shared": {"value": false, "score": 9.5367431640625e-07}}, "references": [{"label": "(Fohr and Illina, 2021)", "normalizedForm": "Fohr and Illina, 2021", "refKey": 4}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "BERTalsem", "normalizedForm": "BERTalsem", "offsetStart": 100, "offsetEnd": 109}, "context": "Statistically significant improvements are observed for all noise levels and clean speech for the P-BERTalsem compared to the GPT-2 resc configuration (lines 8 and 9 versus line 3, the significance is indicated by \"*\").", "mentionContextAttributes": {"used": {"value": false, "score": 0.24007105827331543}, "created": {"value": false, "score": 8.64267349243164e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.999982476234436}, "created": {"value": true, "score": 0.9935000538825989}, "shared": {"value": false, "score": 9.5367431640625e-07}}, "references": [{"label": "(Fohr and Illina, 2021)", "normalizedForm": "Fohr and Illina, 2021", "refKey": 4}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "BERTalsem", "normalizedForm": "BERTalsem", "offsetStart": 102, "offsetEnd": 111}, "context": "In conclusion, the best system P-BERTalsem gives between 1% and 3% relative WER reduction compared to BERTalsem rescoring model (Fohr and Illina, 2021) (lines 8, 9 versus line 4).", "mentionContextAttributes": {"used": {"value": false, "score": 0.015627920627593994}, "created": {"value": false, "score": 9.059906005859375e-06}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.999982476234436}, "created": {"value": true, "score": 0.9935000538825989}, "shared": {"value": false, "score": 9.5367431640625e-07}}, "references": [{"label": "(Fohr and Illina, 2021)", "normalizedForm": "Fohr and Illina, 2021", "refKey": 4}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "BERTalsem", "normalizedForm": "BERTalsem", "offsetStart": 110, "offsetEnd": 119}, "context": "The second potential advantage is to provide complementary information compared to the BERT model included in BERTalsem.", "mentionContextAttributes": {"used": {"value": false, "score": 0.00016820430755615234}, "created": {"value": false, "score": 0.00026279687881469727}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.999982476234436}, "created": {"value": true, "score": 0.9935000538825989}, "shared": {"value": false, "score": 9.5367431640625e-07}}, "references": [{"label": "(Fohr and Illina, 2021)", "normalizedForm": "Fohr and Illina, 2021", "refKey": 4}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "BERTalsem", "normalizedForm": "BERTalsem", "offsetStart": 113, "offsetEnd": 122}, "context": "In the framework of the pairwise rescoring of ASR N-best hypotheses, we would like to enrich the rescoring model BERTalsem.", "mentionContextAttributes": {"used": {"value": false, "score": 0.0019707083702087402}, "created": {"value": false, "score": 0.004515528678894043}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.999982476234436}, "created": {"value": true, "score": 0.9935000538825989}, "shared": {"value": false, "score": 9.5367431640625e-07}}, "references": [{"label": "(Fohr and Illina, 2021)", "normalizedForm": "Fohr and Illina, 2021", "refKey": 4}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "BERTalsem", "normalizedForm": "BERTalsem", "offsetStart": 116, "offsetEnd": 125}, "context": "Analysing the results of P-BERTalsem, we observe that the model corrects syntactic and semantic errors, compared to BERTalsem (lines 8 and 9 versus line 4).", "mentionContextAttributes": {"used": {"value": true, "score": 0.9998082518577576}, "created": {"value": false, "score": 5.4836273193359375e-06}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.999982476234436}, "created": {"value": true, "score": 0.9935000538825989}, "shared": {"value": false, "score": 9.5367431640625e-07}}, "references": [{"label": "(Fohr and Illina, 2021)", "normalizedForm": "Fohr and Illina, 2021", "refKey": 4}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "BERTalsem", "normalizedForm": "BERTalsem", "offsetStart": 121, "offsetEnd": 130}, "context": "The two proposed approaches perhaps contain complementary information and can be combined into a single model, denoted P-BERTalsem-fg.", "mentionContextAttributes": {"used": {"value": false, "score": 7.43865966796875e-05}, "created": {"value": false, "score": 0.41688257455825806}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.999982476234436}, "created": {"value": true, "score": 0.9935000538825989}, "shared": {"value": false, "score": 9.5367431640625e-07}}, "references": [{"label": "(Fohr and Illina, 2021)", "normalizedForm": "Fohr and Illina, 2021", "refKey": 4}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "BERTalsem", "normalizedForm": "BERTalsem", "offsetStart": 139, "offsetEnd": 148}, "context": "For the rescoring models proposed in this article, we studied three configurations: (a) BERTalsem-fg with GPT-2 represents rescoring using BERTalsem-fg.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9566231369972229}, "created": {"value": false, "score": 0.009108126163482666}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.999982476234436}, "created": {"value": true, "score": 0.9935000538825989}, "shared": {"value": false, "score": 9.5367431640625e-07}}, "references": [{"label": "(Fohr and Illina, 2021)", "normalizedForm": "Fohr and Illina, 2021", "refKey": 4}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "BERTalsem", "normalizedForm": "BERTalsem", "offsetStart": 140, "offsetEnd": 149}, "context": "The objective is to provide BERTalsem model with fine-grained information (at the word token level and not just at the sentence level as in BERTalsem).", "mentionContextAttributes": {"used": {"value": false, "score": 0.0001462697982788086}, "created": {"value": false, "score": 0.004530549049377441}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.999982476234436}, "created": {"value": true, "score": 0.9935000538825989}, "shared": {"value": false, "score": 9.5367431640625e-07}}, "references": [{"label": "(Fohr and Illina, 2021)", "normalizedForm": "Fohr and Illina, 2021", "refKey": 4}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "BERTalsem", "normalizedForm": "BERTalsem", "offsetStart": 144, "offsetEnd": 153}, "context": "In our experiments, this model is used for several purposes: (a) as a language model Plm(hi) during N-best rescoring (see eq. ( 1)); (b) inside BERTalsem to represent the language model score Plm(hi) of each hypothesis (see figure 1); (c) inside BERTalsem_fg to compute the score of each word token Pgpt2(tok) of each hypothesis; (d) inside P-BERTalsem to compute Plm(h|prev_sent). ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9999637603759766}, "created": {"value": false, "score": 1.3113021850585938e-06}, "shared": {"value": false, "score": 5.960464477539062e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.999982476234436}, "created": {"value": true, "score": 0.9935000538825989}, "shared": {"value": false, "score": 9.5367431640625e-07}}, "references": [{"label": "(Fohr and Illina, 2021)", "normalizedForm": "Fohr and Illina, 2021", "refKey": 4}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "BERTalsem", "normalizedForm": "BERTalsem", "offsetStart": 146, "offsetEnd": 155}, "context": "To analyse the results, we make the comparisons with: (a) the state-of-the-art rescoring model with GPT-2 (line 3); (b) the best configuration of BERTalsem rescoring model (line 4, (Fohr and Illina, 2021)).", "mentionContextAttributes": {"used": {"value": true, "score": 0.9960344433784485}, "created": {"value": false, "score": 3.6954879760742188e-06}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.999982476234436}, "created": {"value": true, "score": 0.9935000538825989}, "shared": {"value": false, "score": 9.5367431640625e-07}}, "references": [{"label": "(Fohr and Illina, 2021)", "normalizedForm": "Fohr and Illina, 2021", "refKey": 4}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "BERTalsem", "normalizedForm": "BERTalsem", "offsetStart": 160, "offsetEnd": 169}, "context": "This means that probably adding fine-grained information (GPT-2 probabilities at the word token level) does not bring complementary information compared to the BERTalsem model.", "mentionContextAttributes": {"used": {"value": false, "score": 0.005108475685119629}, "created": {"value": false, "score": 1.5735626220703125e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.999982476234436}, "created": {"value": true, "score": 0.9935000538825989}, "shared": {"value": false, "score": 9.5367431640625e-07}}, "references": [{"label": "(Fohr and Illina, 2021)", "normalizedForm": "Fohr and Illina, 2021", "refKey": 4}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "BERTalsem", "normalizedForm": "BERTalsem", "offsetStart": 165, "offsetEnd": 174}, "context": "Configurations with CNN and bi-LSTM models are shown; (b) P-BERTalsem with GPT-2 gives the results for the approach taking into account the previous sentence; (c) P-BERTalsem-fg with GPT-2: the combined model gives no additional improvement compared to P-BERTalsem with GPT-2 and the results are not presented in this paper.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9964569211006165}, "created": {"value": false, "score": 1.6689300537109375e-06}, "shared": {"value": false, "score": 7.152557373046875e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.999982476234436}, "created": {"value": true, "score": 0.9935000538825989}, "shared": {"value": false, "score": 9.5367431640625e-07}}, "references": [{"label": "(Fohr and Illina, 2021)", "normalizedForm": "Fohr and Illina, 2021", "refKey": 4}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "BERTalsem", "normalizedForm": "BERTalsem", "offsetStart": 166, "offsetEnd": 175}, "context": "By studying the results of BERTalsem (line 4), we see that the conclusions given by Fohr and Illina (2021) are still valid when the noisy acoustic model is used: the BERTalsem provides consistent WER reduction compared to the baseline model with GPT-2 rescoring (line 3).", "mentionContextAttributes": {"used": {"value": true, "score": 0.9348282217979431}, "created": {"value": false, "score": 2.86102294921875e-06}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.999982476234436}, "created": {"value": true, "score": 0.9935000538825989}, "shared": {"value": false, "score": 9.5367431640625e-07}}, "references": [{"label": "(Fohr and Illina, 2021)", "normalizedForm": "Fohr and Illina, 2021", "refKey": 4}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "BERTalsem_fg", "normalizedForm": "BERTalsem_fg", "offsetStart": 246, "offsetEnd": 258}, "context": "In our experiments, this model is used for several purposes: (a) as a language model Plm(hi) during N-best rescoring (see eq. ( 1)); (b) inside BERTalsem to represent the language model score Plm(hi) of each hypothesis (see figure 1); (c) inside BERTalsem_fg to compute the score of each word token Pgpt2(tok) of each hypothesis; (d) inside P-BERTalsem to compute Plm(h|prev_sent). ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9999637603759766}, "created": {"value": false, "score": 1.3113021850585938e-06}, "shared": {"value": false, "score": 5.960464477539062e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999637603759766}, "created": {"value": false, "score": 1.3113021850585938e-06}, "shared": {"value": false, "score": 5.960464477539062e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "BERTalsem", "normalizedForm": "BERTalsem", "offsetStart": 255, "offsetEnd": 264}, "context": "Configurations with CNN and bi-LSTM models are shown; (b) P-BERTalsem with GPT-2 gives the results for the approach taking into account the previous sentence; (c) P-BERTalsem-fg with GPT-2: the combined model gives no additional improvement compared to P-BERTalsem with GPT-2 and the results are not presented in this paper.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9964569211006165}, "created": {"value": false, "score": 1.6689300537109375e-06}, "shared": {"value": false, "score": 7.152557373046875e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.999982476234436}, "created": {"value": true, "score": 0.9935000538825989}, "shared": {"value": false, "score": 9.5367431640625e-07}}, "references": [{"label": "(Fohr and Illina, 2021)", "normalizedForm": "Fohr and Illina, 2021", "refKey": 4}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "P-BERTalsem", "normalizedForm": "P-BERTalsem", "offsetStart": 341, "offsetEnd": 352}, "context": "In our experiments, this model is used for several purposes: (a) as a language model Plm(hi) during N-best rescoring (see eq. ( 1)); (b) inside BERTalsem to represent the language model score Plm(hi) of each hypothesis (see figure 1); (c) inside BERTalsem_fg to compute the score of each word token Pgpt2(tok) of each hypothesis; (d) inside P-BERTalsem to compute Plm(h|prev_sent). ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9999637603759766}, "created": {"value": false, "score": 1.3113021850585938e-06}, "shared": {"value": false, "score": 5.960464477539062e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999637603759766}, "created": {"value": false, "score": 1.3113021850585938e-06}, "shared": {"value": false, "score": 5.960464477539062e-07}}}], "references": [{"refKey": 4, "tei": "<biblStruct xml:id=\"b4\">\n\t<analytic>\n\t\t<title level=\"a\" type=\"main\">BERT-Based Semantic Model for Rescoring N-Best Speech Recognition List</title>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Dominique</forename><surname>Fohr</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Irina</forename><surname>Illina</surname></persName>\n\t\t</author>\n\t\t<idno type=\"DOI\">10.21437/interspeech.2021-313</idno>\n\t</analytic>\n\t<monogr>\n\t\t<title level=\"m\">Interspeech 2021</title>\n\t\t<imprint>\n\t\t\t<publisher>ISCA</publisher>\n\t\t\t<date type=\"published\" when=\"2021-08-30\">2021</date>\n\t\t\t<biblScope unit=\"page\" from=\"1867\" to=\"1871\" />\n\t\t</imprint>\n\t</monogr>\n</biblStruct>\n"}], "runtime": 7315, "id": "65eaca40ba297bcec9e46b30383566128422e3c5", "metadata": {"id": "65eaca40ba297bcec9e46b30383566128422e3c5"}, "original_file_path": "../../datalake/Samuel/SOFTware-Sync/downloads/xml/hal-03965397.grobid.tei.xml", "file_name": "hal-03965397.grobid.tei.xml"}