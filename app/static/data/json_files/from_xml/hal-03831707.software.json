{"application": "software-mentions", "version": "0.8.0", "date": "2024-10-07T11:46+0000", "md5": "241D53DBBAA56B19D06B5B09A28BEB40", "mentions": [{"type": "software", "software-type": "software", "software-name": {"rawForm": "HuBERT", "normalizedForm": "HuBERT", "offsetStart": 0, "offsetEnd": 6}, "context": "HuBERT features are better than CPC features in most cases, and seem to benefit from more clusters than CPC features. ", "mentionContextAttributes": {"used": {"value": false, "score": 9.512901306152344e-05}, "created": {"value": false, "score": 7.30752944946289e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.999762237071991}, "created": {"value": true, "score": 0.997262716293335}, "shared": {"value": false, "score": 4.76837158203125e-07}}, "references": [{"label": "[7]", "normalizedForm": "[7]", "refKey": 7}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "sSIMI", "normalizedForm": "sSIMI", "offsetStart": 3, "offsetEnd": 8}, "context": "d) sSIMI similarity metrics: Given a pair of words (e.g., 'happy'-'joyful'), the model has to compute a similarity score based on their representations.", "mentionContextAttributes": {"used": {"value": false, "score": 0.023881971836090088}, "created": {"value": false, "score": 2.4080276489257812e-05}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9941725134849548}, "created": {"value": false, "score": 0.0050046443939208984}, "shared": {"value": false, "score": 5.960464477539062e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "sWUGGY", "normalizedForm": "sWUGGY", "offsetStart": 3, "offsetEnd": 9}, "context": "b) sWUGGY spot-the-word metrics: Given a pair of a word and a similar non-word (e.g., 'brick'-'blick'), the model has to tell which is the word based on their probability.", "mentionContextAttributes": {"used": {"value": false, "score": 0.005363285541534424}, "created": {"value": false, "score": 1.4662742614746094e-05}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999239444732666}, "created": {"value": false, "score": 0.0002697110176086426}, "shared": {"value": false, "score": 5.960464477539062e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "sBLIMP", "normalizedForm": "sBLIMP", "offsetStart": 3, "offsetEnd": 9}, "context": "c) sBLIMP acceptability metrics: Given a linguistic minimal sentence pair of matched grammatical and ungrammatical sentences (e.g., 'he loves it'-'he love it'), the model has to tell which is the grammatical sentence.", "mentionContextAttributes": {"used": {"value": false, "score": 0.012781381607055664}, "created": {"value": false, "score": 2.467632293701172e-05}, "shared": {"value": false, "score": 4.76837158203125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.998274564743042}, "created": {"value": false, "score": 2.467632293701172e-05}, "shared": {"value": false, "score": 5.960464477539062e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "HuBERT", "normalizedForm": "HuBERT", "offsetStart": 4, "offsetEnd": 10}, "context": "The HuBERT model is trained iteratively, using clustering units from features of previous iteration as the teacher. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.04038357734680176}, "created": {"value": false, "score": 0.03151148557662964}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.999762237071991}, "created": {"value": true, "score": 0.997262716293335}, "shared": {"value": false, "score": 4.76837158203125e-07}}, "references": [{"label": "[7]", "normalizedForm": "[7]", "refKey": 7}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "HuBERT", "normalizedForm": "HuBERT", "offsetStart": 4, "offsetEnd": 10}, "context": "The HuBERT model performs surprisingly well, approaching our best model on the language model tasks. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.00020366907119750977}, "created": {"value": false, "score": 0.006932377815246582}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.999762237071991}, "created": {"value": true, "score": 0.997262716293335}, "shared": {"value": false, "score": 4.76837158203125e-07}}, "references": [{"label": "[7]", "normalizedForm": "[7]", "refKey": 7}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "HuBERT", "normalizedForm": "HuBERT", "offsetStart": 8, "offsetEnd": 14}, "context": "[6] and HuBERT [7].", "mentionContextAttributes": {"used": {"value": true, "score": 0.9983305335044861}, "created": {"value": false, "score": 5.602836608886719e-06}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.999762237071991}, "created": {"value": true, "score": 0.997262716293335}, "shared": {"value": false, "score": 4.76837158203125e-07}}, "references": [{"label": "[7]", "normalizedForm": "[7]", "refKey": 7, "offsetStart": 5622, "offsetEnd": 5625}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "HuBERT", "normalizedForm": "HuBERT", "offsetStart": 8, "offsetEnd": 14}, "context": "For the HuBERT features, we train our own HuBERT base model as described in Section IV-D, we then take the features from layer 12 of the Transformer Encoder after the 2nd iteration, which have the best ABX (cf.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9809141159057617}, "created": {"value": false, "score": 0.0015187859535217285}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.999762237071991}, "created": {"value": true, "score": 0.997262716293335}, "shared": {"value": false, "score": 4.76837158203125e-07}}, "references": [{"label": "[7]", "normalizedForm": "[7]", "refKey": 7}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "wav", "normalizedForm": "wav", "offsetStart": 9, "offsetEnd": 12}, "context": "However, wav2vec2.0", "mentionContextAttributes": {"used": {"value": true, "score": 0.9972473978996277}, "created": {"value": false, "score": 6.556510925292969e-06}, "shared": {"value": false, "score": 7.152557373046875e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9994296431541443}, "created": {"value": false, "score": 0.04924213886260986}, "shared": {"value": false, "score": 4.0531158447265625e-06}}, "references": [{"label": "[6]", "normalizedForm": "[6]", "refKey": 6}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "HuBERT", "normalizedForm": "HuBERT", "offsetStart": 13, "offsetEnd": 19}, "context": "We trained a HuBERT base model, which comprises a 7-layer CNN Encoder followed by a 12-layer Transformer Encoder, on the Librispeech 960h dataset for 3 iterations.", "mentionContextAttributes": {"used": {"value": true, "score": 0.999762237071991}, "created": {"value": false, "score": 0.09623193740844727}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.999762237071991}, "created": {"value": true, "score": 0.997262716293335}, "shared": {"value": false, "score": 4.76837158203125e-07}}, "references": [{"label": "[7]", "normalizedForm": "[7]", "refKey": 7}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "HuBERT", "normalizedForm": "HuBERT", "offsetStart": 16, "offsetEnd": 22}, "context": "We evaluate the HuBERT model on the zero-shots metrics and compare the results with our trained BERT models.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9992658495903015}, "created": {"value": false, "score": 9.250640869140625e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.999762237071991}, "created": {"value": true, "score": 0.997262716293335}, "shared": {"value": false, "score": 4.76837158203125e-07}}, "references": [{"label": "[7]", "normalizedForm": "[7]", "refKey": 7}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "HuBERT", "normalizedForm": "HuBERT", "offsetStart": 17, "offsetEnd": 26}, "context": "We also evaluate HuBERT [7], a single model trained from raw waveform with discrete targets, on these metrics and compare the results with our best models.", "mentionContextAttributes": {"used": {"value": true, "score": 0.752685546875}, "created": {"value": false, "score": 3.933906555175781e-06}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.999762237071991}, "created": {"value": true, "score": 0.997262716293335}, "shared": {"value": false, "score": 4.76837158203125e-07}}, "references": [{"label": "[7]", "normalizedForm": "[7]", "refKey": 7}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "sSIMI", "normalizedForm": "sSIMI", "offsetStart": 18, "offsetEnd": 23}, "context": "In this work, the sSIMI scores are weighted across different subsets according to their sizes and averaged across LibriSpeech and synthetic subsets to make it more accurate and consistent.", "mentionContextAttributes": {"used": {"value": true, "score": 0.7671396136283875}, "created": {"value": false, "score": 0.0050046443939208984}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9941725134849548}, "created": {"value": false, "score": 0.0050046443939208984}, "shared": {"value": false, "score": 5.960464477539062e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "HuBERT", "normalizedForm": "HuBERT", "offsetStart": 19, "offsetEnd": 25}, "context": "On the other hand, HuBERT discretizes fixed features obtained from a teacher model and uses these fixed discrete units as the target for the Transformer Encoder using a cross-entropy loss.", "mentionContextAttributes": {"used": {"value": false, "score": 0.00015366077423095703}, "created": {"value": false, "score": 2.014636993408203e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.999762237071991}, "created": {"value": true, "score": 0.997262716293335}, "shared": {"value": false, "score": 4.76837158203125e-07}}, "references": [{"label": "[7]", "normalizedForm": "[7]", "refKey": 7}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "HuBERT", "normalizedForm": "HuBERT", "offsetStart": 22, "offsetEnd": 28}, "context": "For models trained on HuBERT features (section IV-C), we vary M in {5, 10, 15, 20, 25} as the frame rate is 50Hz instead of 100Hz as for CPC.", "mentionContextAttributes": {"used": {"value": false, "score": 0.26202619075775146}, "created": {"value": false, "score": 1.3470649719238281e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.999762237071991}, "created": {"value": true, "score": 0.997262716293335}, "shared": {"value": false, "score": 4.76837158203125e-07}}, "references": [{"label": "[7]", "normalizedForm": "[7]", "refKey": 7}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Lib-riSpeech", "normalizedForm": "Lib-riSpeech", "offsetStart": 23, "offsetEnd": 39}, "context": "We train our models on Lib-riSpeech [17], an English corpus containing 1000 hours of read speech based on public domain audio books. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.02232658863067627}, "created": {"value": true, "score": 0.7057650089263916}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "documentContextAttributes": {"used": {"value": false, "score": 0.02232658863067627}, "created": {"value": true, "score": 0.7057650089263916}, "shared": {"value": false, "score": 3.5762786865234375e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "HuBERT", "normalizedForm": "HuBERT", "offsetStart": 25, "offsetEnd": 31}, "context": "Note though that because HuBERT requires a teacher that learns a discrete representation, the overall training of HuBERT is not end-toend, because the training of the teacher is not (in fact, requires several iterations). ", "mentionContextAttributes": {"used": {"value": false, "score": 0.00023192167282104492}, "created": {"value": false, "score": 1.6570091247558594e-05}, "shared": {"value": false, "score": 4.76837158203125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.999762237071991}, "created": {"value": true, "score": 0.997262716293335}, "shared": {"value": false, "score": 4.76837158203125e-07}}, "references": [{"label": "[7]", "normalizedForm": "[7]", "refKey": 7}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "LibriSpeech", "normalizedForm": "LibriSpeech", "offsetStart": 28, "offsetEnd": 39}, "context": "The models are validated on LibriSpeech dev-clean and dev-other subsets, comprising 10 hours of speech in total.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9907174706459045}, "created": {"value": false, "score": 4.6253204345703125e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999239444732666}, "created": {"value": false, "score": 0.0050046443939208984}, "shared": {"value": false, "score": 2.384185791015625e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "sSIMI", "normalizedForm": "sSIMI", "offsetStart": 29, "offsetEnd": 34}, "context": "c) Similarity Score: For the sSIMI metrics, we extract a fixed-length representation for each audio file by applying a pooling function (mean, max, min) over hidden features from one layer of the Transformer Encoder.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9812486171722412}, "created": {"value": false, "score": 1.049041748046875e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9941725134849548}, "created": {"value": false, "score": 0.0050046443939208984}, "shared": {"value": false, "score": 5.960464477539062e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "sWUGGY", "normalizedForm": "sWUGGY", "offsetStart": 31, "offsetEnd": 37}, "context": "b) Probability Estimation: For sWUGGY and sBLIMP metrics, we compute for each audio file a model-based pseudo log-probability (m-PLP) of the trained BERT model.", "mentionContextAttributes": {"used": {"value": true, "score": 0.998274564743042}, "created": {"value": false, "score": 4.0531158447265625e-06}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999239444732666}, "created": {"value": false, "score": 0.0002697110176086426}, "shared": {"value": false, "score": 5.960464477539062e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "wav", "normalizedForm": "wav", "offsetStart": 35, "offsetEnd": 38}, "context": "Our implementation is based on the wav2vec2.0 ", "mentionContextAttributes": {"used": {"value": true, "score": 0.6589698195457458}, "created": {"value": false, "score": 0.04924213886260986}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9994296431541443}, "created": {"value": false, "score": 0.04924213886260986}, "shared": {"value": false, "score": 4.0531158447265625e-06}}, "references": [{"label": "[6]", "normalizedForm": "[6]", "refKey": 6}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "HuBERT", "normalizedForm": "HuBERT", "offsetStart": 42, "offsetEnd": 48}, "context": "We see that the discretediscrete model on HuBERT Discrete Units (model 26) further improves the scores on all the metrics, confirming again our finding that it's better to train a discrete-discrete model when we have good quality units.", "mentionContextAttributes": {"used": {"value": false, "score": 0.003451049327850342}, "created": {"value": false, "score": 0.0008366107940673828}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.999762237071991}, "created": {"value": true, "score": 0.997262716293335}, "shared": {"value": false, "score": 4.76837158203125e-07}}, "references": [{"label": "[7]", "normalizedForm": "[7]", "refKey": 7}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "sBLIMP", "normalizedForm": "sBLIMP", "offsetStart": 42, "offsetEnd": 48}, "context": "b) Probability Estimation: For sWUGGY and sBLIMP metrics, we compute for each audio file a model-based pseudo log-probability (m-PLP) of the trained BERT model.", "mentionContextAttributes": {"used": {"value": true, "score": 0.998274564743042}, "created": {"value": false, "score": 4.0531158447265625e-06}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.998274564743042}, "created": {"value": false, "score": 2.467632293701172e-05}, "shared": {"value": false, "score": 5.960464477539062e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "HuBERT", "normalizedForm": "HuBERT", "offsetStart": 42, "offsetEnd": 48}, "context": "For the HuBERT features, we train our own HuBERT base model as described in Section IV-D, we then take the features from layer 12 of the Transformer Encoder after the 2nd iteration, which have the best ABX (cf.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9809141159057617}, "created": {"value": false, "score": 0.0015187859535217285}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.999762237071991}, "created": {"value": true, "score": 0.997262716293335}, "shared": {"value": false, "score": 4.76837158203125e-07}}, "references": [{"label": "[7]", "normalizedForm": "[7]", "refKey": 7}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "HuBERT", "normalizedForm": "HuBERT", "offsetStart": 43, "offsetEnd": 49}, "context": "This means that the Transformer Encoder of HuBERT acts as a language model as well. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.001605212688446045}, "created": {"value": false, "score": 0.00036072731018066406}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.999762237071991}, "created": {"value": true, "score": 0.997262716293335}, "shared": {"value": false, "score": 4.76837158203125e-07}}, "references": [{"label": "[7]", "normalizedForm": "[7]", "refKey": 7}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "HuBERT", "normalizedForm": "HuBERT", "offsetStart": 48, "offsetEnd": 54}, "context": "Architecturally, the Transformer Encoder of the HuBERT model is very similar to our model 13 (continuous input layer 0, discrete target layer 2, NLL-e loss) where they both take as input the continuous features of the CNN Encoder and predict discrete targets obtained from features of a higher level with a NLL-eloss.", "mentionContextAttributes": {"used": {"value": false, "score": 0.030671238899230957}, "created": {"value": false, "score": 7.104873657226562e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.999762237071991}, "created": {"value": true, "score": 0.997262716293335}, "shared": {"value": false, "score": 4.76837158203125e-07}}, "references": [{"label": "[7]", "normalizedForm": "[7]", "refKey": 7}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "wav", "normalizedForm": "wav", "offsetStart": 51, "offsetEnd": 54}, "context": "They found that training BERT model on discrete vq-wav2vec units is more effective for ASR.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9994296431541443}, "created": {"value": false, "score": 4.374980926513672e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9994296431541443}, "created": {"value": false, "score": 0.04924213886260986}, "shared": {"value": false, "score": 4.0531158447265625e-06}}, "references": [{"label": "[6]", "normalizedForm": "[6]", "refKey": 6}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "HuBERT", "normalizedForm": "HuBERT", "offsetStart": 54, "offsetEnd": 60}, "context": "We also include the discretediscrete model trained on HuBERT Discrete Units (500 units), which was reported to have the best LM scores in section IV-C.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9978194236755371}, "created": {"value": false, "score": 0.0027231574058532715}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.999762237071991}, "created": {"value": true, "score": 0.997262716293335}, "shared": {"value": false, "score": 4.76837158203125e-07}}, "references": [{"label": "[7]", "normalizedForm": "[7]", "refKey": 7}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "HuBERT", "normalizedForm": "HuBERT", "offsetStart": 69, "offsetEnd": 75}, "context": "We see that as soon as the discrete targets have better quality, the HuBERT model manages to have better results on spoken language modeling metrics. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.0010312795639038086}, "created": {"value": false, "score": 0.0016515254974365234}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.999762237071991}, "created": {"value": true, "score": 0.997262716293335}, "shared": {"value": false, "score": 4.76837158203125e-07}}, "references": [{"label": "[7]", "normalizedForm": "[7]", "refKey": 7}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "LibriSpeech", "normalizedForm": "LibriSpeech", "offsetStart": 75, "offsetEnd": 86}, "context": "b) Metrics Datasets: The metrics datasets are either extracted sounds from LibriSpeech (ABX, sSIMI) or synthesised using Google API1 (sWUGGY, sBLIMP, sSIMI). ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9495762586593628}, "created": {"value": false, "score": 8.225440979003906e-06}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999239444732666}, "created": {"value": false, "score": 0.0050046443939208984}, "shared": {"value": false, "score": 2.384185791015625e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "wav", "normalizedForm": "wav", "offsetStart": 77, "offsetEnd": 80}, "context": "They provided baseline systems consisting of a discrete speech encoder (CPC, wav2vec 2.0, HuBERT), a generative language model (GPTlike model), and a speech decoder (Tacotron-2, [16]).", "mentionContextAttributes": {"used": {"value": false, "score": 0.04132091999053955}, "created": {"value": false, "score": 0.00030291080474853516}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9994296431541443}, "created": {"value": false, "score": 0.04924213886260986}, "shared": {"value": false, "score": 4.0531158447265625e-06}}, "references": [{"label": "[6]", "normalizedForm": "[6]", "refKey": 6}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "LibriSpeech", "normalizedForm": "LibriSpeech", "offsetStart": 77, "offsetEnd": 88}, "context": "The datasets containing words or sentences were filtered to only contain the LibriSpeech vocabulary (except sWUGGY non-words), and are split into dev and test sets. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9999239444732666}, "created": {"value": false, "score": 3.933906555175781e-06}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999239444732666}, "created": {"value": false, "score": 0.0050046443939208984}, "shared": {"value": false, "score": 2.384185791015625e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "HuBERT", "normalizedForm": "HuBERT", "offsetStart": 83, "offsetEnd": 89}, "context": "On the basis of this study, we train a language model on the discrete units of the HuBERT features, reaching new state-of-the-art results in the lexical, syntactic and semantic metrics of the Zero Resource Speech Challenge 2021 (Track 1 -Speech Only).", "mentionContextAttributes": {"used": {"value": false, "score": 0.0024513602256774902}, "created": {"value": true, "score": 0.997262716293335}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.999762237071991}, "created": {"value": true, "score": 0.997262716293335}, "shared": {"value": false, "score": 4.76837158203125e-07}}, "references": [{"label": "[7]", "normalizedForm": "[7]", "refKey": 7}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "HuBERT", "normalizedForm": "HuBERT", "offsetStart": 84, "offsetEnd": 90}, "context": "To support our hypothesis, we run kmeans on the continuous features of both CPC and HuBERT models, and vary k to be 20, 50, 100, 200, 500, 1000, and 2000, after which we train a discrete-discrete BERT model.", "mentionContextAttributes": {"used": {"value": true, "score": 0.8507869839668274}, "created": {"value": true, "score": 0.71209317445755}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.999762237071991}, "created": {"value": true, "score": 0.997262716293335}, "shared": {"value": false, "score": 4.76837158203125e-07}}, "references": [{"label": "[7]", "normalizedForm": "[7]", "refKey": 7}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "LibriSpeech", "normalizedForm": "LibriSpeech", "offsetStart": 88, "offsetEnd": 99}, "context": "). Following[23], we train a speaker classifier in the following way: We randomly split LibriSpeech dev-clean utterances into train/valid/test (80%/10%/10%) sets and train a two-layer", "mentionContextAttributes": {"used": {"value": true, "score": 0.9993137121200562}, "created": {"value": false, "score": 5.125999450683594e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999239444732666}, "created": {"value": false, "score": 0.0050046443939208984}, "shared": {"value": false, "score": 2.384185791015625e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "HuBERT", "normalizedForm": "HuBERT", "offsetStart": 90, "offsetEnd": 96}, "context": "They provided baseline systems consisting of a discrete speech encoder (CPC, wav2vec 2.0, HuBERT), a generative language model (GPTlike model), and a speech decoder (Tacotron-2, [16]).", "mentionContextAttributes": {"used": {"value": false, "score": 0.041320979595184326}, "created": {"value": false, "score": 0.00030291080474853516}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.999762237071991}, "created": {"value": true, "score": 0.997262716293335}, "shared": {"value": false, "score": 4.76837158203125e-07}}, "references": [{"label": "[7]", "normalizedForm": "[7]", "refKey": 7}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "wav", "normalizedForm": "wav", "offsetStart": 91, "offsetEnd": 94}, "context": "This improvement greatly comes from the reimplementation of the BERT model, which uses the wav2vec2 Transformer Encoder model 4 .", "mentionContextAttributes": {"used": {"value": false, "score": 0.003267645835876465}, "created": {"value": false, "score": 7.975101470947266e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9994296431541443}, "created": {"value": false, "score": 0.04924213886260986}, "shared": {"value": false, "score": 4.0531158447265625e-06}}, "references": [{"label": "[6]", "normalizedForm": "[6]", "refKey": 6}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "sSIMI", "normalizedForm": "sSIMI", "offsetStart": 93, "offsetEnd": 98}, "context": "b) Metrics Datasets: The metrics datasets are either extracted sounds from LibriSpeech (ABX, sSIMI) or synthesised using Google API1 (sWUGGY, sBLIMP, sSIMI). ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9495762586593628}, "created": {"value": false, "score": 8.225440979003906e-06}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9941725134849548}, "created": {"value": false, "score": 0.0050046443939208984}, "shared": {"value": false, "score": 5.960464477539062e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "HuBERT", "normalizedForm": "HuBERT", "offsetStart": 93, "offsetEnd": 99}, "context": "\u2022 We show that a self-supervised model trained with a MLM objective on discrete targets like HuBERT achieves very good results on spoken language modeling metrics, showing that it can learn not only acoustic but also highlevel linguistic information.", "mentionContextAttributes": {"used": {"value": false, "score": 0.00550311803817749}, "created": {"value": false, "score": 0.07622230052947998}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.999762237071991}, "created": {"value": true, "score": 0.997262716293335}, "shared": {"value": false, "score": 4.76837158203125e-07}}, "references": [{"label": "[7]", "normalizedForm": "[7]", "refKey": 7}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "HuBERT", "normalizedForm": "HuBERT", "offsetStart": 97, "offsetEnd": 103}, "context": "Interestingly, the optimum number of units seems to be different across the model features (CPC, HuBERT) and linguistic levels.", "mentionContextAttributes": {"used": {"value": false, "score": 0.011072278022766113}, "created": {"value": false, "score": 1.4662742614746094e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.999762237071991}, "created": {"value": true, "score": 0.997262716293335}, "shared": {"value": false, "score": 4.76837158203125e-07}}, "references": [{"label": "[7]", "normalizedForm": "[7]", "refKey": 7}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "sWUGGY", "normalizedForm": "sWUGGY", "offsetStart": 108, "offsetEnd": 114}, "context": "The datasets containing words or sentences were filtered to only contain the LibriSpeech vocabulary (except sWUGGY non-words), and are split into dev and test sets. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9999239444732666}, "created": {"value": false, "score": 3.933906555175781e-06}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999239444732666}, "created": {"value": false, "score": 0.0002697110176086426}, "shared": {"value": false, "score": 5.960464477539062e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "HuBERT", "normalizedForm": "HuBERT", "offsetStart": 114, "offsetEnd": 120}, "context": "Note though that because HuBERT requires a teacher that learns a discrete representation, the overall training of HuBERT is not end-toend, because the training of the teacher is not (in fact, requires several iterations). ", "mentionContextAttributes": {"used": {"value": false, "score": 0.00023192167282104492}, "created": {"value": false, "score": 1.6570091247558594e-05}, "shared": {"value": false, "score": 4.76837158203125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.999762237071991}, "created": {"value": true, "score": 0.997262716293335}, "shared": {"value": false, "score": 4.76837158203125e-07}}, "references": [{"label": "[7]", "normalizedForm": "[7]", "refKey": 7}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "LibriSpeech", "normalizedForm": "LibriSpeech", "offsetStart": 114, "offsetEnd": 125}, "context": "In this work, the sSIMI scores are weighted across different subsets according to their sizes and averaged across LibriSpeech and synthetic subsets to make it more accurate and consistent. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.7671396136283875}, "created": {"value": false, "score": 0.0050046443939208984}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999239444732666}, "created": {"value": false, "score": 0.0050046443939208984}, "shared": {"value": false, "score": 2.384185791015625e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "wav", "normalizedForm": "wav", "offsetStart": 120, "offsetEnd": 123}, "context": "Finally, our work is mostly similar to [12], where they compare BERT models training on discrete units obtained from vq-wav2vec [13] and continuous features obtained from wav2vec [14] on the ASR task.", "mentionContextAttributes": {"used": {"value": false, "score": 0.3878764510154724}, "created": {"value": false, "score": 0.03713333606719971}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9994296431541443}, "created": {"value": false, "score": 0.04924213886260986}, "shared": {"value": false, "score": 4.0531158447265625e-06}}, "references": [{"label": "[13]", "normalizedForm": "[13]", "refKey": 13, "offsetStart": 6303, "offsetEnd": 6307}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Google API", "normalizedForm": "Google API", "offsetStart": 121, "offsetEnd": 132}, "context": "b) Metrics Datasets: The metrics datasets are either extracted sounds from LibriSpeech (ABX, sSIMI) or synthesised using Google API1 (sWUGGY, sBLIMP, sSIMI). ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9495762586593628}, "created": {"value": false, "score": 8.225440979003906e-06}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9495762586593628}, "created": {"value": false, "score": 8.225440979003906e-06}, "shared": {"value": false, "score": 2.384185791015625e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "sWUGGY", "normalizedForm": "sWUGGY", "offsetStart": 134, "offsetEnd": 140}, "context": "b) Metrics Datasets: The metrics datasets are either extracted sounds from LibriSpeech (ABX, sSIMI) or synthesised using Google API1 (sWUGGY, sBLIMP, sSIMI). ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9495762586593628}, "created": {"value": false, "score": 8.225440979003906e-06}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999239444732666}, "created": {"value": false, "score": 0.0002697110176086426}, "shared": {"value": false, "score": 5.960464477539062e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "HuBERT", "normalizedForm": "HuBERT", "offsetStart": 135, "offsetEnd": 141}, "context": "We also show the possibility of learning high-level language properties of a self-supervised speech representation learning model like HuBERT. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.03800314664840698}, "created": {"value": true, "score": 0.9867710471153259}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.999762237071991}, "created": {"value": true, "score": 0.997262716293335}, "shared": {"value": false, "score": 4.76837158203125e-07}}, "references": [{"label": "[7]", "normalizedForm": "[7]", "refKey": 7}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "sBLIMP", "normalizedForm": "sBLIMP", "offsetStart": 142, "offsetEnd": 148}, "context": "b) Metrics Datasets: The metrics datasets are either extracted sounds from LibriSpeech (ABX, sSIMI) or synthesised using Google API1 (sWUGGY, sBLIMP, sSIMI). ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9495762586593628}, "created": {"value": false, "score": 8.225440979003906e-06}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.998274564743042}, "created": {"value": false, "score": 2.467632293701172e-05}, "shared": {"value": false, "score": 5.960464477539062e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "sSIMI", "normalizedForm": "sSIMI", "offsetStart": 150, "offsetEnd": 155}, "context": "b) Metrics Datasets: The metrics datasets are either extracted sounds from LibriSpeech (ABX, sSIMI) or synthesised using Google API1 (sWUGGY, sBLIMP, sSIMI). ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9495762586593628}, "created": {"value": false, "score": 8.225440979003906e-06}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9941725134849548}, "created": {"value": false, "score": 0.0050046443939208984}, "shared": {"value": false, "score": 5.960464477539062e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "wav", "normalizedForm": "wav", "offsetStart": 160, "offsetEnd": 163}, "context": "We experimentally show that discretization is essential for spoken language modeling, although 4 One main difference between the two Transformer models is that wav2vec2 uses a Convolutional Positional Embedding instead of the standard Sinusoidal Positional Embedding.", "mentionContextAttributes": {"used": {"value": false, "score": 0.0018880367279052734}, "created": {"value": false, "score": 0.0009759664535522461}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9994296431541443}, "created": {"value": false, "score": 0.04924213886260986}, "shared": {"value": false, "score": 4.0531158447265625e-06}}, "references": [{"label": "[6]", "normalizedForm": "[6]", "refKey": 6}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Tacotron", "normalizedForm": "Tacotron", "offsetStart": 166, "offsetEnd": 174}, "context": "They provided baseline systems consisting of a discrete speech encoder (CPC, wav2vec 2.0, HuBERT), a generative language model (GPTlike model), and a speech decoder (Tacotron-2, [16]). ", "mentionContextAttributes": {"used": {"value": false, "score": 0.041320979595184326}, "created": {"value": false, "score": 0.00030291080474853516}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": false, "score": 0.041320979595184326}, "created": {"value": false, "score": 0.00030291080474853516}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "references": [{"label": "[16]", "normalizedForm": "[16]", "refKey": 16, "offsetStart": 7572, "offsetEnd": 7576}, {"label": "[16]", "normalizedForm": "[16]", "refKey": 16, "offsetStart": 7572, "offsetEnd": 7576}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "wav", "normalizedForm": "wav", "offsetStart": 171, "offsetEnd": 174}, "context": "Finally, our work is mostly similar to [12], where they compare BERT models training on discrete units obtained from vq-wav2vec [13] and continuous features obtained from wav2vec [14] on the ASR task.", "mentionContextAttributes": {"used": {"value": false, "score": 0.3878764510154724}, "created": {"value": false, "score": 0.03713333606719971}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9994296431541443}, "created": {"value": false, "score": 0.04924213886260986}, "shared": {"value": false, "score": 4.0531158447265625e-06}}, "references": [{"label": "[14]", "normalizedForm": "[14]", "refKey": 14, "offsetStart": 6354, "offsetEnd": 6358}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "wav", "normalizedForm": "wav", "offsetStart": 174, "offsetEnd": 177}, "context": "Then, the ABX distance between two files is computed as the average angular distance of the representations along the 3 https://github.com/pytorch/fairseq/tree/main/examples/wav2vec", "mentionContextAttributes": {"used": {"value": true, "score": 0.9980195760726929}, "created": {"value": false, "score": 2.5033950805664062e-06}, "shared": {"value": false, "score": 4.0531158447265625e-06}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9994296431541443}, "created": {"value": false, "score": 0.04924213886260986}, "shared": {"value": false, "score": 4.0531158447265625e-06}}, "references": [{"label": "[6]", "normalizedForm": "[6]", "refKey": 6}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "HuBERT", "normalizedForm": "HuBERT", "offsetStart": 186, "offsetEnd": 192}, "context": "In addition to performing ABX on input and target features of the BERT models, we also compute the ABX error on the features extracted from hidden Transformer layers of the trained BERT/HuBERT models.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9978440999984741}, "created": {"value": false, "score": 2.956390380859375e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.999762237071991}, "created": {"value": true, "score": 0.997262716293335}, "shared": {"value": false, "score": 4.76837158203125e-07}}, "references": [{"label": "[7]", "normalizedForm": "[7]", "refKey": 7}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "sWUGGY", "normalizedForm": "sWUGGY", "offsetStart": 225, "offsetEnd": 231}, "context": "We evaluate our models with the ZeroSpeech 2021 Benchmark Metrics [9], consisting of 4 zero-shot tests probing for the quality of spoken language models at four linguistic levels: phonetic (Libri-light ABX metrics), lexical (sWUGGY spotthe-word metrics), syntactic (sBLIMP acceptability metrics) and semantic (sSIMI similarity metrics).", "mentionContextAttributes": {"used": {"value": true, "score": 0.9941725134849548}, "created": {"value": false, "score": 2.002716064453125e-05}, "shared": {"value": false, "score": 5.960464477539062e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999239444732666}, "created": {"value": false, "score": 0.0002697110176086426}, "shared": {"value": false, "score": 5.960464477539062e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "HuBERT", "normalizedForm": "HuBERT", "offsetStart": 235, "offsetEnd": 241}, "context": "We could simply employ a linear classification head at the output of the BERT model as usual (which we denote by linear NLL, or NLL-l) or force the BERT output features to be similar to the embedding vectors of the target units as for HuBERT (cf.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9990659356117249}, "created": {"value": false, "score": 4.863739013671875e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.999762237071991}, "created": {"value": true, "score": 0.997262716293335}, "shared": {"value": false, "score": 4.76837158203125e-07}}, "references": [{"label": "[7]", "normalizedForm": "[7]", "refKey": 7}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "sWUGGY", "normalizedForm": "sWUGGY", "offsetStart": 240, "offsetEnd": 246}, "context": "The results are reported in Table III We observe that using continuous input features from a different layer does reduce overfitting during training, which significantly improves the performances of the models on LM metrics, especially for sWUGGY scores.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9703801870346069}, "created": {"value": false, "score": 0.0002697110176086426}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999239444732666}, "created": {"value": false, "score": 0.0002697110176086426}, "shared": {"value": false, "score": 5.960464477539062e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "wav", "normalizedForm": "wav", "offsetStart": 257, "offsetEnd": 260}, "context": "II. RELATED WORK a) Discretization in Self-Supervised Approaches: Selfsupervised models for learning speech representation have become more and more popular as an effective pre-training method for downstream Automatic Speech Recognition (ASR) task, notably wav2vec2.0", "mentionContextAttributes": {"used": {"value": false, "score": 0.00016689300537109375}, "created": {"value": false, "score": 0.00013399124145507812}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9994296431541443}, "created": {"value": false, "score": 0.04924213886260986}, "shared": {"value": false, "score": 4.0531158447265625e-06}}, "references": [{"label": "[6]", "normalizedForm": "[6]", "refKey": 6, "offsetStart": 5607, "offsetEnd": 5610}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "sBLIMP", "normalizedForm": "sBLIMP", "offsetStart": 266, "offsetEnd": 272}, "context": "We evaluate our models with the ZeroSpeech 2021 Benchmark Metrics [9], consisting of 4 zero-shot tests probing for the quality of spoken language models at four linguistic levels: phonetic (Libri-light ABX metrics), lexical (sWUGGY spotthe-word metrics), syntactic (sBLIMP acceptability metrics) and semantic (sSIMI similarity metrics).", "mentionContextAttributes": {"used": {"value": true, "score": 0.9941725134849548}, "created": {"value": false, "score": 2.002716064453125e-05}, "shared": {"value": false, "score": 5.960464477539062e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.998274564743042}, "created": {"value": false, "score": 2.467632293701172e-05}, "shared": {"value": false, "score": 5.960464477539062e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "sSIMI", "normalizedForm": "sSIMI", "offsetStart": 310, "offsetEnd": 315}, "context": "We evaluate our models with the ZeroSpeech 2021 Benchmark Metrics [9], consisting of 4 zero-shot tests probing for the quality of spoken language models at four linguistic levels: phonetic (Libri-light ABX metrics), lexical (sWUGGY spotthe-word metrics), syntactic (sBLIMP acceptability metrics) and semantic (sSIMI similarity metrics).", "mentionContextAttributes": {"used": {"value": true, "score": 0.9941725134849548}, "created": {"value": false, "score": 2.002716064453125e-05}, "shared": {"value": false, "score": 5.960464477539062e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9941725134849548}, "created": {"value": false, "score": 0.0050046443939208984}, "shared": {"value": false, "score": 5.960464477539062e-07}}}], "references": [{"refKey": 7, "tei": "<biblStruct xml:id=\"b7\">\n\t<analytic>\n\t\t<title level=\"a\" type=\"main\">HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units</title>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Wei-Ning</forename><surname>Hsu</surname></persName>\n\t\t\t<idno type=\"ORCID\">0000-0001-5546-5217</idno>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Benjamin</forename><surname>Bolte</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Yao-Hung Hubert</forename><surname>Tsai</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Kushal</forename><surname>Lakhotia</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Ruslan</forename><surname>Salakhutdinov</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Abdelrahman</forename><surname>Mohamed</surname></persName>\n\t\t</author>\n\t\t<idno type=\"DOI\">10.1109/taslp.2021.3122291</idno>\n\t</analytic>\n\t<monogr>\n\t\t<title level=\"j\">IEEE/ACM Transactions on Audio, Speech, and Language Processing</title>\n\t\t<title level=\"j\" type=\"abbrev\">IEEE/ACM Trans. Audio Speech Lang. Process.</title>\n\t\t<idno type=\"ISSN\">2329-9290</idno>\n\t\t<idno type=\"ISSNe\">2329-9304</idno>\n\t\t<imprint>\n\t\t\t<biblScope unit=\"volume\">29</biblScope>\n\t\t\t<biblScope unit=\"page\" from=\"3451\" to=\"3460\" />\n\t\t\t<date type=\"published\" when=\"2021\">2021</date>\n\t\t\t<publisher>Institute of Electrical and Electronics Engineers (IEEE)</publisher>\n\t\t</imprint>\n\t</monogr>\n</biblStruct>\n"}, {"refKey": 6, "tei": "<biblStruct xml:id=\"b6\">\n\t<analytic>\n\t\t<title level=\"a\" type=\"main\">wav2vec 2.0: A framework for self-supervised learning of speech representations</title>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">A</forename><surname>Baevski</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Y</forename><surname>Zhou</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">A</forename><surname>Mohamed</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">M</forename><surname>Auli</surname></persName>\n\t\t</author>\n\t</analytic>\n\t<monogr>\n\t\t<title level=\"m\">Advances in Neural Information Processing Systems</title>\n\t\t<editor>\n\t\t\t<persName><forename type=\"first\">H</forename><surname>Larochelle</surname></persName>\n\t\t</editor>\n\t\t<editor>\n\t\t\t<persName><forename type=\"first\">M</forename><surname>Ranzato</surname></persName>\n\t\t</editor>\n\t\t<editor>\n\t\t\t<persName><forename type=\"first\">R</forename><surname>Hadsell</surname></persName>\n\t\t</editor>\n\t\t<editor>\n\t\t\t<persName><forename type=\"first\">M</forename><forename type=\"middle\">F</forename><surname>Balcan</surname></persName>\n\t\t</editor>\n\t\t<editor>\n\t\t\t<persName><forename type=\"first\">H</forename><surname>Lin</surname></persName>\n\t\t</editor>\n\t\t<imprint>\n\t\t\t<publisher>Curran Associates, Inc</publisher>\n\t\t\t<date>2020</date>\n\t\t\t<biblScope unit=\"volume\">33</biblScope>\n\t\t\t<biblScope unit=\"page\">449</biblScope>\n\t\t</imprint>\n\t</monogr>\n</biblStruct>\n"}, {"refKey": 13, "tei": "<biblStruct xml:id=\"b13\">\n\t<analytic>\n\t\t<title level=\"a\" type=\"main\">vq-wav2vec: Self-supervised learning of discrete speech representations</title>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">A</forename><surname>Baevski</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">S</forename><surname>Schneider</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">M</forename><surname>Auli</surname></persName>\n\t\t</author>\n\t</analytic>\n\t<monogr>\n\t\t<title level=\"m\">8th International Conference on Learning Representations, ICLR 2020</title>\n\t\t<imprint>\n\t\t\t<date>April 26-30, 2020. 2020</date>\n\t\t</imprint>\n\t</monogr>\n</biblStruct>\n"}, {"refKey": 16, "tei": "<biblStruct xml:id=\"b16\">\n\t<analytic>\n\t\t<title level=\"a\" type=\"main\">Natural TTS Synthesis by Conditioning Wavenet on MEL Spectrogram Predictions</title>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Jonathan</forename><surname>Shen</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Ruoming</forename><surname>Pang</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Ron</forename><forename type=\"middle\">J</forename><surname>Weiss</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Mike</forename><surname>Schuster</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Navdeep</forename><surname>Jaitly</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Zongheng</forename><surname>Yang</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Zhifeng</forename><surname>Chen</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Yu</forename><surname>Zhang</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Yuxuan</forename><surname>Wang</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Rj</forename><surname>Skerrv-Ryan</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Rif</forename><forename type=\"middle\">A</forename><surname>Saurous</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Yannis</forename><surname>Agiomvrgiannakis</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Yonghui</forename><surname>Wu</surname></persName>\n\t\t</author>\n\t\t<idno type=\"DOI\">10.1109/icassp.2018.8461368</idno>\n\t</analytic>\n\t<monogr>\n\t\t<title level=\"m\">2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>\n\t\t<imprint>\n\t\t\t<publisher>IEEE</publisher>\n\t\t\t<date type=\"published\" when=\"2018-04\">2018</date>\n\t\t\t<biblScope unit=\"page\">4783</biblScope>\n\t\t</imprint>\n\t</monogr>\n</biblStruct>\n"}, {"refKey": 14, "tei": "<biblStruct xml:id=\"b14\">\n\t<analytic>\n\t\t<title level=\"a\" type=\"main\">wav2vec: Unsupervised Pre-Training for Speech Recognition</title>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Steffen</forename><surname>Schneider</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Alexei</forename><surname>Baevski</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Ronan</forename><surname>Collobert</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Michael</forename><surname>Auli</surname></persName>\n\t\t</author>\n\t\t<idno type=\"DOI\">10.21437/interspeech.2019-1873</idno>\n\t</analytic>\n\t<monogr>\n\t\t<title level=\"m\">Interspeech 2019</title>\n\t\t<editor>\n\t\t\t<persName><forename type=\"first\">G</forename><surname>Kubin</surname></persName>\n\t\t</editor>\n\t\t<editor>\n\t\t\t<persName><forename type=\"first\">Z</forename><surname>Kacic</surname></persName>\n\t\t</editor>\n\t\t<imprint>\n\t\t\t<publisher>ISCA</publisher>\n\t\t\t<date type=\"published\" when=\"2019-09-15\">15-19 September 2019. 2019</date>\n\t\t\t<biblScope unit=\"page\">3469</biblScope>\n\t\t</imprint>\n\t</monogr>\n</biblStruct>\n"}], "runtime": 14781, "id": "91ce6af67f4af44262e6cd2cda44f2aa546cd583", "metadata": {"id": "91ce6af67f4af44262e6cd2cda44f2aa546cd583"}, "original_file_path": "../../datalake/Samuel/SOFTware-Sync/downloads/xml/hal-03831707.grobid.tei.xml", "file_name": "hal-03831707.grobid.tei.xml"}