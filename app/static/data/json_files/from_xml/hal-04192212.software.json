{"application": "software-mentions", "version": "0.8.0", "date": "2024-10-07T12:49+0000", "md5": "4E076EE5D58B8A05CE458A4AEA18B55D", "mentions": [{"type": "software", "software-type": "software", "software-name": {"rawForm": "Pub-", "normalizedForm": "Pub", "offsetStart": 0, "offsetEnd": 4}, "context": "Pub-MedBERT, a model pretrained from scratch using biomedical data, achieves the second best performance when Bi-LSTM is used as classifier, being below SciBERT by around 5 %. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.0001647472381591797}, "created": {"value": false, "score": 5.9485435485839844e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": false, "score": 0.0001647472381591797}, "created": {"value": false, "score": 5.9485435485839844e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "SimplE", "normalizedForm": "SimplE", "offsetStart": 0, "offsetEnd": 6}, "context": "SimplE encodes the embeddings of the two entities h and t into \u20d7 h and \u20d7 t, respectively, by parameter sharing allowing to integrate the dependence between them into a relation vector \u20d7 v r and \u20d7 v r-1 for the inverse relation. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.0004658699035644531}, "created": {"value": false, "score": 5.7220458984375e-06}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999840259552002}, "created": {"value": false, "score": 2.574920654296875e-05}, "shared": {"value": false, "score": 3.5762786865234375e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "SciBERT", "normalizedForm": "SciBERT", "offsetStart": 0, "offsetEnd": 7}, "context": "SciBERT emerged as the highest performing model, presenting a slight improvement compared to baseline models. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.5387330055236816}, "created": {"value": false, "score": 7.009506225585938e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9997219443321228}, "created": {"value": false, "score": 0.03540271520614624}, "shared": {"value": false, "score": 1.0728836059570312e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "SciBERT", "normalizedForm": "SciBERT", "offsetStart": 0, "offsetEnd": 7}, "context": "SciBERT and PubMedBERT were pretrained from scratch using an unique vocabulary and include embeddings that are specific for in-domain words. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9993416666984558}, "created": {"value": false, "score": 1.52587890625e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9997219443321228}, "created": {"value": false, "score": 0.03540271520614624}, "shared": {"value": false, "score": 1.0728836059570312e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "BioBERT", "normalizedForm": "BioBERT", "offsetStart": 0, "offsetEnd": 7}, "context": "BioBERT and BioMedRoBERTa were pretrained starting from the BERT checkpoints, their vocabularies are built with general-domain texts (similar to BERT) as well as the initialization of the embeddings.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9925877451896667}, "created": {"value": false, "score": 7.987022399902344e-06}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9925877451896667}, "created": {"value": false, "score": 0.0003800392150878906}, "shared": {"value": false, "score": 1.0728836059570312e-06}}, "references": [{"label": "(Lee et al., 2020)", "normalizedForm": "Lee et al., 2020", "refKey": 24}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "BioBERT", "normalizedForm": "BioBERT", "offsetStart": 0, "offsetEnd": 7}, "context": "BioBERT (Lee et al., 2020) and BioMedRoBERTa (Gururangan et al., 2020) are some examples of BERT variants pretrained in the biomedical domain.", "mentionContextAttributes": {"used": {"value": false, "score": 0.003861725330352783}, "created": {"value": false, "score": 8.344650268554688e-06}, "shared": {"value": false, "score": 4.76837158203125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9925877451896667}, "created": {"value": false, "score": 0.0003800392150878906}, "shared": {"value": false, "score": 1.0728836059570312e-06}}, "references": [{"label": "(Lee et al., 2020)", "normalizedForm": "Lee et al., 2020", "refKey": 24, "offsetStart": 15288, "offsetEnd": 15306}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "SciBERT", "normalizedForm": "SciBERT", "offsetStart": 0, "offsetEnd": 7}, "context": "SciBERT-KG tr,r,ar , which integrates the KG embeddings of the trigger, the role and the argument, presents the highest F1-score.", "mentionContextAttributes": {"used": {"value": false, "score": 0.0004922151565551758}, "created": {"value": false, "score": 1.5139579772949219e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9997219443321228}, "created": {"value": false, "score": 0.03540271520614624}, "shared": {"value": false, "score": 1.0728836059570312e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "DeepEventMine", "normalizedForm": "DeepEventMine", "offsetStart": 0, "offsetEnd": 13}, "context": "DeepEventMine is a joint model based on BERT for event extraction that simultaneously detect event triggers, identify arguments and construct the final events. ", "mentionContextAttributes": {"used": {"value": false, "score": 8.96453857421875e-05}, "created": {"value": false, "score": 0.08289062976837158}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9762102365493774}, "created": {"value": false, "score": 0.08289062976837158}, "shared": {"value": false, "score": 2.384185791015625e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Deep-EventMine", "normalizedForm": "Deep-EventMine", "offsetStart": 0, "offsetEnd": 35}, "context": "Deep-EventMine (Trieu et al., 2020) is an end-to-end system for event extraction that consists in four main modules that identify the event triggers and the arguments. ", "mentionContextAttributes": {"used": {"value": false, "score": 5.6862831115722656e-05}, "created": {"value": false, "score": 0.0029967427253723145}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": false, "score": 5.6862831115722656e-05}, "created": {"value": false, "score": 0.0029967427253723145}, "shared": {"value": false, "score": 2.384185791015625e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "PubMedBERT", "normalizedForm": "PubMedBERT", "offsetStart": 12, "offsetEnd": 22}, "context": "SciBERT and PubMedBERT were pretrained from scratch using an unique vocabulary and include embeddings that are specific for in-domain words. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9993416666984558}, "created": {"value": false, "score": 1.52587890625e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9993416666984558}, "created": {"value": false, "score": 0.0003800392150878906}, "shared": {"value": false, "score": 1.0728836059570312e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "BioMedRoBERTa", "normalizedForm": "BioMedRoBERTa", "offsetStart": 12, "offsetEnd": 25}, "context": "BioBERT and BioMedRoBERTa were pretrained starting from the BERT checkpoints, their vocabularies are built with general-domain texts (similar to BERT) as well as the initialization of the embeddings.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9925877451896667}, "created": {"value": false, "score": 7.987022399902344e-06}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9925877451896667}, "created": {"value": false, "score": 2.3365020751953125e-05}, "shared": {"value": false, "score": 1.0728836059570312e-06}}, "references": [{"label": "(Gururangan et al., 2020)", "normalizedForm": "Gururangan et al., 2020", "refKey": 14}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "SciBERT", "normalizedForm": "SciBERT", "offsetStart": 18, "offsetEnd": 25}, "context": "Furthermore, when SciBERT is enriched with KG embeddings, especially those corresponding to the roles of arguments in events, it significantly improves the identification of biomedical arguments.", "mentionContextAttributes": {"used": {"value": false, "score": 6.699562072753906e-05}, "created": {"value": false, "score": 8.034706115722656e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9997219443321228}, "created": {"value": false, "score": 0.03540271520614624}, "shared": {"value": false, "score": 1.0728836059570312e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Adam", "normalizedForm": "Adam", "offsetStart": 20, "offsetEnd": 24}, "context": "It is trained using Adam optimizer, with a learning rate of 1e -4 , a weight decay value of 0.01, pair as loss function and 4 as margin value. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.687839925289154}, "created": {"value": false, "score": 6.222724914550781e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.687839925289154}, "created": {"value": false, "score": 6.222724914550781e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "BioBERT", "normalizedForm": "BioBERT", "offsetStart": 24, "offsetEnd": 31}, "context": "For this purpose, BERT, BioBERT, SciBERT, PubMedBERT, and BioMe-dRoBERTa are fine-tuned using two different classifiers, a linear layer and a Bidirectional Long Short Term Memory (Bi-LSTM) layer, to detect biomedical event triggers. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.09168922901153564}, "created": {"value": false, "score": 0.0003800392150878906}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9925877451896667}, "created": {"value": false, "score": 0.0003800392150878906}, "shared": {"value": false, "score": 1.0728836059570312e-06}}, "references": [{"label": "(Lee et al., 2020)", "normalizedForm": "Lee et al., 2020", "refKey": 24}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "SciBERT", "normalizedForm": "SciBERT", "offsetStart": 25, "offsetEnd": 32}, "context": "These last three models, SciBERT, PubMed-BERT and BERT, present similar characteristics. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.0001291036605834961}, "created": {"value": false, "score": 0.00019055604934692383}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9997219443321228}, "created": {"value": false, "score": 0.03540271520614624}, "shared": {"value": false, "score": 1.0728836059570312e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "BioMedRoBERTa", "normalizedForm": "BioMedRoBERTa", "offsetStart": 31, "offsetEnd": 44}, "context": "BioBERT (Lee et al., 2020) and BioMedRoBERTa (Gururangan et al., 2020) are some examples of BERT variants pretrained in the biomedical domain.", "mentionContextAttributes": {"used": {"value": false, "score": 0.003861725330352783}, "created": {"value": false, "score": 8.344650268554688e-06}, "shared": {"value": false, "score": 4.76837158203125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9925877451896667}, "created": {"value": false, "score": 2.3365020751953125e-05}, "shared": {"value": false, "score": 1.0728836059570312e-06}}, "references": [{"label": "(Gururangan et al., 2020)", "normalizedForm": "Gururangan et al., 2020", "refKey": 14, "offsetStart": 15325, "offsetEnd": 15350}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "SciBERT", "normalizedForm": "SciBERT", "offsetStart": 33, "offsetEnd": 40}, "context": "For this purpose, BERT, BioBERT, SciBERT, PubMedBERT, and BioMe-dRoBERTa are fine-tuned using two different classifiers, a linear layer and a Bidirectional Long Short Term Memory (Bi-LSTM) layer, to detect biomedical event triggers. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.09168922901153564}, "created": {"value": false, "score": 0.0003800392150878906}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9997219443321228}, "created": {"value": false, "score": 0.03540271520614624}, "shared": {"value": false, "score": 1.0728836059570312e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "GeForce", "normalizedForm": "GeForce", "offsetStart": 39, "offsetEnd": 46}, "publisher": {"rawForm": "NVIDIA", "normalizedForm": "NVIDIA", "offsetStart": 32, "offsetEnd": 38}, "context": "Experiments are developed using NVIDIA GeForce GTX 1080 Ti (11 GiB) GPU and GeForce RTX 2080 Ti (11 GiB) GPU. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9725897312164307}, "created": {"value": false, "score": 0.0036566853523254395}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9725897312164307}, "created": {"value": false, "score": 0.0036566853523254395}, "shared": {"value": false, "score": 2.384185791015625e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "PubMedBERT", "normalizedForm": "PubMedBERT", "offsetStart": 42, "offsetEnd": 52}, "context": "For this purpose, BERT, BioBERT, SciBERT, PubMedBERT, and BioMe-dRoBERTa are fine-tuned using two different classifiers, a linear layer and a Bidirectional Long Short Term Memory (Bi-LSTM) layer, to detect biomedical event triggers. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.09168922901153564}, "created": {"value": false, "score": 0.0003800392150878906}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9993416666984558}, "created": {"value": false, "score": 0.0003800392150878906}, "shared": {"value": false, "score": 1.0728836059570312e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "spanBERT", "normalizedForm": "spanBERT", "offsetStart": 43, "offsetEnd": 51}, "context": "They show that span-based pretraining from spanBERT provides an improvement in the recognition of ADEs. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.10446292161941528}, "created": {"value": false, "score": 0.00010144710540771484}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": false, "score": 0.10446292161941528}, "created": {"value": false, "score": 0.00010144710540771484}, "shared": {"value": false, "score": 1.1920928955078125e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "SimplE", "normalizedForm": "SimplE", "offsetStart": 46, "offsetEnd": 52}, "context": "Here, the KG embeddings were calculated using SimplE, a model based on the tensor factorization approach, canonical polyadic decomposition (Sorber et al., 2013). ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9999840259552002}, "created": {"value": false, "score": 8.58306884765625e-06}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999840259552002}, "created": {"value": false, "score": 2.574920654296875e-05}, "shared": {"value": false, "score": 3.5762786865234375e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "SciBERT", "normalizedForm": "SciBERT", "offsetStart": 51, "offsetEnd": 58}, "context": "This reveals that integrating the KG embeddings to SciBERT allows to improve the performance in the identification of biomedical arguments, especially when the KG embeddings of the role are used. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.00043588876724243164}, "created": {"value": false, "score": 0.00043839216232299805}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9997219443321228}, "created": {"value": false, "score": 0.03540271520614624}, "shared": {"value": false, "score": 1.0728836059570312e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "BioBERT", "normalizedForm": "BioBERT", "offsetStart": 55, "offsetEnd": 62}, "context": "The two models that present the lowest performance are BioBERT and BioMe-dRoBERTa. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.0004494786262512207}, "created": {"value": false, "score": 8.547306060791016e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9925877451896667}, "created": {"value": false, "score": 0.0003800392150878906}, "shared": {"value": false, "score": 1.0728836059570312e-06}}, "references": [{"label": "(Lee et al., 2020)", "normalizedForm": "Lee et al., 2020", "refKey": 24}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "PyTorch", "normalizedForm": "PyTorch", "offsetStart": 56, "offsetEnd": 63}, "context": "The fine-tuning of the transformer models are done with PyTorch, using the Transformers package and the models are taken from Hugging Face2 . ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9993840456008911}, "created": {"value": false, "score": 4.1484832763671875e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9993840456008911}, "created": {"value": false, "score": 0.00016319751739501953}, "shared": {"value": false, "score": 2.384185791015625e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "PubMedBERT", "normalizedForm": "PubMedBERT", "offsetStart": 56, "offsetEnd": 66}, "context": "However, BERT is pretrained only in the general domain, PubMedBERT in the biomedical domain, and SciBERT in both the general and biomedical domains. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.013338863849639893}, "created": {"value": false, "score": 1.800060272216797e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9993416666984558}, "created": {"value": false, "score": 0.0003800392150878906}, "shared": {"value": false, "score": 1.0728836059570312e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "SimplE", "normalizedForm": "SimplE", "offsetStart": 57, "offsetEnd": 63}, "context": "The highest results are presented in bold, obtained with SimplE, which is the model that we use to compute the KG embeddings that are integrated to SciBERT.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9995349645614624}, "created": {"value": false, "score": 2.574920654296875e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999840259552002}, "created": {"value": false, "score": 2.574920654296875e-05}, "shared": {"value": false, "score": 3.5762786865234375e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "BioMe-dRoBERTa", "normalizedForm": "BioMe-dRoBERTa", "offsetStart": 58, "offsetEnd": 72}, "context": "For this purpose, BERT, BioBERT, SciBERT, PubMedBERT, and BioMe-dRoBERTa are fine-tuned using two different classifiers, a linear layer and a Bidirectional Long Short Term Memory (Bi-LSTM) layer, to detect biomedical event triggers. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.09168922901153564}, "created": {"value": false, "score": 0.0003800392150878906}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": false, "score": 0.09168922901153564}, "created": {"value": false, "score": 0.0003800392150878906}, "shared": {"value": false, "score": 2.384185791015625e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "SciBERT", "normalizedForm": "SciBERT", "offsetStart": 60, "offsetEnd": 67}, "context": "The highest results are presented in bold, corresponding to SciBERT-Bi-LSTM, a model pretrained from scratch using biomedical and general data. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.19896990060806274}, "created": {"value": false, "score": 1.2993812561035156e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9997219443321228}, "created": {"value": false, "score": 0.03540271520614624}, "shared": {"value": false, "score": 1.0728836059570312e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "BioMe-dRoBERTa", "normalizedForm": "BioMe-dRoBERTa", "offsetStart": 67, "offsetEnd": 81}, "context": "The two models that present the lowest performance are BioBERT and BioMe-dRoBERTa. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.0004494786262512207}, "created": {"value": false, "score": 8.547306060791016e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": false, "score": 0.09168922901153564}, "created": {"value": false, "score": 0.0003800392150878906}, "shared": {"value": false, "score": 2.384185791015625e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "SciBERT", "normalizedForm": "SciBERT", "offsetStart": 69, "offsetEnd": 76}, "context": "Besides, we enrich the context of the domain-specific language model SciBERT using KG embeddings. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.029992520809173584}, "created": {"value": false, "score": 0.0041258931159973145}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9997219443321228}, "created": {"value": false, "score": 0.03540271520614624}, "shared": {"value": false, "score": 1.0728836059570312e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "SciBERT", "normalizedForm": "SciBERT", "offsetStart": 70, "offsetEnd": 77}, "context": "However, their F1-score is around 14 % lower than the one obtained by SciBERT-Bi-LSTM.", "mentionContextAttributes": {"used": {"value": true, "score": 0.823284924030304}, "created": {"value": false, "score": 3.0517578125e-05}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9997219443321228}, "created": {"value": false, "score": 0.03540271520614624}, "shared": {"value": false, "score": 1.0728836059570312e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "SciBERT", "normalizedForm": "SciBERT", "offsetStart": 70, "offsetEnd": 77}, "context": "By comparing the performance of the models, we found that fine-tuning SciBERT with a Bi-LSTM classifier is the best strategy to detect events in different biomedical domains. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9997219443321228}, "created": {"value": false, "score": 5.7816505432128906e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9997219443321228}, "created": {"value": false, "score": 0.03540271520614624}, "shared": {"value": false, "score": 1.0728836059570312e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "GeForce", "normalizedForm": "GeForce", "offsetStart": 76, "offsetEnd": 83}, "publisher": {"rawForm": "NVIDIA", "normalizedForm": "NVIDIA"}, "context": "Experiments are developed using NVIDIA GeForce GTX 1080 Ti (11 GiB) GPU and GeForce RTX 2080 Ti (11 GiB) GPU. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9725897312164307}, "created": {"value": false, "score": 0.0036566853523254395}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9725897312164307}, "created": {"value": false, "score": 0.0036566853523254395}, "shared": {"value": false, "score": 2.384185791015625e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "brat https", "normalizedForm": "brat https", "offsetStart": 81, "offsetEnd": 91}, "context": "The visualization of the annotated sentence is done using the visualization tool brat https://brat.nlplab.org/.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9977442026138306}, "created": {"value": false, "score": 2.205371856689453e-05}, "shared": {"value": false, "score": 0.07231038808822632}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9977442026138306}, "created": {"value": false, "score": 2.205371856689453e-05}, "shared": {"value": false, "score": 0.07231038808822632}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "SciBERT", "normalizedForm": "SciBERT", "offsetStart": 90, "offsetEnd": 97}, "context": "However, we observe that when we add the KG embeddings of the trigger and the argument in SciBERT-KG tr,ar , the performance improves by only 1 %, while when we add the KG embeddings of the role in SciBERT-KG r , the improvement is of 17 %.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9743484258651733}, "created": {"value": false, "score": 2.384185791015625e-06}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9997219443321228}, "created": {"value": false, "score": 0.03540271520614624}, "shared": {"value": false, "score": 1.0728836059570312e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "SciBERT", "normalizedForm": "SciBERT", "offsetStart": 97, "offsetEnd": 104}, "context": "However, BERT is pretrained only in the general domain, PubMedBERT in the biomedical domain, and SciBERT in both the general and biomedical domains. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.013338863849639893}, "created": {"value": false, "score": 1.800060272216797e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9997219443321228}, "created": {"value": false, "score": 0.03540271520614624}, "shared": {"value": false, "score": 1.0728836059570312e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "DeepEventMine", "normalizedForm": "DeepEventMine", "offsetStart": 97, "offsetEnd": 110}, "context": "The results reveal that our proposal achieves better F1-score than TEES-CNN by around 3 % and by DeepEventMine by around 1 After observing that SciBERT presents the best performance in trigger detection, we enrich its semantic information with the KG of the biomedical corpora for argument identification. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.4740965962409973}, "created": {"value": false, "score": 0.03540271520614624}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9762102365493774}, "created": {"value": false, "score": 0.08289062976837158}, "shared": {"value": false, "score": 2.384185791015625e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "BioBERT", "normalizedForm": "BioBERT", "offsetStart": 99, "offsetEnd": 124}, "context": "We compare BERT (Devlin et al., 2018), and four BERT variants pretrained in the biomedical domain, BioBERT (Lee et al., 2020), SciBERT (Beltagy et al., 2019), PubMedBERT (Gu et al., 2020), and BioMedRoBERTa (Gururangan et al., 2020) for the detection of event triggers. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9853752851486206}, "created": {"value": false, "score": 2.3365020751953125e-05}, "shared": {"value": false, "score": 1.0728836059570312e-06}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9925877451896667}, "created": {"value": false, "score": 0.0003800392150878906}, "shared": {"value": false, "score": 1.0728836059570312e-06}}, "references": [{"label": "(Lee et al., 2020)", "normalizedForm": "Lee et al., 2020", "refKey": 24}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "PyTorch", "normalizedForm": "PyTorch", "offsetStart": 106, "offsetEnd": 113}, "context": "The transformer  (Hagberg et al., 2008) and the model used to calculate KG embeddings is implemented with PyTorch, using the base code from (Islam et al., 2021). ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9962266683578491}, "created": {"value": false, "score": 0.00016319751739501953}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9993840456008911}, "created": {"value": false, "score": 0.00016319751739501953}, "shared": {"value": false, "score": 2.384185791015625e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "SciBERT", "normalizedForm": "SciBERT", "offsetStart": 126, "offsetEnd": 133}, "context": "MRR Hits@1 Hits@3 Hits@10 Table 6 compares the results of the different strategies followed to integrate the KG embeddings to SciBERT. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9961782693862915}, "created": {"value": false, "score": 1.5497207641601562e-06}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9997219443321228}, "created": {"value": false, "score": 0.03540271520614624}, "shared": {"value": false, "score": 1.0728836059570312e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "SciBERT", "normalizedForm": "SciBERT", "offsetStart": 127, "offsetEnd": 156}, "context": "We compare BERT (Devlin et al., 2018), and four BERT variants pretrained in the biomedical domain, BioBERT (Lee et al., 2020), SciBERT (Beltagy et al., 2019), PubMedBERT (Gu et al., 2020), and BioMedRoBERTa (Gururangan et al., 2020) for the detection of event triggers. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9853752851486206}, "created": {"value": false, "score": 2.3365020751953125e-05}, "shared": {"value": false, "score": 1.0728836059570312e-06}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9997219443321228}, "created": {"value": false, "score": 0.03540271520614624}, "shared": {"value": false, "score": 1.0728836059570312e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "SciBERT", "normalizedForm": "SciBERT", "offsetStart": 128, "offsetEnd": 135}, "context": "Then, for the second subtask we construct a knowledge graph (KG) from the biomedical corpora and integrate its KG embeddings to SciBERT to enrich its semantic information. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.06871366500854492}, "created": {"value": false, "score": 0.031236886978149414}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9997219443321228}, "created": {"value": false, "score": 0.03540271520614624}, "shared": {"value": false, "score": 1.0728836059570312e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Face", "normalizedForm": "Face", "offsetStart": 134, "offsetEnd": 139}, "context": "The fine-tuning of the transformer models are done with PyTorch, using the Transformers package and the models are taken from Hugging Face2 . ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9993840456008911}, "created": {"value": false, "score": 4.1484832763671875e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9993840456008911}, "created": {"value": false, "score": 4.1484832763671875e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "DeepEventMine", "normalizedForm": "DeepEventMine", "offsetStart": 142, "offsetEnd": 175}, "context": "Table 4 compares our model trained only on the CG corpus with two baseline event extraction models, TEES-CNN (Bj\u00f6rne and Salakoski, 2018) and DeepEventMine (Trieu et al., 2020). ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9762102365493774}, "created": {"value": false, "score": 3.0994415283203125e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9762102365493774}, "created": {"value": false, "score": 0.08289062976837158}, "shared": {"value": false, "score": 2.384185791015625e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "SciBERT", "normalizedForm": "SciBERT", "offsetStart": 144, "offsetEnd": 151}, "context": "The results reveal that our proposal achieves better F1-score than TEES-CNN by around 3 % and by DeepEventMine by around 1 After observing that SciBERT presents the best performance in trigger detection, we enrich its semantic information with the KG of the biomedical corpora for argument identification. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.4740965962409973}, "created": {"value": false, "score": 0.03540271520614624}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9997219443321228}, "created": {"value": false, "score": 0.03540271520614624}, "shared": {"value": false, "score": 1.0728836059570312e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "SciBERT", "normalizedForm": "SciBERT", "offsetStart": 148, "offsetEnd": 155}, "context": "The highest results are presented in bold, obtained with SimplE, which is the model that we use to compute the KG embeddings that are integrated to SciBERT.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9995349645614624}, "created": {"value": false, "score": 2.574920654296875e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9997219443321228}, "created": {"value": false, "score": 0.03540271520614624}, "shared": {"value": false, "score": 1.0728836059570312e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "SciBERT", "normalizedForm": "SciBERT", "offsetStart": 153, "offsetEnd": 160}, "context": "Pub-MedBERT, a model pretrained from scratch using biomedical data, achieves the second best performance when Bi-LSTM is used as classifier, being below SciBERT by around 5 %. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.0001647472381591797}, "created": {"value": false, "score": 5.9485435485839844e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9997219443321228}, "created": {"value": false, "score": 0.03540271520614624}, "shared": {"value": false, "score": 1.0728836059570312e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "PubMedBERT", "normalizedForm": "PubMedBERT", "offsetStart": 159, "offsetEnd": 186}, "context": "We compare BERT (Devlin et al., 2018), and four BERT variants pretrained in the biomedical domain, BioBERT (Lee et al., 2020), SciBERT (Beltagy et al., 2019), PubMedBERT (Gu et al., 2020), and BioMedRoBERTa (Gururangan et al., 2020) for the detection of event triggers. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9853752851486206}, "created": {"value": false, "score": 2.3365020751953125e-05}, "shared": {"value": false, "score": 1.0728836059570312e-06}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9993416666984558}, "created": {"value": false, "score": 0.0003800392150878906}, "shared": {"value": false, "score": 1.0728836059570312e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "SimplE", "normalizedForm": "SimplE", "offsetStart": 192, "offsetEnd": 198}, "context": "Table 5 compares five KG models used for the computation of these embeddings, TransE (Bordes et al., 2013), TransH (Wang et al., 2014), TransD (Ji et al., 2015), DistMult (Yang et al., 2014), SimplE (Kazemi andPoole, 2018).", "mentionContextAttributes": {"used": {"value": true, "score": 0.9612212777137756}, "created": {"value": false, "score": 5.0067901611328125e-06}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999840259552002}, "created": {"value": false, "score": 2.574920654296875e-05}, "shared": {"value": false, "score": 3.5762786865234375e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "BioMedRoBERTa", "normalizedForm": "BioMedRoBERTa", "offsetStart": 193, "offsetEnd": 232}, "context": "We compare BERT (Devlin et al., 2018), and four BERT variants pretrained in the biomedical domain, BioBERT (Lee et al., 2020), SciBERT (Beltagy et al., 2019), PubMedBERT (Gu et al., 2020), and BioMedRoBERTa (Gururangan et al., 2020) for the detection of event triggers. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9853752851486206}, "created": {"value": false, "score": 2.3365020751953125e-05}, "shared": {"value": false, "score": 1.0728836059570312e-06}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9925877451896667}, "created": {"value": false, "score": 2.3365020751953125e-05}, "shared": {"value": false, "score": 1.0728836059570312e-06}}, "references": [{"label": "(Gururangan et al., 2020)", "normalizedForm": "Gururangan et al., 2020", "refKey": 14}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "SciBERT", "normalizedForm": "SciBERT", "offsetStart": 198, "offsetEnd": 205}, "context": "However, we observe that when we add the KG embeddings of the trigger and the argument in SciBERT-KG tr,ar , the performance improves by only 1 %, while when we add the KG embeddings of the role in SciBERT-KG r , the improvement is of 17 %.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9743484258651733}, "created": {"value": false, "score": 2.384185791015625e-06}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9997219443321228}, "created": {"value": false, "score": 0.03540271520614624}, "shared": {"value": false, "score": 1.0728836059570312e-06}}}], "references": [{"refKey": 24, "tei": "<biblStruct xml:id=\"b24\">\n\t<analytic>\n\t\t<title level=\"a\" type=\"main\">BioBERT: a pre-trained biomedical language representation model for biomedical text mining</title>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Jinhyuk</forename><surname>Lee</surname></persName>\n\t\t\t<idno type=\"ORCID\">0000-0003-4972-239X</idno>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Wonjin</forename><surname>Yoon</surname></persName>\n\t\t\t<idno type=\"ORCID\">0000-0002-6435-548X</idno>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Sungdong</forename><surname>Kim</surname></persName>\n\t\t\t<idno type=\"ORCID\">0000-0002-0240-6210</idno>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Donghyeon</forename><surname>Kim</surname></persName>\n\t\t\t<idno type=\"ORCID\">0000-0002-8224-8354</idno>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Sunkyu</forename><surname>Kim</surname></persName>\n\t\t\t<idno type=\"ORCID\">0000-0002-0240-6210</idno>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Chan</forename><forename type=\"middle\">Ho</forename><surname>So</surname></persName>\n\t\t\t<idno type=\"ORCID\">0000-0001-7633-1074</idno>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Jaewoo</forename><surname>Kang</surname></persName>\n\t\t\t<idno type=\"ORCID\">0000-0001-6798-9106</idno>\n\t\t</author>\n\t\t<idno type=\"DOI\">10.1093/bioinformatics/btz682</idno>\n\t</analytic>\n\t<monogr>\n\t\t<title level=\"j\">Bioinformatics</title>\n\t\t<idno type=\"ISSN\">1367-4803</idno>\n\t\t<idno type=\"ISSNe\">1367-4811</idno>\n\t\t<imprint>\n\t\t\t<biblScope unit=\"volume\">36</biblScope>\n\t\t\t<biblScope unit=\"issue\">4</biblScope>\n\t\t\t<biblScope unit=\"page\" from=\"1234\" to=\"1240\" />\n\t\t\t<date type=\"published\" when=\"2019-09-10\">2020</date>\n\t\t\t<publisher>Oxford University Press (OUP)</publisher>\n\t\t</imprint>\n\t</monogr>\n</biblStruct>\n"}, {"refKey": 14, "tei": "<biblStruct xml:id=\"b14\">\n\t<analytic>\n\t\t<title level=\"a\" type=\"main\">Don\u2019t Stop Pretraining: Adapt Language Models to Domains and Tasks</title>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Suchin</forename><surname>Gururangan</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Ana</forename><surname>Marasovi\u0107</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Swabha</forename><surname>Swayamdipta</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Kyle</forename><surname>Lo</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Iz</forename><surname>Beltagy</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Doug</forename><surname>Downey</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Noah</forename><forename type=\"middle\">A</forename><surname>Smith</surname></persName>\n\t\t</author>\n\t\t<idno type=\"DOI\">10.18653/v1/2020.acl-main.740</idno>\n\t</analytic>\n\t<monogr>\n\t\t<title level=\"m\">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>\n\t\t<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>\n\t\t<imprint>\n\t\t\t<publisher>Association for Computational Linguistics</publisher>\n\t\t\t<date type=\"published\" when=\"2020\">2020</date>\n\t\t</imprint>\n\t</monogr>\n</biblStruct>\n"}], "runtime": 15012, "id": "24fbfbc68e35d805080c0fa2a4a4225db35952e1", "metadata": {"id": "24fbfbc68e35d805080c0fa2a4a4225db35952e1"}, "original_file_path": "../../datalake/Samuel/SOFTware-Sync/downloads/xml/hal-04192212.grobid.tei.xml", "file_name": "hal-04192212.grobid.tei.xml"}