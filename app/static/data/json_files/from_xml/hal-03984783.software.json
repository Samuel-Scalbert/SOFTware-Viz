{"application": "software-mentions", "version": "0.8.0", "date": "2024-10-07T12:44+0000", "md5": "57DA279FAB06B4F6AB41556A11BFC6DB", "mentions": [{"type": "software", "software-type": "software", "software-name": {"rawForm": "BioBERT", "normalizedForm": "BioBERT", "offsetStart": 0, "offsetEnd": 7}, "context": "BioBERT and BioMedRoBERTa were pre-trained starting from the BERT checkpoints, which means that their vocabularies are built with general-domain texts (similar to BERT) as well as the initialization of the embeddings. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9945969581604004}, "created": {"value": false, "score": 2.5510787963867188e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999889135360718}, "created": {"value": false, "score": 0.003447413444519043}, "shared": {"value": false, "score": 5.960464477539062e-07}}, "references": [{"label": "28,", "normalizedForm": "28", "refKey": 0}, {"label": "996;", "normalizedForm": "996", "refKey": 0}, {"label": "PubMedBERT,", "normalizedForm": "PubMedBERT", "refKey": 0}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "BioBERT", "normalizedForm": "BioBERT", "offsetStart": 0, "offsetEnd": 7}, "context": "BioBERT achieves the best scores in precision, recall and F1-score for this number of epochs, surpassing the rest of the models for at least 0.02, 0.05 and 0.03 points, for each metric respectively. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.0007459521293640137}, "created": {"value": false, "score": 1.609325408935547e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999889135360718}, "created": {"value": false, "score": 0.003447413444519043}, "shared": {"value": false, "score": 5.960464477539062e-07}}, "references": [{"label": "28,", "normalizedForm": "28", "refKey": 0}, {"label": "996;", "normalizedForm": "996", "refKey": 0}, {"label": "PubMedBERT,", "normalizedForm": "PubMedBERT", "refKey": 0}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "BioBERT", "normalizedForm": "BioBERT", "offsetStart": 0, "offsetEnd": 7}, "context": "BioBERT also presents the most important change in performance according to the number of epochs, improving its F1-score by around 0.03 points between 10 and 30 epochs, and 0.15 points between 30 and 100 epochs. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.0005529522895812988}, "created": {"value": false, "score": 5.793571472167969e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999889135360718}, "created": {"value": false, "score": 0.003447413444519043}, "shared": {"value": false, "score": 5.960464477539062e-07}}, "references": [{"label": "28,", "normalizedForm": "28", "refKey": 0}, {"label": "996;", "normalizedForm": "996", "refKey": 0}, {"label": "PubMedBERT,", "normalizedForm": "PubMedBERT", "refKey": 0}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "BioBERT", "normalizedForm": "BioBERT", "offsetStart": 0, "offsetEnd": 7}, "context": "BioBERT emerged as the best model from the evaluation presented here, showing that using a transformer model that is pretrained from the original model, BERT, and uses biomedical data for its training is useful for recognizing biomedical event triggers, if the training is done for enough number of epochs.", "mentionContextAttributes": {"used": {"value": false, "score": 0.057832181453704834}, "created": {"value": false, "score": 0.003447413444519043}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999889135360718}, "created": {"value": false, "score": 0.003447413444519043}, "shared": {"value": false, "score": 5.960464477539062e-07}}, "references": [{"label": "28,", "normalizedForm": "28", "refKey": 0}, {"label": "996;", "normalizedForm": "996", "refKey": 0}, {"label": "PubMedBERT,", "normalizedForm": "PubMedBERT", "refKey": 0}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "DeepEventMine", "normalizedForm": "DeepEventMine", "offsetStart": 0, "offsetEnd": 34}, "context": "DeepEventMine (Trieu et al., 2020) is an end-toend system for event extraction that consists on four main modules; BERT model, trigger and entity detection and classification, relation extraction and event identification. ", "mentionContextAttributes": {"used": {"value": false, "score": 6.258487701416016e-05}, "created": {"value": false, "score": 0.0014709234237670898}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": false, "score": 6.258487701416016e-05}, "created": {"value": false, "score": 0.0014709234237670898}, "shared": {"value": false, "score": 2.384185791015625e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Adam", "normalizedForm": "Adam", "offsetStart": 1, "offsetEnd": 5}, "context": "'Adam' was used as optimizer and 'gelu' as activation function. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9997379183769226}, "created": {"value": false, "score": 1.3828277587890625e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9997379183769226}, "created": {"value": false, "score": 1.3828277587890625e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "BioMedRoBERTa", "normalizedForm": "BioMedRoBERTa", "offsetStart": 12, "offsetEnd": 25}, "context": "BioBERT and BioMedRoBERTa were pre-trained starting from the BERT checkpoints, which means that their vocabularies are built with general-domain texts (similar to BERT) as well as the initialization of the embeddings. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9945969581604004}, "created": {"value": false, "score": 2.5510787963867188e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9983633160591125}, "created": {"value": false, "score": 3.719329833984375e-05}, "shared": {"value": false, "score": 5.960464477539062e-07}}, "references": [{"label": "265.", "normalizedForm": "265", "refKey": 0}, {"label": "50,", "normalizedForm": "50", "refKey": 0}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "SciBERT", "normalizedForm": "SciBERT", "offsetStart": 15, "offsetEnd": 22}, "context": "In the case of SciBERT, a similar behavior is observed even if the performance improvement is less important, the F1-score improves by around 0.02 points between 10 and 30 epochs, and 0.07 points between 30 and 100 epochs. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.0010360479354858398}, "created": {"value": false, "score": 3.0040740966796875e-05}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9983633160591125}, "created": {"value": false, "score": 0.00010204315185546875}, "shared": {"value": false, "score": 5.960464477539062e-07}}, "references": [{"label": "116;", "normalizedForm": "116", "refKey": 0}, {"label": "28,", "normalizedForm": "28", "refKey": 0}, {"label": "31,", "normalizedForm": "31", "refKey": 0}, {"label": "BioBERT,", "normalizedForm": "BioBERT", "refKey": 0}, {"label": "PubMedBERT,", "normalizedForm": "PubMedBERT", "refKey": 0}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "BioMedRoBERTa", "normalizedForm": "BioMedRoBERTa", "offsetStart": 22, "offsetEnd": 35}, "context": "On the contrary case, BioMedRoBERTa, only uses a general domain corpus for its pre-training and presents the lowest performance, even if the size of its pre-training corpus is larger than for the rest of the models.", "mentionContextAttributes": {"used": {"value": false, "score": 0.0026360154151916504}, "created": {"value": false, "score": 1.0013580322265625e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9983633160591125}, "created": {"value": false, "score": 3.719329833984375e-05}, "shared": {"value": false, "score": 5.960464477539062e-07}}, "references": [{"label": "265.", "normalizedForm": "265", "refKey": 0}, {"label": "50,", "normalizedForm": "50", "refKey": 0}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "SciBERT", "normalizedForm": "SciBERT", "offsetStart": 27, "offsetEnd": 34}, "context": "For both number of epochs, SciBERT presents higher precision than BERT, which suggests that from the triggers identified, SciBERT classifies more of them correctly, reducing the false positives. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.010258674621582031}, "created": {"value": false, "score": 8.344650268554688e-06}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9983633160591125}, "created": {"value": false, "score": 0.00010204315185546875}, "shared": {"value": false, "score": 5.960464477539062e-07}}, "references": [{"label": "116;", "normalizedForm": "116", "refKey": 0}, {"label": "28,", "normalizedForm": "28", "refKey": 0}, {"label": "31,", "normalizedForm": "31", "refKey": 0}, {"label": "BioBERT,", "normalizedForm": "BioBERT", "refKey": 0}, {"label": "PubMedBERT,", "normalizedForm": "PubMedBERT", "refKey": 0}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "PyTorch", "normalizedForm": "PyTorch", "offsetStart": 32, "offsetEnd": 39}, "context": "Experiments were developed with PyTorch and the models were taken from the Transformers repository2 . ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9999362230300903}, "created": {"value": false, "score": 0.05451655387878418}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999362230300903}, "created": {"value": false, "score": 0.05451655387878418}, "shared": {"value": false, "score": 1.1920928955078125e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "BioBERT", "normalizedForm": "BioBERT", "offsetStart": 33, "offsetEnd": 40}, "context": "These results were obtained from BioBERT, trained for 100 epochs, since it presented the best performance from all the experiments.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9999889135360718}, "created": {"value": false, "score": 5.245208740234375e-06}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999889135360718}, "created": {"value": false, "score": 0.003447413444519043}, "shared": {"value": false, "score": 5.960464477539062e-07}}, "references": [{"label": "28,", "normalizedForm": "28", "refKey": 0}, {"label": "996;", "normalizedForm": "996", "refKey": 0}, {"label": "PubMedBERT,", "normalizedForm": "PubMedBERT", "refKey": 0}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "BioBERT", "normalizedForm": "BioBERT", "offsetStart": 42, "offsetEnd": 49}, "context": "From the results of the different models, BioBERT presented the highest performance when it is trained for 100 epochs. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.999957799911499}, "created": {"value": false, "score": 1.1563301086425781e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999889135360718}, "created": {"value": false, "score": 0.003447413444519043}, "shared": {"value": false, "score": 5.960464477539062e-07}}, "references": [{"label": "28,", "normalizedForm": "28", "refKey": 0}, {"label": "996;", "normalizedForm": "996", "refKey": 0}, {"label": "PubMedBERT,", "normalizedForm": "PubMedBERT", "refKey": 0}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "BioBERT", "normalizedForm": "BioBERT", "offsetStart": 47, "offsetEnd": 54}, "context": "The model that achieved the top performance is BioBERT when it is trained for 100 epochs, showing that a model that was pre-trained using biomedical domain language data and initialized its pre-training from the BERT weights, is useful for identifying biomedical event triggers.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9951615929603577}, "created": {"value": false, "score": 0.00013768672943115234}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999889135360718}, "created": {"value": false, "score": 0.003447413444519043}, "shared": {"value": false, "score": 5.960464477539062e-07}}, "references": [{"label": "28,", "normalizedForm": "28", "refKey": 0}, {"label": "996;", "normalizedForm": "996", "refKey": 0}, {"label": "PubMedBERT,", "normalizedForm": "PubMedBERT", "refKey": 0}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "SciBERT", "normalizedForm": "SciBERT", "offsetStart": 65, "offsetEnd": 72}, "context": "The vocab size varies for each model, where BERT presents 30,522;SciBERT,31,116;BioBERT,28,996;PubMedBERT,30,522 and BioMedRoBERTa,50,265.", "mentionContextAttributes": {"used": {"value": false, "score": 0.05542457103729248}, "created": {"value": false, "score": 1.9788742065429688e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9983633160591125}, "created": {"value": false, "score": 0.00010204315185546875}, "shared": {"value": false, "score": 5.960464477539062e-07}}, "references": [{"label": "116;", "normalizedForm": "116", "refKey": 0, "offsetStart": 18325, "offsetEnd": 18329}, {"label": "28,", "normalizedForm": "28", "refKey": 0, "offsetStart": 18337, "offsetEnd": 18340}, {"label": "31,", "normalizedForm": "31", "refKey": 0, "offsetStart": 18322, "offsetEnd": 18325}, {"label": "BioBERT,", "normalizedForm": "BioBERT", "refKey": 0, "offsetStart": 18329, "offsetEnd": 18337}, {"label": "PubMedBERT,", "normalizedForm": "PubMedBERT", "refKey": 0, "offsetStart": 18344, "offsetEnd": 18355}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "SciBERT", "normalizedForm": "SciBERT", "offsetStart": 77, "offsetEnd": 84}, "context": "BERT model achieves the top recall and F1-scores for 10 and 30 epochs, while SciBERT achieves the second best values, being lower than BERT only by 0.01 point in the F1-score. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.0008772015571594238}, "created": {"value": false, "score": 1.0609626770019531e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9983633160591125}, "created": {"value": false, "score": 0.00010204315185546875}, "shared": {"value": false, "score": 5.960464477539062e-07}}, "references": [{"label": "116;", "normalizedForm": "116", "refKey": 0}, {"label": "28,", "normalizedForm": "28", "refKey": 0}, {"label": "31,", "normalizedForm": "31", "refKey": 0}, {"label": "BioBERT,", "normalizedForm": "BioBERT", "refKey": 0}, {"label": "PubMedBERT,", "normalizedForm": "PubMedBERT", "refKey": 0}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "BioBERT", "normalizedForm": "BioBERT", "offsetStart": 80, "offsetEnd": 87}, "context": "The vocab size varies for each model, where BERT presents 30,522;SciBERT,31,116;BioBERT,28,996;PubMedBERT,30,522 and BioMedRoBERTa,50,265.", "mentionContextAttributes": {"used": {"value": false, "score": 0.05542457103729248}, "created": {"value": false, "score": 1.9788742065429688e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999889135360718}, "created": {"value": false, "score": 0.003447413444519043}, "shared": {"value": false, "score": 5.960464477539062e-07}}, "references": [{"label": "28,", "normalizedForm": "28", "refKey": 0, "offsetStart": 18337, "offsetEnd": 18340}, {"label": "996;", "normalizedForm": "996", "refKey": 0, "offsetStart": 18340, "offsetEnd": 18344}, {"label": "PubMedBERT,", "normalizedForm": "PubMedBERT", "refKey": 0, "offsetStart": 18344, "offsetEnd": 18355}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "BioBERT", "normalizedForm": "BioBERT", "offsetStart": 81, "offsetEnd": 88}, "context": "For the rest of the models, the change in performance is not as remarkable as in BioBERT and SciBERT, even if they present a slight improvement when increasing the number of epochs and, in the case of PubMedBERT, the score F1-score is decreased by 0.01 points between the training with 30 and 100 epochs. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.004346251487731934}, "created": {"value": false, "score": 3.4570693969726562e-06}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999889135360718}, "created": {"value": false, "score": 0.003447413444519043}, "shared": {"value": false, "score": 5.960464477539062e-07}}, "references": [{"label": "28,", "normalizedForm": "28", "refKey": 0}, {"label": "996;", "normalizedForm": "996", "refKey": 0}, {"label": "PubMedBERT,", "normalizedForm": "PubMedBERT", "refKey": 0}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "BioBERT", "normalizedForm": "BioBERT", "offsetStart": 84, "offsetEnd": 109}, "context": "Together with the original BERT model, four BERT variants were used for comparison, BioBERT (Lee et al., 2020), SciBERT (Beltagy et al., 2019), PubMedBERT (abstracts + full text) (Gu et al., 2020), and BioMedRoBERTa (Gururangan et al., 2020). ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9338964223861694}, "created": {"value": false, "score": 6.67572021484375e-06}, "shared": {"value": false, "score": 5.960464477539062e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999889135360718}, "created": {"value": false, "score": 0.003447413444519043}, "shared": {"value": false, "score": 5.960464477539062e-07}}, "references": [{"label": "28,", "normalizedForm": "28", "refKey": 0}, {"label": "996;", "normalizedForm": "996", "refKey": 0}, {"label": "PubMedBERT,", "normalizedForm": "PubMedBERT", "refKey": 0}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "BioBERT", "normalizedForm": "BioBERT", "offsetStart": 88, "offsetEnd": 95}, "context": "The results suggest that models pre-trained from BERT with a biomedical corpus, such as BioBERT and BioMedRoberta, are useful for detecting biomedical event triggers if the training is done for a sufficient number of epochs, as in the case of BioBERT trained for 100 epochs. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.4048305153846741}, "created": {"value": false, "score": 4.661083221435547e-05}, "shared": {"value": false, "score": 4.76837158203125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999889135360718}, "created": {"value": false, "score": 0.003447413444519043}, "shared": {"value": false, "score": 5.960464477539062e-07}}, "references": [{"label": "28,", "normalizedForm": "28", "refKey": 0}, {"label": "996;", "normalizedForm": "996", "refKey": 0}, {"label": "PubMedBERT,", "normalizedForm": "PubMedBERT", "refKey": 0}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "SciBERT", "normalizedForm": "SciBERT", "offsetStart": 93, "offsetEnd": 100}, "context": "For the rest of the models, the change in performance is not as remarkable as in BioBERT and SciBERT, even if they present a slight improvement when increasing the number of epochs and, in the case of PubMedBERT, the score F1-score is decreased by 0.01 points between the training with 30 and 100 epochs. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.004346251487731934}, "created": {"value": false, "score": 3.4570693969726562e-06}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9983633160591125}, "created": {"value": false, "score": 0.00010204315185546875}, "shared": {"value": false, "score": 5.960464477539062e-07}}, "references": [{"label": "116;", "normalizedForm": "116", "refKey": 0}, {"label": "28,", "normalizedForm": "28", "refKey": 0}, {"label": "31,", "normalizedForm": "31", "refKey": 0}, {"label": "BioBERT,", "normalizedForm": "BioBERT", "refKey": 0}, {"label": "PubMedBERT,", "normalizedForm": "PubMedBERT", "refKey": 0}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "PubMedBERT", "normalizedForm": "PubMedBERT", "offsetStart": 95, "offsetEnd": 105}, "context": "The vocab size varies for each model, where BERT presents 30,522;SciBERT,31,116;BioBERT,28,996;PubMedBERT,30,522 and BioMedRoBERTa,50,265.", "mentionContextAttributes": {"used": {"value": false, "score": 0.05542457103729248}, "created": {"value": false, "score": 1.9788742065429688e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9983633160591125}, "created": {"value": false, "score": 0.00010204315185546875}, "shared": {"value": false, "score": 5.960464477539062e-07}}, "references": [{"label": "30,", "normalizedForm": "30", "refKey": 0, "offsetStart": 18355, "offsetEnd": 18358}, {"label": "522 and BioMedRoBERTa,", "normalizedForm": "522 and BioMedRoBERTa", "refKey": 0, "offsetStart": 18358, "offsetEnd": 18380}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "BioMedRoberta", "normalizedForm": "BioMedRoberta", "offsetStart": 100, "offsetEnd": 113}, "context": "The results suggest that models pre-trained from BERT with a biomedical corpus, such as BioBERT and BioMedRoberta, are useful for detecting biomedical event triggers if the training is done for a sufficient number of epochs, as in the case of BioBERT trained for 100 epochs. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.4048305153846741}, "created": {"value": false, "score": 4.661083221435547e-05}, "shared": {"value": false, "score": 4.76837158203125e-07}}, "documentContextAttributes": {"used": {"value": false, "score": 0.4048305153846741}, "created": {"value": false, "score": 4.661083221435547e-05}, "shared": {"value": false, "score": 4.76837158203125e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "SciBERT", "normalizedForm": "SciBERT", "offsetStart": 103, "offsetEnd": 110}, "context": "Details about the models are presented in Table 2, where can be noticed that two of the BERT variants (SciBERT and PubMedBERT) were pre-trained from scratch, meaning that they use a unique vocabulary on their pre-training corpus and include embeddings that are specific for in-domain words. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.262851357460022}, "created": {"value": false, "score": 0.00010204315185546875}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9983633160591125}, "created": {"value": false, "score": 0.00010204315185546875}, "shared": {"value": false, "score": 5.960464477539062e-07}}, "references": [{"label": "116;", "normalizedForm": "116", "refKey": 0}, {"label": "28,", "normalizedForm": "28", "refKey": 0}, {"label": "31,", "normalizedForm": "31", "refKey": 0}, {"label": "BioBERT,", "normalizedForm": "BioBERT", "refKey": 0}, {"label": "PubMedBERT,", "normalizedForm": "PubMedBERT", "refKey": 0}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "BioBERT", "normalizedForm": "BioBERT", "offsetStart": 112, "offsetEnd": 119}, "context": "Five transformers models are used for comparing their performance in detecting biomedical event triggers; BERT, BioBERT, SciBERT, PubMedBERT and BioMedRoBERTa.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9983633160591125}, "created": {"value": false, "score": 3.719329833984375e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999889135360718}, "created": {"value": false, "score": 0.003447413444519043}, "shared": {"value": false, "score": 5.960464477539062e-07}}, "references": [{"label": "28,", "normalizedForm": "28", "refKey": 0}, {"label": "996;", "normalizedForm": "996", "refKey": 0}, {"label": "PubMedBERT,", "normalizedForm": "PubMedBERT", "refKey": 0}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "SciBERT", "normalizedForm": "SciBERT", "offsetStart": 112, "offsetEnd": 141}, "context": "Together with the original BERT model, four BERT variants were used for comparison, BioBERT (Lee et al., 2020), SciBERT (Beltagy et al., 2019), PubMedBERT (abstracts + full text) (Gu et al., 2020), and BioMedRoBERTa (Gururangan et al., 2020). ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9338964223861694}, "created": {"value": false, "score": 6.67572021484375e-06}, "shared": {"value": false, "score": 5.960464477539062e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9983633160591125}, "created": {"value": false, "score": 0.00010204315185546875}, "shared": {"value": false, "score": 5.960464477539062e-07}}, "references": [{"label": "116;", "normalizedForm": "116", "refKey": 0}, {"label": "28,", "normalizedForm": "28", "refKey": 0}, {"label": "31,", "normalizedForm": "31", "refKey": 0}, {"label": "BioBERT,", "normalizedForm": "BioBERT", "refKey": 0}, {"label": "PubMedBERT,", "normalizedForm": "PubMedBERT", "refKey": 0}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "PubMedBERT", "normalizedForm": "PubMedBERT", "offsetStart": 115, "offsetEnd": 125}, "context": "Details about the models are presented in Table 2, where can be noticed that two of the BERT variants (SciBERT and PubMedBERT) were pre-trained from scratch, meaning that they use a unique vocabulary on their pre-training corpus and include embeddings that are specific for in-domain words. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.262851357460022}, "created": {"value": false, "score": 0.00010204315185546875}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9983633160591125}, "created": {"value": false, "score": 0.00010204315185546875}, "shared": {"value": false, "score": 5.960464477539062e-07}}, "references": [{"label": "30,", "normalizedForm": "30", "refKey": 0}, {"label": "522 and BioMedRoBERTa,", "normalizedForm": "522 and BioMedRoBERTa", "refKey": 0}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "BioMedRoBERTa", "normalizedForm": "BioMedRoBERTa", "offsetStart": 117, "offsetEnd": 130}, "context": "The vocab size varies for each model, where BERT presents 30,522;SciBERT,31,116;BioBERT,28,996;PubMedBERT,30,522 and BioMedRoBERTa,50,265.", "mentionContextAttributes": {"used": {"value": false, "score": 0.05542457103729248}, "created": {"value": false, "score": 1.9788742065429688e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9983633160591125}, "created": {"value": false, "score": 3.719329833984375e-05}, "shared": {"value": false, "score": 5.960464477539062e-07}}, "references": [{"label": "265.", "normalizedForm": "265", "refKey": 0, "offsetStart": 18383, "offsetEnd": 18387}, {"label": "50,", "normalizedForm": "50", "refKey": 0, "offsetStart": 18380, "offsetEnd": 18383}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "SciBERT", "normalizedForm": "SciBERT", "offsetStart": 121, "offsetEnd": 128}, "context": "Five transformers models are used for comparing their performance in detecting biomedical event triggers; BERT, BioBERT, SciBERT, PubMedBERT and BioMedRoBERTa.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9983633160591125}, "created": {"value": false, "score": 3.719329833984375e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9983633160591125}, "created": {"value": false, "score": 0.00010204315185546875}, "shared": {"value": false, "score": 5.960464477539062e-07}}, "references": [{"label": "116;", "normalizedForm": "116", "refKey": 0}, {"label": "28,", "normalizedForm": "28", "refKey": 0}, {"label": "31,", "normalizedForm": "31", "refKey": 0}, {"label": "BioBERT,", "normalizedForm": "BioBERT", "refKey": 0}, {"label": "PubMedBERT,", "normalizedForm": "PubMedBERT", "refKey": 0}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "SciBERT", "normalizedForm": "SciBERT", "offsetStart": 122, "offsetEnd": 129}, "context": "For both number of epochs, SciBERT presents higher precision than BERT, which suggests that from the triggers identified, SciBERT classifies more of them correctly, reducing the false positives. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.010258674621582031}, "created": {"value": false, "score": 8.344650268554688e-06}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9983633160591125}, "created": {"value": false, "score": 0.00010204315185546875}, "shared": {"value": false, "score": 5.960464477539062e-07}}, "references": [{"label": "116;", "normalizedForm": "116", "refKey": 0}, {"label": "28,", "normalizedForm": "28", "refKey": 0}, {"label": "31,", "normalizedForm": "31", "refKey": 0}, {"label": "BioBERT,", "normalizedForm": "BioBERT", "refKey": 0}, {"label": "PubMedBERT,", "normalizedForm": "PubMedBERT", "refKey": 0}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "SciBERT", "normalizedForm": "SciBERT", "offsetStart": 128, "offsetEnd": 135}, "context": "On the other hand, using a model pre-trained from scratch with general and biomedical domain corpus combined, as in the case of SciBERT, presents better capabilities to identify biomedical triggers than using exclusively a biomedical corpus for the pre-training, as in the case of PubMedBERT. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.0001246929168701172}, "created": {"value": false, "score": 4.553794860839844e-05}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9983633160591125}, "created": {"value": false, "score": 0.00010204315185546875}, "shared": {"value": false, "score": 5.960464477539062e-07}}, "references": [{"label": "116;", "normalizedForm": "116", "refKey": 0}, {"label": "28,", "normalizedForm": "28", "refKey": 0}, {"label": "31,", "normalizedForm": "31", "refKey": 0}, {"label": "BioBERT,", "normalizedForm": "BioBERT", "refKey": 0}, {"label": "PubMedBERT,", "normalizedForm": "PubMedBERT", "refKey": 0}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "PubMedBERT", "normalizedForm": "PubMedBERT", "offsetStart": 130, "offsetEnd": 140}, "context": "Five transformers models are used for comparing their performance in detecting biomedical event triggers; BERT, BioBERT, SciBERT, PubMedBERT and BioMedRoBERTa.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9983633160591125}, "created": {"value": false, "score": 3.719329833984375e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9983633160591125}, "created": {"value": false, "score": 0.00010204315185546875}, "shared": {"value": false, "score": 5.960464477539062e-07}}, "references": [{"label": "30,", "normalizedForm": "30", "refKey": 0}, {"label": "522 and BioMedRoBERTa,", "normalizedForm": "522 and BioMedRoBERTa", "refKey": 0}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "PubMedBERT", "normalizedForm": "PubMedBERT", "offsetStart": 144, "offsetEnd": 154}, "context": "Together with the original BERT model, four BERT variants were used for comparison, BioBERT (Lee et al., 2020), SciBERT (Beltagy et al., 2019), PubMedBERT (abstracts + full text) (Gu et al., 2020), and BioMedRoBERTa (Gururangan et al., 2020). ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9338963031768799}, "created": {"value": false, "score": 6.67572021484375e-06}, "shared": {"value": false, "score": 5.960464477539062e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9983633160591125}, "created": {"value": false, "score": 0.00010204315185546875}, "shared": {"value": false, "score": 5.960464477539062e-07}}, "references": [{"label": "30,", "normalizedForm": "30", "refKey": 0}, {"label": "522 and BioMedRoBERTa,", "normalizedForm": "522 and BioMedRoBERTa", "refKey": 0}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "BioMedRoBERTa", "normalizedForm": "BioMedRoBERTa", "offsetStart": 145, "offsetEnd": 158}, "context": "Five transformers models are used for comparing their performance in detecting biomedical event triggers; BERT, BioBERT, SciBERT, PubMedBERT and BioMedRoBERTa. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9983633160591125}, "created": {"value": false, "score": 3.719329833984375e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9983633160591125}, "created": {"value": false, "score": 3.719329833984375e-05}, "shared": {"value": false, "score": 5.960464477539062e-07}}, "references": [{"label": "265.", "normalizedForm": "265", "refKey": 0}, {"label": "50,", "normalizedForm": "50", "refKey": 0}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "PubMedBERT", "normalizedForm": "PubMedBERT", "offsetStart": 201, "offsetEnd": 211}, "context": "For the rest of the models, the change in performance is not as remarkable as in BioBERT and SciBERT, even if they present a slight improvement when increasing the number of epochs and, in the case of PubMedBERT, the score F1-score is decreased by 0.01 points between the training with 30 and 100 epochs. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.004346251487731934}, "created": {"value": false, "score": 3.4570693969726562e-06}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9983633160591125}, "created": {"value": false, "score": 0.00010204315185546875}, "shared": {"value": false, "score": 5.960464477539062e-07}}, "references": [{"label": "30,", "normalizedForm": "30", "refKey": 0}, {"label": "522 and BioMedRoBERTa,", "normalizedForm": "522 and BioMedRoBERTa", "refKey": 0}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "BioMedRoBERTa", "normalizedForm": "BioMedRoBERTa", "offsetStart": 202, "offsetEnd": 240}, "context": "Together with the original BERT model, four BERT variants were used for comparison, BioBERT (Lee et al., 2020), SciBERT (Beltagy et al., 2019), PubMedBERT (abstracts + full text) (Gu et al., 2020), and BioMedRoBERTa (Gururangan et al., 2020). ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9338963031768799}, "created": {"value": false, "score": 6.67572021484375e-06}, "shared": {"value": false, "score": 5.960464477539062e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9983633160591125}, "created": {"value": false, "score": 3.719329833984375e-05}, "shared": {"value": false, "score": 5.960464477539062e-07}}, "references": [{"label": "265.", "normalizedForm": "265", "refKey": 0}, {"label": "50,", "normalizedForm": "50", "refKey": 0}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "BioBERT", "normalizedForm": "BioBERT", "offsetStart": 243, "offsetEnd": 250}, "context": "The results suggest that models pre-trained from BERT with a biomedical corpus, such as BioBERT and BioMedRoberta, are useful for detecting biomedical event triggers if the training is done for a sufficient number of epochs, as in the case of BioBERT trained for 100 epochs. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.404829740524292}, "created": {"value": false, "score": 4.661083221435547e-05}, "shared": {"value": false, "score": 4.76837158203125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999889135360718}, "created": {"value": false, "score": 0.003447413444519043}, "shared": {"value": false, "score": 5.960464477539062e-07}}, "references": [{"label": "28,", "normalizedForm": "28", "refKey": 0}, {"label": "996;", "normalizedForm": "996", "refKey": 0}, {"label": "PubMedBERT,", "normalizedForm": "PubMedBERT", "refKey": 0}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "PubMedBERT", "normalizedForm": "PubMedBERT", "offsetStart": 281, "offsetEnd": 291}, "context": "On the other hand, using a model pre-trained from scratch with general and biomedical domain corpus combined, as in the case of SciBERT, presents better capabilities to identify biomedical triggers than using exclusively a biomedical corpus for the pre-training, as in the case of PubMedBERT. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.0001246929168701172}, "created": {"value": false, "score": 4.553794860839844e-05}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9983633160591125}, "created": {"value": false, "score": 0.00010204315185546875}, "shared": {"value": false, "score": 5.960464477539062e-07}}, "references": [{"label": "30,", "normalizedForm": "30", "refKey": 0}, {"label": "522 and BioMedRoBERTa,", "normalizedForm": "522 and BioMedRoBERTa", "refKey": 0}]}], "references": [{"refKey": 0, "tei": "<biblStruct xml:id=\"b0\">\n\t<analytic>\n\t\t<title level=\"a\" type=\"main\">Adding Linguistic Information to Transformer Models Improves Biomedical Event Detection?</title>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Laura</forename><surname>Zanella</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Yannick</forename><surname>Toussaint</surname></persName>\n\t\t</author>\n\t\t<idno type=\"DOI\">10.15439/2023f2076</idno>\n\t\t<idno>34892F5D566DE6B8690973DEF5BD620F</idno>\n\t</analytic>\n\t<monogr>\n\t\t<title level=\"m\">Annals of Computer Science and Information Systems</title>\n\t\t<imprint>\n\t\t\t<publisher>IEEE</publisher>\n\t\t\t<date type=\"published\" when=\"2023-09-26\" />\n\t\t\t<biblScope unit=\"volume\">35</biblScope>\n\t\t\t<biblScope unit=\"page\" from=\"1211\" to=\"1216\" />\n\t\t</imprint>\n\t</monogr>\n</biblStruct>\n"}], "runtime": 33389, "id": "d6406d6088229bacd339a0c604955e81f10130c6", "metadata": {"id": "d6406d6088229bacd339a0c604955e81f10130c6"}, "original_file_path": "../../datalake/Samuel/SOFTware-Sync/downloads/xml/hal-03984783.grobid.tei.xml", "file_name": "hal-03984783.grobid.tei.xml"}