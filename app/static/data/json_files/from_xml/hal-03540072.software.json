{"application": "software-mentions", "version": "0.8.0", "date": "2024-10-07T11:54+0000", "md5": "7D7A136B08FF5317CB7A2437DD8A07D2", "mentions": [{"type": "software", "software-type": "software", "software-name": {"rawForm": "CrossFit", "normalizedForm": "CrossFit", "offsetStart": 0, "offsetEnd": 8}, "context": "CrossFit and UnifiedQA categorize them by format (multiple-choice vs. extractive vs. abstractive/generative), whereas Brown et al. (2020) categorize by content (reading comprehension vs. commonsense vs. closed-book QA).", "mentionContextAttributes": {"used": {"value": false, "score": 0.00154876708984375}, "created": {"value": false, "score": 6.794929504394531e-06}, "shared": {"value": false, "score": 4.76837158203125e-07}}, "documentContextAttributes": {"used": {"value": false, "score": 0.00154876708984375}, "created": {"value": false, "score": 6.794929504394531e-06}, "shared": {"value": false, "score": 4.76837158203125e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "WinoGender", "normalizedForm": "WinoGender", "offsetStart": 0, "offsetEnd": 10}, "context": "WinoGender Schemas are minimal pairs of sentences that differ only by the gender of one pronoun in the sentence, designed to test for the presence of gender bias.", "mentionContextAttributes": {"used": {"value": false, "score": 0.00020450353622436523}, "created": {"value": false, "score": 9.059906005859375e-05}, "shared": {"value": false, "score": 4.76837158203125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9997743964195251}, "created": {"value": false, "score": 0.00036513805389404297}, "shared": {"value": false, "score": 4.76837158203125e-07}}, "references": [{"label": "(Rudinger et al., 2018)", "normalizedForm": "Rudinger et al., 2018", "refKey": 78}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "OpenAI", "normalizedForm": "OpenAI", "offsetStart": 9, "offsetEnd": 15}, "context": "Although OpenAI does not disclose which one of their commercially available models correspond to which models reported inBrown et al. (2020),Gao et al. (2021) estimate that davinci corresponds to the 175B model.", "mentionContextAttributes": {"used": {"value": false, "score": 0.0015247464179992676}, "created": {"value": false, "score": 4.6372413635253906e-05}, "shared": {"value": false, "score": 5.960464477539062e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9929293990135193}, "created": {"value": false, "score": 4.6372413635253906e-05}, "shared": {"value": false, "score": 5.960464477539062e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "SuperGLUE", "normalizedForm": "SuperGLUE", "offsetStart": 26, "offsetEnd": 56}, "context": "Lastly, T0++ further adds SuperGLUE (Wang et al., 2019a) to the training mixture (except RTE and CB), which leaves NLI and the BIG-bench tasks as the only held-out tasks.", "mentionContextAttributes": {"used": {"value": false, "score": 0.13127803802490234}, "created": {"value": false, "score": 1.430511474609375e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9957643747329712}, "created": {"value": false, "score": 0.008816361427307129}, "shared": {"value": false, "score": 2.384185791015625e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Google", "normalizedForm": "Google", "offsetStart": 27, "offsetEnd": 33}, "context": "Further, T5 was trained in Google's Taiwan datacenter, whereas we trained in the europe-west4-a Cloud region.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9992995262145996}, "created": {"value": false, "score": 0.0012018680572509766}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9992995262145996}, "created": {"value": false, "score": 0.002681434154510498}, "shared": {"value": false, "score": 4.76837158203125e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Google", "normalizedForm": "Google", "offsetStart": 30, "offsetEnd": 36}, "context": "The gCO 2 eq/kWh published by Google for these datacenters are 540 and 410 respectively,5 suggesting that our carbon emissions should further be scaled by a factor of 410/540 \u2248 75.9%.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9915135502815247}, "created": {"value": false, "score": 3.2186508178710938e-06}, "shared": {"value": false, "score": 4.76837158203125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9992995262145996}, "created": {"value": false, "score": 0.002681434154510498}, "shared": {"value": false, "score": 4.76837158203125e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "SuperGLUE", "normalizedForm": "SuperGLUE", "offsetStart": 30, "offsetEnd": 39}, "context": "We hold out ReCoRD as part of SuperGLUE, but it is impractical to inspect every dataset and slice out the subsets of examples which ask entailment or coreference questions.", "mentionContextAttributes": {"used": {"value": false, "score": 0.0012874603271484375}, "created": {"value": false, "score": 0.008816361427307129}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9957643747329712}, "created": {"value": false, "score": 0.008816361427307129}, "shared": {"value": false, "score": 2.384185791015625e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Hugging Face", "normalizedForm": "Hugging Face", "offsetStart": 36, "offsetEnd": 48}, "context": "All experiments use datasets in the Hugging Face datasets library (Lhoest et al., 2021).", "mentionContextAttributes": {"used": {"value": true, "score": 0.9481074213981628}, "created": {"value": false, "score": 1.6808509826660156e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9481074213981628}, "created": {"value": false, "score": 0.43800002336502075}, "shared": {"value": false, "score": 9.5367431640625e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Hugging Face", "normalizedForm": "Hugging Face", "offsetStart": 38, "offsetEnd": 50}, "context": "Some of the datasets available in the Hugging Face library contain potentially harmful content.", "mentionContextAttributes": {"used": {"value": false, "score": 0.00018423795700073242}, "created": {"value": false, "score": 0.00012540817260742188}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9481074213981628}, "created": {"value": false, "score": 0.43800002336502075}, "shared": {"value": false, "score": 9.5367431640625e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Streamlit", "normalizedForm": "Streamlit", "offsetStart": 42, "offsetEnd": 52}, "context": "We implemented a lightweight interface in Streamlit6 that users could download, run locally in a web browser, and then upload their results to a central repository.", "mentionContextAttributes": {"used": {"value": false, "score": 0.00012981891632080078}, "created": {"value": true, "score": 0.9984713196754456}, "shared": {"value": false, "score": 0.04738974571228027}}, "documentContextAttributes": {"used": {"value": false, "score": 0.00012981891632080078}, "created": {"value": true, "score": 0.9984713196754456}, "shared": {"value": false, "score": 0.04738974571228027}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "PromptSource", "normalizedForm": "PromptSource", "offsetStart": 52, "offsetEnd": 64}, "context": "During the development of our tool (which we called PromptSource), we found that a few idioms were particularly useful. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.0003408193588256836}, "created": {"value": true, "score": 0.9998310804367065}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": false, "score": 0.0003408193588256836}, "created": {"value": true, "score": 0.9998310804367065}, "shared": {"value": false, "score": 2.384185791015625e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Hugging Face", "normalizedForm": "Hugging Face", "offsetStart": 55, "offsetEnd": 67}, "context": "The BigScience project was initiated by Thomas Wolf at Hugging Face, and this collaboration would not have been possible without his effort.", "mentionContextAttributes": {"used": {"value": false, "score": 0.287586510181427}, "created": {"value": false, "score": 0.034954607486724854}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9481074213981628}, "created": {"value": false, "score": 0.43800002336502075}, "shared": {"value": false, "score": 9.5367431640625e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "WinoGender", "normalizedForm": "WinoGender", "offsetStart": 56, "offsetEnd": 66}, "context": "We use the version from Poliak et al. (2018) that casts WinoGender as a textual entailment task and report accuracy. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9864354729652405}, "created": {"value": false, "score": 1.9550323486328125e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9997743964195251}, "created": {"value": false, "score": 0.00036513805389404297}, "shared": {"value": false, "score": 4.76837158203125e-07}}, "references": [{"label": "(Rudinger et al., 2018)", "normalizedForm": "Rudinger et al., 2018", "refKey": 78}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "WinoGender", "normalizedForm": "WinoGender", "offsetStart": 58, "offsetEnd": 68}, "context": "Table 3: Average and median accuracies on CrowS-Pairs and WinoGender reformulated as classification tasks.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9997743964195251}, "created": {"value": false, "score": 2.956390380859375e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9997743964195251}, "created": {"value": false, "score": 0.00036513805389404297}, "shared": {"value": false, "score": 4.76837158203125e-07}}, "references": [{"label": "(Rudinger et al., 2018)", "normalizedForm": "Rudinger et al., 2018", "refKey": 78}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Hugging Face", "normalizedForm": "Hugging Face", "offsetStart": 71, "offsetEnd": 83}, "context": "This work is built exclusively on publicly available datasets from the Hugging Face datasets library (Lhoest et al., 2021) and a publicly available model, T5+LM (Lester et al., 2021).", "mentionContextAttributes": {"used": {"value": false, "score": 0.05511808395385742}, "created": {"value": false, "score": 0.43800002336502075}, "shared": {"value": false, "score": 9.5367431640625e-06}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9481074213981628}, "created": {"value": false, "score": 0.43800002336502075}, "shared": {"value": false, "score": 9.5367431640625e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Google", "normalizedForm": "Google", "offsetStart": 82, "offsetEnd": 88}, "context": "Since we based T0 on this T5 variant and performed training on the same hardware (Google Cloud TPUs), we can estimate the carbon emissions produced by our study by simply re-scaling the T5 estimate from Patterson et al. (2021) by the amount of training we performed.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9988490343093872}, "created": {"value": false, "score": 0.002681434154510498}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9992995262145996}, "created": {"value": false, "score": 0.002681434154510498}, "shared": {"value": false, "score": 4.76837158203125e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Hugging Face", "normalizedForm": "Hugging Face", "offsetStart": 92, "offsetEnd": 104}, "context": "We are grateful for the TPU Research Cloud program which generously provided TPU credits to Hugging Face.", "mentionContextAttributes": {"used": {"value": false, "score": 0.03464895486831665}, "created": {"value": false, "score": 0.16783320903778076}, "shared": {"value": false, "score": 1.430511474609375e-06}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9481074213981628}, "created": {"value": false, "score": 0.43800002336502075}, "shared": {"value": false, "score": 9.5367431640625e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "WinoGender", "normalizedForm": "WinoGender", "offsetStart": 97, "offsetEnd": 107}, "context": "To measure the ability of our model to recognize gender biases, we evaluate our models using the WinoGender Schemas (Rudinger et al., 2018) (also called AX-g under SuperGLUE) and CrowS-Pairs (Nangia et al., 2020). ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9957643747329712}, "created": {"value": false, "score": 0.00036513805389404297}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9997743964195251}, "created": {"value": false, "score": 0.00036513805389404297}, "shared": {"value": false, "score": 4.76837158203125e-07}}, "references": [{"label": "(Rudinger et al., 2018)", "normalizedForm": "Rudinger et al., 2018", "refKey": 78, "offsetStart": 35727, "offsetEnd": 35750}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Hugging Face", "normalizedForm": "Hugging Face", "offsetStart": 116, "offsetEnd": 128}, "context": "bigscience-workshop/promptsource Datasets are listed by their task categorization and the canonical dataset name in Hugging Face datasets.", "mentionContextAttributes": {"used": {"value": true, "score": 0.7390277981758118}, "created": {"value": false, "score": 1.0609626770019531e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9481074213981628}, "created": {"value": false, "score": 0.43800002336502075}, "shared": {"value": false, "score": 9.5367431640625e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "OpenAI", "normalizedForm": "OpenAI", "offsetStart": 149, "offsetEnd": 155}, "context": "Comparing T0 and GPT-3's robustness Because Brown et al. (2020) only report one prompt per dataset with no standard deviation, we evaluate GPT-3 via OpenAI's API3 on RTE using the same 10 prompts we evaluate T0 in order to estimate GPT-3 robustness' to different wording of prompts. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9929293990135193}, "created": {"value": false, "score": 3.933906555175781e-06}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9929293990135193}, "created": {"value": false, "score": 4.6372413635253906e-05}, "shared": {"value": false, "score": 5.960464477539062e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Hugging Face", "normalizedForm": "Hugging Face", "offsetStart": 155, "offsetEnd": 167}, "context": "Second, a \"sourcing\" view allows users to select a dataset to prompt, browse examples from that dataset in the form of Python dictionaries provided by the Hugging Face datasets library, and enter a template for that dataset. ", "mentionContextAttributes": {"used": {"value": false, "score": 8.416175842285156e-05}, "created": {"value": false, "score": 5.340576171875e-05}, "shared": {"value": false, "score": 4.76837158203125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9481074213981628}, "created": {"value": false, "score": 0.43800002336502075}, "shared": {"value": false, "score": 9.5367431640625e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Google", "normalizedForm": "Google", "offsetStart": 162, "offsetEnd": 168}, "context": "The maintainers of BIG-bench provide a prompt for each dataset, with which we compare our models to a series of preliminary diagnostic baseline models trained by Google and evaluated by the BIG-bench maintainers. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.15998947620391846}, "created": {"value": false, "score": 0.00013828277587890625}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9992995262145996}, "created": {"value": false, "score": 0.002681434154510498}, "shared": {"value": false, "score": 4.76837158203125e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "SuperGLUE", "normalizedForm": "SuperGLUE", "offsetStart": 164, "offsetEnd": 173}, "context": "To measure the ability of our model to recognize gender biases, we evaluate our models using the WinoGender Schemas (Rudinger et al., 2018) (also called AX-g under SuperGLUE) and CrowS-Pairs (Nangia et al., 2020). ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9957643747329712}, "created": {"value": false, "score": 0.00036513805389404297}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9957643747329712}, "created": {"value": false, "score": 0.008816361427307129}, "shared": {"value": false, "score": 2.384185791015625e-07}}}], "references": [{"refKey": 78, "tei": "<biblStruct xml:id=\"b78\">\n\t<analytic>\n\t\t<title level=\"a\" type=\"main\">Gender Bias in Coreference Resolution</title>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Rachel</forename><surname>Rudinger</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Jason</forename><surname>Naradowsky</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Brian</forename><surname>Leonard</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Benjamin</forename><surname>Van Durme</surname></persName>\n\t\t</author>\n\t\t<idno type=\"DOI\">10.18653/v1/n18-2002</idno>\n\t</analytic>\n\t<monogr>\n\t\t<title level=\"m\">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers)</title>\n\t\t<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers)</meeting>\n\t\t<imprint>\n\t\t\t<publisher>Association for Computational Linguistics</publisher>\n\t\t\t<date type=\"published\" when=\"2018\">June 2018</date>\n\t\t</imprint>\n\t</monogr>\n</biblStruct>\n"}], "runtime": 9904, "id": "409ed02e0019554c775d41265eb1d1b113669105", "metadata": {"id": "409ed02e0019554c775d41265eb1d1b113669105"}, "original_file_path": "../../datalake/Samuel/SOFTware-Sync/downloads/xml/hal-03540072.grobid.tei.xml", "file_name": "hal-03540072.grobid.tei.xml"}