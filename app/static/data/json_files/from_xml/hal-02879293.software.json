{"application": "software-mentions", "version": "0.8.0", "date": "2024-04-15T07:34+0000", "md5": "D3B992DF936244D6C94CCC8C7A629004", "mentions": [{"type": "software", "software-type": "software", "software-name": {"rawForm": "BioBERT", "normalizedForm": "BioBERT", "offsetStart": 0, "offsetEnd": 12}, "context": "BioBERT [21] is pre-trained on large-scale biomedical corpora outperforming the general BERT model in representative biomedical text mining tasks. ", "mentionContextAttributes": {"used": {"value": false, "score": 7.933378219604492e-05}, "created": {"value": false, "score": 1.3768672943115234e-05}, "shared": {"value": false, "score": 1.7881393432617188e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.999978244304657}, "created": {"value": false, "score": 7.772445678710938e-05}, "shared": {"value": false, "score": 2.980232238769531e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "SciBERT", "normalizedForm": "SciBERT", "offsetStart": 4, "offsetEnd": 11}, "context": "For SciBERT, we used the uncased model with the SciBERT vocabulary. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9998211860656738}, "created": {"value": false, "score": 5.781650543212891e-06}, "shared": {"value": false, "score": 2.980232238769531e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999759793281555}, "created": {"value": false, "score": 0.0001232624053955078}, "shared": {"value": false, "score": 4.172325134277344e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "BioBERT", "normalizedForm": "BioBERT", "offsetStart": 4, "offsetEnd": 11}, "context": "For BioBERT, we used version 1.1. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.999978244304657}, "created": {"value": false, "score": 3.3974647521972656e-06}, "shared": {"value": false, "score": 2.980232238769531e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.999978244304657}, "created": {"value": false, "score": 7.772445678710938e-05}, "shared": {"value": false, "score": 2.980232238769531e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "RoBERTa", "normalizedForm": "RoBERTa", "offsetStart": 4, "offsetEnd": 11}, "context": "For RoBERTa, we increased the number of epochs for fine-tuning to 10, as it was done in the original paper. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9959925413131714}, "created": {"value": false, "score": 0.10356330871582031}, "shared": {"value": false, "score": 2.980232238769531e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9959925413131714}, "created": {"value": false, "score": 0.10356330871582031}, "shared": {"value": false, "score": 0.09434205293655396}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Flair     PyTorch NLP", "normalizedForm": "Flair PyTorch NLP", "offsetStart": 4, "offsetEnd": 25}, "version": {"rawForm": "0.4.1", "normalizedForm": "0.4.1", "offsetStart": 44, "offsetEnd": 49}, "context": "The Flair [1] PyTorch NLP framework version 0.4.1 was used for implementing the sequence tagging task. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9998853802680969}, "created": {"value": false, "score": 0.00014495849609375}, "shared": {"value": false, "score": 1.7881393432617188e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9998853802680969}, "created": {"value": false, "score": 0.00014495849609375}, "shared": {"value": false, "score": 1.7881393432617188e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "SciBERT", "normalizedForm": "SciBERT", "offsetStart": 6, "offsetEnd": 13}, "context": "While SciBERT is trained on full papers from Semantic Scholar it also contains biomedical data, but to a smaller degree than BioBERT. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.0033836960792541504}, "created": {"value": false, "score": 7.778406143188477e-05}, "shared": {"value": false, "score": 1.7881393432617188e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999759793281555}, "created": {"value": false, "score": 0.0001232624053955078}, "shared": {"value": false, "score": 4.172325134277344e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "SciBERT", "normalizedForm": "SciBERT", "offsetStart": 6, "offsetEnd": 13}, "context": "While SciBERT as the best performing system on the in-domain test set drops .06 points on the glaucoma test set, RoBERTa stays almost the same and only drops from .67 to .66 ", "mentionContextAttributes": {"used": {"value": false, "score": 0.15614533424377441}, "created": {"value": false, "score": 2.6166439056396484e-05}, "shared": {"value": false, "score": 1.7881393432617188e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999759793281555}, "created": {"value": false, "score": 0.0001232624053955078}, "shared": {"value": false, "score": 4.172325134277344e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "RoBERTa", "normalizedForm": "RoBERTa", "offsetStart": 15, "offsetEnd": 22}, "context": "Interestingly, RoBERTa delivers com-parable results even though it is a model trained on general data.", "mentionContextAttributes": {"used": {"value": false, "score": 3.606081008911133e-05}, "created": {"value": false, "score": 0.014098584651947021}, "shared": {"value": false, "score": 1.7881393432617188e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9959925413131714}, "created": {"value": false, "score": 0.10356330871582031}, "shared": {"value": false, "score": 0.09434205293655396}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "SciBERT", "normalizedForm": "SciBERT", "offsetStart": 18, "offsetEnd": 29}, "context": "Contrary to that, SciBERT [5] is trained from scratch with an own vocabulary. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.0002295970916748047}, "created": {"value": false, "score": 3.8683414459228516e-05}, "shared": {"value": false, "score": 4.172325134277344e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999759793281555}, "created": {"value": false, "score": 0.0001232624053955078}, "shared": {"value": false, "score": 4.172325134277344e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "huggingface", "normalizedForm": "huggingface", "offsetStart": 19, "offsetEnd": 30}, "version": {"rawForm": "2.3", "normalizedForm": "2.3"}, "context": "https://github.com/huggingface/transformers", "mentionContextAttributes": {"used": {"value": false, "score": 0.00020885467529296875}, "created": {"value": false, "score": 1.2218952178955078e-05}, "shared": {"value": true, "score": 0.9900696873664856}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9992465972900391}, "created": {"value": false, "score": 1.7464160919189453e-05}, "shared": {"value": true, "score": 0.9900696873664856}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "PyTorch", "normalizedForm": "PyTorch", "offsetStart": 21, "offsetEnd": 28}, "context": "For BERT, we use the PyTorch implementation of huggingface9 version 2.3. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9992465972900391}, "created": {"value": false, "score": 1.7464160919189453e-05}, "shared": {"value": false, "score": 2.980232238769531e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9992465972900391}, "created": {"value": false, "score": 1.7464160919189453e-05}, "shared": {"value": false, "score": 2.980232238769531e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "RoBERTa", "normalizedForm": "RoBERTa", "offsetStart": 23, "offsetEnd": 30}, "context": "Independently of that, RoBERTa shows more reliable results when looking at the performance on the out of domain test sets. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.08756542205810547}, "created": {"value": false, "score": 7.450580596923828e-06}, "shared": {"value": false, "score": 2.980232238769531e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9959925413131714}, "created": {"value": false, "score": 0.10356330871582031}, "shared": {"value": false, "score": 0.09434205293655396}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "SciBERT", "normalizedForm": "SciBERT", "offsetStart": 28, "offsetEnd": 35}, "context": "We chose to use the uncased SciBERT model, meaning that we ignore the capitalization of words. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9994412660598755}, "created": {"value": false, "score": 0.0001232624053955078}, "shared": {"value": false, "score": 2.980232238769531e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999759793281555}, "created": {"value": false, "score": 0.0001232624053955078}, "shared": {"value": false, "score": 4.172325134277344e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "hyperopt 10", "normalizedForm": "hyperopt 10", "offsetStart": 37, "offsetEnd": 48}, "version": {"rawForm": "0.1.2", "normalizedForm": "0.1.2", "offsetStart": 57, "offsetEnd": 62}, "context": "Hyper parameter tuning was done with hyperopt 10 version 0.1.2. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9999915957450867}, "created": {"value": false, "score": 2.682209014892578e-06}, "shared": {"value": false, "score": 1.7881393432617188e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999915957450867}, "created": {"value": false, "score": 2.682209014892578e-06}, "shared": {"value": false, "score": 1.7881393432617188e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "huggingface", "normalizedForm": "huggingface", "offsetStart": 47, "offsetEnd": 59}, "version": {"rawForm": "2.3", "normalizedForm": "2.3", "offsetStart": 68, "offsetEnd": 71}, "context": "For BERT, we use the PyTorch implementation of huggingface9 version 2.3. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9992465972900391}, "created": {"value": false, "score": 1.7464160919189453e-05}, "shared": {"value": false, "score": 2.980232238769531e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9992465972900391}, "created": {"value": false, "score": 1.7464160919189453e-05}, "shared": {"value": true, "score": 0.9900696873664856}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Sci-", "normalizedForm": "Sci", "offsetStart": 48, "offsetEnd": 52}, "context": "The same configuration was used for fine-tuning Sci-and BioBERT. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.978767454624176}, "created": {"value": false, "score": 1.1146068572998047e-05}, "shared": {"value": false, "score": 2.980232238769531e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.978767454624176}, "created": {"value": false, "score": 1.1146068572998047e-05}, "shared": {"value": false, "score": 2.980232238769531e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "SciBERT", "normalizedForm": "SciBERT", "offsetStart": 48, "offsetEnd": 55}, "context": "For SciBERT, we used the uncased model with the SciBERT vocabulary.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9998211860656738}, "created": {"value": false, "score": 5.781650543212891e-06}, "shared": {"value": false, "score": 2.980232238769531e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999759793281555}, "created": {"value": false, "score": 0.0001232624053955078}, "shared": {"value": false, "score": 4.172325134277344e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "BioBERT", "normalizedForm": "BioBERT", "offsetStart": 56, "offsetEnd": 63}, "context": "The same configuration was used for fine-tuning Sci-and BioBERT. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.978767454624176}, "created": {"value": false, "score": 1.1146068572998047e-05}, "shared": {"value": false, "score": 2.980232238769531e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.999978244304657}, "created": {"value": false, "score": 7.772445678710938e-05}, "shared": {"value": false, "score": 2.980232238769531e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "BPEmb", "normalizedForm": "BPEmb", "offsetStart": 57, "offsetEnd": 67}, "context": "Contrary to that, fastText [13] and byte-pair embeddings BPEmb [16] use subword segments to increase the capability of their vocabulary and might because of that be a better choice for a setting with unusual and specific terminology. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.0003459453582763672}, "created": {"value": false, "score": 1.2934207916259766e-05}, "shared": {"value": false, "score": 2.980232238769531e-07}}, "documentContextAttributes": {"used": {"value": false, "score": 0.0003459453582763672}, "created": {"value": false, "score": 1.2934207916259766e-05}, "shared": {"value": false, "score": 2.980232238769531e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "SciBERT", "normalizedForm": "SciBERT", "offsetStart": 59, "offsetEnd": 66}, "context": "Comparing the specialized with the general models, Bio-and SciBERT show a better performance than the general BERT model, where the cased BioBERT tends to be more reliable for the out of domain test data.", "mentionContextAttributes": {"used": {"value": false, "score": 0.006371736526489258}, "created": {"value": false, "score": 3.594160079956055e-05}, "shared": {"value": false, "score": 2.980232238769531e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999759793281555}, "created": {"value": false, "score": 0.0001232624053955078}, "shared": {"value": false, "score": 4.172325134277344e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "SciBERT", "normalizedForm": "SciBERT", "offsetStart": 62, "offsetEnd": 69}, "context": "Comparing the specialized and general BERT model, the Bio-and SciBERT increase the performance by up to .06 ", "mentionContextAttributes": {"used": {"value": false, "score": 0.0002592802047729492}, "created": {"value": false, "score": 8.577108383178711e-05}, "shared": {"value": false, "score": 2.980232238769531e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999759793281555}, "created": {"value": false, "score": 0.0001232624053955078}, "shared": {"value": false, "score": 4.172325134277344e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "SciBERT", "normalizedForm": "SciBERT", "offsetStart": 63, "offsetEnd": 70}, "context": "As it was the case for the original BERT, the uncased model of SciBERT performs slightly better for sentence classification tasks than the cased model. ", "mentionContextAttributes": {"used": {"value": false, "score": 5.65648078918457e-05}, "created": {"value": false, "score": 1.3768672943115234e-05}, "shared": {"value": false, "score": 1.7881393432617188e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999759793281555}, "created": {"value": false, "score": 0.0001232624053955078}, "shared": {"value": false, "score": 4.172325134277344e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "SciBERT", "normalizedForm": "SciBERT", "offsetStart": 64, "offsetEnd": 71}, "context": "We used the best performing sequence tagger, i.e. the finetuned SciBERT with a GRU and CRF. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9999759793281555}, "created": {"value": false, "score": 1.0907649993896484e-05}, "shared": {"value": false, "score": 1.7881393432617188e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999759793281555}, "created": {"value": false, "score": 0.0001232624053955078}, "shared": {"value": false, "score": 4.172325134277344e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "SciBERT", "normalizedForm": "SciBERT", "offsetStart": 64, "offsetEnd": 71}, "context": "Interestingly, comparing the two domain adapted models, Bio-and SciBERT, there were no regular errors, which allows any conclusion about the advantages or disadvantages of one model. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9989938735961914}, "created": {"value": false, "score": 2.3066997528076172e-05}, "shared": {"value": false, "score": 1.7881393432617188e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999759793281555}, "created": {"value": false, "score": 0.0001232624053955078}, "shared": {"value": false, "score": 4.172325134277344e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "RoBERTa", "normalizedForm": "RoBERTa", "offsetStart": 70, "offsetEnd": 77}, "context": "We speculate that parts of the web crawl data which was used to train RoBERTa contain PubMed articles, since they are freely available on the web.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9628366231918335}, "created": {"value": false, "score": 0.00019484758377075195}, "shared": {"value": false, "score": 0.09434205293655396}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9959925413131714}, "created": {"value": false, "score": 0.10356330871582031}, "shared": {"value": false, "score": 0.09434205293655396}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "RoBERTa", "normalizedForm": "RoBERTa", "offsetStart": 112, "offsetEnd": 123}, "context": "Another new model, which outperforms BERT on the General Language Understanding Evaluation (GLUE) benchmark, is RoBERTa [23]. ", "mentionContextAttributes": {"used": {"value": false, "score": 5.8770179748535156e-05}, "created": {"value": false, "score": 0.0004417300224304199}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9959925413131714}, "created": {"value": false, "score": 0.10356330871582031}, "shared": {"value": false, "score": 0.09434205293655396}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "RoBERTa", "normalizedForm": "RoBERTa", "offsetStart": 113, "offsetEnd": 120}, "context": "While SciBERT as the best performing system on the in-domain test set drops .06 points on the glaucoma test set, RoBERTa stays almost the same and only drops from .67 to .66", "mentionContextAttributes": {"used": {"value": false, "score": 0.15614527463912964}, "created": {"value": false, "score": 2.6106834411621094e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9959925413131714}, "created": {"value": false, "score": 0.10356330871582031}, "shared": {"value": false, "score": 0.09434205293655396}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "BioBERT", "normalizedForm": "BioBERT", "offsetStart": 125, "offsetEnd": 132}, "context": "While SciBERT is trained on full papers from Semantic Scholar it also contains biomedical data, but to a smaller degree than BioBERT. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.0033835768699645996}, "created": {"value": false, "score": 7.772445678710938e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.999978244304657}, "created": {"value": false, "score": 7.772445678710938e-05}, "shared": {"value": false, "score": 2.980232238769531e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "BioBERT", "normalizedForm": "BioBERT", "offsetStart": 138, "offsetEnd": 145}, "context": "Comparing the specialized with the general models, Bio-and SciBERT show a better performance than the general BERT model, where the cased BioBERT tends to be more reliable for the out of domain test data.", "mentionContextAttributes": {"used": {"value": false, "score": 0.006371676921844482}, "created": {"value": false, "score": 3.5881996154785156e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.999978244304657}, "created": {"value": false, "score": 7.772445678710938e-05}, "shared": {"value": false, "score": 2.980232238769531e-07}}}], "references": [], "runtime": 100335, "id": "fc8473702d513a1fcf34e1f9f70d3019ae52b4b6", "metadata": {"id": "fc8473702d513a1fcf34e1f9f70d3019ae52b4b6"}, "original_file_path": "../../datalake/Samuel/SOFTware-Sync/data/xml_not_sofctied/log_xml/hal-02879293.grobid.tei.xml", "file_name": "hal-02879293.grobid.tei.xml"}