{"application": "software-mentions", "version": "0.8.0", "date": "2024-10-07T11:44+0000", "md5": "1C0BCE464171003CEA11AED8B4D300AB", "mentions": [{"type": "software", "software-type": "software", "software-name": {"rawForm": "BioBERT", "normalizedForm": "BioBERT", "offsetStart": 0, "offsetEnd": 12}, "context": "BioBERT [21] is pre-trained on large-scale biomedical corpora outperforming the general BERT model in representative biomedical text mining tasks. ", "mentionContextAttributes": {"used": {"value": false, "score": 7.927417755126953e-05}, "created": {"value": false, "score": 1.3709068298339844e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999783039093018}, "created": {"value": false, "score": 7.772445678710938e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "SciBERT", "normalizedForm": "SciBERT", "offsetStart": 4, "offsetEnd": 11}, "context": "For SciBERT, we used the uncased model with the SciBERT vocabulary. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9998212456703186}, "created": {"value": false, "score": 5.7220458984375e-06}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999760389328003}, "created": {"value": false, "score": 0.0001232624053955078}, "shared": {"value": false, "score": 3.5762786865234375e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "BioBERT", "normalizedForm": "BioBERT", "offsetStart": 4, "offsetEnd": 11}, "context": "For BioBERT, we used version 1.1. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9999783039093018}, "created": {"value": false, "score": 3.337860107421875e-06}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999783039093018}, "created": {"value": false, "score": 7.772445678710938e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "RoBERTa", "normalizedForm": "RoBERTa", "offsetStart": 4, "offsetEnd": 11}, "context": "For RoBERTa, we increased the number of epochs for fine-tuning to 10, as it was done in the original paper. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9959926009178162}, "created": {"value": false, "score": 0.10356295108795166}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9959926009178162}, "created": {"value": false, "score": 0.10356295108795166}, "shared": {"value": false, "score": 0.09434634447097778}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Flair     PyTorch NLP", "normalizedForm": "Flair PyTorch NLP", "offsetStart": 4, "offsetEnd": 25}, "version": {"rawForm": "0.4.1", "normalizedForm": "0.4.1", "offsetStart": 44, "offsetEnd": 49}, "context": "The Flair [1] PyTorch NLP framework version 0.4.1 was used for implementing the sequence tagging task. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9998854398727417}, "created": {"value": false, "score": 0.00014495849609375}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9998854398727417}, "created": {"value": false, "score": 0.00014495849609375}, "shared": {"value": false, "score": 1.1920928955078125e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "SciBERT", "normalizedForm": "SciBERT", "offsetStart": 6, "offsetEnd": 13}, "context": "While SciBERT is trained on full papers from Semantic Scholar it also contains biomedical data, but to a smaller degree than BioBERT. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.0033835768699645996}, "created": {"value": false, "score": 7.772445678710938e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999760389328003}, "created": {"value": false, "score": 0.0001232624053955078}, "shared": {"value": false, "score": 3.5762786865234375e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "SciBERT", "normalizedForm": "SciBERT", "offsetStart": 6, "offsetEnd": 13}, "context": "While SciBERT as the best performing system on the in-domain test set drops .06 points on the glaucoma test set, RoBERTa stays almost the same and only drops from .67 to .66 ", "mentionContextAttributes": {"used": {"value": false, "score": 0.15614408254623413}, "created": {"value": false, "score": 2.6106834411621094e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999760389328003}, "created": {"value": false, "score": 0.0001232624053955078}, "shared": {"value": false, "score": 3.5762786865234375e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "RoBERTa", "normalizedForm": "RoBERTa", "offsetStart": 15, "offsetEnd": 22}, "context": "Interestingly, RoBERTa delivers com-parable results even though it is a model trained on general data.", "mentionContextAttributes": {"used": {"value": false, "score": 3.600120544433594e-05}, "created": {"value": false, "score": 0.014098703861236572}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9959926009178162}, "created": {"value": false, "score": 0.10356295108795166}, "shared": {"value": false, "score": 0.09434634447097778}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "SciBERT", "normalizedForm": "SciBERT", "offsetStart": 18, "offsetEnd": 29}, "context": "Contrary to that, SciBERT [5] is trained from scratch with an own vocabulary. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.0002295374870300293}, "created": {"value": false, "score": 3.8623809814453125e-05}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999760389328003}, "created": {"value": false, "score": 0.0001232624053955078}, "shared": {"value": false, "score": 3.5762786865234375e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "huggingface", "normalizedForm": "huggingface", "offsetStart": 19, "offsetEnd": 30}, "version": {"rawForm": "2.3", "normalizedForm": "2.3"}, "context": "https://github.com/huggingface/transformers", "mentionContextAttributes": {"used": {"value": false, "score": 0.00020891427993774414}, "created": {"value": false, "score": 1.2159347534179688e-05}, "shared": {"value": true, "score": 0.9900696873664856}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9992465972900391}, "created": {"value": false, "score": 1.7404556274414062e-05}, "shared": {"value": true, "score": 0.9900696873664856}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "PyTorch", "normalizedForm": "PyTorch", "offsetStart": 21, "offsetEnd": 28}, "context": "For BERT, we use the PyTorch implementation of huggingface9 version 2.3. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9992465972900391}, "created": {"value": false, "score": 1.7404556274414062e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9992465972900391}, "created": {"value": false, "score": 1.7404556274414062e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "RoBERTa", "normalizedForm": "RoBERTa", "offsetStart": 23, "offsetEnd": 30}, "context": "Independently of that, RoBERTa shows more reliable results when looking at the performance on the out of domain test sets. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.08756589889526367}, "created": {"value": false, "score": 7.3909759521484375e-06}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9959926009178162}, "created": {"value": false, "score": 0.10356295108795166}, "shared": {"value": false, "score": 0.09434634447097778}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "SciBERT", "normalizedForm": "SciBERT", "offsetStart": 28, "offsetEnd": 35}, "context": "We chose to use the uncased SciBERT model, meaning that we ignore the capitalization of words. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9994413256645203}, "created": {"value": false, "score": 0.0001232624053955078}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999760389328003}, "created": {"value": false, "score": 0.0001232624053955078}, "shared": {"value": false, "score": 3.5762786865234375e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "hyperopt 10", "normalizedForm": "hyperopt 10", "offsetStart": 37, "offsetEnd": 48}, "version": {"rawForm": "0.1.2", "normalizedForm": "0.1.2", "offsetStart": 57, "offsetEnd": 62}, "context": "Hyper parameter tuning was done with hyperopt 10 version 0.1.2. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9999916553497314}, "created": {"value": false, "score": 2.6226043701171875e-06}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999916553497314}, "created": {"value": false, "score": 2.6226043701171875e-06}, "shared": {"value": false, "score": 1.1920928955078125e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "huggingface", "normalizedForm": "huggingface", "offsetStart": 47, "offsetEnd": 59}, "version": {"rawForm": "2.3", "normalizedForm": "2.3", "offsetStart": 68, "offsetEnd": 71}, "context": "For BERT, we use the PyTorch implementation of huggingface9 version 2.3. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9992465972900391}, "created": {"value": false, "score": 1.7404556274414062e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9992465972900391}, "created": {"value": false, "score": 1.7404556274414062e-05}, "shared": {"value": true, "score": 0.9900696873664856}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Sci-", "normalizedForm": "Sci", "offsetStart": 48, "offsetEnd": 52}, "context": "The same configuration was used for fine-tuning Sci-and BioBERT. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9787676930427551}, "created": {"value": false, "score": 1.1086463928222656e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9787676930427551}, "created": {"value": false, "score": 1.1086463928222656e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "SciBERT", "normalizedForm": "SciBERT", "offsetStart": 48, "offsetEnd": 55}, "context": "For SciBERT, we used the uncased model with the SciBERT vocabulary.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9998212456703186}, "created": {"value": false, "score": 5.7220458984375e-06}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999760389328003}, "created": {"value": false, "score": 0.0001232624053955078}, "shared": {"value": false, "score": 3.5762786865234375e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "BioBERT", "normalizedForm": "BioBERT", "offsetStart": 56, "offsetEnd": 63}, "context": "The same configuration was used for fine-tuning Sci-and BioBERT. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9787676930427551}, "created": {"value": false, "score": 1.1086463928222656e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999783039093018}, "created": {"value": false, "score": 7.772445678710938e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "BPEmb", "normalizedForm": "BPEmb", "offsetStart": 57, "offsetEnd": 67}, "context": "Contrary to that, fastText [13] and byte-pair embeddings BPEmb [16] use subword segments to increase the capability of their vocabulary and might because of that be a better choice for a setting with unusual and specific terminology. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.0003459453582763672}, "created": {"value": false, "score": 1.2874603271484375e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": false, "score": 0.0003459453582763672}, "created": {"value": false, "score": 1.2874603271484375e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "SciBERT", "normalizedForm": "SciBERT", "offsetStart": 59, "offsetEnd": 66}, "context": "Comparing the specialized with the general models, Bio-and SciBERT show a better performance than the general BERT model, where the cased BioBERT tends to be more reliable for the out of domain test data.", "mentionContextAttributes": {"used": {"value": false, "score": 0.006371736526489258}, "created": {"value": false, "score": 3.5881996154785156e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999760389328003}, "created": {"value": false, "score": 0.0001232624053955078}, "shared": {"value": false, "score": 3.5762786865234375e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "SciBERT", "normalizedForm": "SciBERT", "offsetStart": 62, "offsetEnd": 69}, "context": "Comparing the specialized and general BERT model, the Bio-and SciBERT increase the performance by up to .06 ", "mentionContextAttributes": {"used": {"value": false, "score": 0.0002593398094177246}, "created": {"value": false, "score": 8.571147918701172e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999760389328003}, "created": {"value": false, "score": 0.0001232624053955078}, "shared": {"value": false, "score": 3.5762786865234375e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "SciBERT", "normalizedForm": "SciBERT", "offsetStart": 63, "offsetEnd": 70}, "context": "As it was the case for the original BERT, the uncased model of SciBERT performs slightly better for sentence classification tasks than the cased model. ", "mentionContextAttributes": {"used": {"value": false, "score": 5.650520324707031e-05}, "created": {"value": false, "score": 1.3709068298339844e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999760389328003}, "created": {"value": false, "score": 0.0001232624053955078}, "shared": {"value": false, "score": 3.5762786865234375e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "SciBERT", "normalizedForm": "SciBERT", "offsetStart": 64, "offsetEnd": 71}, "context": "We used the best performing sequence tagger, i.e. the finetuned SciBERT with a GRU and CRF. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9999760389328003}, "created": {"value": false, "score": 1.0848045349121094e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999760389328003}, "created": {"value": false, "score": 0.0001232624053955078}, "shared": {"value": false, "score": 3.5762786865234375e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "SciBERT", "normalizedForm": "SciBERT", "offsetStart": 64, "offsetEnd": 71}, "context": "Interestingly, comparing the two domain adapted models, Bio-and SciBERT, there were no regular errors, which allows any conclusion about the advantages or disadvantages of one model. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9989938139915466}, "created": {"value": false, "score": 2.300739288330078e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999760389328003}, "created": {"value": false, "score": 0.0001232624053955078}, "shared": {"value": false, "score": 3.5762786865234375e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "RoBERTa", "normalizedForm": "RoBERTa", "offsetStart": 70, "offsetEnd": 77}, "context": "We speculate that parts of the web crawl data which was used to train RoBERTa contain PubMed articles, since they are freely available on the web.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9628366231918335}, "created": {"value": false, "score": 0.00019484758377075195}, "shared": {"value": false, "score": 0.09434634447097778}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9959926009178162}, "created": {"value": false, "score": 0.10356295108795166}, "shared": {"value": false, "score": 0.09434634447097778}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "RoBERTa", "normalizedForm": "RoBERTa", "offsetStart": 112, "offsetEnd": 123}, "context": "Another new model, which outperforms BERT on the General Language Understanding Evaluation (GLUE) benchmark, is RoBERTa [23]. ", "mentionContextAttributes": {"used": {"value": false, "score": 5.8770179748535156e-05}, "created": {"value": false, "score": 0.0004417300224304199}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9959926009178162}, "created": {"value": false, "score": 0.10356295108795166}, "shared": {"value": false, "score": 0.09434634447097778}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "RoBERTa", "normalizedForm": "RoBERTa", "offsetStart": 113, "offsetEnd": 120}, "context": "While SciBERT as the best performing system on the in-domain test set drops .06 points on the glaucoma test set, RoBERTa stays almost the same and only drops from .67 to .66", "mentionContextAttributes": {"used": {"value": false, "score": 0.15614408254623413}, "created": {"value": false, "score": 2.6106834411621094e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9959926009178162}, "created": {"value": false, "score": 0.10356295108795166}, "shared": {"value": false, "score": 0.09434634447097778}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "BioBERT", "normalizedForm": "BioBERT", "offsetStart": 125, "offsetEnd": 132}, "context": "While SciBERT is trained on full papers from Semantic Scholar it also contains biomedical data, but to a smaller degree than BioBERT. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.0033835768699645996}, "created": {"value": false, "score": 7.772445678710938e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999783039093018}, "created": {"value": false, "score": 7.772445678710938e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "BioBERT", "normalizedForm": "BioBERT", "offsetStart": 138, "offsetEnd": 145}, "context": "Comparing the specialized with the general models, Bio-and SciBERT show a better performance than the general BERT model, where the cased BioBERT tends to be more reliable for the out of domain test data.", "mentionContextAttributes": {"used": {"value": false, "score": 0.006371736526489258}, "created": {"value": false, "score": 3.5881996154785156e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999783039093018}, "created": {"value": false, "score": 7.772445678710938e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}}], "references": [], "runtime": 13847, "id": "f38074b8402b8e6e41da71e614ab9c2f39c3b21f", "metadata": {"id": "f38074b8402b8e6e41da71e614ab9c2f39c3b21f"}, "original_file_path": "../../datalake/Samuel/SOFTware-Sync/downloads/xml/hal-02879293.grobid.tei.xml", "file_name": "hal-02879293.grobid.tei.xml"}