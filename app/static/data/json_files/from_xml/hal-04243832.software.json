{"application": "software-mentions", "version": "0.8.0", "date": "2024-10-07T12:08+0000", "md5": "CCE9DE3FD39C5F53C88A32710E6CDF85", "mentions": [{"type": "software", "software-type": "software", "software-name": {"rawForm": "NArabizi", "normalizedForm": "NArabizi", "offsetStart": 0, "offsetEnd": 8}, "context": "NArabizi The Arabic language exhibits diglossia, where Modern Standard Arabic (MSA) is employed in formal contexts, while dialectal forms are used informally (Habash, 2010).", "mentionContextAttributes": {"used": {"value": false, "score": 0.0010074973106384277}, "created": {"value": false, "score": 6.198883056640625e-05}, "shared": {"value": false, "score": 8.344650268554688e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.999618411064148}, "created": {"value": true, "score": 0.9998728036880493}, "shared": {"value": false, "score": 1.6570091247558594e-05}}, "references": [{"label": "(Seddah et al., 2020)", "normalizedForm": "Seddah et al., 2020", "refKey": 42}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "NArabizi", "normalizedForm": "NArabizi", "offsetStart": 4, "offsetEnd": 12}, "context": "New NArabizi CharacterBert Model Following Riabi et al. (2021), we train a CharacterBERT (El Boukkouri et al., 2020) model, a characterbased BERT variant, on a NArabizi new filtered corpus.", "mentionContextAttributes": {"used": {"value": false, "score": 0.00036728382110595703}, "created": {"value": false, "score": 0.498552143573761}, "shared": {"value": false, "score": 4.76837158203125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.999618411064148}, "created": {"value": true, "score": 0.9998728036880493}, "shared": {"value": false, "score": 1.6570091247558594e-05}}, "references": [{"label": "(Seddah et al., 2020)", "normalizedForm": "Seddah et al., 2020", "refKey": 42}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "CharacterBERT", "normalizedForm": "CharacterBERT", "offsetStart": 4, "offsetEnd": 17}, "context": "The CharacterBERT model exhibits the best performance on gold and predicted tokenization. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.10167407989501953}, "created": {"value": false, "score": 8.547306060791016e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9991863369941711}, "created": {"value": true, "score": 0.9702248573303223}, "shared": {"value": false, "score": 4.76837158203125e-07}}, "references": [{"label": "Boukkouri et al., 2020)", "normalizedForm": "Boukkouri et al., 2020)", "refKey": 19}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "CharacterBERT", "normalizedForm": "CharacterBERT", "offsetStart": 4, "offsetEnd": 17}, "context": "The CharacterBERT model achieves the highest F1 scores for LOC and OTH categories, as well as the best performance for PERderiv and PERderivA.", "mentionContextAttributes": {"used": {"value": false, "score": 0.00165557861328125}, "created": {"value": false, "score": 5.447864532470703e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9991863369941711}, "created": {"value": true, "score": 0.9702248573303223}, "shared": {"value": false, "score": 4.76837158203125e-07}}, "references": [{"label": "Boukkouri et al., 2020)", "normalizedForm": "Boukkouri et al., 2020)", "refKey": 19}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "CharacterBERT", "normalizedForm": "CharacterBERT", "offsetStart": 6, "offsetEnd": 19}, "context": "Since CharacterBERT uses character-level information, it is more robust to noise, which explains the high performances for those entities.", "mentionContextAttributes": {"used": {"value": false, "score": 7.593631744384766e-05}, "created": {"value": false, "score": 2.9087066650390625e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9991863369941711}, "created": {"value": true, "score": 0.9702248573303223}, "shared": {"value": false, "score": 4.76837158203125e-07}}, "references": [{"label": "Boukkouri et al., 2020)", "normalizedForm": "Boukkouri et al., 2020)", "refKey": 19}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "UDPipe", "normalizedForm": "UDPipe", "offsetStart": 11, "offsetEnd": 17}, "context": "We use the UDPipe tokenizer (Straka et al., 2016) that employs a Gated Linear Units (GRUs) (Cho We conduct a 5-fold evaluation using the UD-Pipe tokenizer and assess its performance based on the token-level, multiword, and word-level scores. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9872426986694336}, "created": {"value": false, "score": 8.463859558105469e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9872426986694336}, "created": {"value": true, "score": 0.8389037251472473}, "shared": {"value": false, "score": 2.384185791015625e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "NArabizi", "normalizedForm": "NArabizi", "offsetStart": 13, "offsetEnd": 21}, "context": "Our enhanced NArabizi Treebank paves the way for creating sophisticated language models and NLP tools for this under-represented language.", "mentionContextAttributes": {"used": {"value": false, "score": 6.854534149169922e-05}, "created": {"value": true, "score": 0.9987325072288513}, "shared": {"value": false, "score": 4.76837158203125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.999618411064148}, "created": {"value": true, "score": 0.9998728036880493}, "shared": {"value": false, "score": 1.6570091247558594e-05}}, "references": [{"label": "(Seddah et al., 2020)", "normalizedForm": "Seddah et al., 2020", "refKey": 42}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "NArabizi", "normalizedForm": "NArabizi", "offsetStart": 13, "offsetEnd": 21}, "context": "The original NArabizi treebank (Seddah et al., 2020), contains about 1500 sentences.", "mentionContextAttributes": {"used": {"value": false, "score": 0.46031928062438965}, "created": {"value": false, "score": 1.4901161193847656e-05}, "shared": {"value": false, "score": 5.960464477539062e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.999618411064148}, "created": {"value": true, "score": 0.9998728036880493}, "shared": {"value": false, "score": 1.6570091247558594e-05}}, "references": [{"label": "(Seddah et al., 2020)", "normalizedForm": "Seddah et al., 2020", "refKey": 42, "offsetStart": 6484, "offsetEnd": 6505}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "NArabizi", "normalizedForm": "NArabizi", "offsetStart": 15, "offsetEnd": 23}, "context": "To exclude non-NArabizi content, we first use a language detection tool (Nakatani, 2010) with a 0.9 confidence threshold to eliminate text in French, English, Hindi, Indonesian, and Russian, which are commonly found in mixed Arabizi data.", "mentionContextAttributes": {"used": {"value": true, "score": 0.5889856219291687}, "created": {"value": false, "score": 0.00021284818649291992}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.999618411064148}, "created": {"value": true, "score": 0.9998728036880493}, "shared": {"value": false, "score": 1.6570091247558594e-05}}, "references": [{"label": "(Seddah et al., 2020)", "normalizedForm": "Seddah et al., 2020", "refKey": 42}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "mBERT", "normalizedForm": "mBERT", "offsetStart": 30, "offsetEnd": 35}, "context": "For example, we fail to train mBERT as it only predicts non-offensive labels corresponding to the majority class.", "mentionContextAttributes": {"used": {"value": false, "score": 0.027163684368133545}, "created": {"value": false, "score": 4.3392181396484375e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9900897741317749}, "created": {"value": false, "score": 4.3392181396484375e-05}, "shared": {"value": false, "score": 3.5762786865234375e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "NArabizi", "normalizedForm": "NArabizi", "offsetStart": 36, "offsetEnd": 44}, "context": "We introduce an enriched version of NArabizi Treebank (Seddah et al., 2020) with three main contributions: the addition of two novel annotation layers (named entity recognition and offensive language detection) and a re-annotation of the tokenization, morpho-syntactic and syntactic layers that ensure annotation consistency. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.00012171268463134766}, "created": {"value": true, "score": 0.8772986531257629}, "shared": {"value": false, "score": 4.76837158203125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.999618411064148}, "created": {"value": true, "score": 0.9998728036880493}, "shared": {"value": false, "score": 1.6570091247558594e-05}}, "references": [{"label": "(Seddah et al., 2020)", "normalizedForm": "Seddah et al., 2020", "refKey": 42, "offsetStart": 256, "offsetEnd": 277}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "NArabizi", "normalizedForm": "NArabizi", "offsetStart": 46, "offsetEnd": 54}, "context": "Future research could emphasize expanding the NArabizi Treebank towards other dialects and ex-amining the treebank's potential applications in various NLP tasks.", "mentionContextAttributes": {"used": {"value": false, "score": 0.00043135881423950195}, "created": {"value": true, "score": 0.9840732216835022}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.999618411064148}, "created": {"value": true, "score": 0.9998728036880493}, "shared": {"value": false, "score": 1.6570091247558594e-05}}, "references": [{"label": "(Seddah et al., 2020)", "normalizedForm": "Seddah et al., 2020", "refKey": 42}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "NArabizi", "normalizedForm": "NArabizi", "offsetStart": 47, "offsetEnd": 55}, "context": "Our work contributes to the enhancement of the NArabizi Treebank, making it a valuable resource for research on low-resource languages and user-generated content with high variability.", "mentionContextAttributes": {"used": {"value": false, "score": 0.00011849403381347656}, "created": {"value": true, "score": 0.9994074106216431}, "shared": {"value": false, "score": 1.6570091247558594e-05}}, "documentContextAttributes": {"used": {"value": true, "score": 0.999618411064148}, "created": {"value": true, "score": 0.9998728036880493}, "shared": {"value": false, "score": 1.6570091247558594e-05}}, "references": [{"label": "(Seddah et al., 2020)", "normalizedForm": "Seddah et al., 2020", "refKey": 42}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "NArabizi", "normalizedForm": "NArabizi", "offsetStart": 53, "offsetEnd": 61}, "context": "We explore the impact of tokenization on the refined NArabizi treebank, employing the UDPipe tokenizer for our evaluation.", "mentionContextAttributes": {"used": {"value": false, "score": 0.002259373664855957}, "created": {"value": true, "score": 0.8389037251472473}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.999618411064148}, "created": {"value": true, "score": 0.9998728036880493}, "shared": {"value": false, "score": 1.6570091247558594e-05}}, "references": [{"label": "(Seddah et al., 2020)", "normalizedForm": "Seddah et al., 2020", "refKey": 42}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "NArabizi", "normalizedForm": "NArabizi", "offsetStart": 57, "offsetEnd": 65}, "context": "In this context, we propose a heavily revised version of NArabizi treebank (Seddah et al., 2020) that includes two novel annotation layers for Named Entity Recognition (NER) and offensive language detection.", "mentionContextAttributes": {"used": {"value": false, "score": 4.1961669921875e-05}, "created": {"value": true, "score": 0.9998570680618286}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.999618411064148}, "created": {"value": true, "score": 0.9998728036880493}, "shared": {"value": false, "score": 1.6570091247558594e-05}}, "references": [{"label": "(Seddah et al., 2020)", "normalizedForm": "Seddah et al., 2020", "refKey": 42, "offsetStart": 4524, "offsetEnd": 4545}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "mBERT", "normalizedForm": "mBERT", "offsetStart": 58, "offsetEnd": 63}, "context": "For the multilingual subword-based language model, we use mBERT, the multilingual version of BERT (Devlin et al., 2018). ", "mentionContextAttributes": {"used": {"value": false, "score": 0.4846290946006775}, "created": {"value": false, "score": 3.2901763916015625e-05}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9900897741317749}, "created": {"value": false, "score": 4.3392181396484375e-05}, "shared": {"value": false, "score": 3.5762786865234375e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "NArabizi", "normalizedForm": "NArabizi", "offsetStart": 59, "offsetEnd": 67}, "context": "Touileb (2022) recently introduced NERDz, a version of the NArabizi treebank annotated for NER.", "mentionContextAttributes": {"used": {"value": false, "score": 4.398822784423828e-05}, "created": {"value": true, "score": 0.8832964301109314}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.999618411064148}, "created": {"value": true, "score": 0.9998728036880493}, "shared": {"value": false, "score": 1.6570091247558594e-05}}, "references": [{"label": "(Seddah et al., 2020)", "normalizedForm": "Seddah et al., 2020", "refKey": 42}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "NArabizi", "normalizedForm": "NArabizi", "offsetStart": 60, "offsetEnd": 68}, "context": "In this paper we address the scarcity of annotated data for NArabizi, a Romanized form of North African Arabic used mostly on social media, which poses challenges for Natural Language Processing (NLP).", "mentionContextAttributes": {"used": {"value": false, "score": 0.00010037422180175781}, "created": {"value": true, "score": 0.9998326301574707}, "shared": {"value": false, "score": 5.960464477539062e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.999618411064148}, "created": {"value": true, "score": 0.9998728036880493}, "shared": {"value": false, "score": 1.6570091247558594e-05}}, "references": [{"label": "(Seddah et al., 2020)", "normalizedForm": "Seddah et al., 2020", "refKey": 42}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "CharacterBERT", "normalizedForm": "CharacterBERT", "offsetStart": 60, "offsetEnd": 73}, "context": "The DziriBERT model exhibits the best performance; however, CharacterBERT delivers competitive results while being trained on a mere 7.5% of the data used for training DziriBERT. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.006636857986450195}, "created": {"value": false, "score": 4.470348358154297e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9991863369941711}, "created": {"value": true, "score": 0.9702248573303223}, "shared": {"value": false, "score": 4.76837158203125e-07}}, "references": [{"label": "Boukkouri et al., 2020)", "normalizedForm": "Boukkouri et al., 2020)", "refKey": 19}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "NArabizi", "normalizedForm": "NArabizi", "offsetStart": 62, "offsetEnd": 70}, "context": "This understanding primarily drives the goal of improving the NArabizi treebank's annotations.", "mentionContextAttributes": {"used": {"value": false, "score": 0.00012445449829101562}, "created": {"value": false, "score": 0.04427057504653931}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.999618411064148}, "created": {"value": true, "score": 0.9998728036880493}, "shared": {"value": false, "score": 1.6570091247558594e-05}}, "references": [{"label": "(Seddah et al., 2020)", "normalizedForm": "Seddah et al., 2020", "refKey": 42}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "CharacterBERT", "normalizedForm": "CharacterBERT", "offsetStart": 63, "offsetEnd": 97}, "context": "Both models' final training data sizes are comparable: 99k for CharacterBERT (Riabi et al., 2021) and 91k for our CharacterBERT. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.8265509605407715}, "created": {"value": false, "score": 7.987022399902344e-06}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9991863369941711}, "created": {"value": true, "score": 0.9702248573303223}, "shared": {"value": false, "score": 4.76837158203125e-07}}, "references": [{"label": "Boukkouri et al., 2020)", "normalizedForm": "Boukkouri et al., 2020)", "refKey": 19}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "NArabizi", "normalizedForm": "NArabizi", "offsetStart": 64, "offsetEnd": 72}, "context": "This annotated text is then used to train an SVM classifier for NArabizi detection.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9442254304885864}, "created": {"value": false, "score": 7.462501525878906e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.999618411064148}, "created": {"value": true, "score": 0.9998728036880493}, "shared": {"value": false, "score": 1.6570091247558594e-05}}, "references": [{"label": "(Seddah et al., 2020)", "normalizedForm": "Seddah et al., 2020", "refKey": 42}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "NArabizi", "normalizedForm": "NArabizi", "offsetStart": 67, "offsetEnd": 75}, "context": "This written form, known as Arabizi and its North African variant, NArabizi, often showcases code-switching with French and Amazigh (Amazouz et al., 2017).", "mentionContextAttributes": {"used": {"value": false, "score": 0.00016570091247558594}, "created": {"value": false, "score": 0.0025600194931030273}, "shared": {"value": false, "score": 4.76837158203125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.999618411064148}, "created": {"value": true, "score": 0.9998728036880493}, "shared": {"value": false, "score": 1.6570091247558594e-05}}, "references": [{"label": "(Seddah et al., 2020)", "normalizedForm": "Seddah et al., 2020", "refKey": 42}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "CharacterBERT", "normalizedForm": "CharacterBERT", "offsetStart": 71, "offsetEnd": 84}, "context": "In Appendix A, we present the results of all our experiments using the CharacterBERT model trained by Riabi et al. (2021). ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9771803617477417}, "created": {"value": true, "score": 0.9702248573303223}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9991863369941711}, "created": {"value": true, "score": 0.9702248573303223}, "shared": {"value": false, "score": 4.76837158203125e-07}}, "references": [{"label": "Boukkouri et al., 2020)", "normalizedForm": "Boukkouri et al., 2020)", "refKey": 19}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "NArabizi", "normalizedForm": "NArabizi", "offsetStart": 75, "offsetEnd": 83}, "context": "Tokenization We address tokenization concerns to uphold consistency in the NArabizi Treebank annotations.", "mentionContextAttributes": {"used": {"value": false, "score": 0.0013559460639953613}, "created": {"value": true, "score": 0.554993212223053}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.999618411064148}, "created": {"value": true, "score": 0.9998728036880493}, "shared": {"value": false, "score": 1.6570091247558594e-05}}, "references": [{"label": "(Seddah et al., 2020)", "normalizedForm": "Seddah et al., 2020", "refKey": 42}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "NArabizi", "normalizedForm": "NArabizi", "offsetStart": 75, "offsetEnd": 83}, "context": "In this section, we investigate the tokenization influence on the enhanced NArabizi Treebank, with a particular emphasis on the homogenization of the tokenization7 and its subsequent impact on our tasks.", "mentionContextAttributes": {"used": {"value": false, "score": 0.008953392505645752}, "created": {"value": true, "score": 0.6941108107566833}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.999618411064148}, "created": {"value": true, "score": 0.9998728036880493}, "shared": {"value": false, "score": 1.6570091247558594e-05}}, "references": [{"label": "(Seddah et al., 2020)", "normalizedForm": "Seddah et al., 2020", "refKey": 42}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "CharacterBERT", "normalizedForm": "CharacterBERT", "offsetStart": 75, "offsetEnd": 88}, "context": "New NArabizi CharacterBert Model Following Riabi et al. (2021), we train a CharacterBERT (El Boukkouri et al., 2020) model, a characterbased BERT variant, on a NArabizi new filtered corpus.", "mentionContextAttributes": {"used": {"value": false, "score": 0.00036728382110595703}, "created": {"value": false, "score": 0.498552143573761}, "shared": {"value": false, "score": 4.76837158203125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9991863369941711}, "created": {"value": true, "score": 0.9702248573303223}, "shared": {"value": false, "score": 4.76837158203125e-07}}, "references": [{"label": "Boukkouri et al., 2020)", "normalizedForm": "Boukkouri et al., 2020)", "refKey": 19, "offsetStart": 20458, "offsetEnd": 20481}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "NArabizi", "normalizedForm": "NArabizi", "offsetStart": 76, "offsetEnd": 84}, "context": "In this section, we outline our methodology for expanding and enhancing the NArabizi treebank.", "mentionContextAttributes": {"used": {"value": false, "score": 5.435943603515625e-05}, "created": {"value": true, "score": 0.9998728036880493}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.999618411064148}, "created": {"value": true, "score": 0.9998728036880493}, "shared": {"value": false, "score": 1.6570091247558594e-05}}, "references": [{"label": "(Seddah et al., 2020)", "normalizedForm": "Seddah et al., 2020", "refKey": 42}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "NArabizi", "normalizedForm": "NArabizi", "offsetStart": 76, "offsetEnd": 84}, "context": "Muller et al. (2020) demonstrated that such a model could be transferred to NArabizi to some degree.", "mentionContextAttributes": {"used": {"value": true, "score": 0.95218425989151}, "created": {"value": false, "score": 0.013319432735443115}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.999618411064148}, "created": {"value": true, "score": 0.9998728036880493}, "shared": {"value": false, "score": 1.6570091247558594e-05}}, "references": [{"label": "(Seddah et al., 2020)", "normalizedForm": "Seddah et al., 2020", "refKey": 42}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "UDPipe", "normalizedForm": "UDPipe", "offsetStart": 86, "offsetEnd": 92}, "context": "We explore the impact of tokenization on the refined NArabizi treebank, employing the UDPipe tokenizer for our evaluation. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.002259373664855957}, "created": {"value": true, "score": 0.8389037251472473}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9872426986694336}, "created": {"value": true, "score": 0.8389037251472473}, "shared": {"value": false, "score": 2.384185791015625e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "NArabizi", "normalizedForm": "NArabizi", "offsetStart": 87, "offsetEnd": 95}, "context": "As mentioned above, Touileb (2022) added a NER annotation for the first version of the NArabizi treebank.", "mentionContextAttributes": {"used": {"value": false, "score": 0.0016487836837768555}, "created": {"value": false, "score": 8.749961853027344e-05}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.999618411064148}, "created": {"value": true, "score": 0.9998728036880493}, "shared": {"value": false, "score": 1.6570091247558594e-05}}, "references": [{"label": "(Seddah et al., 2020)", "normalizedForm": "Seddah et al., 2020", "refKey": 42}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "NArabizi", "normalizedForm": "NArabizi", "offsetStart": 89, "offsetEnd": 97}, "context": "In this paper, we present a comprehensive study on the development and refinement of the NArabizi Treebank (Seddah et al., 2020) by improving its annotations, consistency, and tokenization, as well as providing new annotations for NER and offensive language.", "mentionContextAttributes": {"used": {"value": false, "score": 0.0001436471939086914}, "created": {"value": true, "score": 0.9998594522476196}, "shared": {"value": false, "score": 8.344650268554688e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.999618411064148}, "created": {"value": true, "score": 0.9998728036880493}, "shared": {"value": false, "score": 1.6570091247558594e-05}}, "references": [{"label": "(Seddah et al., 2020)", "normalizedForm": "Seddah et al., 2020", "refKey": 42, "offsetStart": 30002, "offsetEnd": 30023}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "mBERT", "normalizedForm": "mBERT", "offsetStart": 95, "offsetEnd": 100}, "context": "This observation is further substan- tiated by examining the performance of Character-BERT and mBERT, reinforcing the validity of the noted improvements.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9900897741317749}, "created": {"value": false, "score": 1.823902130126953e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9900897741317749}, "created": {"value": false, "score": 4.3392181396484375e-05}, "shared": {"value": false, "score": 3.5762786865234375e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "CharacterBERT", "normalizedForm": "CharacterBERT", "offsetStart": 98, "offsetEnd": 111}, "context": "We observe a heterogeneous improvement in performance, with predominantly better outcomes for our CharacterBERT.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9991863369941711}, "created": {"value": false, "score": 0.008438289165496826}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9991863369941711}, "created": {"value": true, "score": 0.9702248573303223}, "shared": {"value": false, "score": 4.76837158203125e-07}}, "references": [{"label": "Boukkouri et al., 2020)", "normalizedForm": "Boukkouri et al., 2020)", "refKey": 19}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "NArabizi", "normalizedForm": "NArabizi", "offsetStart": 106, "offsetEnd": 114}, "context": "We improve the initial pre-training dataset used by Riabi et al. (2021) by more stringently filtering non-NArabizi examples from the 99k instances provided by Seddah et al. (2020), as well as incorporating new samples from the CTAB corpus (Amara et al., 2021) and 12k comments extracted from various Facebook and forum posts, mostly in the Tunisian dialect taken from different datasets listed by Younes et al. (2020).", "mentionContextAttributes": {"used": {"value": true, "score": 0.9873248338699341}, "created": {"value": false, "score": 1.4662742614746094e-05}, "shared": {"value": false, "score": 8.344650268554688e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.999618411064148}, "created": {"value": true, "score": 0.9998728036880493}, "shared": {"value": false, "score": 1.6570091247558594e-05}}, "references": [{"label": "(Seddah et al., 2020)", "normalizedForm": "Seddah et al., 2020", "refKey": 42}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "CharacterBERT", "normalizedForm": "CharacterBERT", "offsetStart": 114, "offsetEnd": 127}, "context": "Both models' final training data sizes are comparable: 99k for CharacterBERT (Riabi et al., 2021) and 91k for our CharacterBERT. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.8265504240989685}, "created": {"value": false, "score": 7.987022399902344e-06}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9991863369941711}, "created": {"value": true, "score": 0.9702248573303223}, "shared": {"value": false, "score": 4.76837158203125e-07}}, "references": [{"label": "Boukkouri et al., 2020)", "normalizedForm": "Boukkouri et al., 2020)", "refKey": 19}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "NArabizi", "normalizedForm": "NArabizi", "offsetStart": 118, "offsetEnd": 126}, "context": "Furthermore, analyzing the number of sentences without a verb and the average number of verbs per sentence shows that NArabizi speakers tend to favor nominalization, as seen in the abundance of ellipses (e.g., \"rabbi m3ak\" which translates in English to \"God bless you\").", "mentionContextAttributes": {"used": {"value": false, "score": 0.01145082712173462}, "created": {"value": false, "score": 0.00015354156494140625}, "shared": {"value": false, "score": 1.7881393432617188e-06}}, "documentContextAttributes": {"used": {"value": true, "score": 0.999618411064148}, "created": {"value": true, "score": 0.9998728036880493}, "shared": {"value": false, "score": 1.6570091247558594e-05}}, "references": [{"label": "(Seddah et al., 2020)", "normalizedForm": "Seddah et al., 2020", "refKey": 42}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "NArabizi", "normalizedForm": "NArabizi", "offsetStart": 129, "offsetEnd": 137}, "context": "Offensive Language Detection Named Entity Recognition Our NER annotation guidelines are based on the revised tokenization of the NArabizi treebank, which ensures consistency between token-level annotations, an essential aspect of multi-task learning.", "mentionContextAttributes": {"used": {"value": false, "score": 0.3245089054107666}, "created": {"value": false, "score": 0.0005628466606140137}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.999618411064148}, "created": {"value": true, "score": 0.9998728036880493}, "shared": {"value": false, "score": 1.6570091247558594e-05}}, "references": [{"label": "(Seddah et al., 2020)", "normalizedForm": "Seddah et al., 2020", "refKey": 42}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "NArabizi", "normalizedForm": "NArabizi", "offsetStart": 160, "offsetEnd": 168}, "context": "New NArabizi CharacterBert Model Following Riabi et al. (2021), we train a CharacterBERT (El Boukkouri et al., 2020) model, a characterbased BERT variant, on a NArabizi new filtered corpus.", "mentionContextAttributes": {"used": {"value": false, "score": 0.00036728382110595703}, "created": {"value": false, "score": 0.49855202436447144}, "shared": {"value": false, "score": 4.76837158203125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.999618411064148}, "created": {"value": true, "score": 0.9998728036880493}, "shared": {"value": false, "score": 1.6570091247558594e-05}}, "references": [{"label": "(Seddah et al., 2020)", "normalizedForm": "Seddah et al., 2020", "refKey": 42}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "NArabizi", "normalizedForm": "NArabizi", "offsetStart": 188, "offsetEnd": 196}, "context": "The results in Table 7 show high scores for the tokens and words F1 scores demonstrate the tokenizer's efficacy in handling various tokens and words, which shows that the tokenization for NArabizi is learnable.", "mentionContextAttributes": {"used": {"value": true, "score": 0.999618411064148}, "created": {"value": false, "score": 6.794929504394531e-06}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.999618411064148}, "created": {"value": true, "score": 0.9998728036880493}, "shared": {"value": false, "score": 1.6570091247558594e-05}}, "references": [{"label": "(Seddah et al., 2020)", "normalizedForm": "Seddah et al., 2020", "refKey": 42}]}], "references": [{"refKey": 42, "tei": "<biblStruct xml:id=\"b42\">\n\t<analytic>\n\t\t<title level=\"a\" type=\"main\">Building a User-Generated Content North-African Arabizi Treebank: Tackling Hell</title>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Djam\u00e9</forename><surname>Seddah</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Farah</forename><surname>Essaidi</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Amal</forename><surname>Fethi</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Matthieu</forename><surname>Futeral</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Benjamin</forename><surname>Muller</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Pedro</forename><forename type=\"middle\">Javier</forename><surname>Ortiz Su\u00e1rez</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Beno\u00eet</forename><surname>Sagot</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Abhishek</forename><surname>Srivastava</surname></persName>\n\t\t</author>\n\t\t<idno type=\"DOI\">10.18653/v1/2020.acl-main.107</idno>\n\t</analytic>\n\t<monogr>\n\t\t<title level=\"m\">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>\n\t\t<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>\n\t\t<imprint>\n\t\t\t<publisher>Association for Computational Linguistics</publisher>\n\t\t\t<date type=\"published\" when=\"2020\">2020</date>\n\t\t\t<biblScope unit=\"page\" from=\"1139\" to=\"1150\" />\n\t\t</imprint>\n\t</monogr>\n</biblStruct>\n"}, {"refKey": 19, "tei": "<biblStruct xml:id=\"b19\">\n\t<analytic>\n\t\t<title level=\"a\" type=\"main\">CharacterBERT: Reconciling ELMo and BERT for Word-Level Open-Vocabulary Representations From Characters</title>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Hicham</forename><surname>El Boukkouri</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Olivier</forename><surname>Ferret</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Thomas</forename><surname>Lavergne</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Hiroshi</forename><surname>Noji</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Pierre</forename><surname>Zweigenbaum</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Jun\u2019ichi</forename><surname>Tsujii</surname></persName>\n\t\t</author>\n\t\t<idno type=\"DOI\">10.18653/v1/2020.coling-main.609</idno>\n\t</analytic>\n\t<monogr>\n\t\t<title level=\"m\">Proceedings of the 28th International Conference on Computational Linguistics</title>\n\t\t<meeting>the 28th International Conference on Computational Linguistics</meeting>\n\t\t<imprint>\n\t\t\t<publisher>International Committee on Computational Linguistics</publisher>\n\t\t\t<date type=\"published\" when=\"2020\">2020</date>\n\t\t\t<biblScope unit=\"page\">6915</biblScope>\n\t\t</imprint>\n\t</monogr>\n</biblStruct>\n"}], "runtime": 11851, "id": "14b80ccfb6798edeb022a607e5c7e23fcc846253", "metadata": {"id": "14b80ccfb6798edeb022a607e5c7e23fcc846253"}, "original_file_path": "../../datalake/Samuel/SOFTware-Sync/downloads/xml/hal-04243832.grobid.tei.xml", "file_name": "hal-04243832.grobid.tei.xml"}