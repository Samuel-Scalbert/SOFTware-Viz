{"application": "software-mentions", "version": "0.8.0", "date": "2024-10-07T11:49+0000", "md5": "6FE0BBB92E5C109EA003717BBCA12277", "mentions": [{"type": "software", "software-type": "software", "software-name": {"rawForm": "FastAlign", "normalizedForm": "FastAlign", "offsetStart": 4, "offsetEnd": 13}, "context": "For FastAlign, we produced alignments in both direction and symmetrize with the grow-diag-final-and heuristic provided with FastAlign, following the setting of Wu and Dredze (2020b). ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9976139068603516}, "created": {"value": false, "score": 0.33503055572509766}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999840259552002}, "created": {"value": false, "score": 0.33503055572509766}, "shared": {"value": false, "score": 2.9802322387695312e-06}}, "references": [{"label": "(Dyer et al., 2013)", "normalizedForm": "Dyer et al., 2013", "refKey": 13}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "mBERT", "normalizedForm": "mBERT", "offsetStart": 12, "offsetEnd": 17}, "context": "MLLMs, like mBERT (Devlin et al., 2019) and XLM-R (Conneau et al., 2020a), are Transformer Figure 1: Cross-lingual transfer between English and Arabic with and without realignment, using a bilingual dictionary.", "mentionContextAttributes": {"used": {"value": false, "score": 0.00015652179718017578}, "created": {"value": false, "score": 4.291534423828125e-06}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9998948574066162}, "created": {"value": false, "score": 0.0001188516616821289}, "shared": {"value": false, "score": 1.5497207641601562e-06}}, "references": [{"label": "(Devlin et al., 2019)", "normalizedForm": "Devlin et al., 2019", "refKey": 10, "offsetStart": 1877, "offsetEnd": 1898}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "mBERT", "normalizedForm": "mBERT", "offsetStart": 16, "offsetEnd": 21}, "context": "distilmBERT and mBERT mainly show a decrease in alignment for POS-tagging and NER, and smaller improvements than other models on NLI.", "mentionContextAttributes": {"used": {"value": false, "score": 0.037806034088134766}, "created": {"value": false, "score": 9.179115295410156e-06}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9998948574066162}, "created": {"value": false, "score": 0.0001188516616821289}, "shared": {"value": false, "score": 1.5497207641601562e-06}}, "references": [{"label": "(Devlin et al., 2019)", "normalizedForm": "Devlin et al., 2019", "refKey": 10}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "mBERT", "normalizedForm": "mBERT", "offsetStart": 23, "offsetEnd": 28}, "context": "Despite the ability of mBERT and XLM-R to perform CTL, there lacks consensus on whether they actually hold aligned representations (Gaschi et al., 2022).", "mentionContextAttributes": {"used": {"value": false, "score": 0.001207888126373291}, "created": {"value": false, "score": 2.9206275939941406e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9998948574066162}, "created": {"value": false, "score": 0.0001188516616821289}, "shared": {"value": false, "score": 1.5497207641601562e-06}}, "references": [{"label": "(Devlin et al., 2019)", "normalizedForm": "Devlin et al., 2019", "refKey": 10}]}, {"type": "software", "software-type": "implicit", "software-name": {"rawForm": "packages", "normalizedForm": "packages", "offsetStart": 45, "offsetEnd": 53}, "language": {"rawForm": "Python", "normalizedForm": "Python", "wikidataId": "Q28865", "offsetStart": 38, "offsetEnd": 44}, "context": "We relied on the following scientific Python packages for our experiments: the Hugging-Face's libraries transformers (Wolf et al., 2020), datasets (Lhoest et al., 2021) and evaluate 4 , PyTorch (Paszke et al., 2019), NLTK (Bird et al., 2009) and its implementation of the Stanford Chinese Segmenter (Tseng et al., 2005), seqeval (Nakayama, 2018) for evaluating NER, NumPy (Harris et al., 2020), and AWESOME-align (Dou and Neubig, 2021), FastAlign (Dyer et al., 2013), and MUSE dictionaries (Lample et al., 2018) for extracting alignment pairs. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9995526671409607}, "created": {"value": false, "score": 2.4437904357910156e-05}, "shared": {"value": false, "score": 4.76837158203125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9995526671409607}, "created": {"value": false, "score": 2.4437904357910156e-05}, "shared": {"value": false, "score": 4.76837158203125e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "FastAlign", "normalizedForm": "FastAlign", "offsetStart": 47, "offsetEnd": 56}, "context": "Realignment methods using pairs extracted with FastAlign or AWESOME-align do not provide significant improvements over the baseline, whereas using a bilingual dictionary does. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.0024439096450805664}, "created": {"value": false, "score": 3.337860107421875e-06}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999840259552002}, "created": {"value": false, "score": 0.33503055572509766}, "shared": {"value": false, "score": 2.9802322387695312e-06}}, "references": [{"label": "(Dyer et al., 2013)", "normalizedForm": "Dyer et al., 2013", "refKey": 13}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "mBERT", "normalizedForm": "mBERT", "offsetStart": 50, "offsetEnd": 55}, "context": "For distant and low-resources languages, CTL with mBERT can give worse results than fine-tuning a Transformer from scratch (Wu and Dredze, 2020a).", "mentionContextAttributes": {"used": {"value": false, "score": 4.470348358154297e-05}, "created": {"value": false, "score": 3.325939178466797e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9998948574066162}, "created": {"value": false, "score": 0.0001188516616821289}, "shared": {"value": false, "score": 1.5497207641601562e-06}}, "references": [{"label": "(Devlin et al., 2019)", "normalizedForm": "Devlin et al., 2019", "refKey": 10}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "mBERT", "normalizedForm": "mBERT", "offsetStart": 51, "offsetEnd": 56}, "context": "XLM-R Base and Large, which are larger models than mBERT and distilmBERT, have a relative increase that can go as high as 25.36 on the NLI task for distant languages.", "mentionContextAttributes": {"used": {"value": false, "score": 0.00040847063064575195}, "created": {"value": false, "score": 3.790855407714844e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9998948574066162}, "created": {"value": false, "score": 0.0001188516616821289}, "shared": {"value": false, "score": 1.5497207641601562e-06}}, "references": [{"label": "(Devlin et al., 2019)", "normalizedForm": "Devlin et al., 2019", "refKey": 10}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "AWESOME-align", "normalizedForm": "AWESOME-align", "offsetStart": 60, "offsetEnd": 73}, "context": "Realignment methods using pairs extracted with FastAlign or AWESOME-align do not provide significant improvements over the baseline, whereas using a bilingual dictionary does. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.0024439096450805664}, "created": {"value": false, "score": 3.337860107421875e-06}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9995526671409607}, "created": {"value": false, "score": 2.777576446533203e-05}, "shared": {"value": false, "score": 4.76837158203125e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "mBERT", "normalizedForm": "mBERT", "offsetStart": 64, "offsetEnd": 69}, "context": "These results tend to show that smaller models (distilmBERT and mBERT) have a better correlation at the last layer than larger models.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9508352279663086}, "created": {"value": false, "score": 1.7881393432617188e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9998948574066162}, "created": {"value": false, "score": 0.0001188516616821289}, "shared": {"value": false, "score": 1.5497207641601562e-06}}, "references": [{"label": "(Devlin et al., 2019)", "normalizedForm": "Devlin et al., 2019", "refKey": 10}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "mBERT", "normalizedForm": "mBERT", "offsetStart": 67, "offsetEnd": 72}, "context": "Despite the absence of any explicit cross-lingual training signal, mBERT and XLM-R can be fine-tuned on a specific task in one language and then provide high accuracy when evaluated on another language on the same task (Pires et al., 2019;Wu and Dredze, 2019).", "mentionContextAttributes": {"used": {"value": false, "score": 0.00022214651107788086}, "created": {"value": false, "score": 7.903575897216797e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9998948574066162}, "created": {"value": false, "score": 0.0001188516616821289}, "shared": {"value": false, "score": 1.5497207641601562e-06}}, "references": [{"label": "(Devlin et al., 2019)", "normalizedForm": "Devlin et al., 2019", "refKey": 10}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "FastAlign", "normalizedForm": "FastAlign", "offsetStart": 69, "offsetEnd": 78}, "context": "Pairs of words are extracted from translated sentences, usually with FastAlign or a bilingual dictionary (Gaschi et al., 2022).", "mentionContextAttributes": {"used": {"value": true, "score": 0.976902961730957}, "created": {"value": false, "score": 2.1696090698242188e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999840259552002}, "created": {"value": false, "score": 0.33503055572509766}, "shared": {"value": false, "score": 2.9802322387695312e-06}}, "references": [{"label": "(Dyer et al., 2013)", "normalizedForm": "Dyer et al., 2013", "refKey": 13}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "FastAlign", "normalizedForm": "FastAlign", "offsetStart": 80, "offsetEnd": 89}, "context": "To extract contextualized pairs of translated words from a translation dataset, FastAlign is the most widely used word aligner in realignment methods (Wu and Dredze, 2020b;Cao et al., 2020;Zhao et al., 2021;Wang et al., 2019), but it is prone to errors and thus generates noisy training realignment data (Pan et al., 2021;Gaschi et al., 2022). ", "mentionContextAttributes": {"used": {"value": false, "score": 0.0065683722496032715}, "created": {"value": false, "score": 1.609325408935547e-05}, "shared": {"value": false, "score": 2.9802322387695312e-06}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999840259552002}, "created": {"value": false, "score": 0.33503055572509766}, "shared": {"value": false, "score": 2.9802322387695312e-06}}, "references": [{"label": "(Dyer et al., 2013)", "normalizedForm": "Dyer et al., 2013", "refKey": 13}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "FastAlign", "normalizedForm": "FastAlign", "offsetStart": 88, "offsetEnd": 116}, "context": "Realignment methods typically require a translation dataset and an alignment tool, like FastAlign (Dyer et al., 2013), to extract contextualized pairs of translated words that will be realigned.", "mentionContextAttributes": {"used": {"value": false, "score": 0.00010323524475097656}, "created": {"value": false, "score": 0.00011432170867919922}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999840259552002}, "created": {"value": false, "score": 0.33503055572509766}, "shared": {"value": false, "score": 2.9802322387695312e-06}}, "references": [{"label": "(Dyer et al., 2013)", "normalizedForm": "Dyer et al., 2013", "refKey": 13}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "mBERT", "normalizedForm": "mBERT", "offsetStart": 93, "offsetEnd": 98}, "context": "Each correlation value is obtained from 100 samples with four different models (distilmBERT, mBERT, XLM-R Base and Large), five target languages (Arabic, Spanish, French, Russian and Chinese) and five seeds for initialization of the classification head and shuffling of the fine-tuning data.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9998948574066162}, "created": {"value": false, "score": 1.811981201171875e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9998948574066162}, "created": {"value": false, "score": 0.0001188516616821289}, "shared": {"value": false, "score": 1.5497207641601562e-06}}, "references": [{"label": "(Devlin et al., 2019)", "normalizedForm": "Devlin et al., 2019", "refKey": 10}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "mBERT", "normalizedForm": "mBERT", "offsetStart": 113, "offsetEnd": 118}, "context": "When using a bilingual dictionary, it also brings a systematically significant improvement over the baseline for mBERT on POS-tagging.", "mentionContextAttributes": {"used": {"value": false, "score": 0.0005418658256530762}, "created": {"value": false, "score": 2.586841583251953e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9998948574066162}, "created": {"value": false, "score": 0.0001188516616821289}, "shared": {"value": false, "score": 1.5497207641601562e-06}}, "references": [{"label": "(Devlin et al., 2019)", "normalizedForm": "Devlin et al., 2019", "refKey": 10}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "mBERT", "normalizedForm": "mBERT", "offsetStart": 116, "offsetEnd": 121}, "context": "Finally, we worked with four different models: distilmBERT, which was released with distilBERT (Sanh et al., 2019), mBERT, which was released with BERT (Devlin et al., 2019) and XLM-R Base and Large (Conneau et al., 2020a).", "mentionContextAttributes": {"used": {"value": false, "score": 0.4766741394996643}, "created": {"value": false, "score": 0.0001188516616821289}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9998948574066162}, "created": {"value": false, "score": 0.0001188516616821289}, "shared": {"value": false, "score": 1.5497207641601562e-06}}, "references": [{"label": "(Devlin et al., 2019)", "normalizedForm": "Devlin et al., 2019", "refKey": 10}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "FastAlign", "normalizedForm": "FastAlign", "offsetStart": 116, "offsetEnd": 144}, "context": "Realignment methods rely on pairs of words extracted from translated sentences using a word alignment tool, usually FastAlign (Dyer et al., 2013), but other tools like AWESOME-align (Dou and Neubig, 2021) could be used. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.0036995410919189453}, "created": {"value": false, "score": 2.777576446533203e-05}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999840259552002}, "created": {"value": false, "score": 0.33503055572509766}, "shared": {"value": false, "score": 2.9802322387695312e-06}}, "references": [{"label": "(Dyer et al., 2013)", "normalizedForm": "Dyer et al., 2013", "refKey": 13}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "FastAlign", "normalizedForm": "FastAlign", "offsetStart": 124, "offsetEnd": 133}, "context": "For FastAlign, we produced alignments in both direction and symmetrize with the grow-diag-final-and heuristic provided with FastAlign, following the setting of Wu and Dredze (2020b). ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9976139068603516}, "created": {"value": false, "score": 0.33503055572509766}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999840259552002}, "created": {"value": false, "score": 0.33503055572509766}, "shared": {"value": false, "score": 2.9802322387695312e-06}}, "references": [{"label": "(Dyer et al., 2013)", "normalizedForm": "Dyer et al., 2013", "refKey": 13}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "FastAlign", "normalizedForm": "FastAlign", "offsetStart": 124, "offsetEnd": 153}, "context": "From a translation dataset, pairs were extracted either using a bilingual dictionary, following (Gaschi et al., 2022), with FastAlign (Dyer et al., 2013) or AWESOME-align (Dou and Neubig, 2021). ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9999746084213257}, "created": {"value": false, "score": 7.987022399902344e-06}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999840259552002}, "created": {"value": false, "score": 0.33503055572509766}, "shared": {"value": false, "score": 2.9802322387695312e-06}}, "references": [{"label": "(Dyer et al., 2013)", "normalizedForm": "Dyer et al., 2013", "refKey": 13}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "FastAlign", "normalizedForm": "FastAlign", "offsetStart": 126, "offsetEnd": 135}, "context": "We also showed that using a bilingual dictionary for extracting pairs for realignment methods improves over the commonly used FastAlign and over a more precise neural aligner (AWESOME-align).", "mentionContextAttributes": {"used": {"value": true, "score": 0.9605011940002441}, "created": {"value": false, "score": 0.00021833181381225586}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999840259552002}, "created": {"value": false, "score": 0.33503055572509766}, "shared": {"value": false, "score": 2.9802322387695312e-06}}, "references": [{"label": "(Dyer et al., 2013)", "normalizedForm": "Dyer et al., 2013", "refKey": 13}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "mBERT", "normalizedForm": "mBERT", "offsetStart": 148, "offsetEnd": 153}, "context": "Despite some encouraging results on specific tasks, current realignment methods might not consistently improve cross-lingual zero-shot abilities of mBERT and XLM-R (Wu and Dredze, 2020b).", "mentionContextAttributes": {"used": {"value": false, "score": 0.0003903508186340332}, "created": {"value": false, "score": 5.53131103515625e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9998948574066162}, "created": {"value": false, "score": 0.0001188516616821289}, "shared": {"value": false, "score": 1.5497207641601562e-06}}, "references": [{"label": "(Devlin et al., 2019)", "normalizedForm": "Devlin et al., 2019", "refKey": 10}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "mBERT", "normalizedForm": "mBERT", "offsetStart": 161, "offsetEnd": 166}, "context": "For the second experiment, training (fine-tuning and/or realignment) was performed on various smaller GPUs (RTX 2080 Ti, GTX 1080 Ti, Tesla T4) for distilmBERT, mBERT and XLM-R Base, and on a Nvidia A40 for XLM-R Large. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9998527765274048}, "created": {"value": false, "score": 7.62939453125e-06}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9998948574066162}, "created": {"value": false, "score": 0.0001188516616821289}, "shared": {"value": false, "score": 1.5497207641601562e-06}}, "references": [{"label": "(Devlin et al., 2019)", "normalizedForm": "Devlin et al., 2019", "refKey": 10}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "FastAlign", "normalizedForm": "FastAlign", "offsetStart": 164, "offsetEnd": 173}, "context": "We also tested with the multiUN translation data (Ziemski et al., 2016), which conditioned our choice of languages, and with other ways to extract alignment pairs: FastAlign (Dyer et al., 2013)  and AWESOME-align (Dou and Neubig, 2021).", "mentionContextAttributes": {"used": {"value": true, "score": 0.9999840259552002}, "created": {"value": false, "score": 1.0848045349121094e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999840259552002}, "created": {"value": false, "score": 0.33503055572509766}, "shared": {"value": false, "score": 2.9802322387695312e-06}}, "references": [{"label": "(Dyer et al., 2013)", "normalizedForm": "Dyer et al., 2013", "refKey": 13, "offsetStart": 24057, "offsetEnd": 24076}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "mBERT", "normalizedForm": "mBERT", "offsetStart": 168, "offsetEnd": 173}, "context": "This was done for sentence-level representations (Pires et al., 2019) and for word-level alignment (Conneau et al., 2020b;Gaschi et al., 2022), showing that MLLMs like mBERT have a multilingual alignment that is competitive with static embeddings (Bojanowski et al., 2017) explicitly aligned with a supervised alignment method (Joulin et al., 2018).", "mentionContextAttributes": {"used": {"value": true, "score": 0.9958105087280273}, "created": {"value": false, "score": 3.6597251892089844e-05}, "shared": {"value": false, "score": 1.5497207641601562e-06}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9998948574066162}, "created": {"value": false, "score": 0.0001188516616821289}, "shared": {"value": false, "score": 1.5497207641601562e-06}}, "references": [{"label": "(Devlin et al., 2019)", "normalizedForm": "Devlin et al., 2019", "refKey": 10}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "AWESOME-align", "normalizedForm": "AWESOME-align", "offsetStart": 168, "offsetEnd": 202}, "context": "Realignment methods rely on pairs of words extracted from translated sentences using a word alignment tool, usually FastAlign (Dyer et al., 2013), but other tools like AWESOME-align (Dou and Neubig, 2021) could be used. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.0036995410919189453}, "created": {"value": false, "score": 2.777576446533203e-05}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9995526671409607}, "created": {"value": false, "score": 2.777576446533203e-05}, "shared": {"value": false, "score": 4.76837158203125e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "-R Base", "normalizedForm": "-R Base", "offsetStart": 174, "offsetEnd": 181}, "context": "For the second experiment, training (fine-tuning and/or realignment) was performed on various smaller GPUs (RTX 2080 Ti, GTX 1080 Ti, Tesla T4) for distilmBERT, mBERT and XLM-R Base, and on a Nvidia A40 for XLM-R Large. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9998527765274048}, "created": {"value": false, "score": 7.62939453125e-06}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9998527765274048}, "created": {"value": false, "score": 7.62939453125e-06}, "shared": {"value": false, "score": 1.1920928955078125e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "PyTorch", "normalizedForm": "PyTorch", "offsetStart": 186, "offsetEnd": 214}, "context": "We relied on the following scientific Python packages for our experiments: the Hugging-Face's libraries transformers (Wolf et al., 2020), datasets (Lhoest et al., 2021) and evaluate 4 , PyTorch (Paszke et al., 2019), NLTK (Bird et al., 2009) and its implementation of the Stanford Chinese Segmenter (Tseng et al., 2005), seqeval (Nakayama, 2018) for evaluating NER, NumPy (Harris et al., 2020), and AWESOME-align (Dou and Neubig, 2021), FastAlign (Dyer et al., 2013), and MUSE dictionaries (Lample et al., 2018) for extracting alignment pairs. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9995526671409607}, "created": {"value": false, "score": 2.4437904357910156e-05}, "shared": {"value": false, "score": 4.76837158203125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9995526671409607}, "created": {"value": false, "score": 2.4437904357910156e-05}, "shared": {"value": false, "score": 4.76837158203125e-07}}, "references": [{"label": "(Bird et al., 2009)", "normalizedForm": "Bird et al., 2009", "refKey": 1, "offsetStart": 33790, "offsetEnd": 33809}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "NLTK", "normalizedForm": "NLTK", "offsetStart": 217, "offsetEnd": 241}, "context": "We relied on the following scientific Python packages for our experiments: the Hugging-Face's libraries transformers (Wolf et al., 2020), datasets (Lhoest et al., 2021) and evaluate 4 , PyTorch (Paszke et al., 2019), NLTK (Bird et al., 2009) and its implementation of the Stanford Chinese Segmenter (Tseng et al., 2005), seqeval (Nakayama, 2018) for evaluating NER, NumPy (Harris et al., 2020), and AWESOME-align (Dou and Neubig, 2021), FastAlign (Dyer et al., 2013), and MUSE dictionaries (Lample et al., 2018) for extracting alignment pairs. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9995526671409607}, "created": {"value": false, "score": 2.4437904357910156e-05}, "shared": {"value": false, "score": 4.76837158203125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9995526671409607}, "created": {"value": false, "score": 2.4437904357910156e-05}, "shared": {"value": false, "score": 4.76837158203125e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "FastAlign", "normalizedForm": "FastAlign", "offsetStart": 239, "offsetEnd": 248}, "context": "Namely, we find that realignment works better on tasks for which alignment is correlated with cross-lingual transfer when generalizing to a distant language and with smaller models, as well as when using a bilingual dictionary rather than FastAlign to extract realignment pairs. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.0007115602493286133}, "created": {"value": false, "score": 0.0009027719497680664}, "shared": {"value": false, "score": 4.76837158203125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999840259552002}, "created": {"value": false, "score": 0.33503055572509766}, "shared": {"value": false, "score": 2.9802322387695312e-06}}, "references": [{"label": "(Dyer et al., 2013)", "normalizedForm": "Dyer et al., 2013", "refKey": 13}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Segmenter", "normalizedForm": "Segmenter", "offsetStart": 289, "offsetEnd": 318}, "publisher": {"rawForm": "Stanford", "normalizedForm": "Stanford", "offsetStart": 272, "offsetEnd": 280}, "context": "We relied on the following scientific Python packages for our experiments: the Hugging-Face's libraries transformers (Wolf et al., 2020), datasets (Lhoest et al., 2021) and evaluate 4 , PyTorch (Paszke et al., 2019), NLTK (Bird et al., 2009) and its implementation of the Stanford Chinese Segmenter (Tseng et al., 2005), seqeval (Nakayama, 2018) for evaluating NER, NumPy (Harris et al., 2020), and AWESOME-align (Dou and Neubig, 2021), FastAlign (Dyer et al., 2013), and MUSE dictionaries (Lample et al., 2018) for extracting alignment pairs. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9995526671409607}, "created": {"value": false, "score": 2.4437904357910156e-05}, "shared": {"value": false, "score": 4.76837158203125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9995526671409607}, "created": {"value": false, "score": 2.4437904357910156e-05}, "shared": {"value": false, "score": 4.76837158203125e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "seqeval", "normalizedForm": "seqeval", "offsetStart": 321, "offsetEnd": 345}, "context": "We relied on the following scientific Python packages for our experiments: the Hugging-Face's libraries transformers (Wolf et al., 2020), datasets (Lhoest et al., 2021) and evaluate 4 , PyTorch (Paszke et al., 2019), NLTK (Bird et al., 2009) and its implementation of the Stanford Chinese Segmenter (Tseng et al., 2005), seqeval (Nakayama, 2018) for evaluating NER, NumPy (Harris et al., 2020), and AWESOME-align (Dou and Neubig, 2021), FastAlign (Dyer et al., 2013), and MUSE dictionaries (Lample et al., 2018) for extracting alignment pairs. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9995526671409607}, "created": {"value": false, "score": 2.4437904357910156e-05}, "shared": {"value": false, "score": 4.76837158203125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9995526671409607}, "created": {"value": false, "score": 2.4437904357910156e-05}, "shared": {"value": false, "score": 4.76837158203125e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "AWESOME-align", "normalizedForm": "AWESOME-align", "offsetStart": 399, "offsetEnd": 434}, "context": "We relied on the following scientific Python packages for our experiments: the Hugging-Face's libraries transformers (Wolf et al., 2020), datasets (Lhoest et al., 2021) and evaluate 4 , PyTorch (Paszke et al., 2019), NLTK (Bird et al., 2009) and its implementation of the Stanford Chinese Segmenter (Tseng et al., 2005), seqeval (Nakayama, 2018) for evaluating NER, NumPy (Harris et al., 2020), and AWESOME-align (Dou and Neubig, 2021), FastAlign (Dyer et al., 2013), and MUSE dictionaries (Lample et al., 2018) for extracting alignment pairs. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9995526671409607}, "created": {"value": false, "score": 2.4437904357910156e-05}, "shared": {"value": false, "score": 4.76837158203125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9995526671409607}, "created": {"value": false, "score": 2.777576446533203e-05}, "shared": {"value": false, "score": 4.76837158203125e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "FastAlign", "normalizedForm": "FastAlign", "offsetStart": 437, "offsetEnd": 465}, "context": "We relied on the following scientific Python packages for our experiments: the Hugging-Face's libraries transformers (Wolf et al., 2020), datasets (Lhoest et al., 2021) and evaluate 4 , PyTorch (Paszke et al., 2019), NLTK (Bird et al., 2009) and its implementation of the Stanford Chinese Segmenter (Tseng et al., 2005), seqeval (Nakayama, 2018) for evaluating NER, NumPy (Harris et al., 2020), and AWESOME-align (Dou and Neubig, 2021), FastAlign (Dyer et al., 2013), and MUSE dictionaries (Lample et al., 2018) for extracting alignment pairs. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9995526671409607}, "created": {"value": false, "score": 2.4437904357910156e-05}, "shared": {"value": false, "score": 4.76837158203125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999840259552002}, "created": {"value": false, "score": 0.33503055572509766}, "shared": {"value": false, "score": 2.9802322387695312e-06}}, "references": [{"label": "(Dyer et al., 2013)", "normalizedForm": "Dyer et al., 2013", "refKey": 13}]}], "references": [{"refKey": 13, "tei": "<biblStruct xml:id=\"b13\">\n\t<analytic>\n\t\t<title level=\"a\" type=\"main\">The management of software engineering, Part IV: Software development practices</title>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Chris</forename><surname>Dyer</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Victor</forename><surname>Chahuneau</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Noah</forename><forename type=\"middle\">A</forename><surname>Smith</surname></persName>\n\t\t</author>\n\t\t<idno type=\"DOI\">10.1147/sj.194.0451</idno>\n\t</analytic>\n\t<monogr>\n\t\t<title level=\"j\">IBM Systems Journal</title>\n\t\t<title level=\"j\" type=\"abbrev\">IBM Syst. J.</title>\n\t\t<idno type=\"ISSN\">0018-8670</idno>\n\t\t<imprint>\n\t\t\t<biblScope unit=\"volume\">19</biblScope>\n\t\t\t<biblScope unit=\"issue\">4</biblScope>\n\t\t\t<biblScope unit=\"page\" from=\"451\" to=\"465\" />\n\t\t\t<date type=\"published\" when=\"1980\">2013</date>\n\t\t\t<publisher>IBM</publisher>\n\t\t</imprint>\n\t</monogr>\n</biblStruct>\n"}, {"refKey": 10, "tei": "<biblStruct xml:id=\"b10\">\n\t<analytic>\n\t\t<title level=\"a\" type=\"main\"></title>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Jacob</forename><surname>Devlin</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Ming-Wei</forename><surname>Chang</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Kenton</forename><surname>Lee</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Kristina</forename><surname>Toutanova</surname></persName>\n\t\t</author>\n\t\t<idno type=\"DOI\">10.18653/v1/n19-1423</idno>\n\t</analytic>\n\t<monogr>\n\t\t<title level=\"m\">Proceedings of the 2019 Conference of the North</title>\n\t\t<meeting>the 2019 Conference of the North</meeting>\n\t\t<imprint>\n\t\t\t<publisher>Association for Computational Linguistics</publisher>\n\t\t\t<date type=\"published\" when=\"2019\">2019</date>\n\t\t\t<biblScope unit=\"volume\">1</biblScope>\n\t\t\t<biblScope unit=\"page\">4186</biblScope>\n\t\t</imprint>\n\t</monogr>\n</biblStruct>\n"}, {"refKey": 1, "tei": "<biblStruct xml:id=\"b1\">\n\t<monogr>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Steven</forename><surname>Bird</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Ewan</forename><surname>Klein</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Edward</forename><surname>Loper</surname></persName>\n\t\t</author>\n\t\t<title level=\"m\">Natural Language Processing with Python</title>\n\t\t<imprint>\n\t\t\t<date>2009</date>\n\t\t</imprint>\n\t</monogr>\n</biblStruct>\n"}], "runtime": 10367, "id": "a91cd7ff34ef192c7510bb6739edc28edbcb447a", "metadata": {"id": "a91cd7ff34ef192c7510bb6739edc28edbcb447a"}, "original_file_path": "../../datalake/Samuel/SOFTware-Sync/downloads/xml/hal-04193179.grobid.tei.xml", "file_name": "hal-04193179.grobid.tei.xml"}