{"application": "software-mentions", "version": "0.8.0", "date": "2024-10-07T11:40+0000", "md5": "3B862690302D66E04001EAEFEF553E69", "mentions": [{"type": "software", "software-type": "software", "software-name": {"rawForm": "BioPython", "normalizedForm": "BioPython", "offsetStart": 11, "offsetEnd": 20}, "context": "We use the BioPython(Cock et al., 2009) implementation.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9998064637184143}, "created": {"value": false, "score": 1.3470649719238281e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9998064637184143}, "created": {"value": false, "score": 1.3470649719238281e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "fairseq toolkit", "normalizedForm": "fairseq toolkit", "offsetStart": 11, "offsetEnd": 26}, "context": "We use the fairseq toolkit (Ott et al., 2019); the encoders are composed of one embedding layer followed by a bidirectional GRU (embedding dimension: 20, hidden dimension: 50, 1 layer), and the decoders are composed of one embedding layer and one unidirectional GRU with its own attention (same parameters). ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9880362749099731}, "created": {"value": false, "score": 4.887580871582031e-06}, "shared": {"value": false, "score": 7.152557373046875e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9880362749099731}, "created": {"value": false, "score": 4.887580871582031e-06}, "shared": {"value": false, "score": 7.152557373046875e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "BioPython", "normalizedForm": "BioPython", "offsetStart": 11, "offsetEnd": 39}, "context": "We use the BioPython(Cock et al., 2009) implementation.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9998064637184143}, "created": {"value": false, "score": 1.3470649719238281e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9998064637184143}, "created": {"value": false, "score": 1.3470649719238281e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "GIZA++", "normalizedForm": "GIZA++", "offsetStart": 25, "offsetEnd": 31}, "context": "The data is aligned with GIZA++ (Och and Ney, 2003), while a 3-gram language model is trained with KenLM (Heafield, 2011) on the pair of interest target data, then models are tuned using MERT.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9998961687088013}, "created": {"value": false, "score": 5.364418029785156e-06}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9998961687088013}, "created": {"value": false, "score": 5.364418029785156e-06}, "shared": {"value": false, "score": 1.1920928955078125e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Adam", "normalizedForm": "Adam", "offsetStart": 32, "offsetEnd": 36}, "context": "Each model is trained using the Adam optimizer (learning rate: 0.005) and the cross entropy loss, stopping on the first of either 15 epochs or convergence of the BLEU score on the development set used during training.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9929659366607666}, "created": {"value": false, "score": 2.8967857360839844e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9929659366607666}, "created": {"value": false, "score": 2.8967857360839844e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "espeak", "normalizedForm": "espeak", "offsetStart": 34, "offsetEnd": 40}, "context": "All data is then phonetised using espeak (Duddington, 2007-2015), with relevant phonetizers for CA, ES, IT, FR, PT, RO, and approximating the phonetization of OC as CA, RUP as RO, and GL as PT. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9179570078849792}, "created": {"value": false, "score": 2.1457672119140625e-06}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9179570078849792}, "created": {"value": false, "score": 2.1457672119140625e-06}, "shared": {"value": false, "score": 2.384185791015625e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Post", "normalizedForm": "Post", "offsetStart": 40, "offsetEnd": 44}, "context": "To evaluate such 'translations,' we use Post (2018)  implementation of BLEU (Papineni et al., 2002), which does not suffer for cognate prediction from the same drawbacks as for NMT (Fourrier et al.,  2021).", "mentionContextAttributes": {"used": {"value": true, "score": 0.930075466632843}, "created": {"value": false, "score": 1.0013580322265625e-05}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.930075466632843}, "created": {"value": false, "score": 1.0013580322265625e-05}, "shared": {"value": false, "score": 3.5762786865234375e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "code", "normalizedForm": "code", "offsetStart": 67, "offsetEnd": 71}, "url": {"rawForm": "github.com/clefourrier/", "normalizedForm": "github.com/clefourrier", "offsetStart": 75, "offsetEnd": 98}, "context": "Training can be replicated using data provided with the paper, and code at github.com/clefourrier/CopperMT. We can provide all our trained models on request (>10GB).", "mentionContextAttributes": {"used": {"value": false, "score": 0.008806169033050537}, "created": {"value": false, "score": 0.0008420944213867188}, "shared": {"value": false, "score": 0.006918609142303467}}, "documentContextAttributes": {"used": {"value": false, "score": 0.008806169033050537}, "created": {"value": false, "score": 0.0008420944213867188}, "shared": {"value": false, "score": 0.006918609142303467}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "code", "normalizedForm": "code", "offsetStart": 67, "offsetEnd": 71}, "url": {"rawForm": "github.com/clefourrier/", "normalizedForm": "github.com/clefourrier"}, "context": "Training can be replicated using data provided with the paper, and code at github.com/clefourrier/CopperMT. We can provide all our trained models on request (>10GB).", "mentionContextAttributes": {"used": {"value": false, "score": 0.008806169033050537}, "created": {"value": false, "score": 0.0008420944213867188}, "shared": {"value": false, "score": 0.006918609142303467}}, "documentContextAttributes": {"used": {"value": false, "score": 0.008806169033050537}, "created": {"value": false, "score": 0.0008420944213867188}, "shared": {"value": false, "score": 0.006918609142303467}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "SentencePiece", "normalizedForm": "SentencePiece", "offsetStart": 69, "offsetEnd": 82}, "context": "We segmented the data at the character (not subword) level using the SentencePiece (Kudo and  Richardson, 2018) library; more precisely, we trained a character-level model per language for all models, except M-NMT+m+shared_emb and M-NMT+m+shared_all, where sharing embeddings or encoders meant sharing the vocabulary across all languages: in this last case, we used a single segmentation model for all languages (which tend to have similar phone distributions, apart from the rarest phones, such as nasal vowels in French). ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9998801946640015}, "created": {"value": false, "score": 3.337860107421875e-06}, "shared": {"value": false, "score": 8.344650268554688e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9998801946640015}, "created": {"value": false, "score": 3.337860107421875e-06}, "shared": {"value": false, "score": 8.344650268554688e-07}}}, {"type": "software", "software-type": "implicit", "software-name": {"rawForm": "scripts", "normalizedForm": "scripts", "offsetStart": 169, "offsetEnd": 176}, "context": "Extraction and Pre-processing Monolingual 8 and bilingual 9 cognate lexicons are extracted from EtymDB2 (Fourrier and Sagot, 2020b), an etymological database, using the scripts provided. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9998364448547363}, "created": {"value": false, "score": 5.0067901611328125e-06}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9998364448547363}, "created": {"value": false, "score": 5.0067901611328125e-06}, "shared": {"value": false, "score": 1.1920928955078125e-07}}}], "references": [], "runtime": 8440, "id": "4b708b6ef2a1b38c6c3829bf480bc3874a6d5788", "metadata": {"id": "4b708b6ef2a1b38c6c3829bf480bc3874a6d5788"}, "original_file_path": "../../datalake/Samuel/SOFTware-Sync/downloads/xml/hal-03614691.grobid.tei.xml", "file_name": "hal-03614691.grobid.tei.xml"}