{"application": "software-mentions", "version": "0.8.0", "date": "2024-10-07T12:01+0000", "md5": "035A9F9376C46A4616821DD98A957CD3", "mentions": [{"type": "software", "software-type": "software", "software-name": {"rawForm": "FungiCLEF", "normalizedForm": "FungiCLEF", "offsetStart": 0, "offsetEnd": 9}, "context": "FungiCLEF 2023 considered five different decision scenarios, minimizing the empirical loss L = \u2211 i W (y i , q(x i )) for decisions q(x) over observations x and true labels y, given a cost function W (y, q(x)).", "mentionContextAttributes": {"used": {"value": true, "score": 0.9656862020492554}, "created": {"value": false, "score": 4.410743713378906e-06}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9981185793876648}, "created": {"value": false, "score": 0.0026530027389526367}, "shared": {"value": false, "score": 3.5762786865234375e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "PlantCLEF", "normalizedForm": "PlantCLEF", "offsetStart": 9, "offsetEnd": 18}, "context": "Only the PlantCLEF challenge obtained much better results (for the identification of plants from images) with the use of foundation vision transformer models such as EVA [9]. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9994901418685913}, "created": {"value": false, "score": 4.291534423828125e-06}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9998472929000854}, "created": {"value": false, "score": 0.10563671588897705}, "shared": {"value": false, "score": 9.5367431640625e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "GeoLifeCLEF", "normalizedForm": "GeoLifeCLEF", "offsetStart": 17, "offsetEnd": 28}, "context": "The objective of GeoLifeCLEF is to evaluate models with orders of magnitude hitherto unseen, whether in terms of the number of species covered (thousands), Fig. 7: GeoLifeCLEF 2023 graphical abstract spatial resolution (on the order of 10 meters), or the number of occurrences used as training data (several million). ", "mentionContextAttributes": {"used": {"value": false, "score": 0.0004917383193969727}, "created": {"value": false, "score": 5.614757537841797e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9957998394966125}, "created": {"value": false, "score": 0.3500039577484131}, "shared": {"value": false, "score": 3.5762786865234375e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "XGBoost", "normalizedForm": "XGBoost", "offsetStart": 18, "offsetEnd": 25}, "context": "The model used is XGBoost and it was trained on the PA plots. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9995841383934021}, "created": {"value": false, "score": 0.004850924015045166}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9995841383934021}, "created": {"value": false, "score": 0.004850924015045166}, "shared": {"value": false, "score": 2.384185791015625e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "PlantCLEF", "normalizedForm": "PlantCLEF", "offsetStart": 21, "offsetEnd": 30}, "context": "The objective of the PlantCLEF challenges in 2022 and 2023 was to advance the field of plant identification on a global scale.", "mentionContextAttributes": {"used": {"value": false, "score": 0.007470071315765381}, "created": {"value": false, "score": 0.10563671588897705}, "shared": {"value": false, "score": 9.5367431640625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9998472929000854}, "created": {"value": false, "score": 0.10563671588897705}, "shared": {"value": false, "score": 9.5367431640625e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "FungiCLEF", "normalizedForm": "FungiCLEF", "offsetStart": 23, "offsetEnd": 32}, "context": "The best submission to FungiCLEF was based on MetaFormer [8], utilizing both a convolutional backbone and a transformer to fuse visual and meta information. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9981185793876648}, "created": {"value": false, "score": 3.707408905029297e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9981185793876648}, "created": {"value": false, "score": 0.0026530027389526367}, "shared": {"value": false, "score": 3.5762786865234375e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "PlantCLEF", "normalizedForm": "PlantCLEF", "offsetStart": 24, "offsetEnd": 33}, "context": "-Utilizing the complete PlantCLEF training dataset, comprising both the trusted and web datasets, proved advantageous, despite the added training time and the residual noise inherent in the web dataset.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9829050302505493}, "created": {"value": false, "score": 3.921985626220703e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9998472929000854}, "created": {"value": false, "score": 0.10563671588897705}, "shared": {"value": false, "score": 9.5367431640625e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "PlantCLEF", "normalizedForm": "PlantCLEF", "offsetStart": 25, "offsetEnd": 34}, "context": "The test set used in the PlantCLEF challenge was constructed using multiimage plant observations obtained from the Pl@ntNet platform during the year 2021. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9998472929000854}, "created": {"value": false, "score": 0.0013878345489501953}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9998472929000854}, "created": {"value": false, "score": 0.10563671588897705}, "shared": {"value": false, "score": 9.5367431640625e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "GeoLifeCLEF", "normalizedForm": "GeoLifeCLEF", "offsetStart": 25, "offsetEnd": 36}, "context": "Three of the challenges (GeoLifeCLEF, SnakeCLEF, and FungiCLEF) were organized jointly with FGVC 10, an annual workshop dedicated to Fine-Grained Visual Categorization organized in the context of the CVPR international conference on computer vision and pattern recognition.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9915459752082825}, "created": {"value": false, "score": 0.0017045140266418457}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9957998394966125}, "created": {"value": false, "score": 0.3500039577484131}, "shared": {"value": false, "score": 3.5762786865234375e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Kaggle", "normalizedForm": "Kaggle", "offsetStart": 26, "offsetEnd": 32}, "context": "The challenge was held on Kaggle, and the evaluation mode resembled the test mode of previous iterations, i.e., hidden test data, code competition, etc. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9996863603591919}, "created": {"value": false, "score": 0.00012195110321044922}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9996863603591919}, "created": {"value": false, "score": 0.00012195110321044922}, "shared": {"value": false, "score": 3.5762786865234375e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "GeoLifeCLEF", "normalizedForm": "GeoLifeCLEF", "offsetStart": 32, "offsetEnd": 43}, "context": "The detailed description of the GeoLifeCLEF 2023 dataset is provided in [4].", "mentionContextAttributes": {"used": {"value": false, "score": 0.09856551885604858}, "created": {"value": false, "score": 9.179115295410156e-06}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9957998394966125}, "created": {"value": false, "score": 0.3500039577484131}, "shared": {"value": false, "score": 3.5762786865234375e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "FungiCLEF", "normalizedForm": "FungiCLEF", "offsetStart": 33, "offsetEnd": 42}, "context": "Twelve teams participated in the FungiCLEF 2023 challenge; four provided their models for a private evaluation, and three submitted working notes. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9907914400100708}, "created": {"value": false, "score": 0.00044661760330200195}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9981185793876648}, "created": {"value": false, "score": 0.0026530027389526367}, "shared": {"value": false, "score": 3.5762786865234375e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "PlantCLEF", "normalizedForm": "PlantCLEF", "offsetStart": 34, "offsetEnd": 43}, "context": "The evaluation of the task in the PlantCLEF challenge primarily relies on the Mean Reciprocal Rank (MRR) metric. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9702210426330566}, "created": {"value": false, "score": 1.6689300537109375e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9998472929000854}, "created": {"value": false, "score": 0.10563671588897705}, "shared": {"value": false, "score": 9.5367431640625e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "MetaFormer", "normalizedForm": "MetaFormer", "offsetStart": 46, "offsetEnd": 59}, "context": "The best submission to FungiCLEF was based on MetaFormer [8], utilizing both a convolutional backbone and a transformer to fuse visual and meta information. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9981185793876648}, "created": {"value": false, "score": 3.707408905029297e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999589920043945}, "created": {"value": false, "score": 0.001852273941040039}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "references": [{"label": "[33]", "normalizedForm": "[33]", "refKey": 33}, {"label": "[59]", "normalizedForm": "[59]", "refKey": 59}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "ONNX", "normalizedForm": "ONNX", "offsetStart": 51, "offsetEnd": 55}, "context": "Interestingly, participants also experimented with ONNX and openVINO to improve model inference speed.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9986982345581055}, "created": {"value": false, "score": 0.024729609489440918}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9986982345581055}, "created": {"value": false, "score": 0.024729609489440918}, "shared": {"value": false, "score": 2.384185791015625e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "FungiCLEF", "normalizedForm": "FungiCLEF", "offsetStart": 53, "offsetEnd": 62}, "context": "Three of the challenges (GeoLifeCLEF, SnakeCLEF, and FungiCLEF) were organized jointly with FGVC 10, an annual workshop dedicated to Fine-Grained Visual Categorization organized in the context of the CVPR international conference on computer vision and pattern recognition.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9915459752082825}, "created": {"value": false, "score": 0.0017045140266418457}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9981185793876648}, "created": {"value": false, "score": 0.0026530027389526367}, "shared": {"value": false, "score": 3.5762786865234375e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "GeoLifeCLEF", "normalizedForm": "GeoLifeCLEF", "offsetStart": 54, "offsetEnd": 65}, "context": "A brand new dataset was built for the 2023 edition of GeoLifeCLEF in the framework of a large-scale European project on biodiversity monitoring (MAMBO, Horizon EU program).", "mentionContextAttributes": {"used": {"value": false, "score": 0.00019854307174682617}, "created": {"value": false, "score": 0.0746355652809143}, "shared": {"value": false, "score": 8.344650268554688e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9957998394966125}, "created": {"value": false, "score": 0.3500039577484131}, "shared": {"value": false, "score": 3.5762786865234375e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "GeoLifeCLEF", "normalizedForm": "GeoLifeCLEF", "offsetStart": 57, "offsetEnd": 68}, "context": "Six participants from four countries participated in the GeoLifeCLEF 2023 challenge and submitted a total of 121 entries (i.e.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9911801218986511}, "created": {"value": false, "score": 0.000102996826171875}, "shared": {"value": false, "score": 7.152557373046875e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9957998394966125}, "created": {"value": false, "score": 0.3500039577484131}, "shared": {"value": false, "score": 3.5762786865234375e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "PlantCLEF", "normalizedForm": "PlantCLEF", "offsetStart": 64, "offsetEnd": 73}, "context": "LifeCLEF 2023 consists of five challenges (BirdCLEF, SnakeCLEF, PlantCLEF, FungiCLEF, GeoLifeCLEF) whose methodology and main outcomes are described in this paper.", "mentionContextAttributes": {"used": {"value": false, "score": 0.0017970800399780273}, "created": {"value": false, "score": 0.0026530027389526367}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9998472929000854}, "created": {"value": false, "score": 0.10563671588897705}, "shared": {"value": false, "score": 9.5367431640625e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Pl@ntNet", "normalizedForm": "Pl@ntNet", "offsetStart": 68, "offsetEnd": 76}, "context": "Only observations that received a very high confidence score in the Pl@ntNet collaborative review process were selected for the challenge to ensure the highest possible quality of determination.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9998843669891357}, "created": {"value": false, "score": 3.457069396972656e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9998843669891357}, "created": {"value": false, "score": 0.0013878345489501953}, "shared": {"value": false, "score": 1.633167266845703e-05}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "FungiCLEF", "normalizedForm": "FungiCLEF", "offsetStart": 75, "offsetEnd": 84}, "context": "LifeCLEF 2023 consists of five challenges (BirdCLEF, SnakeCLEF, PlantCLEF, FungiCLEF, GeoLifeCLEF) whose methodology and main outcomes are described in this paper.", "mentionContextAttributes": {"used": {"value": false, "score": 0.0017970800399780273}, "created": {"value": false, "score": 0.0026530027389526367}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9981185793876648}, "created": {"value": false, "score": 0.0026530027389526367}, "shared": {"value": false, "score": 3.5762786865234375e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "BirdNET", "normalizedForm": "BirdNET", "offsetStart": 78, "offsetEnd": 85}, "context": "Some teams utilized embeddings of pretrained bird recognition models (such as BirdNET or Google Perch, both were provided as supporting models) to train on high-level features, which somewhat mitigated the need for extensive training data. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.7200080156326294}, "created": {"value": false, "score": 0.00021213293075561523}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.7200080156326294}, "created": {"value": false, "score": 0.00021213293075561523}, "shared": {"value": false, "score": 1.1920928955078125e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "MetaFormer", "normalizedForm": "MetaFormer", "offsetStart": 82, "offsetEnd": 95}, "context": "The best-performing team -meng18 -combined visual information with metadata using MetaFormer [8], tackled class imbalance with the Seesaw loss [55], proposed an entropy-guided recognition of unknown species, and introduced an additional poisonous-classification loss. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.5424379706382751}, "created": {"value": false, "score": 0.001852273941040039}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999589920043945}, "created": {"value": false, "score": 0.001852273941040039}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "references": [{"label": "[33]", "normalizedForm": "[33]", "refKey": 33}, {"label": "[59]", "normalizedForm": "[59]", "refKey": 59}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "GeoLifeCLEF", "normalizedForm": "GeoLifeCLEF", "offsetStart": 86, "offsetEnd": 97}, "context": "LifeCLEF 2023 consists of five challenges (BirdCLEF, SnakeCLEF, PlantCLEF, FungiCLEF, GeoLifeCLEF) whose methodology and main outcomes are described in this paper.", "mentionContextAttributes": {"used": {"value": false, "score": 0.0017970800399780273}, "created": {"value": false, "score": 0.0026530027389526367}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9957998394966125}, "created": {"value": false, "score": 0.3500039577484131}, "shared": {"value": false, "score": 3.5762786865234375e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Google Perch", "normalizedForm": "Google Perch", "offsetStart": 89, "offsetEnd": 101}, "context": "Some teams utilized embeddings of pretrained bird recognition models (such as BirdNET or Google Perch, both were provided as supporting models) to train on high-level features, which somewhat mitigated the need for extensive training data. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.7200080156326294}, "created": {"value": false, "score": 0.00021213293075561523}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.7200080156326294}, "created": {"value": false, "score": 0.00021213293075561523}, "shared": {"value": false, "score": 1.1920928955078125e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "iNaturalist", "normalizedForm": "iNaturalist", "offsetStart": 90, "offsetEnd": 101}, "context": "The dataset was constructed from observations submitted to the citizen science platforms -iNaturalist and HerpMapper -and combined roughly 110,000 reals snake specimen observations with community-verified species labels.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9996223449707031}, "created": {"value": false, "score": 3.910064697265625e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9996223449707031}, "created": {"value": false, "score": 0.00012695789337158203}, "shared": {"value": false, "score": 1.633167266845703e-05}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Kaggle", "normalizedForm": "Kaggle", "offsetStart": 94, "offsetEnd": 100}, "context": "The systems used to run the challenges (registration, submission, leaderboard, etc.) were the Kaggle platform for the BirdCLEF and GeoLifeCLEF challenges, the Hugging Face competition platform for SnakeCLEF and Fung-iCLEF challenges, and the AICrowd platform for the PlantCLEF challenge.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9957998394966125}, "created": {"value": false, "score": 7.486343383789062e-05}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9996863603591919}, "created": {"value": false, "score": 0.00012195110321044922}, "shared": {"value": false, "score": 3.5762786865234375e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Google", "normalizedForm": "Google", "offsetStart": 107, "offsetEnd": 113}, "context": "In contrast, a second \"web\" training dataset comprises images obtained from commercial search engines like Google and Bing. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.003403604030609131}, "created": {"value": false, "score": 0.0001360177993774414}, "shared": {"value": false, "score": 7.152557373046875e-07}}, "documentContextAttributes": {"used": {"value": false, "score": 0.003403604030609131}, "created": {"value": false, "score": 0.0001360177993774414}, "shared": {"value": false, "score": 7.152557373046875e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "MetaFormer", "normalizedForm": "MetaFormer", "offsetStart": 113, "offsetEnd": 126}, "context": "On the vision part, convolutional models (ResNet [18], EfficientNet [48], ConvNext [57]) and Transformer models (MetaFormer [8], Swin [33], VOLO [59]) were used to extract the visual features. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9999589920043945}, "created": {"value": false, "score": 3.0994415283203125e-06}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999589920043945}, "created": {"value": false, "score": 0.001852273941040039}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "references": [{"label": "[33]", "normalizedForm": "[33]", "refKey": 33, "offsetStart": 13170, "offsetEnd": 13174}, {"label": "[59]", "normalizedForm": "[59]", "refKey": 59, "offsetStart": 13181, "offsetEnd": 13185}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Pl@ntNet", "normalizedForm": "Pl@ntNet", "offsetStart": 115, "offsetEnd": 123}, "context": "The test set used in the PlantCLEF challenge was constructed using multiimage plant observations obtained from the Pl@ntNet platform during the year 2021.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9998472929000854}, "created": {"value": false, "score": 0.0013878345489501953}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9998843669891357}, "created": {"value": false, "score": 0.0013878345489501953}, "shared": {"value": false, "score": 1.633167266845703e-05}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Bing", "normalizedForm": "Bing", "offsetStart": 118, "offsetEnd": 122}, "context": "In contrast, a second \"web\" training dataset comprises images obtained from commercial search engines like Google and Bing. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.003403604030609131}, "created": {"value": false, "score": 0.0001360177993774414}, "shared": {"value": false, "score": 7.152557373046875e-07}}, "documentContextAttributes": {"used": {"value": false, "score": 0.003403604030609131}, "created": {"value": false, "score": 0.0001360177993774414}, "shared": {"value": false, "score": 7.152557373046875e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "GeoLifeCLEF", "normalizedForm": "GeoLifeCLEF", "offsetStart": 131, "offsetEnd": 142}, "context": "The systems used to run the challenges (registration, submission, leaderboard, etc.) were the Kaggle platform for the BirdCLEF and GeoLifeCLEF challenges, the Hugging Face competition platform for SnakeCLEF and Fung-iCLEF challenges, and the AICrowd platform for the PlantCLEF challenge.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9957998394966125}, "created": {"value": false, "score": 7.486343383789062e-05}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9957998394966125}, "created": {"value": false, "score": 0.3500039577484131}, "shared": {"value": false, "score": 3.5762786865234375e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "PlantCLEF", "normalizedForm": "PlantCLEF", "offsetStart": 134, "offsetEnd": 143}, "context": "Although over a hundred participants signed up for the challenge, in the end only 3 participants from 3 countries participated to the PlantCLEF 2023 challenge and submitted a total of 22 runs.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9744244813919067}, "created": {"value": false, "score": 0.00010406970977783203}, "shared": {"value": false, "score": 5.960464477539062e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9998472929000854}, "created": {"value": false, "score": 0.10563671588897705}, "shared": {"value": false, "score": 9.5367431640625e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "iNaturalist", "normalizedForm": "iNaturalist", "offsetStart": 156, "offsetEnd": 167}, "context": "The sources of these images include academic institutions such as museums, universities, and national institutions, as well as collaborative platforms like iNaturalist and Pl@ntNet, implying a fairly high certainty of determination quality (collaborative platforms only share their highest quality data qualified as \"research graded\"). ", "mentionContextAttributes": {"used": {"value": false, "score": 0.30347687005996704}, "created": {"value": false, "score": 0.00012695789337158203}, "shared": {"value": false, "score": 1.633167266845703e-05}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9996223449707031}, "created": {"value": false, "score": 0.00012695789337158203}, "shared": {"value": false, "score": 1.633167266845703e-05}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "GeoLifeCLEF", "normalizedForm": "GeoLifeCLEF", "offsetStart": 164, "offsetEnd": 175}, "context": "The objective of GeoLifeCLEF is to evaluate models with orders of magnitude hitherto unseen, whether in terms of the number of species covered (thousands), Fig. 7: GeoLifeCLEF 2023 graphical abstract spatial resolution (on the order of 10 meters), or the number of occurrences used as training data (several million).", "mentionContextAttributes": {"used": {"value": false, "score": 0.0004917383193969727}, "created": {"value": false, "score": 5.614757537841797e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9957998394966125}, "created": {"value": false, "score": 0.3500039577484131}, "shared": {"value": false, "score": 3.5762786865234375e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Pl@ntNet", "normalizedForm": "Pl@ntNet", "offsetStart": 172, "offsetEnd": 180}, "context": "The sources of these images include academic institutions such as museums, universities, and national institutions, as well as collaborative platforms like iNaturalist and Pl@ntNet, implying a fairly high certainty of determination quality (collaborative platforms only share their highest quality data qualified as \"research graded\"). ", "mentionContextAttributes": {"used": {"value": false, "score": 0.30347687005996704}, "created": {"value": false, "score": 0.00012695789337158203}, "shared": {"value": false, "score": 1.633167266845703e-05}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9998843669891357}, "created": {"value": false, "score": 0.0013878345489501953}, "shared": {"value": false, "score": 1.633167266845703e-05}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "PlantCLEF", "normalizedForm": "PlantCLEF", "offsetStart": 267, "offsetEnd": 276}, "context": "The systems used to run the challenges (registration, submission, leaderboard, etc.) were the Kaggle platform for the BirdCLEF and GeoLifeCLEF challenges, the Hugging Face competition platform for SnakeCLEF and Fung-iCLEF challenges, and the AICrowd platform for the PlantCLEF challenge.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9957998394966125}, "created": {"value": false, "score": 7.486343383789062e-05}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9998472929000854}, "created": {"value": false, "score": 0.10563671588897705}, "shared": {"value": false, "score": 9.5367431640625e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "GeoLifeCLEF", "normalizedForm": "GeoLifeCLEF", "offsetStart": 396, "offsetEnd": 407}, "context": "Overall, this study shows that the field continues to progress year after year, and that, although the challenges that are most closely related to common tasks, such as multi-class classification based on images, are able to profit from the most recent advances in computer vision, certain problems are still wide open, such as the prediction of species as a function of location (as part of the GeoLifeCLEF challenge). ", "mentionContextAttributes": {"used": {"value": false, "score": 0.0013011693954467773}, "created": {"value": false, "score": 0.3500039577484131}, "shared": {"value": false, "score": 3.5762786865234375e-06}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9957998394966125}, "created": {"value": false, "score": 0.3500039577484131}, "shared": {"value": false, "score": 3.5762786865234375e-06}}}], "references": [{"refKey": 33, "tei": "<biblStruct xml:id=\"b33\">\n\t<analytic>\n\t\t<title level=\"a\" type=\"main\">Swin Transformer: Hierarchical Vision Transformer using Shifted Windows</title>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Ze</forename><surname>Liu</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Yutong</forename><surname>Lin</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Yue</forename><surname>Cao</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Han</forename><surname>Hu</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Yixuan</forename><surname>Wei</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Zheng</forename><surname>Zhang</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Stephen</forename><surname>Lin</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Baining</forename><surname>Guo</surname></persName>\n\t\t</author>\n\t\t<idno type=\"DOI\">10.1109/iccv48922.2021.00986</idno>\n\t</analytic>\n\t<monogr>\n\t\t<title level=\"m\">2021 IEEE/CVF International Conference on Computer Vision (ICCV)</title>\n\t\t<imprint>\n\t\t\t<publisher>IEEE</publisher>\n\t\t\t<date type=\"published\" when=\"2021-10\">2021</date>\n\t\t\t<biblScope unit=\"page\">10022</biblScope>\n\t\t</imprint>\n\t</monogr>\n</biblStruct>\n"}, {"refKey": 59, "tei": "<biblStruct xml:id=\"b59\">\n\t<analytic>\n\t\t<title level=\"a\" type=\"main\">VOLO: Vision Outlooker for Visual Recognition</title>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Li</forename><surname>Yuan</surname></persName>\n\t\t\t<idno type=\"ORCID\">0000-0002-2120-5588</idno>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Qibin</forename><surname>Hou</surname></persName>\n\t\t\t<idno type=\"ORCID\">0000-0002-8388-8708</idno>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Zihang</forename><surname>Jiang</surname></persName>\n\t\t\t<idno type=\"ORCID\">0000-0002-8096-842X</idno>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Jiashi</forename><surname>Feng</surname></persName>\n\t\t\t<idno type=\"ORCID\">0000-0001-6843-0064</idno>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Shuicheng</forename><surname>Yan</surname></persName>\n\t\t</author>\n\t\t<idno type=\"DOI\">10.1109/tpami.2022.3206108</idno>\n\t</analytic>\n\t<monogr>\n\t\t<title level=\"j\">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>\n\t\t<title level=\"j\" type=\"abbrev\">IEEE Trans. Pattern Anal. Mach. Intell.</title>\n\t\t<idno type=\"ISSN\">0162-8828</idno>\n\t\t<idno type=\"ISSNe\">1939-3539</idno>\n\t\t<imprint>\n\t\t\t<biblScope unit=\"page\" from=\"1\" to=\"13\" />\n\t\t\t<date type=\"published\" when=\"2022\">2022</date>\n\t\t\t<publisher>Institute of Electrical and Electronics Engineers (IEEE)</publisher>\n\t\t</imprint>\n\t</monogr>\n</biblStruct>\n"}], "runtime": 13541, "id": "99670dd30065930705ab8e6abd5316eb96dfc3e9", "metadata": {"id": "99670dd30065930705ab8e6abd5316eb96dfc3e9"}, "original_file_path": "../../datalake/Samuel/SOFTware-Sync/downloads/xml/hal-04322219.grobid.tei.xml", "file_name": "hal-04322219.grobid.tei.xml"}