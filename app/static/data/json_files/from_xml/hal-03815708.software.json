{"application": "software-mentions", "version": "0.8.0", "date": "2024-10-07T11:51+0000", "md5": "9E489FF6066B0C362D09D7B5A3FEAA28", "mentions": [{"type": "software", "software-type": "software", "software-name": {"rawForm": "HatEval", "normalizedForm": "HatEval", "offsetStart": 0, "offsetEnd": 7}, "context": "HatEval has primarily been collected in the year 2018 using a combination of sampling strategies, including keyword-based sampling (with both neutral and derogatory words), collecting the history of identified perpetrators and monitoring the potential victims of hate. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.036469340324401855}, "created": {"value": false, "score": 0.00035583972930908203}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9998302459716797}, "created": {"value": false, "score": 0.00036072731018066406}, "shared": {"value": false, "score": 1.0728836059570312e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "HatEval", "normalizedForm": "HatEval", "offsetStart": 0, "offsetEnd": 7}, "context": "HatEval involves hate against women and immigrants.", "mentionContextAttributes": {"used": {"value": false, "score": 0.00026530027389526367}, "created": {"value": false, "score": 0.00036072731018066406}, "shared": {"value": false, "score": 1.0728836059570312e-06}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9998302459716797}, "created": {"value": false, "score": 0.00036072731018066406}, "shared": {"value": false, "score": 1.0728836059570312e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "HatEval", "normalizedForm": "HatEval", "offsetStart": 4, "offsetEnd": 11}, "context": "The HatEval dataset is part of a shared task and involves a challenging test set with low in-domain performance. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.008449792861938477}, "created": {"value": false, "score": 0.0001436471939086914}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9998302459716797}, "created": {"value": false, "score": 0.00036072731018066406}, "shared": {"value": false, "score": 1.0728836059570312e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "HatEval", "normalizedForm": "HatEval", "offsetStart": 13, "offsetEnd": 20}, "context": "The datasets HatEval (Basile et al., 2019) and Waseem (Waseem and Hovy, 2016) have been sampled from Twitter. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9998302459716797}, "created": {"value": false, "score": 1.8596649169921875e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9998302459716797}, "created": {"value": false, "score": 0.00036072731018066406}, "shared": {"value": false, "score": 1.0728836059570312e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "BERT", "normalizedForm": "BERT", "offsetStart": 14, "offsetEnd": 18}, "context": "Vidgen Waseem BERT Van-MLM-FT 1 m 20 s 3 m 49 s 2 m 10s Dom-spec (\u03b1\u2207\u03b1) 2 m 30s 7 m 3 m 17 s Dom-spec (DL) 4 m 18 m 8 m 16 s the in-domain validation set.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9980825185775757}, "created": {"value": false, "score": 2.2649765014648438e-06}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9980825185775757}, "created": {"value": true, "score": 0.8037917017936707}, "shared": {"value": false, "score": 4.76837158203125e-07}}, "references": [{"label": "(Devlin et al., 2019)", "normalizedForm": "Devlin et al., 2019", "refKey": 15}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "BERT", "normalizedForm": "BERT", "offsetStart": 22, "offsetEnd": 26}, "context": "We initialize all the BERT models with MLM adaptation on the target, except for PERL and AAD, which inherently adapts to the target.", "mentionContextAttributes": {"used": {"value": false, "score": 0.00022965669631958008}, "created": {"value": false, "score": 0.3253445625305176}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9980825185775757}, "created": {"value": true, "score": 0.8037917017936707}, "shared": {"value": false, "score": 4.76837158203125e-07}}, "references": [{"label": "(Devlin et al., 2019)", "normalizedForm": "Devlin et al., 2019", "refKey": 15}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "BERT", "normalizedForm": "BERT", "offsetStart": 23, "offsetEnd": 27}, "context": "We use the pre-trained BERT-base (Devlin et al., 2019)  for 6 epochs with a batch size of 8 for all the BERT baselines and Dom-spec.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9759145379066467}, "created": {"value": false, "score": 2.384185791015625e-06}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9980825185775757}, "created": {"value": true, "score": 0.8037917017936707}, "shared": {"value": false, "score": 4.76837158203125e-07}}, "references": [{"label": "(Devlin et al., 2019)", "normalizedForm": "Devlin et al., 2019", "refKey": 15, "offsetStart": 19427, "offsetEnd": 19448}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "BERT", "normalizedForm": "BERT", "offsetStart": 32, "offsetEnd": 58}, "context": "Each term is tokenized with the BERT (Devlin et al., 2019) WordPiece tokenizer for compatibility with transformer models. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.15074706077575684}, "created": {"value": false, "score": 3.314018249511719e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9980825185775757}, "created": {"value": true, "score": 0.8037917017936707}, "shared": {"value": false, "score": 4.76837158203125e-07}}, "references": [{"label": "(Devlin et al., 2019)", "normalizedForm": "Devlin et al., 2019", "refKey": 15}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "BERT", "normalizedForm": "BERT", "offsetStart": 34, "offsetEnd": 38}, "context": "For a fair comparison, we use the BERT as the underlying model in this approach.", "mentionContextAttributes": {"used": {"value": false, "score": 0.27474498748779297}, "created": {"value": false, "score": 0.011528968811035156}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9980825185775757}, "created": {"value": true, "score": 0.8037917017936707}, "shared": {"value": false, "score": 4.76837158203125e-07}}, "references": [{"label": "(Devlin et al., 2019)", "normalizedForm": "Devlin et al., 2019", "refKey": 15}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "BERT", "normalizedForm": "BERT", "offsetStart": 34, "offsetEnd": 38}, "context": "The full-list of penalized terms (BERT WordPieces) te S across epochs for the examples listed in Section 4.2, is given below.", "mentionContextAttributes": {"used": {"value": true, "score": 0.8086925745010376}, "created": {"value": false, "score": 3.933906555175781e-06}, "shared": {"value": false, "score": 4.76837158203125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9980825185775757}, "created": {"value": true, "score": 0.8037917017936707}, "shared": {"value": false, "score": 4.76837158203125e-07}}, "references": [{"label": "(Devlin et al., 2019)", "normalizedForm": "Devlin et al., 2019", "refKey": 15}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "HatEval", "normalizedForm": "HatEval", "offsetStart": 34, "offsetEnd": 41}, "context": "This is reflected in the cases of HatEval \u2192Waseem and Vidgen \u2192Waseem.", "mentionContextAttributes": {"used": {"value": true, "score": 0.8189033269882202}, "created": {"value": false, "score": 3.421306610107422e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9998302459716797}, "created": {"value": false, "score": 0.00036072731018066406}, "shared": {"value": false, "score": 1.0728836059570312e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "HatEval", "normalizedForm": "HatEval", "offsetStart": 38, "offsetEnd": 45}, "context": "Following is a non-hateful example in HatEval, wrongly classified by Van-MLM-FT but correctly classified by Dom-spec (The darker the shades, the higher the attribution scores assigned): ", "mentionContextAttributes": {"used": {"value": false, "score": 0.08253324031829834}, "created": {"value": false, "score": 7.152557373046875e-06}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9998302459716797}, "created": {"value": false, "score": 0.00036072731018066406}, "shared": {"value": false, "score": 1.0728836059570312e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "HatEval", "normalizedForm": "HatEval", "offsetStart": 41, "offsetEnd": 48}, "context": "We use the standard splits available for HatEval (42.1% hate; train: 89932 , val: 1000; test: 3000) and Vidgen (54.4% hate; train: 32497, val: 1016, test: 4062). ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9960792660713196}, "created": {"value": false, "score": 4.649162292480469e-06}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9998302459716797}, "created": {"value": false, "score": 0.00036072731018066406}, "shared": {"value": false, "score": 1.0728836059570312e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "BERT", "normalizedForm": "BERT", "offsetStart": 44, "offsetEnd": 48}, "context": "To this end, we first continue pre-training BERT on the unlabeled D train T using the Masked Language Model (MLM) objective for incorporating the language-variations of the target domain, following Glava\u0161 et al. (2020).", "mentionContextAttributes": {"used": {"value": false, "score": 0.017984509468078613}, "created": {"value": true, "score": 0.8037917017936707}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9980825185775757}, "created": {"value": true, "score": 0.8037917017936707}, "shared": {"value": false, "score": 4.76837158203125e-07}}, "references": [{"label": "(Devlin et al., 2019)", "normalizedForm": "Devlin et al., 2019", "refKey": 15}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "WordPiece", "normalizedForm": "WordPiece", "offsetStart": 59, "offsetEnd": 68}, "context": "Each term is tokenized with the BERT (Devlin et al., 2019) WordPiece tokenizer for compatibility with transformer models. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.15074706077575684}, "created": {"value": false, "score": 3.314018249511719e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": false, "score": 0.15074706077575684}, "created": {"value": false, "score": 3.314018249511719e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "BERT", "normalizedForm": "BERT", "offsetStart": 72, "offsetEnd": 76}, "context": "However, the overall performance with Dom-spec is comparable to that of BERT Van-FT.", "mentionContextAttributes": {"used": {"value": false, "score": 0.41983896493911743}, "created": {"value": false, "score": 6.771087646484375e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9980825185775757}, "created": {"value": true, "score": 0.8037917017936707}, "shared": {"value": false, "score": 4.76837158203125e-07}}, "references": [{"label": "(Devlin et al., 2019)", "normalizedForm": "Devlin et al., 2019", "refKey": 15}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "BERT", "normalizedForm": "BERT", "offsetStart": 104, "offsetEnd": 108}, "context": "We use the pre-trained BERT-base (Devlin et al., 2019)  for 6 epochs with a batch size of 8 for all the BERT baselines and Dom-spec.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9759145379066467}, "created": {"value": false, "score": 2.384185791015625e-06}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9980825185775757}, "created": {"value": true, "score": 0.8037917017936707}, "shared": {"value": false, "score": 4.76837158203125e-07}}, "references": [{"label": "(Devlin et al., 2019)", "normalizedForm": "Devlin et al., 2019", "refKey": 15}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "HatEval", "normalizedForm": "HatEval", "offsetStart": 124, "offsetEnd": 131}, "context": "The Vidgen dataset is collected through a dynamic data creation process with a human-and-model-in-the-loop strategy, unlike HatEval and Waseem datasets that are sampled from Twitter.", "mentionContextAttributes": {"used": {"value": false, "score": 0.4539489150047302}, "created": {"value": false, "score": 0.00010335445404052734}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9998302459716797}, "created": {"value": false, "score": 0.00036072731018066406}, "shared": {"value": false, "score": 1.0728836059570312e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "BERT", "normalizedForm": "BERT", "offsetStart": 146, "offsetEnd": 150}, "context": "We observe an overall performance drop, compared to Van MLM-FT, with the DA approaches, originally proposed for sentiment classification, namely, BERT PERL, BERT-AAD and HATN.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9428452253341675}, "created": {"value": false, "score": 2.777576446533203e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9980825185775757}, "created": {"value": true, "score": 0.8037917017936707}, "shared": {"value": false, "score": 4.76837158203125e-07}}, "references": [{"label": "(Devlin et al., 2019)", "normalizedForm": "Devlin et al., 2019", "refKey": 15}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "BERT", "normalizedForm": "BERT", "offsetStart": 157, "offsetEnd": 161}, "context": "We observe an overall performance drop, compared to Van MLM-FT, with the DA approaches, originally proposed for sentiment classification, namely, BERT PERL, BERT-AAD and HATN.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9428452253341675}, "created": {"value": false, "score": 2.777576446533203e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9980825185775757}, "created": {"value": true, "score": 0.8037917017936707}, "shared": {"value": false, "score": 4.76837158203125e-07}}, "references": [{"label": "(Devlin et al., 2019)", "normalizedForm": "Devlin et al., 2019", "refKey": 15}]}], "references": [{"refKey": 15, "tei": "<biblStruct xml:id=\"b15\">\n\t<analytic>\n\t\t<title level=\"a\" type=\"main\"></title>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Jacob</forename><surname>Devlin</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Ming-Wei</forename><surname>Chang</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Kenton</forename><surname>Lee</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Kristina</forename><surname>Toutanova</surname></persName>\n\t\t</author>\n\t\t<idno type=\"DOI\">10.18653/v1/n19-1423</idno>\n\t</analytic>\n\t<monogr>\n\t\t<title level=\"m\">Proceedings of the 2019 Conference of the North</title>\n\t\t<meeting>the 2019 Conference of the North</meeting>\n\t\t<imprint>\n\t\t\t<publisher>Association for Computational Linguistics</publisher>\n\t\t\t<date type=\"published\" when=\"2019\">2019</date>\n\t\t\t<biblScope unit=\"volume\">1</biblScope>\n\t\t\t<biblScope unit=\"page\">4186</biblScope>\n\t\t</imprint>\n\t</monogr>\n</biblStruct>\n"}], "runtime": 9457, "id": "21ba34a001ba7d0797e8ce8cce0ee9485d00a58e", "metadata": {"id": "21ba34a001ba7d0797e8ce8cce0ee9485d00a58e"}, "original_file_path": "../../datalake/Samuel/SOFTware-Sync/downloads/xml/hal-03815708.grobid.tei.xml", "file_name": "hal-03815708.grobid.tei.xml"}