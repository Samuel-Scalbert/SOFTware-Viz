{"application": "software-mentions", "version": "0.8.0", "date": "2024-10-07T11:53+0000", "md5": "0DC1AF82A48E9F256FF7D2B075F4ED30", "mentions": [{"type": "software", "software-type": "software", "software-name": {"rawForm": "DeepSpeech", "normalizedForm": "DeepSpeech", "offsetStart": 0, "offsetEnd": 10}, "version": {"rawForm": "0.4.1", "normalizedForm": "0.4.1"}, "context": "DeepSpeech shows the largest discrepancy between the English and French stimuli (larger than English listeners'). ", "mentionContextAttributes": {"used": {"value": true, "score": 0.7030192613601685}, "created": {"value": false, "score": 2.86102294921875e-06}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9991372227668762}, "created": {"value": false, "score": 0.0028018951416015625}, "shared": {"value": true, "score": 0.9759132862091064}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "DeepSpeech DeepSpeech", "normalizedForm": "DeepSpeech DeepSpeech", "offsetStart": 0, "offsetEnd": 41}, "context": "DeepSpeech DeepSpeech (Hannun et al., 2014) is a neural automatic speech recognition model used in the Mozilla speech tools.The model uses bi-directional recurrent units, which integrate information both forwards and backwards in time, to predict text transcriptions (sequences of letters, not phones) from speech. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.0002942681312561035}, "created": {"value": false, "score": 0.011232912540435791}, "shared": {"value": false, "score": 1.5497207641601562e-06}}, "documentContextAttributes": {"used": {"value": false, "score": 0.0002942681312561035}, "created": {"value": false, "score": 0.011232912540435791}, "shared": {"value": false, "score": 1.5497207641601562e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "DeepSpeech", "normalizedForm": "DeepSpeech", "offsetStart": 13, "offsetEnd": 23}, "version": {"rawForm": "0.4.1", "normalizedForm": "0.4.1"}, "context": "Furthermore, DeepSpeech's \u03b4 values for the English-language stimuli are also more homogeneous than the DPGMM's, with a less pronounced slope from difficult to easy stimuli. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.07096028327941895}, "created": {"value": false, "score": 4.291534423828125e-06}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9991372227668762}, "created": {"value": false, "score": 0.0028018951416015625}, "shared": {"value": true, "score": 0.9759132862091064}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "DeepSpeech", "normalizedForm": "DeepSpeech", "offsetStart": 13, "offsetEnd": 23}, "version": {"rawForm": "0.4.1", "normalizedForm": "0.4.1"}, "context": "We show that DeepSpeech, a standard English speech recognizer, is more specialized on English phoneme discrimination than English listeners, and is poorly correlated with their behaviour, even though it yields a low error on the decision task given to humans.", "mentionContextAttributes": {"used": {"value": false, "score": 0.014483153820037842}, "created": {"value": false, "score": 0.0028018951416015625}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9991372227668762}, "created": {"value": false, "score": 0.0028018951416015625}, "shared": {"value": true, "score": 0.9759132862091064}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "DeepSpeech", "normalizedForm": "DeepSpeech", "offsetStart": 27, "offsetEnd": 37}, "version": {"rawForm": "0.4.1", "normalizedForm": "0.4.1"}, "context": "https://github.com/mozilla/DeepSpeech/releases/", "mentionContextAttributes": {"used": {"value": false, "score": 0.00018590688705444336}, "created": {"value": false, "score": 1.8835067749023438e-05}, "shared": {"value": true, "score": 0.9759132862091064}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9991372227668762}, "created": {"value": false, "score": 0.0028018951416015625}, "shared": {"value": true, "score": 0.9759132862091064}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "DeepSpeech", "normalizedForm": "DeepSpeech", "offsetStart": 34, "offsetEnd": 44}, "version": {"rawForm": "0.4.1", "normalizedForm": "0.4.1"}, "context": "The continuous speech recognizer (DeepSpeech) is also bad at predicting human behaviour, but, unlike the articulatory reconstruction, performs well on the experimental task. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.00010609626770019531}, "created": {"value": false, "score": 0.00019127130508422852}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9991372227668762}, "created": {"value": false, "score": 0.0028018951416015625}, "shared": {"value": true, "score": 0.9759132862091064}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Mechanical Turk", "normalizedForm": "Mechanical Turk", "offsetStart": 34, "offsetEnd": 49}, "publisher": {"rawForm": "Amazon", "normalizedForm": "Amazon", "offsetStart": 27, "offsetEnd": 33}, "context": "They performed the task on Amazon Mechanical Turk (US participants) with the LMEDS software (Mahrt, 2016) and were paid for participation. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9999701976776123}, "created": {"value": false, "score": 0.0003490447998046875}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999706745147705}, "created": {"value": false, "score": 0.0003490447998046875}, "shared": {"value": false, "score": 3.5762786865234375e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Kaldi", "normalizedForm": "Kaldi", "offsetStart": 44, "offsetEnd": 70}, "context": "Mel Filterbank Cepstral Coefficients We use Kaldi (Povey et al., 2011) to extract 13 Mel filterbank cepstral coefficients (MFCC): one vector every 10 milliseconds, each analyzing 25 milliseconds of signal. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9999262094497681}, "created": {"value": false, "score": 1.633167266845703e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999262094497681}, "created": {"value": false, "score": 1.633167266845703e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "DeepSpeech", "normalizedForm": "DeepSpeech", "offsetStart": 54, "offsetEnd": 64}, "version": {"rawForm": "0.4.1", "normalizedForm": "0.4.1"}, "context": "We observe a clear separation in the distributions of DeepSpeech's predicted discriminability for the English stimuli (concentrated on the right-hand part of the graph, where the model is better) versus French stimuli. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9963623881340027}, "created": {"value": false, "score": 1.4543533325195312e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9991372227668762}, "created": {"value": false, "score": 0.0028018951416015625}, "shared": {"value": true, "score": 0.9759132862091064}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "DeepSpeech", "normalizedForm": "DeepSpeech", "offsetStart": 59, "offsetEnd": 69}, "version": {"rawForm": "0.4.1", "normalizedForm": "0.4.1"}, "context": "This difference is clear from Figure 1 (left), which plots DeepSpeech's \u03b4 discriminability scores against listeners' averaged accuracy,10 colour-coded for whether the items are native (English) or non-native (French). ", "mentionContextAttributes": {"used": {"value": true, "score": 0.938999354839325}, "created": {"value": false, "score": 6.198883056640625e-06}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9991372227668762}, "created": {"value": false, "score": 0.0028018951416015625}, "shared": {"value": true, "score": 0.9759132862091064}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "LMEDS", "normalizedForm": "LMEDS", "offsetStart": 77, "offsetEnd": 82}, "context": "They performed the task on Amazon Mechanical Turk (US participants) with the LMEDS software (Mahrt, 2016) and were paid for participation. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9999701976776123}, "created": {"value": false, "score": 0.0003490447998046875}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999701976776123}, "created": {"value": false, "score": 0.0003490447998046875}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "references": [{"label": "(Mahrt, 2016)", "normalizedForm": "Mahrt, 2016", "refKey": 13, "offsetStart": 11865, "offsetEnd": 11878}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Mechanical Turk", "normalizedForm": "Mechanical Turk", "offsetStart": 87, "offsetEnd": 103}, "publisher": {"rawForm": "Amazon", "normalizedForm": "Amazon"}, "context": "made a detailed comparison of data from an in-lab speech perception experiment with a Mechanical Turk replication and found a close correspondence between the results.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9999706745147705}, "created": {"value": false, "score": 1.633167266845703e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999706745147705}, "created": {"value": false, "score": 0.0003490447998046875}, "shared": {"value": false, "score": 3.5762786865234375e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "DeepSpeech", "normalizedForm": "DeepSpeech", "offsetStart": 89, "offsetEnd": 99}, "version": {"rawForm": "0.4.1", "normalizedForm": "0.4.1"}, "context": "It would seem that optimizing on the task of predicting English grapheme sequences leads DeepSpeech to attend to, or ignore, very different acoustic information than human listeners, at least in the context of this low-level speech discrimination task.", "mentionContextAttributes": {"used": {"value": false, "score": 0.0001480579376220703}, "created": {"value": false, "score": 0.000601649284362793}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9991372227668762}, "created": {"value": false, "score": 0.0028018951416015625}, "shared": {"value": true, "score": 0.9759132862091064}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "DeepSpeech", "normalizedForm": "DeepSpeech", "offsetStart": 337, "offsetEnd": 347}, "version": {"rawForm": "0.4.1", "normalizedForm": "0.4.1", "offsetStart": 348, "offsetEnd": 354}, "context": "The model has a training objective related to that of the English bottleneck models (predicting text), but the recurrent units allow it to model long distance temporal dependencies, and the units to be predicted are graphemes, which are more similar in their temporal granularity to phonemes than to phone/phoneme-states. We use Mozilla DeepSpeech 0.4.17 , which is trained on the Fisher (Cieri, Miller, & Walker, 2004) and Switchboard (Godfrey, Holliman, & McDaniel, 1992) telephone corpora and the LibriSpeech audio book corpus (Panayotov, Chen, Povey, & Khudanpur, 2015). ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9991372227668762}, "created": {"value": false, "score": 1.7881393432617188e-05}, "shared": {"value": false, "score": 1.0728836059570312e-06}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9991372227668762}, "created": {"value": false, "score": 0.0028018951416015625}, "shared": {"value": true, "score": 0.9759132862091064}}}], "references": [{"refKey": 13, "tei": "<biblStruct xml:id=\"b13\">\n\t<monogr>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">T</forename><surname>Mahrt</surname></persName>\n\t\t</author>\n\t\t<title level=\"m\">LMEDS: Language markup and experimental design software</title>\n\t\t<imprint>\n\t\t\t<date>2016</date>\n\t\t</imprint>\n\t</monogr>\n</biblStruct>\n"}], "runtime": 13073, "id": "89c1d37dd17c4a1a2e125c3e5e60f1fa0ec9cbfd", "metadata": {"id": "89c1d37dd17c4a1a2e125c3e5e60f1fa0ec9cbfd"}, "original_file_path": "../../datalake/Samuel/SOFTware-Sync/downloads/xml/hal-03087248.grobid.tei.xml", "file_name": "hal-03087248.grobid.tei.xml"}