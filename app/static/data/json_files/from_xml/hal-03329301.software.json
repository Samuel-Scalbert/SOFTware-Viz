{"application": "software-mentions", "version": "0.8.0", "date": "2024-10-07T11:51+0000", "md5": "996458787642C912D29E186896B2E904", "mentions": [{"type": "software", "software-type": "software", "software-name": {"rawForm": "sSIMI", "normalizedForm": "sSIMI", "offsetStart": 0, "offsetEnd": 5}, "context": "sSIMI is even harder .", "mentionContextAttributes": {"used": {"value": false, "score": 0.00011169910430908203}, "created": {"value": false, "score": 0.0001437664031982422}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.7923120856285095}, "created": {"value": false, "score": 0.0012881755828857422}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "references": [{"label": "[34]", "normalizedForm": "[34]", "refKey": 34}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "sBLIMP", "normalizedForm": "sBLIMP", "offsetStart": 4, "offsetEnd": 10}, "context": "The sBLIMP scores show smaller improvements over the LSTM baseline than sWUGGY.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9990033507347107}, "created": {"value": false, "score": 3.0994415283203125e-06}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9990033507347107}, "created": {"value": false, "score": 3.4689903259277344e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "sWUGGY", "normalizedForm": "sWUGGY", "offsetStart": 7, "offsetEnd": 13}, "context": "As for sWUGGY, sBLIMP performance shows promising improvements over the baselines. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9627170562744141}, "created": {"value": false, "score": 7.271766662597656e-06}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9990033507347107}, "created": {"value": false, "score": 0.04919910430908203}, "shared": {"value": false, "score": 3.5762786865234375e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "sWUGGY", "normalizedForm": "sWUGGY", "offsetStart": 11, "offsetEnd": 17}, "context": "Unlike for sWUGGY, the toplines show that access to the gold transcription alone is not sufficient: while it helps, good performance is only achieved by RoBERTa, which notably trains on much more data than our other toplines. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.0011522769927978516}, "created": {"value": false, "score": 1.2516975402832031e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9990033507347107}, "created": {"value": false, "score": 0.04919910430908203}, "shared": {"value": false, "score": 3.5762786865234375e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "sBLIMP", "normalizedForm": "sBLIMP", "offsetStart": 12, "offsetEnd": 18}, "context": "Syntax: the sBLIMP acceptability metrics.", "mentionContextAttributes": {"used": {"value": false, "score": 0.01800978183746338}, "created": {"value": false, "score": 3.0994415283203125e-06}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9990033507347107}, "created": {"value": false, "score": 3.4689903259277344e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "PyTorch", "normalizedForm": "PyTorch", "offsetStart": 12, "offsetEnd": 19}, "context": "We used the PyTorch implementation of CPC1  [6], which is a modified version of the CPC model with the following architecture: the encoder genc is a 5-layer 1D-convolutional network with kernel sizes of 10,8,4,4,4 and stride sizes of 5,4,2,2,2 respectively, resulting in a downsampling factor of 160, meaning that, for a 16KHz input, the embeddings have a rate of 100Hz.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9987576007843018}, "created": {"value": false, "score": 3.4570693969726562e-06}, "shared": {"value": false, "score": 1.6689300537109375e-06}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9987576007843018}, "created": {"value": false, "score": 3.4570693969726562e-06}, "shared": {"value": false, "score": 1.6689300537109375e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "sWUGGY", "normalizedForm": "sWUGGY", "offsetStart": 13, "offsetEnd": 19}, "context": "Lexicon: the sWUGGY spot-the-word metrics. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.0022975802421569824}, "created": {"value": false, "score": 1.1324882507324219e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9990033507347107}, "created": {"value": false, "score": 0.04919910430908203}, "shared": {"value": false, "score": 3.5762786865234375e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "sWUGGY", "normalizedForm": "sWUGGY", "offsetStart": 13, "offsetEnd": 19}, "context": "Similarly to sWUGGY, the task is to decide which of the two is grammatical based on the probability of the sentence. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.0005818605422973633}, "created": {"value": false, "score": 0.00018352270126342773}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9990033507347107}, "created": {"value": false, "score": 0.04919910430908203}, "shared": {"value": false, "score": 3.5762786865234375e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "gpred", "normalizedForm": "gpred", "offsetStart": 14, "offsetEnd": 19}, "context": "The predictor gpred is a multi-layer LSTM network, with the same hidden dimension as the encoder, followed by a 1-layer transformer. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.607421338558197}, "created": {"value": false, "score": 1.6808509826660156e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.607421338558197}, "created": {"value": false, "score": 1.6808509826660156e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "sBLIMP", "normalizedForm": "sBLIMP", "offsetStart": 15, "offsetEnd": 21}, "context": "As for sWUGGY, sBLIMP performance shows promising improvements over the baselines. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9627170562744141}, "created": {"value": false, "score": 7.271766662597656e-06}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9990033507347107}, "created": {"value": false, "score": 3.4689903259277344e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "sSIMI", "normalizedForm": "sSIMI", "offsetStart": 23, "offsetEnd": 28}, "context": "Lexical Semantics: the sSIMI similarity metrics.", "mentionContextAttributes": {"used": {"value": true, "score": 0.7923120856285095}, "created": {"value": false, "score": 7.510185241699219e-06}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.7923120856285095}, "created": {"value": false, "score": 0.0012881755828857422}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "references": [{"label": "[34]", "normalizedForm": "[34]", "refKey": 34}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "LibriSpeech", "normalizedForm": "LibriSpeech", "offsetStart": 24, "offsetEnd": 35}, "context": "We encourage use of the LibriSpeech 960h English dataset [2], and for larger models, the clean-6k version of Libri-light [3], a huge collection of speech for unsupervised learning.", "mentionContextAttributes": {"used": {"value": false, "score": 9.143352508544922e-05}, "created": {"value": false, "score": 0.0004975795745849609}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999837875366211}, "created": {"value": false, "score": 0.010391056537628174}, "shared": {"value": false, "score": 3.5762786865234375e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "WUGGY", "normalizedForm": "WUGGY", "offsetStart": 31, "offsetEnd": 39}, "context": "The nonwords are produced with WUGGY [7], which generates, for a given word, a list of candidate nonwords best matched in phonotactics and syllabic structure, which we additionally filtered for pronouncability using G2P, and for having on average the same unigram and bigram phoneme frequencies as words. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.8259254693984985}, "created": {"value": false, "score": 6.771087646484375e-05}, "shared": {"value": false, "score": 7.152557373046875e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.8259254693984985}, "created": {"value": false, "score": 6.771087646484375e-05}, "shared": {"value": false, "score": 7.152557373046875e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Google Speech API", "normalizedForm": "Google Speech API", "offsetStart": 31, "offsetEnd": 48}, "context": "Stimuli were produced with the Google Speech API. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9993865489959717}, "created": {"value": false, "score": 0.009637832641601562}, "shared": {"value": false, "score": 8.344650268554688e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9993865489959717}, "created": {"value": false, "score": 0.009637832641601562}, "shared": {"value": false, "score": 8.344650268554688e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "LibriSpeech", "normalizedForm": "LibriSpeech", "offsetStart": 33, "offsetEnd": 44}, "context": "Stimuli were filtered to contain LibriSpeech vocabulary and for natural prosodic contours, and synthesised as above.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9999837875366211}, "created": {"value": false, "score": 7.62939453125e-06}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999837875366211}, "created": {"value": false, "score": 0.010391056537628174}, "shared": {"value": false, "score": 3.5762786865234375e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "LibriSpeech", "normalizedForm": "LibriSpeech", "offsetStart": 36, "offsetEnd": 47}, "context": "Pairs containing a word absent from LibriSpeech train set [2] were discarded. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9997840523719788}, "created": {"value": false, "score": 1.0371208190917969e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999837875366211}, "created": {"value": false, "score": 0.010391056537628174}, "shared": {"value": false, "score": 3.5762786865234375e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "gpred", "normalizedForm": "gpred", "offsetStart": 41, "offsetEnd": 46}, "context": "At each time step t, a predictor network gpred takes as input the available embeddings z1, . . .", "mentionContextAttributes": {"used": {"value": false, "score": 0.019937455654144287}, "created": {"value": false, "score": 4.172325134277344e-06}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.607421338558197}, "created": {"value": false, "score": 1.6808509826660156e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Google API", "normalizedForm": "Google API", "offsetStart": 69, "offsetEnd": 79}, "context": "We then created two subsets of audio files, one synthetic (using the Google API), one natural obtained by retrieving the audio extracts from LibriSpeech corresponding to each word, following the process presented in [9]. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9965502023696899}, "created": {"value": false, "score": 0.010391056537628174}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9965502023696899}, "created": {"value": false, "score": 0.010391056537628174}, "shared": {"value": false, "score": 2.384185791015625e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "sSIMI", "normalizedForm": "sSIMI", "offsetStart": 72, "offsetEnd": 77}, "context": "Indeed, JC1 demonstrates that explicit term discovery is useful for the sSIMI task.", "mentionContextAttributes": {"used": {"value": false, "score": 5.829334259033203e-05}, "created": {"value": false, "score": 0.0012881755828857422}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.7923120856285095}, "created": {"value": false, "score": 0.0012881755828857422}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "references": [{"label": "[34]", "normalizedForm": "[34]", "refKey": 34}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "sWUGGY", "normalizedForm": "sWUGGY", "offsetStart": 72, "offsetEnd": 78}, "context": "The sBLIMP scores show smaller improvements over the LSTM baseline than sWUGGY. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9990033507347107}, "created": {"value": false, "score": 3.0994415283203125e-06}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9990033507347107}, "created": {"value": false, "score": 0.04919910430908203}, "shared": {"value": false, "score": 3.5762786865234375e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "sBLIMP", "normalizedForm": "sBLIMP", "offsetStart": 72, "offsetEnd": 78}, "context": "Still, the gaps with text-based approaches are more pronounced for both sBLIMP and sSIMI. ", "mentionContextAttributes": {"used": {"value": false, "score": 8.189678192138672e-05}, "created": {"value": false, "score": 3.4689903259277344e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9990033507347107}, "created": {"value": false, "score": 3.4689903259277344e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Libri-light", "normalizedForm": "Libri-light", "offsetStart": 74, "offsetEnd": 85}, "context": "We report results from a 4-layer LSTM, trained on the clean-6k version of Libri-light (see [4] for additional results from a smaller model). ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9999192953109741}, "created": {"value": false, "score": 2.086162567138672e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999192953109741}, "created": {"value": false, "score": 0.0004975795745849609}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "references": [{"label": "[4]", "normalizedForm": "[4]", "refKey": 4, "offsetStart": 8953, "offsetEnd": 8956}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "sSIMI", "normalizedForm": "sSIMI", "offsetStart": 83, "offsetEnd": 88}, "context": "Still, the gaps with text-based approaches are more pronounced for both sBLIMP and sSIMI. ", "mentionContextAttributes": {"used": {"value": false, "score": 8.189678192138672e-05}, "created": {"value": false, "score": 3.4689903259277344e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.7923120856285095}, "created": {"value": false, "score": 0.0012881755828857422}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "references": [{"label": "[34]", "normalizedForm": "[34]", "refKey": 34}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "sWUGGY", "normalizedForm": "sWUGGY", "offsetStart": 95, "offsetEnd": 101}, "context": "Most prominently, progress has been made in the span of one iteration of this challenge on the sWUGGY metric.", "mentionContextAttributes": {"used": {"value": false, "score": 0.0027611255645751953}, "created": {"value": false, "score": 0.04919910430908203}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9990033507347107}, "created": {"value": false, "score": 0.04919910430908203}, "shared": {"value": false, "score": 3.5762786865234375e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "sWUGGY", "normalizedForm": "sWUGGY", "offsetStart": 99, "offsetEnd": 105}, "context": "Beginning from a (k-means) discretized version of the respective acoustic CPC representations, for sWUGGY, they treat the training corpus as a dictionary and extract a distance to the best match; for sBLIMP, they train an LSTM; and, for sSIMI, they perform segmentation to discover word types. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.019780337810516357}, "created": {"value": false, "score": 1.7881393432617188e-06}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9990033507347107}, "created": {"value": false, "score": 0.04919910430908203}, "shared": {"value": false, "score": 3.5762786865234375e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "LibriSpeech", "normalizedForm": "LibriSpeech", "offsetStart": 107, "offsetEnd": 118}, "context": "We trained a BERT model on forcealigned phoneme labels (one per frame) using the gold transcription of the LibriSpeech dataset. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9999830722808838}, "created": {"value": false, "score": 0.0035558342933654785}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999837875366211}, "created": {"value": false, "score": 0.010391056537628174}, "shared": {"value": false, "score": 3.5762786865234375e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Libri-light", "normalizedForm": "Libri-light", "offsetStart": 109, "offsetEnd": 123}, "context": "We encourage use of the LibriSpeech 960h English dataset [2], and for larger models, the clean-6k version of Libri-light [3], a huge collection of speech for unsupervised learning.", "mentionContextAttributes": {"used": {"value": false, "score": 9.143352508544922e-05}, "created": {"value": false, "score": 0.0004975795745849609}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999192953109741}, "created": {"value": false, "score": 0.0004975795745849609}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "references": [{"label": "[4]", "normalizedForm": "[4]", "refKey": 4}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "LibriSpeech", "normalizedForm": "LibriSpeech", "offsetStart": 110, "offsetEnd": 121}, "context": "The synthesized subset is composed of 9744 and 705 word pairs for the test and dev sets respectively, and the LibriSpeech subset is composed of 3753 and 309 pairs for the test and dev sets.", "mentionContextAttributes": {"used": {"value": true, "score": 0.7115128040313721}, "created": {"value": false, "score": 1.0371208190917969e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999837875366211}, "created": {"value": false, "score": 0.010391056537628174}, "shared": {"value": false, "score": 3.5762786865234375e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "sWUGGY", "normalizedForm": "sWUGGY", "offsetStart": 119, "offsetEnd": 125}, "context": "Participants are to provide a number (probability or pseudo-probability) associated to We also prepared additional OOV-sWUGGY test and development sets consisting of 20,000 and 5,000 pairs respectively, with existing words which do not appear in the LibriSpeech training set.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9989713430404663}, "created": {"value": false, "score": 0.001264035701751709}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9990033507347107}, "created": {"value": false, "score": 0.04919910430908203}, "shared": {"value": false, "score": 3.5762786865234375e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "LibriSpeech", "normalizedForm": "LibriSpeech", "offsetStart": 122, "offsetEnd": 133}, "context": "In addition to the forced-alignment BERT, we also included a BERT model trained on the gold phonetic transcription of the LibriSpeech dataset (no framewise repetitions), with the difference that we only mask one token instead of a span of tokens, since each token is the width of a phoneme rather than a frame. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9827067852020264}, "created": {"value": false, "score": 6.99758529663086e-05}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999837875366211}, "created": {"value": false, "score": 0.010391056537628174}, "shared": {"value": false, "score": 3.5762786865234375e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "LibriSpeech", "normalizedForm": "LibriSpeech", "offsetStart": 141, "offsetEnd": 152}, "context": "We then created two subsets of audio files, one synthetic (using the Google API), one natural obtained by retrieving the audio extracts from LibriSpeech corresponding to each word, following the process presented in [9]. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9965502023696899}, "created": {"value": false, "score": 0.010391056537628174}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999837875366211}, "created": {"value": false, "score": 0.010391056537628174}, "shared": {"value": false, "score": 3.5762786865234375e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "LibriSpeech", "normalizedForm": "LibriSpeech", "offsetStart": 148, "offsetEnd": 159}, "context": "The natural subset is smaller than its synthesised counterpart, as we had to discard pairs from the test and dev sets which were not present in the LibriSpeech test and dev sets respectively. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9986554384231567}, "created": {"value": false, "score": 3.159046173095703e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999837875366211}, "created": {"value": false, "score": 0.010391056537628174}, "shared": {"value": false, "score": 3.5762786865234375e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "LibriSpeech", "normalizedForm": "LibriSpeech", "offsetStart": 151, "offsetEnd": 162}, "context": "The force-aligned topline, which takes frames as input, shows much poorer performance-in some cases worse than the random baseline (note also, for the LibriSpeech test set, the random baseline is actually the best).", "mentionContextAttributes": {"used": {"value": false, "score": 0.00663071870803833}, "created": {"value": false, "score": 2.086162567138672e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999837875366211}, "created": {"value": false, "score": 0.010391056537628174}, "shared": {"value": false, "score": 3.5762786865234375e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "sSIMI", "normalizedForm": "sSIMI", "offsetStart": 165, "offsetEnd": 170}, "context": "RoBERTa may also be limited by its use of word-pieces as input, as even simple word embeddings obtain better correlations than those seen here on metrics related to sSIMI [34].", "mentionContextAttributes": {"used": {"value": false, "score": 7.283687591552734e-05}, "created": {"value": false, "score": 1.2755393981933594e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.7923120856285095}, "created": {"value": false, "score": 0.0012881755828857422}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "references": [{"label": "[34]", "normalizedForm": "[34]", "refKey": 34, "offsetStart": 18808, "offsetEnd": 18812}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "sBLIMP", "normalizedForm": "sBLIMP", "offsetStart": 200, "offsetEnd": 206}, "context": "Beginning from a (k-means) discretized version of the respective acoustic CPC representations, for sWUGGY, they treat the training corpus as a dictionary and extract a distance to the best match; for sBLIMP, they train an LSTM; and, for sSIMI, they perform segmentation to discover word types.", "mentionContextAttributes": {"used": {"value": false, "score": 0.019780337810516357}, "created": {"value": false, "score": 1.7881393432617188e-06}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9990033507347107}, "created": {"value": false, "score": 3.4689903259277344e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "LibriSpeech", "normalizedForm": "LibriSpeech", "offsetStart": 209, "offsetEnd": 220}, "context": "For an absolute topline comparison, we used the pretrained RoBERTa large model ( [24]), which was trained on 50K subword units on a huge dataset of total 160GB, 3000 times bigger than the transcription of the LibriSpeech 960h dataset.", "mentionContextAttributes": {"used": {"value": true, "score": 0.999970555305481}, "created": {"value": false, "score": 3.4570693969726562e-06}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999837875366211}, "created": {"value": false, "score": 0.010391056537628174}, "shared": {"value": false, "score": 3.5762786865234375e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "sSIMI", "normalizedForm": "sSIMI", "offsetStart": 237, "offsetEnd": 242}, "context": "Beginning from a (k-means) discretized version of the respective acoustic CPC representations, for sWUGGY, they treat the training corpus as a dictionary and extract a distance to the best match; for sBLIMP, they train an LSTM; and, for sSIMI, they perform segmentation to discover word types.", "mentionContextAttributes": {"used": {"value": false, "score": 0.019780337810516357}, "created": {"value": false, "score": 1.7881393432617188e-06}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.7923120856285095}, "created": {"value": false, "score": 0.0012881755828857422}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "references": [{"label": "[34]", "normalizedForm": "[34]", "refKey": 34}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "LibriSpeech", "normalizedForm": "LibriSpeech", "offsetStart": 250, "offsetEnd": 261}, "context": "Participants are to provide a number (probability or pseudo-probability) associated to We also prepared additional OOV-sWUGGY test and development sets consisting of 20,000 and 5,000 pairs respectively, with existing words which do not appear in the LibriSpeech training set. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9989713430404663}, "created": {"value": false, "score": 0.001264035701751709}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999837875366211}, "created": {"value": false, "score": 0.010391056537628174}, "shared": {"value": false, "score": 3.5762786865234375e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "LibriSpeech", "normalizedForm": "LibriSpeech", "offsetStart": 280, "offsetEnd": 291}, "context": "The systems are in principle less comparable than in previous years, because the choice of training sets was freer, but all systems made use of either the baseline representations or the same choices of corpus for re-training, with the exception of HL, which trained its units on LibriSpeech 100 hrs (clean), a smaller data set.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9561719298362732}, "created": {"value": false, "score": 4.947185516357422e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999837875366211}, "created": {"value": false, "score": 0.010391056537628174}, "shared": {"value": false, "score": 3.5762786865234375e-07}}}], "references": [{"refKey": 34, "tei": "<biblStruct xml:id=\"b34\">\n\t<analytic>\n\t\t<title level=\"a\" type=\"main\">Replicability Analysis for Natural Language Processing: Testing Significance with Multiple Datasets</title>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Rotem</forename><surname>Dror</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Gili</forename><surname>Baumer</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Marina</forename><surname>Bogomolov</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Roi</forename><surname>Reichart</surname></persName>\n\t\t</author>\n\t\t<idno type=\"DOI\">10.1162/tacl_a_00074</idno>\n\t</analytic>\n\t<monogr>\n\t\t<title level=\"j\">Transactions of the Association for Computational Linguistics</title>\n\t\t<title level=\"j\" type=\"abbrev\">TACL</title>\n\t\t<idno type=\"ISSNe\">2307-387X</idno>\n\t\t<imprint>\n\t\t\t<biblScope unit=\"volume\">5</biblScope>\n\t\t\t<biblScope unit=\"page\" from=\"471\" to=\"486\" />\n\t\t\t<date type=\"published\" when=\"2017-12\">2017</date>\n\t\t\t<publisher>MIT Press</publisher>\n\t\t</imprint>\n\t</monogr>\n</biblStruct>\n"}, {"refKey": 4, "tei": "<biblStruct xml:id=\"b4\">\n\t<monogr>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">T</forename><forename type=\"middle\">A</forename><surname>Nguyen</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">M</forename><surname>Seyssel</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">P</forename><surname>Roz\u00e9</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">M</forename><surname>Rivi\u00e8re</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">E</forename><surname>Kharitonov</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">A</forename><surname>Baevski</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">E</forename><surname>Dunbar</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">E</forename><surname>Dupoux</surname></persName>\n\t\t</author>\n\t\t<idno>arXiv:2011.11588</idno>\n\t\t<title level=\"m\">The zero resource speech benchmark 2021: Metrics and baselines for unsupervised spoken language modeling</title>\n\t\t<imprint>\n\t\t\t<date>2020</date>\n\t\t</imprint>\n\t</monogr>\n</biblStruct>\n"}], "runtime": 19423, "id": "64d7b1a8c487996ab49e952babbace4b21707e32", "metadata": {"id": "64d7b1a8c487996ab49e952babbace4b21707e32"}, "original_file_path": "../../datalake/Samuel/SOFTware-Sync/downloads/xml/hal-03329301.grobid.tei.xml", "file_name": "hal-03329301.grobid.tei.xml"}