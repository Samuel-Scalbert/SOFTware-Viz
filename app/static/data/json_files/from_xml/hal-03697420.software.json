{"application": "software-mentions", "version": "0.8.0", "date": "2024-10-07T12:45+0000", "md5": "538B4A44902CCAC0F53A8B97AC910619", "mentions": [{"type": "software", "software-type": "software", "software-name": {"rawForm": "HuBERT", "normalizedForm": "HuBERT", "offsetStart": 0, "offsetEnd": 6}, "context": "HuBERT is trained to guess this cluster assignment for masked and unmasked frames at the same time. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.0015815496444702148}, "created": {"value": false, "score": 0.00040608644485473633}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999879598617554}, "created": {"value": false, "score": 0.006211221218109131}, "shared": {"value": false, "score": 7.152557373046875e-07}}, "references": [{"label": "(Hsu et al., 2021a,b", "normalizedForm": "(Hsu et al., 2021a,b", "refKey": 0}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "DeepSpeech", "normalizedForm": "DeepSpeech", "offsetStart": 0, "offsetEnd": 10}, "context": "DeepSpeech is composed of two convolutional layers followed by five RNN layers and a fully connected layer. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.0002967715263366699}, "created": {"value": false, "score": 4.756450653076172e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.999743640422821}, "created": {"value": true, "score": 0.9944478273391724}, "shared": {"value": false, "score": 8.344650268554688e-07}}, "references": [{"label": "(Amodei et al., 2016)", "normalizedForm": "Amodei et al., 2016", "refKey": 1}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "HuBERT", "normalizedForm": "HuBERT", "offsetStart": 4, "offsetEnd": 10}, "context": "For HuBERT, we also use the Fairseq Base implementation 10 and the LibriSpeech configuration. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9987646341323853}, "created": {"value": false, "score": 3.5762786865234375e-06}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999879598617554}, "created": {"value": false, "score": 0.006211221218109131}, "shared": {"value": false, "score": 7.152557373046875e-07}}, "references": [{"label": "(Hsu et al., 2021a,b", "normalizedForm": "(Hsu et al., 2021a,b", "refKey": 0}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "PyTorch", "normalizedForm": "PyTorch", "offsetStart": 9, "offsetEnd": 16}, "context": "We use a PyTorch implementation of Deep- Speech. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9976968169212341}, "created": {"value": false, "score": 0.0032799839973449707}, "shared": {"value": false, "score": 5.960464477539062e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9976968169212341}, "created": {"value": false, "score": 0.0032799839973449707}, "shared": {"value": false, "score": 5.960464477539062e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "DeepSpeech", "normalizedForm": "DeepSpeech", "offsetStart": 9, "offsetEnd": 19}, "context": "To train DeepSpeech as a phone recognizer, the text transcriptions included in CommonVoice are phonemized using eSpeakNG. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.01383960247039795}, "created": {"value": false, "score": 2.47955322265625e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.999743640422821}, "created": {"value": true, "score": 0.9944478273391724}, "shared": {"value": false, "score": 8.344650268554688e-07}}, "references": [{"label": "(Amodei et al., 2016)", "normalizedForm": "Amodei et al., 2016", "refKey": 1}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "DeepSpeech", "normalizedForm": "DeepSpeech", "offsetStart": 14, "offsetEnd": 46}, "context": "We also train DeepSpeech (Amodei et al., 2016) as a supervised reference.", "mentionContextAttributes": {"used": {"value": false, "score": 0.0004870891571044922}, "created": {"value": true, "score": 0.9944478273391724}, "shared": {"value": false, "score": 8.344650268554688e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.999743640422821}, "created": {"value": true, "score": 0.9944478273391724}, "shared": {"value": false, "score": 8.344650268554688e-07}}, "references": [{"label": "(Amodei et al., 2016)", "normalizedForm": "Amodei et al., 2016", "refKey": 1}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "HuBERT", "normalizedForm": "HuBERT", "offsetStart": 15, "offsetEnd": 21}, "context": "We also test a HuBERT model (Hsu et al., 2021a,b).", "mentionContextAttributes": {"used": {"value": true, "score": 0.9660319685935974}, "created": {"value": false, "score": 0.002313852310180664}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999879598617554}, "created": {"value": false, "score": 0.006211221218109131}, "shared": {"value": false, "score": 7.152557373046875e-07}}, "references": [{"label": "(Hsu et al., 2021a,b)", "normalizedForm": "Hsu et al., 2021a,b", "refKey": 0, "offsetStart": 14088, "offsetEnd": 14109}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "HuBERT", "normalizedForm": "HuBERT", "offsetStart": 16, "offsetEnd": 22}, "context": "Wav2vec 2.0 and HuBERT do not model language-specific differences in human speech perception, and can be seen as modelling a language-neutral or universal speech perception space.", "mentionContextAttributes": {"used": {"value": false, "score": 0.00015628337860107422}, "created": {"value": false, "score": 2.5272369384765625e-05}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999879598617554}, "created": {"value": false, "score": 0.006211221218109131}, "shared": {"value": false, "score": 7.152557373046875e-07}}, "references": [{"label": "(Hsu et al., 2021a,b", "normalizedForm": "(Hsu et al., 2021a,b", "refKey": 0}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "HuBERT", "normalizedForm": "HuBERT", "offsetStart": 17, "offsetEnd": 23}, "context": "The loss used by HuBERT is the following:", "mentionContextAttributes": {"used": {"value": true, "score": 0.9937711358070374}, "created": {"value": false, "score": 1.6689300537109375e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999879598617554}, "created": {"value": false, "score": 0.006211221218109131}, "shared": {"value": false, "score": 7.152557373046875e-07}}, "references": [{"label": "(Hsu et al., 2021a,b", "normalizedForm": "(Hsu et al., 2021a,b", "refKey": 0}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "HuBERT", "normalizedForm": "HuBERT", "offsetStart": 27, "offsetEnd": 33}, "context": "This model is smaller than HuBERT or wav2vec 2.0, as it is only made up of 5 convolutions (the encoder) and one LSTM layer (the sequence model).", "mentionContextAttributes": {"used": {"value": false, "score": 0.003080129623413086}, "created": {"value": false, "score": 1.5020370483398438e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999879598617554}, "created": {"value": false, "score": 0.006211221218109131}, "shared": {"value": false, "score": 7.152557373046875e-07}}, "references": [{"label": "(Hsu et al., 2021a,b", "normalizedForm": "(Hsu et al., 2021a,b", "refKey": 0}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Fairseq Base", "normalizedForm": "Fairseq Base", "offsetStart": 28, "offsetEnd": 40}, "context": "For HuBERT, we also use the Fairseq Base implementation 10 and the LibriSpeech configuration. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9987646341323853}, "created": {"value": false, "score": 3.5762786865234375e-06}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9993832111358643}, "created": {"value": false, "score": 3.5762786865234375e-06}, "shared": {"value": false, "score": 1.1920928955078125e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Fairseq Base", "normalizedForm": "Fairseq Base", "offsetStart": 28, "offsetEnd": 40}, "context": "For wav2vec 2.0, we use the Fairseq Base implementation, 9 using the LibriSpeech configuration.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9993832111358643}, "created": {"value": false, "score": 3.4570693969726562e-06}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9993832111358643}, "created": {"value": false, "score": 3.5762786865234375e-06}, "shared": {"value": false, "score": 1.1920928955078125e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "HuBERT", "normalizedForm": "HuBERT", "offsetStart": 33, "offsetEnd": 39}, "context": "Similarly, in French, the native HuBERT shows an advantage over the non-native (English-trained) HuBERT, while the reverse is true in English.", "mentionContextAttributes": {"used": {"value": false, "score": 0.0006414055824279785}, "created": {"value": false, "score": 1.8835067749023438e-05}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999879598617554}, "created": {"value": false, "score": 0.006211221218109131}, "shared": {"value": false, "score": 7.152557373046875e-07}}, "references": [{"label": "(Hsu et al., 2021a,b", "normalizedForm": "(Hsu et al., 2021a,b", "refKey": 0}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Facebook Research", "normalizedForm": "Facebook Research", "offsetStart": 33, "offsetEnd": 50}, "context": "7  For the CPC model, we use the Facebook Research implementation 8 with all the default parameters. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9945008754730225}, "created": {"value": false, "score": 0.0006066560745239258}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9945008754730225}, "created": {"value": false, "score": 0.0006066560745239258}, "shared": {"value": false, "score": 1.1920928955078125e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "HuBERT", "normalizedForm": "HuBERT", "offsetStart": 39, "offsetEnd": 45}, "context": "For English, we tested a wav2vec and a HuBERT model trained on Librispeech (Panayotov et al., 2015) (960 h) and for French, we tested a wav2vec model trained on the French Voxpopuli dataset (Wang et al., 2021) (4.5k h).", "mentionContextAttributes": {"used": {"value": true, "score": 0.9999674558639526}, "created": {"value": false, "score": 3.4570693969726562e-06}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999879598617554}, "created": {"value": false, "score": 0.006211221218109131}, "shared": {"value": false, "score": 7.152557373046875e-07}}, "references": [{"label": "(Hsu et al., 2021a,b", "normalizedForm": "(Hsu et al., 2021a,b", "refKey": 0}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "- Speech", "normalizedForm": "Speech", "offsetStart": 39, "offsetEnd": 47}, "context": "We use a PyTorch implementation of Deep- Speech. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9976968169212341}, "created": {"value": false, "score": 0.0032799839973449707}, "shared": {"value": false, "score": 5.960464477539062e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999449253082275}, "created": {"value": false, "score": 0.006211221218109131}, "shared": {"value": false, "score": 5.960464477539062e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "HuBERT", "normalizedForm": "HuBERT", "offsetStart": 45, "offsetEnd": 51}, "context": "Lakhotia et al. (2021) compared wav2vec 2.0, HuBERT, and contrastive predictive coding (CPC: Oord et al. 2017; Rivi\u00e8re and Dupoux 2021) using an ABX discriminability metric (Schatz, 2016), demonstrating that all three models preserve and enhance linguistically relevant speech sound contrasts in the language they are trained on.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9997469782829285}, "created": {"value": false, "score": 1.4066696166992188e-05}, "shared": {"value": false, "score": 7.152557373046875e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999879598617554}, "created": {"value": false, "score": 0.006211221218109131}, "shared": {"value": false, "score": 7.152557373046875e-07}}, "references": [{"label": "(Hsu et al., 2021a,b", "normalizedForm": "(Hsu et al., 2021a,b", "refKey": 0}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "DeepSpeech", "normalizedForm": "DeepSpeech", "offsetStart": 52, "offsetEnd": 62}, "context": "As a supervised reference system, we test a trained DeepSpeech model (Amodei et al., 2016).", "mentionContextAttributes": {"used": {"value": false, "score": 0.3341515064239502}, "created": {"value": true, "score": 0.8992266058921814}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.999743640422821}, "created": {"value": true, "score": 0.9944478273391724}, "shared": {"value": false, "score": 8.344650268554688e-07}}, "references": [{"label": "(Amodei et al., 2016)", "normalizedForm": "Amodei et al., 2016", "refKey": 1, "offsetStart": 14925, "offsetEnd": 14946}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "HuBERT", "normalizedForm": "HuBERT", "offsetStart": 54, "offsetEnd": 60}, "context": "We tested this using available pretrained wav2vec and HuBERT models trained on much larger amounts of data.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9999879598617554}, "created": {"value": false, "score": 0.0008209347724914551}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999879598617554}, "created": {"value": false, "score": 0.006211221218109131}, "shared": {"value": false, "score": 7.152557373046875e-07}}, "references": [{"label": "(Hsu et al., 2021a,b", "normalizedForm": "(Hsu et al., 2021a,b", "refKey": 0}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "DeepSpeech", "normalizedForm": "DeepSpeech", "offsetStart": 71, "offsetEnd": 81}, "context": "There is a striking difference between languages in the performance of DeepSpeech: for English, the native DeepSpeech shows a substantial advantage over the non-native (French-trained) Deep-Speech which is not present for the French datasets.", "mentionContextAttributes": {"used": {"value": false, "score": 0.00041109323501586914}, "created": {"value": false, "score": 0.00025326013565063477}, "shared": {"value": false, "score": 5.960464477539062e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.999743640422821}, "created": {"value": true, "score": 0.9944478273391724}, "shared": {"value": false, "score": 8.344650268554688e-07}}, "references": [{"label": "(Amodei et al., 2016)", "normalizedForm": "Amodei et al., 2016", "refKey": 1}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "HuBERT", "normalizedForm": "HuBERT", "offsetStart": 77, "offsetEnd": 104}, "context": "Systems using self-supervised pre-training, both using wav2vec 2.0 and using HuBERT (Hsu et al., 2021a,b), show excellent word error rates after having been fine-tuned on only ten minutes of labelled data.", "mentionContextAttributes": {"used": {"value": false, "score": 0.00992727279663086}, "created": {"value": false, "score": 1.4901161193847656e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999879598617554}, "created": {"value": false, "score": 0.006211221218109131}, "shared": {"value": false, "score": 7.152557373046875e-07}}, "references": [{"label": "(Hsu et al., 2021a,b", "normalizedForm": "(Hsu et al., 2021a,b", "refKey": 0}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "DeepSpeech", "normalizedForm": "DeepSpeech", "offsetStart": 79, "offsetEnd": 89}, "context": "For both the (experimental item-level) and the (phone contrast-level) \u03c1 score, DeepSpeech consistently outperforms over wav2vec 2.0. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.010903418064117432}, "created": {"value": false, "score": 1.7881393432617188e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.999743640422821}, "created": {"value": true, "score": 0.9944478273391724}, "shared": {"value": false, "score": 8.344650268554688e-07}}, "references": [{"label": "(Amodei et al., 2016)", "normalizedForm": "Amodei et al., 2016", "refKey": 1}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "DeepSpeech", "normalizedForm": "DeepSpeech", "offsetStart": 88, "offsetEnd": 119}, "context": "We compare the performance of these self-supervised models with a supervised ASR model, DeepSpeech (Amodei et al., 2016), trained on the same data but using phonemic labels. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9366549849510193}, "created": {"value": false, "score": 1.4781951904296875e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.999743640422821}, "created": {"value": true, "score": 0.9944478273391724}, "shared": {"value": false, "score": 8.344650268554688e-07}}, "references": [{"label": "(Amodei et al., 2016)", "normalizedForm": "Amodei et al., 2016", "refKey": 1}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "DeepSpeech", "normalizedForm": "DeepSpeech", "offsetStart": 94, "offsetEnd": 104}, "context": "The main differences in our experiment with (Millet and Dunbar, 2020b) are the type of model (DeepSpeech instead of HMM-GMM), and with (Millet and Dunbar, 2020a) the type of training objective (phone recognition rather than prediction of orthographic text), and the size of the training corpora (we use fewer data).", "mentionContextAttributes": {"used": {"value": false, "score": 0.4430801272392273}, "created": {"value": false, "score": 2.5153160095214844e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.999743640422821}, "created": {"value": true, "score": 0.9944478273391724}, "shared": {"value": false, "score": 8.344650268554688e-07}}, "references": [{"label": "(Amodei et al., 2016)", "normalizedForm": "Amodei et al., 2016", "refKey": 1}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "HuBERT", "normalizedForm": "HuBERT", "offsetStart": 97, "offsetEnd": 103}, "context": "Similarly, in French, the native HuBERT shows an advantage over the non-native (English-trained) HuBERT, while the reverse is true in English.", "mentionContextAttributes": {"used": {"value": false, "score": 0.0006414055824279785}, "created": {"value": false, "score": 1.8835067749023438e-05}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999879598617554}, "created": {"value": false, "score": 0.006211221218109131}, "shared": {"value": false, "score": 7.152557373046875e-07}}, "references": [{"label": "(Hsu et al., 2021a,b", "normalizedForm": "(Hsu et al., 2021a,b", "refKey": 0}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "HuBERT", "normalizedForm": "HuBERT", "offsetStart": 100, "offsetEnd": 106}, "context": "However, these two major differences may be in part explained by global effects: the French-trained HuBERT model is better at predicting the results for all participants (not just French-speaking participants), as is the English-trained DeepSpeech model.", "mentionContextAttributes": {"used": {"value": false, "score": 0.003963649272918701}, "created": {"value": false, "score": 0.00014925003051757812}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999879598617554}, "created": {"value": false, "score": 0.006211221218109131}, "shared": {"value": false, "score": 7.152557373046875e-07}}, "references": [{"label": "(Hsu et al., 2021a,b", "normalizedForm": "(Hsu et al., 2021a,b", "refKey": 0}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "HuBERT", "normalizedForm": "HuBERT", "offsetStart": 107, "offsetEnd": 113}, "context": "Our CPC model differs from the other models tested in its small size, its causal architecture (wav2vec and HuBERT use transformers), and in that it does not use masking during its training.", "mentionContextAttributes": {"used": {"value": false, "score": 0.000647127628326416}, "created": {"value": false, "score": 0.00507742166519165}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999879598617554}, "created": {"value": false, "score": 0.006211221218109131}, "shared": {"value": false, "score": 7.152557373046875e-07}}, "references": [{"label": "(Hsu et al., 2021a,b", "normalizedForm": "(Hsu et al., 2021a,b", "refKey": 0}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "DeepSpeech", "normalizedForm": "DeepSpeech", "offsetStart": 107, "offsetEnd": 117}, "context": "There is a striking difference between languages in the performance of DeepSpeech: for English, the native DeepSpeech shows a substantial advantage over the non-native (French-trained) Deep-Speech which is not present for the French datasets.", "mentionContextAttributes": {"used": {"value": false, "score": 0.00041109323501586914}, "created": {"value": false, "score": 0.00025326013565063477}, "shared": {"value": false, "score": 5.960464477539062e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.999743640422821}, "created": {"value": true, "score": 0.9944478273391724}, "shared": {"value": false, "score": 8.344650268554688e-07}}, "references": [{"label": "(Amodei et al., 2016)", "normalizedForm": "Amodei et al., 2016", "refKey": 1}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "HuBERT", "normalizedForm": "HuBERT", "offsetStart": 110, "offsetEnd": 116}, "context": "We examine the representational spaces of three kinds of stateof-the-art self-supervised models: wav2vec 2.0, HuBERT and contrastive predictive coding (CPC), and compare them with the perceptual spaces of French-speaking and Englishspeaking human listeners, both globally and taking account of the behavioural differences between the two language groups.", "mentionContextAttributes": {"used": {"value": true, "score": 0.8120762705802917}, "created": {"value": false, "score": 0.00018769502639770508}, "shared": {"value": false, "score": 4.76837158203125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999879598617554}, "created": {"value": false, "score": 0.006211221218109131}, "shared": {"value": false, "score": 7.152557373046875e-07}}, "references": [{"label": "(Hsu et al., 2021a,b", "normalizedForm": "(Hsu et al., 2021a,b", "refKey": 0}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "DeepSpeech", "normalizedForm": "DeepSpeech", "offsetStart": 110, "offsetEnd": 120}, "context": "This is in contrast with the overall prediction performance reported above, where wav2vec 2.0 was on par with DeepSpeech, DeepSpeech generally shows a relative advantage for predicting the behaviour of listeners whose native language is the same as the training language, while wav2vec 2.0 does not.", "mentionContextAttributes": {"used": {"value": true, "score": 0.6130122542381287}, "created": {"value": false, "score": 3.075599670410156e-05}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.999743640422821}, "created": {"value": true, "score": 0.9944478273391724}, "shared": {"value": false, "score": 8.344650268554688e-07}}, "references": [{"label": "(Amodei et al., 2016)", "normalizedForm": "Amodei et al., 2016", "refKey": 1}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "eSpeakNG", "normalizedForm": "eSpeakNG", "offsetStart": 112, "offsetEnd": 120}, "context": "To train DeepSpeech as a phone recognizer, the text transcriptions included in CommonVoice are phonemized using eSpeakNG. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.01383960247039795}, "created": {"value": false, "score": 2.47955322265625e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": false, "score": 0.01383960247039795}, "created": {"value": false, "score": 2.47955322265625e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "HuBERT", "normalizedForm": "HuBERT", "offsetStart": 114, "offsetEnd": 120}, "context": "As in Lakhotia et al. (2021), we test state-of-theart self-supervised models: wav2vec 2.0 (Baevski et al., 2020), HuBERT (Hsu et al., 2021a,b) and a CPC model (Rivi\u00e8re and Dupoux, 2021).", "mentionContextAttributes": {"used": {"value": false, "score": 0.18109846115112305}, "created": {"value": false, "score": 7.915496826171875e-05}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999879598617554}, "created": {"value": false, "score": 0.006211221218109131}, "shared": {"value": false, "score": 7.152557373046875e-07}}, "references": [{"label": "(Hsu et al., 2021a,b", "normalizedForm": "(Hsu et al., 2021a,b", "refKey": 0, "offsetStart": 4900, "offsetEnd": 4920}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "DeepSpeech", "normalizedForm": "DeepSpeech", "offsetStart": 122, "offsetEnd": 132}, "context": "This is in contrast with the overall prediction performance reported above, where wav2vec 2.0 was on par with DeepSpeech, DeepSpeech generally shows a relative advantage for predicting the behaviour of listeners whose native language is the same as the training language, while wav2vec 2.0 does not.", "mentionContextAttributes": {"used": {"value": true, "score": 0.6130106449127197}, "created": {"value": false, "score": 3.075599670410156e-05}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.999743640422821}, "created": {"value": true, "score": 0.9944478273391724}, "shared": {"value": false, "score": 8.344650268554688e-07}}, "references": [{"label": "(Amodei et al., 2016)", "normalizedForm": "Amodei et al., 2016", "refKey": 1}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Speech", "normalizedForm": "Speech", "offsetStart": 136, "offsetEnd": 142}, "context": "The five datasets use different kinds of stimulus triplets, including short three-phone extracts cut from running speech (Zero Resource Speech Challenge 2017 and Pilot July 2018 datasets), as well as readspeech nonwords, which highlight English consonants and vowels (Pilot August 2018), compare English with French vowels in a crosslinguistic task (Cogsci-2019), or highlight vowel contrasts in a variety of languages (WorldVowels).", "mentionContextAttributes": {"used": {"value": true, "score": 0.978701114654541}, "created": {"value": false, "score": 4.76837158203125e-06}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999449253082275}, "created": {"value": false, "score": 0.006211221218109131}, "shared": {"value": false, "score": 5.960464477539062e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Speech", "normalizedForm": "Speech", "offsetStart": 173, "offsetEnd": 179}, "context": "We choose to test three state-of-the-art self-supervised models: contrastive predictive coding (CPC), the basis for the current best-performing systems on the Zero Resource Speech Challenge evaluation (Dunbar et al., 2021); wav2vec 2.0; and a HuBERT model.", "mentionContextAttributes": {"used": {"value": false, "score": 0.15421491861343384}, "created": {"value": false, "score": 0.006211221218109131}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999449253082275}, "created": {"value": false, "score": 0.006211221218109131}, "shared": {"value": false, "score": 5.960464477539062e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "DeepSpeech", "normalizedForm": "DeepSpeech", "offsetStart": 175, "offsetEnd": 185}, "context": "To illustrate the comparisons at the level of phone contrasts, in Figure 3 we plot the average accuracy (per contrast) for French-speaking participants results against (left) DeepSpeech trained on French, one of the best-performing models, and (right) wav2vec 2.0 trained on AudioSet (aud-w2v), one of the models that is the least similar to humans.", "mentionContextAttributes": {"used": {"value": true, "score": 0.999743640422821}, "created": {"value": false, "score": 1.6689300537109375e-06}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.999743640422821}, "created": {"value": true, "score": 0.9944478273391724}, "shared": {"value": false, "score": 8.344650268554688e-07}}, "references": [{"label": "(Amodei et al., 2016)", "normalizedForm": "Amodei et al., 2016", "refKey": 1}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Speech", "normalizedForm": "Speech", "offsetStart": 190, "offsetEnd": 196}, "context": "There is a striking difference between languages in the performance of DeepSpeech: for English, the native DeepSpeech shows a substantial advantage over the non-native (French-trained) Deep-Speech which is not present for the French datasets.", "mentionContextAttributes": {"used": {"value": false, "score": 0.00041109323501586914}, "created": {"value": false, "score": 0.00025326013565063477}, "shared": {"value": false, "score": 5.960464477539062e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999449253082275}, "created": {"value": false, "score": 0.006211221218109131}, "shared": {"value": false, "score": 5.960464477539062e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Speech", "normalizedForm": "Speech", "offsetStart": 203, "offsetEnd": 209}, "context": "For interpretability, we calculate scores only on the subsets of Perceptimatic containing monolingual English and French stimuli which were presented to listeners in their native language (Zero Resource Speech Challenge 2017, WorldVowelsn and Pilot August).", "mentionContextAttributes": {"used": {"value": true, "score": 0.9999449253082275}, "created": {"value": false, "score": 5.364418029785156e-06}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999449253082275}, "created": {"value": false, "score": 0.006211221218109131}, "shared": {"value": false, "score": 5.960464477539062e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "DeepSpeech", "normalizedForm": "DeepSpeech", "offsetStart": 237, "offsetEnd": 247}, "context": "However, these two major differences may be in part explained by global effects: the French-trained HuBERT model is better at predicting the results for all participants (not just French-speaking participants), as is the English-trained DeepSpeech model.", "mentionContextAttributes": {"used": {"value": false, "score": 0.003963649272918701}, "created": {"value": false, "score": 0.00014925003051757812}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.999743640422821}, "created": {"value": true, "score": 0.9944478273391724}, "shared": {"value": false, "score": 8.344650268554688e-07}}, "references": [{"label": "(Amodei et al., 2016)", "normalizedForm": "Amodei et al., 2016", "refKey": 1}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "HuBERT", "normalizedForm": "HuBERT", "offsetStart": 243, "offsetEnd": 249}, "context": "We choose to test three state-of-the-art self-supervised models: contrastive predictive coding (CPC), the basis for the current best-performing systems on the Zero Resource Speech Challenge evaluation (Dunbar et al., 2021); wav2vec 2.0; and a HuBERT model.", "mentionContextAttributes": {"used": {"value": false, "score": 0.15421491861343384}, "created": {"value": false, "score": 0.006211221218109131}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999879598617554}, "created": {"value": false, "score": 0.006211221218109131}, "shared": {"value": false, "score": 7.152557373046875e-07}}, "references": [{"label": "(Hsu et al., 2021a,b", "normalizedForm": "(Hsu et al., 2021a,b", "refKey": 0}]}], "references": [{"refKey": 0, "tei": "<biblStruct xml:id=\"b0\">\n\t<monogr>\n\t\t<title level=\"m\" type=\"main\">Do self-supervised speech models develop human-like perception biases?</title>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Juliette</forename><forename type=\"middle\">Millet</forename><surname>Coml</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Ewan</forename><surname>Dunbar</surname></persName>\n\t\t</author>\n\t\t<idno type=\"DOI\">10.18653/v1/2022.acl-long.523</idno>\n\t\t<idno>97EA3C4333BE71F8C085928050CC2641</idno>\n\t\t<imprint>\n\t\t\t<date></date>\n\t\t</imprint>\n\t</monogr>\n</biblStruct>\n"}, {"refKey": 1, "tei": "<biblStruct xml:id=\"b1\">\n\t<analytic>\n\t\t<title level=\"a\" type=\"main\">Deep speech 2: End-to-end speech recognition in english and mandarin</title>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Dario</forename><surname>Amodei</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Rishita</forename><surname>Sundaram Ananthanarayanan</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Jingliang</forename><surname>Anubhai</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Eric</forename><surname>Bai</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Carl</forename><surname>Battenberg</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Jared</forename><surname>Case</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Bryan</forename><surname>Casper</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Qiang</forename><surname>Catanzaro</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Guoliang</forename><surname>Cheng</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><surname>Chen</surname></persName>\n\t\t</author>\n\t</analytic>\n\t<monogr>\n\t\t<title level=\"m\">ternational conference on machine learning</title>\n\t\t<imprint>\n\t\t\t<date>2016</date>\n\t\t\t<biblScope unit=\"page\">182</biblScope>\n\t\t</imprint>\n\t</monogr>\n</biblStruct>\n"}], "runtime": 9765, "id": "50c92b5708bdcb520c24323a7458893a62fdfb6b", "metadata": {"id": "50c92b5708bdcb520c24323a7458893a62fdfb6b"}, "original_file_path": "../../datalake/Samuel/SOFTware-Sync/downloads/xml/hal-03697420.grobid.tei.xml", "file_name": "hal-03697420.grobid.tei.xml"}