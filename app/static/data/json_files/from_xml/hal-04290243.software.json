{"application": "software-mentions", "version": "0.8.0", "date": "2024-10-07T11:45+0000", "md5": "B3B0BE8B54138ECE8A08C2F9EC0942CE", "mentions": [{"type": "software", "software-type": "software", "software-name": {"rawForm": "RoBERTa", "normalizedForm": "RoBERTa", "offsetStart": 0, "offsetEnd": 7}, "context": "RoBERTa fine-tuned on MNLI dataset performs better than chance for WEP-UNLI. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.5484113097190857}, "created": {"value": false, "score": 2.9802322387695312e-06}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.999976634979248}, "created": {"value": true, "score": 0.6704810261726379}, "shared": {"value": false, "score": 5.960464477539062e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "RoBERTa", "normalizedForm": "RoBERTa", "offsetStart": 0, "offsetEnd": 7}, "context": "RoBERTa We also use the pretrained RoBERTa base model with 123M parameters (Liu et al., 2019) to score the masked language modeling likelihood of the premise/hypothesis pair.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9992094039916992}, "created": {"value": false, "score": 2.86102294921875e-06}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.999976634979248}, "created": {"value": true, "score": 0.6704810261726379}, "shared": {"value": false, "score": 5.960464477539062e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "RoBERTa", "normalizedForm": "RoBERTa", "offsetStart": 13, "offsetEnd": 20}, "context": "We fine-tune RoBERTa on the MNLI entailment detection dataset (Williams et al., 2018) with standard hyperparameters (see the following subsection).", "mentionContextAttributes": {"used": {"value": true, "score": 0.8480170369148254}, "created": {"value": false, "score": 1.537799835205078e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.999976634979248}, "created": {"value": true, "score": 0.6704810261726379}, "shared": {"value": false, "score": 5.960464477539062e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "RoBERTa", "normalizedForm": "RoBERTa", "offsetStart": 13, "offsetEnd": 20}, "context": "We fine-tune RoBERTa-base models on our datasets, using standard (Mosbach et al., 2021) hyperparameters3 (3 epochs, sequence length of 256, learning rate of 2.10 -5 batch size of 16.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9842119812965393}, "created": {"value": false, "score": 2.086162567138672e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.999976634979248}, "created": {"value": true, "score": 0.6704810261726379}, "shared": {"value": false, "score": 5.960464477539062e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "ProbLog", "normalizedForm": "ProbLog", "offsetStart": 15, "offsetEnd": 46}, "context": "We rely on the ProbLog (De Raedt et al., 2007) reasoner which implements Dantsin (1992) semantics.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9951338171958923}, "created": {"value": false, "score": 0.009208440780639648}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9951338171958923}, "created": {"value": false, "score": 0.009208440780639648}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "references": [{"label": "(De Raedt et al., 2007)", "normalizedForm": "De Raedt et al., 2007", "refKey": 5}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "ProbLog", "normalizedForm": "ProbLog", "offsetStart": 22, "offsetEnd": 29}, "context": "They also rely on the ProbLog solver (De Raedt et al., 2007), but focus on numeric probability problems. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.20077162981033325}, "created": {"value": false, "score": 0.00023597478866577148}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9951338171958923}, "created": {"value": false, "score": 0.009208440780639648}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "references": [{"label": "(De Raedt et al., 2007)", "normalizedForm": "De Raedt et al., 2007", "refKey": 5, "offsetStart": 5365, "offsetEnd": 5388}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "RoBERTa", "normalizedForm": "RoBERTa", "offsetStart": 35, "offsetEnd": 42}, "context": "RoBERTa We also use the pretrained RoBERTa base model with 123M parameters (Liu et al., 2019) to score the masked language modeling likelihood of the premise/hypothesis pair.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9992094039916992}, "created": {"value": false, "score": 2.86102294921875e-06}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.999976634979248}, "created": {"value": true, "score": 0.6704810261726379}, "shared": {"value": false, "score": 5.960464477539062e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "RoBERTa", "normalizedForm": "RoBERTa", "offsetStart": 70, "offsetEnd": 77}, "context": "We use length-normalization with GPT2 likelihood and calibration with RoBERTa likelihood as they worked best on the validation sets.).", "mentionContextAttributes": {"used": {"value": true, "score": 0.999976634979248}, "created": {"value": false, "score": 1.7881393432617188e-06}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.999976634979248}, "created": {"value": true, "score": 0.6704810261726379}, "shared": {"value": false, "score": 5.960464477539062e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "RoBERTa", "normalizedForm": "RoBERTa", "offsetStart": 164, "offsetEnd": 171}, "context": "Our contributions are as follows: (i) two datasets and methods to measure understanding of WEP; and (ii) evaluation of the ability of neural language models (GPT2, RoBERTa-trained on MNLI) to tackle WEP-related problems, showing that offthe-shelf models are very little influenced by them, even though fine-tuning on our constructed datasets quickly leads to high accuracies.", "mentionContextAttributes": {"used": {"value": false, "score": 0.013638734817504883}, "created": {"value": true, "score": 0.6704810261726379}, "shared": {"value": false, "score": 5.960464477539062e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.999976634979248}, "created": {"value": true, "score": 0.6704810261726379}, "shared": {"value": false, "score": 5.960464477539062e-07}}}], "references": [{"refKey": 5, "tei": "<biblStruct xml:id=\"b5\">\n\t<analytic>\n\t\t<title level=\"a\" type=\"main\">Problog: A probabilistic prolog and its application in link discovery</title>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Luc</forename><surname>De Raedt</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Angelika</forename><surname>Kimmig</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Hannu</forename><surname>Toivonen</surname></persName>\n\t\t</author>\n\t</analytic>\n\t<monogr>\n\t\t<title level=\"m\">IJCAI</title>\n\t\t<imprint>\n\t\t\t<date>2007</date>\n\t\t\t<biblScope unit=\"volume\">7</biblScope>\n\t\t\t<biblScope unit=\"page\">2467</biblScope>\n\t\t</imprint>\n\t</monogr>\n</biblStruct>\n"}], "runtime": 19132, "id": "bbff9ebdb8f8cd808dbb7f6c5e5950ec94ca253c", "metadata": {"id": "bbff9ebdb8f8cd808dbb7f6c5e5950ec94ca253c"}, "original_file_path": "../../datalake/Samuel/SOFTware-Sync/downloads/xml/hal-04290243.grobid.tei.xml", "file_name": "hal-04290243.grobid.tei.xml"}