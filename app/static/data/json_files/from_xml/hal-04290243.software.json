{"application": "software-mentions", "version": "0.8.0", "date": "2024-04-12T15:40+0000", "md5": "2EFEC18CF6E94674FFAEA0C73FB08981", "mentions": [{"type": "software", "software-type": "software", "software-name": {"rawForm": "RoBERTa", "normalizedForm": "RoBERTa", "offsetStart": 0, "offsetEnd": 7}, "context": "RoBERTa fine-tuned on MNLI dataset performs better than chance for WEP-UNLI. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.5484075546264648}, "created": {"value": false, "score": 3.039836883544922e-06}, "shared": {"value": false, "score": 1.7881393432617188e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999765753746033}, "created": {"value": true, "score": 0.6704790592193604}, "shared": {"value": false, "score": 5.960464477539062e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "RoBERTa", "normalizedForm": "RoBERTa", "offsetStart": 0, "offsetEnd": 7}, "context": "RoBERTa We also use the pretrained RoBERTa base model with 123M parameters (Liu et al., 2019) to score the masked language modeling likelihood of the premise/hypothesis pair.", "mentionContextAttributes": {"used": {"value": true, "score": 0.999209463596344}, "created": {"value": false, "score": 2.9206275939941406e-06}, "shared": {"value": false, "score": 1.7881393432617188e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999765753746033}, "created": {"value": true, "score": 0.6704790592193604}, "shared": {"value": false, "score": 5.960464477539062e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "RoBERTa", "normalizedForm": "RoBERTa", "offsetStart": 13, "offsetEnd": 20}, "context": "We fine-tune RoBERTa on the MNLI entailment detection dataset (Williams et al., 2018) with standard hyperparameters (see the following subsection).", "mentionContextAttributes": {"used": {"value": true, "score": 0.8480190634727478}, "created": {"value": false, "score": 1.5437602996826172e-05}, "shared": {"value": false, "score": 2.980232238769531e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999765753746033}, "created": {"value": true, "score": 0.6704790592193604}, "shared": {"value": false, "score": 5.960464477539062e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "RoBERTa", "normalizedForm": "RoBERTa", "offsetStart": 13, "offsetEnd": 20}, "context": "We fine-tune RoBERTa-base models on our datasets, using standard (Mosbach et al., 2021) hyperparameters3 (3 epochs, sequence length of 256, learning rate of 2.10 -5 batch size of 16.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9842117428779602}, "created": {"value": false, "score": 2.092123031616211e-05}, "shared": {"value": false, "score": 1.7881393432617188e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999765753746033}, "created": {"value": true, "score": 0.6704790592193604}, "shared": {"value": false, "score": 5.960464477539062e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "ProbLog", "normalizedForm": "ProbLog", "offsetStart": 15, "offsetEnd": 46}, "context": "We rely on the ProbLog (De Raedt et al., 2007) reasoner which implements Dantsin (1992) semantics.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9951336979866028}, "created": {"value": false, "score": 0.009208321571350098}, "shared": {"value": false, "score": 2.980232238769531e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9951336979866028}, "created": {"value": false, "score": 0.009208321571350098}, "shared": {"value": false, "score": 2.980232238769531e-07}}, "references": [{"label": "(De Raedt et al., 2007)", "normalizedForm": "De Raedt et al., 2007", "refKey": 5}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "ProbLog", "normalizedForm": "ProbLog", "offsetStart": 22, "offsetEnd": 29}, "context": "They also rely on the ProbLog solver (De Raedt et al., 2007), but focus on numeric probability problems. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.2007707953453064}, "created": {"value": false, "score": 0.00023603439331054688}, "shared": {"value": false, "score": 2.980232238769531e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9951336979866028}, "created": {"value": false, "score": 0.009208321571350098}, "shared": {"value": false, "score": 2.980232238769531e-07}}, "references": [{"label": "(De Raedt et al., 2007)", "normalizedForm": "De Raedt et al., 2007", "refKey": 5, "offsetStart": 5365, "offsetEnd": 5388}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "RoBERTa", "normalizedForm": "RoBERTa", "offsetStart": 35, "offsetEnd": 42}, "context": "RoBERTa We also use the pretrained RoBERTa base model with 123M parameters (Liu et al., 2019) to score the masked language modeling likelihood of the premise/hypothesis pair.", "mentionContextAttributes": {"used": {"value": true, "score": 0.999209463596344}, "created": {"value": false, "score": 2.9206275939941406e-06}, "shared": {"value": false, "score": 1.7881393432617188e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999765753746033}, "created": {"value": true, "score": 0.6704790592193604}, "shared": {"value": false, "score": 5.960464477539062e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "RoBERTa", "normalizedForm": "RoBERTa", "offsetStart": 70, "offsetEnd": 77}, "context": "We use length-normalization with GPT2 likelihood and calibration with RoBERTa likelihood as they worked best on the validation sets.).", "mentionContextAttributes": {"used": {"value": true, "score": 0.9999765753746033}, "created": {"value": false, "score": 1.8477439880371094e-06}, "shared": {"value": false, "score": 1.7881393432617188e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999765753746033}, "created": {"value": true, "score": 0.6704790592193604}, "shared": {"value": false, "score": 5.960464477539062e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "RoBERTa", "normalizedForm": "RoBERTa", "offsetStart": 164, "offsetEnd": 171}, "context": "Our contributions are as follows: (i) two datasets and methods to measure understanding of WEP; and (ii) evaluation of the ability of neural language models (GPT2, RoBERTa-trained on MNLI) to tackle WEP-related problems, showing that offthe-shelf models are very little influenced by them, even though fine-tuning on our constructed datasets quickly leads to high accuracies.", "mentionContextAttributes": {"used": {"value": false, "score": 0.013638496398925781}, "created": {"value": true, "score": 0.6704790592193604}, "shared": {"value": false, "score": 5.960464477539062e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999765753746033}, "created": {"value": true, "score": 0.6704790592193604}, "shared": {"value": false, "score": 5.960464477539062e-07}}}], "references": [{"refKey": 5, "tei": "<biblStruct xml:id=\"b5\">\n\t<analytic>\n\t\t<title level=\"a\" type=\"main\">Problog: A probabilistic prolog and its application in link discovery</title>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Luc</forename><surname>De Raedt</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Angelika</forename><surname>Kimmig</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Hannu</forename><surname>Toivonen</surname></persName>\n\t\t</author>\n\t</analytic>\n\t<monogr>\n\t\t<title level=\"m\">IJCAI</title>\n\t\t<imprint>\n\t\t\t<date>2007</date>\n\t\t\t<biblScope unit=\"volume\">7</biblScope>\n\t\t\t<biblScope unit=\"page\">2467</biblScope>\n\t\t</imprint>\n\t</monogr>\n</biblStruct>\n"}], "runtime": 50355, "id": "16b309a0b3bbd8e9788eac42921e86d6eb20518c", "metadata": {"id": "16b309a0b3bbd8e9788eac42921e86d6eb20518c"}, "original_file_path": "../../datalake/Samuel/SOFTware-Sync/data/xml_files/hal-04290243.grobid.tei.xml", "file_name": "hal-04290243.grobid.tei.xml"}