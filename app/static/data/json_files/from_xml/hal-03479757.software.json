{"application": "software-mentions", "version": "0.8.0", "date": "2024-10-07T12:46+0000", "md5": "A055C3FD70E3BDFD8A13969837DFE625", "mentions": [{"type": "software", "software-type": "software", "software-name": {"rawForm": "mRASP", "normalizedForm": "mRASP", "offsetStart": 0, "offsetEnd": 5}, "context": "mRASP uses unsupervised word alignments generated by MUSE (Conneau et al. 2018) to perform random substitutions of words with their translations in another language, with the aim of bringing words with similar meanings across multiple languages closer in the representation space. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.0002447962760925293}, "created": {"value": false, "score": 5.5909156799316406e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": false, "score": 0.007277429103851318}, "created": {"value": false, "score": 0.049027204513549805}, "shared": {"value": false, "score": 4.76837158203125e-07}}, "references": [{"label": "(Lin et al. 2020", "normalizedForm": "(Lin et al. 2020", "refKey": 178}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "mRASP", "normalizedForm": "mRASP", "offsetStart": 0, "offsetEnd": 5}, "context": "mRASP2 (Pan et al. 2021) extends this work by incorporating monolingual data into the training.", "mentionContextAttributes": {"used": {"value": false, "score": 0.007277429103851318}, "created": {"value": false, "score": 0.049027204513549805}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": false, "score": 0.007277429103851318}, "created": {"value": false, "score": 0.049027204513549805}, "shared": {"value": false, "score": 4.76837158203125e-07}}, "references": [{"label": "(Pan et al. 2021", "normalizedForm": "(Pan et al. 2021", "refKey": 218, "offsetStart": 53995, "offsetEnd": 54011}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "mRASP", "normalizedForm": "mRASP", "offsetStart": 25, "offsetEnd": 30}, "context": "Wu et al. (2020) use the mRASP approach: a universal multilingual model involving language data for English to and from Pashto, Khmer, Tamil, Inuktitut, German and Polish, which is then fine-tuned to the individual low-resource language pairs.", "mentionContextAttributes": {"used": {"value": false, "score": 0.005662024021148682}, "created": {"value": false, "score": 0.0002703070640563965}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "documentContextAttributes": {"used": {"value": false, "score": 0.007277429103851318}, "created": {"value": false, "score": 0.049027204513549805}, "shared": {"value": false, "score": 4.76837158203125e-07}}, "references": [{"label": "(Lin et al. 2020", "normalizedForm": "(Lin et al. 2020", "refKey": 178}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "scripts", "normalizedForm": "scripts", "offsetStart": 42, "offsetEnd": 49}, "context": "Still, if languages are unrelated and the scripts are different, for example transferring from an Arabic-Russian parent to Estonian-English, transfer learning is less useful.", "mentionContextAttributes": {"used": {"value": false, "score": 0.0001518726348876953}, "created": {"value": false, "score": 0.00019174814224243164}, "shared": {"value": false, "score": 1.5497207641601562e-06}}, "documentContextAttributes": {"used": {"value": false, "score": 0.1388998031616211}, "created": {"value": false, "score": 0.000765681266784668}, "shared": {"value": false, "score": 2.0265579223632812e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "mRASP", "normalizedForm": "mRASP", "offsetStart": 48, "offsetEnd": 53}, "context": "A recent multilingual pre-trained method called mRASP (Lin et al. 2020) has shown strong performance across a range of MT tasks: medium, low and very low-resource.", "mentionContextAttributes": {"used": {"value": false, "score": 0.0001571178436279297}, "created": {"value": false, "score": 0.0015473365783691406}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": false, "score": 0.007277429103851318}, "created": {"value": false, "score": 0.049027204513549805}, "shared": {"value": false, "score": 4.76837158203125e-07}}, "references": [{"label": "(Lin et al. 2020", "normalizedForm": "(Lin et al. 2020", "refKey": 178, "offsetStart": 53488, "offsetEnd": 53504}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "MUSE", "normalizedForm": "MUSE", "offsetStart": 53, "offsetEnd": 79}, "context": "mRASP uses unsupervised word alignments generated by MUSE (Conneau et al. 2018) to perform random substitutions of words with their translations in another language, with the aim of bringing words with similar meanings across multiple languages closer in the representation space. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.0002447962760925293}, "created": {"value": false, "score": 5.5909156799316406e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": false, "score": 0.0002447962760925293}, "created": {"value": false, "score": 5.5909156799316406e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "scripts", "normalizedForm": "scripts", "offsetStart": 63, "offsetEnd": 70}, "context": "Even though the BPE toolkit is not compatible with the Abugida scripts used for Gujarati, Tamil and Khmer (in these scripts, two unicode codepoints can be used to represent one glyph), we only found one group who modified BPE to take this into account (Shi et al. 2020). ", "mentionContextAttributes": {"used": {"value": false, "score": 0.1388998031616211}, "created": {"value": false, "score": 0.00019603967666625977}, "shared": {"value": false, "score": 1.0728836059570312e-06}}, "documentContextAttributes": {"used": {"value": false, "score": 0.1388998031616211}, "created": {"value": false, "score": 0.000765681266784668}, "shared": {"value": false, "score": 2.0265579223632812e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "MIXER", "normalizedForm": "MIXER", "offsetStart": 65, "offsetEnd": 70}, "context": "explore multiple training strategies and propose a method called MIXER which is a variation of the REINFORCE algorithm (Williams 1992;Zaremba and Sutskever 2015). ", "mentionContextAttributes": {"used": {"value": false, "score": 4.2319297790527344e-05}, "created": {"value": true, "score": 0.9684934616088867}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": false, "score": 4.2319297790527344e-05}, "created": {"value": true, "score": 0.9684934616088867}, "shared": {"value": false, "score": 1.1920928955078125e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "scripts", "normalizedForm": "scripts", "offsetStart": 77, "offsetEnd": 84}, "context": "In cases where the languages are highly related but are written in different scripts, transliteration may be used to increase the overlap in terms of the surface forms (Dabre et al. 2018;Goyal, Kumar, and Sharma 2020).", "mentionContextAttributes": {"used": {"value": false, "score": 0.0014790892601013184}, "created": {"value": false, "score": 5.1021575927734375e-05}, "shared": {"value": false, "score": 2.0265579223632812e-06}}, "documentContextAttributes": {"used": {"value": false, "score": 0.1388998031616211}, "created": {"value": false, "score": 0.000765681266784668}, "shared": {"value": false, "score": 2.0265579223632812e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "mRASP", "normalizedForm": "mRASP", "offsetStart": 91, "offsetEnd": 96}, "context": "The most recent work on using lexicons in pretrained multilingual models (Lin et al. 2020, mRASP) shows the most promise.", "mentionContextAttributes": {"used": {"value": false, "score": 7.68899917602539e-05}, "created": {"value": false, "score": 0.011662542819976807}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": false, "score": 0.007277429103851318}, "created": {"value": false, "score": 0.049027204513549805}, "shared": {"value": false, "score": 4.76837158203125e-07}}, "references": [{"label": "(Lin et al. 2020", "normalizedForm": "(Lin et al. 2020", "refKey": 178}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "mRASP", "normalizedForm": "mRASP", "offsetStart": 92, "offsetEnd": 97}, "context": "The striking success of multilingual pre-trained models such as mBART (Liu et al. 2020) and mRASP (Pan et al. 2021) still needs further investigation, and massively multilingual models clearly confer advantage to both highand low-resource pairs (Tran et al. 2021;Yang et al. 2021).", "mentionContextAttributes": {"used": {"value": false, "score": 0.0004774332046508789}, "created": {"value": false, "score": 7.49826431274414e-05}, "shared": {"value": false, "score": 4.76837158203125e-07}}, "documentContextAttributes": {"used": {"value": false, "score": 0.007277429103851318}, "created": {"value": false, "score": 0.049027204513549805}, "shared": {"value": false, "score": 4.76837158203125e-07}}, "references": [{"label": "(Pan et al. 2021", "normalizedForm": "(Pan et al. 2021", "refKey": 218, "offsetStart": 129575, "offsetEnd": 129591}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "scripts", "normalizedForm": "scripts", "offsetStart": 93, "offsetEnd": 100}, "context": "Although few systematic studies are reported, one hypothesis could be that even if different scripts are used there is no disadvantage to sharing segmentation; it could help with named entities and therefore reducing the overall vocabulary size of the model (Ding, Renduchintala, and Duh 2019).", "mentionContextAttributes": {"used": {"value": false, "score": 0.0002669692039489746}, "created": {"value": false, "score": 9.775161743164062e-05}, "shared": {"value": false, "score": 4.76837158203125e-07}}, "documentContextAttributes": {"used": {"value": false, "score": 0.1388998031616211}, "created": {"value": false, "score": 0.000765681266784668}, "shared": {"value": false, "score": 2.0265579223632812e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "scripts", "normalizedForm": "scripts", "offsetStart": 110, "offsetEnd": 117}, "context": "They find greater difficulties for languages which are more distant from those in mBERT and/or have different scripts -but the latter problem can be mitigated with careful transliteration.", "mentionContextAttributes": {"used": {"value": false, "score": 0.0001518726348876953}, "created": {"value": false, "score": 0.0003821849822998047}, "shared": {"value": false, "score": 5.960464477539062e-07}}, "documentContextAttributes": {"used": {"value": false, "score": 0.1388998031616211}, "created": {"value": false, "score": 0.000765681266784668}, "shared": {"value": false, "score": 2.0265579223632812e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "scripts", "normalizedForm": "scripts", "offsetStart": 116, "offsetEnd": 123}, "context": "Even though the BPE toolkit is not compatible with the Abugida scripts used for Gujarati, Tamil and Khmer (in these scripts, two unicode codepoints can be used to represent one glyph), we only found one group who modified BPE to take this into account (Shi et al. 2020). ", "mentionContextAttributes": {"used": {"value": false, "score": 0.1388998031616211}, "created": {"value": false, "score": 0.00019603967666625977}, "shared": {"value": false, "score": 1.0728836059570312e-06}}, "documentContextAttributes": {"used": {"value": false, "score": 0.1388998031616211}, "created": {"value": false, "score": 0.000765681266784668}, "shared": {"value": false, "score": 2.0265579223632812e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "scripts", "normalizedForm": "scripts", "offsetStart": 150, "offsetEnd": 157}, "context": "Transliteration and alphabet mapping has been principally used in the context of exploiting data from related languages that are written in different scripts.", "mentionContextAttributes": {"used": {"value": false, "score": 0.0006731748580932617}, "created": {"value": false, "score": 0.000765681266784668}, "shared": {"value": false, "score": 7.152557373046875e-07}}, "documentContextAttributes": {"used": {"value": false, "score": 0.1388998031616211}, "created": {"value": false, "score": 0.000765681266784668}, "shared": {"value": false, "score": 2.0265579223632812e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "scripts", "normalizedForm": "scripts", "offsetStart": 229, "offsetEnd": 236}, "context": "Among the different possible pre-processing steps, we review participants choices concerning tokenisation, subword segmentation and transliteration/alphabet mapping (relevant when translating between languages that use different scripts).", "mentionContextAttributes": {"used": {"value": false, "score": 0.012418031692504883}, "created": {"value": false, "score": 4.112720489501953e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": false, "score": 0.1388998031616211}, "created": {"value": false, "score": 0.000765681266784668}, "shared": {"value": false, "score": 2.0265579223632812e-06}}}], "references": [{"refKey": 178, "tei": "<biblStruct xml:id=\"b178\">\n\t<analytic>\n\t\t<title level=\"a\" type=\"main\">Pre-training multilingual neural machine translation by leveraging alignment information</title>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Zehui</forename><surname>Lin</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Xiao</forename><surname>Pan</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Mingxuan</forename><surname>Wang</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Xipeng</forename><surname>Qiu</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Jiangtao</forename><surname>Feng</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Hao</forename><surname>Zhou</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Lei</forename><surname>Li</surname></persName>\n\t\t</author>\n\t</analytic>\n\t<monogr>\n\t\t<title level=\"m\">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>\n\t\t<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>\n\t\t<imprint>\n\t\t\t<publisher>Association for Computational Linguistics</publisher>\n\t\t\t<date>2020</date>\n\t\t\t<biblScope unit=\"page\">2663</biblScope>\n\t\t</imprint>\n\t</monogr>\n</biblStruct>\n"}, {"refKey": 218, "tei": "<biblStruct xml:id=\"b218\">\n\t<analytic>\n\t\t<title level=\"a\" type=\"main\">Contrastive learning for many-to-many multilingual neural machine translation</title>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Xiao</forename><surname>Pan</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Mingxuan</forename><surname>Wang</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Liwei</forename><surname>Wu</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Lei</forename><surname>Li</surname></persName>\n\t\t</author>\n\t</analytic>\n\t<monogr>\n\t\t<title level=\"m\">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>\n\t\t<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</meeting>\n\t\t<imprint>\n\t\t\t<publisher>Association for Computational Linguistics</publisher>\n\t\t\t<date>2021</date>\n\t\t\t<biblScope unit=\"volume\">1</biblScope>\n\t\t\t<biblScope unit=\"page\">258</biblScope>\n\t\t</imprint>\n\t</monogr>\n</biblStruct>\n"}], "runtime": 30766, "id": "d8db85227713a30a608eaaf42cbcf644d1382e70", "metadata": {"id": "d8db85227713a30a608eaaf42cbcf644d1382e70"}, "original_file_path": "../../datalake/Samuel/SOFTware-Sync/downloads/xml/hal-03479757.grobid.tei.xml", "file_name": "hal-03479757.grobid.tei.xml"}