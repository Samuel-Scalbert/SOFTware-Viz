{"application": "software-mentions", "version": "0.8.0", "date": "2024-04-12T17:21+0000", "md5": "B8678282E93FB274C9CA3BDBE778BAC2", "mentions": [{"type": "software", "software-type": "software", "software-name": {"rawForm": "PyTorch", "normalizedForm": "PyTorch", "offsetStart": 4, "offsetEnd": 11}, "version": {"rawForm": "1.10", "normalizedForm": "1.10", "offsetStart": 52, "offsetEnd": 56}, "context": "The PyTorch framework (Paszke et al., 2019) version 1.10 was used for implementing the LSTM model with a learning rate selected from 0.05, 0.1, RNN layers 1, 2, dropout 0.1, 0.3, 0.5, and batch size from 8, 16, 32 and a hidden size of 128. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9977232813835144}, "created": {"value": false, "score": 4.708766937255859e-06}, "shared": {"value": false, "score": 2.980232238769531e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9977232813835144}, "created": {"value": false, "score": 1.1265277862548828e-05}, "shared": {"value": false, "score": 2.980232238769531e-07}}, "references": [{"label": "(Paszke et al., 2019)", "normalizedForm": "Paszke et al., 2019", "refKey": 19, "offsetStart": 25148, "offsetEnd": 25169}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Longformer", "normalizedForm": "Longformer", "offsetStart": 4, "offsetEnd": 14}, "context": "For Longformer, BERT and T5 pre-trained models, we use the PyTorch implementation of huggingface (Wolf et al., 2019) version 4.16.2.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9918444156646729}, "created": {"value": false, "score": 1.1265277862548828e-05}, "shared": {"value": false, "score": 2.980232238769531e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999882578849792}, "created": {"value": true, "score": 0.9998221397399902}, "shared": {"value": false, "score": 7.748603820800781e-07}}, "references": [{"label": "(Beltagy et al., 2020)", "normalizedForm": "Beltagy et al., 2020", "refKey": 4}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Scikit-learn", "normalizedForm": "Scikit-learn", "offsetStart": 4, "offsetEnd": 16}, "context": "The Scikit-learn (Pedregosa et al., 2011) framework was employed for the implementation of the Random Forest and SVM models. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.5963938236236572}, "created": {"value": false, "score": 0.00036650896072387695}, "shared": {"value": false, "score": 2.980232238769531e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.5963938236236572}, "created": {"value": false, "score": 0.00036650896072387695}, "shared": {"value": false, "score": 2.980232238769531e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Glove", "normalizedForm": "Glove", "offsetStart": 29, "offsetEnd": 34}, "context": "The T5 fine-tuned model with Glove embeddings shows the best performance with a .73 ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9526016116142273}, "created": {"value": false, "score": 1.4603137969970703e-05}, "shared": {"value": false, "score": 2.980232238769531e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999869465827942}, "created": {"value": false, "score": 3.618001937866211e-05}, "shared": {"value": false, "score": 2.980232238769531e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Karate", "normalizedForm": "Karate", "offsetStart": 40, "offsetEnd": 46}, "context": "For the graph FEATHER-G embeddings, the Karate Club framework (Rozemberczki et al., 2020) was used with the standard hyperparameters. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9991565346717834}, "created": {"value": false, "score": 4.112720489501953e-06}, "shared": {"value": false, "score": 2.980232238769531e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9991565346717834}, "created": {"value": false, "score": 4.112720489501953e-06}, "shared": {"value": false, "score": 2.980232238769531e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "PyTorch", "normalizedForm": "PyTorch", "offsetStart": 59, "offsetEnd": 66}, "version": {"rawForm": "1.10", "normalizedForm": "1.10"}, "context": "For Longformer, BERT and T5 pre-trained models, we use the PyTorch implementation of huggingface (Wolf et al., 2019) version 4.16.2. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9918444156646729}, "created": {"value": false, "score": 1.1265277862548828e-05}, "shared": {"value": false, "score": 2.980232238769531e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9977232813835144}, "created": {"value": false, "score": 1.1265277862548828e-05}, "shared": {"value": false, "score": 2.980232238769531e-07}}, "references": [{"label": "(Paszke et al., 2019)", "normalizedForm": "Paszke et al., 2019", "refKey": 19}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Longformer", "normalizedForm": "Longformer", "offsetStart": 72, "offsetEnd": 82}, "context": "For the rhetoric attribute, we trained the SVM models concatenating the Longformer and FEATHER-G embeddings with the emotion word embedding.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9999364018440247}, "created": {"value": false, "score": 0.0009761452674865723}, "shared": {"value": false, "score": 2.980232238769531e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999882578849792}, "created": {"value": true, "score": 0.9998221397399902}, "shared": {"value": false, "score": 7.748603820800781e-07}}, "references": [{"label": "(Beltagy et al., 2020)", "normalizedForm": "Beltagy et al., 2020", "refKey": 4}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Longformer", "normalizedForm": "Longformer", "offsetStart": 73, "offsetEnd": 83}, "context": "We followed the same process for each annotator with our baseline model (Longformer embedding + SVM).", "mentionContextAttributes": {"used": {"value": true, "score": 0.9999882578849792}, "created": {"value": false, "score": 0.0002770423889160156}, "shared": {"value": false, "score": 1.7881393432617188e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999882578849792}, "created": {"value": true, "score": 0.9998221397399902}, "shared": {"value": false, "score": 7.748603820800781e-07}}, "references": [{"label": "(Beltagy et al., 2020)", "normalizedForm": "Beltagy et al., 2020", "refKey": 4}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "huggingface", "normalizedForm": "huggingface", "offsetStart": 85, "offsetEnd": 116}, "version": {"rawForm": "4.16.2", "normalizedForm": "4.16.2", "offsetStart": 125, "offsetEnd": 131}, "context": "For Longformer, BERT and T5 pre-trained models, we use the PyTorch implementation of huggingface (Wolf et al., 2019) version 4.16.2. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9918444156646729}, "created": {"value": false, "score": 1.1265277862548828e-05}, "shared": {"value": false, "score": 2.980232238769531e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9918444156646729}, "created": {"value": false, "score": 1.1265277862548828e-05}, "shared": {"value": false, "score": 2.980232238769531e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Longformer", "normalizedForm": "Longformer", "offsetStart": 86, "offsetEnd": 96}, "context": "We then train a transfomer-based neural classifier with an attention mechanism called Longformer (Beltagy et al., 2020) empowered with graph embeddings to address the task.", "mentionContextAttributes": {"used": {"value": false, "score": 0.00024175643920898438}, "created": {"value": true, "score": 0.9998221397399902}, "shared": {"value": false, "score": 1.7881393432617188e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999882578849792}, "created": {"value": true, "score": 0.9998221397399902}, "shared": {"value": false, "score": 7.748603820800781e-07}}, "references": [{"label": "(Beltagy et al., 2020)", "normalizedForm": "Beltagy et al., 2020", "refKey": 4, "offsetStart": 3351, "offsetEnd": 3373}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Longformer", "normalizedForm": "Longformer", "offsetStart": 86, "offsetEnd": 96}, "context": "macro F1-score) can be seen when the FEATHER-G graph embeddings are combined with the Longformer embeddings.", "mentionContextAttributes": {"used": {"value": true, "score": 0.939170777797699}, "created": {"value": false, "score": 1.138448715209961e-05}, "shared": {"value": false, "score": 1.7881393432617188e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999882578849792}, "created": {"value": true, "score": 0.9998221397399902}, "shared": {"value": false, "score": 7.748603820800781e-07}}, "references": [{"label": "(Beltagy et al., 2020)", "normalizedForm": "Beltagy et al., 2020", "refKey": 4}]}, {"type": "software", "software-type": "software", "wikidataId": "Q22826110", "wikipediaExternalRef": 49489032, "lang": "en", "confidence": 0.4586, "software-name": {"rawForm": "GloVe", "normalizedForm": "GloVe", "wikidataId": "Q22826110", "wikipediaExternalRef": 49489032, "lang": "en", "confidence": 0.4586, "offsetStart": 96, "offsetEnd": 127}, "context": "For feature generation, we employ different embeddings methods ranging from static methods like GloVe (Pennington et al., 2014) to contextualized embeddings, such as BERT (Devlin et al., 2019) and Longformer (Beltagy et al., 2020). ", "mentionContextAttributes": {"used": {"value": false, "score": 0.0007430315017700195}, "created": {"value": false, "score": 0.002176821231842041}, "shared": {"value": false, "score": 4.172325134277344e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.930587649345398}, "created": {"value": false, "score": 0.002176821231842041}, "shared": {"value": false, "score": 4.172325134277344e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Glove", "normalizedForm": "Glove", "offsetStart": 131, "offsetEnd": 136}, "context": "The latest was obtained by (i) running the fine-tuned T5 model to detect emotions, and (ii) either using that label as an input on Glove, or extracting directly from the model the representation of the labels by summarizing the hidden states of the last four layers in the model. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9999869465827942}, "created": {"value": false, "score": 3.618001937866211e-05}, "shared": {"value": false, "score": 1.7881393432617188e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999869465827942}, "created": {"value": false, "score": 3.618001937866211e-05}, "shared": {"value": false, "score": 2.980232238769531e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Longformer", "normalizedForm": "Longformer", "offsetStart": 160, "offsetEnd": 193}, "context": "Given that joining all these sentences for every argument component results in a document no longer than 2000 tokens per argument, we use the pre-trained model Longformer (Beltagy et al., 2020) which allows us to process documents up to 4096 tokens with stateof-the-art results. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.8810473084449768}, "created": {"value": false, "score": 0.0001156926155090332}, "shared": {"value": false, "score": 2.980232238769531e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999882578849792}, "created": {"value": true, "score": 0.9998221397399902}, "shared": {"value": false, "score": 7.748603820800781e-07}}, "references": [{"label": "(Beltagy et al., 2020)", "normalizedForm": "Beltagy et al., 2020", "refKey": 4}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Longformer", "normalizedForm": "Longformer", "offsetStart": 182, "offsetEnd": 192}, "context": "For Cogency and Reasonableness, Support Vector Machines (SVM) (Chang and Lin, 2011), Random Forests (Cutler et al., 2012), Bidirectional LSTM-CRF (Huang et al., 2015) and fine-tuned Longformer (Beltagy et al., 2020) with an added dense layer for classification models were investigated.", "mentionContextAttributes": {"used": {"value": false, "score": 0.23552966117858887}, "created": {"value": false, "score": 0.000179290771484375}, "shared": {"value": false, "score": 7.748603820800781e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999882578849792}, "created": {"value": true, "score": 0.9998221397399902}, "shared": {"value": false, "score": 7.748603820800781e-07}}, "references": [{"label": "(Beltagy et al., 2020)", "normalizedForm": "Beltagy et al., 2020", "refKey": 4, "offsetStart": 19596, "offsetEnd": 19618}]}, {"type": "software", "software-type": "software", "wikidataId": "Q22826110", "wikipediaExternalRef": 49489032, "lang": "en", "confidence": 0.4586, "software-name": {"rawForm": "GloVe", "normalizedForm": "GloVe", "wikidataId": "Q22826110", "wikipediaExternalRef": 49489032, "lang": "en", "confidence": 0.4586, "offsetStart": 189, "offsetEnd": 194}, "context": "We then obtain a word embedding as a feature vector by either directly extracting the label representation from the fine-tuned model or employing the label to obtain a word embedding using GloVe.", "mentionContextAttributes": {"used": {"value": true, "score": 0.930587649345398}, "created": {"value": false, "score": 5.453824996948242e-05}, "shared": {"value": false, "score": 4.172325134277344e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.930587649345398}, "created": {"value": false, "score": 0.002176821231842041}, "shared": {"value": false, "score": 4.172325134277344e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Longformer", "normalizedForm": "Longformer", "offsetStart": 197, "offsetEnd": 229}, "context": "For feature generation, we employ different embeddings methods ranging from static methods like GloVe (Pennington et al., 2014) to contextualized embeddings, such as BERT (Devlin et al., 2019) and Longformer (Beltagy et al., 2020). ", "mentionContextAttributes": {"used": {"value": false, "score": 0.0007430911064147949}, "created": {"value": false, "score": 0.0021767616271972656}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999882578849792}, "created": {"value": true, "score": 0.9998221397399902}, "shared": {"value": false, "score": 7.748603820800781e-07}}, "references": [{"label": "(Beltagy et al., 2020)", "normalizedForm": "Beltagy et al., 2020", "refKey": 4}]}], "references": [{"refKey": 19, "tei": "<biblStruct xml:id=\"b19\">\n\t<analytic>\n\t\t<title level=\"a\" type=\"main\">Finite frequency control of discrete linear repetitive processes with application in iterative learning control</title>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Adam</forename><surname>Paszke</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Sam</forename><surname>Gross</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Francisco</forename><surname>Massa</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Adam</forename><surname>Lerer</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">James</forename><surname>Bradbury</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Gregory</forename><surname>Chanan</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Trevor</forename><surname>Killeen</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Zeming</forename><surname>Lin</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Natalia</forename><surname>Gimelshein</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Luca</forename><surname>Antiga</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Alban</forename><surname>Desmaison</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Andreas</forename><surname>Kopf</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Edward</forename><surname>Yang</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Zachary</forename><surname>Devito</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Martin</forename><surname>Raison</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Alykhan</forename><surname>Tejani</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Sasank</forename><surname>Chilamkurthy</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Benoit</forename><surname>Steiner</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Lu</forename><surname>Fang</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Junjie</forename><surname>Bai</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Soumith</forename><surname>Chintala</surname></persName>\n\t\t</author>\n\t\t<idno type=\"DOI\">10.1109/mmar.2010.5587249</idno>\n\t</analytic>\n\t<monogr>\n\t\t<title level=\"m\">2010 15th International Conference on Methods and Models in Automation and Robotics</title>\n\t\t<editor>\n\t\t\t<persName><forename type=\"first\">H</forename><surname>Wallach</surname></persName>\n\t\t</editor>\n\t\t<editor>\n\t\t\t<persName><forename type=\"first\">H</forename><surname>Larochelle</surname></persName>\n\t\t</editor>\n\t\t<editor>\n\t\t\t<persName><forename type=\"first\">A</forename><surname>Beygelzimer</surname></persName>\n\t\t</editor>\n\t\t<editor>\n\t\t\t<persName><forename type=\"first\">F</forename><surname>Alch\u00e9-Buc</surname></persName>\n\t\t</editor>\n\t\t<editor>\n\t\t\t<persName><forename type=\"first\">E</forename><surname>Fox</surname></persName>\n\t\t</editor>\n\t\t<editor>\n\t\t\t<persName><forename type=\"first\">R</forename><surname>Garnett</surname></persName>\n\t\t</editor>\n\t\t<imprint>\n\t\t\t<publisher>IEEE</publisher>\n\t\t\t<date type=\"published\" when=\"2010-08\">2019</date>\n\t\t\t<biblScope unit=\"volume\">32</biblScope>\n\t\t\t<biblScope unit=\"page\">8035</biblScope>\n\t\t</imprint>\n\t</monogr>\n</biblStruct>\n"}, {"refKey": 4, "tei": "<biblStruct xml:id=\"b4\">\n\t<monogr>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Iz</forename><surname>Beltagy</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Matthew</forename><forename type=\"middle\">E</forename><surname>Peters</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Arman</forename><surname>Cohan</surname></persName>\n\t\t</author>\n\t\t<idno>CoRR, abs/2004.05150</idno>\n\t\t<title level=\"m\">Longformer: The long-document transformer</title>\n\t\t<imprint>\n\t\t\t<date>2020</date>\n\t\t</imprint>\n\t</monogr>\n</biblStruct>\n"}], "runtime": 103114, "id": "fc28a19e2c41c23e5fe644f7b8901fd4b865cb35", "metadata": {"id": "fc28a19e2c41c23e5fe644f7b8901fd4b865cb35"}, "original_file_path": "../../datalake/Samuel/SOFTware-Sync/data/xml_not_sofctied/hal-03934474.grobid.tei.xml", "file_name": "hal-03934474.grobid.tei.xml"}