{"application": "software-mentions", "version": "0.8.0", "date": "2024-10-07T11:58+0000", "md5": "46E65940945B4E2A0832F6FB16A76569", "mentions": [{"type": "software", "software-type": "software", "software-name": {"rawForm": "PyTorch", "normalizedForm": "PyTorch", "offsetStart": 0, "offsetEnd": 7}, "context": "PyTorch code for the TPDN model is available on GitHub,1 along with an interactive demo.2", "mentionContextAttributes": {"used": {"value": false, "score": 0.028890252113342285}, "created": {"value": false, "score": 0.001088261604309082}, "shared": {"value": false, "score": 0.44818228483200073}}, "documentContextAttributes": {"used": {"value": false, "score": 0.028890252113342285}, "created": {"value": false, "score": 0.001088261604309082}, "shared": {"value": false, "score": 0.44818228483200073}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "InferSent", "normalizedForm": "InferSent", "offsetStart": 0, "offsetEnd": 9}, "context": "InferSent is better approximated with structural roles than with bag-of-words roles, but all structural role schemes perform similarly. ", "mentionContextAttributes": {"used": {"value": false, "score": 3.814697265625e-05}, "created": {"value": false, "score": 3.647804260253906e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999887943267822}, "created": {"value": false, "score": 3.647804260253906e-05}, "shared": {"value": true, "score": 0.9947108030319214}}, "references": [{"label": "Conneau et al. 2017)", "normalizedForm": "Conneau et al. 2017)", "refKey": 11}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "InferSent", "normalizedForm": "InferSent", "offsetStart": 0, "offsetEnd": 9}, "context": "InferSent, Skip-thought, and SPINN all show results most consistent with bidirectional roles, while SST shows results most consistent with tree-based or bidirectional roles.", "mentionContextAttributes": {"used": {"value": false, "score": 0.3155469298362732}, "created": {"value": false, "score": 2.110004425048828e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999887943267822}, "created": {"value": false, "score": 3.647804260253906e-05}, "shared": {"value": true, "score": 0.9947108030319214}}, "references": [{"label": "Conneau et al. 2017)", "normalizedForm": "Conneau et al. 2017)", "refKey": 11}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "InferSent", "normalizedForm": "InferSent", "offsetStart": 0, "offsetEnd": 9}, "context": "InferSent is a bidirectional LSTM with 4096-dimensional hidden states.", "mentionContextAttributes": {"used": {"value": false, "score": 0.0002544522285461426}, "created": {"value": false, "score": 1.33514404296875e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999887943267822}, "created": {"value": false, "score": 3.647804260253906e-05}, "shared": {"value": true, "score": 0.9947108030319214}}, "references": [{"label": "Conneau et al. 2017)", "normalizedForm": "Conneau et al. 2017)", "refKey": 11}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "InferSent", "normalizedForm": "InferSent", "offsetStart": 2, "offsetEnd": 11}, "context": "\u2022 InferSent: https://github.com/facebookresearch/InferSent", "mentionContextAttributes": {"used": {"value": false, "score": 0.0001500844955444336}, "created": {"value": false, "score": 2.6941299438476562e-05}, "shared": {"value": true, "score": 0.9947108030319214}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999887943267822}, "created": {"value": false, "score": 3.647804260253906e-05}, "shared": {"value": true, "score": 0.9947108030319214}}, "references": [{"label": "Conneau et al. 2017)", "normalizedForm": "Conneau et al. 2017)", "refKey": 11}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "code", "normalizedForm": "code", "offsetStart": 8, "offsetEnd": 12}, "context": "PyTorch code for the TPDN model is available on GitHub,1 along with an interactive demo.2", "mentionContextAttributes": {"used": {"value": false, "score": 0.028890252113342285}, "created": {"value": false, "score": 0.001088261604309082}, "shared": {"value": false, "score": 0.44818228483200073}}, "documentContextAttributes": {"used": {"value": false, "score": 0.028890252113342285}, "created": {"value": false, "score": 0.001088261604309082}, "shared": {"value": false, "score": 0.44818228483200073}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "SentEval", "normalizedForm": "SentEval", "offsetStart": 13, "offsetEnd": 45}, "context": "Here, we use SentEval (Conneau & Kiela, 2018) to train linear classifiers for all downstream tasks on the original sentence encoding model; then, we freeze the weights of these classifiers and use them to classify the test-set encodings generated by the TPDN approximation. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9890034794807434}, "created": {"value": false, "score": 0.000530540943145752}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9996582269668579}, "created": {"value": false, "score": 0.23470920324325562}, "shared": {"value": false, "score": 8.344650268554688e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "SentEval", "normalizedForm": "SentEval", "offsetStart": 14, "offsetEnd": 22}, "context": "We also train SentEval classifiers on top of the TPDN instead of the original model; see Appendix F for these results.", "mentionContextAttributes": {"used": {"value": false, "score": 0.005565464496612549}, "created": {"value": false, "score": 0.23470920324325562}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9996582269668579}, "created": {"value": false, "score": 0.23470920324325562}, "shared": {"value": false, "score": 8.344650268554688e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "SPINN-PI-NT", "normalizedForm": "SPINN-PI-NT", "offsetStart": 31, "offsetEnd": 42}, "context": "Finally, for SPINN, we use the SPINN-PI-NT version, which is equivalent to a tree-LSTM (Tai et al., 2015) with 300dimensional hidden states.", "mentionContextAttributes": {"used": {"value": true, "score": 0.534120500087738}, "created": {"value": false, "score": 9.775161743164062e-06}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.534120500087738}, "created": {"value": false, "score": 9.775161743164062e-06}, "shared": {"value": false, "score": 2.384185791015625e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "InferSent", "normalizedForm": "InferSent", "offsetStart": 49, "offsetEnd": 58}, "context": "\u2022 InferSent: https://github.com/facebookresearch/InferSent", "mentionContextAttributes": {"used": {"value": false, "score": 0.0001500844955444336}, "created": {"value": false, "score": 2.6941299438476562e-05}, "shared": {"value": true, "score": 0.9947108030319214}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999887943267822}, "created": {"value": false, "score": 3.647804260253906e-05}, "shared": {"value": true, "score": 0.9947108030319214}}, "references": [{"label": "Conneau et al. 2017)", "normalizedForm": "Conneau et al. 2017)", "refKey": 11}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "SentEval", "normalizedForm": "SentEval", "offsetStart": 52, "offsetEnd": 60}, "context": "We use the classifier parameters recommended by the SentEval authors: using a linear classifier (not a multi-layer perceptron) trained with the Adam algorithm (Kingma & Ba, 2015) using a batch size of 64, a tenacity of 5, and an epoch size of 4. The results are shown in Table 8.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9995279312133789}, "created": {"value": false, "score": 7.152557373046875e-06}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9996582269668579}, "created": {"value": false, "score": 0.23470920324325562}, "shared": {"value": false, "score": 8.344650268554688e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "InferSent", "normalizedForm": "InferSent", "offsetStart": 74, "offsetEnd": 83}, "context": "We explore this question using sentence representations from four models: InferSent (Conneau et al., 2017), a BiLSTM trained on the Stanford Natural Language Inference (SNLI) corpus (Bowman et al., 2015); Skip-thought (Kiros et al., 2015), an LSTM trained to predict the sentence before or after a given sentence; the Stanford sentiment model (SST) (Socher et al., 2013), a tree-based recursive neural tensor network trained to predict movie review sentiment; and SPINN (Bowman et al., 2016), a tree-based RNN trained on SNLI.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9986109733581543}, "created": {"value": false, "score": 2.7418136596679688e-05}, "shared": {"value": false, "score": 5.960464477539062e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999887943267822}, "created": {"value": false, "score": 3.647804260253906e-05}, "shared": {"value": true, "score": 0.9947108030319214}}, "references": [{"label": "(Conneau et al., 2017)", "normalizedForm": "Conneau et al., 2017", "refKey": 11, "offsetStart": 13675, "offsetEnd": 13697}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "InferSent", "normalizedForm": "InferSent", "offsetStart": 164, "offsetEnd": 173}, "context": "The best-performing dimension was chosen based on preliminary experiments and used for all subsequent experiments; we thereby chose role dimensionalities of 10 for InferSent and Skip-thought, 20 for SST, and 5 for SPINN.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9999887943267822}, "created": {"value": false, "score": 1.1324882507324219e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999887943267822}, "created": {"value": false, "score": 3.647804260253906e-05}, "shared": {"value": true, "score": 0.9947108030319214}}, "references": [{"label": "Conneau et al. 2017)", "normalizedForm": "Conneau et al. 2017)", "refKey": 11}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "InferSent", "normalizedForm": "InferSent", "offsetStart": 188, "offsetEnd": 197}, "context": "For training a TPDN to approximate the sentence encoding models, the filler embedding dimensions were dictated by the size of the pretrained word embeddings; these dimensions were 300 for InferSent and SPINN, 620 for Skip-thought, and 25 for SST.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9997580647468567}, "created": {"value": false, "score": 1.1324882507324219e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999887943267822}, "created": {"value": false, "score": 3.647804260253906e-05}, "shared": {"value": true, "score": 0.9947108030319214}}, "references": [{"label": "Conneau et al. 2017)", "normalizedForm": "Conneau et al. 2017)", "refKey": 11}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "SentEval", "normalizedForm": "SentEval", "offsetStart": 287, "offsetEnd": 319}, "context": "Tasks: We assess how the tensor product approximations compare to the models they approximate at four tasks that are widely accepted for evaluating sentence embeddings: (1) Stanford Sentiment Treebank (SST), rating the sentiment of movie reviews (Socher et al., 2013) Evaluation: We use SentEval (Conneau & Kiela, 2018) to train a classifier for each task on the original encodings produced by the sentence encoding model. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9996582269668579}, "created": {"value": false, "score": 4.291534423828125e-06}, "shared": {"value": false, "score": 8.344650268554688e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9996582269668579}, "created": {"value": false, "score": 0.23470920324325562}, "shared": {"value": false, "score": 8.344650268554688e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "InferSent", "normalizedForm": "InferSent", "offsetStart": 307, "offsetEnd": 316}, "context": "This linear combination can predict the linear relations between sequence representations illustrated in Figure 1. Figure 1: Plots of the first two principal components of (a) word embeddings (Pennington et al., 2014), (b) digit-sequence embeddings learned by an autoencoder (Section 2), and (c) sentences (InferSent: Conneau et al. 2017).", "mentionContextAttributes": {"used": {"value": false, "score": 0.45806413888931274}, "created": {"value": false, "score": 2.9802322387695312e-06}, "shared": {"value": false, "score": 5.960464477539062e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999887943267822}, "created": {"value": false, "score": 3.647804260253906e-05}, "shared": {"value": true, "score": 0.9947108030319214}}, "references": [{"label": "Conneau et al. 2017)", "normalizedForm": "Conneau et al. 2017)", "refKey": 11, "offsetStart": 3540, "offsetEnd": 3560}]}], "references": [{"refKey": 11, "tei": "<biblStruct xml:id=\"b11\">\n\t<analytic>\n\t\t<title level=\"a\" type=\"main\">Supervised Learning of Universal Sentence Representations from Natural Language Inference Data</title>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Alexis</forename><surname>Conneau</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Douwe</forename><surname>Kiela</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Holger</forename><surname>Schwenk</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Lo\u00efc</forename><surname>Barrault</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Antoine</forename><surname>Bordes</surname></persName>\n\t\t</author>\n\t\t<idno type=\"DOI\">10.18653/v1/d17-1070</idno>\n\t</analytic>\n\t<monogr>\n\t\t<title level=\"m\">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>\n\t\t<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing</meeting>\n\t\t<imprint>\n\t\t\t<publisher>Association for Computational Linguistics</publisher>\n\t\t\t<date type=\"published\" when=\"2017\">2017</date>\n\t\t\t<biblScope unit=\"page\">680</biblScope>\n\t\t</imprint>\n\t</monogr>\n</biblStruct>\n"}], "runtime": 10442, "id": "8e2771d9a5ab2d7c69c2b747bca068690ca3c811", "metadata": {"id": "8e2771d9a5ab2d7c69c2b747bca068690ca3c811"}, "original_file_path": "../../datalake/Samuel/SOFTware-Sync/downloads/xml/hal-02274498.grobid.tei.xml", "file_name": "hal-02274498.grobid.tei.xml"}