{"application": "software-mentions", "version": "0.8.0", "date": "2024-10-07T11:48+0000", "md5": "069CBC0E78FCD86B88604F81086D0344", "mentions": [{"type": "software", "software-type": "software", "software-name": {"rawForm": "SEM", "normalizedForm": "SEM", "offsetStart": 0, "offsetEnd": 3}, "context": "SEM uses Wapiti [13] v1.5.05 as linear-chain CRFs implementation. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.10659116506576538}, "created": {"value": false, "score": 1.1920928955078125e-06}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": false, "score": 0.3441835045814514}, "created": {"value": false, "score": 0.22266548871994019}, "shared": {"value": true, "score": 0.9937916398048401}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "SEM", "normalizedForm": "SEM", "offsetStart": 0, "offsetEnd": 3}, "context": "SEM uses the following features for NER:", "mentionContextAttributes": {"used": {"value": false, "score": 0.00027245283126831055}, "created": {"value": false, "score": 3.325939178466797e-05}, "shared": {"value": false, "score": 1.0728836059570312e-06}}, "documentContextAttributes": {"used": {"value": false, "score": 0.3441835045814514}, "created": {"value": false, "score": 0.22266548871994019}, "shared": {"value": true, "score": 0.9937916398048401}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "SEM", "normalizedForm": "SEM", "offsetStart": 0, "offsetEnd": 3}, "context": "SEM makes it simple to switch between different sequence segmentations, which allowed us to label sentences and output segments.", "mentionContextAttributes": {"used": {"value": false, "score": 0.3441835045814514}, "created": {"value": false, "score": 0.0024139881134033203}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": false, "score": 0.3441835045814514}, "created": {"value": false, "score": 0.22266548871994019}, "shared": {"value": true, "score": 0.9937916398048401}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "SEM", "normalizedForm": "SEM", "offsetStart": 0, "offsetEnd": 3}, "context": "SEM's sentence segmentation engine works using mainly local rules at token level rule to determine whether a token is the last of a sequence.", "mentionContextAttributes": {"used": {"value": false, "score": 0.0001360177993774414}, "created": {"value": false, "score": 3.5881996154785156e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": false, "score": 0.3441835045814514}, "created": {"value": false, "score": 0.22266548871994019}, "shared": {"value": true, "score": 0.9937916398048401}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "ELMo", "normalizedForm": "ELMo", "offsetStart": 9, "offsetEnd": 13}, "context": "However, ELMo in particular uses a bidirectional language model (biLM) consisting of L LSTM layers, that is, it combines both a forward and a backward language model jointly maximizing the log likelihood of the forward and backward directions; ELMo also computes a context-independent token representation via token embeddings or via a CNN over characters.", "mentionContextAttributes": {"used": {"value": false, "score": 0.0007883310317993164}, "created": {"value": false, "score": 1.5616416931152344e-05}, "shared": {"value": false, "score": 1.6689300537109375e-06}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9773378372192383}, "created": {"value": true, "score": 0.683404266834259}, "shared": {"value": false, "score": 1.6689300537109375e-06}}, "references": [{"label": "[22]", "normalizedForm": "[22]", "refKey": 22}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Wapiti", "normalizedForm": "Wapiti", "offsetStart": 9, "offsetEnd": 20}, "version": {"rawForm": "1.5.0", "normalizedForm": "1.5.0", "offsetStart": 22, "offsetEnd": 28}, "context": "SEM uses Wapiti [13] v1.5.05 as linear-chain CRFs implementation. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.10659116506576538}, "created": {"value": false, "score": 1.1920928955078125e-06}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999302625656128}, "created": {"value": false, "score": 5.245208740234375e-05}, "shared": {"value": true, "score": 0.9913809895515442}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "SEM", "normalizedForm": "SEM", "offsetStart": 13, "offsetEnd": 16}, "context": "For example, SEM decides at tokenization stage whether a dot is a strong punctuation or part of a larger token, as for abreviations.", "mentionContextAttributes": {"used": {"value": false, "score": 0.00015175342559814453}, "created": {"value": false, "score": 1.537799835205078e-05}, "shared": {"value": false, "score": 4.76837158203125e-07}}, "documentContextAttributes": {"used": {"value": false, "score": 0.3441835045814514}, "created": {"value": false, "score": 0.22266548871994019}, "shared": {"value": true, "score": 0.9937916398048401}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "SEM", "normalizedForm": "SEM", "offsetStart": 16, "offsetEnd": 19}, "context": "One interest of SEM is that it has a builtin sentence tokenizer for french using a rule-based approach.", "mentionContextAttributes": {"used": {"value": false, "score": 2.6702880859375e-05}, "created": {"value": false, "score": 0.22266548871994019}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": false, "score": 0.3441835045814514}, "created": {"value": false, "score": 0.22266548871994019}, "shared": {"value": true, "score": 0.9937916398048401}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "SEM", "normalizedForm": "SEM", "offsetStart": 25, "offsetEnd": 28}, "context": "Another interest is that SEM has an NE mention broadcasting process.", "mentionContextAttributes": {"used": {"value": false, "score": 0.00017815828323364258}, "created": {"value": false, "score": 0.00717616081237793}, "shared": {"value": false, "score": 1.0728836059570312e-06}}, "documentContextAttributes": {"used": {"value": false, "score": 0.3441835045814514}, "created": {"value": false, "score": 0.22266548871994019}, "shared": {"value": true, "score": 0.9937916398048401}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "ELMo", "normalizedForm": "ELMo", "offsetStart": 33, "offsetEnd": 37}, "context": "Embeddings from Language Models (ELMo) [22] is a Language Model, i.e, a model that given a sequence of N tokens, (t 1 , t 2 , ..., t N ), computes the probability of the sequence by modeling the probability of token t k given the history (t 1 , ..., t k-1 ):", "mentionContextAttributes": {"used": {"value": false, "score": 0.005106091499328613}, "created": {"value": false, "score": 6.67572021484375e-06}, "shared": {"value": false, "score": 8.344650268554688e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9773378372192383}, "created": {"value": true, "score": 0.683404266834259}, "shared": {"value": false, "score": 1.6689300537109375e-06}}, "references": [{"label": "[22]", "normalizedForm": "[22]", "refKey": 22, "offsetStart": 10626, "offsetEnd": 10630}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "ELMo", "normalizedForm": "ELMo", "offsetStart": 37, "offsetEnd": 41}, "context": "Following [22], we use in this paper ELMo models where L = 2, i.e., the ELMo architecture involves a character-level CNN layer followed by a 2-layer biLSTM.", "mentionContextAttributes": {"used": {"value": false, "score": 0.10419082641601562}, "created": {"value": false, "score": 0.012152552604675293}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9773378372192383}, "created": {"value": true, "score": 0.683404266834259}, "shared": {"value": false, "score": 1.6689300537109375e-06}}, "references": [{"label": "[22]", "normalizedForm": "[22]", "refKey": 22}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Wapiti", "normalizedForm": "Wapiti", "offsetStart": 40, "offsetEnd": 46}, "version": {"rawForm": "1.5.0", "normalizedForm": "1.5.0"}, "context": "available at : https://github.com/Jekub/Wapiti", "mentionContextAttributes": {"used": {"value": false, "score": 9.107589721679688e-05}, "created": {"value": false, "score": 5.245208740234375e-05}, "shared": {"value": true, "score": 0.9913809895515442}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999302625656128}, "created": {"value": false, "score": 5.245208740234375e-05}, "shared": {"value": true, "score": 0.9913809895515442}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "SEM", "normalizedForm": "SEM", "offsetStart": 46, "offsetEnd": 49}, "context": "available at : https://github.com/YoannDupont/SEM", "mentionContextAttributes": {"used": {"value": false, "score": 7.665157318115234e-05}, "created": {"value": false, "score": 3.3855438232421875e-05}, "shared": {"value": true, "score": 0.9937916398048401}}, "documentContextAttributes": {"used": {"value": false, "score": 0.3441835045814514}, "created": {"value": false, "score": 0.22266548871994019}, "shared": {"value": true, "score": 0.9937916398048401}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "SEM", "normalizedForm": "SEM", "offsetStart": 63, "offsetEnd": 66}, "context": "The cost of using another segmentation is relatively cheap, as SEM can process nearly 1GB of raw text per hour.", "mentionContextAttributes": {"used": {"value": false, "score": 0.00018781423568725586}, "created": {"value": false, "score": 1.7523765563964844e-05}, "shared": {"value": false, "score": 5.960464477539062e-07}}, "documentContextAttributes": {"used": {"value": false, "score": 0.3441835045814514}, "created": {"value": false, "score": 0.22266548871994019}, "shared": {"value": true, "score": 0.9937916398048401}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "ELMo", "normalizedForm": "ELMo", "offsetStart": 70, "offsetEnd": 74}, "context": "When included in a downstream model, as it is the case in this paper, ELMo collapses all L layers into a single vector, generally computing a task specific weighting of all biLM layers applying layer normalization to each biLM layer before weighting.", "mentionContextAttributes": {"used": {"value": false, "score": 0.0010089278221130371}, "created": {"value": false, "score": 0.00010001659393310547}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9773378372192383}, "created": {"value": true, "score": 0.683404266834259}, "shared": {"value": false, "score": 1.6689300537109375e-06}}, "references": [{"label": "[22]", "normalizedForm": "[22]", "refKey": 22}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "ELMo", "normalizedForm": "ELMo", "offsetStart": 72, "offsetEnd": 76}, "context": "Following [22], we use in this paper ELMo models where L = 2, i.e., the ELMo architecture involves a character-level CNN layer followed by a 2-layer biLSTM.", "mentionContextAttributes": {"used": {"value": false, "score": 0.10419082641601562}, "created": {"value": false, "score": 0.012152552604675293}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9773378372192383}, "created": {"value": true, "score": 0.683404266834259}, "shared": {"value": false, "score": 1.6689300537109375e-06}}, "references": [{"label": "[22]", "normalizedForm": "[22]", "refKey": 22}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Wapiti", "normalizedForm": "Wapiti", "offsetStart": 86, "offsetEnd": 92}, "version": {"rawForm": "1.5.0", "normalizedForm": "1.5.0"}, "context": "For French, our optimal L1 and L2 penalties were 0.5 and 0.0001 respectively (default Wapiti parameters).", "mentionContextAttributes": {"used": {"value": true, "score": 0.9999302625656128}, "created": {"value": false, "score": 3.933906555175781e-06}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999302625656128}, "created": {"value": false, "score": 5.245208740234375e-05}, "shared": {"value": true, "score": 0.9913809895515442}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "ELMo", "normalizedForm": "ELMo", "offsetStart": 134, "offsetEnd": 138}, "context": "The first method relied on linear-chain CRFs while the other two methods use a Bidirectional LSTM and a bidirectional Language Model (ELMo).", "mentionContextAttributes": {"used": {"value": true, "score": 0.9773378372192383}, "created": {"value": false, "score": 3.5762786865234375e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9773378372192383}, "created": {"value": true, "score": 0.683404266834259}, "shared": {"value": false, "score": 1.6689300537109375e-06}}, "references": [{"label": "[22]", "normalizedForm": "[22]", "refKey": 22}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "ELMo", "normalizedForm": "ELMo", "offsetStart": 138, "offsetEnd": 142}, "context": "The best system we proposed ranked third for these two languages, it uses FastText embeddings and Elmo language models (FrELMo and German ELMo).", "mentionContextAttributes": {"used": {"value": false, "score": 0.2736339569091797}, "created": {"value": true, "score": 0.683404266834259}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9773378372192383}, "created": {"value": true, "score": 0.683404266834259}, "shared": {"value": false, "score": 1.6689300537109375e-06}}, "references": [{"label": "[22]", "normalizedForm": "[22]", "refKey": 22}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "ELMo", "normalizedForm": "ELMo", "offsetStart": 244, "offsetEnd": 248}, "context": "However, ELMo in particular uses a bidirectional language model (biLM) consisting of L LSTM layers, that is, it combines both a forward and a backward language model jointly maximizing the log likelihood of the forward and backward directions; ELMo also computes a context-independent token representation via token embeddings or via a CNN over characters.", "mentionContextAttributes": {"used": {"value": false, "score": 0.0007883310317993164}, "created": {"value": false, "score": 1.5616416931152344e-05}, "shared": {"value": false, "score": 1.6689300537109375e-06}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9773378372192383}, "created": {"value": true, "score": 0.683404266834259}, "shared": {"value": false, "score": 1.6689300537109375e-06}}, "references": [{"label": "[22]", "normalizedForm": "[22]", "refKey": 22}]}], "references": [{"refKey": 22, "tei": "<biblStruct xml:id=\"b22\">\n\t<analytic>\n\t\t<title level=\"a\" type=\"main\">Deep Contextualized Word Representations</title>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Matthew</forename><surname>Peters</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Mark</forename><surname>Neumann</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Mohit</forename><surname>Iyyer</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Matt</forename><surname>Gardner</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Christopher</forename><surname>Clark</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Kenton</forename><surname>Lee</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Luke</forename><surname>Zettlemoyer</surname></persName>\n\t\t</author>\n\t\t<idno type=\"DOI\">10.18653/v1/n18-1202</idno>\n\t</analytic>\n\t<monogr>\n\t\t<title level=\"m\">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)</title>\n\t\t<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)</meeting>\n\t\t<imprint>\n\t\t\t<publisher>Association for Computational Linguistics</publisher>\n\t\t\t<date type=\"published\" when=\"2018\">Jun 2018</date>\n\t\t\t<biblScope unit=\"volume\">1</biblScope>\n\t\t\t<biblScope unit=\"page\">2237</biblScope>\n\t\t</imprint>\n\t</monogr>\n</biblStruct>\n"}], "runtime": 9539, "id": "4af0f898e20a9ed3fdf4d9efdb76490d777a9c89", "metadata": {"id": "4af0f898e20a9ed3fdf4d9efdb76490d777a9c89"}, "original_file_path": "../../datalake/Samuel/SOFTware-Sync/downloads/xml/hal-02984746.grobid.tei.xml", "file_name": "hal-02984746.grobid.tei.xml"}