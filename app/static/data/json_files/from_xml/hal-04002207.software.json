{"application": "software-mentions", "version": "0.8.0", "date": "2024-04-12T16:01+0000", "md5": "7662FF6EF7F4F3CD6A1ED8982243EB55", "mentions": [{"type": "software", "software-type": "software", "software-name": {"rawForm": "MedSpacy", "normalizedForm": "MedSpacy", "offsetStart": 15, "offsetEnd": 23}, "context": "In particular, MedSpacy is built on top of Spacy specifically for clinical natural language processing, while CLAMP offers a method for named entity recognition (NER) as well as a visual interface for annotating and labeling clinical text.", "mentionContextAttributes": {"used": {"value": false, "score": 7.110834121704102e-05}, "created": {"value": false, "score": 0.042521774768829346}, "shared": {"value": false, "score": 4.172325134277344e-07}}, "documentContextAttributes": {"used": {"value": false, "score": 7.110834121704102e-05}, "created": {"value": false, "score": 0.042521774768829346}, "shared": {"value": false, "score": 6.556510925292969e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "UmlsBERT", "normalizedForm": "UmlsBERT", "offsetStart": 15, "offsetEnd": 23}, "context": "They show that UmlsBERT can associate different clinical terms with similar meanings in the UMLS knowledge base and create meaningful input embeddings by leveraging information from the semantic type of each word.", "mentionContextAttributes": {"used": {"value": false, "score": 0.0003622770309448242}, "created": {"value": false, "score": 0.00011450052261352539}, "shared": {"value": false, "score": 2.980232238769531e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9998717308044434}, "created": {"value": true, "score": 0.8711295127868652}, "shared": {"value": false, "score": 5.364418029785156e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "PyTorch", "normalizedForm": "PyTorch", "offsetStart": 33, "offsetEnd": 40}, "context": "To fine-tune the LMs, we use the PyTorch implementation of huggingface (Wolf et al., 2020) (v4.18). ", "mentionContextAttributes": {"used": {"value": true, "score": 0.999068021774292}, "created": {"value": false, "score": 4.32133674621582e-05}, "shared": {"value": false, "score": 2.980232238769531e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.999068021774292}, "created": {"value": false, "score": 4.32133674621582e-05}, "shared": {"value": false, "score": 2.980232238769531e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "UmlsBERT", "normalizedForm": "UmlsBERT", "offsetStart": 37, "offsetEnd": 45}, "context": "(Michalopoulos et al., 2020) propose UmlsBERT, a contextual embedding model that integrates domain knowledge from the Unified Medical Language System (UMLS) (Bodenreider, 2004), taking into consideration structured expert domain knowledge.", "mentionContextAttributes": {"used": {"value": false, "score": 7.766485214233398e-05}, "created": {"value": true, "score": 0.8711295127868652}, "shared": {"value": false, "score": 5.364418029785156e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9998717308044434}, "created": {"value": true, "score": 0.8711295127868652}, "shared": {"value": false, "score": 5.364418029785156e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "SciBERT", "normalizedForm": "SciBERT", "offsetStart": 40, "offsetEnd": 47}, "context": "We can observe that all models Overall, SciBERT uncased is the best-performing model (in bold) with a macro F1-score of 0.86, outperforming the other approaches for each of the categories. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.03431910276412964}, "created": {"value": false, "score": 1.1265277862548828e-05}, "shared": {"value": false, "score": 2.980232238769531e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9998717308044434}, "created": {"value": false, "score": 0.00034427642822265625}, "shared": {"value": false, "score": 1.0728836059570312e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "SciBERT", "normalizedForm": "SciBERT", "offsetStart": 45, "offsetEnd": 52}, "context": "The same configuration was used to fine-tune SciBERT BioBERT, PubMedBERT and UmlsBERT. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9998717308044434}, "created": {"value": false, "score": 1.3053417205810547e-05}, "shared": {"value": false, "score": 1.7881393432617188e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9998717308044434}, "created": {"value": false, "score": 0.00034427642822265625}, "shared": {"value": false, "score": 1.0728836059570312e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "PubMedBERT", "normalizedForm": "PubMedBERT", "offsetStart": 52, "offsetEnd": 62}, "context": "In Table 4 we show only the bestperforming baseline PubMedBERT no context obtaining similar results to DASH (0.41 and 0.37, respectively).", "mentionContextAttributes": {"used": {"value": true, "score": 0.9988976716995239}, "created": {"value": false, "score": 1.4901161193847656e-06}, "shared": {"value": false, "score": 1.7881393432617188e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9998717308044434}, "created": {"value": false, "score": 5.8770179748535156e-05}, "shared": {"value": false, "score": 1.0728836059570312e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "BioBERT", "normalizedForm": "BioBERT", "offsetStart": 53, "offsetEnd": 60}, "version": {"rawForm": "1.2", "normalizedForm": "1.2"}, "context": "The same configuration was used to fine-tune SciBERT BioBERT, PubMedBERT and UmlsBERT. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9998717308044434}, "created": {"value": false, "score": 1.3053417205810547e-05}, "shared": {"value": false, "score": 1.7881393432617188e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9998717308044434}, "created": {"value": false, "score": 9.232759475708008e-05}, "shared": {"value": false, "score": 1.0728836059570312e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "PubMedBERT", "normalizedForm": "PubMedBERT", "offsetStart": 62, "offsetEnd": 72}, "context": "The same configuration was used to fine-tune SciBERT BioBERT, PubMedBERT and UmlsBERT. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9998717308044434}, "created": {"value": false, "score": 1.3053417205810547e-05}, "shared": {"value": false, "score": 1.7881393432617188e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9998717308044434}, "created": {"value": false, "score": 5.8770179748535156e-05}, "shared": {"value": false, "score": 1.0728836059570312e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "BioBERT", "normalizedForm": "BioBERT", "offsetStart": 66, "offsetEnd": 73}, "version": {"rawForm": "1.2", "normalizedForm": "1.2", "offsetStart": 89, "offsetEnd": 92}, "context": "For SciB-ERT, we use both the cased and uncased versions, and for BioBERT we use version 1.2. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.7840084433555603}, "created": {"value": false, "score": 1.8477439880371094e-06}, "shared": {"value": false, "score": 2.980232238769531e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9998717308044434}, "created": {"value": false, "score": 9.232759475708008e-05}, "shared": {"value": false, "score": 1.0728836059570312e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "MedSpacy", "normalizedForm": "MedSpacy", "offsetStart": 72, "offsetEnd": 100}, "context": "Off-the-shelf NLP tool-kits such as Spacy (Honnibal and Montani, 2017), MedSpacy (Eyre et al., 2021) and CLAMP (Soysal et al., 2018) provide multiple modules for text processing. ", "mentionContextAttributes": {"used": {"value": false, "score": 4.0590763092041016e-05}, "created": {"value": false, "score": 0.00015306472778320312}, "shared": {"value": false, "score": 6.556510925292969e-07}}, "documentContextAttributes": {"used": {"value": false, "score": 7.110834121704102e-05}, "created": {"value": false, "score": 0.042521774768829346}, "shared": {"value": false, "score": 6.556510925292969e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Glove", "normalizedForm": "Glove", "offsetStart": 75, "offsetEnd": 105}, "context": "symptom into words that we encode by either using each word as an input on Glove (Pennington et al., 2014), or extracting directly from the contextualized models the representation of the symptom by summarizing the hidden states of the last four layers in the model. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.30584716796875}, "created": {"value": false, "score": 4.762411117553711e-05}, "shared": {"value": false, "score": 2.980232238769531e-07}}, "documentContextAttributes": {"used": {"value": false, "score": 0.30584716796875}, "created": {"value": false, "score": 4.762411117553711e-05}, "shared": {"value": false, "score": 2.980232238769531e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "UmlsBERT", "normalizedForm": "UmlsBERT", "offsetStart": 77, "offsetEnd": 85}, "context": "The same configuration was used to fine-tune SciBERT BioBERT, PubMedBERT and UmlsBERT.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9998717308044434}, "created": {"value": false, "score": 1.3053417205810547e-05}, "shared": {"value": false, "score": 1.7881393432617188e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9998717308044434}, "created": {"value": true, "score": 0.8711295127868652}, "shared": {"value": false, "score": 5.364418029785156e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "GloVe", "normalizedForm": "GloVe", "offsetStart": 86, "offsetEnd": 91}, "context": "In the experimental setting of (i) and (ii), we use the static pre-trained embeddings GloVe 6B as well as BERT, SciBERT, BioBERT and UmlsBERT in the same configurations as in the symptom detection task. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9768832325935364}, "created": {"value": false, "score": 9.232759475708008e-05}, "shared": {"value": false, "score": 1.7881393432617188e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9768832325935364}, "created": {"value": false, "score": 9.232759475708008e-05}, "shared": {"value": false, "score": 1.7881393432617188e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "SciBERT", "normalizedForm": "SciBERT", "offsetStart": 112, "offsetEnd": 119}, "context": "In the experimental setting of (i) and (ii), we use the static pre-trained embeddings GloVe 6B as well as BERT, SciBERT, BioBERT and UmlsBERT in the same configurations as in the symptom detection task. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9768832325935364}, "created": {"value": false, "score": 9.232759475708008e-05}, "shared": {"value": false, "score": 1.7881393432617188e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9998717308044434}, "created": {"value": false, "score": 0.00034427642822265625}, "shared": {"value": false, "score": 1.0728836059570312e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "BioBERT", "normalizedForm": "BioBERT", "offsetStart": 121, "offsetEnd": 128}, "version": {"rawForm": "1.2", "normalizedForm": "1.2"}, "context": "In the experimental setting of (i) and (ii), we use the static pre-trained embeddings GloVe 6B as well as BERT, SciBERT, BioBERT and UmlsBERT in the same configurations as in the symptom detection task. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9768832325935364}, "created": {"value": false, "score": 9.232759475708008e-05}, "shared": {"value": false, "score": 1.7881393432617188e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9998717308044434}, "created": {"value": false, "score": 9.232759475708008e-05}, "shared": {"value": false, "score": 1.0728836059570312e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "SciBERT", "normalizedForm": "SciBERT", "offsetStart": 125, "offsetEnd": 154}, "context": "Sentence-BERT can be used with different pretrained models, in this work we focus on the models BERT (Devlin et al., 2019) , SciBERT (Beltagy et al., 2019), UMLSBERT (Michalopoulos et al., 2020) and S-PubMedBert by (Deka et al., 2022). ", "mentionContextAttributes": {"used": {"value": false, "score": 0.025809168815612793}, "created": {"value": false, "score": 0.00034427642822265625}, "shared": {"value": false, "score": 8.344650268554688e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9998717308044434}, "created": {"value": false, "score": 0.00034427642822265625}, "shared": {"value": false, "score": 1.0728836059570312e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "UmlsBERT", "normalizedForm": "UmlsBERT", "offsetStart": 133, "offsetEnd": 141}, "context": "In the experimental setting of (i) and (ii), we use the static pre-trained embeddings GloVe 6B as well as BERT, SciBERT, BioBERT and UmlsBERT in the same configurations as in the symptom detection task. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9768832921981812}, "created": {"value": false, "score": 9.226799011230469e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9998717308044434}, "created": {"value": true, "score": 0.8711295127868652}, "shared": {"value": false, "score": 5.364418029785156e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "SciBERT", "normalizedForm": "SciBERT", "offsetStart": 139, "offsetEnd": 168}, "context": "For the symptom detection task, we experimented with different transformer-based Language Models (LMs) such as BERT (Devlin et al., 2019), SciBERT (Beltagy et al., 2019), BioBERT (Lee et al., 2020), PubMedBERT (Gu et al., 2020) and Umls-BERT (Michalopoulos et al., 2020) initialized with their respective pre-trained weights. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9994369149208069}, "created": {"value": false, "score": 5.8770179748535156e-05}, "shared": {"value": false, "score": 1.0728836059570312e-06}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9998717308044434}, "created": {"value": false, "score": 0.00034427642822265625}, "shared": {"value": false, "score": 1.0728836059570312e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "BioBERT", "normalizedForm": "BioBERT", "offsetStart": 171, "offsetEnd": 196}, "version": {"rawForm": "1.2", "normalizedForm": "1.2"}, "context": "For the symptom detection task, we experimented with different transformer-based Language Models (LMs) such as BERT (Devlin et al., 2019), SciBERT (Beltagy et al., 2019), BioBERT (Lee et al., 2020), PubMedBERT (Gu et al., 2020) and Umls-BERT (Michalopoulos et al., 2020) initialized with their respective pre-trained weights. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9994369149208069}, "created": {"value": false, "score": 5.8770179748535156e-05}, "shared": {"value": false, "score": 1.0728836059570312e-06}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9998717308044434}, "created": {"value": false, "score": 9.232759475708008e-05}, "shared": {"value": false, "score": 1.0728836059570312e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "PubMedBERT", "normalizedForm": "PubMedBERT", "offsetStart": 199, "offsetEnd": 227}, "context": "For the symptom detection task, we experimented with different transformer-based Language Models (LMs) such as BERT (Devlin et al., 2019), SciBERT (Beltagy et al., 2019), BioBERT (Lee et al., 2020), PubMedBERT (Gu et al., 2020) and Umls-BERT (Michalopoulos et al., 2020) initialized with their respective pre-trained weights. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9994369149208069}, "created": {"value": false, "score": 5.8770179748535156e-05}, "shared": {"value": false, "score": 1.0728836059570312e-06}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9998717308044434}, "created": {"value": false, "score": 5.8770179748535156e-05}, "shared": {"value": false, "score": 1.0728836059570312e-06}}}], "references": [], "runtime": 79293, "id": "19cf91409d1bf008625c6e70d3088ea106faab2b", "metadata": {"id": "19cf91409d1bf008625c6e70d3088ea106faab2b"}, "original_file_path": "../../datalake/Samuel/SOFTware-Sync/data/xml_files/hal-04002207.grobid.tei.xml", "file_name": "hal-04002207.grobid.tei.xml"}