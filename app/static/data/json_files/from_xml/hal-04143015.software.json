{"application": "software-mentions", "version": "0.8.0", "date": "2024-10-07T11:50+0000", "md5": "8707FD8AC8B59BC9C4CF2991B95F16A6", "mentions": [{"type": "software", "software-type": "software", "software-name": {"rawForm": "BlenderBot", "normalizedForm": "BlenderBot", "offsetStart": 0, "offsetEnd": 10}, "context": "BlenderBot once again per- forms well on all metrics and has a virtually identical F1 score to BART. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.003264129161834717}, "created": {"value": false, "score": 1.2993812561035156e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.994888961315155}, "created": {"value": false, "score": 0.0003229379653930664}, "shared": {"value": false, "score": 1.5497207641601562e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "DialoGPT", "normalizedForm": "DialoGPT", "offsetStart": 0, "offsetEnd": 28}, "context": "DialoGPT (Zhang et al., 2020b) is a dialogue version of GPT-2 (Radford et al., 2019), an autoregressive language model with a multi-layer Transformer (Vaswani et al., 2017)   (Zhang et al., 2018), ConvAI2 (Dinan et al., 2020), and other datasets that, while largely handcrafted, focus on personality and emotions, enabling it to potentially develop some version of social skills.", "mentionContextAttributes": {"used": {"value": false, "score": 0.021388709545135498}, "created": {"value": false, "score": 0.00021260976791381836}, "shared": {"value": false, "score": 1.6689300537109375e-06}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9768190979957581}, "created": {"value": false, "score": 0.0003229379653930664}, "shared": {"value": false, "score": 1.6689300537109375e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "BlenderBot", "normalizedForm": "BlenderBot", "offsetStart": 6, "offsetEnd": 16}, "context": "While BlenderBot showed strong performance when using reranking, a certain number of generated utterances still did not match the real tutor labels. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.994888961315155}, "created": {"value": false, "score": 8.58306884765625e-06}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.994888961315155}, "created": {"value": false, "score": 0.0003229379653930664}, "shared": {"value": false, "score": 1.5497207641601562e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "AdamW", "normalizedForm": "AdamW", "offsetStart": 9, "offsetEnd": 44}, "context": "We apply AdamW (Loshchilov and Hutter, 2018) as our optimizer with a learning rate 10e -5. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9711863994598389}, "created": {"value": false, "score": 0.0009606480598449707}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9711863994598389}, "created": {"value": false, "score": 0.0009606480598449707}, "shared": {"value": false, "score": 1.1920928955078125e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Chat-GPT", "normalizedForm": "Chat-GPT", "offsetStart": 10, "offsetEnd": 18}, "context": "Recently, Chat-GPT (OpenAI, 2022) used reinforcement learning with human feedback, and has shown impressive performance.", "mentionContextAttributes": {"used": {"value": false, "score": 0.002543032169342041}, "created": {"value": false, "score": 0.0004387497901916504}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": false, "score": 0.002543032169342041}, "created": {"value": false, "score": 0.0004387497901916504}, "shared": {"value": false, "score": 2.384185791015625e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "LaMDA", "normalizedForm": "LaMDA", "offsetStart": 15, "offsetEnd": 45}, "context": "The authors of LaMDA (Thoppilan et al., 2022) used various classifiers to rerank and filter out inappropriate responses. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9996705055236816}, "created": {"value": false, "score": 9.417533874511719e-06}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9996705055236816}, "created": {"value": false, "score": 9.417533874511719e-06}, "shared": {"value": false, "score": 2.384185791015625e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "BlenderBot", "normalizedForm": "BlenderBot", "offsetStart": 22, "offsetEnd": 32}, "context": "This could be because BlenderBot is pretrained with various social dialogue datasets, giving it a certain ability to generate the social aspects of dialogue.", "mentionContextAttributes": {"used": {"value": false, "score": 0.0025838613510131836}, "created": {"value": false, "score": 0.0001157522201538086}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.994888961315155}, "created": {"value": false, "score": 0.0003229379653930664}, "shared": {"value": false, "score": 1.5497207641601562e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "BlenderBot", "normalizedForm": "BlenderBot", "offsetStart": 64, "offsetEnd": 74}, "context": "We use the base version of the BART model, the small version of BlenderBot, and also the small version of DialoGPT. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9710648655891418}, "created": {"value": false, "score": 3.528594970703125e-05}, "shared": {"value": false, "score": 4.76837158203125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.994888961315155}, "created": {"value": false, "score": 0.0003229379653930664}, "shared": {"value": false, "score": 1.5497207641601562e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Blender-Bot", "normalizedForm": "Blender-Bot", "offsetStart": 69, "offsetEnd": 80}, "context": "Additionally, we find some interesting similarities among models: 1) Blender-Bot and DialoGPT outperform BART in both the fine-tuning and the reranking methods (Table 2) with respect to reference-based metrics such as BLEU, ROUGE-L, etc., and 2) DialoGPT still underperforms the other two models in terms of F1 score, and in the reranking condition the gap widens. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.08047902584075928}, "created": {"value": false, "score": 2.0265579223632812e-06}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "documentContextAttributes": {"used": {"value": false, "score": 0.08047902584075928}, "created": {"value": false, "score": 1.3232231140136719e-05}, "shared": {"value": false, "score": 3.5762786865234375e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "DialoGPT", "normalizedForm": "DialoGPT", "offsetStart": 71, "offsetEnd": 79}, "context": "This result could suggest that 1) the pretraining of the models (i.e., DialoGPT, BlenderBot) on dialogue datasets may help to generate longer utterances, and therefore to improve the referencebased metrics performance, and 2) the autoregressive model (e.g., DialoGPT) may not be suitable for the generation of social dialogue such as hedges.", "mentionContextAttributes": {"used": {"value": false, "score": 0.019712209701538086}, "created": {"value": false, "score": 0.00013446807861328125}, "shared": {"value": false, "score": 1.5497207641601562e-06}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9768190979957581}, "created": {"value": false, "score": 0.0003229379653930664}, "shared": {"value": false, "score": 1.6689300537109375e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "BlenderBot", "normalizedForm": "BlenderBot", "offsetStart": 81, "offsetEnd": 91}, "context": "This result could suggest that 1) the pretraining of the models (i.e., DialoGPT, BlenderBot) on dialogue datasets may help to generate longer utterances, and therefore to improve the referencebased metrics performance, and 2) the autoregressive model (e.g., DialoGPT) may not be suitable for the generation of social dialogue such as hedges.", "mentionContextAttributes": {"used": {"value": false, "score": 0.019712209701538086}, "created": {"value": false, "score": 0.00013446807861328125}, "shared": {"value": false, "score": 1.5497207641601562e-06}}, "documentContextAttributes": {"used": {"value": true, "score": 0.994888961315155}, "created": {"value": false, "score": 0.0003229379653930664}, "shared": {"value": false, "score": 1.5497207641601562e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "DialoGPT", "normalizedForm": "DialoGPT", "offsetStart": 85, "offsetEnd": 93}, "context": "Additionally, we find some interesting similarities among models: 1) Blender-Bot and DialoGPT outperform BART in both the fine-tuning and the reranking methods (Table 2) with respect to reference-based metrics such as BLEU, ROUGE-L, etc., and 2) DialoGPT still underperforms the other two models in terms of F1 score, and in the reranking condition the gap widens. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.08047902584075928}, "created": {"value": false, "score": 2.0265579223632812e-06}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9768190979957581}, "created": {"value": false, "score": 0.0003229379653930664}, "shared": {"value": false, "score": 1.6689300537109375e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Pytorch-Lightning  library", "normalizedForm": "Pytorch-Lightning library", "offsetStart": 90, "offsetEnd": 116}, "context": "The implementation of all models was based on the Transformer library3 , in addition, the Pytorch-Lightning4 library was used for training control. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9951887130737305}, "created": {"value": false, "score": 0.000706791877746582}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9951887130737305}, "created": {"value": false, "score": 0.000706791877746582}, "shared": {"value": false, "score": 1.1920928955078125e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "DialoGPT", "normalizedForm": "DialoGPT", "offsetStart": 106, "offsetEnd": 114}, "context": "We use the base version of the BART model, the small version of BlenderBot, and also the small version of DialoGPT. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9710648655891418}, "created": {"value": false, "score": 3.528594970703125e-05}, "shared": {"value": false, "score": 4.76837158203125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9768190979957581}, "created": {"value": false, "score": 0.0003229379653930664}, "shared": {"value": false, "score": 1.6689300537109375e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "DialoGPT", "normalizedForm": "DialoGPT", "offsetStart": 123, "offsetEnd": 131}, "context": "We compare the performance of different state-ofthe-art (SOTA) free open-source pretrained models as our generators: BART, DialoGPT, and BlenderBot. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9734140634536743}, "created": {"value": false, "score": 0.0003229379653930664}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9768190979957581}, "created": {"value": false, "score": 0.0003229379653930664}, "shared": {"value": false, "score": 1.6689300537109375e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "DialoGPT", "normalizedForm": "DialoGPT", "offsetStart": 124, "offsetEnd": 153}, "context": "first applied reranking to conversational strategy generation by controlling the level of self-disclosure in the outputs of DialoGPT (Zhang et al., 2020b). ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9768190979957581}, "created": {"value": false, "score": 6.747245788574219e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9768190979957581}, "created": {"value": false, "score": 0.0003229379653930664}, "shared": {"value": false, "score": 1.6689300537109375e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "BlenderBot", "normalizedForm": "BlenderBot", "offsetStart": 137, "offsetEnd": 147}, "context": "We compare the performance of different state-ofthe-art (SOTA) free open-source pretrained models as our generators: BART, DialoGPT, and BlenderBot. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9734140634536743}, "created": {"value": false, "score": 0.0003229379653930664}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.994888961315155}, "created": {"value": false, "score": 0.0003229379653930664}, "shared": {"value": false, "score": 1.5497207641601562e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Blender-Bot", "normalizedForm": "Blender-Bot", "offsetStart": 191, "offsetEnd": 202}, "context": "To summarize results on the fine-tuning versus reranking approaches we observe that: 1) With the help of a hedge classifier, the reranking approach can do a good job at generating hedges, 2) Blender-Bot is better suited to the task of generating long utterances, as described in Section 5.1. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.006692469120025635}, "created": {"value": false, "score": 1.3232231140136719e-05}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "documentContextAttributes": {"used": {"value": false, "score": 0.08047902584075928}, "created": {"value": false, "score": 1.3232231140136719e-05}, "shared": {"value": false, "score": 3.5762786865234375e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "DialoGPT", "normalizedForm": "DialoGPT", "offsetStart": 246, "offsetEnd": 254}, "context": "Additionally, we find some interesting similarities among models: 1) Blender-Bot and DialoGPT outperform BART in both the fine-tuning and the reranking methods (Table 2) with respect to reference-based metrics such as BLEU, ROUGE-L, etc., and 2) DialoGPT still underperforms the other two models in terms of F1 score, and in the reranking condition the gap widens. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.08047902584075928}, "created": {"value": false, "score": 2.0265579223632812e-06}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9768190979957581}, "created": {"value": false, "score": 0.0003229379653930664}, "shared": {"value": false, "score": 1.6689300537109375e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "DialoGPT", "normalizedForm": "DialoGPT", "offsetStart": 258, "offsetEnd": 266}, "context": "This result could suggest that 1) the pretraining of the models (i.e., DialoGPT, BlenderBot) on dialogue datasets may help to generate longer utterances, and therefore to improve the referencebased metrics performance, and 2) the autoregressive model (e.g., DialoGPT) may not be suitable for the generation of social dialogue such as hedges.", "mentionContextAttributes": {"used": {"value": false, "score": 0.019712209701538086}, "created": {"value": false, "score": 0.00013446807861328125}, "shared": {"value": false, "score": 1.5497207641601562e-06}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9768190979957581}, "created": {"value": false, "score": 0.0003229379653930664}, "shared": {"value": false, "score": 1.6689300537109375e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "BlenderBot", "normalizedForm": "BlenderBot", "offsetStart": 280, "offsetEnd": 290}, "context": "We find that an implicit fine-tuning approach (i.e., without any supervision by a hedge label) is not sufficient for generating hedges, but a reranking method significantly improves performance in generating hedges, with a final F1 score of .85 for the BART model and .84 for the BlenderBot model.", "mentionContextAttributes": {"used": {"value": false, "score": 0.023179292678833008}, "created": {"value": false, "score": 1.1563301086425781e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.994888961315155}, "created": {"value": false, "score": 0.0003229379653930664}, "shared": {"value": false, "score": 1.5497207641601562e-06}}}], "references": [], "runtime": 13191, "id": "507fe6b6bbf3e17e58deb31069afc6b97d520721", "metadata": {"id": "507fe6b6bbf3e17e58deb31069afc6b97d520721"}, "original_file_path": "../../datalake/Samuel/SOFTware-Sync/downloads/xml/hal-04143015.grobid.tei.xml", "file_name": "hal-04143015.grobid.tei.xml"}