{"application": "software-mentions", "version": "0.8.0", "date": "2024-10-07T12:04+0000", "md5": "71B672ECBA2AD46500B118B0393CAFF8", "mentions": [{"type": "software", "software-type": "software", "software-name": {"rawForm": "COMET", "normalizedForm": "COMET", "offsetStart": 0, "offsetEnd": 5}, "context": "COMET results are given in Table 5 and we include a fuller analysis for COMET-QE results in Table 8 in Appendix E. Note that we only include annotation types that appear in at least 50 sentences, and that the 'all' column refers to the scores over all sentences and not just the ones annotated for UGC phenomena.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9924114942550659}, "created": {"value": false, "score": 2.9802322387695312e-06}, "shared": {"value": false, "score": 5.960464477539062e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9998440742492676}, "created": {"value": true, "score": 0.5016334056854248}, "shared": {"value": false, "score": 5.960464477539062e-07}}, "references": [{"label": "(Rei et al., 2020)", "normalizedForm": "Rei et al., 2020", "refKey": 21}, {"label": "(Rei et al., 2020)", "normalizedForm": "Rei et al., 2020", "refKey": 21}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "spaCy", "normalizedForm": "spaCy", "offsetStart": 4, "offsetEnd": 9}, "context": "The spaCy segmentation was obtained by concatenating all normalised sentences from a single text and then automatically splitting.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9999409914016724}, "created": {"value": false, "score": 4.649162292480469e-06}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999409914016724}, "created": {"value": false, "score": 0.4721764326095581}, "shared": {"value": false, "score": 1.430511474609375e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "COMET", "normalizedForm": "COMET", "offsetStart": 4, "offsetEnd": 9}, "context": "For COMET and COMET-QE, which also use the source sentence, we choose to evaluate system outputs against both the manseg-norm and manseg-raw source sentences, regardless of which set was translated by the system and take the highest score of all the source-reference combinations.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9680407047271729}, "created": {"value": false, "score": 1.4424324035644531e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9998440742492676}, "created": {"value": true, "score": 0.5016334056854248}, "shared": {"value": false, "score": 5.960464477539062e-07}}, "references": [{"label": "(Rei et al., 2020)", "normalizedForm": "Rei et al., 2020", "refKey": 21}, {"label": "(Rei et al., 2020)", "normalizedForm": "Rei et al., 2020", "refKey": 21}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "CUNI-DocTrans", "normalizedForm": "CUNI-DocTrans", "offsetStart": 6, "offsetEnd": 19}, "context": "------CUNI-DocTrans 28.9 18.0 10.9 ------CUNI-GA 28.9 18.0 10.9 ------Table 7: BLEU scores of systems on the manseg-norm and manseg-raw subsets. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.8466238379478455}, "created": {"value": false, "score": 2.9802322387695312e-06}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9961813688278198}, "created": {"value": false, "score": 4.649162292480469e-06}, "shared": {"value": false, "score": 4.76837158203125e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "COMET", "normalizedForm": "COMET", "offsetStart": 9, "offsetEnd": 14}, "context": "Table 4: COMET-QE scores of systems on the manseg-norm and manseg-raw subsets.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9189594388008118}, "created": {"value": false, "score": 5.9604644775390625e-06}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9998440742492676}, "created": {"value": true, "score": 0.5016334056854248}, "shared": {"value": false, "score": 5.960464477539062e-07}}, "references": [{"label": "(Rei et al., 2020)", "normalizedForm": "Rei et al., 2020", "refKey": 21}, {"label": "(Rei et al., 2020)", "normalizedForm": "Rei et al., 2020", "refKey": 21}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "COMET-QE", "normalizedForm": "COMET-QE", "offsetStart": 14, "offsetEnd": 22}, "context": "For COMET and COMET-QE, which also use the source sentence, we choose to evaluate system outputs against both the manseg-norm and manseg-raw source sentences, regardless of which set was translated by the system and take the highest score of all the source-reference combinations. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9680407047271729}, "created": {"value": false, "score": 1.4424324035644531e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9680407047271729}, "created": {"value": false, "score": 0.2998555302619934}, "shared": {"value": false, "score": 4.76837158203125e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "spaCy", "normalizedForm": "spaCy", "offsetStart": 16, "offsetEnd": 21}, "context": "\u2022 spacyseg-raw: spaCy segmentation with original (raw) text", "mentionContextAttributes": {"used": {"value": false, "score": 0.1327117681503296}, "created": {"value": false, "score": 2.9802322387695312e-06}, "shared": {"value": false, "score": 9.5367431640625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999409914016724}, "created": {"value": false, "score": 0.4721764326095581}, "shared": {"value": false, "score": 1.430511474609375e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "spaCy", "normalizedForm": "spaCy", "offsetStart": 17, "offsetEnd": 22}, "context": "\u2022 spacyseg-norm: spaCy segmentation with manual normalisation10", "mentionContextAttributes": {"used": {"value": false, "score": 0.0020388364791870117}, "created": {"value": false, "score": 2.5033950805664062e-06}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999409914016724}, "created": {"value": false, "score": 0.4721764326095581}, "shared": {"value": false, "score": 1.430511474609375e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "COMET", "normalizedForm": "COMET", "offsetStart": 18, "offsetEnd": 23}, "context": "The comparison of COMET and COMET-QE metrics suggest that it may be possible to draw similar conclusions from automatic scoring without using references, although future work could go into more depth into analysing what is captured by the different metrics.", "mentionContextAttributes": {"used": {"value": false, "score": 0.009729146957397461}, "created": {"value": false, "score": 0.00016820430755615234}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9998440742492676}, "created": {"value": true, "score": 0.5016334056854248}, "shared": {"value": false, "score": 5.960464477539062e-07}}, "references": [{"label": "(Rei et al., 2020)", "normalizedForm": "Rei et al., 2020", "refKey": 21}, {"label": "(Rei et al., 2020)", "normalizedForm": "Rei et al., 2020", "refKey": 21}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "COMET", "normalizedForm": "COMET", "offsetStart": 19, "offsetEnd": 24}, "context": "On the other hand, COMET-QE scores show more similar trends to COMET, suggesting that it could be possible to use it to evaluate without having to produce reference translations.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9956580400466919}, "created": {"value": false, "score": 1.6927719116210938e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9998440742492676}, "created": {"value": true, "score": 0.5016334056854248}, "shared": {"value": false, "score": 5.960464477539062e-07}}, "references": [{"label": "(Rei et al., 2020)", "normalizedForm": "Rei et al., 2020", "refKey": 21}, {"label": "(Rei et al., 2020)", "normalizedForm": "Rei et al., 2020", "refKey": 21}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "COMET", "normalizedForm": "COMET", "offsetStart": 22, "offsetEnd": 27}, "context": "We provide in Table 8 COMET-QE scores per annotation type for all from-English language pairs of the shared task.", "mentionContextAttributes": {"used": {"value": false, "score": 0.37126588821411133}, "created": {"value": false, "score": 0.00024050474166870117}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9998440742492676}, "created": {"value": true, "score": 0.5016334056854248}, "shared": {"value": false, "score": 5.960464477539062e-07}}, "references": [{"label": "(Rei et al., 2020)", "normalizedForm": "Rei et al., 2020", "refKey": 21}, {"label": "(Rei et al., 2020)", "normalizedForm": "Rei et al., 2020", "refKey": 21}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "COMET", "normalizedForm": "COMET", "offsetStart": 25, "offsetEnd": 30}, "context": "We nevertheless add that COMET remains an automatic metric that does not produce perfect correlation with human judgments, more research would be necessary to stress-test the metric for MT robustness evaluation, particularly in terms of evaluating which of COMET and COMET-QE is better correlated with human judgments.", "mentionContextAttributes": {"used": {"value": false, "score": 0.08937215805053711}, "created": {"value": false, "score": 0.0011638998985290527}, "shared": {"value": false, "score": 5.960464477539062e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9998440742492676}, "created": {"value": true, "score": 0.5016334056854248}, "shared": {"value": false, "score": 5.960464477539062e-07}}, "references": [{"label": "(Rei et al., 2020)", "normalizedForm": "Rei et al., 2020", "refKey": 21}, {"label": "(Rei et al., 2020)", "normalizedForm": "Rei et al., 2020", "refKey": 21}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "COMET", "normalizedForm": "COMET", "offsetStart": 28, "offsetEnd": 33}, "context": "We provide full results for COMET and COMET-QE in Table 3 and 4 respectively, and we include results for BLEU in Table 7 in Appendix D.", "mentionContextAttributes": {"used": {"value": true, "score": 0.8907544612884521}, "created": {"value": false, "score": 0.00010251998901367188}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9998440742492676}, "created": {"value": true, "score": 0.5016334056854248}, "shared": {"value": false, "score": 5.960464477539062e-07}}, "references": [{"label": "(Rei et al., 2020)", "normalizedForm": "Rei et al., 2020", "refKey": 21}, {"label": "(Rei et al., 2020)", "normalizedForm": "Rei et al., 2020", "refKey": 21}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "COMET", "normalizedForm": "COMET", "offsetStart": 28, "offsetEnd": 33}, "context": "The comparison of COMET and COMET-QE metrics suggest that it may be possible to draw similar conclusions from automatic scoring without using references, although future work could go into more depth into analysing what is captured by the different metrics.", "mentionContextAttributes": {"used": {"value": false, "score": 0.009729146957397461}, "created": {"value": false, "score": 0.00016820430755615234}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9998440742492676}, "created": {"value": true, "score": 0.5016334056854248}, "shared": {"value": false, "score": 5.960464477539062e-07}}, "references": [{"label": "(Rei et al., 2020)", "normalizedForm": "Rei et al., 2020", "refKey": 21}, {"label": "(Rei et al., 2020)", "normalizedForm": "Rei et al., 2020", "refKey": 21}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "COMET", "normalizedForm": "COMET", "offsetStart": 37, "offsetEnd": 42}, "context": "12 We compare BLEU to referencebased COMET (Rei et al., 2020)  13 for those language pairs for which we have a reference, and to COMET's reference-less (quality estimation) version, which we refer to as COMET-QE (Rei et al., 2022).", "mentionContextAttributes": {"used": {"value": true, "score": 0.7890669703483582}, "created": {"value": false, "score": 2.7418136596679688e-06}, "shared": {"value": false, "score": 4.76837158203125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9998440742492676}, "created": {"value": true, "score": 0.5016334056854248}, "shared": {"value": false, "score": 5.960464477539062e-07}}, "references": [{"label": "(Rei et al., 2020)", "normalizedForm": "Rei et al., 2020", "refKey": 21, "offsetStart": 19883, "offsetEnd": 19901}, {"label": "(Rei et al., 2020)", "normalizedForm": "Rei et al., 2020", "refKey": 21, "offsetStart": 19883, "offsetEnd": 19901}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "COMET", "normalizedForm": "COMET", "offsetStart": 38, "offsetEnd": 43}, "context": "We provide full results for COMET and COMET-QE in Table 3 and 4 respectively, and we include results for BLEU in Table 7 in Appendix D.", "mentionContextAttributes": {"used": {"value": true, "score": 0.8907544612884521}, "created": {"value": false, "score": 0.00010251998901367188}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9998440742492676}, "created": {"value": true, "score": 0.5016334056854248}, "shared": {"value": false, "score": 5.960464477539062e-07}}, "references": [{"label": "(Rei et al., 2020)", "normalizedForm": "Rei et al., 2020", "refKey": 21}, {"label": "(Rei et al., 2020)", "normalizedForm": "Rei et al., 2020", "refKey": 21}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "COMET", "normalizedForm": "COMET", "offsetStart": 38, "offsetEnd": 43}, "context": "The results are provided (as with the COMET scores in the main part of the paper) for the original raw subset (manseg-raw) and for its normalised version ((manseg-norm)) as well as the difference between the two scores (\u03b4).", "mentionContextAttributes": {"used": {"value": true, "score": 0.9887131452560425}, "created": {"value": false, "score": 1.8596649169921875e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9998440742492676}, "created": {"value": true, "score": 0.5016334056854248}, "shared": {"value": false, "score": 5.960464477539062e-07}}, "references": [{"label": "(Rei et al., 2020)", "normalizedForm": "Rei et al., 2020", "refKey": 21}, {"label": "(Rei et al., 2020)", "normalizedForm": "Rei et al., 2020", "refKey": 21}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "COMET", "normalizedForm": "COMET", "offsetStart": 39, "offsetEnd": 44}, "context": "We test three different metrics (BLEU, COMET and COMET-QE) to evaluate the systems' translations of RoCS-MT, looking at how coherent they are between each other, and whether it is possible to use quality estimation to evaluate MT robustness in order to remove the need for reference translations (Section 5.1).", "mentionContextAttributes": {"used": {"value": true, "score": 0.5712105631828308}, "created": {"value": false, "score": 7.271766662597656e-06}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9998440742492676}, "created": {"value": true, "score": 0.5016334056854248}, "shared": {"value": false, "score": 5.960464477539062e-07}}, "references": [{"label": "(Rei et al., 2020)", "normalizedForm": "Rei et al., 2020", "refKey": 21}, {"label": "(Rei et al., 2020)", "normalizedForm": "Rei et al., 2020", "refKey": 21}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "CUNI-GA", "normalizedForm": "CUNI-GA", "offsetStart": 41, "offsetEnd": 48}, "context": "------CUNI-DocTrans 28.9 18.0 10.9 ------CUNI-GA 28.9 18.0 10.9 ------Table 7: BLEU scores of systems on the manseg-norm and manseg-raw subsets. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.8466238379478455}, "created": {"value": false, "score": 2.9802322387695312e-06}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.8466238379478455}, "created": {"value": false, "score": 2.9802322387695312e-06}, "shared": {"value": false, "score": 1.1920928955078125e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "COMET", "normalizedForm": "COMET", "offsetStart": 48, "offsetEnd": 53}, "context": "For these sentences, we choose to take the best COMET score of the two references.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9998440742492676}, "created": {"value": false, "score": 0.00018066167831420898}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9998440742492676}, "created": {"value": true, "score": 0.5016334056854248}, "shared": {"value": false, "score": 5.960464477539062e-07}}, "references": [{"label": "(Rei et al., 2020)", "normalizedForm": "Rei et al., 2020", "refKey": 21}, {"label": "(Rei et al., 2020)", "normalizedForm": "Rei et al., 2020", "refKey": 21}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "COMET", "normalizedForm": "COMET", "offsetStart": 49, "offsetEnd": 54}, "context": "We test three different metrics (BLEU, COMET and COMET-QE) to evaluate the systems' translations of RoCS-MT, looking at how coherent they are between each other, and whether it is possible to use quality estimation to evaluate MT robustness in order to remove the need for reference translations (Section 5.1).", "mentionContextAttributes": {"used": {"value": true, "score": 0.5712105631828308}, "created": {"value": false, "score": 7.271766662597656e-06}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9998440742492676}, "created": {"value": true, "score": 0.5016334056854248}, "shared": {"value": false, "score": 5.960464477539062e-07}}, "references": [{"label": "(Rei et al., 2020)", "normalizedForm": "Rei et al., 2020", "refKey": 21}, {"label": "(Rei et al., 2020)", "normalizedForm": "Rei et al., 2020", "refKey": 21}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "COMET-QE", "normalizedForm": "COMET-QE", "offsetStart": 56, "offsetEnd": 64}, "context": "14 We notably aim to test whether it is possible to use COMET-QE for evaluation rather than reference-based COMET, which would remove the dependency on reference translations and make evaluation possible for a wider range of languages.", "mentionContextAttributes": {"used": {"value": false, "score": 0.0030786991119384766}, "created": {"value": false, "score": 0.2998555302619934}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9680407047271729}, "created": {"value": false, "score": 0.2998555302619934}, "shared": {"value": false, "score": 4.76837158203125e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "COMET", "normalizedForm": "COMET", "offsetStart": 60, "offsetEnd": 65}, "context": "For example GPT4 is ranked above other systems by COMET and COMET-QE, whereas the BLEU scores of other systems (and in particular ONLINE-W and sometimes ONLINE-B) are higher.", "mentionContextAttributes": {"used": {"value": false, "score": 0.01314997673034668}, "created": {"value": false, "score": 4.4465065002441406e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9998440742492676}, "created": {"value": true, "score": 0.5016334056854248}, "shared": {"value": false, "score": 5.960464477539062e-07}}, "references": [{"label": "(Rei et al., 2020)", "normalizedForm": "Rei et al., 2020", "refKey": 21}, {"label": "(Rei et al., 2020)", "normalizedForm": "Rei et al., 2020", "refKey": 21}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "COMET", "normalizedForm": "COMET", "offsetStart": 63, "offsetEnd": 68}, "context": "On the other hand, COMET-QE scores show more similar trends to COMET, suggesting that it could be possible to use it to evaluate without having to produce reference translations.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9956580400466919}, "created": {"value": false, "score": 1.6927719116210938e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9998440742492676}, "created": {"value": true, "score": 0.5016334056854248}, "shared": {"value": false, "score": 5.960464477539062e-07}}, "references": [{"label": "(Rei et al., 2020)", "normalizedForm": "Rei et al., 2020", "refKey": 21}, {"label": "(Rei et al., 2020)", "normalizedForm": "Rei et al., 2020", "refKey": 21}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "COMET", "normalizedForm": "COMET", "offsetStart": 71, "offsetEnd": 76}, "context": "However, there are some clear inconsistencies between BLEU and the two COMET metrics when evaluating nonstandard data (manseg-raw).", "mentionContextAttributes": {"used": {"value": false, "score": 0.011391282081604004}, "created": {"value": false, "score": 1.1086463928222656e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9998440742492676}, "created": {"value": true, "score": 0.5016334056854248}, "shared": {"value": false, "score": 5.960464477539062e-07}}, "references": [{"label": "(Rei et al., 2020)", "normalizedForm": "Rei et al., 2020", "refKey": 21}, {"label": "(Rei et al., 2020)", "normalizedForm": "Rei et al., 2020", "refKey": 21}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "COMET", "normalizedForm": "COMET", "offsetStart": 72, "offsetEnd": 77}, "context": "COMET results are given in Table 5 and we include a fuller analysis for COMET-QE results in Table 8 in Appendix E. Note that we only include annotation types that appear in at least 50 sentences, and that the 'all' column refers to the scores over all sentences and not just the ones annotated for UGC phenomena.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9924114942550659}, "created": {"value": false, "score": 2.9802322387695312e-06}, "shared": {"value": false, "score": 5.960464477539062e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9998440742492676}, "created": {"value": true, "score": 0.5016334056854248}, "shared": {"value": false, "score": 5.960464477539062e-07}}, "references": [{"label": "(Rei et al., 2020)", "normalizedForm": "Rei et al., 2020", "refKey": 21}, {"label": "(Rei et al., 2020)", "normalizedForm": "Rei et al., 2020", "refKey": 21}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "COMET", "normalizedForm": "COMET", "offsetStart": 76, "offsetEnd": 81}, "context": "We also look at the MT quality of each system per phenomenon by calculating COMET scores over subsets of the data.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9963541030883789}, "created": {"value": false, "score": 0.0031906962394714355}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9998440742492676}, "created": {"value": true, "score": 0.5016334056854248}, "shared": {"value": false, "score": 5.960464477539062e-07}}, "references": [{"label": "(Rei et al., 2020)", "normalizedForm": "Rei et al., 2020", "refKey": 21}, {"label": "(Rei et al., 2020)", "normalizedForm": "Rei et al., 2020", "refKey": 21}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "CUNI-Transformer", "normalizedForm": "CUNI-Transformer", "offsetStart": 78, "offsetEnd": 94}, "context": "The constrained systems submitted were AIRC (Rikters and Miwa, 2023), ANVITA, CUNI-Transformer and CUNI-DocTransformer (Popel, 2020) (we refer to these system as CUNI-Trans and CUNI-DocTrans to save space in the results tables), CUNI-GA (Jon et al., 2023), HW-TSC (Wu et al., 2023b), IOL_Research (Zhang, 2023), NAIST-NICT (Deguchi et al., 2023), Sam-sung_Research_Philippines (Cruz, 2023) (hereafter Samsung_RP), SKIM (Kudo et al., 2023) and UvA-LTL (Wu et al., 2023a).", "mentionContextAttributes": {"used": {"value": true, "score": 0.9961812496185303}, "created": {"value": false, "score": 4.649162292480469e-06}, "shared": {"value": false, "score": 4.76837158203125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9961812496185303}, "created": {"value": false, "score": 4.649162292480469e-06}, "shared": {"value": false, "score": 4.76837158203125e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "COMET", "normalizedForm": "COMET", "offsetStart": 79, "offsetEnd": 84}, "context": "While BLEU is designed to handle multiple references, this is not inbuilt into COMET.", "mentionContextAttributes": {"used": {"value": false, "score": 0.00012433528900146484}, "created": {"value": true, "score": 0.5016334056854248}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9998440742492676}, "created": {"value": true, "score": 0.5016334056854248}, "shared": {"value": false, "score": 5.960464477539062e-07}}, "references": [{"label": "(Rei et al., 2020)", "normalizedForm": "Rei et al., 2020", "refKey": 21}, {"label": "(Rei et al., 2020)", "normalizedForm": "Rei et al., 2020", "refKey": 21}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "spaCy", "normalizedForm": "spaCy", "offsetStart": 97, "offsetEnd": 102}, "context": "Impact of sentence splitting While the number of sentences is fixed for the manual segmentation, spaCy segmentation is highly dependent on whether the text has been normalised or not, likely due to the tool being less well adapted to nonstandard text; when applied to raw text, the resulting number of sentences is far lower than manual segmentation (1660 vs. 1922), whereas the resulting number of sentences is more similar to manual segmentation when applied to the normalised text.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9676637053489685}, "created": {"value": false, "score": 4.601478576660156e-05}, "shared": {"value": false, "score": 1.430511474609375e-06}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999409914016724}, "created": {"value": false, "score": 0.4721764326095581}, "shared": {"value": false, "score": 1.430511474609375e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "CUNI-DocTransformer", "normalizedForm": "CUNI-DocTransformer", "offsetStart": 99, "offsetEnd": 132}, "context": "The constrained systems submitted were AIRC (Rikters and Miwa, 2023), ANVITA, CUNI-Transformer and CUNI-DocTransformer (Popel, 2020) (we refer to these system as CUNI-Trans and CUNI-DocTrans to save space in the results tables), CUNI-GA (Jon et al., 2023), HW-TSC (Wu et al., 2023b), IOL_Research (Zhang, 2023), NAIST-NICT (Deguchi et al., 2023), Sam-sung_Research_Philippines (Cruz, 2023) (hereafter Samsung_RP), SKIM (Kudo et al., 2023) and UvA-LTL (Wu et al., 2023a).", "mentionContextAttributes": {"used": {"value": true, "score": 0.9961812496185303}, "created": {"value": false, "score": 4.649162292480469e-06}, "shared": {"value": false, "score": 4.76837158203125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9961812496185303}, "created": {"value": false, "score": 4.649162292480469e-06}, "shared": {"value": false, "score": 4.76837158203125e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "COMET", "normalizedForm": "COMET", "offsetStart": 108, "offsetEnd": 113}, "context": "14 We notably aim to test whether it is possible to use COMET-QE for evaluation rather than reference-based COMET, which would remove the dependency on reference translations and make evaluation possible for a wider range of languages.", "mentionContextAttributes": {"used": {"value": false, "score": 0.0030786991119384766}, "created": {"value": false, "score": 0.2998555302619934}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9998440742492676}, "created": {"value": true, "score": 0.5016334056854248}, "shared": {"value": false, "score": 5.960464477539062e-07}}, "references": [{"label": "(Rei et al., 2020)", "normalizedForm": "Rei et al., 2020", "refKey": 21}, {"label": "(Rei et al., 2020)", "normalizedForm": "Rei et al., 2020", "refKey": 21}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "COMET", "normalizedForm": "COMET", "offsetStart": 108, "offsetEnd": 113}, "context": "The highest performing systems are the unconstrained online systems, with GPT4 getting significantly higher COMET and COMET-QE scores than other systems when translating non-standard (raw) text for all languages tested.", "mentionContextAttributes": {"used": {"value": false, "score": 0.17106980085372925}, "created": {"value": false, "score": 8.58306884765625e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9998440742492676}, "created": {"value": true, "score": 0.5016334056854248}, "shared": {"value": false, "score": 5.960464477539062e-07}}, "references": [{"label": "(Rei et al., 2020)", "normalizedForm": "Rei et al., 2020", "refKey": 21}, {"label": "(Rei et al., 2020)", "normalizedForm": "Rei et al., 2020", "refKey": 21}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "spaCy", "normalizedForm": "spaCy", "offsetStart": 115, "offsetEnd": 120}, "context": "We create four subsets of the challenge set to test the impact of sentence segmentation (manual or automatic using spaCy) and of normalisation (manual or none, i.e. the original raw text):", "mentionContextAttributes": {"used": {"value": true, "score": 0.594176709651947}, "created": {"value": false, "score": 0.4721764326095581}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999409914016724}, "created": {"value": false, "score": 0.4721764326095581}, "shared": {"value": false, "score": 1.430511474609375e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "COMET", "normalizedForm": "COMET", "offsetStart": 118, "offsetEnd": 123}, "context": "The highest performing systems are the unconstrained online systems, with GPT4 getting significantly higher COMET and COMET-QE scores than other systems when translating non-standard (raw) text for all languages tested.", "mentionContextAttributes": {"used": {"value": false, "score": 0.17106980085372925}, "created": {"value": false, "score": 8.58306884765625e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9998440742492676}, "created": {"value": true, "score": 0.5016334056854248}, "shared": {"value": false, "score": 5.960464477539062e-07}}, "references": [{"label": "(Rei et al., 2020)", "normalizedForm": "Rei et al., 2020", "refKey": 21}, {"label": "(Rei et al., 2020)", "normalizedForm": "Rei et al., 2020", "refKey": 21}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "COMET", "normalizedForm": "COMET", "offsetStart": 129, "offsetEnd": 134}, "context": "12 We compare BLEU to referencebased COMET (Rei et al., 2020)  13 for those language pairs for which we have a reference, and to COMET's reference-less (quality estimation) version, which we refer to as COMET-QE (Rei et al., 2022). ", "mentionContextAttributes": {"used": {"value": true, "score": 0.789063572883606}, "created": {"value": false, "score": 2.7418136596679688e-06}, "shared": {"value": false, "score": 4.76837158203125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9998440742492676}, "created": {"value": true, "score": 0.5016334056854248}, "shared": {"value": false, "score": 5.960464477539062e-07}}, "references": [{"label": "(Rei et al., 2020)", "normalizedForm": "Rei et al., 2020", "refKey": 21}, {"label": "(Rei et al., 2020)", "normalizedForm": "Rei et al., 2020", "refKey": 21}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "COMET", "normalizedForm": "COMET", "offsetStart": 131, "offsetEnd": 136}, "context": "In order to analyse how systems handle different non-standard phenomena, we evaluate sentences by annotation types, by calculating COMET and COMET-QE scores for sentences containing at least one occurrence of a particular normalisation annotation.", "mentionContextAttributes": {"used": {"value": true, "score": 0.7578834295272827}, "created": {"value": false, "score": 0.0001049041748046875}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9998440742492676}, "created": {"value": true, "score": 0.5016334056854248}, "shared": {"value": false, "score": 5.960464477539062e-07}}, "references": [{"label": "(Rei et al., 2020)", "normalizedForm": "Rei et al., 2020", "refKey": 21}, {"label": "(Rei et al., 2020)", "normalizedForm": "Rei et al., 2020", "refKey": 21}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "COMET", "normalizedForm": "COMET", "offsetStart": 141, "offsetEnd": 146}, "context": "In order to analyse how systems handle different non-standard phenomena, we evaluate sentences by annotation types, by calculating COMET and COMET-QE scores for sentences containing at least one occurrence of a particular normalisation annotation.", "mentionContextAttributes": {"used": {"value": true, "score": 0.7578834295272827}, "created": {"value": false, "score": 0.0001049041748046875}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9998440742492676}, "created": {"value": true, "score": 0.5016334056854248}, "shared": {"value": false, "score": 5.960464477539062e-07}}, "references": [{"label": "(Rei et al., 2020)", "normalizedForm": "Rei et al., 2020", "refKey": 21}, {"label": "(Rei et al., 2020)", "normalizedForm": "Rei et al., 2020", "refKey": 21}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "spaCy", "normalizedForm": "spaCy", "offsetStart": 149, "offsetEnd": 154}, "context": "As shown in Section 3.3, the two different segmentation methods result in different numbers of individual sentences, and automatic segmentation with spaCy differs depending on whether the text has been normalised or not.", "mentionContextAttributes": {"used": {"value": false, "score": 0.0016724467277526855}, "created": {"value": false, "score": 4.00543212890625e-05}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999409914016724}, "created": {"value": false, "score": 0.4721764326095581}, "shared": {"value": false, "score": 1.430511474609375e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "-Trans", "normalizedForm": "-Trans", "offsetStart": 166, "offsetEnd": 172}, "context": "The constrained systems submitted were AIRC (Rikters and Miwa, 2023), ANVITA, CUNI-Transformer and CUNI-DocTransformer (Popel, 2020) (we refer to these system as CUNI-Trans and CUNI-DocTrans to save space in the results tables), CUNI-GA (Jon et al., 2023), HW-TSC (Wu et al., 2023b), IOL_Research (Zhang, 2023), NAIST-NICT (Deguchi et al., 2023), Sam-sung_Research_Philippines (Cruz, 2023) (hereafter Samsung_RP), SKIM (Kudo et al., 2023) and UvA-LTL (Wu et al., 2023a).", "mentionContextAttributes": {"used": {"value": true, "score": 0.9961813688278198}, "created": {"value": false, "score": 4.649162292480469e-06}, "shared": {"value": false, "score": 4.76837158203125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9961813688278198}, "created": {"value": false, "score": 4.649162292480469e-06}, "shared": {"value": false, "score": 4.76837158203125e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "CUNI-DocTrans", "normalizedForm": "CUNI-DocTrans", "offsetStart": 177, "offsetEnd": 190}, "context": "The constrained systems submitted were AIRC (Rikters and Miwa, 2023), ANVITA, CUNI-Transformer and CUNI-DocTransformer (Popel, 2020) (we refer to these system as CUNI-Trans and CUNI-DocTrans to save space in the results tables), CUNI-GA (Jon et al., 2023), HW-TSC (Wu et al., 2023b), IOL_Research (Zhang, 2023), NAIST-NICT (Deguchi et al., 2023), Sam-sung_Research_Philippines (Cruz, 2023) (hereafter Samsung_RP), SKIM (Kudo et al., 2023) and UvA-LTL (Wu et al., 2023a).", "mentionContextAttributes": {"used": {"value": true, "score": 0.9961813688278198}, "created": {"value": false, "score": 4.649162292480469e-06}, "shared": {"value": false, "score": 4.76837158203125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9961813688278198}, "created": {"value": false, "score": 4.649162292480469e-06}, "shared": {"value": false, "score": 4.76837158203125e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Google Translate", "normalizedForm": "Google Translate", "offsetStart": 179, "offsetEnd": 195}, "context": "translators should not use machine translation systems or other computational systems to aid translation as this could bias the translations to look like translations produced by Google Translate, DeepL, ChatGPT, etc.", "mentionContextAttributes": {"used": {"value": false, "score": 0.0006266236305236816}, "created": {"value": false, "score": 0.00011837482452392578}, "shared": {"value": false, "score": 3.2186508178710938e-06}}, "documentContextAttributes": {"used": {"value": false, "score": 0.0006266236305236816}, "created": {"value": false, "score": 0.00011837482452392578}, "shared": {"value": false, "score": 3.2186508178710938e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "DeepL", "normalizedForm": "DeepL", "offsetStart": 197, "offsetEnd": 202}, "context": "translators should not use machine translation systems or other computational systems to aid translation as this could bias the translations to look like translations produced by Google Translate, DeepL, ChatGPT, etc.", "mentionContextAttributes": {"used": {"value": false, "score": 0.0006266236305236816}, "created": {"value": false, "score": 0.00011837482452392578}, "shared": {"value": false, "score": 3.2186508178710938e-06}}, "documentContextAttributes": {"used": {"value": false, "score": 0.0006266236305236816}, "created": {"value": false, "score": 0.00011837482452392578}, "shared": {"value": false, "score": 3.2186508178710938e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "COMET-QE", "normalizedForm": "COMET-QE", "offsetStart": 203, "offsetEnd": 229}, "context": "12 We compare BLEU to referencebased COMET (Rei et al., 2020)  13 for those language pairs for which we have a reference, and to COMET's reference-less (quality estimation) version, which we refer to as COMET-QE (Rei et al., 2022). ", "mentionContextAttributes": {"used": {"value": true, "score": 0.789063572883606}, "created": {"value": false, "score": 2.7418136596679688e-06}, "shared": {"value": false, "score": 4.76837158203125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9680407047271729}, "created": {"value": false, "score": 0.2998555302619934}, "shared": {"value": false, "score": 4.76837158203125e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "ChatGPT", "normalizedForm": "ChatGPT", "offsetStart": 204, "offsetEnd": 211}, "context": "translators should not use machine translation systems or other computational systems to aid translation as this could bias the translations to look like translations produced by Google Translate, DeepL, ChatGPT, etc.", "mentionContextAttributes": {"used": {"value": false, "score": 0.0006266236305236816}, "created": {"value": false, "score": 0.00011837482452392578}, "shared": {"value": false, "score": 3.2186508178710938e-06}}, "documentContextAttributes": {"used": {"value": false, "score": 0.0006266236305236816}, "created": {"value": false, "score": 0.00011837482452392578}, "shared": {"value": false, "score": 3.2186508178710938e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "COMET", "normalizedForm": "COMET", "offsetStart": 243, "offsetEnd": 248}, "context": "This indicates that GPT4 outputs are more surfacically different from the reference translations, which could be a result of paraphrasing or non-standard translations rather than a reflection of MT quality, especially given the high scores by COMET.", "mentionContextAttributes": {"used": {"value": false, "score": 0.20853424072265625}, "created": {"value": false, "score": 1.0371208190917969e-05}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9998440742492676}, "created": {"value": true, "score": 0.5016334056854248}, "shared": {"value": false, "score": 5.960464477539062e-07}}, "references": [{"label": "(Rei et al., 2020)", "normalizedForm": "Rei et al., 2020", "refKey": 21}, {"label": "(Rei et al., 2020)", "normalizedForm": "Rei et al., 2020", "refKey": 21}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "COMET", "normalizedForm": "COMET", "offsetStart": 257, "offsetEnd": 262}, "context": "We nevertheless add that COMET remains an automatic metric that does not produce perfect correlation with human judgments, more research would be necessary to stress-test the metric for MT robustness evaluation, particularly in terms of evaluating which of COMET and COMET-QE is better correlated with human judgments.", "mentionContextAttributes": {"used": {"value": false, "score": 0.08937227725982666}, "created": {"value": false, "score": 0.0011638998985290527}, "shared": {"value": false, "score": 5.960464477539062e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9998440742492676}, "created": {"value": true, "score": 0.5016334056854248}, "shared": {"value": false, "score": 5.960464477539062e-07}}, "references": [{"label": "(Rei et al., 2020)", "normalizedForm": "Rei et al., 2020", "refKey": 21}, {"label": "(Rei et al., 2020)", "normalizedForm": "Rei et al., 2020", "refKey": 21}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "COMET", "normalizedForm": "COMET", "offsetStart": 267, "offsetEnd": 272}, "context": "We nevertheless add that COMET remains an automatic metric that does not produce perfect correlation with human judgments, more research would be necessary to stress-test the metric for MT robustness evaluation, particularly in terms of evaluating which of COMET and COMET-QE is better correlated with human judgments.", "mentionContextAttributes": {"used": {"value": false, "score": 0.08937227725982666}, "created": {"value": false, "score": 0.0011638998985290527}, "shared": {"value": false, "score": 5.960464477539062e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9998440742492676}, "created": {"value": true, "score": 0.5016334056854248}, "shared": {"value": false, "score": 5.960464477539062e-07}}, "references": [{"label": "(Rei et al., 2020)", "normalizedForm": "Rei et al., 2020", "refKey": 21}, {"label": "(Rei et al., 2020)", "normalizedForm": "Rei et al., 2020", "refKey": 21}]}], "references": [{"refKey": 21, "tei": "<biblStruct xml:id=\"b21\">\n\t<analytic>\n\t\t<title level=\"a\" type=\"main\">COMET: A Neural Framework for MT Evaluation</title>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Ricardo</forename><surname>Rei</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Craig</forename><surname>Stewart</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Ana</forename><forename type=\"middle\">C</forename><surname>Farinha</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Alon</forename><surname>Lavie</surname></persName>\n\t\t</author>\n\t\t<idno type=\"DOI\">10.18653/v1/2020.emnlp-main.213</idno>\n\t</analytic>\n\t<monogr>\n\t\t<title level=\"m\">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>\n\t\t<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>\n\t\t<imprint>\n\t\t\t<publisher>Association for Computational Linguistics</publisher>\n\t\t\t<date type=\"published\" when=\"2020\">2020</date>\n\t\t\t<biblScope unit=\"page\">2702</biblScope>\n\t\t</imprint>\n\t</monogr>\n</biblStruct>\n"}], "runtime": 10300, "id": "bca05df58e3d3eb4f22d797e88069543bbf61b43", "metadata": {"id": "bca05df58e3d3eb4f22d797e88069543bbf61b43"}, "original_file_path": "../../datalake/Samuel/SOFTware-Sync/downloads/xml/hal-04300824.grobid.tei.xml", "file_name": "hal-04300824.grobid.tei.xml"}