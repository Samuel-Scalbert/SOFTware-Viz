{"application": "software-mentions", "version": "0.8.0", "date": "2024-10-07T12:01+0000", "md5": "C7A98FF58E141C4A37838183B363F0EB", "mentions": [{"type": "software", "software-type": "implicit", "software-name": {"rawForm": "script", "normalizedForm": "script", "offsetStart": 4, "offsetEnd": 10}, "context": "The script compute_emotional_map.py ", "mentionContextAttributes": {"used": {"value": false, "score": 0.010187327861785889}, "created": {"value": false, "score": 1.3947486877441406e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": false, "score": 0.010187327861785889}, "created": {"value": false, "score": 1.3947486877441406e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "FOVE", "normalizedForm": "FOVE", "offsetStart": 11, "offsetEnd": 15}, "context": "We use the FOVE Unity plugin to record head and gaze positions.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9996961355209351}, "created": {"value": false, "score": 1.0848045349121094e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999879598617554}, "created": {"value": false, "score": 0.0001831650733947754}, "shared": {"value": false, "score": 5.960464477539062e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "TensorFlow", "normalizedForm": "TensorFlow", "offsetStart": 28, "offsetEnd": 38}, "version": {"rawForm": "2", "normalizedForm": "2", "offsetStart": 39, "offsetEnd": 40}, "context": "For HL saliency, we use the TensorFlow 2 implementation of YOLOv4 [21]. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9986900687217712}, "created": {"value": false, "score": 9.775161743164062e-06}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9986900687217712}, "created": {"value": false, "score": 9.775161743164062e-06}, "shared": {"value": false, "score": 1.1920928955078125e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": ".py", "normalizedForm": ".py", "offsetStart": 32, "offsetEnd": 35}, "context": "The script compute_emotional_map.py ", "mentionContextAttributes": {"used": {"value": false, "score": 0.010187327861785889}, "created": {"value": false, "score": 1.3947486877441406e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": false, "score": 0.010187327861785889}, "created": {"value": false, "score": 1.3947486877441406e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Jupyter", "normalizedForm": "Jupyter", "offsetStart": 34, "offsetEnd": 41}, "context": "Along with the data, we provide a Jupyter notebook to reproduce the entire processing of head and gaze data, EDA, ratings of valence and arousal, and the code to produce saliency maps from the content.", "mentionContextAttributes": {"used": {"value": false, "score": 0.00665283203125}, "created": {"value": false, "score": 0.2938452959060669}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": false, "score": 0.49390435218811035}, "created": {"value": false, "score": 0.2938452959060669}, "shared": {"value": false, "score": 2.384185791015625e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "FOVE", "normalizedForm": "FOVE", "offsetStart": 44, "offsetEnd": 48}, "context": "Eye tracking calibration was done using the FOVE software for each user before beginning the experiment to make sure the eye tracking data is properly recorded. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9999879598617554}, "created": {"value": false, "score": 0.00018268823623657227}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999879598617554}, "created": {"value": false, "score": 0.0001831650733947754}, "shared": {"value": false, "score": 5.960464477539062e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "GitLab", "normalizedForm": "GitLab", "offsetStart": 52, "offsetEnd": 58}, "context": "The dataset and tools are now available in a public GitLab repository2 .", "mentionContextAttributes": {"used": {"value": false, "score": 0.0007405877113342285}, "created": {"value": false, "score": 0.007945537567138672}, "shared": {"value": false, "score": 0.4103289246559143}}, "documentContextAttributes": {"used": {"value": false, "score": 0.0007405877113342285}, "created": {"value": false, "score": 0.007945537567138672}, "shared": {"value": false, "score": 0.4103289246559143}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Jupyter", "normalizedForm": "Jupyter", "offsetStart": 57, "offsetEnd": 64}, "context": "Finally, the root folder PEM360 also contains the Python Jupyter notebook providing the software tools described in Sec. 4, and the entire data processing workflow to reproduce the analysis presented in Sec. 5. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.49390435218811035}, "created": {"value": false, "score": 5.543231964111328e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": false, "score": 0.49390435218811035}, "created": {"value": false, "score": 0.2938452959060669}, "shared": {"value": false, "score": 2.384185791015625e-07}}}, {"type": "software", "software-type": "implicit", "software-name": {"rawForm": "tools", "normalizedForm": "tools", "offsetStart": 58, "offsetEnd": 63}, "context": "The entire collection of artifacts is presented as Python tools and notebooks to enable reproducibility of the data processing. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.2570299506187439}, "created": {"value": false, "score": 0.07207924127578735}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": false, "score": 0.2570299506187439}, "created": {"value": false, "score": 0.07207924127578735}, "shared": {"value": false, "score": 2.384185791015625e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "FOVE", "normalizedForm": "FOVE", "offsetStart": 59, "offsetEnd": 63}, "context": "Recordings of head and eye movements have been made with a FOVE headset, equipped with an eye-tracker with a 120Hz acquisition rate, and tethered to a desktop computer.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9999163150787354}, "created": {"value": false, "score": 0.0001831650733947754}, "shared": {"value": false, "score": 5.960464477539062e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999879598617554}, "created": {"value": false, "score": 0.0001831650733947754}, "shared": {"value": false, "score": 5.960464477539062e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "EmojiGrid", "normalizedForm": "EmojiGrid", "offsetStart": 61, "offsetEnd": 70}, "context": "Toet et al. [22] presented a new emotions rating tool, named EmojiGrid, tested on 40 users viewing 62 videos from the reference database of Li et al. [16]. ", "mentionContextAttributes": {"used": {"value": false, "score": 6.186962127685547e-05}, "created": {"value": false, "score": 0.01780802011489868}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": false, "score": 6.186962127685547e-05}, "created": {"value": false, "score": 0.01780802011489868}, "shared": {"value": false, "score": 2.384185791015625e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "equirectangular-toolbox", "normalizedForm": "equirectangular-toolbox", "offsetStart": 112, "offsetEnd": 139}, "context": "We first uniformly sample 100 points on the unit sphere and project them on the equirectangular frame using the equirectangular-toolbox [18]. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9986710548400879}, "created": {"value": false, "score": 5.650520324707031e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9986710548400879}, "created": {"value": false, "score": 5.650520324707031e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}}], "references": [], "runtime": 22215, "id": "b7640940ba0d6b76c8f21aa9fedfe9dde32db83e", "metadata": {"id": "b7640940ba0d6b76c8f21aa9fedfe9dde32db83e"}, "original_file_path": "../../datalake/Samuel/SOFTware-Sync/downloads/xml/hal-03710323.grobid.tei.xml", "file_name": "hal-03710323.grobid.tei.xml"}