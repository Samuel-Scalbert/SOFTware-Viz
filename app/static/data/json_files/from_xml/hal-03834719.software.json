{"application": "software-mentions", "version": "0.8.0", "date": "2024-10-07T11:48+0000", "md5": "CBBA416F158F3B9EC642A580E1041463", "mentions": [{"type": "software", "software-type": "software", "software-name": {"rawForm": "MUSS", "normalizedForm": "MUSS", "offsetStart": 0, "offsetEnd": 4}, "context": "MUSS uses a novel approach to sentence simplification that trains strong models using sentencelevel paraphrase data instead of proper simplification data. ", "mentionContextAttributes": {"used": {"value": false, "score": 3.3736228942871094e-05}, "created": {"value": false, "score": 0.007282614707946777}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.999964714050293}, "created": {"value": true, "score": 0.9999152421951294}, "shared": {"value": false, "score": 5.960464477539062e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "MUSS", "normalizedForm": "MUSS", "offsetStart": 0, "offsetEnd": 4}, "context": "MUSS further improves the state of the art on all English datasets by incorporating additional labeled simplification data. ", "mentionContextAttributes": {"used": {"value": false, "score": 4.9233436584472656e-05}, "created": {"value": false, "score": 0.0005567669868469238}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.999964714050293}, "created": {"value": true, "score": 0.9999152421951294}, "shared": {"value": false, "score": 5.960464477539062e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "MUSS", "normalizedForm": "MUSS", "offsetStart": 0, "offsetEnd": 4}, "context": "MUSS bridges the gap with supervised method and removes the need for deciding in advance how complex and simple sentences should be separated, but instead trains directly on paraphrases mined from the raw corpora. ", "mentionContextAttributes": {"used": {"value": false, "score": 5.0902366638183594e-05}, "created": {"value": false, "score": 2.574920654296875e-05}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.999964714050293}, "created": {"value": true, "score": 0.9999152421951294}, "shared": {"value": false, "score": 5.960464477539062e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "MUSS", "normalizedForm": "MUSS", "offsetStart": 0, "offsetEnd": 4}, "context": "MUSS on the other hand almost never resorts to exactly copying the source sentence which leads to higher addition and deletion F1.", "mentionContextAttributes": {"used": {"value": false, "score": 0.00017696619033813477}, "created": {"value": false, "score": 8.463859558105469e-06}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.999964714050293}, "created": {"value": true, "score": 0.9999152421951294}, "shared": {"value": false, "score": 5.960464477539062e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "LASER", "normalizedForm": "LASER", "offsetStart": 0, "offsetEnd": 5}, "context": "LASER provides joint multilingual sentence embeddings in 93 languages that have been successfully applied to the task of bilingual bitext mining (Schwenk et al., 2019). ", "mentionContextAttributes": {"used": {"value": false, "score": 9.071826934814453e-05}, "created": {"value": false, "score": 0.0013609528541564941}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9998292922973633}, "created": {"value": true, "score": 0.9629368782043457}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "references": [{"label": "(Artetxe and Schwenk, 2019b)", "normalizedForm": "Artetxe and Schwenk, 2019b", "refKey": 7}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "TurkCorpus", "normalizedForm": "TurkCorpus", "offsetStart": 0, "offsetEnd": 10}, "url": {"rawForm": "https://github.com/cocoxu/ simplification/", "normalizedForm": "https://github.com/cocoxu/ simplification"}, "context": "TurkCorpus and ASSET were created using the same 2000 valid and 359 test source sentences and they respectively contain 8 and 10 reference simplifications per source sentence.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9381291270256042}, "created": {"value": false, "score": 5.841255187988281e-06}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999442100524902}, "created": {"value": false, "score": 9.274482727050781e-05}, "shared": {"value": true, "score": 0.9868447780609131}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "TurkCorpus", "normalizedForm": "TurkCorpus", "offsetStart": 2, "offsetEnd": 12}, "url": {"rawForm": "https://github.com/cocoxu/ simplification/", "normalizedForm": "https://github.com/cocoxu/ simplification", "offsetStart": 14, "offsetEnd": 56}, "context": "\u2022 TurkCorpus: https://github.com/cocoxu/ simplification/ or https://github. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.0001125335693359375}, "created": {"value": false, "score": 1.4781951904296875e-05}, "shared": {"value": true, "score": 0.9868447780609131}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999442100524902}, "created": {"value": false, "score": 9.274482727050781e-05}, "shared": {"value": true, "score": 0.9868447780609131}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "MUSS", "normalizedForm": "MUSS", "offsetStart": 7, "offsetEnd": 11}, "context": "We use MUSS trained on mined data + WikiLarge as the English simplification model. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9996927976608276}, "created": {"value": false, "score": 1.3470649719238281e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.999964714050293}, "created": {"value": true, "score": 0.9999152421951294}, "shared": {"value": false, "score": 5.960464477539062e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "MUSS", "normalizedForm": "MUSS", "offsetStart": 8, "offsetEnd": 12}, "context": "Results MUSS outperforms our strongest baseline by +8.25 SARI for French, while matching the pivot baseline performance for Spanish.", "mentionContextAttributes": {"used": {"value": false, "score": 0.06645619869232178}, "created": {"value": false, "score": 8.940696716308594e-06}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.999964714050293}, "created": {"value": true, "score": 0.9999152421951294}, "shared": {"value": false, "score": 5.960464477539062e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "TurkCorpus", "normalizedForm": "TurkCorpus", "offsetStart": 8, "offsetEnd": 18}, "url": {"rawForm": "https://github.com/cocoxu/ simplification/", "normalizedForm": "https://github.com/cocoxu/ simplification"}, "context": "For the TurkCorpus and Newsela datasets, the unsupervised MUSS approach achieves strong results, either outperforming or closely matching unsupervised and supervised previous works.", "mentionContextAttributes": {"used": {"value": false, "score": 0.002225339412689209}, "created": {"value": false, "score": 7.033348083496094e-06}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999442100524902}, "created": {"value": false, "score": 9.274482727050781e-05}, "shared": {"value": true, "score": 0.9868447780609131}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "MUSS", "normalizedForm": "MUSS", "offsetStart": 9, "offsetEnd": 13}, "context": "Training MUSS on the mined data or on PARANMT obtains similar results for text simplification, confirming that mining paraphrase data is a viable alternative to using existing paraphrase datasets relying on labeled parallel machine translation corpora.", "mentionContextAttributes": {"used": {"value": false, "score": 0.00800853967666626}, "created": {"value": false, "score": 9.179115295410156e-06}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.999964714050293}, "created": {"value": true, "score": 0.9999152421951294}, "shared": {"value": false, "score": 5.960464477539062e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "MUSS", "normalizedForm": "MUSS", "offsetStart": 10, "offsetEnd": 14}, "context": "We report MUSS scores in Table 2.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9963162541389465}, "created": {"value": false, "score": 9.655952453613281e-06}, "shared": {"value": false, "score": 5.960464477539062e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.999964714050293}, "created": {"value": true, "score": 0.9999152421951294}, "shared": {"value": false, "score": 5.960464477539062e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "EASSE", "normalizedForm": "EASSE", "offsetStart": 11, "offsetEnd": 16}, "context": "We use the EASSE library(Alva-Manchego et al., 2019) to compute SARI. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9999673366546631}, "created": {"value": false, "score": 2.2172927856445312e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999673366546631}, "created": {"value": false, "score": 0.07808011770248413}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "references": [{"label": "(Alva-Manchego et al., 2019)", "normalizedForm": "Alva-Manchego et al., 2019", "refKey": 2, "offsetStart": 27700, "offsetEnd": 27728}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "LASER", "normalizedForm": "LASER", "offsetStart": 11, "offsetEnd": 16}, "context": "We compute LASER embeddings of dimension 1024 and reduce dimensionality with a 512 PCA followed by random rotation.", "mentionContextAttributes": {"used": {"value": true, "score": 0.999630331993103}, "created": {"value": false, "score": 2.1457672119140625e-06}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9998292922973633}, "created": {"value": true, "score": 0.9629368782043457}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "references": [{"label": "(Artetxe and Schwenk, 2019b)", "normalizedForm": "Artetxe and Schwenk, 2019b", "refKey": 7}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "OnePlusOne", "normalizedForm": "OnePlusOne", "offsetStart": 11, "offsetEnd": 21}, "context": "We use the OnePlusOne optimizer with a budget of 64 evaluations (approximately 1 hour of optimization on a single GPU). ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9992595314979553}, "created": {"value": false, "score": 1.1563301086425781e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9992595314979553}, "created": {"value": false, "score": 1.1563301086425781e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "MUSS", "normalizedForm": "MUSS", "offsetStart": 13, "offsetEnd": 17}, "context": "We introduce MUSS, a Multilingual Unsupervised Sentence Simplification system that does not require labeled simplification data. ", "mentionContextAttributes": {"used": {"value": false, "score": 8.285045623779297e-05}, "created": {"value": true, "score": 0.9999152421951294}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.999964714050293}, "created": {"value": true, "score": 0.9999152421951294}, "shared": {"value": false, "score": 5.960464477539062e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "MUSS", "normalizedForm": "MUSS", "offsetStart": 16, "offsetEnd": 20}, "context": "We now describe MUSS, our approach to training controllable simplification models on mined data.", "mentionContextAttributes": {"used": {"value": false, "score": 3.540515899658203e-05}, "created": {"value": true, "score": 0.9999140501022339}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.999964714050293}, "created": {"value": true, "score": 0.9999152421951294}, "shared": {"value": false, "score": 5.960464477539062e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "MUSS", "normalizedForm": "MUSS", "offsetStart": 26, "offsetEnd": 30}, "context": "Human judgments show that MUSS models are more fluent and produce simpler outputs than previous work (Martin et al., 2020).", "mentionContextAttributes": {"used": {"value": false, "score": 0.0001251697540283203}, "created": {"value": false, "score": 0.00047969818115234375}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.999964714050293}, "created": {"value": true, "score": 0.9999152421951294}, "shared": {"value": false, "score": 5.960464477539062e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "MUSS", "normalizedForm": "MUSS", "offsetStart": 26, "offsetEnd": 30}, "context": "We compare SARI scores of MUSS trained either on our mined data or on PARANMT (Wieting and Gimpel, 2018) on the test sets of ASSET, TurkCorpus and Newsela.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9999442100524902}, "created": {"value": false, "score": 2.2649765014648438e-06}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.999964714050293}, "created": {"value": true, "score": 0.9999152421951294}, "shared": {"value": false, "score": 5.960464477539062e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "LASER", "normalizedForm": "LASER", "offsetStart": 27, "offsetEnd": 32}, "context": "In this work, we show that LASER can also be used to mine monolingual paraphrase datasets but also highlights its limits (cf.", "mentionContextAttributes": {"used": {"value": false, "score": 0.00012981891632080078}, "created": {"value": true, "score": 0.9629368782043457}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9998292922973633}, "created": {"value": true, "score": 0.9629368782043457}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "references": [{"label": "(Artetxe and Schwenk, 2019b)", "normalizedForm": "Artetxe and Schwenk, 2019b", "refKey": 7}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "TurkCorpus", "normalizedForm": "TurkCorpus", "offsetStart": 27, "offsetEnd": 37}, "url": {"rawForm": "https://github.com/cocoxu/ simplification/", "normalizedForm": "https://github.com/cocoxu/ simplification"}, "context": "92.81 on ASSET or 99.36 on TurkCorpus), which underlines the weaknesses of this metric.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9978218078613281}, "created": {"value": false, "score": 6.4373016357421875e-06}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999442100524902}, "created": {"value": false, "score": 9.274482727050781e-05}, "shared": {"value": true, "score": 0.9868447780609131}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "TurkCorpus", "normalizedForm": "TurkCorpus", "offsetStart": 35, "offsetEnd": 45}, "url": {"rawForm": "https://github.com/cocoxu/ simplification/", "normalizedForm": "https://github.com/cocoxu/ simplification"}, "context": "We display SARI and FKGL on ASSET, TurkCorpus and Newsela test sets for English.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9978218078613281}, "created": {"value": false, "score": 4.744529724121094e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999442100524902}, "created": {"value": false, "score": 9.274482727050781e-05}, "shared": {"value": true, "score": 0.9868447780609131}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "MUSS", "normalizedForm": "MUSS", "offsetStart": 37, "offsetEnd": 41}, "context": "Mined Data limits Sentence Splitting MUSS rarely perform sentence splitting when trained on mined data only (3.45% of the time) while it becomes way better at this operation when incorporating labelled data from WikiLarge (34.26%).", "mentionContextAttributes": {"used": {"value": false, "score": 0.00027507543563842773}, "created": {"value": false, "score": 4.291534423828125e-06}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.999964714050293}, "created": {"value": true, "score": 0.9999152421951294}, "shared": {"value": false, "score": 5.960464477539062e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "MBART", "normalizedForm": "MBART", "offsetStart": 38, "offsetEnd": 43}, "context": "In Appendix Table 11, we can see that MBART has a small loss in performance of 1.54 SARI compared to its monolingual counterpart BART, due to the fact that it handles 25 languages instead of one.", "mentionContextAttributes": {"used": {"value": false, "score": 0.0176580548286438}, "created": {"value": false, "score": 2.384185791015625e-05}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9238197803497314}, "created": {"value": false, "score": 0.0011038780212402344}, "shared": {"value": false, "score": 5.960464477539062e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "MUSS", "normalizedForm": "MUSS", "offsetStart": 40, "offsetEnd": 44}, "context": "In French and Spanish, the unsupervised MUSS model performs better or similar than the supervised pivot baseline which has been trained on labeled English simplifications.", "mentionContextAttributes": {"used": {"value": false, "score": 5.3048133850097656e-05}, "created": {"value": false, "score": 5.2809715270996094e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.999964714050293}, "created": {"value": true, "score": 0.9999152421951294}, "shared": {"value": false, "score": 5.960464477539062e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "NEVERGRAD", "normalizedForm": "NEVERGRAD", "offsetStart": 40, "offsetEnd": 49}, "context": "We use zero-order optimization with the NEVERGRAD library (Rapin and Teytaud, 2018). ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9984520673751831}, "created": {"value": false, "score": 9.02414321899414e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9984520673751831}, "created": {"value": false, "score": 9.02414321899414e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "references": [{"label": "(Rapin and Teytaud, 2018)", "normalizedForm": "Rapin and Teytaud, 2018", "refKey": 45, "offsetStart": 30668, "offsetEnd": 30693}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "CCMATRIX", "normalizedForm": "CCMATRIX", "offsetStart": 43, "offsetEnd": 74}, "context": "For French and Spanish translation, we use CCMATRIX (Schwenk et al., 2019) to train Transformer models with LayerDrop (Fan et al., 2019). ", "mentionContextAttributes": {"used": {"value": true, "score": 0.995341420173645}, "created": {"value": false, "score": 3.0875205993652344e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.995341420173645}, "created": {"value": false, "score": 3.0875205993652344e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "MBART", "normalizedForm": "MBART", "offsetStart": 49, "offsetEnd": 72}, "context": "For non-English, we use its multilingual version MBART (Liu et al., 2020), pretrained on 25 languages.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9238197803497314}, "created": {"value": false, "score": 4.8279762268066406e-05}, "shared": {"value": false, "score": 5.960464477539062e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9238197803497314}, "created": {"value": false, "score": 0.0011038780212402344}, "shared": {"value": false, "score": 5.960464477539062e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "LASER", "normalizedForm": "LASER", "offsetStart": 51, "offsetEnd": 56}, "context": "Our intuition is that this is due to the fact that LASER embeddings do not work well across multiple sentences, thus preventing single sentences to be matched with multiple corresponding sentences.", "mentionContextAttributes": {"used": {"value": false, "score": 0.008140504360198975}, "created": {"value": false, "score": 0.021206378936767578}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9998292922973633}, "created": {"value": true, "score": 0.9629368782043457}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "references": [{"label": "(Artetxe and Schwenk, 2019b)", "normalizedForm": "Artetxe and Schwenk, 2019b", "refKey": 7}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "MUSS", "normalizedForm": "MUSS", "offsetStart": 57, "offsetEnd": 61}, "context": "In table 6, we analyse the types of simplifications that MUSS performs using quality estimation features computed with the EASSE library. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.09584635496139526}, "created": {"value": false, "score": 6.961822509765625e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.999964714050293}, "created": {"value": true, "score": 0.9999152421951294}, "shared": {"value": false, "score": 5.960464477539062e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "MUSS", "normalizedForm": "MUSS", "offsetStart": 58, "offsetEnd": 62}, "context": "For the TurkCorpus and Newsela datasets, the unsupervised MUSS approach achieves strong results, either outperforming or closely matching unsupervised and supervised previous works.", "mentionContextAttributes": {"used": {"value": false, "score": 0.002225339412689209}, "created": {"value": false, "score": 7.033348083496094e-06}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.999964714050293}, "created": {"value": true, "score": 0.9999152421951294}, "shared": {"value": false, "score": 5.960464477539062e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "MUSS", "normalizedForm": "MUSS", "offsetStart": 60, "offsetEnd": 64}, "context": "When incorporating labeled data from WikiLarge and Newsela, MUSS obtains state-of-the-art results on all datasets. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.013164281845092773}, "created": {"value": false, "score": 3.147125244140625e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.999964714050293}, "created": {"value": true, "score": 0.9999152421951294}, "shared": {"value": false, "score": 5.960464477539062e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "TurkCorpus", "normalizedForm": "TurkCorpus", "offsetStart": 60, "offsetEnd": 70}, "url": {"rawForm": "https://github.com/cocoxu/ simplification/", "normalizedForm": "https://github.com/cocoxu/ simplification"}, "context": "ASSET features more varied set of rewriting operations than TurkCorpus, and is considered simpler by human judges (Alva-Manchego et al., 2020a). ", "mentionContextAttributes": {"used": {"value": false, "score": 0.00011527538299560547}, "created": {"value": false, "score": 2.8848648071289062e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999442100524902}, "created": {"value": false, "score": 9.274482727050781e-05}, "shared": {"value": true, "score": 0.9868447780609131}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "MUSS", "normalizedForm": "MUSS", "offsetStart": 61, "offsetEnd": 65}, "context": "On the ASSET benchmark, with no labeled simplification data, MUSS obtains a +5.98 SARI improvement with respect to previous unsupervised methods, and a +2.52 SARI improvement over the state-of-the-art supervised methods.", "mentionContextAttributes": {"used": {"value": false, "score": 0.017058193683624268}, "created": {"value": false, "score": 1.0371208190917969e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.999964714050293}, "created": {"value": true, "score": 0.9999152421951294}, "shared": {"value": false, "score": 5.960464477539062e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "MBART", "normalizedForm": "MBART", "offsetStart": 62, "offsetEnd": 67}, "context": "Second it uses the stronger monolingual BART model instead of MBART.", "mentionContextAttributes": {"used": {"value": false, "score": 0.002436518669128418}, "created": {"value": false, "score": 5.841255187988281e-06}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9238197803497314}, "created": {"value": false, "score": 0.0011038780212402344}, "shared": {"value": false, "score": 5.960464477539062e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "MUSS", "normalizedForm": "MUSS", "offsetStart": 66, "offsetEnd": 70}, "context": "We show simplifications generated by our best unsupervised model: MUSS trained on mined data only. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.999964714050293}, "created": {"value": false, "score": 0.08439981937408447}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.999964714050293}, "created": {"value": true, "score": 0.9999152421951294}, "shared": {"value": false, "score": 5.960464477539062e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "MBART", "normalizedForm": "MBART", "offsetStart": 66, "offsetEnd": 71}, "context": "We replace the monolingual BART with its multilingual counterpart MBART, trained on 25 languages.", "mentionContextAttributes": {"used": {"value": false, "score": 0.0014818310737609863}, "created": {"value": false, "score": 0.0011038780212402344}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9238197803497314}, "created": {"value": false, "score": 0.0011038780212402344}, "shared": {"value": false, "score": 5.960464477539062e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "TurkCorpus", "normalizedForm": "TurkCorpus", "offsetStart": 66, "offsetEnd": 76}, "url": {"rawForm": "https://github.com/cocoxu/ simplification/", "normalizedForm": "https://github.com/cocoxu/ simplification"}, "context": "In practice, we obtain the following approximations: ASSET = 0.8, TurkCorpus = 0.95, and Newsela = 0.4 (rounded to 0.05). ", "mentionContextAttributes": {"used": {"value": true, "score": 0.832809329032898}, "created": {"value": false, "score": 2.6226043701171875e-06}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999442100524902}, "created": {"value": false, "score": 9.274482727050781e-05}, "shared": {"value": true, "score": 0.9868447780609131}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "MUSS", "normalizedForm": "MUSS", "offsetStart": 72, "offsetEnd": 76}, "context": "Our resulting Multilingual Unsupervised Sentence Simplification method, MUSS, is unsupervised because it can be trained without relying on labeled simplification data, 1 even though we mine using supervised sentence embeddings. 2 We apply MUSS on English, French, and Spanish to closely match or outperform the supervised state of the art in all languages. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.003128230571746826}, "created": {"value": false, "score": 0.08825325965881348}, "shared": {"value": false, "score": 5.960464477539062e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.999964714050293}, "created": {"value": true, "score": 0.9999152421951294}, "shared": {"value": false, "score": 5.960464477539062e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "EASSE", "normalizedForm": "EASSE", "offsetStart": 72, "offsetEnd": 77}, "context": "SARI score computation We use the latest version of SARI implemented in EASSE (Alva-Manchego et al., 2019) which fixes bugs and inconsistencies from the traditional implementation of SARI. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.012330949306488037}, "created": {"value": false, "score": 0.0016199350357055664}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999673366546631}, "created": {"value": false, "score": 0.07808011770248413}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "references": [{"label": "(Alva-Manchego et al., 2019)", "normalizedForm": "Alva-Manchego et al., 2019", "refKey": 2}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "LayerDrop", "normalizedForm": "LayerDrop", "offsetStart": 77, "offsetEnd": 104}, "context": "Our models use the Transformer architecture with 240 million parameters with LayerDrop (Fan et al., 2019). ", "mentionContextAttributes": {"used": {"value": true, "score": 0.7325736880302429}, "created": {"value": false, "score": 0.01771301031112671}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.995341420173645}, "created": {"value": false, "score": 0.01771301031112671}, "shared": {"value": false, "score": 2.384185791015625e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "TurkCorpus", "normalizedForm": "TurkCorpus", "offsetStart": 86, "offsetEnd": 96}, "url": {"rawForm": "https://github.com/cocoxu/ simplification/", "normalizedForm": "https://github.com/cocoxu/ simplification"}, "context": "Gold Reference We report gold reference scores for multi-reference datasets ASSET and TurkCorpus.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9992341995239258}, "created": {"value": false, "score": 3.3855438232421875e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999442100524902}, "created": {"value": false, "score": 9.274482727050781e-05}, "shared": {"value": true, "score": 0.9868447780609131}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "EASSE", "normalizedForm": "EASSE", "offsetStart": 94, "offsetEnd": 99}, "context": "We do so by using the system predictions provided by the respective authors, and available in EASSE. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9838898181915283}, "created": {"value": false, "score": 0.07808011770248413}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999673366546631}, "created": {"value": false, "score": 0.07808011770248413}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "references": [{"label": "(Alva-Manchego et al., 2019)", "normalizedForm": "Alva-Manchego et al., 2019", "refKey": 2}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "MUSS", "normalizedForm": "MUSS", "offsetStart": 104, "offsetEnd": 108}, "context": "They are deemed as fluent and simpler than the human simplifications on ASSET test set, indicating that MUSS is able to reach a high level of simplicity thanks to the control mechanism. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.007346391677856445}, "created": {"value": false, "score": 0.00014400482177734375}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.999964714050293}, "created": {"value": true, "score": 0.9999152421951294}, "shared": {"value": false, "score": 5.960464477539062e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "LayerDrop", "normalizedForm": "LayerDrop", "offsetStart": 108, "offsetEnd": 135}, "context": "For French and Spanish translation, we use CCMATRIX (Schwenk et al., 2019) to train Transformer models with LayerDrop (Fan et al., 2019). ", "mentionContextAttributes": {"used": {"value": true, "score": 0.995341420173645}, "created": {"value": false, "score": 3.0875205993652344e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.995341420173645}, "created": {"value": false, "score": 0.01771301031112671}, "shared": {"value": false, "score": 2.384185791015625e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "TurkCorpus", "normalizedForm": "TurkCorpus", "offsetStart": 121, "offsetEnd": 131}, "url": {"rawForm": "https://github.com/cocoxu/ simplification/", "normalizedForm": "https://github.com/cocoxu/ simplification"}, "context": "Their T5+ACCESS setup trained on WikiLarge increases the performance with respectively 45.04 and 43.31 SARI on ASSET and TurkCorpus.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9294924736022949}, "created": {"value": false, "score": 9.274482727050781e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999442100524902}, "created": {"value": false, "score": 9.274482727050781e-05}, "shared": {"value": true, "score": 0.9868447780609131}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "LASER", "normalizedForm": "LASER", "offsetStart": 122, "offsetEnd": 127}, "context": "To automatically mine our paraphrase corpora, we first compute n-dimensional embeddings for each extracted sequence using LASER (Artetxe and Schwenk, 2019b).", "mentionContextAttributes": {"used": {"value": true, "score": 0.9998292922973633}, "created": {"value": false, "score": 0.006203889846801758}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9998292922973633}, "created": {"value": true, "score": 0.9629368782043457}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "references": [{"label": "(Artetxe and Schwenk, 2019b)", "normalizedForm": "Artetxe and Schwenk, 2019b", "refKey": 7, "offsetStart": 9176, "offsetEnd": 9204}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "EASSE", "normalizedForm": "EASSE", "offsetStart": 123, "offsetEnd": 128}, "context": "In table 6, we analyse the types of simplifications that MUSS performs using quality estimation features computed with the EASSE library. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.09584635496139526}, "created": {"value": false, "score": 6.961822509765625e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999673366546631}, "created": {"value": false, "score": 0.07808011770248413}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "references": [{"label": "(Alva-Manchego et al., 2019)", "normalizedForm": "Alva-Manchego et al., 2019", "refKey": 2}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "TurkCorpus", "normalizedForm": "TurkCorpus", "offsetStart": 132, "offsetEnd": 142}, "url": {"rawForm": "https://github.com/cocoxu/ simplification/", "normalizedForm": "https://github.com/cocoxu/ simplification"}, "context": "We compare SARI scores of MUSS trained either on our mined data or on PARANMT (Wieting and Gimpel, 2018) on the test sets of ASSET, TurkCorpus and Newsela.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9999442100524902}, "created": {"value": false, "score": 2.2649765014648438e-06}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999442100524902}, "created": {"value": false, "score": 9.274482727050781e-05}, "shared": {"value": true, "score": 0.9868447780609131}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "LASER", "normalizedForm": "LASER", "offsetStart": 139, "offsetEnd": 144}, "context": "Mining Paraphrases We use each sequence as a query q i against the billion-scale faiss index to retrieve the top-8 nearest neighbor in the LASER embedding space (L2 distance).", "mentionContextAttributes": {"used": {"value": true, "score": 0.998932421207428}, "created": {"value": false, "score": 2.3126602172851562e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9998292922973633}, "created": {"value": true, "score": 0.9629368782043457}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "references": [{"label": "(Artetxe and Schwenk, 2019b)", "normalizedForm": "Artetxe and Schwenk, 2019b", "refKey": 7}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "EASSE", "normalizedForm": "EASSE", "offsetStart": 176, "offsetEnd": 181}, "context": "Newsela is a collection of news articles with professional simplifications, later aligned into 94k simplifications by (Zhang and Lapata, work's system predictions available in EASSE.", "mentionContextAttributes": {"used": {"value": false, "score": 0.016068637371063232}, "created": {"value": false, "score": 0.0002346634864807129}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999673366546631}, "created": {"value": false, "score": 0.07808011770248413}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "references": [{"label": "(Alva-Manchego et al., 2019)", "normalizedForm": "Alva-Manchego et al., 2019", "refKey": 2}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "LASER", "normalizedForm": "LASER", "offsetStart": 178, "offsetEnd": 183}, "context": "For each source document, we therefore align each line, provided it is not too long (less than 6 sentences), with the most appropriate line in the simplified document, using the LASER embedding space.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9541091918945312}, "created": {"value": false, "score": 6.556510925292969e-06}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9998292922973633}, "created": {"value": true, "score": 0.9629368782043457}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "references": [{"label": "(Artetxe and Schwenk, 2019b)", "normalizedForm": "Artetxe and Schwenk, 2019b", "refKey": 7}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "MUSS", "normalizedForm": "MUSS", "offsetStart": 239, "offsetEnd": 243}, "context": "Our resulting Multilingual Unsupervised Sentence Simplification method, MUSS, is unsupervised because it can be trained without relying on labeled simplification data, 1 even though we mine using supervised sentence embeddings. 2 We apply MUSS on English, French, and Spanish to closely match or outperform the supervised state of the art in all languages. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.003128230571746826}, "created": {"value": false, "score": 0.08825325965881348}, "shared": {"value": false, "score": 5.960464477539062e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.999964714050293}, "created": {"value": true, "score": 0.9999152421951294}, "shared": {"value": false, "score": 5.960464477539062e-07}}}], "references": [{"refKey": 7, "tei": "<biblStruct xml:id=\"b7\">\n\t<analytic>\n\t\t<title level=\"a\" type=\"main\">Massively Multilingual Sentence Embeddings for Zero-Shot Cross-Lingual Transfer and Beyond</title>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Mikel</forename><surname>Artetxe</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Holger</forename><surname>Schwenk</surname></persName>\n\t\t</author>\n\t\t<idno type=\"DOI\">10.1162/tacl_a_00288</idno>\n\t</analytic>\n\t<monogr>\n\t\t<title level=\"j\">Transactions of the Association for Computational Linguistics</title>\n\t\t<title level=\"j\" type=\"abbrev\">Transactions of the Association for Computational Linguistics</title>\n\t\t<idno type=\"ISSNe\">2307-387X</idno>\n\t\t<imprint>\n\t\t\t<biblScope unit=\"volume\">7</biblScope>\n\t\t\t<biblScope unit=\"page\" from=\"597\" to=\"610\" />\n\t\t\t<date type=\"published\" when=\"2019-11\">2019b</date>\n\t\t\t<publisher>MIT Press - Journals</publisher>\n\t\t</imprint>\n\t</monogr>\n</biblStruct>\n"}, {"refKey": 2, "tei": "<biblStruct xml:id=\"b2\">\n\t<analytic>\n\t\t<title level=\"a\" type=\"main\">EASSE: Easier Automatic Sentence Simplification Evaluation</title>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Fernando</forename><surname>Alva-Manchego</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Louis</forename><surname>Martin</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Carolina</forename><surname>Scarton</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Lucia</forename><surname>Specia</surname></persName>\n\t\t</author>\n\t\t<idno type=\"DOI\">10.18653/v1/d19-3009</idno>\n\t</analytic>\n\t<monogr>\n\t\t<title level=\"m\">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP): System Demonstrations</title>\n\t\t<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP): System Demonstrations</meeting>\n\t\t<imprint>\n\t\t\t<publisher>Association for Computational Linguistics</publisher>\n\t\t\t<date type=\"published\" when=\"2019\">2019. November</date>\n\t\t\t<biblScope unit=\"page\" from=\"49\" to=\"54\" />\n\t\t</imprint>\n\t</monogr>\n</biblStruct>\n"}, {"refKey": 45, "tei": "<biblStruct xml:id=\"b45\">\n\t<monogr>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">J</forename><surname>Rapin</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">O</forename><surname>Teytaud</surname></persName>\n\t\t</author>\n\t\t<title level=\"m\">Nevergrad -A gradient-free optimization platform</title>\n\t\t<imprint>\n\t\t\t<date>2018</date>\n\t\t</imprint>\n\t</monogr>\n</biblStruct>\n"}], "runtime": 8206, "id": "efbb98f5921bbc6ec42f325c23c210df4850bb8c", "metadata": {"id": "efbb98f5921bbc6ec42f325c23c210df4850bb8c"}, "original_file_path": "../../datalake/Samuel/SOFTware-Sync/downloads/xml/hal-03834719.grobid.tei.xml", "file_name": "hal-03834719.grobid.tei.xml"}