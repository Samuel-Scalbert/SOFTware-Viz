{"application": "software-mentions", "version": "0.8.0", "date": "2024-10-07T11:50+0000", "md5": "4293FB46187856F676EEDED6CCEEDDEF", "mentions": [{"type": "software", "software-type": "software", "software-name": {"rawForm": "LASER", "normalizedForm": "LASER", "offsetStart": 0, "offsetEnd": 5}, "context": "LASER has been successfully used for large-scale bitext mining like in the CCMatrix project (Schwenk et al., 2021).", "mentionContextAttributes": {"used": {"value": false, "score": 0.00018805265426635742}, "created": {"value": false, "score": 0.00048744678497314453}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999927282333374}, "created": {"value": true, "score": 0.9914092421531677}, "shared": {"value": false, "score": 8.344650268554688e-07}}, "references": [{"label": "(Artetxe and Schwenk, 2019)", "normalizedForm": "Artetxe and Schwenk, 2019", "refKey": 3}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "LASER", "normalizedForm": "LASER", "offsetStart": 0, "offsetEnd": 5}, "context": "LASER has been trained with a decoding objective, whereas other works like LaBSE (Feng et al., 2020) have been trained with a contrastive objective.", "mentionContextAttributes": {"used": {"value": false, "score": 0.00026983022689819336}, "created": {"value": false, "score": 0.0004011988639831543}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999927282333374}, "created": {"value": true, "score": 0.9914092421531677}, "shared": {"value": false, "score": 8.344650268554688e-07}}, "references": [{"label": "(Artetxe and Schwenk, 2019)", "normalizedForm": "Artetxe and Schwenk, 2019", "refKey": 3}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "LASER", "normalizedForm": "LASER", "offsetStart": 4, "offsetEnd": 9}, "context": "The LASER encoder handles several languages: decoding these multilingual embeddings enables to translate the input sentence into English with the original LASER decoder.", "mentionContextAttributes": {"used": {"value": false, "score": 0.0019287467002868652}, "created": {"value": false, "score": 2.8252601623535156e-05}, "shared": {"value": false, "score": 4.76837158203125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999927282333374}, "created": {"value": true, "score": 0.9914092421531677}, "shared": {"value": false, "score": 8.344650268554688e-07}}, "references": [{"label": "(Artetxe and Schwenk, 2019)", "normalizedForm": "Artetxe and Schwenk, 2019", "refKey": 3}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Fairseq", "normalizedForm": "Fairseq", "offsetStart": 7, "offsetEnd": 14}, "context": "We use Fairseq to train our models. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.6722215414047241}, "created": {"value": false, "score": 0.13785284757614136}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.6722215414047241}, "created": {"value": false, "score": 0.13785284757614136}, "shared": {"value": false, "score": 3.5762786865234375e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "CCMatrix", "normalizedForm": "CCMatrix", "offsetStart": 7, "offsetEnd": 15}, "context": "We use CCMatrix bi-texts to learn our text students, and bi-texts mined with LASER3 (Heffernan et al., 2022) for Mongolian.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9985257983207703}, "created": {"value": false, "score": 1.9788742065429688e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9998512268066406}, "created": {"value": false, "score": 0.00048744678497314453}, "shared": {"value": false, "score": 3.2186508178710938e-06}}, "references": [{"label": "(Schwenk et al., 2021)", "normalizedForm": "Schwenk et al., 2021", "refKey": 38}, {"label": "(Schwenk et al., 2021)", "normalizedForm": "Schwenk et al., 2021", "refKey": 38}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "LASER", "normalizedForm": "LASER", "offsetStart": 13, "offsetEnd": 18}, "context": "The original LASER encoder is fixed during training to encode English translation, behaving as the teacher, while we train a new Japanese encoder as a student to fit English sentence embeddings.", "mentionContextAttributes": {"used": {"value": false, "score": 0.0012862682342529297}, "created": {"value": false, "score": 0.0013137459754943848}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999927282333374}, "created": {"value": true, "score": 0.9914092421531677}, "shared": {"value": false, "score": 8.344650268554688e-07}}, "references": [{"label": "(Artetxe and Schwenk, 2019)", "normalizedForm": "Artetxe and Schwenk, 2019", "refKey": 3}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "LASER", "normalizedForm": "LASER", "offsetStart": 14, "offsetEnd": 19}, "context": "For instance, LASER (Artetxe and Schwenk, 2019) is a multilingual sentence embedding space, where sentences are close in the embedding space if they are paraphrases or translations.", "mentionContextAttributes": {"used": {"value": false, "score": 0.00011932849884033203}, "created": {"value": false, "score": 5.447864532470703e-05}, "shared": {"value": false, "score": 4.76837158203125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999927282333374}, "created": {"value": true, "score": 0.9914092421531677}, "shared": {"value": false, "score": 8.344650268554688e-07}}, "references": [{"label": "(Artetxe and Schwenk, 2019", "normalizedForm": "(Artetxe and Schwenk, 2019", "refKey": 3, "offsetStart": 10233, "offsetEnd": 10259}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "LASER", "normalizedForm": "LASER", "offsetStart": 14, "offsetEnd": 19}, "context": "We focused on LASER as it originally has a decoder, and we studied how we can improve the decoding of sentence embeddings.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9987030029296875}, "created": {"value": true, "score": 0.9914092421531677}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999927282333374}, "created": {"value": true, "score": 0.9914092421531677}, "shared": {"value": false, "score": 8.344650268554688e-07}}, "references": [{"label": "(Artetxe and Schwenk, 2019)", "normalizedForm": "Artetxe and Schwenk, 2019", "refKey": 3}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "LASER", "normalizedForm": "LASER", "offsetStart": 17, "offsetEnd": 22}, "context": "We decide to use LASER English embeddings as our teacher.", "mentionContextAttributes": {"used": {"value": true, "score": 0.963367223739624}, "created": {"value": false, "score": 0.0001455545425415039}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999927282333374}, "created": {"value": true, "score": 0.9914092421531677}, "shared": {"value": false, "score": 8.344650268554688e-07}}, "references": [{"label": "(Artetxe and Schwenk, 2019)", "normalizedForm": "Artetxe and Schwenk, 2019", "refKey": 3}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "LASER", "normalizedForm": "LASER", "offsetStart": 17, "offsetEnd": 22}, "context": "A.1 Distances in LASER text space", "mentionContextAttributes": {"used": {"value": true, "score": 0.9397505521774292}, "created": {"value": false, "score": 1.9431114196777344e-05}, "shared": {"value": false, "score": 5.960464477539062e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999927282333374}, "created": {"value": true, "score": 0.9914092421531677}, "shared": {"value": false, "score": 8.344650268554688e-07}}, "references": [{"label": "(Artetxe and Schwenk, 2019)", "normalizedForm": "Artetxe and Schwenk, 2019", "refKey": 3}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "CCMatrix", "normalizedForm": "CCMatrix", "offsetStart": 20, "offsetEnd": 28}, "context": "We use bitexts from CCMatrix for the ja-en pair to train the Japanese text student.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9998512268066406}, "created": {"value": false, "score": 1.9431114196777344e-05}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9998512268066406}, "created": {"value": false, "score": 0.00048744678497314453}, "shared": {"value": false, "score": 3.2186508178710938e-06}}, "references": [{"label": "(Schwenk et al., 2021)", "normalizedForm": "Schwenk et al., 2021", "refKey": 38}, {"label": "(Schwenk et al., 2021)", "normalizedForm": "Schwenk et al., 2021", "refKey": 38}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "LASER", "normalizedForm": "LASER", "offsetStart": 22, "offsetEnd": 27}, "context": "However, the original LASER decoder is really shallow (one LSTM decoder layer), an interesting question is: can we improve decoding by training a new deeper decoder?", "mentionContextAttributes": {"used": {"value": false, "score": 0.0001424551010131836}, "created": {"value": false, "score": 0.0008955597877502441}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999927282333374}, "created": {"value": true, "score": 0.9914092421531677}, "shared": {"value": false, "score": 8.344650268554688e-07}}, "references": [{"label": "(Artetxe and Schwenk, 2019)", "normalizedForm": "Artetxe and Schwenk, 2019", "refKey": 3}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "LASER", "normalizedForm": "LASER", "offsetStart": 28, "offsetEnd": 33}, "context": "These experiments show that LASER sentence embeddings can be better decoded by training a new decoder on a large amount of raw text data. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.00035572052001953125}, "created": {"value": false, "score": 0.001162707805633545}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999927282333374}, "created": {"value": true, "score": 0.9914092421531677}, "shared": {"value": false, "score": 8.344650268554688e-07}}, "references": [{"label": "(Artetxe and Schwenk, 2019)", "normalizedForm": "Artetxe and Schwenk, 2019", "refKey": 3}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "CCnet", "normalizedForm": "CCnet", "offsetStart": 35, "offsetEnd": 40}, "context": "We used 15B English sentences from CCnet (Wenzek et al., 2019) to train the decoder.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9999839067459106}, "created": {"value": false, "score": 9.417533874511719e-06}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999839067459106}, "created": {"value": false, "score": 2.002716064453125e-05}, "shared": {"value": false, "score": 5.960464477539062e-07}}, "references": [{"label": "(Wenzek et al., 2019)", "normalizedForm": "Wenzek et al., 2019", "refKey": 45, "offsetStart": 12336, "offsetEnd": 12357}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "LASER", "normalizedForm": "LASER", "offsetStart": 41, "offsetEnd": 46}, "context": "We compare the new decoder with original LASER decoder on the auto-encoding task and the German-English translation task of FLO-RES in Figure 2.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9304730296134949}, "created": {"value": false, "score": 0.00040775537490844727}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999927282333374}, "created": {"value": true, "score": 0.9914092421531677}, "shared": {"value": false, "score": 8.344650268554688e-07}}, "references": [{"label": "(Artetxe and Schwenk, 2019)", "normalizedForm": "Artetxe and Schwenk, 2019", "refKey": 3}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "spBLEU", "normalizedForm": "spBLEU", "offsetStart": 45, "offsetEnd": 51}, "context": "For text-to-text translation results, we use spBLEU of M2M-100 with the public checkpoint and script to evaluate on FLORES.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9999675750732422}, "created": {"value": false, "score": 1.3113021850585938e-06}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999675750732422}, "created": {"value": false, "score": 1.3113021850585938e-06}, "shared": {"value": false, "score": 1.1920928955078125e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "CCMatrix", "normalizedForm": "CCMatrix", "offsetStart": 48, "offsetEnd": 78}, "context": "For all decoder trainings, we use bi-texts from CCMatrix (Schwenk et al., 2021), for the auto-encoding loss we use one side of the bi-texts corresponding to the language that we are trying to decode, except for Mongolian where we take all the raw Mongolian text data from CCnet. (Wenzek et al., 2019). ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9654121398925781}, "created": {"value": false, "score": 5.0067901611328125e-06}, "shared": {"value": false, "score": 5.960464477539062e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9998512268066406}, "created": {"value": false, "score": 0.00048744678497314453}, "shared": {"value": false, "score": 3.2186508178710938e-06}}, "references": [{"label": "(Schwenk et al., 2021)", "normalizedForm": "Schwenk et al., 2021", "refKey": 38}, {"label": "(Schwenk et al., 2021)", "normalizedForm": "Schwenk et al., 2021", "refKey": 38}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "LASER", "normalizedForm": "LASER", "offsetStart": 53, "offsetEnd": 58}, "context": "The English speech encoder previously trained to fit LASER text space on CoVoST 2 training set is used to encode raw speech, and its weights are not updated during training.", "mentionContextAttributes": {"used": {"value": true, "score": 0.8423865437507629}, "created": {"value": false, "score": 3.743171691894531e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999927282333374}, "created": {"value": true, "score": 0.9914092421531677}, "shared": {"value": false, "score": 8.344650268554688e-07}}, "references": [{"label": "(Artetxe and Schwenk, 2019)", "normalizedForm": "Artetxe and Schwenk, 2019", "refKey": 3}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "LASER", "normalizedForm": "LASER", "offsetStart": 55, "offsetEnd": 60}, "context": "Recently, Duquenne et al. (2021) extended the existing LASER model (Artetxe and Schwenk, 2019) built for multilingual text to the speech modality for several spoken languages.", "mentionContextAttributes": {"used": {"value": false, "score": 0.00017756223678588867}, "created": {"value": false, "score": 0.2008988857269287}, "shared": {"value": false, "score": 4.76837158203125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999927282333374}, "created": {"value": true, "score": 0.9914092421531677}, "shared": {"value": false, "score": 8.344650268554688e-07}}, "references": [{"label": "(Artetxe and Schwenk, 2019)", "normalizedForm": "Artetxe and Schwenk, 2019", "refKey": 3, "offsetStart": 4957, "offsetEnd": 4984}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "LASER", "normalizedForm": "LASER", "offsetStart": 58, "offsetEnd": 63}, "context": "We noticed that high resource languages are closer in the LASER space to English, compared to low resource languages.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9967001080513}, "created": {"value": false, "score": 0.00028258562088012695}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999927282333374}, "created": {"value": true, "score": 0.9914092421531677}, "shared": {"value": false, "score": 8.344650268554688e-07}}, "references": [{"label": "(Artetxe and Schwenk, 2019)", "normalizedForm": "Artetxe and Schwenk, 2019", "refKey": 3}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "LASER", "normalizedForm": "LASER", "offsetStart": 60, "offsetEnd": 65}, "context": "Unlike (Duquenne et al., 2021), we did not use the original LASER encoder to encode text transcripts but our newly trained text students which are supposed to be close to the LASER English embeddings.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9973469972610474}, "created": {"value": false, "score": 0.00017827749252319336}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999927282333374}, "created": {"value": true, "score": 0.9914092421531677}, "shared": {"value": false, "score": 8.344650268554688e-07}}, "references": [{"label": "(Artetxe and Schwenk, 2019)", "normalizedForm": "Artetxe and Schwenk, 2019", "refKey": 3}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "LASER", "normalizedForm": "LASER", "offsetStart": 63, "offsetEnd": 68}, "context": "The best pooling method seems to be max-pooling, maybe because LASER has been trained with max-pooling.", "mentionContextAttributes": {"used": {"value": false, "score": 0.0001647472381591797}, "created": {"value": false, "score": 2.562999725341797e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999927282333374}, "created": {"value": true, "score": 0.9914092421531677}, "shared": {"value": false, "score": 8.344650268554688e-07}}, "references": [{"label": "(Artetxe and Schwenk, 2019)", "normalizedForm": "Artetxe and Schwenk, 2019", "refKey": 3}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "LASER", "normalizedForm": "LASER", "offsetStart": 64, "offsetEnd": 69}, "context": "Training new decoders We chose to train a new decoder to decode LASER sentence embeddings, with a transformer architecture and 12 layers.", "mentionContextAttributes": {"used": {"value": false, "score": 0.0009181499481201172}, "created": {"value": true, "score": 0.8692856431007385}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999927282333374}, "created": {"value": true, "score": 0.9914092421531677}, "shared": {"value": false, "score": 8.344650268554688e-07}}, "references": [{"label": "(Artetxe and Schwenk, 2019)", "normalizedForm": "Artetxe and Schwenk, 2019", "refKey": 3}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "CCMatrix", "normalizedForm": "CCMatrix", "offsetStart": 65, "offsetEnd": 95}, "context": "When training with additional bi-text data, we use bi-texts from CCMatrix (Schwenk et al., 2021), and the English part of the bi-texts for the auxiliary auto-encoding loss in order to have a good balance between bitexts and raw data. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.7306040525436401}, "created": {"value": false, "score": 2.4080276489257812e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9998512268066406}, "created": {"value": false, "score": 0.00048744678497314453}, "shared": {"value": false, "score": 3.2186508178710938e-06}}, "references": [{"label": "(Schwenk et al., 2021)", "normalizedForm": "Schwenk et al., 2021", "refKey": 38}, {"label": "(Schwenk et al., 2021)", "normalizedForm": "Schwenk et al., 2021", "refKey": 38}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "LASER", "normalizedForm": "LASER", "offsetStart": 67, "offsetEnd": 72}, "context": "Motivations To get an idea of the closeness of translations in the LASER space, we inspected the L2 squared distances of sentence embeddings in different languages to their English translations sentence embeddings.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9999927282333374}, "created": {"value": false, "score": 0.00021666288375854492}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999927282333374}, "created": {"value": true, "score": 0.9914092421531677}, "shared": {"value": false, "score": 8.344650268554688e-07}}, "references": [{"label": "(Artetxe and Schwenk, 2019)", "normalizedForm": "Artetxe and Schwenk, 2019", "refKey": 3}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "CCMatrix", "normalizedForm": "CCMatrix", "offsetStart": 75, "offsetEnd": 83}, "context": "LASER has been successfully used for large-scale bitext mining like in the CCMatrix project (Schwenk et al., 2021). ", "mentionContextAttributes": {"used": {"value": false, "score": 0.00018805265426635742}, "created": {"value": false, "score": 0.00048744678497314453}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9998512268066406}, "created": {"value": false, "score": 0.00048744678497314453}, "shared": {"value": false, "score": 3.2186508178710938e-06}}, "references": [{"label": "(Schwenk et al., 2021)", "normalizedForm": "Schwenk et al., 2021", "refKey": 38, "offsetStart": 10487, "offsetEnd": 10509}, {"label": "(Schwenk et al., 2021)", "normalizedForm": "Schwenk et al., 2021", "refKey": 38, "offsetStart": 10487, "offsetEnd": 10509}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "LaBSE", "normalizedForm": "LaBSE", "offsetStart": 75, "offsetEnd": 100}, "context": "LASER has been trained with a decoding objective, whereas other works like LaBSE (Feng et al., 2020) have been trained with a contrastive objective.", "mentionContextAttributes": {"used": {"value": false, "score": 0.00026983022689819336}, "created": {"value": false, "score": 0.0004011988639831543}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": false, "score": 0.00026983022689819336}, "created": {"value": false, "score": 0.0004011988639831543}, "shared": {"value": false, "score": 2.384185791015625e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "LASER", "normalizedForm": "LASER", "offsetStart": 77, "offsetEnd": 82}, "context": "We use CCMatrix bi-texts to learn our text students, and bi-texts mined with LASER3 (Heffernan et al., 2022) for Mongolian.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9985257983207703}, "created": {"value": false, "score": 1.9788742065429688e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999927282333374}, "created": {"value": true, "score": 0.9914092421531677}, "shared": {"value": false, "score": 8.344650268554688e-07}}, "references": [{"label": "(Heffernan et al., 2022)", "normalizedForm": "Heffernan et al., 2022", "refKey": 25, "offsetStart": 17854, "offsetEnd": 17878}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "LASER", "normalizedForm": "LASER", "offsetStart": 85, "offsetEnd": 90}, "context": "We studied how our newly trained decoder is performing on a more distant language in LASER space, Japanese.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9997583031654358}, "created": {"value": true, "score": 0.9885864853858948}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999927282333374}, "created": {"value": true, "score": 0.9914092421531677}, "shared": {"value": false, "score": 8.344650268554688e-07}}, "references": [{"label": "(Artetxe and Schwenk, 2019)", "normalizedForm": "Artetxe and Schwenk, 2019", "refKey": 3}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "LASER", "normalizedForm": "LASER", "offsetStart": 88, "offsetEnd": 93}, "context": "Using this new Japanese encoder, our new decoder significantly outperforms the original LASER encoder.", "mentionContextAttributes": {"used": {"value": false, "score": 0.00022047758102416992}, "created": {"value": true, "score": 0.687781572341919}, "shared": {"value": false, "score": 4.76837158203125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999927282333374}, "created": {"value": true, "score": 0.9914092421531677}, "shared": {"value": false, "score": 8.344650268554688e-07}}, "references": [{"label": "(Artetxe and Schwenk, 2019)", "normalizedForm": "Artetxe and Schwenk, 2019", "refKey": 3}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "LASER", "normalizedForm": "LASER", "offsetStart": 91, "offsetEnd": 96}, "context": "This new decoder can be used to decode sentence embeddings from other languages handled by LASER. ", "mentionContextAttributes": {"used": {"value": false, "score": 5.8531761169433594e-05}, "created": {"value": false, "score": 0.17323213815689087}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999927282333374}, "created": {"value": true, "score": 0.9914092421531677}, "shared": {"value": false, "score": 8.344650268554688e-07}}, "references": [{"label": "(Artetxe and Schwenk, 2019)", "normalizedForm": "Artetxe and Schwenk, 2019", "refKey": 3}]}, {"type": "software", "software-type": "implicit", "software-name": {"rawForm": "script", "normalizedForm": "script", "offsetStart": 94, "offsetEnd": 100}, "context": "For text-to-text translation results, we use spBLEU of M2M-100 with the public checkpoint and script to evaluate on FLORES.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9999675750732422}, "created": {"value": false, "score": 1.3113021850585938e-06}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999675750732422}, "created": {"value": false, "score": 1.3113021850585938e-06}, "shared": {"value": false, "score": 1.1920928955078125e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "LASER", "normalizedForm": "LASER", "offsetStart": 95, "offsetEnd": 100}, "context": "Duquenne et al. (2021) showed that it is possible to learn speech students compatible with the LASER text space.", "mentionContextAttributes": {"used": {"value": false, "score": 0.04365664720535278}, "created": {"value": false, "score": 0.004033803939819336}, "shared": {"value": false, "score": 4.76837158203125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999927282333374}, "created": {"value": true, "score": 0.9914092421531677}, "shared": {"value": false, "score": 8.344650268554688e-07}}, "references": [{"label": "(Artetxe and Schwenk, 2019)", "normalizedForm": "Artetxe and Schwenk, 2019", "refKey": 3}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "CCnet", "normalizedForm": "CCnet", "offsetStart": 95, "offsetEnd": 121}, "context": "When training a decoder with raw text data, we use 15 billion English sentences extracted from CCnet (Wenzek et al., 2019). ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9699977040290833}, "created": {"value": false, "score": 2.002716064453125e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999839067459106}, "created": {"value": false, "score": 2.002716064453125e-05}, "shared": {"value": false, "score": 5.960464477539062e-07}}, "references": [{"label": "(Wenzek et al., 2019)", "normalizedForm": "Wenzek et al., 2019", "refKey": 45}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "LASER", "normalizedForm": "LASER", "offsetStart": 99, "offsetEnd": 104}, "context": "We notice that both decoders performs poorly on the ja-en translation tasks, and that the original LASER decoder leads to better results.", "mentionContextAttributes": {"used": {"value": false, "score": 0.007225751876831055}, "created": {"value": false, "score": 0.0001367330551147461}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999927282333374}, "created": {"value": true, "score": 0.9914092421531677}, "shared": {"value": false, "score": 8.344650268554688e-07}}, "references": [{"label": "(Artetxe and Schwenk, 2019)", "normalizedForm": "Artetxe and Schwenk, 2019", "refKey": 3}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "LASER", "normalizedForm": "LASER", "offsetStart": 122, "offsetEnd": 127}, "context": "To train this new decoder, we use an auto-encoding objective, feeding raw English sentences to the model: we use original LASER encoder, whose weights are not updated during training, and plug a new transformer decoder to decode the fixed-size sentence representation output by the LASER encoder (the decoder attends on the sentence embedding output by the encoder).", "mentionContextAttributes": {"used": {"value": true, "score": 0.9295647740364075}, "created": {"value": false, "score": 0.00033152103424072266}, "shared": {"value": false, "score": 8.344650268554688e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999927282333374}, "created": {"value": true, "score": 0.9914092421531677}, "shared": {"value": false, "score": 8.344650268554688e-07}}, "references": [{"label": "(Artetxe and Schwenk, 2019)", "normalizedForm": "Artetxe and Schwenk, 2019", "refKey": 3}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "LASER", "normalizedForm": "LASER", "offsetStart": 134, "offsetEnd": 139}, "context": "We report the L2 squared distances of sentence embeddings in different languages to their English translations sentence embeddings in LASER space.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9999822378158569}, "created": {"value": false, "score": 7.62939453125e-06}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999927282333374}, "created": {"value": true, "score": 0.9914092421531677}, "shared": {"value": false, "score": 8.344650268554688e-07}}, "references": [{"label": "(Artetxe and Schwenk, 2019)", "normalizedForm": "Artetxe and Schwenk, 2019", "refKey": 3}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "LASER", "normalizedForm": "LASER", "offsetStart": 143, "offsetEnd": 148}, "context": "As an initial experiment, we evaluated auto-encoding of English sentences from FLORES (Goyal et al., 2022) in Figure 2 left, with the original LASER encoder and decoder, bucketing sentences by length, and reporting BLEU scores.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9999266862869263}, "created": {"value": false, "score": 0.0006254315376281738}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999927282333374}, "created": {"value": true, "score": 0.9914092421531677}, "shared": {"value": false, "score": 8.344650268554688e-07}}, "references": [{"label": "(Artetxe and Schwenk, 2019)", "normalizedForm": "Artetxe and Schwenk, 2019", "refKey": 3}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "LASER", "normalizedForm": "LASER", "offsetStart": 150, "offsetEnd": 155}, "context": "They fine-tune XLSR, a multilingual pretrained model for speech and minimized the cosine loss between the output of the speech encoder and the target LASER sentence embedding.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9906156659126282}, "created": {"value": false, "score": 0.006545901298522949}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999927282333374}, "created": {"value": true, "score": 0.9914092421531677}, "shared": {"value": false, "score": 8.344650268554688e-07}}, "references": [{"label": "(Artetxe and Schwenk, 2019)", "normalizedForm": "Artetxe and Schwenk, 2019", "refKey": 3}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "LASER", "normalizedForm": "LASER", "offsetStart": 155, "offsetEnd": 160}, "context": "The LASER encoder handles several languages: decoding these multilingual embeddings enables to translate the input sentence into English with the original LASER decoder.", "mentionContextAttributes": {"used": {"value": false, "score": 0.0019287467002868652}, "created": {"value": false, "score": 2.8252601623535156e-05}, "shared": {"value": false, "score": 4.76837158203125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999927282333374}, "created": {"value": true, "score": 0.9914092421531677}, "shared": {"value": false, "score": 8.344650268554688e-07}}, "references": [{"label": "(Artetxe and Schwenk, 2019)", "normalizedForm": "Artetxe and Schwenk, 2019", "refKey": 3}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "LASER", "normalizedForm": "LASER", "offsetStart": 164, "offsetEnd": 169}, "context": "The new decoder can be directly applied to German sentence embeddings because German embeddings are supposed to be close to their English translations encoded with LASER.", "mentionContextAttributes": {"used": {"value": false, "score": 0.0002754330635070801}, "created": {"value": false, "score": 0.0005523562431335449}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999927282333374}, "created": {"value": true, "score": 0.9914092421531677}, "shared": {"value": false, "score": 8.344650268554688e-07}}, "references": [{"label": "(Artetxe and Schwenk, 2019)", "normalizedForm": "Artetxe and Schwenk, 2019", "refKey": 3}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "LASER", "normalizedForm": "LASER", "offsetStart": 167, "offsetEnd": 172}, "context": "We explore several variants of teacher-student training to learn text and speech encoders for multiple languages, which are compatible with the embedding space of the LASER encoder (Artetxe and Schwenk, 2019).", "mentionContextAttributes": {"used": {"value": false, "score": 6.008148193359375e-05}, "created": {"value": true, "score": 0.9761329293251038}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999927282333374}, "created": {"value": true, "score": 0.9914092421531677}, "shared": {"value": false, "score": 8.344650268554688e-07}}, "references": [{"label": "(Artetxe and Schwenk, 2019)", "normalizedForm": "Artetxe and Schwenk, 2019", "refKey": 3, "offsetStart": 2610, "offsetEnd": 2637}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "LASER", "normalizedForm": "LASER", "offsetStart": 167, "offsetEnd": 172}, "context": "We summarize the speech decoder training in Figure 4. Another method is to leverage English speech recognition data where English text transcripts are encoded through LASER encoder which weights are fixed during training and a decoder predicts the sequence of units of the corresponding speech.", "mentionContextAttributes": {"used": {"value": false, "score": 0.0015366077423095703}, "created": {"value": false, "score": 0.0010666251182556152}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999927282333374}, "created": {"value": true, "score": 0.9914092421531677}, "shared": {"value": false, "score": 8.344650268554688e-07}}, "references": [{"label": "(Artetxe and Schwenk, 2019)", "normalizedForm": "Artetxe and Schwenk, 2019", "refKey": 3}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "LASER", "normalizedForm": "LASER", "offsetStart": 175, "offsetEnd": 180}, "context": "Unlike (Duquenne et al., 2021), we did not use the original LASER encoder to encode text transcripts but our newly trained text students which are supposed to be close to the LASER English embeddings.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9973468780517578}, "created": {"value": false, "score": 0.00017827749252319336}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999927282333374}, "created": {"value": true, "score": 0.9914092421531677}, "shared": {"value": false, "score": 8.344650268554688e-07}}, "references": [{"label": "(Artetxe and Schwenk, 2019)", "normalizedForm": "Artetxe and Schwenk, 2019", "refKey": 3}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Deepnet", "normalizedForm": "Deepnet", "offsetStart": 201, "offsetEnd": 208}, "context": "This work -zero-shot expect for en-de de-de+en-de decoder 39.1 -32.6 24.6 29.2 20.9 27.9 12.8 Previous works -supervised M2M-100 (12B -48 layers) (Fan et al., 2021) 42.1 -34.5 27.1 30.9 21.4 28.4 15.9 Deepnet (3.2B -200 layers) (Wang et al., 2022)   (Fan et al., 2021) 12.0 10.7 10.9 9.2 10.8 9.3 11.0 -Deepnet (3.2B -200 layers) (Wang et al., 2022) 18.3 16.8 16.2 15.0 15.8 13.7 15.9 -Table 7: BLEU on FLORES devtest for text-to-text translation for de, es, fr, tr and mn decoders", "mentionContextAttributes": {"used": {"value": false, "score": 0.3943629264831543}, "created": {"value": false, "score": 4.649162292480469e-06}, "shared": {"value": false, "score": 5.960464477539062e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9686853885650635}, "created": {"value": false, "score": 0.00019413232803344727}, "shared": {"value": false, "score": 1.430511474609375e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Deepnet", "normalizedForm": "Deepnet", "offsetStart": 213, "offsetEnd": 220}, "context": "Figure 3 summarizes the process to train a speech student with transcriptions only: First, we train a text student for the language we want to cover, we will  (Fan et al., 2021) 44.7 45.5 31.1 42.5 26.1 36.9 20.9 Deepnet (3.2B -200 layers) (Wang et al., 2022) 48.0 49.9 35.2 46.2 32.7 44.2 23.9 Table 2: BLEU on FLORES devtest for text-to-text xx-en translation using different English decoders.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9686853885650635}, "created": {"value": false, "score": 2.2649765014648438e-06}, "shared": {"value": false, "score": 1.430511474609375e-06}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9686853885650635}, "created": {"value": false, "score": 0.00019413232803344727}, "shared": {"value": false, "score": 1.430511474609375e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "CCMatrix", "normalizedForm": "CCMatrix", "offsetStart": 231, "offsetEnd": 239}, "context": "For text-to-text (T2T) translation, this labeled data, called bitexts, is available in large amounts for a number of language pairs, in particular since large-scale bitext mining initiatives like ParaCrawl (Ba\u00f1\u00f3n et al., 2020) and CCMatrix (Schwenk et al., 2021).", "mentionContextAttributes": {"used": {"value": false, "score": 0.0002339482307434082}, "created": {"value": false, "score": 5.507469177246094e-05}, "shared": {"value": false, "score": 3.2186508178710938e-06}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9998512268066406}, "created": {"value": false, "score": 0.00048744678497314453}, "shared": {"value": false, "score": 3.2186508178710938e-06}}, "references": [{"label": "(Schwenk et al., 2021)", "normalizedForm": "Schwenk et al., 2021", "refKey": 38, "offsetStart": 1261, "offsetEnd": 1283}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "LASER", "normalizedForm": "LASER", "offsetStart": 252, "offsetEnd": 257}, "context": "To extract the sentence embedding, we tested two methods: The classical output of the encoder corresponding to the beginning-of-sentence (BOS) token, a method widely used for text classification ; or max-pooling of the encoder outputs, less common but LASER has been trained with such pooling method.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9999614953994751}, "created": {"value": false, "score": 0.0018908977508544922}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999927282333374}, "created": {"value": true, "score": 0.9914092421531677}, "shared": {"value": false, "score": 8.344650268554688e-07}}, "references": [{"label": "(Artetxe and Schwenk, 2019)", "normalizedForm": "Artetxe and Schwenk, 2019", "refKey": 3}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Deepnet", "normalizedForm": "Deepnet", "offsetStart": 254, "offsetEnd": 281}, "context": "We compare our zero-shot text-to-text translation results with two supervised baselines: M2M-100 (Fan et al., 2021), a massively multilingual trained on many-to-many training data from different sources, with 24 encoder layers and 24 decoder layers; and Deepnet (Wang et al., 2022) a recent work trained on 1932 language directions from different sources with 100 encoder layers and 100 decoder layers. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9167784452438354}, "created": {"value": false, "score": 0.00019413232803344727}, "shared": {"value": false, "score": 7.152557373046875e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9686853885650635}, "created": {"value": false, "score": 0.00019413232803344727}, "shared": {"value": false, "score": 1.430511474609375e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "CCnet", "normalizedForm": "CCnet", "offsetStart": 272, "offsetEnd": 277}, "context": "For all decoder trainings, we use bi-texts from CCMatrix (Schwenk et al., 2021), for the auto-encoding loss we use one side of the bi-texts corresponding to the language that we are trying to decode, except for Mongolian where we take all the raw Mongolian text data from CCnet. (Wenzek et al., 2019). ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9654120206832886}, "created": {"value": false, "score": 5.0067901611328125e-06}, "shared": {"value": false, "score": 5.960464477539062e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999839067459106}, "created": {"value": false, "score": 2.002716064453125e-05}, "shared": {"value": false, "score": 5.960464477539062e-07}}, "references": [{"label": "(Wenzek et al., 2019)", "normalizedForm": "Wenzek et al., 2019", "refKey": 45, "offsetStart": 34471, "offsetEnd": 34492}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "LASER", "normalizedForm": "LASER", "offsetStart": 282, "offsetEnd": 287}, "context": "To train this new decoder, we use an auto-encoding objective, feeding raw English sentences to the model: we use original LASER encoder, whose weights are not updated during training, and plug a new transformer decoder to decode the fixed-size sentence representation output by the LASER encoder (the decoder attends on the sentence embedding output by the encoder).", "mentionContextAttributes": {"used": {"value": true, "score": 0.9295647740364075}, "created": {"value": false, "score": 0.00033152103424072266}, "shared": {"value": false, "score": 8.344650268554688e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999927282333374}, "created": {"value": true, "score": 0.9914092421531677}, "shared": {"value": false, "score": 8.344650268554688e-07}}, "references": [{"label": "(Artetxe and Schwenk, 2019)", "normalizedForm": "Artetxe and Schwenk, 2019", "refKey": 3}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Deepnet", "normalizedForm": "Deepnet", "offsetStart": 303, "offsetEnd": 310}, "context": "This work -zero-shot expect for en-de de-de+en-de decoder 39.1 -32.6 24.6 29.2 20.9 27.9 12.8 Previous works -supervised M2M-100 (12B -48 layers) (Fan et al., 2021) 42.1 -34.5 27.1 30.9 21.4 28.4 15.9 Deepnet (3.2B -200 layers) (Wang et al., 2022)   (Fan et al., 2021) 12.0 10.7 10.9 9.2 10.8 9.3 11.0 -Deepnet (3.2B -200 layers) (Wang et al., 2022) 18.3 16.8 16.2 15.0 15.8 13.7 15.9 -Table 7: BLEU on FLORES devtest for text-to-text translation for de, es, fr, tr and mn decoders", "mentionContextAttributes": {"used": {"value": false, "score": 0.3943629264831543}, "created": {"value": false, "score": 4.649162292480469e-06}, "shared": {"value": false, "score": 5.960464477539062e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9686853885650635}, "created": {"value": false, "score": 0.00019413232803344727}, "shared": {"value": false, "score": 1.430511474609375e-06}}}], "references": [{"refKey": 3, "tei": "<biblStruct xml:id=\"b3\">\n\t<analytic>\n\t\t<title level=\"a\" type=\"main\">Massively Multilingual Sentence Embeddings for Zero-Shot Cross-Lingual Transfer and Beyond</title>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Mikel</forename><surname>Artetxe</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Holger</forename><surname>Schwenk</surname></persName>\n\t\t</author>\n\t\t<idno type=\"DOI\">10.1162/tacl_a_00288</idno>\n\t</analytic>\n\t<monogr>\n\t\t<title level=\"j\">Transactions of the Association for Computational Linguistics</title>\n\t\t<title level=\"j\" type=\"abbrev\">Transactions of the Association for Computational Linguistics</title>\n\t\t<idno type=\"ISSNe\">2307-387X</idno>\n\t\t<imprint>\n\t\t\t<biblScope unit=\"volume\">7</biblScope>\n\t\t\t<biblScope unit=\"page\" from=\"597\" to=\"610\" />\n\t\t\t<date type=\"published\" when=\"2019-11\">2019</date>\n\t\t\t<publisher>MIT Press - Journals</publisher>\n\t\t</imprint>\n\t</monogr>\n</biblStruct>\n"}, {"refKey": 38, "tei": "<biblStruct xml:id=\"b38\">\n\t<analytic>\n\t\t<title level=\"a\" type=\"main\">CCMatrix: Mining Billions of High-Quality Parallel Sentences on the Web</title>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Holger</forename><surname>Schwenk</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Guillaume</forename><surname>Wenzek</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Sergey</forename><surname>Edunov</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Edouard</forename><surname>Grave</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Armand</forename><surname>Joulin</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Angela</forename><surname>Fan</surname></persName>\n\t\t</author>\n\t\t<idno type=\"DOI\">10.18653/v1/2021.acl-long.507</idno>\n\t</analytic>\n\t<monogr>\n\t\t<title level=\"m\">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)</title>\n\t\t<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)</meeting>\n\t\t<imprint>\n\t\t\t<publisher>Association for Computational Linguistics</publisher>\n\t\t\t<date type=\"published\" when=\"2021\">2021</date>\n\t\t\t<biblScope unit=\"page\" from=\"6490\" to=\"6500\" />\n\t\t</imprint>\n\t</monogr>\n</biblStruct>\n"}, {"refKey": 45, "tei": "<biblStruct xml:id=\"b45\">\n\t<monogr>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Guillaume</forename><surname>Wenzek</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Marie-Anne</forename><surname>Lachaux</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Alexis</forename><surname>Conneau</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Vishrav</forename><surname>Chaudhary</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Francisco</forename><surname>Guzm\u00e1n</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Armand</forename><surname>Joulin</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Edouard</forename><surname>Grave</surname></persName>\n\t\t</author>\n\t\t<idno>arXiv:1911.00359</idno>\n\t\t<title level=\"m\">Ccnet: Extracting high quality monolingual datasets from web crawl data</title>\n\t\t<imprint>\n\t\t\t<date>2019</date>\n\t\t</imprint>\n\t</monogr>\n</biblStruct>\n"}, {"refKey": 25, "tei": "<biblStruct xml:id=\"b25\">\n\t<monogr>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Kevin</forename><surname>Heffernan</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Onur</forename><surname>\u00c7elebi</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Holger</forename><surname>Schwenk</surname></persName>\n\t\t</author>\n\t\t<idno>arXiv:2205.12654</idno>\n\t\t<title level=\"m\">Bitext mining using distilled sentence representations for low-resource languages</title>\n\t\t<imprint>\n\t\t\t<date>2022</date>\n\t\t</imprint>\n\t</monogr>\n</biblStruct>\n"}], "runtime": 14967, "id": "c02904fdb81a12194a735e12ac504844cf5b5574", "metadata": {"id": "c02904fdb81a12194a735e12ac504844cf5b5574"}, "original_file_path": "../../datalake/Samuel/SOFTware-Sync/downloads/xml/hal-03834732.grobid.tei.xml", "file_name": "hal-03834732.grobid.tei.xml"}