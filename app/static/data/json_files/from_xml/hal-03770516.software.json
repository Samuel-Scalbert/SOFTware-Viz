{"application": "software-mentions", "version": "0.8.0", "date": "2024-10-07T11:51+0000", "md5": "4FCBFC2E8C85D406BF763E1789F1CD48", "mentions": [{"type": "software", "software-type": "software", "software-name": {"rawForm": "CQAPri", "normalizedForm": "CQAPri", "offsetStart": 0, "offsetEnd": 6}, "context": "CQAPri makes a single SAT call for each candidate query answer, whereas CAvSAT treats all candidate answers at the same time via calls to a weighted MaxSAT solver.", "mentionContextAttributes": {"used": {"value": false, "score": 0.0017937421798706055}, "created": {"value": false, "score": 3.0994415283203125e-06}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999769926071167}, "created": {"value": false, "score": 0.3470569849014282}, "shared": {"value": false, "score": 7.152557373046875e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "ORBITS", "normalizedForm": "ORBITS", "offsetStart": 0, "offsetEnd": 6}, "url": {"rawForm": "https://github. com/", "normalizedForm": "https://github. com"}, "context": "ORBITS relies on the Sat4j java library (version 2.3.4) to solve the SAT, weighted MaxSAT, and MUS enumeration problems (Berre and Parrain 2010). ", "mentionContextAttributes": {"used": {"value": false, "score": 0.1407625675201416}, "created": {"value": false, "score": 1.800060272216797e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999842643737793}, "created": {"value": true, "score": 0.9999393224716187}, "shared": {"value": true, "score": 0.9904106855392456}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "CAvSAT", "normalizedForm": "CAvSAT", "offsetStart": 0, "offsetEnd": 32}, "context": "CAvSAT (Dixit and Kolaitis 2019) targets relational databases equipped with denial constraints (which include FDs as a special case) and computes query answers under the AR semantics w.r.t. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.000141143798828125}, "created": {"value": false, "score": 2.288818359375e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9429602026939392}, "created": {"value": false, "score": 0.3470569849014282}, "shared": {"value": false, "score": 4.76837158203125e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "CQAPri", "normalizedForm": "CQAPri", "offsetStart": 0, "offsetEnd": 51}, "context": "CQAPri (Bienvenu, Bourgaux, and Goasdou\u00e9 2014;2019) uses tractable approximations together with calls to SAT solvers to answer queries over inconsistent DL-Lite knowledge bases, under the AR, brave, and IAR semantics, w.r.t. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.0002700686454772949}, "created": {"value": false, "score": 1.049041748046875e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999769926071167}, "created": {"value": false, "score": 0.3470569849014282}, "shared": {"value": false, "score": 7.152557373046875e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Simple", "normalizedForm": "Simple", "offsetStart": 2, "offsetEnd": 8}, "context": "\u2022 Simple is similar to the algorithm used by CQAPri.", "mentionContextAttributes": {"used": {"value": false, "score": 0.00013697147369384766}, "created": {"value": false, "score": 0.0010748505592346191}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": false, "score": 0.0807805061340332}, "created": {"value": false, "score": 0.0010748505592346191}, "shared": {"value": false, "score": 3.5762786865234375e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "CQAPri", "normalizedForm": "CQAPri", "offsetStart": 8, "offsetEnd": 14}, "context": "For the CQAPri benchmark, we build two priority relations, one score-structured with n = 5 and one non-score-structured with p = 0.8, on our largest dataset (u20c50), then propagate them to the other datasets.", "mentionContextAttributes": {"used": {"value": true, "score": 0.7679123878479004}, "created": {"value": false, "score": 0.00043970346450805664}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999769926071167}, "created": {"value": false, "score": 0.3470569849014282}, "shared": {"value": false, "score": 7.152557373046875e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "ORBITS", "normalizedForm": "ORBITS", "offsetStart": 11, "offsetEnd": 17}, "url": {"rawForm": "https://github. com/", "normalizedForm": "https://github. com"}, "context": "Our system ORBITS (Optimal Repair-Based Inconsistency-Tolerant Semantics) takes as input two JSON files containing the directed conflict graph G K , and the potential answers PotAns of the query associated with their causes. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.0003675222396850586}, "created": {"value": true, "score": 0.6871989369392395}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999842643737793}, "created": {"value": true, "score": 0.9999393224716187}, "shared": {"value": true, "score": 0.9904106855392456}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "CQAPri", "normalizedForm": "CQAPri", "offsetStart": 17, "offsetEnd": 23}, "context": "The first is the CQAPri benchmark (Bourgaux 2016), a synthetic benchmark crafted to evaluate inconsistency-tolerant query answering over DL-Lite KBs, adapted from the LUBM \u2203 20 benchmark (Lutz et al. 2013).", "mentionContextAttributes": {"used": {"value": false, "score": 0.002502739429473877}, "created": {"value": false, "score": 0.0006725788116455078}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999769926071167}, "created": {"value": false, "score": 0.3470569849014282}, "shared": {"value": false, "score": 7.152557373046875e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "ORBITS", "normalizedForm": "ORBITS", "offsetStart": 19, "offsetEnd": 25}, "url": {"rawForm": "https://github. com/", "normalizedForm": "https://github. com", "offsetStart": 42, "offsetEnd": 62}, "context": "The source code of ORBITS is available at https://github. com/bourgaux/orbits, the inputs files we used in the experiments at https://zenodo.org/record/5946827, ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9333693981170654}, "created": {"value": false, "score": 1.1682510375976562e-05}, "shared": {"value": true, "score": 0.9904106855392456}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999842643737793}, "created": {"value": true, "score": 0.9999393224716187}, "shared": {"value": true, "score": 0.9904106855392456}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Sat4j", "normalizedForm": "Sat4j", "offsetStart": 21, "offsetEnd": 26}, "version": {"rawForm": "2.3.4", "normalizedForm": "2.3.4", "offsetStart": 49, "offsetEnd": 54}, "context": "ORBITS relies on the Sat4j java library (version 2.3.4) to solve the SAT, weighted MaxSAT, and MUS enumeration problems (Berre and Parrain 2010). ", "mentionContextAttributes": {"used": {"value": false, "score": 0.1407625675201416}, "created": {"value": false, "score": 1.800060272216797e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9178310632705688}, "created": {"value": false, "score": 5.435943603515625e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "CQAPri", "normalizedForm": "CQAPri", "offsetStart": 21, "offsetEnd": 27}, "context": "Two recent sys-tems, CQAPri and CAvSAT, have begun to explore such an approach. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.0002015233039855957}, "created": {"value": false, "score": 0.21861928701400757}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999769926071167}, "created": {"value": false, "score": 0.3470569849014282}, "shared": {"value": false, "score": 7.152557373046875e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "ORBITS", "normalizedForm": "ORBITS", "offsetStart": 21, "offsetEnd": 27}, "url": {"rawForm": "https://github. com/", "normalizedForm": "https://github. com"}, "context": "Test KBs We evaluate ORBITS on three (sets of) KBs. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9999842643737793}, "created": {"value": false, "score": 2.1457672119140625e-06}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999842643737793}, "created": {"value": true, "score": 0.9999393224716187}, "shared": {"value": true, "score": 0.9904106855392456}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Simple", "normalizedForm": "Simple", "offsetStart": 26, "offsetEnd": 32}, "context": "The 'generic' algorithms (Simple, All-MaxSAT, All-MUSes, Assumptions) perform quite poorly, except on the simplest cases.", "mentionContextAttributes": {"used": {"value": false, "score": 0.002786576747894287}, "created": {"value": false, "score": 5.137920379638672e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": false, "score": 0.0807805061340332}, "created": {"value": false, "score": 0.0010748505592346191}, "shared": {"value": false, "score": 3.5762786865234375e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "CAvSAT", "normalizedForm": "CAvSAT", "offsetStart": 31, "offsetEnd": 37}, "context": "\u2022 All-MaxSAT is similar to the CAvSAT algorithm.", "mentionContextAttributes": {"used": {"value": false, "score": 6.389617919921875e-05}, "created": {"value": false, "score": 4.458427429199219e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9429602026939392}, "created": {"value": false, "score": 0.3470569849014282}, "shared": {"value": false, "score": 4.76837158203125e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "CAvSAT", "normalizedForm": "CAvSAT", "offsetStart": 32, "offsetEnd": 38}, "context": "Two recent sys-tems, CQAPri and CAvSAT, have begun to explore such an approach. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.0002015233039855957}, "created": {"value": false, "score": 0.21861928701400757}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9429602026939392}, "created": {"value": false, "score": 0.3470569849014282}, "shared": {"value": false, "score": 4.76837158203125e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "CQAPri", "normalizedForm": "CQAPri", "offsetStart": 45, "offsetEnd": 51}, "context": "\u2022 Simple is similar to the algorithm used by CQAPri.", "mentionContextAttributes": {"used": {"value": false, "score": 0.00013697147369384766}, "created": {"value": false, "score": 0.0010748505592346191}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999769926071167}, "created": {"value": false, "score": 0.3470569849014282}, "shared": {"value": false, "score": 7.152557373046875e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "ORBITS", "normalizedForm": "ORBITS", "offsetStart": 49, "offsetEnd": 55}, "url": {"rawForm": "https://github. com/", "normalizedForm": "https://github. com"}, "context": "In Section 5, we present our implemented system, ORBITS, which computes query answers under the chosen semantics using the selected encoding and algorithm. ", "mentionContextAttributes": {"used": {"value": false, "score": 3.8504600524902344e-05}, "created": {"value": true, "score": 0.9999393224716187}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999842643737793}, "created": {"value": true, "score": 0.9999393224716187}, "shared": {"value": true, "score": 0.9904106855392456}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "CQAPri", "normalizedForm": "CQAPri", "offsetStart": 53, "offsetEnd": 59}, "context": "Inspired by the different use of SAT solvers made by CQAPri and CAvSAT, we propose several algorithms based on the encodings of Section 3. The pseudo code of all algorithms is available in (Bienvenu and Bourgaux 2022).", "mentionContextAttributes": {"used": {"value": false, "score": 0.00016224384307861328}, "created": {"value": false, "score": 0.3470569849014282}, "shared": {"value": false, "score": 4.76837158203125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999769926071167}, "created": {"value": false, "score": 0.3470569849014282}, "shared": {"value": false, "score": 7.152557373046875e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "ORBITS", "normalizedForm": "ORBITS", "offsetStart": 54, "offsetEnd": 60}, "url": {"rawForm": "https://github. com/", "normalizedForm": "https://github. com"}, "context": "However, in real-world applications, we would not use ORBITS as a standalone tool, but rather make it a library to be integrated in a full query answering system.", "mentionContextAttributes": {"used": {"value": false, "score": 0.00017791986465454102}, "created": {"value": false, "score": 0.006343841552734375}, "shared": {"value": false, "score": 4.76837158203125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999842643737793}, "created": {"value": true, "score": 0.9999393224716187}, "shared": {"value": true, "score": 0.9904106855392456}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "CAvSAT", "normalizedForm": "CAvSAT", "offsetStart": 64, "offsetEnd": 70}, "context": "Inspired by the different use of SAT solvers made by CQAPri and CAvSAT, we propose several algorithms based on the encodings of Section 3. The pseudo code of all algorithms is available in (Bienvenu and Bourgaux 2022).", "mentionContextAttributes": {"used": {"value": false, "score": 0.00016224384307861328}, "created": {"value": false, "score": 0.3470569849014282}, "shared": {"value": false, "score": 4.76837158203125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9429602026939392}, "created": {"value": false, "score": 0.3470569849014282}, "shared": {"value": false, "score": 4.76837158203125e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "CAvSAT", "normalizedForm": "CAvSAT", "offsetStart": 72, "offsetEnd": 78}, "context": "CQAPri makes a single SAT call for each candidate query answer, whereas CAvSAT treats all candidate answers at the same time via calls to a weighted MaxSAT solver.", "mentionContextAttributes": {"used": {"value": false, "score": 0.0017937421798706055}, "created": {"value": false, "score": 3.0994415283203125e-06}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9429602026939392}, "created": {"value": false, "score": 0.3470569849014282}, "shared": {"value": false, "score": 4.76837158203125e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Simple", "normalizedForm": "Simple", "offsetStart": 85, "offsetEnd": 91}, "context": "For non-scorestructured priority relations and completion-optimal repairs, algorithm Simple seems to be the best choice for both C-AR and C-brave semantics, but all algorithms fail in most cases.", "mentionContextAttributes": {"used": {"value": false, "score": 8.571147918701172e-05}, "created": {"value": false, "score": 0.00017011165618896484}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": false, "score": 0.0807805061340332}, "created": {"value": false, "score": 0.0010748505592346191}, "shared": {"value": false, "score": 3.5762786865234375e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "CQAPri", "normalizedForm": "CQAPri", "offsetStart": 90, "offsetEnd": 96}, "context": "We did this comparison for all queries on Physicians and Food Inspection datasets and two CQAPri datasets.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9999769926071167}, "created": {"value": false, "score": 3.0040740966796875e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999769926071167}, "created": {"value": false, "score": 0.3470569849014282}, "shared": {"value": false, "score": 7.152557373046875e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "CQAPri", "normalizedForm": "CQAPri", "offsetStart": 91, "offsetEnd": 97}, "context": "We use the DL-Lite ontology (which includes 875 disjointness axioms) and 20 queries of the CQAPri benchmark, together with the 18 datasets named uXcY with X \u2208 {1, 5, 20} and Y \u2208 {1, 5, 10, 20, 30, 50}.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9924463629722595}, "created": {"value": false, "score": 1.5497207641601562e-06}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999769926071167}, "created": {"value": false, "score": 0.3470569849014282}, "shared": {"value": false, "score": 7.152557373046875e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "CentOS", "normalizedForm": "CentOS", "offsetStart": 93, "offsetEnd": 99}, "version": {"rawForm": "7.9", "normalizedForm": "7.9", "offsetStart": 100, "offsetEnd": 103}, "context": "Experimental Environment All experiments were run with 16GB of RAM in a cluster node running CentOS 7.9 with linux kernel 3.10.0, ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9999921321868896}, "created": {"value": false, "score": 8.702278137207031e-06}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999921321868896}, "created": {"value": false, "score": 8.702278137207031e-06}, "shared": {"value": false, "score": 2.384185791015625e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "CQAPri", "normalizedForm": "CQAPri", "offsetStart": 95, "offsetEnd": 101}, "context": "However, there are a few cases where \u03d5 P2-max performs significantly better, especially on the CQAPri datasets with fewer conflicts (e.g., on u20c1 with a score-structured priority relation, the best time for filtering q9 answers under P-AR semantics is 480ms with \u03d5 P2-max , while the best time with another encoding is 825ms).", "mentionContextAttributes": {"used": {"value": false, "score": 0.01051551103591919}, "created": {"value": false, "score": 2.7418136596679688e-06}, "shared": {"value": false, "score": 7.152557373046875e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999769926071167}, "created": {"value": false, "score": 0.3470569849014282}, "shared": {"value": false, "score": 7.152557373046875e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "ORBITS", "normalizedForm": "ORBITS", "offsetStart": 96, "offsetEnd": 102}, "url": {"rawForm": "https://github. com/", "normalizedForm": "https://github. com"}, "context": "Regarding the impact of the choice between Pareto-and completion-optimal repairs, in many cases ORBITS did not manage to compute the answers for completion-optimal repairs in our given time and memory limits. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9673571586608887}, "created": {"value": false, "score": 7.152557373046875e-06}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999842643737793}, "created": {"value": true, "score": 0.9999393224716187}, "shared": {"value": true, "score": 0.9904106855392456}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "CQAPri", "normalizedForm": "CQAPri", "offsetStart": 101, "offsetEnd": 107}, "context": "An interesting observation is that while, in the absence of a priority relation, many queries of the CQAPri benchmark do not have any AR answers that are not trivial, which makes trivial answers a good approximation of AR, this is no longer the case for optimal repairs.", "mentionContextAttributes": {"used": {"value": false, "score": 0.0011155009269714355}, "created": {"value": false, "score": 4.5299530029296875e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999769926071167}, "created": {"value": false, "score": 0.3470569849014282}, "shared": {"value": false, "score": 7.152557373046875e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "linux kernel", "normalizedForm": "linux kernel", "offsetStart": 109, "offsetEnd": 121}, "version": {"rawForm": "3.10.0", "normalizedForm": "3.10.0", "offsetStart": 122, "offsetEnd": 128}, "context": "Experimental Environment All experiments were run with 16GB of RAM in a cluster node running CentOS 7.9 with linux kernel 3.10.0, ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9999921321868896}, "created": {"value": false, "score": 8.702278137207031e-06}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999921321868896}, "created": {"value": false, "score": 8.702278137207031e-06}, "shared": {"value": false, "score": 2.384185791015625e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "CQAPri", "normalizedForm": "CQAPri", "offsetStart": 140, "offsetEnd": 146}, "context": "Absence or presence of causes To contradict a specific cause C, we use \u03d5 \u00acC , with two alternative definitions inspired respectively by the CQAPri and CAvSAT encodings:", "mentionContextAttributes": {"used": {"value": true, "score": 0.9429602026939392}, "created": {"value": false, "score": 8.344650268554688e-06}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999769926071167}, "created": {"value": false, "score": 0.3470569849014282}, "shared": {"value": false, "score": 7.152557373046875e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "CQAPri", "normalizedForm": "CQAPri", "offsetStart": 144, "offsetEnd": 150}, "context": "For S-AR, P-AR and P-brave, we observe different behaviours depending on the benchmark: All-MaxSAT and All-MUSes tend to perform better for the CQAPri benchmark, while Simple tends to perform better for the Food Inspection dataset.", "mentionContextAttributes": {"used": {"value": false, "score": 0.08078056573867798}, "created": {"value": false, "score": 1.3589859008789062e-05}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999769926071167}, "created": {"value": false, "score": 0.3470569849014282}, "shared": {"value": false, "score": 7.152557373046875e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "CAvSAT", "normalizedForm": "CAvSAT", "offsetStart": 151, "offsetEnd": 157}, "context": "Absence or presence of causes To contradict a specific cause C, we use \u03d5 \u00acC , with two alternative definitions inspired respectively by the CQAPri and CAvSAT encodings:", "mentionContextAttributes": {"used": {"value": true, "score": 0.9429602026939392}, "created": {"value": false, "score": 8.344650268554688e-06}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9429602026939392}, "created": {"value": false, "score": 0.3470569849014282}, "shared": {"value": false, "score": 4.76837158203125e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Simple", "normalizedForm": "Simple", "offsetStart": 168, "offsetEnd": 174}, "context": "For S-AR, P-AR and P-brave, we observe different behaviours depending on the benchmark: All-MaxSAT and All-MUSes tend to perform better for the CQAPri benchmark, while Simple tends to perform better for the Food Inspection dataset.", "mentionContextAttributes": {"used": {"value": false, "score": 0.0807805061340332}, "created": {"value": false, "score": 1.3589859008789062e-05}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "documentContextAttributes": {"used": {"value": false, "score": 0.0807805061340332}, "created": {"value": false, "score": 0.0010748505592346191}, "shared": {"value": false, "score": 3.5762786865234375e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Sat4j", "normalizedForm": "Sat4j", "offsetStart": 184, "offsetEnd": 189}, "version": {"rawForm": "2.3.4", "normalizedForm": "2.3.4"}, "context": "In principle, a standalone solver could be used, but we found that the time needed to print out the encoding to pass it to an external solver tends to be prohibitive compared to using Sat4j.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9178310632705688}, "created": {"value": false, "score": 5.435943603515625e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9178310632705688}, "created": {"value": false, "score": 5.435943603515625e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Simple", "normalizedForm": "Simple", "offsetStart": 321, "offsetEnd": 327}, "context": "It illustrates the fact that the relative performance of the algorithms depends on the query and dataset (here, the proportion of facts involved in some conflict): For example, All-MaxSAT is the best for q9 over the three first datasets, but runs out of time on the three last (more that 20% of facts in conflict), while Simple can handle them.", "mentionContextAttributes": {"used": {"value": false, "score": 0.003852248191833496}, "created": {"value": false, "score": 4.5299530029296875e-06}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "documentContextAttributes": {"used": {"value": false, "score": 0.0807805061340332}, "created": {"value": false, "score": 0.0010748505592346191}, "shared": {"value": false, "score": 3.5762786865234375e-07}}}], "references": [], "runtime": 12834, "id": "baf4856fa7778014b0ad44fe1905bdae950a27ec", "metadata": {"id": "baf4856fa7778014b0ad44fe1905bdae950a27ec"}, "original_file_path": "../../datalake/Samuel/SOFTware-Sync/downloads/xml/hal-03770516.grobid.tei.xml", "file_name": "hal-03770516.grobid.tei.xml"}