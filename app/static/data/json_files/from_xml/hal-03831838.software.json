{"application": "software-mentions", "version": "0.8.0", "date": "2024-10-07T11:47+0000", "md5": "313519D95002E94DD969925F396A4985", "mentions": [{"type": "software", "software-type": "software", "software-name": {"rawForm": "WaveGlow", "normalizedForm": "WaveGlow", "offsetStart": 0, "offsetEnd": 8}, "context": "WaveGlow is used as a part of TacotronVocoder. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.0007317662239074707}, "created": {"value": false, "score": 6.449222564697266e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9998136162757874}, "created": {"value": false, "score": 6.449222564697266e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "textless-lib", "normalizedForm": "textless-lib", "offsetStart": 0, "offsetEnd": 12}, "context": "textless-lib provides a tool for preprocessing arbitrary sets of audio files into a stream of pseudo-unit tokens and, optionally, streams of per-frame tempo and F0 values, aligned to the token stream. ", "mentionContextAttributes": {"used": {"value": false, "score": 2.4199485778808594e-05}, "created": {"value": false, "score": 0.0009923577308654785}, "shared": {"value": false, "score": 4.76837158203125e-07}}, "documentContextAttributes": {"used": {"value": false, "score": 6.532669067382812e-05}, "created": {"value": true, "score": 0.9999213218688965}, "shared": {"value": false, "score": 5.960464477539062e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "SpeechEncoder", "normalizedForm": "SpeechEncoder", "offsetStart": 0, "offsetEnd": 13}, "context": "SpeechEncoder obtains a dense vector representation from a given self-supervised model, discretizes the dense representation into units, extracts pitch, aligns it with the unit streams, and potentially, applies run-length encoding with per-frame pitch averaging. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.0018107295036315918}, "created": {"value": false, "score": 1.9073486328125e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9893402457237244}, "created": {"value": false, "score": 9.202957153320312e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "textless", "normalizedForm": "textless", "offsetStart": 4, "offsetEnd": 12}, "context": "The textless approach has several advantages.", "mentionContextAttributes": {"used": {"value": false, "score": 4.00543212890625e-05}, "created": {"value": false, "score": 0.00041681528091430664}, "shared": {"value": false, "score": 4.76837158203125e-07}}, "documentContextAttributes": {"used": {"value": false, "score": 0.0424540638923645}, "created": {"value": true, "score": 0.9999426603317261}, "shared": {"value": false, "score": 1.3113021850585938e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "QuantizeDataset", "normalizedForm": "QuantizeDataset", "offsetStart": 4, "offsetEnd": 19}, "context": "The QuantizeDataset runs an instance of a dense representation model, which can be computationally heavy (e.g., the HuBERT-base model has 7 convolutional layers and 12 Transformer layers).", "mentionContextAttributes": {"used": {"value": false, "score": 0.0003262758255004883}, "created": {"value": false, "score": 1.8596649169921875e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": false, "score": 0.19986414909362793}, "created": {"value": false, "score": 0.0007723569869995117}, "shared": {"value": false, "score": 1.0728836059570312e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Tacotron2", "normalizedForm": "Tacotron2", "offsetStart": 5, "offsetEnd": 14}, "context": "Both Tacotron2 and WaveGlow were trained on LJ speech(Ito and Johnson, 2017).", "mentionContextAttributes": {"used": {"value": true, "score": 0.9998136162757874}, "created": {"value": false, "score": 8.463859558105469e-06}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9998136162757874}, "created": {"value": false, "score": 0.0009385943412780762}, "shared": {"value": false, "score": 1.0728836059570312e-06}}, "references": [{"label": "(Shen et al., 2018)", "normalizedForm": "Shen et al., 2018", "refKey": 48}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "SpeechEncoder", "normalizedForm": "SpeechEncoder", "offsetStart": 8, "offsetEnd": 21}, "context": "As with SpeechEncoder, we can retrieve a pretrained model by setting the expected input specification (model, quantizer, and the size of the codebook), see Figure 4 lines 17-21. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.0011452436447143555}, "created": {"value": false, "score": 9.202957153320312e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9893402457237244}, "created": {"value": false, "score": 9.202957153320312e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "textless", "normalizedForm": "textless", "offsetStart": 9, "offsetEnd": 17}, "context": "With the textless-lib we provide several pre-trained quantization functions for both HuBERT and CPC dense models using a vocabulary sizes K \u2208 {50, 100, 200, 500}.", "mentionContextAttributes": {"used": {"value": false, "score": 0.0035546422004699707}, "created": {"value": false, "score": 5.066394805908203e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": false, "score": 0.0424540638923645}, "created": {"value": true, "score": 0.9999426603317261}, "shared": {"value": false, "score": 1.3113021850585938e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "textless", "normalizedForm": "textless", "offsetStart": 10, "offsetEnd": 18}, "context": "Under the textless-lib we provide two pitchnormalization methods: per-speaker and prefixbased.", "mentionContextAttributes": {"used": {"value": false, "score": 4.851818084716797e-05}, "created": {"value": false, "score": 0.008061587810516357}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": false, "score": 0.0424540638923645}, "created": {"value": true, "score": 0.9999426603317261}, "shared": {"value": false, "score": 1.3113021850585938e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "textless", "normalizedForm": "textless", "offsetStart": 11, "offsetEnd": 19}, "context": "As part of textless-lib we provide several pre-trained models that proved to work best in prior work (Lakhotia et al., 2021;Polyak et al., 2021).", "mentionContextAttributes": {"used": {"value": false, "score": 9.071826934814453e-05}, "created": {"value": true, "score": 0.88486248254776}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": false, "score": 0.0424540638923645}, "created": {"value": true, "score": 0.9999426603317261}, "shared": {"value": false, "score": 1.3113021850585938e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "textless-lib", "normalizedForm": "textless-lib", "offsetStart": 11, "offsetEnd": 23}, "context": "We present textless-lib, a PyTorch library for textless spoken language processing. ", "mentionContextAttributes": {"used": {"value": false, "score": 6.532669067382812e-05}, "created": {"value": true, "score": 0.9999213218688965}, "shared": {"value": false, "score": 5.960464477539062e-07}}, "documentContextAttributes": {"used": {"value": false, "score": 6.532669067382812e-05}, "created": {"value": true, "score": 0.9999213218688965}, "shared": {"value": false, "score": 5.960464477539062e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "library", "normalizedForm": "library", "offsetStart": 13, "offsetEnd": 20}, "context": "The proposed library greatly simplifies research in the textless spoken language processing, hence we believe it will be a handful not only for speech researchers but to the entire NLP community.", "mentionContextAttributes": {"used": {"value": false, "score": 0.00017875432968139648}, "created": {"value": true, "score": 0.953739583492279}, "shared": {"value": false, "score": 1.3113021850585938e-06}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999403953552246}, "created": {"value": true, "score": 0.9999426603317261}, "shared": {"value": false, "score": 4.76837158203125e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "textless", "normalizedForm": "textless", "offsetStart": 14, "offsetEnd": 22}, "context": "We introduced textless-lib, a Pytorch library aimed to advance research in textless modeling of spoken language, by simplifying textless processing and synthesizing spoken language. ", "mentionContextAttributes": {"used": {"value": false, "score": 9.1552734375e-05}, "created": {"value": true, "score": 0.9999268054962158}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "documentContextAttributes": {"used": {"value": false, "score": 0.0424540638923645}, "created": {"value": true, "score": 0.9999426603317261}, "shared": {"value": false, "score": 1.3113021850585938e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "library", "normalizedForm": "library", "offsetStart": 15, "offsetEnd": 22}, "context": "We believe our library can provide a convenient tool for research in this area.", "mentionContextAttributes": {"used": {"value": false, "score": 0.00011229515075683594}, "created": {"value": true, "score": 0.9952526092529297}, "shared": {"value": false, "score": 4.76837158203125e-06}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999403953552246}, "created": {"value": true, "score": 0.9999426603317261}, "shared": {"value": false, "score": 4.76837158203125e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "lib", "normalizedForm": "lib", "offsetStart": 18, "offsetEnd": 21}, "context": "With the textless-lib we provide several pre-trained quantization functions for both HuBERT and CPC dense models using a vocabulary sizes K \u2208 {50, 100, 200, 500}.", "mentionContextAttributes": {"used": {"value": false, "score": 0.0035546422004699707}, "created": {"value": false, "score": 5.066394805908203e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": false, "score": 0.0424540638923645}, "created": {"value": true, "score": 0.9999426603317261}, "shared": {"value": false, "score": 9.5367431640625e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "lib", "normalizedForm": "lib", "offsetStart": 19, "offsetEnd": 22}, "context": "Under the textless-lib we provide two pitchnormalization methods: per-speaker and prefixbased.", "mentionContextAttributes": {"used": {"value": false, "score": 4.851818084716797e-05}, "created": {"value": false, "score": 0.008061587810516357}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": false, "score": 0.0424540638923645}, "created": {"value": true, "score": 0.9999426603317261}, "shared": {"value": false, "score": 9.5367431640625e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "WaveGlow", "normalizedForm": "WaveGlow", "offsetStart": 19, "offsetEnd": 27}, "context": "Both Tacotron2 and WaveGlow were trained on LJ speech(Ito and Johnson, 2017).", "mentionContextAttributes": {"used": {"value": true, "score": 0.9998136162757874}, "created": {"value": false, "score": 8.463859558105469e-06}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9998136162757874}, "created": {"value": false, "score": 6.449222564697266e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "lib", "normalizedForm": "lib", "offsetStart": 20, "offsetEnd": 23}, "context": "As part of textless-lib we provide several pre-trained models that proved to work best in prior work (Lakhotia et al., 2021;Polyak et al., 2021).", "mentionContextAttributes": {"used": {"value": false, "score": 9.071826934814453e-05}, "created": {"value": true, "score": 0.88486248254776}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": false, "score": 0.0424540638923645}, "created": {"value": true, "score": 0.9999426603317261}, "shared": {"value": false, "score": 9.5367431640625e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "textless", "normalizedForm": "textless", "offsetStart": 21, "offsetEnd": 29}, "context": "As a future work for textless-lib we envision improving performance of the existing building blocks, adding new example tasks (e.g., translation (Lee et al., 2021b) or dialog (Nguyen et al., 2022)), extending the set of provided pre-trained models, and introducing the possibility of training the different components.", "mentionContextAttributes": {"used": {"value": false, "score": 0.00021332502365112305}, "created": {"value": true, "score": 0.9855287671089172}, "shared": {"value": false, "score": 9.5367431640625e-07}}, "documentContextAttributes": {"used": {"value": false, "score": 0.0424540638923645}, "created": {"value": true, "score": 0.9999426603317261}, "shared": {"value": false, "score": 1.3113021850585938e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "textless", "normalizedForm": "textless", "offsetStart": 22, "offsetEnd": 30}, "context": "Finally, we include a textless-lib reimplementation of the full GSLM speech continuation pipeline (Lakhotia et al., 2021), as depicted in Figure 1.", "mentionContextAttributes": {"used": {"value": false, "score": 0.0424540638923645}, "created": {"value": false, "score": 0.023909330368041992}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "documentContextAttributes": {"used": {"value": false, "score": 0.0424540638923645}, "created": {"value": true, "score": 0.9999426603317261}, "shared": {"value": false, "score": 1.3113021850585938e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "lib", "normalizedForm": "lib", "offsetStart": 23, "offsetEnd": 26}, "context": "We introduced textless-lib, a Pytorch library aimed to advance research in textless modeling of spoken language, by simplifying textless processing and synthesizing spoken language. ", "mentionContextAttributes": {"used": {"value": false, "score": 9.1552734375e-05}, "created": {"value": true, "score": 0.9999268054962158}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "documentContextAttributes": {"used": {"value": false, "score": 0.0424540638923645}, "created": {"value": true, "score": 0.9999426603317261}, "shared": {"value": false, "score": 9.5367431640625e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "-lib", "normalizedForm": "-lib", "offsetStart": 24, "offsetEnd": 28}, "context": "We believe that textless-lib substantially simplifies research the textless setting and will be handful not only for speech researchers but also for the NLP community at large. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.0002289414405822754}, "created": {"value": false, "score": 0.1078917384147644}, "shared": {"value": false, "score": 1.1920928955078125e-06}}, "documentContextAttributes": {"used": {"value": false, "score": 0.000826418399810791}, "created": {"value": false, "score": 0.1078917384147644}, "shared": {"value": false, "score": 1.1920928955078125e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "SpeechEncoder", "normalizedForm": "SpeechEncoder", "offsetStart": 26, "offsetEnd": 39}, "context": "We denote the encoders as SpeechEncoder. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9893402457237244}, "created": {"value": false, "score": 2.5987625122070312e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9893402457237244}, "created": {"value": false, "score": 9.202957153320312e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "textless", "normalizedForm": "textless", "offsetStart": 28, "offsetEnd": 36}, "context": "In this paper, we introduce textless-lib, a PyTorch-based library aimed to facilitate research in this research area.", "mentionContextAttributes": {"used": {"value": false, "score": 5.137920379638672e-05}, "created": {"value": true, "score": 0.9999426603317261}, "shared": {"value": false, "score": 9.5367431640625e-07}}, "documentContextAttributes": {"used": {"value": false, "score": 0.0424540638923645}, "created": {"value": true, "score": 0.9999426603317261}, "shared": {"value": false, "score": 1.3113021850585938e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "lib", "normalizedForm": "lib", "offsetStart": 30, "offsetEnd": 33}, "context": "As a future work for textless-lib we envision improving performance of the existing building blocks, adding new example tasks (e.g., translation (Lee et al., 2021b) or dialog (Nguyen et al., 2022)), extending the set of provided pre-trained models, and introducing the possibility of training the different components.", "mentionContextAttributes": {"used": {"value": false, "score": 0.00021332502365112305}, "created": {"value": true, "score": 0.9855287671089172}, "shared": {"value": false, "score": 9.5367431640625e-07}}, "documentContextAttributes": {"used": {"value": false, "score": 0.0424540638923645}, "created": {"value": true, "score": 0.9999426603317261}, "shared": {"value": false, "score": 9.5367431640625e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "library", "normalizedForm": "library", "offsetStart": 30, "offsetEnd": 37}, "context": "In the initial release of the library, we provide Tacotron2 as a mel-spectrogram estimation module (i.e., the G function) followed by Wave-Glow (Prenger et al., 2019) neural vocoder (i.e., the V function) as used by Lakhotia et al. (2021).2", "mentionContextAttributes": {"used": {"value": false, "score": 0.0038231611251831055}, "created": {"value": false, "score": 0.0009385943412780762}, "shared": {"value": false, "score": 8.344650268554688e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999403953552246}, "created": {"value": true, "score": 0.9999426603317261}, "shared": {"value": false, "score": 4.76837158203125e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "lib", "normalizedForm": "lib", "offsetStart": 31, "offsetEnd": 34}, "context": "Finally, we include a textless-lib reimplementation of the full GSLM speech continuation pipeline (Lakhotia et al., 2021), as depicted in Figure 1.", "mentionContextAttributes": {"used": {"value": false, "score": 0.0424540638923645}, "created": {"value": false, "score": 0.023909330368041992}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "documentContextAttributes": {"used": {"value": false, "score": 0.0424540638923645}, "created": {"value": true, "score": 0.9999426603317261}, "shared": {"value": false, "score": 9.5367431640625e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "textless", "normalizedForm": "textless", "offsetStart": 32, "offsetEnd": 40}, "context": "In this section, we present the textless-lib library, intending to simplify future research on textless spoken language modeling.", "mentionContextAttributes": {"used": {"value": false, "score": 9.191036224365234e-05}, "created": {"value": true, "score": 0.9999290704727173}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": false, "score": 0.0424540638923645}, "created": {"value": true, "score": 0.9999426603317261}, "shared": {"value": false, "score": 1.3113021850585938e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "library", "normalizedForm": "library", "offsetStart": 35, "offsetEnd": 42}, "context": "We present textless-lib, a PyTorch library for textless spoken language processing. ", "mentionContextAttributes": {"used": {"value": false, "score": 6.532669067382812e-05}, "created": {"value": true, "score": 0.9999213218688965}, "shared": {"value": false, "score": 5.960464477539062e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999403953552246}, "created": {"value": true, "score": 0.9999426603317261}, "shared": {"value": false, "score": 4.76837158203125e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "library", "normalizedForm": "library", "offsetStart": 36, "offsetEnd": 43}, "context": "To demonstrate the usability of the library, we provided three usage examples related to (i) representation probing, (ii) speech compression, and (iii) speech continuation.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9999403953552246}, "created": {"value": false, "score": 2.3126602172851562e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999403953552246}, "created": {"value": true, "score": 0.9999426603317261}, "shared": {"value": false, "score": 4.76837158203125e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "lib", "normalizedForm": "lib", "offsetStart": 37, "offsetEnd": 40}, "context": "In this paper, we introduce textless-lib, a PyTorch-based library aimed to facilitate research in this research area. ", "mentionContextAttributes": {"used": {"value": false, "score": 5.137920379638672e-05}, "created": {"value": true, "score": 0.9999426603317261}, "shared": {"value": false, "score": 9.5367431640625e-07}}, "documentContextAttributes": {"used": {"value": false, "score": 0.0424540638923645}, "created": {"value": true, "score": 0.9999426603317261}, "shared": {"value": false, "score": 9.5367431640625e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "QuantizeDataset", "normalizedForm": "QuantizeDataset", "offsetStart": 37, "offsetEnd": 52}, "context": "Those datasets are implemented via a QuantizeDataset wrapper which can be used to wrap any map-style PyTorch dataset, containing raw waveform data. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.19986414909362793}, "created": {"value": false, "score": 0.0007723569869995117}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": false, "score": 0.19986414909362793}, "created": {"value": false, "score": 0.0007723569869995117}, "shared": {"value": false, "score": 1.0728836059570312e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "library", "normalizedForm": "library", "offsetStart": 38, "offsetEnd": 45}, "context": "We introduced textless-lib, a Pytorch library aimed to advance research in textless modeling of spoken language, by simplifying textless processing and synthesizing spoken language.", "mentionContextAttributes": {"used": {"value": false, "score": 9.1552734375e-05}, "created": {"value": true, "score": 0.9999268054962158}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999403953552246}, "created": {"value": true, "score": 0.9999426603317261}, "shared": {"value": false, "score": 4.76837158203125e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "library", "normalizedForm": "library", "offsetStart": 40, "offsetEnd": 47}, "context": "Alongside the core functionality of the library, we provide a set of illustrative examples.", "mentionContextAttributes": {"used": {"value": false, "score": 0.0001266002655029297}, "created": {"value": true, "score": 0.9921054840087891}, "shared": {"value": false, "score": 5.960464477539062e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999403953552246}, "created": {"value": true, "score": 0.9999426603317261}, "shared": {"value": false, "score": 4.76837158203125e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "lib", "normalizedForm": "lib", "offsetStart": 41, "offsetEnd": 44}, "context": "In this section, we present the textless-lib library, intending to simplify future research on textless spoken language modeling.", "mentionContextAttributes": {"used": {"value": false, "score": 9.191036224365234e-05}, "created": {"value": true, "score": 0.9999290704727173}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": false, "score": 0.0424540638923645}, "created": {"value": true, "score": 0.9999426603317261}, "shared": {"value": false, "score": 9.5367431640625e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "library", "normalizedForm": "library", "offsetStart": 41, "offsetEnd": 48}, "context": "We describe the building blocks that the library provides and demonstrate its usability by discuss three different use-case examples: (i) speaker probing, (ii) speech resynthesis and compression, and (iii) speech continuation.", "mentionContextAttributes": {"used": {"value": false, "score": 0.06552773714065552}, "created": {"value": false, "score": 0.009426653385162354}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999403953552246}, "created": {"value": true, "score": 0.9999426603317261}, "shared": {"value": false, "score": 4.76837158203125e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "textless-lib", "normalizedForm": "textless-lib", "offsetStart": 41, "offsetEnd": 53}, "context": "Apart from encoders and vocoders, in the textless-lib we provide several components aimed to simplify frequent data loading use-cases. ", "mentionContextAttributes": {"used": {"value": false, "score": 4.649162292480469e-05}, "created": {"value": false, "score": 0.020770907402038574}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": false, "score": 6.532669067382812e-05}, "created": {"value": true, "score": 0.9999213218688965}, "shared": {"value": false, "score": 5.960464477539062e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "textless", "normalizedForm": "textless", "offsetStart": 43, "offsetEnd": 51}, "context": "Below we provide an overview of the common textless spoken language modeling pipeline.", "mentionContextAttributes": {"used": {"value": false, "score": 0.00014030933380126953}, "created": {"value": true, "score": 0.9908818006515503}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "documentContextAttributes": {"used": {"value": false, "score": 0.0424540638923645}, "created": {"value": true, "score": 0.9999426603317261}, "shared": {"value": false, "score": 1.3113021850585938e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "library", "normalizedForm": "library", "offsetStart": 45, "offsetEnd": 52}, "context": "In this section, we present the textless-lib library, intending to simplify future research on textless spoken language modeling.", "mentionContextAttributes": {"used": {"value": false, "score": 9.191036224365234e-05}, "created": {"value": true, "score": 0.9999290704727173}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999403953552246}, "created": {"value": true, "score": 0.9999426603317261}, "shared": {"value": false, "score": 4.76837158203125e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "textless", "normalizedForm": "textless", "offsetStart": 47, "offsetEnd": 55}, "context": "We present textless-lib, a PyTorch library for textless spoken language processing.", "mentionContextAttributes": {"used": {"value": false, "score": 6.532669067382812e-05}, "created": {"value": true, "score": 0.9999213218688965}, "shared": {"value": false, "score": 5.960464477539062e-07}}, "documentContextAttributes": {"used": {"value": false, "score": 0.0424540638923645}, "created": {"value": true, "score": 0.9999426603317261}, "shared": {"value": false, "score": 1.3113021850585938e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Tacotron2", "normalizedForm": "Tacotron2", "offsetStart": 48, "offsetEnd": 57}, "context": "For instance, Lakhotia et al. ( 2021) trained a Tacotron2 model (Shen et al., 2018) to perform units to mel-spectrogram conversion followed by Formally, to reconstruct a time-domain speech signal from a sequence of discrete units, z q we define the composition as, V (G(z q )) = x, where G is a mel-spectrogram estimation module (e.g., Tacotron2), and V is a phase vocoder module responsible for time-domain synthesis (e.g., Wave-Glow).", "mentionContextAttributes": {"used": {"value": true, "score": 0.9995918869972229}, "created": {"value": false, "score": 7.748603820800781e-06}, "shared": {"value": false, "score": 1.0728836059570312e-06}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9998136162757874}, "created": {"value": false, "score": 0.0009385943412780762}, "shared": {"value": false, "score": 1.0728836059570312e-06}}, "references": [{"label": "(Shen et al., 2018)", "normalizedForm": "Shen et al., 2018", "refKey": 48, "offsetStart": 7079, "offsetEnd": 7098}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Tacotron2", "normalizedForm": "Tacotron2", "offsetStart": 50, "offsetEnd": 59}, "context": "In the initial release of the library, we provide Tacotron2 as a mel-spectrogram estimation module (i.e., the G function) followed by Wave-Glow (Prenger et al., 2019) neural vocoder (i.e., the V function) as used by Lakhotia et al. (2021).2", "mentionContextAttributes": {"used": {"value": false, "score": 0.0038231611251831055}, "created": {"value": false, "score": 0.0009385943412780762}, "shared": {"value": false, "score": 8.344650268554688e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9998136162757874}, "created": {"value": false, "score": 0.0009385943412780762}, "shared": {"value": false, "score": 1.0728836059570312e-06}}, "references": [{"label": "(Shen et al., 2018)", "normalizedForm": "Shen et al., 2018", "refKey": 48}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "textless", "normalizedForm": "textless", "offsetStart": 56, "offsetEnd": 64}, "context": "The proposed library greatly simplifies research in the textless spoken language processing, hence we believe it will be a handful not only for speech researchers but to the entire NLP community.", "mentionContextAttributes": {"used": {"value": false, "score": 0.00017875432968139648}, "created": {"value": true, "score": 0.9537399411201477}, "shared": {"value": false, "score": 1.3113021850585938e-06}}, "documentContextAttributes": {"used": {"value": false, "score": 0.0424540638923645}, "created": {"value": true, "score": 0.9999426603317261}, "shared": {"value": false, "score": 1.3113021850585938e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "textless", "normalizedForm": "textless", "offsetStart": 57, "offsetEnd": 65}, "context": "Fig. 4 illustrates how simple its implementation is with textless-lib.", "mentionContextAttributes": {"used": {"value": false, "score": 0.0003120899200439453}, "created": {"value": false, "score": 6.842613220214844e-05}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "documentContextAttributes": {"used": {"value": false, "score": 0.0424540638923645}, "created": {"value": true, "score": 0.9999426603317261}, "shared": {"value": false, "score": 1.3113021850585938e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "library", "normalizedForm": "library", "offsetStart": 58, "offsetEnd": 65}, "context": "In this paper, we introduce textless-lib, a PyTorch-based library aimed to facilitate research in this research area.", "mentionContextAttributes": {"used": {"value": false, "score": 5.137920379638672e-05}, "created": {"value": true, "score": 0.9999426603317261}, "shared": {"value": false, "score": 9.5367431640625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999403953552246}, "created": {"value": true, "score": 0.9999426603317261}, "shared": {"value": false, "score": 4.76837158203125e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "lib", "normalizedForm": "lib", "offsetStart": 66, "offsetEnd": 69}, "context": "Fig. 4 illustrates how simple its implementation is with textless-lib.", "mentionContextAttributes": {"used": {"value": false, "score": 0.0003120899200439453}, "created": {"value": false, "score": 6.842613220214844e-05}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "documentContextAttributes": {"used": {"value": false, "score": 0.0424540638923645}, "created": {"value": true, "score": 0.9999426603317261}, "shared": {"value": false, "score": 9.5367431640625e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "textless", "normalizedForm": "textless", "offsetStart": 67, "offsetEnd": 75}, "context": "We believe that textless-lib substantially simplifies research the textless setting and will be handful not only for speech researchers but also for the NLP community at large.", "mentionContextAttributes": {"used": {"value": false, "score": 0.0002289414405822754}, "created": {"value": false, "score": 0.10789191722869873}, "shared": {"value": false, "score": 1.1920928955078125e-06}}, "documentContextAttributes": {"used": {"value": false, "score": 0.0424540638923645}, "created": {"value": true, "score": 0.9999426603317261}, "shared": {"value": false, "score": 1.3113021850585938e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "-lib", "normalizedForm": "-lib", "offsetStart": 69, "offsetEnd": 73}, "context": "Hence, we provide two possible solutions: (i) as part of the textless-lib we provide a way to spread QuantizeDataset and DataLoader preprocessing workers (each with its copy of a dense model) across multiple GPUs, hence potentially balancing training and preprocessing across different devices; (ii) in cases where on-the-fly preprocessing is not required (e.g., there is no randomized data augmentation (Kharitonov et al., 2021b)), an alternative is to preprocess the entire dataset in advance. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.000826418399810791}, "created": {"value": false, "score": 0.00012028217315673828}, "shared": {"value": false, "score": 1.0728836059570312e-06}}, "documentContextAttributes": {"used": {"value": false, "score": 0.000826418399810791}, "created": {"value": false, "score": 0.1078917384147644}, "shared": {"value": false, "score": 1.1920928955078125e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "textless", "normalizedForm": "textless", "offsetStart": 75, "offsetEnd": 83}, "context": "We introduced textless-lib, a Pytorch library aimed to advance research in textless modeling of spoken language, by simplifying textless processing and synthesizing spoken language.", "mentionContextAttributes": {"used": {"value": false, "score": 9.1552734375e-05}, "created": {"value": true, "score": 0.9999268054962158}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "documentContextAttributes": {"used": {"value": false, "score": 0.0424540638923645}, "created": {"value": true, "score": 0.9999426603317261}, "shared": {"value": false, "score": 1.3113021850585938e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "library", "normalizedForm": "library", "offsetStart": 91, "offsetEnd": 98}, "context": "The pipeline presented in Figure 1 hints a straightforward way to decouple elements of the library into two principal blocks: (i) encoding speech; and (ii) decoding speech, with the only interdependence being the format of the data in-between (e.g., vocabulary size).", "mentionContextAttributes": {"used": {"value": false, "score": 0.0002893805503845215}, "created": {"value": false, "score": 0.00016939640045166016}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999403953552246}, "created": {"value": true, "score": 0.9999426603317261}, "shared": {"value": false, "score": 4.76837158203125e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "textless", "normalizedForm": "textless", "offsetStart": 95, "offsetEnd": 103}, "context": "In this section, we present the textless-lib library, intending to simplify future research on textless spoken language modeling.", "mentionContextAttributes": {"used": {"value": false, "score": 9.191036224365234e-05}, "created": {"value": true, "score": 0.9999290704727173}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": false, "score": 0.0424540638923645}, "created": {"value": true, "score": 0.9999426603317261}, "shared": {"value": false, "score": 1.3113021850585938e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "library", "normalizedForm": "library", "offsetStart": 100, "offsetEnd": 107}, "context": "The goal of these examples is two-fold: (a) to illustrate the usage of particular components of the library, and (b) to serve as a starter code for a particular type of application.", "mentionContextAttributes": {"used": {"value": false, "score": 0.0005823373794555664}, "created": {"value": false, "score": 0.0016475915908813477}, "shared": {"value": false, "score": 4.76837158203125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999403953552246}, "created": {"value": true, "score": 0.9999426603317261}, "shared": {"value": false, "score": 4.76837158203125e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "QuantizeDataset", "normalizedForm": "QuantizeDataset", "offsetStart": 101, "offsetEnd": 116}, "context": "Hence, we provide two possible solutions: (i) as part of the textless-lib we provide a way to spread QuantizeDataset and DataLoader preprocessing workers (each with its copy of a dense model) across multiple GPUs, hence potentially balancing training and preprocessing across different devices; (ii) in cases where on-the-fly preprocessing is not required (e.g., there is no randomized data augmentation (Kharitonov et al., 2021b)), an alternative is to preprocess the entire dataset in advance.", "mentionContextAttributes": {"used": {"value": false, "score": 0.000826418399810791}, "created": {"value": false, "score": 0.00012028217315673828}, "shared": {"value": false, "score": 1.0728836059570312e-06}}, "documentContextAttributes": {"used": {"value": false, "score": 0.19986414909362793}, "created": {"value": false, "score": 0.0007723569869995117}, "shared": {"value": false, "score": 1.0728836059570312e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "library", "normalizedForm": "library", "offsetStart": 103, "offsetEnd": 110}, "context": "We decided to exclude both U2U models as well as evaluation metrics from the core functionality of the library as we believe these models should be an example usage.", "mentionContextAttributes": {"used": {"value": false, "score": 0.11879509687423706}, "created": {"value": false, "score": 0.0031995773315429688}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999403953552246}, "created": {"value": true, "score": 0.9999426603317261}, "shared": {"value": false, "score": 4.76837158203125e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "textless", "normalizedForm": "textless", "offsetStart": 128, "offsetEnd": 136}, "context": "We introduced textless-lib, a Pytorch library aimed to advance research in textless modeling of spoken language, by simplifying textless processing and synthesizing spoken language.", "mentionContextAttributes": {"used": {"value": false, "score": 9.1552734375e-05}, "created": {"value": true, "score": 0.9999268054962158}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "documentContextAttributes": {"used": {"value": false, "score": 0.0424540638923645}, "created": {"value": true, "score": 0.9999426603317261}, "shared": {"value": false, "score": 1.3113021850585938e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "textless", "normalizedForm": "textless", "offsetStart": 172, "offsetEnd": 180}, "context": "Lakhotia et al. (2021) presented a Generative Spoken Language Modeling (GSLM) pipeline trained from raw audio, consisting in a speech en-Figure 1: A visual description for textless modeling of spoken language.", "mentionContextAttributes": {"used": {"value": false, "score": 9.274482727050781e-05}, "created": {"value": false, "score": 0.13796722888946533}, "shared": {"value": false, "score": 4.76837158203125e-07}}, "documentContextAttributes": {"used": {"value": false, "score": 0.0424540638923645}, "created": {"value": true, "score": 0.9999426603317261}, "shared": {"value": false, "score": 1.3113021850585938e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Tacotron2", "normalizedForm": "Tacotron2", "offsetStart": 336, "offsetEnd": 345}, "context": "For instance, Lakhotia et al. ( 2021) trained a Tacotron2 model (Shen et al., 2018) to perform units to mel-spectrogram conversion followed by Formally, to reconstruct a time-domain speech signal from a sequence of discrete units, z q we define the composition as, V (G(z q )) = x, where G is a mel-spectrogram estimation module (e.g., Tacotron2), and V is a phase vocoder module responsible for time-domain synthesis (e.g., Wave-Glow).", "mentionContextAttributes": {"used": {"value": true, "score": 0.9995918869972229}, "created": {"value": false, "score": 7.748603820800781e-06}, "shared": {"value": false, "score": 1.0728836059570312e-06}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9998136162757874}, "created": {"value": false, "score": 0.0009385943412780762}, "shared": {"value": false, "score": 1.0728836059570312e-06}}, "references": [{"label": "(Shen et al., 2018)", "normalizedForm": "Shen et al., 2018", "refKey": 48}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "library", "normalizedForm": "library", "offsetStart": 346, "offsetEnd": 353}, "context": "There are plenty of ways to evaluate the overall pipeline (Lakhotia et al., 2021;Dunbar et al., 2019Dunbar et al., , 2020;;Nguyen et al., 2020) and different ways to model the \"pseudo-text\" units (Shi et al., 2021;Kharitonov et al., 2021a;Polyak et al., 2021;Kreuk et al., 2021;Lee et al., 2021a), hence including them as an integral part of the library will make it overcomplicated.", "mentionContextAttributes": {"used": {"value": true, "score": 0.8786696195602417}, "created": {"value": false, "score": 1.2278556823730469e-05}, "shared": {"value": false, "score": 2.384185791015625e-06}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999403953552246}, "created": {"value": true, "score": 0.9999426603317261}, "shared": {"value": false, "score": 4.76837158203125e-06}}}], "references": [{"refKey": 48, "tei": "<biblStruct xml:id=\"b48\">\n\t<analytic>\n\t\t<title level=\"a\" type=\"main\">Natural TTS Synthesis by Conditioning Wavenet on MEL Spectrogram Predictions</title>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Jonathan</forename><surname>Shen</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Ruoming</forename><surname>Pang</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Ron</forename><forename type=\"middle\">J</forename><surname>Weiss</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Mike</forename><surname>Schuster</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Navdeep</forename><surname>Jaitly</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Zongheng</forename><surname>Yang</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Zhifeng</forename><surname>Chen</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Yu</forename><surname>Zhang</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Yuxuan</forename><surname>Wang</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Rj</forename><surname>Skerrv-Ryan</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Rif</forename><forename type=\"middle\">A</forename><surname>Saurous</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Yannis</forename><surname>Agiomvrgiannakis</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Yonghui</forename><surname>Wu</surname></persName>\n\t\t</author>\n\t\t<idno type=\"DOI\">10.1109/icassp.2018.8461368</idno>\n\t</analytic>\n\t<monogr>\n\t\t<title level=\"m\">2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>\n\t\t<imprint>\n\t\t\t<publisher>IEEE</publisher>\n\t\t\t<date type=\"published\" when=\"2018-04\">2018</date>\n\t\t\t<biblScope unit=\"page\">4783</biblScope>\n\t\t</imprint>\n\t</monogr>\n</biblStruct>\n"}], "runtime": 17735, "id": "b7565bdacb360d432582eabc7cf4afa9ea486ed0", "metadata": {"id": "b7565bdacb360d432582eabc7cf4afa9ea486ed0"}, "original_file_path": "../../datalake/Samuel/SOFTware-Sync/downloads/xml/hal-03831838.grobid.tei.xml", "file_name": "hal-03831838.grobid.tei.xml"}