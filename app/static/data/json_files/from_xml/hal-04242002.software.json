{"application": "software-mentions", "version": "0.8.0", "date": "2024-10-07T12:01+0000", "md5": "ACC12A16739174BC8A1CCC8215FD04D4", "mentions": [{"type": "software", "software-type": "software", "software-name": {"rawForm": "Code", "normalizedForm": "Code", "offsetStart": 0, "offsetEnd": 4}, "context": "Code may be found at https://github.com/chengemily/EGG/tree/imitation.", "mentionContextAttributes": {"used": {"value": false, "score": 0.00014698505401611328}, "created": {"value": false, "score": 2.562999725341797e-05}, "shared": {"value": true, "score": 0.9665614366531372}}, "documentContextAttributes": {"used": {"value": false, "score": 0.00014698505401611328}, "created": {"value": false, "score": 2.562999725341797e-05}, "shared": {"value": true, "score": 0.9665614366531372}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Topsim", "normalizedForm": "Topsim", "offsetStart": 0, "offsetEnd": 6}, "context": "Topsim is defined as the Spearman correlation \u03c1 between Euclidean distances in the input space and Levenstein distances in the message space-that is, it captures the intuition that nearby inputs should be described with similar messages. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.000498652458190918}, "created": {"value": false, "score": 4.971027374267578e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": false, "score": 0.000498652458190918}, "created": {"value": false, "score": 4.971027374267578e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Expert", "normalizedForm": "Expert", "offsetStart": 0, "offsetEnd": 6}, "context": "Expert training produces the following data: 1) inputs x; 2) messages m corresponding to Expert Senders' encodings of x; and 3) outputs x, the Expert Receivers' reconstructions of x given m.", "mentionContextAttributes": {"used": {"value": false, "score": 0.013628661632537842}, "created": {"value": false, "score": 3.6954879760742188e-06}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999884366989136}, "created": {"value": false, "score": 0.014738857746124268}, "shared": {"value": false, "score": 5.960464477539062e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Expert", "normalizedForm": "Expert", "offsetStart": 0, "offsetEnd": 6}, "context": "Expert pairs converge to high validation accuracy and generalize to the indistribution set well (statistics in table A.2).", "mentionContextAttributes": {"used": {"value": false, "score": 0.0022243857383728027}, "created": {"value": false, "score": 2.9802322387695312e-06}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999884366989136}, "created": {"value": false, "score": 0.014738857746124268}, "shared": {"value": false, "score": 5.960464477539062e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Expert", "normalizedForm": "Expert", "offsetStart": 4, "offsetEnd": 10}, "context": "per Expert, which is averaged over Experts to produce the mixture-policy loss.", "mentionContextAttributes": {"used": {"value": true, "score": 0.970097541809082}, "created": {"value": false, "score": 3.6954879760742188e-06}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999884366989136}, "created": {"value": false, "score": 0.014738857746124268}, "shared": {"value": false, "score": 5.960464477539062e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Expert", "normalizedForm": "Expert", "offsetStart": 6, "offsetEnd": 12}, "context": "5 For Expert i, this corresponds to a reward r (i) of", "mentionContextAttributes": {"used": {"value": false, "score": 0.3013828992843628}, "created": {"value": false, "score": 7.987022399902344e-06}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999884366989136}, "created": {"value": false, "score": 0.014738857746124268}, "shared": {"value": false, "score": 5.960464477539062e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Expert", "normalizedForm": "Expert", "offsetStart": 10, "offsetEnd": 16}, "context": "Using all Expert topsim distributions generated so far (those where topsim ranks are evenly spaced, and those exhibiting varying skews), we investigate the effect of topsim distribution spread, quantified by standard deviation, on the learned weights.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9993436932563782}, "created": {"value": false, "score": 3.7550926208496094e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999884366989136}, "created": {"value": false, "score": 0.014738857746124268}, "shared": {"value": false, "score": 5.960464477539062e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "LayerNorm", "normalizedForm": "LayerNorm", "offsetStart": 11, "offsetEnd": 38}, "context": "We include LayerNorm (Ba et al., 2016) after the hidden state to improve training stability. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.882443904876709}, "created": {"value": true, "score": 0.9910300970077515}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.882443904876709}, "created": {"value": true, "score": 0.9910300970077515}, "shared": {"value": false, "score": 3.5762786865234375e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Expert", "normalizedForm": "Expert", "offsetStart": 12, "offsetEnd": 18}, "context": "We train 30 Expert pairs on the reconstruction task over 1000 epochs.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9748859405517578}, "created": {"value": false, "score": 0.0032817721366882324}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999884366989136}, "created": {"value": false, "score": 0.014738857746124268}, "shared": {"value": false, "score": 5.960464477539062e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Expert", "normalizedForm": "Expert", "offsetStart": 16, "offsetEnd": 22}, "context": "Recall that the Expert Senders are deterministic at evaluation time.", "mentionContextAttributes": {"used": {"value": false, "score": 0.020048856735229492}, "created": {"value": false, "score": 8.225440979003906e-06}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999884366989136}, "created": {"value": false, "score": 0.014738857746124268}, "shared": {"value": false, "score": 5.960464477539062e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Expert", "normalizedForm": "Expert", "offsetStart": 17, "offsetEnd": 23}, "context": "The shape of the Expert weight distribution is tempered by the entropy regularization coefficient \u03bb: smaller \u03bb results in greater compositional selec-tion (that is, lower entropy and more negative skew) of the weight distribution (fig.", "mentionContextAttributes": {"used": {"value": false, "score": 0.1068926453590393}, "created": {"value": false, "score": 7.140636444091797e-05}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999884366989136}, "created": {"value": false, "score": 0.014738857746124268}, "shared": {"value": false, "score": 5.960464477539062e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "topsim", "normalizedForm": "topsim", "offsetStart": 17, "offsetEnd": 23}, "context": "Using all Expert topsim distributions generated so far (those where topsim ranks are evenly spaced, and those exhibiting varying skews), we investigate the effect of topsim distribution spread, quantified by standard deviation, on the learned weights.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9993436932563782}, "created": {"value": false, "score": 3.7550926208496094e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999884366989136}, "created": {"value": true, "score": 0.9998252987861633}, "shared": {"value": false, "score": 8.344650268554688e-07}}, "references": [{"label": "(Kirby and Brighton, 2006)", "normalizedForm": "Kirby and Brighton, 2006", "refKey": 12}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "topsim", "normalizedForm": "topsim", "offsetStart": 20, "offsetEnd": 26}, "context": "We considered using topsim, positional disentanglement (posdis) (Chaabouni et al., 2020), bagof-symbols disentanglement (bosdis) (Chaabouni et al., 2020), and context independence (ci) (Bogin et al., 2018) for our experiments (see fig.", "mentionContextAttributes": {"used": {"value": true, "score": 0.995937705039978}, "created": {"value": false, "score": 3.7789344787597656e-05}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999884366989136}, "created": {"value": true, "score": 0.9998252987861633}, "shared": {"value": false, "score": 8.344650268554688e-07}}, "references": [{"label": "(Kirby and Brighton, 2006)", "normalizedForm": "Kirby and Brighton, 2006", "refKey": 12}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Expert", "normalizedForm": "Expert", "offsetStart": 22, "offsetEnd": 28}, "context": "First, we generate 30 Expert languages from the referential task, initially considering Expert distributions corresponding to evenly-spaced percentiles of topsim, including the minimum and maximum (0.26, 0.43).", "mentionContextAttributes": {"used": {"value": true, "score": 0.9866998195648193}, "created": {"value": false, "score": 0.0011522769927978516}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999884366989136}, "created": {"value": false, "score": 0.014738857746124268}, "shared": {"value": false, "score": 5.960464477539062e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Expert", "normalizedForm": "Expert", "offsetStart": 27, "offsetEnd": 33}, "context": "We then test the effect of Expert topsim distribution asymmetry on the learned weights.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9998103976249695}, "created": {"value": false, "score": 0.0004299283027648926}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999884366989136}, "created": {"value": false, "score": 0.014738857746124268}, "shared": {"value": false, "score": 5.960464477539062e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Expert", "normalizedForm": "Expert", "offsetStart": 28, "offsetEnd": 34}, "context": "The distribution of learned Expert weights in fig.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9869639277458191}, "created": {"value": false, "score": 9.5367431640625e-06}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999884366989136}, "created": {"value": false, "score": 0.014738857746124268}, "shared": {"value": false, "score": 5.960464477539062e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "topsim", "normalizedForm": "topsim", "offsetStart": 29, "offsetEnd": 35}, "context": "Otherwise, it is the maximum topsim.", "mentionContextAttributes": {"used": {"value": false, "score": 0.02414470911026001}, "created": {"value": false, "score": 0.00011146068572998047}, "shared": {"value": false, "score": 8.344650268554688e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999884366989136}, "created": {"value": true, "score": 0.9998252987861633}, "shared": {"value": false, "score": 8.344650268554688e-07}}, "references": [{"label": "(Kirby and Brighton, 2006)", "normalizedForm": "Kirby and Brighton, 2006", "refKey": 12}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Expert", "normalizedForm": "Expert", "offsetStart": 30, "offsetEnd": 36}, "context": "These data splits are used in Expert training and are 12433 inherited by the imitation task (see appendix B.2 for details on generating the data split).", "mentionContextAttributes": {"used": {"value": false, "score": 0.014550447463989258}, "created": {"value": false, "score": 1.2755393981933594e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999884366989136}, "created": {"value": false, "score": 0.014738857746124268}, "shared": {"value": false, "score": 5.960464477539062e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "topsim", "normalizedForm": "topsim", "offsetStart": 30, "offsetEnd": 36}, "context": "This is likely because higher topsim languages are easier to imitate.", "mentionContextAttributes": {"used": {"value": false, "score": 0.0014069676399230957}, "created": {"value": false, "score": 9.822845458984375e-05}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999884366989136}, "created": {"value": true, "score": 0.9998252987861633}, "shared": {"value": false, "score": 8.344650268554688e-07}}, "references": [{"label": "(Kirby and Brighton, 2006)", "normalizedForm": "Kirby and Brighton, 2006", "refKey": 12}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Expert", "normalizedForm": "Expert", "offsetStart": 31, "offsetEnd": 37}, "context": "That is, the more disperse the Expert topsims, the more the Imitator can differentiate between and select compositional Experts (shown by a more negative skew in learned Expert weights).", "mentionContextAttributes": {"used": {"value": false, "score": 0.004536449909210205}, "created": {"value": false, "score": 2.2530555725097656e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999884366989136}, "created": {"value": false, "score": 0.014738857746124268}, "shared": {"value": false, "score": 5.960464477539062e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "topsim", "normalizedForm": "topsim", "offsetStart": 31, "offsetEnd": 37}, "context": "Moreover, correlations between topsim and easeof-imitation are stronger than those between Expert validation accuracy and ease-of-imitation (table C.1).", "mentionContextAttributes": {"used": {"value": false, "score": 0.2331947684288025}, "created": {"value": false, "score": 5.245208740234375e-06}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999884366989136}, "created": {"value": true, "score": 0.9998252987861633}, "shared": {"value": false, "score": 8.344650268554688e-07}}, "references": [{"label": "(Kirby and Brighton, 2006)", "normalizedForm": "Kirby and Brighton, 2006", "refKey": 12}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Expert", "normalizedForm": "Expert", "offsetStart": 33, "offsetEnd": 39}, "context": "Proof Let \u03c0 (1) \u2022 \u2022 \u2022 \u03c0 (k) be k Expert Senders and let \u03c0 I be the Imitator Sender.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9993165731430054}, "created": {"value": false, "score": 4.76837158203125e-06}, "shared": {"value": false, "score": 4.76837158203125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999884366989136}, "created": {"value": false, "score": 0.014738857746124268}, "shared": {"value": false, "score": 5.960464477539062e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "EGG", "normalizedForm": "EGG", "offsetStart": 34, "offsetEnd": 37}, "context": "Experiments are implemented using EGG(Kharitonov et al., 2021).", "mentionContextAttributes": {"used": {"value": false, "score": 0.00687021017074585}, "created": {"value": false, "score": 0.05883580446243286}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999420642852783}, "created": {"value": false, "score": 0.05883580446243286}, "shared": {"value": true, "score": 0.9665614366531372}}, "references": [{"label": "(Kharitonov et al., 2021)", "normalizedForm": "Kharitonov et al., 2021", "refKey": 11, "offsetStart": 30165, "offsetEnd": 30190}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "topsim", "normalizedForm": "topsim", "offsetStart": 34, "offsetEnd": 40}, "context": "We then test the effect of Expert topsim distribution asymmetry on the learned weights.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9998103976249695}, "created": {"value": false, "score": 0.0004299283027648926}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999884366989136}, "created": {"value": true, "score": 0.9998252987861633}, "shared": {"value": false, "score": 8.344650268554688e-07}}, "references": [{"label": "(Kirby and Brighton, 2006)", "normalizedForm": "Kirby and Brighton, 2006", "refKey": 12}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Expert", "normalizedForm": "Expert", "offsetStart": 35, "offsetEnd": 41}, "context": "Correlations between topsims of 30 Expert languages and Imitator performance (averaged over three random seeds) are shown in table 1.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9997078776359558}, "created": {"value": false, "score": 7.152557373046875e-06}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999884366989136}, "created": {"value": false, "score": 0.014738857746124268}, "shared": {"value": false, "score": 5.960464477539062e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "PyTorch", "normalizedForm": "PyTorch", "offsetStart": 35, "offsetEnd": 42}, "context": "Experiments were implemented using PyTorch and the EGG toolkit (Kharitonov et al., 2021). ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9999420642852783}, "created": {"value": false, "score": 0.005986034870147705}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999420642852783}, "created": {"value": false, "score": 0.005986034870147705}, "shared": {"value": false, "score": 2.384185791015625e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Expert", "normalizedForm": "Expert", "offsetStart": 37, "offsetEnd": 43}, "context": "The Imitator learns a mixture of the Expert languages with weights W := (w i ) 1\u2264i\u2264k (normalized).", "mentionContextAttributes": {"used": {"value": true, "score": 0.6610971689224243}, "created": {"value": false, "score": 2.384185791015625e-06}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999884366989136}, "created": {"value": false, "score": 0.014738857746124268}, "shared": {"value": false, "score": 5.960464477539062e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "topsim", "normalizedForm": "topsim", "offsetStart": 37, "offsetEnd": 43}, "context": "Furthermore, the correlation between topsim and validation accuracy is only \u03c1 = 0.57 (\u03b1 = 1e-2) suggesting that the relationship between generalization and compositionality is not explained by high validation accuracy.", "mentionContextAttributes": {"used": {"value": true, "score": 0.6366278529167175}, "created": {"value": false, "score": 6.318092346191406e-06}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999884366989136}, "created": {"value": true, "score": 0.9998252987861633}, "shared": {"value": false, "score": 8.344650268554688e-07}}, "references": [{"label": "(Kirby and Brighton, 2006)", "normalizedForm": "Kirby and Brighton, 2006", "refKey": 12}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Expert", "normalizedForm": "Expert", "offsetStart": 37, "offsetEnd": 43}, "context": "We do not report the Pearson R 2 for Expert validation accuracy as the its distribution violates normality assumptions according to a Shapiro-Wilk non-normality test (\u03b1 = 1e-3)).", "mentionContextAttributes": {"used": {"value": true, "score": 0.9990480542182922}, "created": {"value": false, "score": 1.3947486877441406e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999884366989136}, "created": {"value": false, "score": 0.014738857746124268}, "shared": {"value": false, "score": 5.960464477539062e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Expert", "normalizedForm": "Expert", "offsetStart": 41, "offsetEnd": 47}, "context": "To do so, for each k > 2, we generate 10 Expert topsim distributions with varying skew, following the procedure outlined in appendix D.2 (when k = 2, skew is mechanically 0).", "mentionContextAttributes": {"used": {"value": true, "score": 0.8748096823692322}, "created": {"value": false, "score": 1.0609626770019531e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999884366989136}, "created": {"value": false, "score": 0.014738857746124268}, "shared": {"value": false, "score": 5.960464477539062e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "topsim", "normalizedForm": "topsim", "offsetStart": 44, "offsetEnd": 50}, "context": "In both cases, REINFORCE will select a high-topsim Expert, and supervision will weight all Experts equally, that is, supervision is unable to de-select poor topsims.", "mentionContextAttributes": {"used": {"value": false, "score": 0.00984489917755127}, "created": {"value": false, "score": 8.344650268554688e-06}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999884366989136}, "created": {"value": true, "score": 0.9998252987861633}, "shared": {"value": false, "score": 8.344650268554688e-07}}, "references": [{"label": "(Kirby and Brighton, 2006)", "normalizedForm": "Kirby and Brighton, 2006", "refKey": 12}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Expert", "normalizedForm": "Expert", "offsetStart": 44, "offsetEnd": 50}, "context": "4, we note a significant negative effect of Expert topsim standard deviation on the degree of compositional selection.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9992650151252747}, "created": {"value": false, "score": 3.504753112792969e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999884366989136}, "created": {"value": false, "score": 0.014738857746124268}, "shared": {"value": false, "score": 5.960464477539062e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Expert", "normalizedForm": "Expert", "offsetStart": 47, "offsetEnd": 53}, "context": "Intuitively, the more negative the skew of the Expert weights, the more weight lies on the right side of the distribution, hence the greater \"compositional selection effect\".", "mentionContextAttributes": {"used": {"value": false, "score": 0.1231876015663147}, "created": {"value": false, "score": 2.7060508728027344e-05}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999884366989136}, "created": {"value": false, "score": 0.014738857746124268}, "shared": {"value": false, "score": 5.960464477539062e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "topsim", "normalizedForm": "topsim", "offsetStart": 48, "offsetEnd": 54}, "context": "To do so, for each k > 2, we generate 10 Expert topsim distributions with varying skew, following the procedure outlined in appendix D.2 (when k = 2, skew is mechanically 0).", "mentionContextAttributes": {"used": {"value": true, "score": 0.8748096823692322}, "created": {"value": false, "score": 1.0609626770019531e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999884366989136}, "created": {"value": true, "score": 0.9998252987861633}, "shared": {"value": false, "score": 8.344650268554688e-07}}, "references": [{"label": "(Kirby and Brighton, 2006)", "normalizedForm": "Kirby and Brighton, 2006", "refKey": 12}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "EGG", "normalizedForm": "EGG", "offsetStart": 51, "offsetEnd": 54}, "context": "Experiments were implemented using PyTorch and the EGG toolkit (Kharitonov et al., 2021). ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9999420642852783}, "created": {"value": false, "score": 0.005986034870147705}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999420642852783}, "created": {"value": false, "score": 0.05883580446243286}, "shared": {"value": true, "score": 0.9665614366531372}}, "references": [{"label": "(Kharitonov et al., 2021)", "normalizedForm": "Kharitonov et al., 2021", "refKey": 11, "offsetStart": 23625, "offsetEnd": 23650}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "EGG", "normalizedForm": "EGG", "offsetStart": 51, "offsetEnd": 54}, "context": "Code may be found at https://github.com/chengemily/EGG/tree/imitation.", "mentionContextAttributes": {"used": {"value": false, "score": 0.00014698505401611328}, "created": {"value": false, "score": 2.562999725341797e-05}, "shared": {"value": true, "score": 0.9665614366531372}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999420642852783}, "created": {"value": false, "score": 0.05883580446243286}, "shared": {"value": true, "score": 0.9665614366531372}}, "references": [{"label": "(Kharitonov et al., 2021)", "normalizedForm": "Kharitonov et al., 2021", "refKey": 11}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Expert", "normalizedForm": "Expert", "offsetStart": 51, "offsetEnd": 57}, "context": "In both cases, REINFORCE will select a high-topsim Expert, and supervision will weight all Experts equally, that is, supervision is unable to de-select poor topsims.", "mentionContextAttributes": {"used": {"value": false, "score": 0.00984489917755127}, "created": {"value": false, "score": 8.344650268554688e-06}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999884366989136}, "created": {"value": false, "score": 0.014738857746124268}, "shared": {"value": false, "score": 5.960464477539062e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "topsim", "normalizedForm": "topsim", "offsetStart": 51, "offsetEnd": 57}, "context": "4, we note a significant negative effect of Expert topsim standard deviation on the degree of compositional selection.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9992650151252747}, "created": {"value": false, "score": 3.504753112792969e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999884366989136}, "created": {"value": true, "score": 0.9998252987861633}, "shared": {"value": false, "score": 8.344650268554688e-07}}, "references": [{"label": "(Kirby and Brighton, 2006)", "normalizedForm": "Kirby and Brighton, 2006", "refKey": 12}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Expert", "normalizedForm": "Expert", "offsetStart": 52, "offsetEnd": 58}, "context": "Claim A Sender that imitates a uniform mixture of k Expert Senders will output a uniform mixture of the k Expert languages.", "mentionContextAttributes": {"used": {"value": false, "score": 0.00014293193817138672}, "created": {"value": false, "score": 1.1920928955078125e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999884366989136}, "created": {"value": false, "score": 0.014738857746124268}, "shared": {"value": false, "score": 5.960464477539062e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Expert", "normalizedForm": "Expert", "offsetStart": 56, "offsetEnd": 62}, "context": "We present results for imitation on mixtures of k = 2-5 Expert Senders.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9940060377120972}, "created": {"value": false, "score": 0.0003606081008911133}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999884366989136}, "created": {"value": false, "score": 0.014738857746124268}, "shared": {"value": false, "score": 5.960464477539062e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "LayerNorm", "normalizedForm": "LayerNorm", "offsetStart": 56, "offsetEnd": 65}, "context": "It consists of an FC symbol-embedding layer, a GRU with LayerNorm (hidden dim=128), and an FC output head.", "mentionContextAttributes": {"used": {"value": false, "score": 0.0544244647026062}, "created": {"value": false, "score": 8.320808410644531e-05}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.882443904876709}, "created": {"value": true, "score": 0.9910300970077515}, "shared": {"value": false, "score": 3.5762786865234375e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Expert", "normalizedForm": "Expert", "offsetStart": 58, "offsetEnd": 64}, "context": "The Sender imitation task is as follows: given a set of k Expert Senders, we train an identical, newly initialized Sender on the Experts' inputs and outputs (x, m).", "mentionContextAttributes": {"used": {"value": false, "score": 0.010015130043029785}, "created": {"value": false, "score": 0.009588062763214111}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999884366989136}, "created": {"value": false, "score": 0.014738857746124268}, "shared": {"value": false, "score": 5.960464477539062e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Expert", "normalizedForm": "Expert", "offsetStart": 59, "offsetEnd": 65}, "context": "Then, the Imitator's language will consist of a mixture of Expert languages, where the mixture weights reveal the extent of selection.", "mentionContextAttributes": {"used": {"value": false, "score": 0.09176987409591675}, "created": {"value": false, "score": 0.0001093149185180664}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999884366989136}, "created": {"value": false, "score": 0.014738857746124268}, "shared": {"value": false, "score": 5.960464477539062e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Expert", "normalizedForm": "Expert", "offsetStart": 60, "offsetEnd": 66}, "context": "The Receiver imitation task is as follows: given a set of k Expert Receivers and their corresponding Senders, we train an identical, newly initialized Receiver on the Experts' inputs and outputs (m, x).", "mentionContextAttributes": {"used": {"value": false, "score": 0.0340040922164917}, "created": {"value": false, "score": 0.0008781552314758301}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999884366989136}, "created": {"value": false, "score": 0.014738857746124268}, "shared": {"value": false, "score": 5.960464477539062e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Expert", "normalizedForm": "Expert", "offsetStart": 62, "offsetEnd": 68}, "context": "Note that the coefficients may not add to one: if the highest Expert accuracy for a message does not exceed chance (10%), we consider the message unmatched.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9560964703559875}, "created": {"value": false, "score": 1.2874603271484375e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999884366989136}, "created": {"value": false, "score": 0.014738857746124268}, "shared": {"value": false, "score": 5.960464477539062e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Expert", "normalizedForm": "Expert", "offsetStart": 63, "offsetEnd": 69}, "context": "In this mixture, we proxy the Imitator's learned weight for an Expert as the proportion of messages in the training set for which Imitator accuracy on the Expert message is the highest.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9025466442108154}, "created": {"value": false, "score": 3.409385681152344e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999884366989136}, "created": {"value": false, "score": 0.014738857746124268}, "shared": {"value": false, "score": 5.960464477539062e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "topsim", "normalizedForm": "topsim", "offsetStart": 66, "offsetEnd": 72}, "context": "We find that for both imitation by supervision and reinforcement, topsim is (1) significantly negatively correlated to imitation sample complexity T ; (2) significantly positively correlated to speed-of-imitation SOL.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9653425812721252}, "created": {"value": false, "score": 1.0013580322265625e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999884366989136}, "created": {"value": true, "score": 0.9998252987861633}, "shared": {"value": false, "score": 8.344650268554688e-07}}, "references": [{"label": "(Kirby and Brighton, 2006)", "normalizedForm": "Kirby and Brighton, 2006", "refKey": 12}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Expert", "normalizedForm": "Expert", "offsetStart": 66, "offsetEnd": 72}, "context": "To test the effect of the shape (skew, standard deviation) of the Expert topsims on imitation, we define a set of 10 distributions for each setting of k > 2 Experts, noting that when k = 2, the skew is mechanically equal to 0.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9917261600494385}, "created": {"value": false, "score": 4.5180320739746094e-05}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999884366989136}, "created": {"value": false, "score": 0.014738857746124268}, "shared": {"value": false, "score": 5.960464477539062e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Expert", "normalizedForm": "Expert", "offsetStart": 67, "offsetEnd": 73}, "context": "Accuracy We evaluate imitation performance between an Imitator and Expert by the average persymbol accuracy between their messages given an input.", "mentionContextAttributes": {"used": {"value": false, "score": 0.47342222929000854}, "created": {"value": false, "score": 2.5272369384765625e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999884366989136}, "created": {"value": false, "score": 0.014738857746124268}, "shared": {"value": false, "score": 5.960464477539062e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "topsim", "normalizedForm": "topsim", "offsetStart": 68, "offsetEnd": 74}, "context": "Using all Expert topsim distributions generated so far (those where topsim ranks are evenly spaced, and those exhibiting varying skews), we investigate the effect of topsim distribution spread, quantified by standard deviation, on the learned weights.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9993436932563782}, "created": {"value": false, "score": 3.7550926208496094e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999884366989136}, "created": {"value": true, "score": 0.9998252987861633}, "shared": {"value": false, "score": 8.344650268554688e-07}}, "references": [{"label": "(Kirby and Brighton, 2006)", "normalizedForm": "Kirby and Brighton, 2006", "refKey": 12}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Expert", "normalizedForm": "Expert", "offsetStart": 68, "offsetEnd": 74}, "context": "We compute the corresponding policy loss (using a mean baseline per Expert and \u03bb defined in table B.1), and average over all Experts to get the overall policy loss for the Receiver.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9972975850105286}, "created": {"value": false, "score": 5.125999450683594e-06}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999884366989136}, "created": {"value": false, "score": 0.014738857746124268}, "shared": {"value": false, "score": 5.960464477539062e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "topsim", "normalizedForm": "topsim", "offsetStart": 70, "offsetEnd": 76}, "context": "Therefore, we quantify compositionality using topographic similarity (topsim) (Kirby and Brighton, 2006), a grammar-agnostic metric widely applied to emergent languages in the literature. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.39074820280075073}, "created": {"value": false, "score": 8.416175842285156e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999884366989136}, "created": {"value": true, "score": 0.9998252987861633}, "shared": {"value": false, "score": 8.344650268554688e-07}}, "references": [{"label": "(Kirby and Brighton, 2006)", "normalizedForm": "Kirby and Brighton, 2006", "refKey": 12, "offsetStart": 4032, "offsetEnd": 4058}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "topsim", "normalizedForm": "topsim", "offsetStart": 75, "offsetEnd": 81}, "context": "We proxy selection, then, by a negative skew (more weight assigned to high-topsim Experts) and low entropy (peakedness) in the Expert weight distribution.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9999884366989136}, "created": {"value": false, "score": 6.67572021484375e-06}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999884366989136}, "created": {"value": true, "score": 0.9998252987861633}, "shared": {"value": false, "score": 8.344650268554688e-07}}, "references": [{"label": "(Kirby and Brighton, 2006)", "normalizedForm": "Kirby and Brighton, 2006", "refKey": 12}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "topsim", "normalizedForm": "topsim", "offsetStart": 75, "offsetEnd": 81}, "context": "For example, when k = 3, we take the lowest, 50 th percentile, and highest-topsim languages.", "mentionContextAttributes": {"used": {"value": false, "score": 0.05606508255004883}, "created": {"value": false, "score": 6.520748138427734e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999884366989136}, "created": {"value": true, "score": 0.9998252987861633}, "shared": {"value": false, "score": 8.344650268554688e-07}}, "references": [{"label": "(Kirby and Brighton, 2006)", "normalizedForm": "Kirby and Brighton, 2006", "refKey": 12}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "topsim", "normalizedForm": "topsim", "offsetStart": 75, "offsetEnd": 81}, "context": "We establish a positive and statistically significant relationship between topsim and ease-ofimitation, expanding the explorations in Ren et al.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9944621920585632}, "created": {"value": true, "score": 0.9998252987861633}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999884366989136}, "created": {"value": true, "score": 0.9998252987861633}, "shared": {"value": false, "score": 8.344650268554688e-07}}, "references": [{"label": "(Kirby and Brighton, 2006)", "normalizedForm": "Kirby and Brighton, 2006", "refKey": 12}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Expert", "normalizedForm": "Expert", "offsetStart": 76, "offsetEnd": 82}, "context": "Only in imitation training, when unrolling the Imitator Sender, we take the Expert Sender's previous output to be the Imitator's next input so that the Imitator learns an autoregressive model of the Expert language.", "mentionContextAttributes": {"used": {"value": false, "score": 0.01016700267791748}, "created": {"value": false, "score": 7.450580596923828e-05}, "shared": {"value": false, "score": 4.76837158203125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999884366989136}, "created": {"value": false, "score": 0.014738857746124268}, "shared": {"value": false, "score": 5.960464477539062e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Expert", "normalizedForm": "Expert", "offsetStart": 78, "offsetEnd": 84}, "context": "Dataset Data in the imitation task consists of inputs and outputs of pairs of Expert agents trained to convergence on a communication game-in our case, the two-agent reconstruction task of Kottur et al. (2017).", "mentionContextAttributes": {"used": {"value": true, "score": 0.5220407843589783}, "created": {"value": false, "score": 6.830692291259766e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999884366989136}, "created": {"value": false, "score": 0.014738857746124268}, "shared": {"value": false, "score": 5.960464477539062e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "topsim", "normalizedForm": "topsim", "offsetStart": 81, "offsetEnd": 87}, "context": "Formally, let there be k Experts, where Experts are sorted in ascending order of topsim (Expert i=1 is the least and i=k is the most compositional, respectively). ", "mentionContextAttributes": {"used": {"value": false, "score": 0.4847298264503479}, "created": {"value": false, "score": 2.0623207092285156e-05}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999884366989136}, "created": {"value": true, "score": 0.9998252987861633}, "shared": {"value": false, "score": 8.344650268554688e-07}}, "references": [{"label": "(Kirby and Brighton, 2006)", "normalizedForm": "Kirby and Brighton, 2006", "refKey": 12}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Expert", "normalizedForm": "Expert", "offsetStart": 86, "offsetEnd": 92}, "context": "Importantly, when doing a forward pass for the Sender during training, we feed it the Expert symbol from the previous timestep as input so that the Sender learns an autoregressive model of language.", "mentionContextAttributes": {"used": {"value": false, "score": 0.008032262325286865}, "created": {"value": false, "score": 4.76837158203125e-05}, "shared": {"value": false, "score": 4.76837158203125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999884366989136}, "created": {"value": false, "score": 0.014738857746124268}, "shared": {"value": false, "score": 5.960464477539062e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Expert", "normalizedForm": "Expert", "offsetStart": 88, "offsetEnd": 94}, "context": "First, we generate 30 Expert languages from the referential task, initially considering Expert distributions corresponding to evenly-spaced percentiles of topsim, including the minimum and maximum (0.26, 0.43).", "mentionContextAttributes": {"used": {"value": true, "score": 0.9866998195648193}, "created": {"value": false, "score": 0.0011522769927978516}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999884366989136}, "created": {"value": false, "score": 0.014738857746124268}, "shared": {"value": false, "score": 5.960464477539062e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Expert", "normalizedForm": "Expert", "offsetStart": 89, "offsetEnd": 95}, "context": "Expert training produces the following data: 1) inputs x; 2) messages m corresponding to Expert Senders' encodings of x; and 3) outputs x, the Expert Receivers' reconstructions of x given m.", "mentionContextAttributes": {"used": {"value": false, "score": 0.013628661632537842}, "created": {"value": false, "score": 3.6954879760742188e-06}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999884366989136}, "created": {"value": false, "score": 0.014738857746124268}, "shared": {"value": false, "score": 5.960464477539062e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Expert", "normalizedForm": "Expert", "offsetStart": 89, "offsetEnd": 95}, "context": "Formally, let there be k Experts, where Experts are sorted in ascending order of topsim (Expert i=1 is the least and i=k is the most compositional, respectively).", "mentionContextAttributes": {"used": {"value": false, "score": 0.4847298264503479}, "created": {"value": false, "score": 2.0623207092285156e-05}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999884366989136}, "created": {"value": false, "score": 0.014738857746124268}, "shared": {"value": false, "score": 5.960464477539062e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Expert", "normalizedForm": "Expert", "offsetStart": 91, "offsetEnd": 97}, "context": "Moreover, correlations between topsim and easeof-imitation are stronger than those between Expert validation accuracy and ease-of-imitation (table C.1).", "mentionContextAttributes": {"used": {"value": false, "score": 0.2331947684288025}, "created": {"value": false, "score": 5.245208740234375e-06}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999884366989136}, "created": {"value": false, "score": 0.014738857746124268}, "shared": {"value": false, "score": 5.960464477539062e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Expert", "normalizedForm": "Expert", "offsetStart": 96, "offsetEnd": 102}, "context": "Successful imitation minimizes the Kullback-Leibler divergence between the Imitator \u03c0 I and the Expert policies \u03c0 E ; supervision is classically known to minimize the forward KL divergence D KL (\u03c0 E ||\u03c0 I ), while reinforcement minimizes the reverse KL divergence D KL (\u03c0 I ||\u03c0 E ) with respect to \u03c0 I .", "mentionContextAttributes": {"used": {"value": false, "score": 0.016600430011749268}, "created": {"value": false, "score": 3.5762786865234375e-06}, "shared": {"value": false, "score": 4.76837158203125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999884366989136}, "created": {"value": false, "score": 0.014738857746124268}, "shared": {"value": false, "score": 5.960464477539062e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "topsim", "normalizedForm": "topsim", "offsetStart": 100, "offsetEnd": 106}, "context": "We note that, when imitating by both reinforcement and supervision, there is no initial increase in topsim followed by a convergence to Expert topsim (fig.", "mentionContextAttributes": {"used": {"value": false, "score": 0.023745298385620117}, "created": {"value": false, "score": 7.987022399902344e-06}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999884366989136}, "created": {"value": true, "score": 0.9998252987861633}, "shared": {"value": false, "score": 8.344650268554688e-07}}, "references": [{"label": "(Kirby and Brighton, 2006)", "normalizedForm": "Kirby and Brighton, 2006", "refKey": 12}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Expert", "normalizedForm": "Expert", "offsetStart": 102, "offsetEnd": 108}, "context": "We find that for both REINFORCE and supervision, holding k equal, the skew and entropy of the learned Expert weight distribution are robust (i.e., not correlated) to the skew of the underlying Expert topsim distribution (fig.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9857760071754456}, "created": {"value": false, "score": 5.125999450683594e-06}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999884366989136}, "created": {"value": false, "score": 0.014738857746124268}, "shared": {"value": false, "score": 5.960464477539062e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Expert", "normalizedForm": "Expert", "offsetStart": 102, "offsetEnd": 108}, "context": "Given an input x, we write m (i) j as the value of the j th position in the message m (i) produced by Expert i.", "mentionContextAttributes": {"used": {"value": true, "score": 0.997969925403595}, "created": {"value": false, "score": 0.003756284713745117}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999884366989136}, "created": {"value": false, "score": 0.014738857746124268}, "shared": {"value": false, "score": 5.960464477539062e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Expert", "normalizedForm": "Expert", "offsetStart": 104, "offsetEnd": 110}, "context": "With the large communication channel size typical of emergent communication games, we can expect little Expert message collision.", "mentionContextAttributes": {"used": {"value": false, "score": 0.00014257431030273438}, "created": {"value": false, "score": 0.00017660856246948242}, "shared": {"value": false, "score": 5.960464477539062e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999884366989136}, "created": {"value": false, "score": 0.014738857746124268}, "shared": {"value": false, "score": 5.960464477539062e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Expert", "normalizedForm": "Expert", "offsetStart": 105, "offsetEnd": 111}, "context": "Imitators are then tasked to minimize the difference between their output and a uniform mixture of the k Expert outputs.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9497766494750977}, "created": {"value": false, "score": 0.00016188621520996094}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999884366989136}, "created": {"value": false, "score": 0.014738857746124268}, "shared": {"value": false, "score": 5.960464477539062e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Expert", "normalizedForm": "Expert", "offsetStart": 105, "offsetEnd": 111}, "context": "Then, the loss over the entire mixture is the average cross-entropy loss per Receiver, aggregated across Expert Receivers.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9935238361358643}, "created": {"value": false, "score": 2.777576446533203e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999884366989136}, "created": {"value": false, "score": 0.014738857746124268}, "shared": {"value": false, "score": 5.960464477539062e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Expert", "normalizedForm": "Expert", "offsetStart": 106, "offsetEnd": 112}, "context": "Claim A Sender that imitates a uniform mixture of k Expert Senders will output a uniform mixture of the k Expert languages.", "mentionContextAttributes": {"used": {"value": false, "score": 0.00014293193817138672}, "created": {"value": false, "score": 1.1920928955078125e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999884366989136}, "created": {"value": false, "score": 0.014738857746124268}, "shared": {"value": false, "score": 5.960464477539062e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Expert", "normalizedForm": "Expert", "offsetStart": 107, "offsetEnd": 113}, "context": "The Imitator is then tasked to minimize the difference between their output and a uniform mixture of the k Expert outputs.", "mentionContextAttributes": {"used": {"value": false, "score": 0.4969159960746765}, "created": {"value": false, "score": 0.001081228256225586}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999884366989136}, "created": {"value": false, "score": 0.014738857746124268}, "shared": {"value": false, "score": 5.960464477539062e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Expert", "normalizedForm": "Expert", "offsetStart": 108, "offsetEnd": 114}, "context": "We evaluate peakedness using the Shannon entropy and asymmetry using Fisher's moment coefficient of skew of Expert weights.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9998672008514404}, "created": {"value": false, "score": 2.384185791015625e-06}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999884366989136}, "created": {"value": false, "score": 0.014738857746124268}, "shared": {"value": false, "score": 5.960464477539062e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "topsim", "normalizedForm": "topsim", "offsetStart": 108, "offsetEnd": 114}, "context": "Comparing SOL and T for both Sender and Receiver to other compositionality metrics (table C.1), we see that topsim is generally most correlated with sample complexity and speed-oflearning.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9276832342147827}, "created": {"value": false, "score": 3.933906555175781e-06}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999884366989136}, "created": {"value": true, "score": 0.9998252987861633}, "shared": {"value": false, "score": 8.344650268554688e-07}}, "references": [{"label": "(Kirby and Brighton, 2006)", "normalizedForm": "Kirby and Brighton, 2006", "refKey": 12}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Expert", "normalizedForm": "Expert", "offsetStart": 109, "offsetEnd": 115}, "context": "In the compositionality vs. ease-of-imitation experiments, we train newly initialized Imitator pairs on each Expert pair over 500 epochs for supervision and 2000 epochs for reinforcement, aggregating over 3 random seeds.", "mentionContextAttributes": {"used": {"value": false, "score": 0.4811664819717407}, "created": {"value": false, "score": 0.0005541443824768066}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999884366989136}, "created": {"value": false, "score": 0.014738857746124268}, "shared": {"value": false, "score": 5.960464477539062e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Expert", "normalizedForm": "Expert", "offsetStart": 113, "offsetEnd": 119}, "context": "In the direct supervision setting, for the Sender producing a distribution over messages \u03c0 I given x and a given Expert i producing message m (i) , where m j is the j th symbol of m (i) , the overall loss for a uniform mixture of k Expert Senders is the following cross-entropy loss:", "mentionContextAttributes": {"used": {"value": true, "score": 0.9083145260810852}, "created": {"value": false, "score": 1.9311904907226562e-05}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999884366989136}, "created": {"value": false, "score": 0.014738857746124268}, "shared": {"value": false, "score": 5.960464477539062e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Expert", "normalizedForm": "Expert", "offsetStart": 114, "offsetEnd": 120}, "context": "For supervised learning, the Receiver imitation loss is equal to the crossentropy loss between its output and the Expert Receiver's output given the same corresponding Expert Sender's message m.", "mentionContextAttributes": {"used": {"value": false, "score": 0.1608128547668457}, "created": {"value": false, "score": 8.225440979003906e-06}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999884366989136}, "created": {"value": false, "score": 0.014738857746124268}, "shared": {"value": false, "score": 5.960464477539062e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Expert", "normalizedForm": "Expert", "offsetStart": 117, "offsetEnd": 123}, "context": "Finally, selection is less salient as the number of Experts increases, seen by the increasing entropies and skews of Expert weights (figs.", "mentionContextAttributes": {"used": {"value": false, "score": 0.0025072097778320312}, "created": {"value": false, "score": 8.630752563476562e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999884366989136}, "created": {"value": false, "score": 0.014738857746124268}, "shared": {"value": false, "score": 5.960464477539062e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Expert", "normalizedForm": "Expert", "offsetStart": 118, "offsetEnd": 124}, "context": "The ideal Imitator \u03c0 I * minimizes the crossentropy objective between its messages and that of a uniform mixture of k Expert Senders.", "mentionContextAttributes": {"used": {"value": false, "score": 0.000584721565246582}, "created": {"value": false, "score": 3.814697265625e-06}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999884366989136}, "created": {"value": false, "score": 0.014738857746124268}, "shared": {"value": false, "score": 5.960464477539062e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Expert", "normalizedForm": "Expert", "offsetStart": 118, "offsetEnd": 124}, "context": "That is, for each round of training, all k Experts as well as the Imitator Receiver receive input m, or the output of Expert Sender given x, and output x(1) \u2022 \u2022 \u2022 x(k) and xI , respectively.", "mentionContextAttributes": {"used": {"value": false, "score": 0.46241945028305054}, "created": {"value": false, "score": 1.4185905456542969e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999884366989136}, "created": {"value": false, "score": 0.014738857746124268}, "shared": {"value": false, "score": 5.960464477539062e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Expert", "normalizedForm": "Expert", "offsetStart": 119, "offsetEnd": 125}, "context": "This is desirable when imitating by reinforcement and undesirable when imitating by supervision: for example, consider Expert topsim distributions [low high high] (skew< 0) and [low low high] (skew> 0).", "mentionContextAttributes": {"used": {"value": false, "score": 0.0008444786071777344}, "created": {"value": false, "score": 4.887580871582031e-06}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999884366989136}, "created": {"value": false, "score": 0.014738857746124268}, "shared": {"value": false, "score": 5.960464477539062e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "topsim", "normalizedForm": "topsim", "offsetStart": 121, "offsetEnd": 127}, "context": "Then, we fill the other k -3 points to create a uniform distribution with mean M. If the median is less than the average topsim, then the left endpoint of this uniform distribution is the minimum topsim.", "mentionContextAttributes": {"used": {"value": true, "score": 0.8480278253555298}, "created": {"value": false, "score": 2.372264862060547e-05}, "shared": {"value": false, "score": 7.152557373046875e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999884366989136}, "created": {"value": true, "score": 0.9998252987861633}, "shared": {"value": false, "score": 8.344650268554688e-07}}, "references": [{"label": "(Kirby and Brighton, 2006)", "normalizedForm": "Kirby and Brighton, 2006", "refKey": 12}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "topsim", "normalizedForm": "topsim", "offsetStart": 122, "offsetEnd": 128}, "context": "While we consider other compositionality metrics such as positional disentanglement (Chaabouni et al., 2020), we focus on topsim due to its high correlation with generalization accuracy (\u03c1 = 0.83) (Rita et al., 2022b).", "mentionContextAttributes": {"used": {"value": false, "score": 0.01821690797805786}, "created": {"value": false, "score": 1.430511474609375e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999884366989136}, "created": {"value": true, "score": 0.9998252987861633}, "shared": {"value": false, "score": 8.344650268554688e-07}}, "references": [{"label": "(Kirby and Brighton, 2006)", "normalizedForm": "Kirby and Brighton, 2006", "refKey": 12}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "topsim", "normalizedForm": "topsim", "offsetStart": 125, "offsetEnd": 131}, "context": "However, as a fundamental reason we care about compositionality is due to its link to linguistic generalization, we focus on topsim, which we found has the highest correlation with generalization accuracy on the reconstruction task (table A.1).", "mentionContextAttributes": {"used": {"value": true, "score": 0.9982465505599976}, "created": {"value": false, "score": 4.398822784423828e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999884366989136}, "created": {"value": true, "score": 0.9998252987861633}, "shared": {"value": false, "score": 8.344650268554688e-07}}, "references": [{"label": "(Kirby and Brighton, 2006)", "normalizedForm": "Kirby and Brighton, 2006", "refKey": 12}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "topsim", "normalizedForm": "topsim", "offsetStart": 126, "offsetEnd": 132}, "context": "This is desirable when imitating by reinforcement and undesirable when imitating by supervision: for example, consider Expert topsim distributions [low high high] (skew< 0) and [low low high] (skew> 0).", "mentionContextAttributes": {"used": {"value": false, "score": 0.0008444786071777344}, "created": {"value": false, "score": 4.887580871582031e-06}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999884366989136}, "created": {"value": true, "score": 0.9998252987861633}, "shared": {"value": false, "score": 8.344650268554688e-07}}, "references": [{"label": "(Kirby and Brighton, 2006)", "normalizedForm": "Kirby and Brighton, 2006", "refKey": 12}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Expert", "normalizedForm": "Expert", "offsetStart": 127, "offsetEnd": 133}, "context": "We proxy selection, then, by a negative skew (more weight assigned to high-topsim Experts) and low entropy (peakedness) in the Expert weight distribution.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9999884366989136}, "created": {"value": false, "score": 6.67572021484375e-06}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999884366989136}, "created": {"value": false, "score": 0.014738857746124268}, "shared": {"value": false, "score": 5.960464477539062e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Expert", "normalizedForm": "Expert", "offsetStart": 136, "offsetEnd": 142}, "context": "We note that, when imitating by both reinforcement and supervision, there is no initial increase in topsim followed by a convergence to Expert topsim (fig.", "mentionContextAttributes": {"used": {"value": false, "score": 0.023745298385620117}, "created": {"value": false, "score": 7.987022399902344e-06}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999884366989136}, "created": {"value": false, "score": 0.014738857746124268}, "shared": {"value": false, "score": 5.960464477539062e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Expert", "normalizedForm": "Expert", "offsetStart": 137, "offsetEnd": 143}, "context": "Selection of compositional languages Sender imitation consists of learning one-to-one input-tomessage mappings from a sea of one-to-many Expert mappings.", "mentionContextAttributes": {"used": {"value": false, "score": 0.00019794702529907227}, "created": {"value": false, "score": 9.083747863769531e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999884366989136}, "created": {"value": false, "score": 0.014738857746124268}, "shared": {"value": false, "score": 5.960464477539062e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Expert", "normalizedForm": "Expert", "offsetStart": 138, "offsetEnd": 144}, "context": "That is, imitation by supervision is mean-fitting while imitation by reinforcement is mode-fitting-the former learns a uniform mixture of Expert languages (see appendix D.4 for proof), and the latter selects the best Expert language.", "mentionContextAttributes": {"used": {"value": false, "score": 0.002335190773010254}, "created": {"value": false, "score": 1.895427703857422e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999884366989136}, "created": {"value": false, "score": 0.014738857746124268}, "shared": {"value": false, "score": 5.960464477539062e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Expert", "normalizedForm": "Expert", "offsetStart": 140, "offsetEnd": 146}, "context": "To quantify selection, we use the intuition that selection corresponds jointly to peakedness and asymmetry in the learned distribution over Expert languages sorted by topsim.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9995469450950623}, "created": {"value": false, "score": 4.649162292480469e-06}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999884366989136}, "created": {"value": false, "score": 0.014738857746124268}, "shared": {"value": false, "score": 5.960464477539062e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Expert", "normalizedForm": "Expert", "offsetStart": 143, "offsetEnd": 149}, "context": "Expert training produces the following data: 1) inputs x; 2) messages m corresponding to Expert Senders' encodings of x; and 3) outputs x, the Expert Receivers' reconstructions of x given m.", "mentionContextAttributes": {"used": {"value": false, "score": 0.013628661632537842}, "created": {"value": false, "score": 3.6954879760742188e-06}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999884366989136}, "created": {"value": false, "score": 0.014738857746124268}, "shared": {"value": false, "score": 5.960464477539062e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "topsim", "normalizedForm": "topsim", "offsetStart": 143, "offsetEnd": 149}, "context": "We note that, when imitating by both reinforcement and supervision, there is no initial increase in topsim followed by a convergence to Expert topsim (fig.", "mentionContextAttributes": {"used": {"value": false, "score": 0.023745298385620117}, "created": {"value": false, "score": 7.987022399902344e-06}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999884366989136}, "created": {"value": true, "score": 0.9998252987861633}, "shared": {"value": false, "score": 8.344650268554688e-07}}, "references": [{"label": "(Kirby and Brighton, 2006)", "normalizedForm": "Kirby and Brighton, 2006", "refKey": 12}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "topsim", "normalizedForm": "topsim", "offsetStart": 144, "offsetEnd": 150}, "context": "Having (1) demonstrated a selection of compositional languages in imitation by reinforcement; (2) established a significant correlation between topsim and ease-of-imitation; we offer the following explanation for compositional selection: mode-seeking behavior in reinforcement learning exploits easeof-learning of compositional languages, resulting in a selection of compositionality.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9992357492446899}, "created": {"value": false, "score": 0.0004999637603759766}, "shared": {"value": false, "score": 5.960464477539062e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999884366989136}, "created": {"value": true, "score": 0.9998252987861633}, "shared": {"value": false, "score": 8.344650268554688e-07}}, "references": [{"label": "(Kirby and Brighton, 2006)", "normalizedForm": "Kirby and Brighton, 2006", "refKey": 12}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Expert", "normalizedForm": "Expert", "offsetStart": 155, "offsetEnd": 161}, "context": "In this mixture, we proxy the Imitator's learned weight for an Expert as the proportion of messages in the training set for which Imitator accuracy on the Expert message is the highest.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9025466442108154}, "created": {"value": false, "score": 3.409385681152344e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999884366989136}, "created": {"value": false, "score": 0.014738857746124268}, "shared": {"value": false, "score": 5.960464477539062e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "topsim", "normalizedForm": "topsim", "offsetStart": 155, "offsetEnd": 161}, "context": "First, we generate 30 Expert languages from the referential task, initially considering Expert distributions corresponding to evenly-spaced percentiles of topsim, including the minimum and maximum (0.26, 0.43).", "mentionContextAttributes": {"used": {"value": true, "score": 0.9866998195648193}, "created": {"value": false, "score": 0.0011522769927978516}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999884366989136}, "created": {"value": true, "score": 0.9998252987861633}, "shared": {"value": false, "score": 8.344650268554688e-07}}, "references": [{"label": "(Kirby and Brighton, 2006)", "normalizedForm": "Kirby and Brighton, 2006", "refKey": 12}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "topsim", "normalizedForm": "topsim", "offsetStart": 166, "offsetEnd": 172}, "context": "Using all Expert topsim distributions generated so far (those where topsim ranks are evenly spaced, and those exhibiting varying skews), we investigate the effect of topsim distribution spread, quantified by standard deviation, on the learned weights.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9993436932563782}, "created": {"value": false, "score": 3.7550926208496094e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999884366989136}, "created": {"value": true, "score": 0.9998252987861633}, "shared": {"value": false, "score": 8.344650268554688e-07}}, "references": [{"label": "(Kirby and Brighton, 2006)", "normalizedForm": "Kirby and Brighton, 2006", "refKey": 12}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "topsim", "normalizedForm": "topsim", "offsetStart": 167, "offsetEnd": 173}, "context": "To quantify selection, we use the intuition that selection corresponds jointly to peakedness and asymmetry in the learned distribution over Expert languages sorted by topsim. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9995469450950623}, "created": {"value": false, "score": 4.649162292480469e-06}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999884366989136}, "created": {"value": true, "score": 0.9998252987861633}, "shared": {"value": false, "score": 8.344650268554688e-07}}, "references": [{"label": "(Kirby and Brighton, 2006)", "normalizedForm": "Kirby and Brighton, 2006", "refKey": 12}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Expert", "normalizedForm": "Expert", "offsetStart": 168, "offsetEnd": 174}, "context": "For supervised learning, the Receiver imitation loss is equal to the crossentropy loss between its output and the Expert Receiver's output given the same corresponding Expert Sender's message m.", "mentionContextAttributes": {"used": {"value": false, "score": 0.1608130931854248}, "created": {"value": false, "score": 8.225440979003906e-06}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999884366989136}, "created": {"value": false, "score": 0.014738857746124268}, "shared": {"value": false, "score": 5.960464477539062e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "topsim", "normalizedForm": "topsim", "offsetStart": 170, "offsetEnd": 176}, "context": "Similar to Rita et al. (2022a); Auersperger and Pecina (2022) and in contrast to Chaabouni et al. (2020); Kharitonov and Baroni (2020), we find that correlations between topsim and both indistribution and zero-shot generalization on the reconstruction task are high, and highly significant (\u03b1 = 1e-2): Spearman's \u03c1 = 0.83 and Pearson's R 2 = 0.81 for in-distribution generalization, and \u03c1 = 0.81, R 2 = 0.78 for zero-shot generalization. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9980796575546265}, "created": {"value": false, "score": 4.410743713378906e-06}, "shared": {"value": false, "score": 4.76837158203125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999884366989136}, "created": {"value": true, "score": 0.9998252987861633}, "shared": {"value": false, "score": 8.344650268554688e-07}}, "references": [{"label": "(Kirby and Brighton, 2006)", "normalizedForm": "Kirby and Brighton, 2006", "refKey": 12}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Expert", "normalizedForm": "Expert", "offsetStart": 170, "offsetEnd": 176}, "context": "That is, the more disperse the Expert topsims, the more the Imitator can differentiate between and select compositional Experts (shown by a more negative skew in learned Expert weights).", "mentionContextAttributes": {"used": {"value": false, "score": 0.004536449909210205}, "created": {"value": false, "score": 2.2530555725097656e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999884366989136}, "created": {"value": false, "score": 0.014738857746124268}, "shared": {"value": false, "score": 5.960464477539062e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Expert", "normalizedForm": "Expert", "offsetStart": 179, "offsetEnd": 185}, "context": "To investigate whether compositional languages are selected for in imitation, we posit an imitation task where one new Imitator Sender or Receiver simultaneously imitates several Expert Senders or Receivers with varying topsims.", "mentionContextAttributes": {"used": {"value": false, "score": 0.013565778732299805}, "created": {"value": false, "score": 0.014738857746124268}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999884366989136}, "created": {"value": false, "score": 0.014738857746124268}, "shared": {"value": false, "score": 5.960464477539062e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Expert", "normalizedForm": "Expert", "offsetStart": 193, "offsetEnd": 199}, "context": "While we explore imitation for both agents, we focus on Sender imitation in the main text, and extend to Receiver imitation in appendix E. A minimal example of imitation learning with only one Expert Sender-Receiver pair is shown in fig. 1.", "mentionContextAttributes": {"used": {"value": false, "score": 0.023929476737976074}, "created": {"value": false, "score": 0.0003167390823364258}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999884366989136}, "created": {"value": false, "score": 0.014738857746124268}, "shared": {"value": false, "score": 5.960464477539062e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Expert", "normalizedForm": "Expert", "offsetStart": 193, "offsetEnd": 199}, "context": "We find that for both REINFORCE and supervision, holding k equal, the skew and entropy of the learned Expert weight distribution are robust (i.e., not correlated) to the skew of the underlying Expert topsim distribution (fig.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9857760071754456}, "created": {"value": false, "score": 5.125999450683594e-06}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999884366989136}, "created": {"value": false, "score": 0.014738857746124268}, "shared": {"value": false, "score": 5.960464477539062e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "topsim", "normalizedForm": "topsim", "offsetStart": 196, "offsetEnd": 202}, "context": "Then, we fill the other k -3 points to create a uniform distribution with mean M. If the median is less than the average topsim, then the left endpoint of this uniform distribution is the minimum topsim.", "mentionContextAttributes": {"used": {"value": true, "score": 0.8480284810066223}, "created": {"value": false, "score": 2.372264862060547e-05}, "shared": {"value": false, "score": 7.152557373046875e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999884366989136}, "created": {"value": true, "score": 0.9998252987861633}, "shared": {"value": false, "score": 8.344650268554688e-07}}, "references": [{"label": "(Kirby and Brighton, 2006)", "normalizedForm": "Kirby and Brighton, 2006", "refKey": 12}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Expert", "normalizedForm": "Expert", "offsetStart": 199, "offsetEnd": 205}, "context": "Only in imitation training, when unrolling the Imitator Sender, we take the Expert Sender's previous output to be the Imitator's next input so that the Imitator learns an autoregressive model of the Expert language.", "mentionContextAttributes": {"used": {"value": false, "score": 0.01016700267791748}, "created": {"value": false, "score": 7.450580596923828e-05}, "shared": {"value": false, "score": 4.76837158203125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999884366989136}, "created": {"value": false, "score": 0.014738857746124268}, "shared": {"value": false, "score": 5.960464477539062e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "topsim", "normalizedForm": "topsim", "offsetStart": 200, "offsetEnd": 206}, "context": "We find that for both REINFORCE and supervision, holding k equal, the skew and entropy of the learned Expert weight distribution are robust (i.e., not correlated) to the skew of the underlying Expert topsim distribution (fig.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9857760071754456}, "created": {"value": false, "score": 5.125999450683594e-06}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999884366989136}, "created": {"value": true, "score": 0.9998252987861633}, "shared": {"value": false, "score": 8.344650268554688e-07}}, "references": [{"label": "(Kirby and Brighton, 2006)", "normalizedForm": "Kirby and Brighton, 2006", "refKey": 12}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Expert", "normalizedForm": "Expert", "offsetStart": 217, "offsetEnd": 223}, "context": "That is, imitation by supervision is mean-fitting while imitation by reinforcement is mode-fitting-the former learns a uniform mixture of Expert languages (see appendix D.4 for proof), and the latter selects the best Expert language.", "mentionContextAttributes": {"used": {"value": false, "score": 0.002335190773010254}, "created": {"value": false, "score": 1.895427703857422e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999884366989136}, "created": {"value": false, "score": 0.014738857746124268}, "shared": {"value": false, "score": 5.960464477539062e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Expert", "normalizedForm": "Expert", "offsetStart": 228, "offsetEnd": 234}, "context": "Imitation learning algorithms While imitation is classically implemented as supervised learning, we test two imitation learning procedures: 1) supervised learning (SV) with respect to the cross-entropy loss between Imitator and Expert outputs; and 2) reinforcement learning with the RE-INFORCE algorithm (RF) (Williams, 1992), using per-symbol accuracy as a reward.", "mentionContextAttributes": {"used": {"value": false, "score": 0.02150672674179077}, "created": {"value": false, "score": 7.045269012451172e-05}, "shared": {"value": false, "score": 4.76837158203125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999884366989136}, "created": {"value": false, "score": 0.014738857746124268}, "shared": {"value": false, "score": 5.960464477539062e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Expert", "normalizedForm": "Expert", "offsetStart": 232, "offsetEnd": 238}, "context": "In the direct supervision setting, for the Sender producing a distribution over messages \u03c0 I given x and a given Expert i producing message m (i) , where m j is the j th symbol of m (i) , the overall loss for a uniform mixture of k Expert Senders is the following cross-entropy loss:", "mentionContextAttributes": {"used": {"value": true, "score": 0.9083144068717957}, "created": {"value": false, "score": 1.9311904907226562e-05}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999884366989136}, "created": {"value": false, "score": 0.014738857746124268}, "shared": {"value": false, "score": 5.960464477539062e-07}}}], "references": [{"refKey": 12, "tei": "<biblStruct xml:id=\"b12\">\n\t<analytic>\n\t\t<title level=\"a\" type=\"main\">Understanding linguistic evolution by visualizing the emergence of topographic mappings</title>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Simon</forename><surname>Kirby</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Henry</forename><surname>Brighton</surname></persName>\n\t\t</author>\n\t</analytic>\n\t<monogr>\n\t\t<title level=\"j\">Artifical Life</title>\n\t\t<imprint>\n\t\t\t<date>2006</date>\n\t\t</imprint>\n\t</monogr>\n</biblStruct>\n"}, {"refKey": 11, "tei": "<biblStruct xml:id=\"b11\">\n\t<monogr>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Eugene</forename><surname>Kharitonov</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Roberto</forename><surname>Dess\u00ec</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Rahma</forename><surname>Chaabouni</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Diane</forename><surname>Bouchacourt</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Marco</forename><surname>Baroni</surname></persName>\n\t\t</author>\n\t\t<title level=\"m\">EGG: a toolkit for research on Emergence of lanGuage in Games</title>\n\t\t<imprint>\n\t\t\t<date>2021</date>\n\t\t</imprint>\n\t</monogr>\n</biblStruct>\n"}], "runtime": 13338, "id": "7887b41dfea32aa672de70da38dc4aafb8053962", "metadata": {"id": "7887b41dfea32aa672de70da38dc4aafb8053962"}, "original_file_path": "../../datalake/Samuel/SOFTware-Sync/downloads/xml/hal-04242002.grobid.tei.xml", "file_name": "hal-04242002.grobid.tei.xml"}