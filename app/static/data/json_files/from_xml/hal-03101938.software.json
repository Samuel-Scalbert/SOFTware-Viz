{"application": "software-mentions", "version": "0.8.0", "date": "2024-10-07T11:47+0000", "md5": "66EE4050CD077B178F38FAF8CE1FB8BE", "mentions": [{"type": "software", "software-type": "software", "software-name": {"rawForm": "fastText", "normalizedForm": "fastText", "offsetStart": 0, "offsetEnd": 8}, "context": "fastText is based on the skip-gram model, where each word is represented as a bag of character n-grams.", "mentionContextAttributes": {"used": {"value": false, "score": 0.005881309509277344}, "created": {"value": false, "score": 0.0001754164695739746}, "shared": {"value": false, "score": 4.76837158203125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9998316764831543}, "created": {"value": true, "score": 0.9998674392700195}, "shared": {"value": false, "score": 9.5367431640625e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "fastText", "normalizedForm": "fastText", "offsetStart": 0, "offsetEnd": 8}, "context": "fastText embedding: It is an extension of Mikolov's embedding (Mikolov et al., 2013).", "mentionContextAttributes": {"used": {"value": false, "score": 0.0008911490440368652}, "created": {"value": false, "score": 7.05718994140625e-05}, "shared": {"value": false, "score": 4.76837158203125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9998316764831543}, "created": {"value": true, "score": 0.9998674392700195}, "shared": {"value": false, "score": 9.5367431640625e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "fastText", "normalizedForm": "fastText", "offsetStart": 0, "offsetEnd": 8}, "context": "fastText model: We use pre-trained fastText embedding model and apply this model to generate one embedding for each word of a given comment.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9616010785102844}, "created": {"value": false, "score": 0.006826341152191162}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9998316764831543}, "created": {"value": true, "score": 0.9998674392700195}, "shared": {"value": false, "score": 9.5367431640625e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "fastText", "normalizedForm": "fastText", "offsetStart": 4, "offsetEnd": 12}, "context": "The fastText approach is based on the skip-gram model, where each word is represented as a bag of character n-grams (Joulin et al., 2016;Bojanowski et al., 2017).", "mentionContextAttributes": {"used": {"value": false, "score": 0.0002232193946838379}, "created": {"value": false, "score": 0.00019794702529907227}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9998316764831543}, "created": {"value": true, "score": 0.9998674392700195}, "shared": {"value": false, "score": 9.5367431640625e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "fastText", "normalizedForm": "fastText", "offsetStart": 10, "offsetEnd": 18}, "context": "For both, fastText and BER, we decided to remove the numbers and all the special characters except '!', '?', ',', '.' and apostrophe.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9795539379119873}, "created": {"value": false, "score": 0.0006858110427856445}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9998316764831543}, "created": {"value": true, "score": 0.9998674392700195}, "shared": {"value": false, "score": 9.5367431640625e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "fastText", "normalizedForm": "fastText", "offsetStart": 11, "offsetEnd": 19}, "context": "We compare fastText and BERT (Bidirectional Encoder Representations from Transformers) embedding representations of words.", "mentionContextAttributes": {"used": {"value": true, "score": 0.7970993518829346}, "created": {"value": false, "score": 4.851818084716797e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9998316764831543}, "created": {"value": true, "score": 0.9998674392700195}, "shared": {"value": false, "score": 9.5367431640625e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "fastText", "normalizedForm": "fastText", "offsetStart": 29, "offsetEnd": 37}, "context": "Unlike Mikolov's embeddings, fastText is able to provide an embedding for misspelled word, rare words or words that were not present in the training corpus, because fastText uses character n-gram word tokenization.", "mentionContextAttributes": {"used": {"value": false, "score": 0.0006704330444335938}, "created": {"value": false, "score": 0.00021535158157348633}, "shared": {"value": false, "score": 4.76837158203125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9998316764831543}, "created": {"value": true, "score": 0.9998674392700195}, "shared": {"value": false, "score": 9.5367431640625e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "fastText", "normalizedForm": "fastText", "offsetStart": 31, "offsetEnd": 39}, "context": "For example, Facebook provided fastText model, Google provided several BERT models for different languages.", "mentionContextAttributes": {"used": {"value": false, "score": 0.13569539785385132}, "created": {"value": false, "score": 0.00048553943634033203}, "shared": {"value": false, "score": 5.960464477539062e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9998316764831543}, "created": {"value": true, "score": 0.9998674392700195}, "shared": {"value": false, "score": 9.5367431640625e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "fastText", "normalizedForm": "fastText", "offsetStart": 35, "offsetEnd": 43}, "context": "fastText model: We use pre-trained fastText embedding model and apply this model to generate one embedding for each word of a given comment.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9616010785102844}, "created": {"value": false, "score": 0.006826341152191162}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9998316764831543}, "created": {"value": true, "score": 0.9998674392700195}, "shared": {"value": false, "score": 9.5367431640625e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "fastText", "normalizedForm": "fastText", "offsetStart": 35, "offsetEnd": 43}, "context": "From table 2, we observe that both fastText and BERT embeddings provide nearly the same results.", "mentionContextAttributes": {"used": {"value": false, "score": 0.009542226791381836}, "created": {"value": false, "score": 0.00018554925918579102}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9998316764831543}, "created": {"value": true, "score": 0.9998674392700195}, "shared": {"value": false, "score": 9.5367431640625e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "fastText", "normalizedForm": "fastText", "offsetStart": 36, "offsetEnd": 44}, "context": "The obtained embeddings from either fastText or BERT models are then used as input to the SVM and DNN classifiers.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9998316764831543}, "created": {"value": false, "score": 1.3113021850585938e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9998316764831543}, "created": {"value": true, "score": 0.9998674392700195}, "shared": {"value": false, "score": 9.5367431640625e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "fastText", "normalizedForm": "fastText", "offsetStart": 37, "offsetEnd": 45}, "context": "As a word embedding, we investigated fastText embedding and the BERT embedding.", "mentionContextAttributes": {"used": {"value": true, "score": 0.99781334400177}, "created": {"value": false, "score": 0.39391273260116577}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9998316764831543}, "created": {"value": true, "score": 0.9998674392700195}, "shared": {"value": false, "score": 9.5367431640625e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "fastText", "normalizedForm": "fastText", "offsetStart": 48, "offsetEnd": 56}, "context": "Thanks to the bag of character n-grams model of fastText, every word in a given comment will have an embedding, even rare words.", "mentionContextAttributes": {"used": {"value": false, "score": 9.906291961669922e-05}, "created": {"value": false, "score": 9.298324584960938e-05}, "shared": {"value": false, "score": 9.5367431640625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9998316764831543}, "created": {"value": true, "score": 0.9998674392700195}, "shared": {"value": false, "score": 9.5367431640625e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "fastText", "normalizedForm": "fastText", "offsetStart": 50, "offsetEnd": 58}, "context": "For feature-based approaches, we used pre-trained fastText and BERT models to obtain the sequence of embeddings for a given comment.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9992179870605469}, "created": {"value": false, "score": 0.005617380142211914}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9998316764831543}, "created": {"value": true, "score": 0.9998674392700195}, "shared": {"value": false, "score": 9.5367431640625e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "fastText", "normalizedForm": "fastText", "offsetStart": 55, "offsetEnd": 63}, "context": "The contributions of our paper are as follow: \u2022 We use fastText embeddings and BERT embeddings as input features to SVM, CNN, Bi-LSTM and CRNN classifiers.", "mentionContextAttributes": {"used": {"value": false, "score": 0.22151708602905273}, "created": {"value": true, "score": 0.5513567924499512}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9998316764831543}, "created": {"value": true, "score": 0.9998674392700195}, "shared": {"value": false, "score": 9.5367431640625e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "fastText", "normalizedForm": "fastText", "offsetStart": 64, "offsetEnd": 72}, "context": "Thus, it is possible to have embeddings for rare words, like in fastText.", "mentionContextAttributes": {"used": {"value": false, "score": 6.42538070678711e-05}, "created": {"value": false, "score": 0.0002785325050354004}, "shared": {"value": false, "score": 5.960464477539062e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9998316764831543}, "created": {"value": true, "score": 0.9998674392700195}, "shared": {"value": false, "score": 9.5367431640625e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "CrowdFlower", "normalizedForm": "CrowdFlower", "offsetStart": 64, "offsetEnd": 75}, "context": "The data set contains 24883 tweets and annotations performed by CrowdFlower. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.7826975584030151}, "created": {"value": false, "score": 8.58306884765625e-06}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.7826975584030151}, "created": {"value": false, "score": 8.58306884765625e-06}, "shared": {"value": false, "score": 2.384185791015625e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "fastText", "normalizedForm": "fastText", "offsetStart": 77, "offsetEnd": 85}, "context": "Table 3 shows the confusion matrix for the classification using Bi-LSTM with fastText and BERT embeddings.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9978243112564087}, "created": {"value": false, "score": 5.4836273193359375e-06}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9998316764831543}, "created": {"value": true, "score": 0.9998674392700195}, "shared": {"value": false, "score": 9.5367431640625e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "fastText", "normalizedForm": "fastText", "offsetStart": 132, "offsetEnd": 140}, "context": "In this article, we propose a multiclass classification approach for hate speech detection using two powerful word representations: fastText and BERT embeddings.", "mentionContextAttributes": {"used": {"value": false, "score": 7.164478302001953e-05}, "created": {"value": true, "score": 0.9998674392700195}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9998316764831543}, "created": {"value": true, "score": 0.9998674392700195}, "shared": {"value": false, "score": 9.5367431640625e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "fastText", "normalizedForm": "fastText", "offsetStart": 137, "offsetEnd": 145}, "context": "First, each comment is represented as a sequence of words or word-pieces and for each word or word-piece, an embedding is computed using fastText or BERT.", "mentionContextAttributes": {"used": {"value": true, "score": 0.992949366569519}, "created": {"value": false, "score": 1.800060272216797e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9998316764831543}, "created": {"value": true, "score": 0.9998674392700195}, "shared": {"value": false, "score": 9.5367431640625e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "fastText", "normalizedForm": "fastText", "offsetStart": 160, "offsetEnd": 168}, "context": "Table 2 gives the macro-average F1 and the weighted-average F1 results for the multiclass classification task using SVM, CNN, Bi-LSTM and CRNN classifiers with fastText and BERT embeddings as input features.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9451362490653992}, "created": {"value": false, "score": 1.1205673217773438e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9998316764831543}, "created": {"value": true, "score": 0.9998674392700195}, "shared": {"value": false, "score": 9.5367431640625e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "fastText", "normalizedForm": "fastText", "offsetStart": 165, "offsetEnd": 173}, "context": "Unlike Mikolov's embeddings, fastText is able to provide an embedding for misspelled word, rare words or words that were not present in the training corpus, because fastText uses character n-gram word tokenization.", "mentionContextAttributes": {"used": {"value": false, "score": 0.0006704330444335938}, "created": {"value": false, "score": 0.00021535158157348633}, "shared": {"value": false, "score": 4.76837158203125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9998316764831543}, "created": {"value": true, "score": 0.9998674392700195}, "shared": {"value": false, "score": 9.5367431640625e-07}}}], "references": [], "runtime": 13708, "id": "db69c36973feb36de24b0d1e926e429de5d8d541", "metadata": {"id": "db69c36973feb36de24b0d1e926e429de5d8d541"}, "original_file_path": "../../datalake/Samuel/SOFTware-Sync/downloads/xml/hal-03101938.grobid.tei.xml", "file_name": "hal-03101938.grobid.tei.xml"}