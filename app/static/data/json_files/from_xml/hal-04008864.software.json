{"application": "software-mentions", "version": "0.8.0", "date": "2024-10-07T11:50+0000", "md5": "2FEE8DD563CFD879B3BDE1BFF1321922", "mentions": [{"type": "software", "software-type": "software", "software-name": {"rawForm": "PyTorch", "normalizedForm": "PyTorch", "offsetStart": 7, "offsetEnd": 14}, "context": "In the PyTorch implementation of the feature reg loss, the authors use: g_feat_reg = torch.mean(torch.pow(torch.mean(D_real_features, ", "mentionContextAttributes": {"used": {"value": true, "score": 0.7772639393806458}, "created": {"value": false, "score": 5.0067901611328125e-06}, "shared": {"value": false, "score": 1.0728836059570312e-06}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9981770515441895}, "created": {"value": true, "score": 0.6874710321426392}, "shared": {"value": false, "score": 1.0728836059570312e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "GAN-BERT", "normalizedForm": "GAN-BERT", "offsetStart": 29, "offsetEnd": 37}, "context": "For our experiments, we used GAN-BERT's latest PyTorch implementation, which is compatible with the transformer package. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9981770515441895}, "created": {"value": true, "score": 0.6874710321426392}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9981770515441895}, "created": {"value": true, "score": 0.6874710321426392}, "shared": {"value": false, "score": 2.384185791015625e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "CamemBERT", "normalizedForm": "CamemBERT", "offsetStart": 29, "offsetEnd": 38}, "context": "ChouBERT takes a pre-trained CamemBERT (Martin et al., 2020) model and further pre-trains it on a plant health domain corpus in French to improve the generalizability of plant health hazards detection on Twitter.", "mentionContextAttributes": {"used": {"value": false, "score": 0.0004788637161254883}, "created": {"value": false, "score": 0.000102996826171875}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999866485595703}, "created": {"value": false, "score": 0.005912899971008301}, "shared": {"value": false, "score": 1.0728836059570312e-06}}, "references": [{"label": "(Martin et al., 2020)", "normalizedForm": "Martin et al., 2020", "refKey": 15, "offsetStart": 4852, "offsetEnd": 4873}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "CamemBERT", "normalizedForm": "CamemBERT", "offsetStart": 36, "offsetEnd": 45}, "context": "Among the French varieties of BERT, CamemBERT (Martin et al., 2020) is a model based on the same architecture as BERT but trained on a French corpus with MLM only.", "mentionContextAttributes": {"used": {"value": false, "score": 0.00019341707229614258}, "created": {"value": false, "score": 0.00035762786865234375}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999866485595703}, "created": {"value": false, "score": 0.005912899971008301}, "shared": {"value": false, "score": 1.0728836059570312e-06}}, "references": [{"label": "(Martin et al., 2020)", "normalizedForm": "Martin et al., 2020", "refKey": 15, "offsetStart": 7016, "offsetEnd": 7037}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "PyTorch", "normalizedForm": "PyTorch", "offsetStart": 47, "offsetEnd": 54}, "context": "For our experiments, we used GAN-BERT's latest PyTorch implementation, which is compatible with the transformer package. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9981770515441895}, "created": {"value": true, "score": 0.6874710321426392}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9981770515441895}, "created": {"value": true, "score": 0.6874710321426392}, "shared": {"value": false, "score": 1.0728836059570312e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "CamemBERT", "normalizedForm": "CamemBERT", "offsetStart": 49, "offsetEnd": 58}, "context": "To make our state-of-the-art model, we fine-tune CamemBERT, ChouBERT-16, and ChouBERT-32 for the sequence classification task over the same training/validation/test sets by adding a linear regression layer a to the final hidden state h of the [CLS] token to predict the probability of a label o:", "mentionContextAttributes": {"used": {"value": true, "score": 0.9942042231559753}, "created": {"value": false, "score": 5.0067901611328125e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999866485595703}, "created": {"value": false, "score": 0.005912899971008301}, "shared": {"value": false, "score": 1.0728836059570312e-06}}, "references": [{"label": "(Martin et al., 2020)", "normalizedForm": "Martin et al., 2020", "refKey": 15}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "CamemBERT", "normalizedForm": "CamemBERT", "offsetStart": 50, "offsetEnd": 59}, "context": "ChouBERT (Jiang et al., 2022) takes a pre-trained CamemBERT-base checkpoint and further pretrains it with MLM over a corpus in French in the plant health domain to improve performance in detecting plant health issues from short texts, particularly, from Twitter.", "mentionContextAttributes": {"used": {"value": false, "score": 0.0016708970069885254}, "created": {"value": false, "score": 1.0848045349121094e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999866485595703}, "created": {"value": false, "score": 0.005912899971008301}, "shared": {"value": false, "score": 1.0728836059570312e-06}}, "references": [{"label": "(Martin et al., 2020)", "normalizedForm": "Martin et al., 2020", "refKey": 15}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "CamemBERT", "normalizedForm": "CamemBERT", "offsetStart": 51, "offsetEnd": 60}, "context": "With the optimizations mentioned above, L sup with CamemBERT decreases at a steady pace and \"troubled decrease\" happens less often with ChouBERT models.", "mentionContextAttributes": {"used": {"value": false, "score": 0.026160478591918945}, "created": {"value": false, "score": 2.7418136596679688e-06}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999866485595703}, "created": {"value": false, "score": 0.005912899971008301}, "shared": {"value": false, "score": 1.0728836059570312e-06}}, "references": [{"label": "(Martin et al., 2020)", "normalizedForm": "Martin et al., 2020", "refKey": 15}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "CamemBERT", "normalizedForm": "CamemBERT", "offsetStart": 55, "offsetEnd": 64}, "context": "Thus, in this study, we combine GAN-BERT settings with CamemBERT, ChouBERT-16, and ChouBERT-32.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9996258020401001}, "created": {"value": false, "score": 0.005912899971008301}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999866485595703}, "created": {"value": false, "score": 0.005912899971008301}, "shared": {"value": false, "score": 1.0728836059570312e-06}}, "references": [{"label": "(Martin et al., 2020)", "normalizedForm": "Martin et al., 2020", "refKey": 15}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "CamemBERT", "normalizedForm": "CamemBERT", "offsetStart": 62, "offsetEnd": 71}, "context": "Thus, ChouBERT models produce more homogeneous encodings than CamemBERT.", "mentionContextAttributes": {"used": {"value": false, "score": 0.0001774430274963379}, "created": {"value": false, "score": 8.58306884765625e-06}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999866485595703}, "created": {"value": false, "score": 0.005912899971008301}, "shared": {"value": false, "score": 1.0728836059570312e-06}}, "references": [{"label": "(Martin et al., 2020)", "normalizedForm": "Martin et al., 2020", "refKey": 15}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "CamemBERT", "normalizedForm": "CamemBERT", "offsetStart": 78, "offsetEnd": 87}, "context": "The training with ChouBERT models has more difficulties to converge than with CamemBERT.", "mentionContextAttributes": {"used": {"value": false, "score": 0.0015543103218078613}, "created": {"value": false, "score": 2.5033950805664062e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999866485595703}, "created": {"value": false, "score": 0.005912899971008301}, "shared": {"value": false, "score": 1.0728836059570312e-06}}, "references": [{"label": "(Martin et al., 2020)", "normalizedForm": "Martin et al., 2020", "refKey": 15}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "CamemBERT", "normalizedForm": "CamemBERT", "offsetStart": 80, "offsetEnd": 89}, "context": "Compared with ChouBERT-16, the L sup had more difficulties decreasing than with CamemBERT.", "mentionContextAttributes": {"used": {"value": true, "score": 0.999496579170227}, "created": {"value": false, "score": 6.4373016357421875e-06}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999866485595703}, "created": {"value": false, "score": 0.005912899971008301}, "shared": {"value": false, "score": 1.0728836059570312e-06}}, "references": [{"label": "(Martin et al., 2020)", "normalizedForm": "Martin et al., 2020", "refKey": 15}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "CamemBERT", "normalizedForm": "CamemBERT", "offsetStart": 82, "offsetEnd": 91}, "context": "In this study, we address these shortcomings by applying the GAN-BERT settings to CamemBERT (Martin et al., 2020), ChouBERT-16, and ChouBERT-32 and probing the different losses over varying labeled and unlabeled data sizes to give more insights into when and how to train GAN-BERT for domain-specific document classification.", "mentionContextAttributes": {"used": {"value": false, "score": 0.07416301965713501}, "created": {"value": false, "score": 0.002103567123413086}, "shared": {"value": false, "score": 4.76837158203125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999866485595703}, "created": {"value": false, "score": 0.005912899971008301}, "shared": {"value": false, "score": 1.0728836059570312e-06}}, "references": [{"label": "(Martin et al., 2020)", "normalizedForm": "Martin et al., 2020", "refKey": 15, "offsetStart": 14854, "offsetEnd": 14875}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "CamemBERT", "normalizedForm": "CamemBERT", "offsetStart": 88, "offsetEnd": 97}, "context": "Following the study of Jiang et al. (2022), the ChouBERT models are further-pre-trained CamemBERT-base models over French Plant Health Bulletins and Tweets and the ChouBERT pre-trained for 16 epochs (denoted as ChouBERT-16) and for 32 epochs (denoted as ChouBERT-32) are the most efficient in finding observations about plant health issues.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9969882369041443}, "created": {"value": false, "score": 1.3947486877441406e-05}, "shared": {"value": false, "score": 4.76837158203125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999866485595703}, "created": {"value": false, "score": 0.005912899971008301}, "shared": {"value": false, "score": 1.0728836059570312e-06}}, "references": [{"label": "(Martin et al., 2020)", "normalizedForm": "Martin et al., 2020", "refKey": 15}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "CamemBERT", "normalizedForm": "CamemBERT", "offsetStart": 95, "offsetEnd": 104}, "context": "In both figures, we can see that the deep blue lines (ChouBERT-32) are above the yellow lines (CamemBERT), which is clearly coherent with the results presented in Jiang et al. (2022), indicating that pre-training helps to improve generalizability.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9902655482292175}, "created": {"value": false, "score": 4.184246063232422e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999866485595703}, "created": {"value": false, "score": 0.005912899971008301}, "shared": {"value": false, "score": 1.0728836059570312e-06}}, "references": [{"label": "(Martin et al., 2020)", "normalizedForm": "Martin et al., 2020", "refKey": 15}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "CamemBERT", "normalizedForm": "CamemBERT", "offsetStart": 95, "offsetEnd": 104}, "context": "Thus, the training of GAN plus ChouBERT needs lower learning rates to converge, while GAN plus CamemBERT is a robust approach to converge in most configurations.", "mentionContextAttributes": {"used": {"value": false, "score": 8.535385131835938e-05}, "created": {"value": false, "score": 2.0623207092285156e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999866485595703}, "created": {"value": false, "score": 0.005912899971008301}, "shared": {"value": false, "score": 1.0728836059570312e-06}}, "references": [{"label": "(Martin et al., 2020)", "normalizedForm": "Martin et al., 2020", "refKey": 15}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "CamemBERT", "normalizedForm": "CamemBERT", "offsetStart": 120, "offsetEnd": 129}, "context": "When we examined the embeddings of [CLS] produced by the PLM, we found that there is more variance in each dimension of CamemBERT embeddings than in each dimension of ChouBERT embeddings, before and after the fine-tuning: Var CamemBERT > Var ChouBERT-32 > Var ChouBERT-16 .", "mentionContextAttributes": {"used": {"value": true, "score": 0.9999866485595703}, "created": {"value": false, "score": 2.6226043701171875e-06}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999866485595703}, "created": {"value": false, "score": 0.005912899971008301}, "shared": {"value": false, "score": 1.0728836059570312e-06}}, "references": [{"label": "(Martin et al., 2020)", "normalizedForm": "Martin et al., 2020", "refKey": 15}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "CamemBERT", "normalizedForm": "CamemBERT", "offsetStart": 123, "offsetEnd": 132}, "context": "It is also remarkable that, in the group with 64 training examples (see Figure 7), ChouBERT-16 gives better F1 scores than CamemBERT in the early stages.", "mentionContextAttributes": {"used": {"value": false, "score": 0.0029183030128479004}, "created": {"value": false, "score": 2.467632293701172e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999866485595703}, "created": {"value": false, "score": 0.005912899971008301}, "shared": {"value": false, "score": 1.0728836059570312e-06}}, "references": [{"label": "(Martin et al., 2020)", "normalizedForm": "Martin et al., 2020", "refKey": 15}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "CamemBERT", "normalizedForm": "CamemBERT", "offsetStart": 126, "offsetEnd": 135}, "context": "We also demonstrate that training such a combination may also suffer from extra instabilities compared to using GAN-BERT with CamemBERT, a general PLM.", "mentionContextAttributes": {"used": {"value": false, "score": 0.2535003423690796}, "created": {"value": false, "score": 2.4557113647460938e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999866485595703}, "created": {"value": false, "score": 0.005912899971008301}, "shared": {"value": false, "score": 1.0728836059570312e-06}}, "references": [{"label": "(Martin et al., 2020)", "normalizedForm": "Martin et al., 2020", "refKey": 15}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "CamemBERT", "normalizedForm": "CamemBERT", "offsetStart": 226, "offsetEnd": 235}, "context": "When we examined the embeddings of [CLS] produced by the PLM, we found that there is more variance in each dimension of CamemBERT embeddings than in each dimension of ChouBERT embeddings, before and after the fine-tuning: Var CamemBERT > Var ChouBERT-32 > Var ChouBERT-16 .", "mentionContextAttributes": {"used": {"value": true, "score": 0.9999866485595703}, "created": {"value": false, "score": 2.6226043701171875e-06}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999866485595703}, "created": {"value": false, "score": 0.005912899971008301}, "shared": {"value": false, "score": 1.0728836059570312e-06}}, "references": [{"label": "(Martin et al., 2020)", "normalizedForm": "Martin et al., 2020", "refKey": 15}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "CamemBERT", "normalizedForm": "CamemBERT", "offsetStart": 230, "offsetEnd": 239}, "context": "Thus, we explored the evolution of different losses and the classifiers' performance metrics on the validation and test sets in Figures 6,7, where the discriminators' losses with ChouBERT-16 take more epochs to decrease than with CamemBERT.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9998836517333984}, "created": {"value": false, "score": 0.00025576353073120117}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999866485595703}, "created": {"value": false, "score": 0.005912899971008301}, "shared": {"value": false, "score": 1.0728836059570312e-06}}, "references": [{"label": "(Martin et al., 2020)", "normalizedForm": "Martin et al., 2020", "refKey": 15}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "CamemBERT", "normalizedForm": "CamemBERT", "offsetStart": 299, "offsetEnd": 308}, "context": "According to the authors of SSGAN (Salimans et al., 2016), \"in practice, L unsup will only help if it is not trivial to minimize for our classifier and we thus need to train G to approximate the data distribution, \" which explains that, while the L unsup of D and G converge at the same rhythm with CamemBERT and with ChouBERT-16, the troubled decrease of L sup with ChouBERT-16 renders worse F1 scores than those with CamemBERT. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.8436009883880615}, "created": {"value": false, "score": 2.0265579223632812e-06}, "shared": {"value": false, "score": 1.0728836059570312e-06}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999866485595703}, "created": {"value": false, "score": 0.005912899971008301}, "shared": {"value": false, "score": 1.0728836059570312e-06}}, "references": [{"label": "(Martin et al., 2020)", "normalizedForm": "Martin et al., 2020", "refKey": 15}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "CamemBERT", "normalizedForm": "CamemBERT", "offsetStart": 419, "offsetEnd": 428}, "context": "According to the authors of SSGAN (Salimans et al., 2016), \"in practice, L unsup will only help if it is not trivial to minimize for our classifier and we thus need to train G to approximate the data distribution, \" which explains that, while the L unsup of D and G converge at the same rhythm with CamemBERT and with ChouBERT-16, the troubled decrease of L sup with ChouBERT-16 renders worse F1 scores than those with CamemBERT. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.8436009883880615}, "created": {"value": false, "score": 2.0265579223632812e-06}, "shared": {"value": false, "score": 1.0728836059570312e-06}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999866485595703}, "created": {"value": false, "score": 0.005912899971008301}, "shared": {"value": false, "score": 1.0728836059570312e-06}}, "references": [{"label": "(Martin et al., 2020)", "normalizedForm": "Martin et al., 2020", "refKey": 15}]}], "references": [{"refKey": 15, "tei": "<biblStruct xml:id=\"b15\">\n\t<analytic>\n\t\t<title level=\"a\" type=\"main\">CamemBERT: a Tasty French Language Model</title>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Louis</forename><surname>Martin</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Benjamin</forename><surname>Muller</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Pedro</forename><forename type=\"middle\">Javier</forename><surname>Ortiz Su\u00e1rez</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Yoann</forename><surname>Dupont</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Laurent</forename><surname>Romary</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">\u00c9ric</forename><surname>De La Clergerie</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Djam\u00e9</forename><surname>Seddah</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Beno\u00eet</forename><surname>Sagot</surname></persName>\n\t\t</author>\n\t\t<idno type=\"DOI\">10.18653/v1/2020.acl-main.645</idno>\n\t</analytic>\n\t<monogr>\n\t\t<title level=\"m\">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>\n\t\t<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>\n\t\t<imprint>\n\t\t\t<publisher>Association for Computational Linguistics</publisher>\n\t\t\t<date type=\"published\" when=\"2020\">2020</date>\n\t\t\t<biblScope unit=\"page\">7219</biblScope>\n\t\t</imprint>\n\t</monogr>\n</biblStruct>\n"}], "runtime": 10009, "id": "b5c040a02878b50ca927a03a75506a9b2095c10c", "metadata": {"id": "b5c040a02878b50ca927a03a75506a9b2095c10c"}, "original_file_path": "../../datalake/Samuel/SOFTware-Sync/downloads/xml/hal-04008864.grobid.tei.xml", "file_name": "hal-04008864.grobid.tei.xml"}