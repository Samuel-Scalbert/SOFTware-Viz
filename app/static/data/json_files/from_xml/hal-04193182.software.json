{"application": "software-mentions", "version": "0.8.0", "date": "2024-10-07T11:42+0000", "md5": "D5DD3582EBD7F9E05C53761DA7F8E124", "mentions": [{"type": "software", "software-type": "software", "software-name": {"rawForm": "AWESOME", "normalizedForm": "AWESOME", "offsetStart": 1, "offsetEnd": 8}, "context": "\"AWESOME ft\" is awesome-align with the raw mBERT model fine-tuned on the clinical parallel data only and \"AWESOME pt+ft\" is the pretrained model, fine-tuned again on the clinical parallel data.", "mentionContextAttributes": {"used": {"value": false, "score": 0.4179432988166809}, "created": {"value": false, "score": 6.4373016357421875e-06}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": false, "score": 0.41794490814208984}, "created": {"value": false, "score": 6.4373016357421875e-06}, "shared": {"value": false, "score": 2.384185791015625e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "translate", "normalizedForm": "translate", "offsetStart": 2, "offsetEnd": 11}, "context": "\u2022 translate-train with language-and domain-specific models: Table 23 and24; \u2022 translate-test with multilingual language models: Table 25 and26; \u2022 translate-test with PubmedBERT: Table 27 and 28;", "mentionContextAttributes": {"used": {"value": true, "score": 0.9504609704017639}, "created": {"value": false, "score": 3.814697265625e-06}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999926090240479}, "created": {"value": false, "score": 0.002916872501373291}, "shared": {"value": false, "score": 1.3113021850585938e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "translate", "normalizedForm": "translate", "offsetStart": 4, "offsetEnd": 13}, "context": "The translate-train approach consists in constructing a translated version of the n2c2 dataset and in training a NER algorithm in French or German on the translated dataset.", "mentionContextAttributes": {"used": {"value": false, "score": 0.0003795623779296875}, "created": {"value": false, "score": 0.0002009272575378418}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999926090240479}, "created": {"value": false, "score": 0.002916872501373291}, "shared": {"value": false, "score": 1.3113021850585938e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "translate", "normalizedForm": "translate", "offsetStart": 4, "offsetEnd": 13}, "context": "The translate-test method consists in translating the data into English at inference time and in applying an English NER model on it.", "mentionContextAttributes": {"used": {"value": false, "score": 0.004237234592437744}, "created": {"value": false, "score": 4.0531158447265625e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999926090240479}, "created": {"value": false, "score": 0.002916872501373291}, "shared": {"value": false, "score": 1.3113021850585938e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "translate", "normalizedForm": "translate", "offsetStart": 4, "offsetEnd": 13}, "context": "The translate-test method is consistently outperformed by translate-train for large models.", "mentionContextAttributes": {"used": {"value": false, "score": 0.0006992816925048828}, "created": {"value": false, "score": 7.236003875732422e-05}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999926090240479}, "created": {"value": false, "score": 0.002916872501373291}, "shared": {"value": false, "score": 1.3113021850585938e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "translate", "normalizedForm": "translate", "offsetStart": 4, "offsetEnd": 13}, "context": "The translate-train approach allows to rely on models that are specific to the language and domain of the target evaluation.", "mentionContextAttributes": {"used": {"value": false, "score": 3.1828880310058594e-05}, "created": {"value": false, "score": 0.002052485942840576}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999926090240479}, "created": {"value": false, "score": 0.002916872501373291}, "shared": {"value": false, "score": 1.3113021850585938e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "translate", "normalizedForm": "translate", "offsetStart": 4, "offsetEnd": 13}, "context": "The translate-test approach should then be used only when large multilingual models cannot be used.", "mentionContextAttributes": {"used": {"value": false, "score": 0.0007440447807312012}, "created": {"value": false, "score": 1.7762184143066406e-05}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999926090240479}, "created": {"value": false, "score": 0.002916872501373291}, "shared": {"value": false, "score": 1.3113021850585938e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "translate", "normalizedForm": "translate", "offsetStart": 4, "offsetEnd": 13}, "context": "The translate-test method needs translation algorithms from German and French to English.", "mentionContextAttributes": {"used": {"value": false, "score": 0.00042301416397094727}, "created": {"value": false, "score": 4.5299530029296875e-05}, "shared": {"value": false, "score": 1.3113021850585938e-06}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999926090240479}, "created": {"value": false, "score": 0.002916872501373291}, "shared": {"value": false, "score": 1.3113021850585938e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "translate", "normalizedForm": "translate", "offsetStart": 6, "offsetEnd": 15}, "context": "Since translate-train can leverage monolingual domain-specific model, we evaluate the outcome of such a strategy. ", "mentionContextAttributes": {"used": {"value": false, "score": 8.153915405273438e-05}, "created": {"value": false, "score": 0.002916872501373291}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999926090240479}, "created": {"value": false, "score": 0.002916872501373291}, "shared": {"value": false, "score": 1.3113021850585938e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "translate", "normalizedForm": "translate", "offsetStart": 10, "offsetEnd": 19}, "context": "\u2022 CLT and translate-train with multilingual models: Table 21 (fr) and 22 (de);", "mentionContextAttributes": {"used": {"value": false, "score": 0.014195144176483154}, "created": {"value": false, "score": 3.6954879760742188e-06}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999926090240479}, "created": {"value": false, "score": 0.002916872501373291}, "shared": {"value": false, "score": 1.3113021850585938e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "translate", "normalizedForm": "translate", "offsetStart": 14, "offsetEnd": 23}, "context": "Table6for the translate-train and translate-test methods.", "mentionContextAttributes": {"used": {"value": true, "score": 0.7246471643447876}, "created": {"value": false, "score": 5.602836608886719e-06}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999926090240479}, "created": {"value": false, "score": 0.002916872501373291}, "shared": {"value": false, "score": 1.3113021850585938e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "translate", "normalizedForm": "translate", "offsetStart": 14, "offsetEnd": 23}, "context": "Table6for the translate-train and translate-test methods.", "mentionContextAttributes": {"used": {"value": true, "score": 0.7246471643447876}, "created": {"value": false, "score": 5.602836608886719e-06}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999926090240479}, "created": {"value": false, "score": 0.002916872501373291}, "shared": {"value": false, "score": 1.3113021850585938e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "translate", "normalizedForm": "translate", "offsetStart": 14, "offsetEnd": 23}, "context": "Table6for the translate-train and translate-test methods.", "mentionContextAttributes": {"used": {"value": true, "score": 0.7246471643447876}, "created": {"value": false, "score": 5.602836608886719e-06}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999926090240479}, "created": {"value": false, "score": 0.002916872501373291}, "shared": {"value": false, "score": 1.3113021850585938e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Pytorch", "normalizedForm": "Pytorch", "offsetStart": 27, "offsetEnd": 34}, "context": "All models were written in Pytorch using the Huggingface librairies (Wolf et al., 2020) and were finetuned using the AdamW optimizer (Loshchilov and Hutter, 2019) and a learning rate of 6 \u2022 10 -6 with linear decay. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9998207688331604}, "created": {"value": false, "score": 6.198883056640625e-06}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9998207688331604}, "created": {"value": false, "score": 6.198883056640625e-06}, "shared": {"value": false, "score": 2.384185791015625e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "AWE-SOME", "normalizedForm": "AWE-SOME", "offsetStart": 34, "offsetEnd": 42}, "context": "Throughout the paper, in tables, \"AWE-SOME\" designates this latter pre-trained version. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.06640374660491943}, "created": {"value": false, "score": 0.005498170852661133}, "shared": {"value": false, "score": 5.960464477539062e-07}}, "documentContextAttributes": {"used": {"value": false, "score": 0.06640374660491943}, "created": {"value": false, "score": 0.005498170852661133}, "shared": {"value": false, "score": 5.960464477539062e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "translate", "normalizedForm": "translate", "offsetStart": 34, "offsetEnd": 43}, "context": "Table6for the translate-train and translate-test methods.", "mentionContextAttributes": {"used": {"value": true, "score": 0.7246471643447876}, "created": {"value": false, "score": 5.602836608886719e-06}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999926090240479}, "created": {"value": false, "score": 0.002916872501373291}, "shared": {"value": false, "score": 1.3113021850585938e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "translate", "normalizedForm": "translate", "offsetStart": 34, "offsetEnd": 43}, "context": "Table6for the translate-train and translate-test methods.", "mentionContextAttributes": {"used": {"value": true, "score": 0.7246471643447876}, "created": {"value": false, "score": 5.602836608886719e-06}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999926090240479}, "created": {"value": false, "score": 0.002916872501373291}, "shared": {"value": false, "score": 1.3113021850585938e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "translate", "normalizedForm": "translate", "offsetStart": 34, "offsetEnd": 43}, "context": "Table6for the translate-train and translate-test methods.", "mentionContextAttributes": {"used": {"value": true, "score": 0.7246471643447876}, "created": {"value": false, "score": 5.602836608886719e-06}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999926090240479}, "created": {"value": false, "score": 0.002916872501373291}, "shared": {"value": false, "score": 1.3113021850585938e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "translate", "normalizedForm": "translate", "offsetStart": 35, "offsetEnd": 44}, "context": "However, once training is done the translate-train method has the same inference time as the CLT method, while the translate-test method now suffers from the need of translation at inference time.", "mentionContextAttributes": {"used": {"value": false, "score": 0.0001385211944580078}, "created": {"value": false, "score": 3.898143768310547e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999926090240479}, "created": {"value": false, "score": 0.002916872501373291}, "shared": {"value": false, "score": 1.3113021850585938e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "translate", "normalizedForm": "translate", "offsetStart": 40, "offsetEnd": 49}, "context": "Finally, it must be noted that the best translate-train model in French, DrBERT Pubmed, is actually pre-trained on the English PubMed dataset and then on French clinical texts, which suggests that multilingual models should be preferred, even with a translation-based cross-lingual adaptation.", "mentionContextAttributes": {"used": {"value": false, "score": 0.0006582140922546387}, "created": {"value": false, "score": 7.796287536621094e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999926090240479}, "created": {"value": false, "score": 0.002916872501373291}, "shared": {"value": false, "score": 1.3113021850585938e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Huggingface", "normalizedForm": "Huggingface", "offsetStart": 45, "offsetEnd": 56}, "context": "All models were written in Pytorch using the Huggingface librairies (Wolf et al., 2020) and were finetuned using the AdamW optimizer (Loshchilov and Hutter, 2019) and a learning rate of 6 \u2022 10 -6 with linear decay. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9998207688331604}, "created": {"value": false, "score": 6.198883056640625e-06}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9998207688331604}, "created": {"value": false, "score": 6.198883056640625e-06}, "shared": {"value": false, "score": 2.384185791015625e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "translate", "normalizedForm": "translate", "offsetStart": 48, "offsetEnd": 57}, "context": "We report in Table 13 and 14 the results of the translate-train method for the best translation/alignment algorithms pair and for the pre-selected one, compared to using XLM-R Base.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9999618530273438}, "created": {"value": false, "score": 2.3484230041503906e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999926090240479}, "created": {"value": false, "score": 0.002916872501373291}, "shared": {"value": false, "score": 1.3113021850585938e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "GeForce", "normalizedForm": "GeForce", "offsetStart": 52, "offsetEnd": 59}, "context": "The NER models were trained on a single GPU (Nvidia GeForce RTX 2070 with 8GB of RAM) for approximately one hour.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9999779462814331}, "created": {"value": false, "score": 1.1086463928222656e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999779462814331}, "created": {"value": false, "score": 1.1086463928222656e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "translate", "normalizedForm": "translate", "offsetStart": 53, "offsetEnd": 62}, "context": "This comparison shows a longer training time for the translate-train method, which is due to the translation of the whole training set before the training of the NER model.", "mentionContextAttributes": {"used": {"value": true, "score": 0.7830997705459595}, "created": {"value": false, "score": 1.1324882507324219e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999926090240479}, "created": {"value": false, "score": 0.002916872501373291}, "shared": {"value": false, "score": 1.3113021850585938e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "translate", "normalizedForm": "translate", "offsetStart": 56, "offsetEnd": 65}, "context": "The same pre-selection evaluation was performed for the translate-test approach, with translation models from German and French to English.", "mentionContextAttributes": {"used": {"value": true, "score": 0.999991774559021}, "created": {"value": false, "score": 8.106231689453125e-06}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999926090240479}, "created": {"value": false, "score": 0.002916872501373291}, "shared": {"value": false, "score": 1.3113021850585938e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "translate", "normalizedForm": "translate", "offsetStart": 57, "offsetEnd": 66}, "context": "The major drawback of this method is that it requires to translate the text at inference time while the translation in the translate-train method occurs only once during training.", "mentionContextAttributes": {"used": {"value": false, "score": 0.00010156631469726562}, "created": {"value": false, "score": 3.9696693420410156e-05}, "shared": {"value": false, "score": 8.344650268554688e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999926090240479}, "created": {"value": false, "score": 0.002916872501373291}, "shared": {"value": false, "score": 1.3113021850585938e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "translate", "normalizedForm": "translate", "offsetStart": 58, "offsetEnd": 67}, "context": "The translate-test method is consistently outperformed by translate-train for large models. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.0006992816925048828}, "created": {"value": false, "score": 7.236003875732422e-05}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999926090240479}, "created": {"value": false, "score": 0.002916872501373291}, "shared": {"value": false, "score": 1.3113021850585938e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "translate", "normalizedForm": "translate", "offsetStart": 60, "offsetEnd": 69}, "context": "Providing a large-enough MLLM, CLT outperforms pre-selected translate-train and translate-test.", "mentionContextAttributes": {"used": {"value": false, "score": 0.00013768672943115234}, "created": {"value": false, "score": 4.8279762268066406e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999926090240479}, "created": {"value": false, "score": 0.002916872501373291}, "shared": {"value": false, "score": 1.3113021850585938e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "COMET", "normalizedForm": "COMET", "offsetStart": 62, "offsetEnd": 86}, "context": "It has been used for years but new scoring algorithms such as COMET (Rei et al., 2020) which leverage large multilingual models seem to provide more accurate evaluations.", "mentionContextAttributes": {"used": {"value": false, "score": 0.0002891421318054199}, "created": {"value": false, "score": 0.007448077201843262}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999736547470093}, "created": {"value": false, "score": 0.007448077201843262}, "shared": {"value": false, "score": 2.384185791015625e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "translate", "normalizedForm": "translate", "offsetStart": 66, "offsetEnd": 75}, "context": "Even using a specific biomedical model like (Gu et al., 2021) for translate-test does not improve the results (results in Appendix F).", "mentionContextAttributes": {"used": {"value": false, "score": 0.0013034939765930176}, "created": {"value": false, "score": 9.5367431640625e-06}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999926090240479}, "created": {"value": false, "score": 0.002916872501373291}, "shared": {"value": false, "score": 1.3113021850585938e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "translate", "normalizedForm": "translate", "offsetStart": 70, "offsetEnd": 79}, "context": "Indeed translation and alignment errors only harm the training set of translate-train, which does not prevent a large model from generalizing despite some errors in the training data, while errors of translation or alignment in translate-test are directly reflected in the test score.", "mentionContextAttributes": {"used": {"value": false, "score": 0.01842278242111206}, "created": {"value": false, "score": 8.809566497802734e-05}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999926090240479}, "created": {"value": false, "score": 0.002916872501373291}, "shared": {"value": false, "score": 1.3113021850585938e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "translate-train", "normalizedForm": "translate-train", "offsetStart": 72, "offsetEnd": 87}, "context": "In the rest of this analysis we will consequently compare CLT only with translate-train.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9994750618934631}, "created": {"value": false, "score": 1.621246337890625e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9994750618934631}, "created": {"value": false, "score": 1.621246337890625e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "translate", "normalizedForm": "translate", "offsetStart": 73, "offsetEnd": 82}, "context": "While the results show that using a domain-specific monolingual model in translate-train or translate-test is not on par with general-purpose multilingual models, they also show that the French clinical model DrBERT provides the best results for translate-train when it uses the English biomedical model PubmedBERT as initialization.", "mentionContextAttributes": {"used": {"value": false, "score": 0.43821561336517334}, "created": {"value": false, "score": 1.1324882507324219e-05}, "shared": {"value": false, "score": 4.76837158203125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999926090240479}, "created": {"value": false, "score": 0.002916872501373291}, "shared": {"value": false, "score": 1.3113021850585938e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Optical Character Recognition (", "normalizedForm": "Optical Character Recognition (", "offsetStart": 73, "offsetEnd": 104}, "context": "After anonymization of the drug prescriptions we used a state-of-the-art Optical Character Recognition (OCR) software3 . ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9999855756759644}, "created": {"value": false, "score": 0.00016927719116210938}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999855756759644}, "created": {"value": false, "score": 0.00016927719116210938}, "shared": {"value": false, "score": 1.1920928955078125e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "translate", "normalizedForm": "translate", "offsetStart": 78, "offsetEnd": 87}, "context": "\u2022 translate-train with language-and domain-specific models: Table 23 and24; \u2022 translate-test with multilingual language models: Table 25 and26; \u2022 translate-test with PubmedBERT: Table 27 and 28;", "mentionContextAttributes": {"used": {"value": true, "score": 0.950461208820343}, "created": {"value": false, "score": 3.814697265625e-06}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999926090240479}, "created": {"value": false, "score": 0.002916872501373291}, "shared": {"value": false, "score": 1.3113021850585938e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "translate", "normalizedForm": "translate", "offsetStart": 80, "offsetEnd": 89}, "context": "Providing a large-enough MLLM, CLT outperforms pre-selected translate-train and translate-test.", "mentionContextAttributes": {"used": {"value": false, "score": 0.00013768672943115234}, "created": {"value": false, "score": 4.8279762268066406e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999926090240479}, "created": {"value": false, "score": 0.002916872501373291}, "shared": {"value": false, "score": 1.3113021850585938e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "translate", "normalizedForm": "translate", "offsetStart": 90, "offsetEnd": 99}, "context": "The selection of the alignment model was shown to be particularly crucial, and results of translate-train could probably be improved by post-processing the alignment.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9999926090240479}, "created": {"value": false, "score": 0.00010657310485839844}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999926090240479}, "created": {"value": false, "score": 0.002916872501373291}, "shared": {"value": false, "score": 1.3113021850585938e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "translate", "normalizedForm": "translate", "offsetStart": 92, "offsetEnd": 101}, "context": "While the results show that using a domain-specific monolingual model in translate-train or translate-test is not on par with general-purpose multilingual models, they also show that the French clinical model DrBERT provides the best results for translate-train when it uses the English biomedical model PubmedBERT as initialization.", "mentionContextAttributes": {"used": {"value": false, "score": 0.43821561336517334}, "created": {"value": false, "score": 1.1324882507324219e-05}, "shared": {"value": false, "score": 4.76837158203125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999926090240479}, "created": {"value": false, "score": 0.002916872501373291}, "shared": {"value": false, "score": 1.3113021850585938e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "translate", "normalizedForm": "translate", "offsetStart": 93, "offsetEnd": 102}, "context": "With the right translation and alignment model, it seems that CLT can be outperformed by the translate-train method.", "mentionContextAttributes": {"used": {"value": false, "score": 0.00012791156768798828}, "created": {"value": false, "score": 9.441375732421875e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999926090240479}, "created": {"value": false, "score": 0.002916872501373291}, "shared": {"value": false, "score": 1.3113021850585938e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "AWESOME", "normalizedForm": "AWESOME", "offsetStart": 106, "offsetEnd": 113}, "context": "\"AWESOME ft\" is awesome-align with the raw mBERT model fine-tuned on the clinical parallel data only and \"AWESOME pt+ft\" is the pretrained model, fine-tuned again on the clinical parallel data.", "mentionContextAttributes": {"used": {"value": false, "score": 0.41794490814208984}, "created": {"value": false, "score": 6.4373016357421875e-06}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": false, "score": 0.41794490814208984}, "created": {"value": false, "score": 6.4373016357421875e-06}, "shared": {"value": false, "score": 2.384185791015625e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "translate", "normalizedForm": "translate", "offsetStart": 106, "offsetEnd": 115}, "context": "Having selected the best translation and alignment model for each language based on intrinsic evaluation, translate-train and translate-test approaches can be compared to Cross-lingual Transfer (CLT).", "mentionContextAttributes": {"used": {"value": false, "score": 0.37034547328948975}, "created": {"value": false, "score": 5.8531761169433594e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999926090240479}, "created": {"value": false, "score": 0.002916872501373291}, "shared": {"value": false, "score": 1.3113021850585938e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "translate", "normalizedForm": "translate", "offsetStart": 115, "offsetEnd": 124}, "context": "However, once training is done the translate-train method has the same inference time as the CLT method, while the translate-test method now suffers from the need of translation at inference time.", "mentionContextAttributes": {"used": {"value": false, "score": 0.0001385211944580078}, "created": {"value": false, "score": 3.898143768310547e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999926090240479}, "created": {"value": false, "score": 0.002916872501373291}, "shared": {"value": false, "score": 1.3113021850585938e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "AdamW", "normalizedForm": "AdamW", "offsetStart": 117, "offsetEnd": 122}, "context": "All models were written in Pytorch using the Huggingface librairies (Wolf et al., 2020) and were finetuned using the AdamW optimizer (Loshchilov and Hutter, 2019) and a learning rate of 6 \u2022 10 -6 with linear decay. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9998207688331604}, "created": {"value": false, "score": 6.198883056640625e-06}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9998207688331604}, "created": {"value": false, "score": 6.198883056640625e-06}, "shared": {"value": false, "score": 2.384185791015625e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "COMET", "normalizedForm": "COMET", "offsetStart": 117, "offsetEnd": 122}, "context": "Similarly to Section 5.1 we fine-tuned the Opus-MT and FAIR algorithms on the same medical datasets and obtained the COMET and BLEU scores presented in Tables 16 and17.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9999736547470093}, "created": {"value": false, "score": 2.3365020751953125e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999736547470093}, "created": {"value": false, "score": 0.007448077201843262}, "shared": {"value": false, "score": 2.384185791015625e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "XTREME", "normalizedForm": "XTREME", "offsetStart": 117, "offsetEnd": 141}, "context": "The CLT abilities of MLLMs can be assessed on several tasks in many languages thanks to multilingual benchmarks like XTREME (Hu et al., 2020) which cannot be used to evaluate medical NER models since they contain only general-domain tasks. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.0033947229385375977}, "created": {"value": false, "score": 3.838539123535156e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": false, "score": 0.0033947229385375977}, "created": {"value": false, "score": 3.838539123535156e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "translate", "normalizedForm": "translate", "offsetStart": 123, "offsetEnd": 132}, "context": "The major drawback of this method is that it requires to translate the text at inference time while the translation in the translate-train method occurs only once during training.", "mentionContextAttributes": {"used": {"value": false, "score": 0.00010156631469726562}, "created": {"value": false, "score": 3.9696693420410156e-05}, "shared": {"value": false, "score": 8.344650268554688e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999926090240479}, "created": {"value": false, "score": 0.002916872501373291}, "shared": {"value": false, "score": 1.3113021850585938e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "translate", "normalizedForm": "translate", "offsetStart": 126, "offsetEnd": 135}, "context": "Having selected the best translation and alignment model for each language based on intrinsic evaluation, translate-train and translate-test approaches can be compared to Cross-lingual Transfer (CLT).", "mentionContextAttributes": {"used": {"value": false, "score": 0.37034547328948975}, "created": {"value": false, "score": 5.8531761169433594e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999926090240479}, "created": {"value": false, "score": 0.002916872501373291}, "shared": {"value": false, "score": 1.3113021850585938e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "translate", "normalizedForm": "translate", "offsetStart": 142, "offsetEnd": 151}, "context": "To the exception of XLM-R Base in French, there always exists a pair of translation and alignment models that leads to a better score for the translate-train method over CLT.", "mentionContextAttributes": {"used": {"value": false, "score": 0.0002670884132385254}, "created": {"value": false, "score": 1.8715858459472656e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999926090240479}, "created": {"value": false, "score": 0.002916872501373291}, "shared": {"value": false, "score": 1.3113021850585938e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "translate", "normalizedForm": "translate", "offsetStart": 146, "offsetEnd": 155}, "context": "\u2022 translate-train with language-and domain-specific models: Table 23 and24; \u2022 translate-test with multilingual language models: Table 25 and26; \u2022 translate-test with PubmedBERT: Table 27 and 28;", "mentionContextAttributes": {"used": {"value": true, "score": 0.950461208820343}, "created": {"value": false, "score": 3.814697265625e-06}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999926090240479}, "created": {"value": false, "score": 0.002916872501373291}, "shared": {"value": false, "score": 1.3113021850585938e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "translate", "normalizedForm": "translate", "offsetStart": 149, "offsetEnd": 158}, "context": "The takeaway is that, while a small gain in translation accuracy (obtained with further fine-tuning) might not necessarily improve the result of the translate-train approach, a completely different model (like FAIR with respect to Opus) has more chance to improve cross-lingual adaptation.", "mentionContextAttributes": {"used": {"value": false, "score": 8.14199447631836e-05}, "created": {"value": false, "score": 2.7894973754882812e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999926090240479}, "created": {"value": false, "score": 0.002916872501373291}, "shared": {"value": false, "score": 1.3113021850585938e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "PubmedBERT", "normalizedForm": "PubmedBERT", "offsetStart": 166, "offsetEnd": 176}, "context": "\u2022 translate-train with language-and domain-specific models: Table 23 and24; \u2022 translate-test with multilingual language models: Table 25 and26; \u2022 translate-test with PubmedBERT: Table 27 and 28;", "mentionContextAttributes": {"used": {"value": true, "score": 0.950461208820343}, "created": {"value": false, "score": 3.814697265625e-06}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.950461208820343}, "created": {"value": false, "score": 1.1324882507324219e-05}, "shared": {"value": false, "score": 4.76837158203125e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "PubmedBERT", "normalizedForm": "PubmedBERT", "offsetStart": 166, "offsetEnd": 176}, "context": "\u2022 translate-train with language-and domain-specific models: Table 23 and24; \u2022 translate-test with multilingual language models: Table 25 and26; \u2022 translate-test with PubmedBERT: Table 27 and 28;", "mentionContextAttributes": {"used": {"value": true, "score": 0.950461208820343}, "created": {"value": false, "score": 3.814697265625e-06}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.950461208820343}, "created": {"value": false, "score": 1.1324882507324219e-05}, "shared": {"value": false, "score": 4.76837158203125e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "translate", "normalizedForm": "translate", "offsetStart": 204, "offsetEnd": 213}, "context": "12 show that while realignment does not systematically provide improvement over CLT as observed by Wu and Dredze (2020), it does significantly boost results in some cases, allowing to outperform the best translate-train baseline in German for XLM-R Base and in French for XLM-R Large.", "mentionContextAttributes": {"used": {"value": true, "score": 0.5713656544685364}, "created": {"value": false, "score": 1.7762184143066406e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999926090240479}, "created": {"value": false, "score": 0.002916872501373291}, "shared": {"value": false, "score": 1.3113021850585938e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "translate", "normalizedForm": "translate", "offsetStart": 228, "offsetEnd": 237}, "context": "Indeed translation and alignment errors only harm the training set of translate-train, which does not prevent a large model from generalizing despite some errors in the training data, while errors of translation or alignment in translate-test are directly reflected in the test score.", "mentionContextAttributes": {"used": {"value": false, "score": 0.01842278242111206}, "created": {"value": false, "score": 8.809566497802734e-05}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999926090240479}, "created": {"value": false, "score": 0.002916872501373291}, "shared": {"value": false, "score": 1.3113021850585938e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "translate", "normalizedForm": "translate", "offsetStart": 246, "offsetEnd": 255}, "context": "While the results show that using a domain-specific monolingual model in translate-train or translate-test is not on par with general-purpose multilingual models, they also show that the French clinical model DrBERT provides the best results for translate-train when it uses the English biomedical model PubmedBERT as initialization.", "mentionContextAttributes": {"used": {"value": false, "score": 0.43821561336517334}, "created": {"value": false, "score": 1.1324882507324219e-05}, "shared": {"value": false, "score": 4.76837158203125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999926090240479}, "created": {"value": false, "score": 0.002916872501373291}, "shared": {"value": false, "score": 1.3113021850585938e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "COMET", "normalizedForm": "COMET", "offsetStart": 268, "offsetEnd": 273}, "context": "While choosing between different base translation models (like Opus or FAIR) based on their translation scores on in-domain data seems to provide the best results, deciding between the finetuned version of a translation model and the base one by comparing the BLEU or COMET scores on biomedical data does not guarantee the best downstream F1 score as Table 9 shows.", "mentionContextAttributes": {"used": {"value": false, "score": 0.018893122673034668}, "created": {"value": false, "score": 1.7881393432617188e-06}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999736547470093}, "created": {"value": false, "score": 0.007448077201843262}, "shared": {"value": false, "score": 2.384185791015625e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "PubmedBERT", "normalizedForm": "PubmedBERT", "offsetStart": 304, "offsetEnd": 314}, "context": "While the results show that using a domain-specific monolingual model in translate-train or translate-test is not on par with general-purpose multilingual models, they also show that the French clinical model DrBERT provides the best results for translate-train when it uses the English biomedical model PubmedBERT as initialization.", "mentionContextAttributes": {"used": {"value": false, "score": 0.43821561336517334}, "created": {"value": false, "score": 1.1324882507324219e-05}, "shared": {"value": false, "score": 4.76837158203125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.950461208820343}, "created": {"value": false, "score": 1.1324882507324219e-05}, "shared": {"value": false, "score": 4.76837158203125e-07}}}], "references": [], "runtime": 12801, "id": "83dc5f0ab65821e6cd6c7935806c22ac832a5aa5", "metadata": {"id": "83dc5f0ab65821e6cd6c7935806c22ac832a5aa5"}, "original_file_path": "../../datalake/Samuel/SOFTware-Sync/downloads/xml/hal-04193182.grobid.tei.xml", "file_name": "hal-04193182.grobid.tei.xml"}