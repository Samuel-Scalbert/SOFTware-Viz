{"application": "software-mentions", "version": "0.8.0", "date": "2024-10-07T11:46+0000", "md5": "9476301596E913160896C0452301C82E", "mentions": [{"type": "software", "software-type": "software", "software-name": {"rawForm": "XNLI", "normalizedForm": "XNLI", "offsetStart": 0, "offsetEnd": 4}, "context": "XNLI is indeed based on multiNLI which covers a range of genres of spoken and written text.", "mentionContextAttributes": {"used": {"value": false, "score": 0.0014780163764953613}, "created": {"value": false, "score": 0.007296085357666016}, "shared": {"value": false, "score": 4.76837158203125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999645948410034}, "created": {"value": false, "score": 0.07769900560379028}, "shared": {"value": false, "score": 7.152557373046875e-07}}, "references": [{"label": "(Conneau et al., 2018)", "normalizedForm": "Conneau et al., 2018", "refKey": 12}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "UDPipe", "normalizedForm": "UDPipe", "offsetStart": 0, "offsetEnd": 6}, "context": "UDPipe Fu-ture+mBERT +Flair uses the contextualized string embeddings Flair (Akbik et al., 2018), which are in fact pretrained contextualized character-level word embeddings specifically designed to handle misspelled words as well as subword structures such as prefixes and suffixes. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.02225250005722046}, "created": {"value": false, "score": 2.7418136596679688e-06}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9930972456932068}, "created": {"value": false, "score": 0.00012981891632080078}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "references": [{"label": "(Straka, 2018)", "normalizedForm": "Straka, 2018", "refKey": 57}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "RoBERTa", "normalizedForm": "RoBERTa", "offsetStart": 0, "offsetEnd": 7}, "context": "RoBERTa trained on more than 100GB of data).", "mentionContextAttributes": {"used": {"value": true, "score": 0.9995390176773071}, "created": {"value": false, "score": 5.054473876953125e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9995390176773071}, "created": {"value": true, "score": 0.9962459206581116}, "shared": {"value": false, "score": 1.430511474609375e-06}}, "references": [{"label": "(Liu et al., 2019)", "normalizedForm": "Liu et al., 2019", "refKey": 36}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "RoBERTa", "normalizedForm": "RoBERTa", "offsetStart": 0, "offsetEnd": 7}, "context": "RoBERTa improves the original implementation of BERT by identifying key design choices for better performance, using dynamic masking, removing the next sentence prediction task, training with larger batches, on more data, and for longer.", "mentionContextAttributes": {"used": {"value": false, "score": 4.6253204345703125e-05}, "created": {"value": false, "score": 0.0037099123001098633}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9995390176773071}, "created": {"value": true, "score": 0.9962459206581116}, "shared": {"value": false, "score": 1.430511474609375e-06}}, "references": [{"label": "(Liu et al., 2019)", "normalizedForm": "Liu et al., 2019", "refKey": 36}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "fairseq", "normalizedForm": "fairseq", "offsetStart": 0, "offsetEnd": 7}, "context": "fairseq/blob/master/examples/roberta/ README.glue.md.10 UDPipe Future is available at https://github.", "mentionContextAttributes": {"used": {"value": false, "score": 5.245208740234375e-05}, "created": {"value": false, "score": 1.4543533325195312e-05}, "shared": {"value": true, "score": 0.9743472933769226}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9688633680343628}, "created": {"value": true, "score": 0.9155601859092712}, "shared": {"value": true, "score": 0.9743472933769226}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "CamemBERT", "normalizedForm": "CamemBERT", "offsetStart": 0, "offsetEnd": 9}, "publisher": {"rawForm": "Hugging Face", "normalizedForm": "Hugging Face"}, "context": "CamemBERT uses the RoBERTa architecture (Liu et al., 2019), an improved variant of the high-performing and widely used BERT architecture (Devlin et al., 2019).", "mentionContextAttributes": {"used": {"value": false, "score": 0.01318436861038208}, "created": {"value": false, "score": 4.112720489501953e-05}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9995896220207214}, "created": {"value": true, "score": 0.9988622665405273}, "shared": {"value": false, "score": 0.0001812577247619629}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "CamemBERT", "normalizedForm": "CamemBERT", "offsetStart": 0, "offsetEnd": 9}, "publisher": {"rawForm": "Hugging Face", "normalizedForm": "Hugging Face"}, "context": "CamemBERT improves on the state of the art in all four tasks compared to previous monolingual and multilingual approaches including mBERT, XLM and XLM-R, which confirms the effectiveness of large pretrained language models for French.", "mentionContextAttributes": {"used": {"value": false, "score": 4.8995018005371094e-05}, "created": {"value": false, "score": 7.343292236328125e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9995896220207214}, "created": {"value": true, "score": 0.9988622665405273}, "shared": {"value": false, "score": 0.0001812577247619629}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "CamemBERT", "normalizedForm": "CamemBERT", "offsetStart": 0, "offsetEnd": 9}, "publisher": {"rawForm": "Hugging Face", "normalizedForm": "Hugging Face"}, "context": "CamemBERT is very similar to RoBERTa, the main difference being the use of whole-word masking and the usage of SentencePiece tokenization (Kudo and Richardson, 2018) instead of WordPiece (Schuster and Nakajima, 2012).", "mentionContextAttributes": {"used": {"value": false, "score": 0.0007841587066650391}, "created": {"value": false, "score": 1.823902130126953e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9995896220207214}, "created": {"value": true, "score": 0.9988622665405273}, "shared": {"value": false, "score": 0.0001812577247619629}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "CamemBERT", "normalizedForm": "CamemBERT", "offsetStart": 0, "offsetEnd": 9}, "publisher": {"rawForm": "Hugging Face", "normalizedForm": "Hugging Face"}, "context": "CamemBERT (fine-tuned) 89.08 LSTM+CRF+CamemBERT (embeddings) 89.55  (Conneau et al., 2019) 80.1 270M", "mentionContextAttributes": {"used": {"value": true, "score": 0.9992942810058594}, "created": {"value": false, "score": 8.344650268554688e-06}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9995896220207214}, "created": {"value": true, "score": 0.9988622665405273}, "shared": {"value": false, "score": 0.0001812577247619629}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "CamemBERT", "normalizedForm": "CamemBERT", "offsetStart": 0, "offsetEnd": 9}, "publisher": {"rawForm": "Hugging Face", "normalizedForm": "Hugging Face"}, "context": "CamemBERT reaches state-of-the-art scores on all treebanks and metrics in both scenarios. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.0006186962127685547}, "created": {"value": false, "score": 9.298324584960938e-06}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9995896220207214}, "created": {"value": true, "score": 0.9988622665405273}, "shared": {"value": false, "score": 0.0001812577247619629}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "CamemBERT", "normalizedForm": "CamemBERT", "offsetStart": 0, "offsetEnd": 9}, "publisher": {"rawForm": "Hugging Face", "normalizedForm": "Hugging Face"}, "context": "CamemBERT achieves overall slightly better results than the previous state-of-the-art and task-specific architecture UDPipe Future+mBERT +Flair, except for POS tagging on Sequoia and POS tagging on Spoken, where CamemBERT lags by 0.03% and 0.14% UPOS respectively. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.001154780387878418}, "created": {"value": false, "score": 8.58306884765625e-06}, "shared": {"value": false, "score": 4.76837158203125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9995896220207214}, "created": {"value": true, "score": 0.9988622665405273}, "shared": {"value": false, "score": 0.0001812577247619629}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "CamemBERT", "normalizedForm": "CamemBERT", "offsetStart": 0, "offsetEnd": 9}, "publisher": {"rawForm": "Hugging Face", "normalizedForm": "Hugging Face"}, "context": "CamemBERT reaches higher accuracy than its BASE counterparts reaching +5.6% over mBERT, +2.3 over XLM MLM-TLM , and +2.4 over XLM-R BASE . ", "mentionContextAttributes": {"used": {"value": false, "score": 0.0011674761772155762}, "created": {"value": false, "score": 6.4373016357421875e-06}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9995896220207214}, "created": {"value": true, "score": 0.9988622665405273}, "shared": {"value": false, "score": 0.0001812577247619629}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "CamemBERT", "normalizedForm": "CamemBERT", "offsetStart": 0, "offsetEnd": 9}, "publisher": {"rawForm": "Hugging Face", "normalizedForm": "Hugging Face"}, "context": "CamemBERT also uses as few as half as many parameters (110M vs. 270M for XLM-R BASE ).", "mentionContextAttributes": {"used": {"value": false, "score": 0.003198862075805664}, "created": {"value": false, "score": 5.364418029785156e-06}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9995896220207214}, "created": {"value": true, "score": 0.9988622665405273}, "shared": {"value": false, "score": 0.0001812577247619629}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "CamemBERT", "normalizedForm": "CamemBERT", "offsetStart": 0, "offsetEnd": 9}, "publisher": {"rawForm": "Hugging Face", "normalizedForm": "Hugging Face"}, "context": "CamemBERT LARGE achieves a state-of-the-art accuracy of 85.7% on the XNLI benchmark, as opposed to 85.2, for the recent XLM-R LARGE .", "mentionContextAttributes": {"used": {"value": false, "score": 0.00039201974868774414}, "created": {"value": false, "score": 4.100799560546875e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9995896220207214}, "created": {"value": true, "score": 0.9988622665405273}, "shared": {"value": false, "score": 0.0001812577247619629}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "CamemBERT", "normalizedForm": "CamemBERT", "offsetStart": 0, "offsetEnd": 9}, "publisher": {"rawForm": "Hugging Face", "normalizedForm": "Hugging Face"}, "context": "CamemBERT uses fewer parameters than multilingual models, mostly because of its smaller vocabulary size (e.g. ", "mentionContextAttributes": {"used": {"value": false, "score": 6.747245788574219e-05}, "created": {"value": false, "score": 5.245208740234375e-06}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9995896220207214}, "created": {"value": true, "score": 0.9988622665405273}, "shared": {"value": false, "score": 0.0001812577247619629}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "CamemBERT", "normalizedForm": "CamemBERT", "offsetStart": 0, "offsetEnd": 9}, "publisher": {"rawForm": "Hugging Face", "normalizedForm": "Hugging Face"}, "context": "CamemBERT uses the original architectures of BERT BASE (12 layers, 768 hidden dimensions, 12 attention heads, 110M parameters) and BERT LARGE (24 layers, 1024 hidden dimensions, 16 attention heads, 335M parameters).", "mentionContextAttributes": {"used": {"value": false, "score": 0.06899958848953247}, "created": {"value": false, "score": 7.867813110351562e-06}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9995896220207214}, "created": {"value": true, "score": 0.9988622665405273}, "shared": {"value": false, "score": 0.0001812577247619629}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "SentencePiece", "normalizedForm": "SentencePiece", "offsetStart": 0, "offsetEnd": 13}, "context": "SentencePiece is an extension of Byte-Pair encoding (BPE) (Sennrich et al., 2016) and WordPiece (Kudo, 2018) that does not require pre-tokenization (at the word or token level), thus removing the need for language-specific tokenisers. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.0001493692398071289}, "created": {"value": false, "score": 0.000258028507232666}, "shared": {"value": false, "score": 8.344650268554688e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9998041987419128}, "created": {"value": false, "score": 0.000258028507232666}, "shared": {"value": false, "score": 8.344650268554688e-07}}, "references": [{"label": "(Kudo and Richardson, 2018)", "normalizedForm": "Kudo and Richardson, 2018", "refKey": 28}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "UDPipe", "normalizedForm": "UDPipe", "offsetStart": 2, "offsetEnd": 8}, "context": "\u2022 UDPipe Future (Straka, 2018): An LSTMbased model ranked 3 rd in dependency parsing and 6 th in POS tagging at the CoNLL 2018 shared task (Seker et al., 2018). ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9930972456932068}, "created": {"value": false, "score": 5.841255187988281e-06}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9930972456932068}, "created": {"value": false, "score": 0.00012981891632080078}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "references": [{"label": "(Straka, 2018)", "normalizedForm": "Straka, 2018", "refKey": 57, "offsetStart": 9687, "offsetEnd": 9701}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "UDPipe", "normalizedForm": "UDPipe", "offsetStart": 2, "offsetEnd": 8}, "context": "\u2022 UDPipe Future + mBERT + Flair (Straka et al., 2019): The original UDPipe Future implementation using mBERT and Flair as feature-based contextualized word embeddings.", "mentionContextAttributes": {"used": {"value": false, "score": 0.0028687715530395508}, "created": {"value": false, "score": 0.00012981891632080078}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9930972456932068}, "created": {"value": false, "score": 0.00012981891632080078}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "references": [{"label": "(Straka, 2018)", "normalizedForm": "Straka, 2018", "refKey": 57}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "XNLI", "normalizedForm": "XNLI", "offsetStart": 4, "offsetEnd": 8}, "context": "The XNLI dataset is the exten-sion of the Multi-Genre NLI (MultiNLI) corpus (Williams et al., 2018) to 15 languages by translating the validation and test sets manually into each of those languages.", "mentionContextAttributes": {"used": {"value": true, "score": 0.8627021908760071}, "created": {"value": false, "score": 9.179115295410156e-06}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999645948410034}, "created": {"value": false, "score": 0.07769900560379028}, "shared": {"value": false, "score": 7.152557373046875e-07}}, "references": [{"label": "(Conneau et al., 2018)", "normalizedForm": "Conneau et al., 2018", "refKey": 12}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "XNLI", "normalizedForm": "XNLI", "offsetStart": 4, "offsetEnd": 8}, "context": "For XNLI, we provide the scores of mBERT which has been reported for French by Wu and Dredze (2019).", "mentionContextAttributes": {"used": {"value": false, "score": 0.08569961786270142}, "created": {"value": false, "score": 0.002192556858062744}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999645948410034}, "created": {"value": false, "score": 0.07769900560379028}, "shared": {"value": false, "score": 7.152557373046875e-07}}, "references": [{"label": "(Conneau et al., 2018)", "normalizedForm": "Conneau et al., 2018", "refKey": 12}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "CamemBERT", "normalizedForm": "CamemBERT", "offsetStart": 6, "offsetEnd": 15}, "publisher": {"rawForm": "Hugging Face", "normalizedForm": "Hugging Face"}, "context": "Using CamemBERT as embeddings to the traditional LSTM+CRF architecture gives slightly higher scores than by fine-tuning the model (89.08 vs. 89.55). ", "mentionContextAttributes": {"used": {"value": false, "score": 0.02375960350036621}, "created": {"value": false, "score": 6.9141387939453125e-06}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9995896220207214}, "created": {"value": true, "score": 0.9988622665405273}, "shared": {"value": false, "score": 0.0001812577247619629}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Future", "normalizedForm": "Future", "offsetStart": 9, "offsetEnd": 15}, "context": "\u2022 UDPipe Future + mBERT + Flair (Straka et al., 2019): The original UDPipe Future implementation using mBERT and Flair as feature-based contextualized word embeddings. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.0028687715530395508}, "created": {"value": false, "score": 0.00012981891632080078}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": false, "score": 0.14906632900238037}, "created": {"value": false, "score": 0.00012981891632080078}, "shared": {"value": false, "score": 2.384185791015625e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "-ture+", "normalizedForm": "-ture+", "offsetStart": 9, "offsetEnd": 15}, "context": "UDPipe Fu-ture+mBERT +Flair uses the contextualized string embeddings Flair (Akbik et al., 2018), which are in fact pretrained contextualized character-level word embeddings specifically designed to handle misspelled words as well as subword structures such as prefixes and suffixes. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.02225250005722046}, "created": {"value": false, "score": 2.7418136596679688e-06}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": false, "score": 0.02225250005722046}, "created": {"value": false, "score": 2.7418136596679688e-06}, "shared": {"value": false, "score": 2.384185791015625e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "RoBERTa", "normalizedForm": "RoBERTa", "offsetStart": 9, "offsetEnd": 16}, "context": "BERT and RoBERTa Our approach is based on RoBERTa (Liu et al., 2019) which itself is based on BERT (Devlin et al., 2019).", "mentionContextAttributes": {"used": {"value": false, "score": 0.3318215608596802}, "created": {"value": false, "score": 0.010234713554382324}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9995390176773071}, "created": {"value": true, "score": 0.9962459206581116}, "shared": {"value": false, "score": 1.430511474609375e-06}}, "references": [{"label": "(Liu et al., 2019)", "normalizedForm": "Liu et al., 2019", "refKey": 36}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "CamemBERT", "normalizedForm": "CamemBERT", "offsetStart": 12, "offsetEnd": 21}, "publisher": {"rawForm": "Hugging Face", "normalizedForm": "Hugging Face"}, "context": "We evaluate CamemBERT in the embeddings setting for POS tagging, dependency parsing and NER; using the open-source implementations of Straka et al. (2019)    (Dupont, 2017) 85.57 mBERT (fine-tuned) 87.35", "mentionContextAttributes": {"used": {"value": true, "score": 0.9872889518737793}, "created": {"value": false, "score": 1.2516975402832031e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9995896220207214}, "created": {"value": true, "score": 0.9988622665405273}, "shared": {"value": false, "score": 0.0001812577247619629}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "mBERT", "normalizedForm": "mBERT", "offsetStart": 13, "offsetEnd": 18}, "version": {"rawForm": ".57", "normalizedForm": ".57"}, "context": "We fine-tune mBERT on each of the treebanks with an additional layer for POS-tagging and dependency parsing, in the same conditions as our Camem-BERT model.", "mentionContextAttributes": {"used": {"value": false, "score": 0.07321369647979736}, "created": {"value": false, "score": 0.00034248828887939453}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999656677246094}, "created": {"value": false, "score": 0.003240525722503662}, "shared": {"value": false, "score": 5.960464477539062e-07}}, "references": [{"label": "(Devlin et al., 2018)", "normalizedForm": "Devlin et al., 2018", "refKey": 15}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "CamemBERT", "normalizedForm": "CamemBERT", "offsetStart": 13, "offsetEnd": 22}, "publisher": {"rawForm": "Hugging Face", "normalizedForm": "Hugging Face"}, "context": "We fine-tune CamemBERT independently for each task and each dataset. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.8382394909858704}, "created": {"value": false, "score": 0.0014027953147888184}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9995896220207214}, "created": {"value": true, "score": 0.9988622665405273}, "shared": {"value": false, "score": 0.0001812577247619629}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "SentencePiece", "normalizedForm": "SentencePiece", "offsetStart": 13, "offsetEnd": 26}, "context": "Since we use SentencePiece to tokenize our corpus, the input tokens to the model are a mix of whole words and subwords.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9943535327911377}, "created": {"value": false, "score": 8.976459503173828e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9998041987419128}, "created": {"value": false, "score": 0.000258028507232666}, "shared": {"value": false, "score": 8.344650268554688e-07}}, "references": [{"label": "(Kudo and Richardson, 2018)", "normalizedForm": "Kudo and Richardson, 2018", "refKey": 28}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "RoBERTa", "normalizedForm": "RoBERTa", "offsetStart": 14, "offsetEnd": 21}, "context": "Following the RoBERTa approach, we dynamically mask tokens instead of fixing them statically for the whole dataset during preprocessing.", "mentionContextAttributes": {"used": {"value": false, "score": 0.00020688772201538086}, "created": {"value": false, "score": 0.0450972318649292}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9995390176773071}, "created": {"value": true, "score": 0.9962459206581116}, "shared": {"value": false, "score": 1.430511474609375e-06}}, "references": [{"label": "(Liu et al., 2019)", "normalizedForm": "Liu et al., 2019", "refKey": 36}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "CamemBERTIn", "normalizedForm": "CamemBERTIn", "offsetStart": 14, "offsetEnd": 25}, "context": "Evaluation of CamemBERTIn this section, we measure the performance of our models by evaluating them on the four aforementioned tasks: POS tagging, dependency parsing, NER and NLI.9", "mentionContextAttributes": {"used": {"value": true, "score": 0.819888174533844}, "created": {"value": false, "score": 1.2993812561035156e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.819888174533844}, "created": {"value": false, "score": 1.2993812561035156e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "mBERT", "normalizedForm": "mBERT", "offsetStart": 15, "offsetEnd": 20}, "version": {"rawForm": ".57", "normalizedForm": ".57"}, "context": "UDPipe Fu-ture+mBERT +Flair uses the contextualized string embeddings Flair (Akbik et al., 2018), which are in fact pretrained contextualized character-level word embeddings specifically designed to handle misspelled words as well as subword structures such as prefixes and suffixes.", "mentionContextAttributes": {"used": {"value": false, "score": 0.02225250005722046}, "created": {"value": false, "score": 2.7418136596679688e-06}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999656677246094}, "created": {"value": false, "score": 0.003240525722503662}, "shared": {"value": false, "score": 5.960464477539062e-07}}, "references": [{"label": "(Devlin et al., 2018)", "normalizedForm": "Devlin et al., 2018", "refKey": 15}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "XNLI", "normalizedForm": "XNLI", "offsetStart": 16, "offsetEnd": 20}, "context": "We fine-tune on XNLI by adding a classification head composed of one hidden layer with a non-linearity and one linear projection layer, with input dropout for both.", "mentionContextAttributes": {"used": {"value": false, "score": 0.0009680986404418945}, "created": {"value": false, "score": 0.0014722347259521484}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999645948410034}, "created": {"value": false, "score": 0.07769900560379028}, "shared": {"value": false, "score": 7.152557373046875e-07}}, "references": [{"label": "(Conneau et al., 2018)", "normalizedForm": "Conneau et al., 2018", "refKey": 12}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "RoBERTa", "normalizedForm": "RoBERTa", "offsetStart": 19, "offsetEnd": 26}, "context": "CamemBERT uses the RoBERTa architecture (Liu et al., 2019), an improved variant of the high-performing and widely used BERT architecture (Devlin et al., 2019).", "mentionContextAttributes": {"used": {"value": false, "score": 0.01318436861038208}, "created": {"value": false, "score": 4.112720489501953e-05}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9995390176773071}, "created": {"value": true, "score": 0.9962459206581116}, "shared": {"value": false, "score": 1.430511474609375e-06}}, "references": [{"label": "(Liu et al., 2019)", "normalizedForm": "Liu et al., 2019", "refKey": 36}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "CamemBERT", "normalizedForm": "CamemBERT", "offsetStart": 19, "offsetEnd": 28}, "publisher": {"rawForm": "Hugging Face", "normalizedForm": "Hugging Face"}, "context": "In both scenarios, CamemBERT achieves higher F1 scores than the traditional CRF-based architectures, both non-neural and neural, and than finetuned multilingual BERT models. ", "mentionContextAttributes": {"used": {"value": false, "score": 8.106231689453125e-05}, "created": {"value": false, "score": 3.361701965332031e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9995896220207214}, "created": {"value": true, "score": 0.9988622665405273}, "shared": {"value": false, "score": 0.0001812577247619629}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "CamemBERT", "normalizedForm": "CamemBERT", "offsetStart": 22, "offsetEnd": 31}, "publisher": {"rawForm": "Hugging Face", "normalizedForm": "Hugging Face"}, "context": "We use the pretrained CamemBERT in two ways. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9083039164543152}, "created": {"value": false, "score": 3.4570693969726562e-06}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9995896220207214}, "created": {"value": true, "score": 0.9988622665405273}, "shared": {"value": false, "score": 0.0001812577247619629}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "XNLI", "normalizedForm": "XNLI", "offsetStart": 23, "offsetEnd": 27}, "context": "accuracy on the French XNLI test set (best model selected on validation out of 10).", "mentionContextAttributes": {"used": {"value": true, "score": 0.9999645948410034}, "created": {"value": false, "score": 1.823902130126953e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999645948410034}, "created": {"value": false, "score": 0.07769900560379028}, "shared": {"value": false, "score": 7.152557373046875e-07}}, "references": [{"label": "(Conneau et al., 2018)", "normalizedForm": "Conneau et al., 2018", "refKey": 12}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "RoBERTa", "normalizedForm": "RoBERTa", "offsetStart": 23, "offsetEnd": 30}, "context": "Pretraining We use the RoBERTa implementation in the fairseq library (Ott et al., 2019). ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9917842149734497}, "created": {"value": false, "score": 0.00027817487716674805}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9995390176773071}, "created": {"value": true, "score": 0.9962459206581116}, "shared": {"value": false, "score": 1.430511474609375e-06}}, "references": [{"label": "(Liu et al., 2019)", "normalizedForm": "Liu et al., 2019", "refKey": 36}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "RoBERTa", "normalizedForm": "RoBERTa", "offsetStart": 23, "offsetEnd": 30}, "context": "Transformer Similar to RoBERTa and BERT, CamemBERT is a multi-layer bidirectional Transformer (Vaswani et al., 2017).", "mentionContextAttributes": {"used": {"value": false, "score": 0.00015747547149658203}, "created": {"value": false, "score": 3.9577484130859375e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9995390176773071}, "created": {"value": true, "score": 0.9962459206581116}, "shared": {"value": false, "score": 1.430511474609375e-06}}, "references": [{"label": "(Liu et al., 2019)", "normalizedForm": "Liu et al., 2019", "refKey": 36}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "CamemBERT", "normalizedForm": "CamemBERT", "offsetStart": 25, "offsetEnd": 34}, "publisher": {"rawForm": "Hugging Face", "normalizedForm": "Hugging Face"}, "context": "We show that fine-tuning CamemBERT in a straightforward manner leads to state-of-the-art results on all tasks and outperforms the existing BERT-based models in all cases. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.0030821561813354492}, "created": {"value": false, "score": 0.02859574556350708}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9995896220207214}, "created": {"value": true, "score": 0.9988622665405273}, "shared": {"value": false, "score": 0.0001812577247619629}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "fairseq", "normalizedForm": "fairseq", "offsetStart": 28, "offsetEnd": 35}, "context": "The NLI experiments use the fairseq library following the RoBERTa implementation.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9688633680343628}, "created": {"value": false, "score": 3.3974647521972656e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9688633680343628}, "created": {"value": true, "score": 0.9155601859092712}, "shared": {"value": true, "score": 0.9743472933769226}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "RoBERTa", "normalizedForm": "RoBERTa", "offsetStart": 29, "offsetEnd": 36}, "context": "CamemBERT is very similar to RoBERTa, the main difference being the use of whole-word masking and the usage of SentencePiece tokenization (Kudo and Richardson, 2018) instead of WordPiece (Schuster and Nakajima, 2012).", "mentionContextAttributes": {"used": {"value": false, "score": 0.0007841587066650391}, "created": {"value": false, "score": 1.823902130126953e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9995390176773071}, "created": {"value": true, "score": 0.9962459206581116}, "shared": {"value": false, "score": 1.430511474609375e-06}}, "references": [{"label": "(Liu et al., 2019)", "normalizedForm": "Liu et al., 2019", "refKey": 36}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "CamemBERT", "normalizedForm": "CamemBERT", "offsetStart": 31, "offsetEnd": 40}, "publisher": {"rawForm": "Hugging Face", "normalizedForm": "Hugging Face"}, "context": "For NER, we similarly evaluate CamemBERT in the fine-tuning setting and as input embeddings to the task specific architecture LSTM+CRF. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.991365909576416}, "created": {"value": false, "score": 6.389617919921875e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9995896220207214}, "created": {"value": true, "score": 0.9988622665405273}, "shared": {"value": false, "score": 0.0001812577247619629}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "CamemBERT", "normalizedForm": "CamemBERT", "offsetStart": 32, "offsetEnd": 41}, "publisher": {"rawForm": "Hugging Face", "normalizedForm": "Hugging Face"}, "context": "This demonstrates that although CamemBERT can be used successfully without any task-specific architecture, it can still produce high quality contextualized embeddings that might be useful in scenarios where powerful downstream architectures exist.", "mentionContextAttributes": {"used": {"value": false, "score": 4.1484832763671875e-05}, "created": {"value": false, "score": 6.937980651855469e-05}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9995896220207214}, "created": {"value": true, "score": 0.9988622665405273}, "shared": {"value": false, "score": 0.0001812577247619629}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "RoBERTa", "normalizedForm": "RoBERTa", "offsetStart": 33, "offsetEnd": 40}, "context": "\u2022 First release of a monolingual RoBERTa model for the French language using recently introduced large-scale open source corpora from the Oscar collection and first outside the original BERT authors to release such a large model for an other language than English. 1", "mentionContextAttributes": {"used": {"value": false, "score": 0.0007991194725036621}, "created": {"value": false, "score": 0.0006856918334960938}, "shared": {"value": false, "score": 4.76837158203125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9995390176773071}, "created": {"value": true, "score": 0.9962459206581116}, "shared": {"value": false, "score": 1.430511474609375e-06}}, "references": [{"label": "(Liu et al., 2019)", "normalizedForm": "Liu et al., 2019", "refKey": 36}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "CamemBERT", "normalizedForm": "CamemBERT", "offsetStart": 33, "offsetEnd": 42}, "publisher": {"rawForm": "Hugging Face", "normalizedForm": "Hugging Face"}, "context": "Additionally, this suggests that CamemBERT is also able to produce high-quality representations out-of-the-box without further tuning.", "mentionContextAttributes": {"used": {"value": false, "score": 0.00021272897720336914}, "created": {"value": false, "score": 0.0002587437629699707}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9995896220207214}, "created": {"value": true, "score": 0.9988622665405273}, "shared": {"value": false, "score": 0.0001812577247619629}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "XNLI", "normalizedForm": "XNLI", "offsetStart": 34, "offsetEnd": 38}, "context": "Natural Language Inference On the XNLI benchmark, we compare CamemBERT to previous state-of-the-art multilingual models in the finetuning setting.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9994801878929138}, "created": {"value": false, "score": 7.033348083496094e-06}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999645948410034}, "created": {"value": false, "score": 0.07769900560379028}, "shared": {"value": false, "score": 7.152557373046875e-07}}, "references": [{"label": "(Conneau et al., 2018)", "normalizedForm": "Conneau et al., 2018", "refKey": 12}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "mBERT", "normalizedForm": "mBERT", "offsetStart": 35, "offsetEnd": 40}, "version": {"rawForm": ".57", "normalizedForm": ".57"}, "context": "For XNLI, we provide the scores of mBERT which has been reported for French by Wu and Dredze (2019).", "mentionContextAttributes": {"used": {"value": false, "score": 0.08569961786270142}, "created": {"value": false, "score": 0.002192556858062744}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999656677246094}, "created": {"value": false, "score": 0.003240525722503662}, "shared": {"value": false, "score": 5.960464477539062e-07}}, "references": [{"label": "(Devlin et al., 2018)", "normalizedForm": "Devlin et al., 2018", "refKey": 15}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "XNLI", "normalizedForm": "XNLI", "offsetStart": 36, "offsetEnd": 40}, "context": "The increase is slightly higher for XNLI (+0.84).", "mentionContextAttributes": {"used": {"value": true, "score": 0.9151538610458374}, "created": {"value": false, "score": 2.7060508728027344e-05}, "shared": {"value": false, "score": 7.152557373046875e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999645948410034}, "created": {"value": false, "score": 0.07769900560379028}, "shared": {"value": false, "score": 7.152557373046875e-07}}, "references": [{"label": "(Conneau et al., 2018)", "normalizedForm": "Conneau et al., 2018", "refKey": 12}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "CamemBERT", "normalizedForm": "CamemBERT", "offsetStart": 38, "offsetEnd": 47}, "publisher": {"rawForm": "Hugging Face", "normalizedForm": "Hugging Face"}, "context": "We also train alternative versions of CamemBERT on different smaller corpora with different levels of homogeneity in genre and style in order to assess the impact of these parameters on downstream task performance. ", "mentionContextAttributes": {"used": {"value": false, "score": 9.167194366455078e-05}, "created": {"value": false, "score": 0.24378418922424316}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9995896220207214}, "created": {"value": true, "score": 0.9988622665405273}, "shared": {"value": false, "score": 0.0001812577247619629}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "CamemBERT", "normalizedForm": "CamemBERT", "offsetStart": 38, "offsetEnd": 47}, "publisher": {"rawForm": "Hugging Face", "normalizedForm": "Hugging Face"}, "context": "CamemBERT (fine-tuned) 89.08 LSTM+CRF+CamemBERT (embeddings) 89.55  (Conneau et al., 2019) 80.1 270M", "mentionContextAttributes": {"used": {"value": true, "score": 0.9992942810058594}, "created": {"value": false, "score": 8.344650268554688e-06}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9995896220207214}, "created": {"value": true, "score": 0.9988622665405273}, "shared": {"value": false, "score": 0.0001812577247619629}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "CamemBERT", "normalizedForm": "CamemBERT", "offsetStart": 39, "offsetEnd": 48}, "publisher": {"rawForm": "Hugging Face", "normalizedForm": "Hugging Face"}, "context": "Using French as an example, we trained CamemBERT, a language model based on RoBERTa.", "mentionContextAttributes": {"used": {"value": false, "score": 0.3398348093032837}, "created": {"value": true, "score": 0.9962459206581116}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9995896220207214}, "created": {"value": true, "score": 0.9988622665405273}, "shared": {"value": false, "score": 0.0001812577247619629}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "XNLI", "normalizedForm": "XNLI", "offsetStart": 41, "offsetEnd": 45}, "context": "The OSCAR-4GB model gets slightly better XNLI accuracy than the full OSCAR-138GB model(81.88", "mentionContextAttributes": {"used": {"value": false, "score": 0.006543457508087158}, "created": {"value": false, "score": 9.894371032714844e-06}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999645948410034}, "created": {"value": false, "score": 0.07769900560379028}, "shared": {"value": false, "score": 7.152557373046875e-07}}, "references": [{"label": "(Conneau et al., 2018)", "normalizedForm": "Conneau et al., 2018", "refKey": 12}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "CamemBERT", "normalizedForm": "CamemBERT", "offsetStart": 41, "offsetEnd": 50}, "publisher": {"rawForm": "Hugging Face", "normalizedForm": "Hugging Face"}, "context": "Transformer Similar to RoBERTa and BERT, CamemBERT is a multi-layer bidirectional Transformer (Vaswani et al., 2017).", "mentionContextAttributes": {"used": {"value": false, "score": 0.00015747547149658203}, "created": {"value": false, "score": 3.9577484130859375e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9995896220207214}, "created": {"value": true, "score": 0.9988622665405273}, "shared": {"value": false, "score": 0.0001812577247619629}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "RoBERTa", "normalizedForm": "RoBERTa", "offsetStart": 42, "offsetEnd": 49}, "context": "BERT and RoBERTa Our approach is based on RoBERTa (Liu et al., 2019) which itself is based on BERT (Devlin et al., 2019).", "mentionContextAttributes": {"used": {"value": false, "score": 0.3318215608596802}, "created": {"value": false, "score": 0.010234713554382324}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9995390176773071}, "created": {"value": true, "score": 0.9962459206581116}, "shared": {"value": false, "score": 1.430511474609375e-06}}, "references": [{"label": "(Liu et al., 2019)", "normalizedForm": "Liu et al., 2019", "refKey": 36, "offsetStart": 6058, "offsetEnd": 6076}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "XNLI", "normalizedForm": "XNLI", "offsetStart": 46, "offsetEnd": 50}, "context": "The performance gap is also very large on the XNLI task, probably as a consequence of the larger diversity of Common-Crawl-based corpora in terms of genres and topics.", "mentionContextAttributes": {"used": {"value": false, "score": 0.004434049129486084}, "created": {"value": false, "score": 5.316734313964844e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999645948410034}, "created": {"value": false, "score": 0.07769900560379028}, "shared": {"value": false, "score": 7.152557373046875e-07}}, "references": [{"label": "(Conneau et al., 2018)", "normalizedForm": "Conneau et al., 2018", "refKey": 12}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Face transformer", "normalizedForm": "Face transformer", "offsetStart": 46, "offsetEnd": 62}, "publisher": {"rawForm": "Hugging", "normalizedForm": "Hugging", "offsetStart": 36, "offsetEnd": 43}, "context": "We use the version available in the Hugging's Face transformer library (Wolf et al., 2019); like mBERT, we fine-tune it in the same conditions as our model. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.5628630518913269}, "created": {"value": false, "score": 0.0001227855682373047}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.5628630518913269}, "created": {"value": false, "score": 0.0001227855682373047}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "references": [{"label": "(Wolf et al., 2019)", "normalizedForm": "Wolf et al., 2019", "refKey": 66, "offsetStart": 9585, "offsetEnd": 9604}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "CamemBERT", "normalizedForm": "CamemBERT", "offsetStart": 47, "offsetEnd": 56}, "publisher": {"rawForm": "Hugging Face", "normalizedForm": "Hugging Face"}, "context": "With this aim, we train alternative version of CamemBERT by varying the pretraining datasets. ", "mentionContextAttributes": {"used": {"value": false, "score": 4.565715789794922e-05}, "created": {"value": true, "score": 0.9988622665405273}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9995896220207214}, "created": {"value": true, "score": 0.9988622665405273}, "shared": {"value": false, "score": 0.0001812577247619629}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "CamemBERT", "normalizedForm": "CamemBERT", "offsetStart": 53, "offsetEnd": 62}, "publisher": {"rawForm": "Hugging Face", "normalizedForm": "Hugging Face"}, "context": "Two elements might explain the better performance of CamemBERT over XLM-R. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9995896220207214}, "created": {"value": false, "score": 3.981590270996094e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9995896220207214}, "created": {"value": true, "score": 0.9988622665405273}, "shared": {"value": false, "score": 0.0001812577247619629}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "fairseq library", "normalizedForm": "fairseq library", "offsetStart": 53, "offsetEnd": 86}, "context": "Pretraining We use the RoBERTa implementation in the fairseq library (Ott et al., 2019). ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9917842149734497}, "created": {"value": false, "score": 0.00027817487716674805}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9917842149734497}, "created": {"value": false, "score": 0.00027817487716674805}, "shared": {"value": false, "score": 1.1920928955078125e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "CamemBERT", "normalizedForm": "CamemBERT", "offsetStart": 56, "offsetEnd": 65}, "publisher": {"rawForm": "Hugging Face", "normalizedForm": "Hugging Face"}, "context": "In the appendix, we analyse different design choices of CamemBERT (Table 8), namely with respect to the use of whole-word masking, the training dataset, the model size, and the number of training steps in complement with the analyses of the impact of corpus origin an size (Section 6. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.856377124786377}, "created": {"value": false, "score": 5.996227264404297e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9995896220207214}, "created": {"value": true, "score": 0.9988622665405273}, "shared": {"value": false, "score": 0.0001812577247619629}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "UDPipe Future", "normalizedForm": "UDPipe Future", "offsetStart": 56, "offsetEnd": 69}, "url": {"rawForm": "https://github. com/CoNLL-UD-2018/UDPipe-Future", "normalizedForm": "https://github. com/CoNLL-UD-2018/UDPipe-Future", "offsetStart": 86, "offsetEnd": 133}, "context": "fairseq/blob/master/examples/roberta/ README.glue.md.10 UDPipe Future is available at https://github. com/CoNLL-UD-2018/UDPipe-Future, and the code for nested NER is available at https://github.com/ ufal/acl2019_nested_ner.", "mentionContextAttributes": {"used": {"value": false, "score": 0.0037519335746765137}, "created": {"value": false, "score": 2.9802322387695312e-06}, "shared": {"value": false, "score": 0.0706106424331665}}, "documentContextAttributes": {"used": {"value": false, "score": 0.0037519335746765137}, "created": {"value": false, "score": 0.00028592348098754883}, "shared": {"value": false, "score": 0.0706106424331665}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "SentencePiece", "normalizedForm": "SentencePiece", "offsetStart": 56, "offsetEnd": 69}, "context": "We segment the input text data into subword units using SentencePiece (Kudo and Richardson, 2018).", "mentionContextAttributes": {"used": {"value": true, "score": 0.9998041987419128}, "created": {"value": false, "score": 6.437301635742188e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9998041987419128}, "created": {"value": false, "score": 0.000258028507232666}, "shared": {"value": false, "score": 8.344650268554688e-07}}, "references": [{"label": "(Kudo and Richardson, 2018)", "normalizedForm": "Kudo and Richardson, 2018", "refKey": 28, "offsetStart": 11808, "offsetEnd": 11835}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "RoBERTa", "normalizedForm": "RoBERTa", "offsetStart": 58, "offsetEnd": 65}, "context": "The NLI experiments use the fairseq library following the RoBERTa implementation.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9688633680343628}, "created": {"value": false, "score": 3.3974647521972656e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9995390176773071}, "created": {"value": true, "score": 0.9962459206581116}, "shared": {"value": false, "score": 1.430511474609375e-06}}, "references": [{"label": "(Liu et al., 2019)", "normalizedForm": "Liu et al., 2019", "refKey": 36}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "CamemBERT", "normalizedForm": "CamemBERT", "offsetStart": 60, "offsetEnd": 69}, "publisher": {"rawForm": "Hugging Face", "normalizedForm": "Hugging Face"}, "context": "Downstream task performance for our alternative versions of CamemBERT are provided in Table 5. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.10080277919769287}, "created": {"value": false, "score": 0.0011469125747680664}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9995896220207214}, "created": {"value": true, "score": 0.9988622665405273}, "shared": {"value": false, "score": 0.0001812577247619629}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "CamemBERT", "normalizedForm": "CamemBERT", "offsetStart": 61, "offsetEnd": 70}, "publisher": {"rawForm": "Hugging Face", "normalizedForm": "Hugging Face"}, "context": "Natural Language Inference On the XNLI benchmark, we compare CamemBERT to previous state-of-the-art multilingual models in the finetuning setting. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9994801878929138}, "created": {"value": false, "score": 7.033348083496094e-06}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9995896220207214}, "created": {"value": true, "score": 0.9988622665405273}, "shared": {"value": false, "score": 0.0001812577247619629}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "XNLI", "normalizedForm": "XNLI", "offsetStart": 68, "offsetEnd": 72}, "context": "Finally, we evaluate our model on NLI, using the French part of the XNLI dataset (Conneau et al., 2018).", "mentionContextAttributes": {"used": {"value": true, "score": 0.9397934675216675}, "created": {"value": false, "score": 0.07769900560379028}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999645948410034}, "created": {"value": false, "score": 0.07769900560379028}, "shared": {"value": false, "score": 7.152557373046875e-07}}, "references": [{"label": "(Conneau et al., 2018)", "normalizedForm": "Conneau et al., 2018", "refKey": 12, "offsetStart": 8505, "offsetEnd": 8527}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "RoBERTa", "normalizedForm": "RoBERTa", "offsetStart": 68, "offsetEnd": 75}, "context": "Interestingly, Le et al. (2019) showed that using their FlauBert, a RoBERTa-based language model for French, which was trained on less but more edited data, in conjunction to Camem-BERT in an ensemble system could improve the performance of a parsing model and establish a new state-of-the-art in constituency parsing of French, highlighting thus the complementarity of both models.", "mentionContextAttributes": {"used": {"value": false, "score": 0.04778009653091431}, "created": {"value": false, "score": 0.3211096525192261}, "shared": {"value": false, "score": 1.430511474609375e-06}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9995390176773071}, "created": {"value": true, "score": 0.9962459206581116}, "shared": {"value": false, "score": 1.430511474609375e-06}}, "references": [{"label": "(Liu et al., 2019)", "normalizedForm": "Liu et al., 2019", "refKey": 36}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "UDPipe Future", "normalizedForm": "UDPipe Future", "offsetStart": 68, "offsetEnd": 81}, "url": {"rawForm": "https://github. com/CoNLL-UD-2018/UDPipe-Future", "normalizedForm": "https://github. com/CoNLL-UD-2018/UDPipe-Future"}, "context": "\u2022 UDPipe Future + mBERT + Flair (Straka et al., 2019): The original UDPipe Future implementation using mBERT and Flair as feature-based contextualized word embeddings. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.0028687715530395508}, "created": {"value": false, "score": 0.00012981891632080078}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": false, "score": 0.0037519335746765137}, "created": {"value": false, "score": 0.00028592348098754883}, "shared": {"value": false, "score": 0.0706106424331665}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "XNLI", "normalizedForm": "XNLI", "offsetStart": 69, "offsetEnd": 73}, "context": "CamemBERT LARGE achieves a state-of-the-art accuracy of 85.7% on the XNLI benchmark, as opposed to 85.2, for the recent XLM-R LARGE .", "mentionContextAttributes": {"used": {"value": false, "score": 0.00039201974868774414}, "created": {"value": false, "score": 4.100799560546875e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999645948410034}, "created": {"value": false, "score": 0.07769900560379028}, "shared": {"value": false, "score": 7.152557373046875e-07}}, "references": [{"label": "(Conneau et al., 2018)", "normalizedForm": "Conneau et al., 2018", "refKey": 12}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "RoBERTa", "normalizedForm": "RoBERTa", "offsetStart": 70, "offsetEnd": 77}, "context": "For NLI we use the default hyperparameters provided by the authors of RoBERTa on the MNLI task. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.998794436454773}, "created": {"value": false, "score": 1.71661376953125e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9995390176773071}, "created": {"value": true, "score": 0.9962459206581116}, "shared": {"value": false, "score": 1.430511474609375e-06}}, "references": [{"label": "(Liu et al., 2019)", "normalizedForm": "Liu et al., 2019", "refKey": 36}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "mBERT", "normalizedForm": "mBERT", "offsetStart": 73, "offsetEnd": 78}, "version": {"rawForm": ".57", "normalizedForm": ".57"}, "context": "Embeddings Following Strakov\u00e1 et al. (2019) and Straka et al. (2019) for mBERT and the English BERT, we make use of CamemBERT in a feature-based embeddings setting.", "mentionContextAttributes": {"used": {"value": false, "score": 0.017403066158294678}, "created": {"value": false, "score": 0.0003172159194946289}, "shared": {"value": false, "score": 4.76837158203125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999656677246094}, "created": {"value": false, "score": 0.003240525722503662}, "shared": {"value": false, "score": 5.960464477539062e-07}}, "references": [{"label": "(Devlin et al., 2018)", "normalizedForm": "Devlin et al., 2018", "refKey": 15}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "RoBERTa", "normalizedForm": "RoBERTa", "offsetStart": 76, "offsetEnd": 83}, "context": "Using French as an example, we trained CamemBERT, a language model based on RoBERTa.", "mentionContextAttributes": {"used": {"value": false, "score": 0.3398348093032837}, "created": {"value": true, "score": 0.9962459206581116}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9995390176773071}, "created": {"value": true, "score": 0.9962459206581116}, "shared": {"value": false, "score": 1.430511474609375e-06}}, "references": [{"label": "(Liu et al., 2019)", "normalizedForm": "Liu et al., 2019", "refKey": 36}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "CamemBERT", "normalizedForm": "CamemBERT", "offsetStart": 77, "offsetEnd": 86}, "publisher": {"rawForm": "Hugging Face", "normalizedForm": "Hugging Face"}, "context": "Fine-tuning For each task, we append the relevant predictive layer on top of CamemBERT's architecture. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.7908370494842529}, "created": {"value": false, "score": 0.00030624866485595703}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9995896220207214}, "created": {"value": true, "score": 0.9988622665405273}, "shared": {"value": false, "score": 0.0001812577247619629}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "CamemBERT", "normalizedForm": "CamemBERT", "offsetStart": 78, "offsetEnd": 87}, "publisher": {"rawForm": "Hugging Face", "normalizedForm": "Hugging Face"}, "context": "In this section, we present the four downstream tasks that we use to evaluate CamemBERT, namely: Part-Of-Speech (POS) tagging, dependency parsing, Named Entity Recognition (NER) and Natural Language Inference (NLI). ", "mentionContextAttributes": {"used": {"value": false, "score": 0.10192787647247314}, "created": {"value": true, "score": 0.8941318392753601}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9995896220207214}, "created": {"value": true, "score": 0.9988622665405273}, "shared": {"value": false, "score": 0.0001812577247619629}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "CamemBERT", "normalizedForm": "CamemBERT", "offsetStart": 78, "offsetEnd": 87}, "publisher": {"rawForm": "Hugging Face", "normalizedForm": "Hugging Face"}, "context": "This design choice might explain the difference in score for POS tagging with CamemBERT, especially for the Spoken treebank where words are not capitalized, a factor that might pose a problem for CamemBERT which was trained on capitalized data, but that might be properly handle by Flair on the UDPipe Future+mBERT +Flair model.", "mentionContextAttributes": {"used": {"value": false, "score": 0.14906638860702515}, "created": {"value": false, "score": 3.0994415283203125e-06}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9995896220207214}, "created": {"value": true, "score": 0.9988622665405273}, "shared": {"value": false, "score": 0.0001812577247619629}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "mBERT", "normalizedForm": "mBERT", "offsetStart": 81, "offsetEnd": 86}, "version": {"rawForm": ".57", "normalizedForm": ".57"}, "context": "CamemBERT reaches higher accuracy than its BASE counterparts reaching +5.6% over mBERT, +2.3 over XLM MLM-TLM , and +2.4 over XLM-R BASE . ", "mentionContextAttributes": {"used": {"value": false, "score": 0.0011674761772155762}, "created": {"value": false, "score": 6.4373016357421875e-06}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999656677246094}, "created": {"value": false, "score": 0.003240525722503662}, "shared": {"value": false, "score": 5.960464477539062e-07}}, "references": [{"label": "(Devlin et al., 2018)", "normalizedForm": "Devlin et al., 2018", "refKey": 15}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "WordPiece", "normalizedForm": "WordPiece", "offsetStart": 86, "offsetEnd": 108}, "context": "SentencePiece is an extension of Byte-Pair encoding (BPE) (Sennrich et al., 2016) and WordPiece (Kudo, 2018) that does not require pre-tokenization (at the word or token level), thus removing the need for language-specific tokenisers. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.0001493692398071289}, "created": {"value": false, "score": 0.000258028507232666}, "shared": {"value": false, "score": 8.344650268554688e-07}}, "documentContextAttributes": {"used": {"value": false, "score": 0.0007841587066650391}, "created": {"value": false, "score": 0.000258028507232666}, "shared": {"value": false, "score": 8.344650268554688e-07}}, "references": [{"label": "(Schuster and Nakajima, 2012)", "normalizedForm": "Schuster and Nakajima, 2012", "refKey": 54}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "mBERT", "normalizedForm": "mBERT", "offsetStart": 93, "offsetEnd": 98}, "version": {"rawForm": ".57", "normalizedForm": ".57"}, "context": "Camem-BERT also reaches better performance than other multilingual pretrained models such as mBERT and XLM MLM-TLM on all treebanks.", "mentionContextAttributes": {"used": {"value": false, "score": 9.644031524658203e-05}, "created": {"value": false, "score": 3.1113624572753906e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999656677246094}, "created": {"value": false, "score": 0.003240525722503662}, "shared": {"value": false, "score": 5.960464477539062e-07}}, "references": [{"label": "(Devlin et al., 2018)", "normalizedForm": "Devlin et al., 2018", "refKey": 15}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "mBERT", "normalizedForm": "mBERT", "offsetStart": 97, "offsetEnd": 102}, "version": {"rawForm": ".57", "normalizedForm": ".57"}, "context": "We use the version available in the Hugging's Face transformer library (Wolf et al., 2019); like mBERT, we fine-tune it in the same conditions as our model. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.5628630518913269}, "created": {"value": false, "score": 0.0001227855682373047}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999656677246094}, "created": {"value": false, "score": 0.003240525722503662}, "shared": {"value": false, "score": 5.960464477539062e-07}}, "references": [{"label": "(Devlin et al., 2018)", "normalizedForm": "Devlin et al., 2018", "refKey": 15}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "mBERT", "normalizedForm": "mBERT", "offsetStart": 97, "offsetEnd": 102}, "version": {"rawForm": ".57", "normalizedForm": ".57"}, "context": "Additionally, as for POS and dependency parsing, we compare our model to a fine-tuned version of mBERT for the NER task.", "mentionContextAttributes": {"used": {"value": false, "score": 0.05056637525558472}, "created": {"value": false, "score": 0.0032111406326293945}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999656677246094}, "created": {"value": false, "score": 0.003240525722503662}, "shared": {"value": false, "score": 5.960464477539062e-07}}, "references": [{"label": "(Devlin et al., 2018)", "normalizedForm": "Devlin et al., 2018", "refKey": 15}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "CamemBERT", "normalizedForm": "CamemBERT", "offsetStart": 97, "offsetEnd": 106}, "publisher": {"rawForm": "Hugging Face", "normalizedForm": "Hugging Face"}, "context": "The two approaches achieve similar scores, with a slight advantage for the fine-tuned version of CamemBERT, thus questioning the need for complex task-specific architectures such as UDPipe Future.", "mentionContextAttributes": {"used": {"value": false, "score": 0.00011360645294189453}, "created": {"value": false, "score": 0.00028592348098754883}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9995896220207214}, "created": {"value": true, "score": 0.9988622665405273}, "shared": {"value": false, "score": 0.0001812577247619629}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "mBERT", "normalizedForm": "mBERT", "offsetStart": 103, "offsetEnd": 108}, "version": {"rawForm": ".57", "normalizedForm": ".57"}, "context": "\u2022 UDPipe Future + mBERT + Flair (Straka et al., 2019): The original UDPipe Future implementation using mBERT and Flair as feature-based contextualized word embeddings.", "mentionContextAttributes": {"used": {"value": false, "score": 0.0028687715530395508}, "created": {"value": false, "score": 0.00012981891632080078}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999656677246094}, "created": {"value": false, "score": 0.003240525722503662}, "shared": {"value": false, "score": 5.960464477539062e-07}}, "references": [{"label": "(Devlin et al., 2018)", "normalizedForm": "Devlin et al., 2018", "refKey": 15}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "code", "normalizedForm": "code", "offsetStart": 107, "offsetEnd": 111}, "url": {"rawForm": "https://github.com/ ufal/acl2019_nested_ner", "normalizedForm": "https://github.com/ ufal/acl2019_nested_ner"}, "context": "It was built using a language model trained on Wikipedia, in order to filter out bad quality texts such as code or tables.", "mentionContextAttributes": {"used": {"value": false, "score": 0.46750837564468384}, "created": {"value": true, "score": 0.8837794065475464}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "documentContextAttributes": {"used": {"value": false, "score": 0.46750837564468384}, "created": {"value": true, "score": 0.8837794065475464}, "shared": {"value": false, "score": 0.0706106424331665}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "mBERT", "normalizedForm": "mBERT", "offsetStart": 108, "offsetEnd": 133}, "version": {"rawForm": ".57", "normalizedForm": ".57"}, "context": "In almost all tested configurations they displayed better results than multilingual language models such as mBERT (Pires et al., 2019). ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9999656677246094}, "created": {"value": false, "score": 1.0013580322265625e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999656677246094}, "created": {"value": false, "score": 0.003240525722503662}, "shared": {"value": false, "score": 5.960464477539062e-07}}, "references": [{"label": "(Devlin et al., 2018)", "normalizedForm": "Devlin et al., 2018", "refKey": 15}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "SentencePiece", "normalizedForm": "SentencePiece", "offsetStart": 111, "offsetEnd": 124}, "context": "CamemBERT is very similar to RoBERTa, the main difference being the use of whole-word masking and the usage of SentencePiece tokenization (Kudo and Richardson, 2018) instead of WordPiece (Schuster and Nakajima, 2012).", "mentionContextAttributes": {"used": {"value": false, "score": 0.0007841587066650391}, "created": {"value": false, "score": 1.823902130126953e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9998041987419128}, "created": {"value": false, "score": 0.000258028507232666}, "shared": {"value": false, "score": 8.344650268554688e-07}}, "references": [{"label": "(Kudo and Richardson, 2018)", "normalizedForm": "Kudo and Richardson, 2018", "refKey": 28}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "fairseq", "normalizedForm": "fairseq", "offsetStart": 113, "offsetEnd": 120}, "context": "In that spirit, we make the models used in our experiments available via our website and via the huggingface and fairseq APIs, in addition to the base CamemBERT model.", "mentionContextAttributes": {"used": {"value": false, "score": 0.0003331899642944336}, "created": {"value": true, "score": 0.9155601859092712}, "shared": {"value": false, "score": 0.0001812577247619629}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9688633680343628}, "created": {"value": true, "score": 0.9155601859092712}, "shared": {"value": true, "score": 0.9743472933769226}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "CamemBERT", "normalizedForm": "CamemBERT", "offsetStart": 114, "offsetEnd": 123}, "publisher": {"rawForm": "Hugging Face", "normalizedForm": "Hugging Face"}, "context": "These two complementary approaches shed light on the quality of the pretrained hidden representations captured by CamemBERT.", "mentionContextAttributes": {"used": {"value": false, "score": 0.0005652308464050293}, "created": {"value": false, "score": 0.0016106367111206055}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9995896220207214}, "created": {"value": true, "score": 0.9988622665405273}, "shared": {"value": false, "score": 0.0001812577247619629}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "CamemBERT", "normalizedForm": "CamemBERT", "offsetStart": 116, "offsetEnd": 125}, "publisher": {"rawForm": "Hugging Face", "normalizedForm": "Hugging Face"}, "context": "Embeddings Following Strakov\u00e1 et al. (2019) and Straka et al. (2019) for mBERT and the English BERT, we make use of CamemBERT in a feature-based embeddings setting.", "mentionContextAttributes": {"used": {"value": false, "score": 0.017403066158294678}, "created": {"value": false, "score": 0.0003172159194946289}, "shared": {"value": false, "score": 4.76837158203125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9995896220207214}, "created": {"value": true, "score": 0.9988622665405273}, "shared": {"value": false, "score": 0.0001812577247619629}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "UDPipe Future+mBERT +", "normalizedForm": "UDPipe Future+mBERT +", "offsetStart": 117, "offsetEnd": 138}, "context": "CamemBERT achieves overall slightly better results than the previous state-of-the-art and task-specific architecture UDPipe Future+mBERT +Flair, except for POS tagging on Sequoia and POS tagging on Spoken, where CamemBERT lags by 0.03% and 0.14% UPOS respectively. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.001154780387878418}, "created": {"value": false, "score": 8.58306884765625e-06}, "shared": {"value": false, "score": 4.76837158203125e-07}}, "documentContextAttributes": {"used": {"value": false, "score": 0.001154780387878418}, "created": {"value": false, "score": 8.58306884765625e-06}, "shared": {"value": false, "score": 4.76837158203125e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "CamemBERT", "normalizedForm": "CamemBERT", "offsetStart": 118, "offsetEnd": 127}, "publisher": {"rawForm": "Hugging Face", "normalizedForm": "Hugging Face"}, "context": "In this section, we describe the pretraining data, architecture, training objective and optimisation setup we use for CamemBERT.", "mentionContextAttributes": {"used": {"value": false, "score": 0.014596819877624512}, "created": {"value": false, "score": 0.21488827466964722}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9995896220207214}, "created": {"value": true, "score": 0.9988622665405273}, "shared": {"value": false, "score": 0.0001812577247619629}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "mBERT", "normalizedForm": "mBERT", "offsetStart": 126, "offsetEnd": 131}, "version": {"rawForm": ".57", "normalizedForm": ".57"}, "context": "\u2022 XLM MLM-TLM : A multilingual pretrained language model from Lample and Conneau (2019), which showed better performance than mBERT on NLI. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.00021308660507202148}, "created": {"value": false, "score": 0.00013744831085205078}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999656677246094}, "created": {"value": false, "score": 0.003240525722503662}, "shared": {"value": false, "score": 5.960464477539062e-07}}, "references": [{"label": "(Devlin et al., 2018)", "normalizedForm": "Devlin et al., 2018", "refKey": 15}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "CamemBERT", "normalizedForm": "CamemBERT", "offsetStart": 126, "offsetEnd": 135}, "publisher": {"rawForm": "Hugging Face", "normalizedForm": "Hugging Face", "offsetStart": 71, "offsetEnd": 83}, "context": "The POS tagging, dependency parsing, and NER experiments are run using Hugging Face's Transformer library extended to support CamemBERT and dependency parsing (Wolf et al., 2019). ", "mentionContextAttributes": {"used": {"value": true, "score": 0.8507935404777527}, "created": {"value": false, "score": 0.00013935565948486328}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9995896220207214}, "created": {"value": true, "score": 0.9988622665405273}, "shared": {"value": false, "score": 0.0001812577247619629}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "mBERT", "normalizedForm": "mBERT", "offsetStart": 132, "offsetEnd": 137}, "version": {"rawForm": ".57", "normalizedForm": ".57"}, "context": "CamemBERT improves on the state of the art in all four tasks compared to previous monolingual and multilingual approaches including mBERT, XLM and XLM-R, which confirms the effectiveness of large pretrained language models for French.", "mentionContextAttributes": {"used": {"value": false, "score": 4.8995018005371094e-05}, "created": {"value": false, "score": 7.343292236328125e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999656677246094}, "created": {"value": false, "score": 0.003240525722503662}, "shared": {"value": false, "score": 5.960464477539062e-07}}, "references": [{"label": "(Devlin et al., 2018)", "normalizedForm": "Devlin et al., 2018", "refKey": 15}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "CamemBERT", "normalizedForm": "CamemBERT", "offsetStart": 133, "offsetEnd": 142}, "publisher": {"rawForm": "Hugging Face", "normalizedForm": "Hugging Face"}, "context": "In the second one, referred to as feature-based embeddings or simply embeddings, we extract frozen contextual embedding vectors from CamemBERT. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.0008938908576965332}, "created": {"value": false, "score": 0.3784739375114441}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9995896220207214}, "created": {"value": true, "score": 0.9988622665405273}, "shared": {"value": false, "score": 0.0001812577247619629}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "CamemBERT", "normalizedForm": "CamemBERT", "offsetStart": 133, "offsetEnd": 142}, "publisher": {"rawForm": "Hugging Face", "normalizedForm": "Hugging Face"}, "context": "Being the first pre-trained language model that used the opensource Common Crawl Oscar corpus and given its impact on the community, CamemBERT paved the way for many works on monolingual language models that followed. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.0012568831443786621}, "created": {"value": false, "score": 0.0017684102058410645}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9995896220207214}, "created": {"value": true, "score": 0.9988622665405273}, "shared": {"value": false, "score": 0.0001812577247619629}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "fairseq", "normalizedForm": "fairseq", "offsetStart": 135, "offsetEnd": 142}, "context": "Pretrained on pure open-source corpora, Camem-BERT is freely available and distributed with the MIT license via popular NLP libraries (fairseq and huggingface) as well as on our website camembert-model.fr.", "mentionContextAttributes": {"used": {"value": false, "score": 0.0060544610023498535}, "created": {"value": false, "score": 0.01594477891921997}, "shared": {"value": true, "score": 0.9271854162216187}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9688633680343628}, "created": {"value": true, "score": 0.9155601859092712}, "shared": {"value": true, "score": 0.9743472933769226}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "CamemBERT", "normalizedForm": "CamemBERT", "offsetStart": 138, "offsetEnd": 147}, "publisher": {"rawForm": "Hugging Face", "normalizedForm": "Hugging Face"}, "context": "In addition to the standard Camem-BERT model with a BASE architecture, we train another model with the LARGE architecture, referred to as CamemBERT LARGE , for a fair comparison with XLM-R LARGE . ", "mentionContextAttributes": {"used": {"value": false, "score": 0.0003502368927001953}, "created": {"value": false, "score": 0.006843268871307373}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9995896220207214}, "created": {"value": true, "score": 0.9988622665405273}, "shared": {"value": false, "score": 0.0001812577247619629}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "code", "normalizedForm": "code", "offsetStart": 143, "offsetEnd": 147}, "url": {"rawForm": "https://github.com/ ufal/acl2019_nested_ner", "normalizedForm": "https://github.com/ ufal/acl2019_nested_ner", "offsetStart": 179, "offsetEnd": 222}, "context": "fairseq/blob/master/examples/roberta/ README.glue.md.10 UDPipe Future is available at https://github. com/CoNLL-UD-2018/UDPipe-Future, and the code for nested NER is available at https://github.com/ ufal/acl2019_nested_ner.", "mentionContextAttributes": {"used": {"value": false, "score": 0.0037519335746765137}, "created": {"value": false, "score": 2.9802322387695312e-06}, "shared": {"value": false, "score": 0.0706106424331665}}, "documentContextAttributes": {"used": {"value": false, "score": 0.46750837564468384}, "created": {"value": true, "score": 0.8837794065475464}, "shared": {"value": false, "score": 0.0706106424331665}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "CamemBERT", "normalizedForm": "CamemBERT", "offsetStart": 151, "offsetEnd": 160}, "publisher": {"rawForm": "Hugging Face", "normalizedForm": "Hugging Face"}, "context": "In that spirit, we make the models used in our experiments available via our website and via the huggingface and fairseq APIs, in addition to the base CamemBERT model.", "mentionContextAttributes": {"used": {"value": false, "score": 0.0003331899642944336}, "created": {"value": true, "score": 0.9155601859092712}, "shared": {"value": false, "score": 0.0001812577247619629}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9995896220207214}, "created": {"value": true, "score": 0.9988622665405273}, "shared": {"value": false, "score": 0.0001812577247619629}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "CamemBERT", "normalizedForm": "CamemBERT", "offsetStart": 155, "offsetEnd": 164}, "publisher": {"rawForm": "Hugging Face", "normalizedForm": "Hugging Face"}, "context": "Supplement: LARGE models XLM-RLARGE (Conneau et al., 2019) 85.2 550M POS tagging and dependency parsing For POS tagging and dependency parsing, we compare CamemBERT with other models in the two settings: fine-tuning and as feature-based embeddings.", "mentionContextAttributes": {"used": {"value": false, "score": 0.0013625025749206543}, "created": {"value": false, "score": 5.125999450683594e-06}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9995896220207214}, "created": {"value": true, "score": 0.9988622665405273}, "shared": {"value": false, "score": 0.0001812577247619629}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "XNLI", "normalizedForm": "XNLI", "offsetStart": 158, "offsetEnd": 162}, "context": "Additionally XLM-R also handles 100 languages, and the authors show that when reducing the number of languages to 7, they can reach 82.5% accuracy for French XNLI with their BASE architecture.", "mentionContextAttributes": {"used": {"value": false, "score": 0.0008618831634521484}, "created": {"value": false, "score": 4.4345855712890625e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999645948410034}, "created": {"value": false, "score": 0.07769900560379028}, "shared": {"value": false, "score": 7.152557373046875e-07}}, "references": [{"label": "(Conneau et al., 2018)", "normalizedForm": "Conneau et al., 2018", "refKey": 12}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "RoBERTa", "normalizedForm": "RoBERTa", "offsetStart": 163, "offsetEnd": 170}, "context": "17 This calls into question the need to use a very large corpus such as OSCAR or CCNet when training a monolingual Transformerbased language model such as BERT or RoBERTa.", "mentionContextAttributes": {"used": {"value": false, "score": 0.0004132390022277832}, "created": {"value": false, "score": 0.00028961896896362305}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9995390176773071}, "created": {"value": true, "score": 0.9962459206581116}, "shared": {"value": false, "score": 1.430511474609375e-06}}, "references": [{"label": "(Liu et al., 2019)", "normalizedForm": "Liu et al., 2019", "refKey": 36}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "CamemBERT", "normalizedForm": "CamemBERT", "offsetStart": 167, "offsetEnd": 176}, "publisher": {"rawForm": "Hugging Face", "normalizedForm": "Hugging Face"}, "context": "Not only does this mean that the computational (and therefore environmental) cost of training a state-of-the-art language model can be reduced, but it also means that CamemBERT-like models can be trained for all languages for which a Common-Crawl-based corpus of 4GB or more can be created.", "mentionContextAttributes": {"used": {"value": false, "score": 0.00029212236404418945}, "created": {"value": false, "score": 0.0006750822067260742}, "shared": {"value": false, "score": 1.5497207641601562e-06}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9995896220207214}, "created": {"value": true, "score": 0.9988622665405273}, "shared": {"value": false, "score": 0.0001812577247619629}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "mBERT", "normalizedForm": "mBERT", "offsetStart": 174, "offsetEnd": 179}, "version": {"rawForm": ".57", "normalizedForm": ".57"}, "context": "Non-English contextualized models Following the success of large pretrained language models, they were extended to the multilingual setting with multilingual BERT (hereafter mBERT) (Devlin et al., 2018), a single multilingual model for 104 different languages trained on Wikipedia data, and later XLM (Lample and Conneau, 2019), which significantly improved unsupervized machine translation.", "mentionContextAttributes": {"used": {"value": false, "score": 0.021662652492523193}, "created": {"value": false, "score": 0.003240525722503662}, "shared": {"value": false, "score": 5.960464477539062e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999656677246094}, "created": {"value": false, "score": 0.003240525722503662}, "shared": {"value": false, "score": 5.960464477539062e-07}}, "references": [{"label": "(Devlin et al., 2018)", "normalizedForm": "Devlin et al., 2018", "refKey": 15, "offsetStart": 5033, "offsetEnd": 5054}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "WordPiece", "normalizedForm": "WordPiece", "offsetStart": 177, "offsetEnd": 186}, "context": "CamemBERT is very similar to RoBERTa, the main difference being the use of whole-word masking and the usage of SentencePiece tokenization (Kudo and Richardson, 2018) instead of WordPiece (Schuster and Nakajima, 2012).", "mentionContextAttributes": {"used": {"value": false, "score": 0.0007841587066650391}, "created": {"value": false, "score": 1.823902130126953e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": false, "score": 0.0007841587066650391}, "created": {"value": false, "score": 0.000258028507232666}, "shared": {"value": false, "score": 8.344650268554688e-07}}, "references": [{"label": "(Schuster and Nakajima, 2012)", "normalizedForm": "Schuster and Nakajima, 2012", "refKey": 54, "offsetStart": 13001, "offsetEnd": 13030}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "mBERT", "normalizedForm": "mBERT", "offsetStart": 179, "offsetEnd": 184}, "version": {"rawForm": ".57", "normalizedForm": ".57", "offsetStart": 175, "offsetEnd": 178}, "context": "We evaluate CamemBERT in the embeddings setting for POS tagging, dependency parsing and NER; using the open-source implementations of Straka et al. (2019)    (Dupont, 2017) 85.57 mBERT (fine-tuned) 87.35", "mentionContextAttributes": {"used": {"value": true, "score": 0.9872889518737793}, "created": {"value": false, "score": 1.2516975402832031e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999656677246094}, "created": {"value": false, "score": 0.003240525722503662}, "shared": {"value": false, "score": 5.960464477539062e-07}}, "references": [{"label": "(Devlin et al., 2018)", "normalizedForm": "Devlin et al., 2018", "refKey": 15}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "UDPipe Future", "normalizedForm": "UDPipe Future", "offsetStart": 182, "offsetEnd": 195}, "url": {"rawForm": "https://github. com/CoNLL-UD-2018/UDPipe-Future", "normalizedForm": "https://github. com/CoNLL-UD-2018/UDPipe-Future"}, "context": "The two approaches achieve similar scores, with a slight advantage for the fine-tuned version of CamemBERT, thus questioning the need for complex task-specific architectures such as UDPipe Future.", "mentionContextAttributes": {"used": {"value": false, "score": 0.00011360645294189453}, "created": {"value": false, "score": 0.00028592348098754883}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": false, "score": 0.0037519335746765137}, "created": {"value": false, "score": 0.00028592348098754883}, "shared": {"value": false, "score": 0.0706106424331665}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "CamemBERT", "normalizedForm": "CamemBERT", "offsetStart": 196, "offsetEnd": 205}, "publisher": {"rawForm": "Hugging Face", "normalizedForm": "Hugging Face"}, "context": "This design choice might explain the difference in score for POS tagging with CamemBERT, especially for the Spoken treebank where words are not capitalized, a factor that might pose a problem for CamemBERT which was trained on capitalized data, but that might be properly handle by Flair on the UDPipe Future+mBERT +Flair model.", "mentionContextAttributes": {"used": {"value": false, "score": 0.14906632900238037}, "created": {"value": false, "score": 3.0994415283203125e-06}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9995896220207214}, "created": {"value": true, "score": 0.9988622665405273}, "shared": {"value": false, "score": 0.0001812577247619629}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "CamemBERT", "normalizedForm": "CamemBERT", "offsetStart": 212, "offsetEnd": 221}, "publisher": {"rawForm": "Hugging Face", "normalizedForm": "Hugging Face"}, "context": "CamemBERT achieves overall slightly better results than the previous state-of-the-art and task-specific architecture UDPipe Future+mBERT +Flair, except for POS tagging on Sequoia and POS tagging on Spoken, where CamemBERT lags by 0.03% and 0.14% UPOS respectively. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.001154780387878418}, "created": {"value": false, "score": 8.58306884765625e-06}, "shared": {"value": false, "score": 4.76837158203125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9995896220207214}, "created": {"value": true, "score": 0.9988622665405273}, "shared": {"value": false, "score": 0.0001812577247619629}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "CamemBERT", "normalizedForm": "CamemBERT", "offsetStart": 215, "offsetEnd": 224}, "publisher": {"rawForm": "Hugging Face", "normalizedForm": "Hugging Face"}, "context": "18 As it was the case for English when BERT was first released, the availability of similar scale language models for French enabled interesting applications, such as large scale anonymization of legal texts, where CamemBERT-based models established a new state-of-the-art on this task (Benesty, 2019), or the first large question answering experiments on a French Squad data set that was released very recently (d'Hoffschmidt et al., 2020) where the authors matched human performance using CamemBERT LARGE .", "mentionContextAttributes": {"used": {"value": false, "score": 0.055317461490631104}, "created": {"value": false, "score": 0.07798123359680176}, "shared": {"value": false, "score": 9.5367431640625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9995896220207214}, "created": {"value": true, "score": 0.9988622665405273}, "shared": {"value": false, "score": 0.0001812577247619629}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "RoBERTa", "normalizedForm": "RoBERTa", "offsetStart": 238, "offsetEnd": 245}, "context": "Trained with language modeling objectives, these approaches range from LSTMbased architectures such as (Dai and Le, 2015), to the successful transformer-based architectures such as GPT2 (Radford et al., 2019), BERT (Devlin et al., 2019), RoBERTa (Liu et al., 2019) and more recently ALBERT (Lan et al., 2019) and T5 (Raffel et al., 2019).", "mentionContextAttributes": {"used": {"value": false, "score": 0.0031322836875915527}, "created": {"value": false, "score": 0.010301411151885986}, "shared": {"value": false, "score": 1.0728836059570312e-06}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9995390176773071}, "created": {"value": true, "score": 0.9962459206581116}, "shared": {"value": false, "score": 1.430511474609375e-06}}, "references": [{"label": "(Liu et al., 2019)", "normalizedForm": "Liu et al., 2019", "refKey": 36, "offsetStart": 4760, "offsetEnd": 4778}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "CamemBERT", "normalizedForm": "CamemBERT", "offsetStart": 245, "offsetEnd": 254}, "publisher": {"rawForm": "Hugging Face", "normalizedForm": "Hugging Face"}, "context": "In order to reproduce and validate results that have so far only been obtained for English, we take advantage of the newly available multilingual corpora OSCAR (Ortiz Su\u00e1rez et al., 2019) to train a monolingual language model for French, dubbed CamemBERT. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.0004922151565551758}, "created": {"value": false, "score": 0.0626448392868042}, "shared": {"value": false, "score": 4.76837158203125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9995896220207214}, "created": {"value": true, "score": 0.9988622665405273}, "shared": {"value": false, "score": 0.0001812577247619629}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "mBERT", "normalizedForm": "mBERT", "offsetStart": 274, "offsetEnd": 279}, "version": {"rawForm": ".57", "normalizedForm": ".57"}, "context": "In other words, when trained on corpora such as OSCAR and CCNet, which are heterogeneous in terms of genre and style, 4GB of uncompressed text is large enough as pretraining corpus to reach state-of-the-art results with the BASE architecure, better than those obtained with mBERT (pretrained on 60GB of text). ", "mentionContextAttributes": {"used": {"value": false, "score": 0.056163668632507324}, "created": {"value": false, "score": 3.6954879760742188e-06}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999656677246094}, "created": {"value": false, "score": 0.003240525722503662}, "shared": {"value": false, "score": 5.960464477539062e-07}}, "references": [{"label": "(Devlin et al., 2018)", "normalizedForm": "Devlin et al., 2018", "refKey": 15}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "UDPipe", "normalizedForm": "UDPipe", "offsetStart": 295, "offsetEnd": 301}, "context": "This design choice might explain the difference in score for POS tagging with CamemBERT, especially for the Spoken treebank where words are not capitalized, a factor that might pose a problem for CamemBERT which was trained on capitalized data, but that might be properly handle by Flair on the UDPipe Future+mBERT +Flair model.", "mentionContextAttributes": {"used": {"value": false, "score": 0.14906632900238037}, "created": {"value": false, "score": 3.0994415283203125e-06}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9930972456932068}, "created": {"value": false, "score": 0.00012981891632080078}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "references": [{"label": "(Straka, 2018)", "normalizedForm": "Straka, 2018", "refKey": 57}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "mBERT", "normalizedForm": "mBERT", "offsetStart": 298, "offsetEnd": 303}, "version": {"rawForm": ".57", "normalizedForm": ".57"}, "context": "We evaluated Camem-BERT on four downstream tasks (part-of-speech tagging, dependency parsing, named entity recognition and natural language inference) in which our best model reached or improved the state of the art in all tasks considered, even when compared to strong multilingual models such as mBERT, XLM and XLM-R, while also having fewer parameters.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9996662139892578}, "created": {"value": false, "score": 1.4185905456542969e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999656677246094}, "created": {"value": false, "score": 0.003240525722503662}, "shared": {"value": false, "score": 5.960464477539062e-07}}, "references": [{"label": "(Devlin et al., 2018)", "normalizedForm": "Devlin et al., 2018", "refKey": 15}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Future", "normalizedForm": "Future", "offsetStart": 302, "offsetEnd": 308}, "context": "This design choice might explain the difference in score for POS tagging with CamemBERT, especially for the Spoken treebank where words are not capitalized, a factor that might pose a problem for CamemBERT which was trained on capitalized data, but that might be properly handle by Flair on the UDPipe Future+mBERT +Flair model.", "mentionContextAttributes": {"used": {"value": false, "score": 0.14906632900238037}, "created": {"value": false, "score": 3.0994415283203125e-06}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": false, "score": 0.14906632900238037}, "created": {"value": false, "score": 0.00012981891632080078}, "shared": {"value": false, "score": 2.384185791015625e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "mBERT", "normalizedForm": "mBERT", "offsetStart": 309, "offsetEnd": 314}, "version": {"rawForm": ".57", "normalizedForm": ".57"}, "context": "This design choice might explain the difference in score for POS tagging with CamemBERT, especially for the Spoken treebank where words are not capitalized, a factor that might pose a problem for CamemBERT which was trained on capitalized data, but that might be properly handle by Flair on the UDPipe Future+mBERT +Flair model.", "mentionContextAttributes": {"used": {"value": false, "score": 0.14906632900238037}, "created": {"value": false, "score": 3.0994415283203125e-06}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999656677246094}, "created": {"value": false, "score": 0.003240525722503662}, "shared": {"value": false, "score": 5.960464477539062e-07}}, "references": [{"label": "(Devlin et al., 2018)", "normalizedForm": "Devlin et al., 2018", "refKey": 15}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "CamemBERT", "normalizedForm": "CamemBERT", "offsetStart": 488, "offsetEnd": 497}, "publisher": {"rawForm": "Hugging Face", "normalizedForm": "Hugging Face"}, "context": "As it was the case for English when BERT was first released, the availability of similar scale language models for French enabled interesting applications, such as large scale anonymization of legal texts, where CamemBERT-based models established a new state-of-the-art on this task (Benesty, 2019), or the first large question answering experiments on a French Squad data set that was released very recently (d'Hoffschmidt et al., 2020) where the authors matched human performance using CamemBERT LARGE . ", "mentionContextAttributes": {"used": {"value": false, "score": 0.05088400840759277}, "created": {"value": false, "score": 0.17670375108718872}, "shared": {"value": false, "score": 1.3113021850585938e-06}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9995896220207214}, "created": {"value": true, "score": 0.9988622665405273}, "shared": {"value": false, "score": 0.0001812577247619629}}}], "references": [{"refKey": 12, "tei": "<biblStruct xml:id=\"b12\">\n\t<analytic>\n\t\t<title level=\"a\" type=\"main\">XNLI: Evaluating Cross-lingual Sentence Representations</title>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Alexis</forename><surname>Conneau</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Ruty</forename><surname>Rinott</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Guillaume</forename><surname>Lample</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Adina</forename><surname>Williams</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Samuel</forename><forename type=\"middle\">R</forename><surname>Bowman</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Holger</forename><surname>Schwenk</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Veselin</forename><surname>Stoyanov</surname></persName>\n\t\t</author>\n\t\t<idno type=\"DOI\">10.18653/v1/d18-1269</idno>\n\t</analytic>\n\t<monogr>\n\t\t<title level=\"m\">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>\n\t\t<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing</meeting>\n\t\t<imprint>\n\t\t\t<publisher>Association for Computational Linguistics</publisher>\n\t\t\t<date type=\"published\" when=\"2018\">2018. October 31 -November 4, 2018</date>\n\t\t\t<biblScope unit=\"page\">2485</biblScope>\n\t\t</imprint>\n\t</monogr>\n</biblStruct>\n"}, {"refKey": 57, "tei": "<biblStruct xml:id=\"b57\">\n\t<analytic>\n\t\t<title level=\"a\" type=\"main\">Tokenizing, POS Tagging, Lemmatizing and Parsing UD 2.0 with UDPipe</title>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Milan</forename><surname>Straka</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Jana</forename><surname>Strakov\u00e1</surname></persName>\n\t\t</author>\n\t\t<idno type=\"DOI\">10.18653/v1/k17-3009</idno>\n\t</analytic>\n\t<monogr>\n\t\t<title level=\"m\">Proceedings of the CoNLL 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies</title>\n\t\t<meeting>the CoNLL 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies</meeting>\n\t\t<imprint>\n\t\t\t<publisher>Association for Computational Linguistics</publisher>\n\t\t\t<date type=\"published\" when=\"2017\">2018</date>\n\t\t\t<biblScope unit=\"page\">207</biblScope>\n\t\t</imprint>\n\t</monogr>\n</biblStruct>\n"}, {"refKey": 36, "tei": "<biblStruct xml:id=\"b36\">\n\t<monogr>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Yinhan</forename><surname>Liu</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Myle</forename><surname>Ott</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Naman</forename><surname>Goyal</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Jingfei</forename><surname>Du</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Mandar</forename><surname>Joshi</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Danqi</forename><surname>Chen</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Omer</forename><surname>Levy</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Mike</forename><surname>Lewis</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Luke</forename><surname>Zettlemoyer</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Veselin</forename><surname>Stoyanov</surname></persName>\n\t\t</author>\n\t\t<idno>1907.11692</idno>\n\t\t<title level=\"m\">Roberta: A robustly optimized BERT pretraining approach</title>\n\t\t<imprint>\n\t\t\t<date>2019</date>\n\t\t</imprint>\n\t</monogr>\n</biblStruct>\n"}, {"refKey": 28, "tei": "<biblStruct xml:id=\"b28\">\n\t<analytic>\n\t\t<title level=\"a\" type=\"main\">SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing</title>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Taku</forename><surname>Kudo</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">John</forename><surname>Richardson</surname></persName>\n\t\t</author>\n\t\t<idno type=\"DOI\">10.18653/v1/d18-2012</idno>\n\t</analytic>\n\t<monogr>\n\t\t<title level=\"m\">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</title>\n\t\t<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</meeting>\n\t\t<imprint>\n\t\t\t<publisher>Association for Computational Linguistics</publisher>\n\t\t\t<date type=\"published\" when=\"2018\">2018. October 31 -November 4, 2018</date>\n\t\t\t<biblScope unit=\"page\">71</biblScope>\n\t\t</imprint>\n\t</monogr>\n</biblStruct>\n"}, {"refKey": 15, "tei": "<biblStruct xml:id=\"b15\">\n\t<monogr>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Jacob</forename><surname>Devlin</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Ming-Wei</forename><surname>Chang</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Kenton</forename><surname>Lee</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Kristina</forename><surname>Toutanova</surname></persName>\n\t\t</author>\n\t\t<title level=\"m\">Multilingual bert</title>\n\t\t<imprint>\n\t\t\t<date>2018</date>\n\t\t</imprint>\n\t</monogr>\n</biblStruct>\n"}, {"refKey": 66, "tei": "<biblStruct xml:id=\"b66\">\n\t<monogr>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Thomas</forename><surname>Wolf</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Lysandre</forename><surname>Debut</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Victor</forename><surname>Sanh</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Julien</forename><surname>Chaumond</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Clement</forename><surname>Delangue</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Anthony</forename><surname>Moi</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Pierric</forename><surname>Cistac</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Tim</forename><surname>Rault</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">R'emi</forename><surname>Louf</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Morgan</forename><surname>Funtowicz</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Jamie</forename><surname>Brew</surname></persName>\n\t\t</author>\n\t\t<idno>1910.03771</idno>\n\t\t<title level=\"m\">Huggingface&apos;s transformers: State-of-the-art natural language processing</title>\n\t\t<imprint>\n\t\t\t<date>2019</date>\n\t\t</imprint>\n\t</monogr>\n</biblStruct>\n"}, {"refKey": 54, "tei": "<biblStruct xml:id=\"b54\">\n\t<analytic>\n\t\t<title level=\"a\" type=\"main\">Japanese and Korean voice search</title>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Mike</forename><surname>Schuster</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Kaisuke</forename><surname>Nakajima</surname></persName>\n\t\t</author>\n\t\t<idno type=\"DOI\">10.1109/icassp.2012.6289079</idno>\n\t</analytic>\n\t<monogr>\n\t\t<title level=\"m\">2012 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>\n\t\t<imprint>\n\t\t\t<publisher>IEEE</publisher>\n\t\t\t<date type=\"published\" when=\"2012-03\">2012</date>\n\t\t\t<biblScope unit=\"page\">5152</biblScope>\n\t\t</imprint>\n\t</monogr>\n</biblStruct>\n"}], "runtime": 15524, "id": "192e73b63aba1b70d9fa5066153e0d2050bdbdf2", "metadata": {"id": "192e73b63aba1b70d9fa5066153e0d2050bdbdf2"}, "original_file_path": "../../datalake/Samuel/SOFTware-Sync/downloads/xml/hal-02889805.grobid.tei.xml", "file_name": "hal-02889805.grobid.tei.xml"}