{"application": "software-mentions", "version": "0.8.0", "date": "2024-10-07T11:44+0000", "md5": "01EAB92D220F778DD44536321458DE7C", "mentions": [{"type": "software", "software-type": "software", "software-name": {"rawForm": "mBERT", "normalizedForm": "mBERT", "offsetStart": 4, "offsetEnd": 9}, "context": "Our mBERT baseline, which is the closest to their configuration, shows that even without any additional data, task-specific fine-tuning already brings significant improvements, while our models refined using our raw corpus of Medieval French bring further improvements, leading to state-of-the-art results that are consistent with their results on the development set.", "mentionContextAttributes": {"used": {"value": false, "score": 0.018143951892852783}, "created": {"value": false, "score": 0.0025767087936401367}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9877892732620239}, "created": {"value": false, "score": 0.0025767087936401367}, "shared": {"value": false, "score": 9.5367431640625e-07}}, "references": [{"label": "(Devlin et al. 2019)", "normalizedForm": "Devlin et al. 2019", "refKey": 10}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "mBERT", "normalizedForm": "mBERT", "offsetStart": 13, "offsetEnd": 18}, "context": "Furthermore, mBERT (Devlin et al. 2019), a model trained on a multilingual corpus which does not include Old French (possibly apart from some fragments in its contemporary French training data), has been shown to be suitable for many languages, and in particular for Indo-European and Romance languages (Straka et al. 2019;Muller et al. 2020).", "mentionContextAttributes": {"used": {"value": false, "score": 0.00067138671875}, "created": {"value": false, "score": 0.0010396242141723633}, "shared": {"value": false, "score": 9.5367431640625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9877892732620239}, "created": {"value": false, "score": 0.0025767087936401367}, "shared": {"value": false, "score": 9.5367431640625e-07}}, "references": [{"label": "(Devlin et al. 2019", "normalizedForm": "(Devlin et al. 2019", "refKey": 10, "offsetStart": 15640, "offsetEnd": 15659}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "mBERT", "normalizedForm": "mBERT", "offsetStart": 21, "offsetEnd": 26}, "context": "This could mean that mBERT benefits from having been pretrained for a wider range of languages, including in particular other Romance languages that share with Old French some features, lost in contemporary French: for instance null subjects.", "mentionContextAttributes": {"used": {"value": false, "score": 0.0007574558258056641}, "created": {"value": false, "score": 9.059906005859375e-05}, "shared": {"value": false, "score": 9.5367431640625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9877892732620239}, "created": {"value": false, "score": 0.0025767087936401367}, "shared": {"value": false, "score": 9.5367431640625e-07}}, "references": [{"label": "(Devlin et al. 2019)", "normalizedForm": "Devlin et al. 2019", "refKey": 10}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "mBERT", "normalizedForm": "mBERT", "offsetStart": 59, "offsetEnd": 64}, "context": "More surprisingly, the best results here are obtained with mBERT. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9877892732620239}, "created": {"value": false, "score": 3.981590270996094e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9877892732620239}, "created": {"value": false, "score": 0.0025767087936401367}, "shared": {"value": false, "score": 9.5367431640625e-07}}, "references": [{"label": "(Devlin et al. 2019)", "normalizedForm": "Devlin et al. 2019", "refKey": 10}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "mBERT", "normalizedForm": "mBERT", "offsetStart": 60, "offsetEnd": 65}, "context": "In a multilingual context, transformer-based models such as mBERT have been adapted to low-resource languages and evaluated in dependency parsing and POS-tagging, showing promising results (Chau et al. 2020;Muller et al. 2020;Gururangan et al. 2020;Z.", "mentionContextAttributes": {"used": {"value": false, "score": 0.0006369948387145996}, "created": {"value": false, "score": 0.0017948150634765625}, "shared": {"value": false, "score": 8.344650268554688e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9877892732620239}, "created": {"value": false, "score": 0.0025767087936401367}, "shared": {"value": false, "score": 9.5367431640625e-07}}, "references": [{"label": "(Devlin et al. 2019)", "normalizedForm": "Devlin et al. 2019", "refKey": 10}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "mBERT", "normalizedForm": "mBERT", "offsetStart": 104, "offsetEnd": 109}, "context": "For less-resourced languages, the most common approach has been to leverage multilingual models such as mBERT (Devlin et al. 2019) Historical languages are typical cases where available linguistic data is limited, with no chance of acquiring new texts.", "mentionContextAttributes": {"used": {"value": false, "score": 0.00027877092361450195}, "created": {"value": false, "score": 0.0008585453033447266}, "shared": {"value": false, "score": 8.344650268554688e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9877892732620239}, "created": {"value": false, "score": 0.0025767087936401367}, "shared": {"value": false, "score": 9.5367431640625e-07}}, "references": [{"label": "(Devlin et al. 2019)", "normalizedForm": "Devlin et al. 2019", "refKey": 10, "offsetStart": 1134, "offsetEnd": 1154}]}], "references": [{"refKey": 10, "tei": "<biblStruct xml:id=\"b10\">\n\t<analytic>\n\t\t<title level=\"a\" type=\"main\">Exploring teachers\u2019 confidence in addressing mental health issues in learners with Profound and Multiple Learning Difficulties (PMLD) pre and post training</title>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">J</forename><surname>Devlin</surname></persName>\n\t\t</author>\n\t\t<idno type=\"DOI\">10.16922/wje.p5</idno>\n\t</analytic>\n\t<monogr>\n\t\t<title level=\"j\">Cylchgrawn Addysg Cymru / Wales Journal of Education</title>\n\t\t<title level=\"j\" type=\"abbrev\">WJE</title>\n\t\t<idno type=\"ISSN\">2059-3708</idno>\n\t\t<idno type=\"ISSNe\">2059-3716</idno>\n\t\t<imprint>\n\t\t\t<biblScope unit=\"volume\">1</biblScope>\n\t\t\t<biblScope unit=\"page\">4186</biblScope>\n\t\t\t<date type=\"published\" when=\"2024-03-28\">June 2019</date>\n\t\t\t<publisher>University of Wales Press/Gwasg Prifysgol Cymru</publisher>\n\t\t</imprint>\n\t</monogr>\n</biblStruct>\n"}], "runtime": 14829, "id": "04b4c25acca8dbef778c3312366795a015433f97", "metadata": {"id": "04b4c25acca8dbef778c3312366795a015433f97"}, "original_file_path": "../../datalake/Samuel/SOFTware-Sync/downloads/xml/hal-03736840.grobid.tei.xml", "file_name": "hal-03736840.grobid.tei.xml"}