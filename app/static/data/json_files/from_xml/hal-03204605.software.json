{"application": "software-mentions", "version": "0.8.0", "date": "2024-10-07T11:37+0000", "md5": "D6228B0BED33BDD5BCD277F35B55CC14", "mentions": [{"type": "software", "software-type": "software", "software-name": {"rawForm": "HatEval", "normalizedForm": "HatEval", "offsetStart": 0, "offsetEnd": 7}, "context": "HatEval shared task presents a challenging test set and similar performance have been reported in prior work (Caselli et al., 2021). ", "mentionContextAttributes": {"used": {"value": false, "score": 0.00042515993118286133}, "created": {"value": false, "score": 8.749961853027344e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999470710754395}, "created": {"value": true, "score": 0.971354067325592}, "shared": {"value": false, "score": 1.430511474609375e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "HateBERT", "normalizedForm": "HateBERT", "offsetStart": 0, "offsetEnd": 8}, "context": "HateBERT MLM Fine-tuning", "mentionContextAttributes": {"used": {"value": true, "score": 0.5340138673782349}, "created": {"value": false, "score": 3.2901763916015625e-05}, "shared": {"value": false, "score": 7.152557373046875e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999910593032837}, "created": {"value": true, "score": 0.9245532155036926}, "shared": {"value": false, "score": 7.152557373046875e-07}}, "references": [{"label": "(Caselli et al., 2021)", "normalizedForm": "Caselli et al., 2021", "refKey": 7}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "HateBERT", "normalizedForm": "HateBERT", "offsetStart": 22, "offsetEnd": 30}, "context": "The recently proposed HateBERT model (Caselli et al., 2021) extends the pre-trained BERT model using the MLM objective over a large corpus of unlabeled abusive comments from Reddit.", "mentionContextAttributes": {"used": {"value": false, "score": 8.058547973632812e-05}, "created": {"value": false, "score": 0.4051981568336487}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999910593032837}, "created": {"value": true, "score": 0.9245532155036926}, "shared": {"value": false, "score": 7.152557373046875e-07}}, "references": [{"label": "(Caselli et al., 2021)", "normalizedForm": "Caselli et al., 2021", "refKey": 7, "offsetStart": 11239, "offsetEnd": 11261}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "HateBERT", "normalizedForm": "HateBERT", "offsetStart": 23, "offsetEnd": 31}, "context": "Model Fine-tuning with HateBERT", "mentionContextAttributes": {"used": {"value": true, "score": 0.9742740988731384}, "created": {"value": false, "score": 1.2636184692382812e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999910593032837}, "created": {"value": true, "score": 0.9245532155036926}, "shared": {"value": false, "score": 7.152557373046875e-07}}, "references": [{"label": "(Caselli et al., 2021)", "normalizedForm": "Caselli et al., 2021", "refKey": 7}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "HateBERT", "normalizedForm": "HateBERT", "offsetStart": 28, "offsetEnd": 36}, "context": "Crosscorpora performance of HateBERT and the UDA models discussed in Section 3.1, is presented in Table 3. Comparing Table 2 andTable 3, substantial degradation of performance is observed across the datasets in the cross-corpora setting.", "mentionContextAttributes": {"used": {"value": false, "score": 0.2014162540435791}, "created": {"value": false, "score": 1.0848045349121094e-05}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999910593032837}, "created": {"value": true, "score": 0.9245532155036926}, "shared": {"value": false, "score": 7.152557373046875e-07}}, "references": [{"label": "(Caselli et al., 2021)", "normalizedForm": "Caselli et al., 2021", "refKey": 7}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "HateBERT", "normalizedForm": "HateBERT", "offsetStart": 30, "offsetEnd": 38}, "context": "Unsupervised adaptation using HateBERT involves training of the HateBERT model on the target corpus train set using the MLM objective. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.00045162439346313477}, "created": {"value": false, "score": 3.075599670410156e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999910593032837}, "created": {"value": true, "score": 0.9245532155036926}, "shared": {"value": false, "score": 7.152557373046875e-07}}, "references": [{"label": "(Caselli et al., 2021)", "normalizedForm": "Caselli et al., 2021", "refKey": 7}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "HateBERT", "normalizedForm": "HateBERT", "offsetStart": 32, "offsetEnd": 40}, "context": "\u2022 We analyze the performance of HateBERT in our cross-corpora evaluation set-up.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9987717270851135}, "created": {"value": false, "score": 0.000888526439666748}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999910593032837}, "created": {"value": true, "score": 0.9245532155036926}, "shared": {"value": false, "score": 7.152557373046875e-07}}, "references": [{"label": "(Caselli et al., 2021)", "normalizedForm": "Caselli et al., 2021", "refKey": 7}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "HateBERT", "normalizedForm": "HateBERT", "offsetStart": 33, "offsetEnd": 41}, "context": "In particular, we begin with the HateBERT model and perform MLM fine-tuning incorporating the unlabeled train set from the target corpus.", "mentionContextAttributes": {"used": {"value": true, "score": 0.7171086072921753}, "created": {"value": true, "score": 0.8601285219192505}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999910593032837}, "created": {"value": true, "score": 0.9245532155036926}, "shared": {"value": false, "score": 7.152557373046875e-07}}, "references": [{"label": "(Caselli et al., 2021)", "normalizedForm": "Caselli et al., 2021", "refKey": 7}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "HateBERT", "normalizedForm": "HateBERT", "offsetStart": 33, "offsetEnd": 41}, "context": "In the \"no adaptation\" case, the HateBERT model is fine-tuned in a supervised manner on the labeled source corpus train set, and evaluated on the target test set.", "mentionContextAttributes": {"used": {"value": true, "score": 0.8067470192909241}, "created": {"value": false, "score": 1.8835067749023438e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999910593032837}, "created": {"value": true, "score": 0.9245532155036926}, "shared": {"value": false, "score": 7.152557373046875e-07}}, "references": [{"label": "(Caselli et al., 2021)", "normalizedForm": "Caselli et al., 2021", "refKey": 7}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "HatEval", "normalizedForm": "HatEval", "offsetStart": 35, "offsetEnd": 42}, "context": "One of the primary reasons is that HatEval captures wider forms of abuse directed towards both immigrants and women.", "mentionContextAttributes": {"used": {"value": false, "score": 0.0011383295059204102}, "created": {"value": false, "score": 0.0004070401191711426}, "shared": {"value": false, "score": 9.5367431640625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999470710754395}, "created": {"value": true, "score": 0.971354067325592}, "shared": {"value": false, "score": 1.430511474609375e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "HatEval", "normalizedForm": "HatEval", "offsetStart": 41, "offsetEnd": 48}, "context": "In general, when models are trained over HatEval, they are found to be more robust towards addressing the shifts across corpora.", "mentionContextAttributes": {"used": {"value": false, "score": 0.279476523399353}, "created": {"value": false, "score": 0.0002967715263366699}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999470710754395}, "created": {"value": true, "score": 0.971354067325592}, "shared": {"value": false, "score": 1.430511474609375e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "HateBERT", "normalizedForm": "HateBERT", "offsetStart": 42, "offsetEnd": 50}, "context": "It is shown by Caselli et al. (2021) that HateBERT is more portable across abusive language datasets, as compared to BERT. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.00027245283126831055}, "created": {"value": false, "score": 2.0742416381835938e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999910593032837}, "created": {"value": true, "score": 0.9245532155036926}, "shared": {"value": false, "score": 7.152557373046875e-07}}, "references": [{"label": "(Caselli et al., 2021)", "normalizedForm": "Caselli et al., 2021", "refKey": 7}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "HateBERT", "normalizedForm": "HateBERT", "offsetStart": 47, "offsetEnd": 55}, "context": "Furthermore, we perform the MLM fine-tuning of HateBERT on target corpus, which can be considered a form of unsupervised adaptation.", "mentionContextAttributes": {"used": {"value": true, "score": 0.756385862827301}, "created": {"value": false, "score": 0.038866400718688965}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999910593032837}, "created": {"value": true, "score": 0.9245532155036926}, "shared": {"value": false, "score": 7.152557373046875e-07}}, "references": [{"label": "(Caselli et al., 2021)", "normalizedForm": "Caselli et al., 2021", "refKey": 7}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "HatEval", "normalizedForm": "HatEval", "offsetStart": 48, "offsetEnd": 55}, "context": "Best case: The best case in PBLM corresponds to HatEval \u2192Davidson.", "mentionContextAttributes": {"used": {"value": false, "score": 0.18084049224853516}, "created": {"value": false, "score": 4.0531158447265625e-06}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999470710754395}, "created": {"value": true, "score": 0.971354067325592}, "shared": {"value": false, "score": 1.430511474609375e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "HateBERT", "normalizedForm": "HateBERT", "offsetStart": 50, "offsetEnd": 58}, "context": "We, thus, decide to perform further analysis over HateBERT for our task.", "mentionContextAttributes": {"used": {"value": true, "score": 0.6776348352432251}, "created": {"value": true, "score": 0.8699519038200378}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999910593032837}, "created": {"value": true, "score": 0.9245532155036926}, "shared": {"value": false, "score": 7.152557373046875e-07}}, "references": [{"label": "(Caselli et al., 2021)", "normalizedForm": "Caselli et al., 2021", "refKey": 7}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "HateBERT", "normalizedForm": "HateBERT", "offsetStart": 50, "offsetEnd": 58}, "context": "We first present the in-corpus performance of the HateBERT model in Table 2, obtained after supervised fine-tuning on the respective datasets, along with the frequent abuse-related words.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9359340071678162}, "created": {"value": true, "score": 0.9245532155036926}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999910593032837}, "created": {"value": true, "score": 0.9245532155036926}, "shared": {"value": false, "score": 7.152557373046875e-07}}, "references": [{"label": "(Caselli et al., 2021)", "normalizedForm": "Caselli et al., 2021", "refKey": 7}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "HateBERT", "normalizedForm": "HateBERT", "offsetStart": 51, "offsetEnd": 59}, "context": "However, the additional step of MLM fine-tuning of HateBERT on the unlabeled train set from target corpus results in an improved performance in most of the cases.", "mentionContextAttributes": {"used": {"value": false, "score": 0.0005105733871459961}, "created": {"value": false, "score": 0.00013387203216552734}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999910593032837}, "created": {"value": true, "score": 0.9245532155036926}, "shared": {"value": false, "score": 7.152557373046875e-07}}, "references": [{"label": "(Caselli et al., 2021)", "normalizedForm": "Caselli et al., 2021", "refKey": 7}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "HateBERT", "normalizedForm": "HateBERT", "offsetStart": 54, "offsetEnd": 62}, "context": "However, it stills remains behind the best performing HateBERT model with MLM fine-tuning on target.", "mentionContextAttributes": {"used": {"value": false, "score": 0.0011380910873413086}, "created": {"value": false, "score": 0.00016021728515625}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999910593032837}, "created": {"value": true, "score": 0.9245532155036926}, "shared": {"value": false, "score": 7.152557373046875e-07}}, "references": [{"label": "(Caselli et al., 2021)", "normalizedForm": "Caselli et al., 2021", "refKey": 7}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "HateBERT", "normalizedForm": "HateBERT", "offsetStart": 55, "offsetEnd": 63}, "context": "It is evident from Table 3 that the MLM fine-tuning of HateBERT, before the subsequent supervised fine-tuning over the source corpus, results in improved performance in majority of the cases.", "mentionContextAttributes": {"used": {"value": false, "score": 0.020306169986724854}, "created": {"value": false, "score": 1.919269561767578e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999910593032837}, "created": {"value": true, "score": 0.9245532155036926}, "shared": {"value": false, "score": 7.152557373046875e-07}}, "references": [{"label": "(Caselli et al., 2021)", "normalizedForm": "Caselli et al., 2021", "refKey": 7}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "HateBERT", "normalizedForm": "HateBERT", "offsetStart": 57, "offsetEnd": 65}, "context": "The analysis of the Masked Language Model fine-tuning of HateBERT on the target corpus displayed improvements in general as compared to only fine-tuning HateBERT over the source corpus, suggesting that it helps in adapting the model towards target-specific language variations.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9994422793388367}, "created": {"value": false, "score": 0.0001361370086669922}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999910593032837}, "created": {"value": true, "score": 0.9245532155036926}, "shared": {"value": false, "score": 7.152557373046875e-07}}, "references": [{"label": "(Caselli et al., 2021)", "normalizedForm": "Caselli et al., 2021", "refKey": 7}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Crazy-Tokenizer", "normalizedForm": "Crazy-Tokenizer", "offsetStart": 59, "offsetEnd": 74}, "context": "1 The words contained in hashtags are split using the tool Crazy-Tokenizer 2 and the words are converted into lowercase.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9579627513885498}, "created": {"value": false, "score": 1.1563301086425781e-05}, "shared": {"value": false, "score": 4.76837158203125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9579627513885498}, "created": {"value": false, "score": 1.1563301086425781e-05}, "shared": {"value": false, "score": 4.76837158203125e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "HateBERT", "normalizedForm": "HateBERT", "offsetStart": 64, "offsetEnd": 72}, "context": "Unsupervised adaptation using HateBERT involves training of the HateBERT model on the target corpus train set using the MLM objective.", "mentionContextAttributes": {"used": {"value": false, "score": 0.00045162439346313477}, "created": {"value": false, "score": 3.075599670410156e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999910593032837}, "created": {"value": true, "score": 0.9245532155036926}, "shared": {"value": false, "score": 7.152557373046875e-07}}, "references": [{"label": "(Caselli et al., 2021)", "normalizedForm": "Caselli et al., 2021", "refKey": 7}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "HateBERT", "normalizedForm": "HateBERT", "offsetStart": 76, "offsetEnd": 84}, "context": "We use the original implementations of the UDA models 3 and the pre-trained HateBERT 4 model for our experiments.", "mentionContextAttributes": {"used": {"value": true, "score": 0.8774465322494507}, "created": {"value": false, "score": 0.030771374702453613}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999910593032837}, "created": {"value": true, "score": 0.9245532155036926}, "shared": {"value": false, "score": 7.152557373046875e-07}}, "references": [{"label": "(Caselli et al., 2021)", "normalizedForm": "Caselli et al., 2021", "refKey": 7}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "HatEval", "normalizedForm": "HatEval", "offsetStart": 92, "offsetEnd": 99}, "context": "As shown in Table 2, the in-corpus performance is high for Davidson and Waseem, but not for HatEval. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.26686251163482666}, "created": {"value": false, "score": 4.935264587402344e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999470710754395}, "created": {"value": true, "score": 0.971354067325592}, "shared": {"value": false, "score": 1.430511474609375e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "HateBERT", "normalizedForm": "HateBERT", "offsetStart": 94, "offsetEnd": 102}, "context": "F1 macro-average (mean \u00b1 std-dev) for incorpus classification using supervised fine-tuning of HateBERT.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9999910593032837}, "created": {"value": false, "score": 1.9073486328125e-06}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999910593032837}, "created": {"value": true, "score": 0.9245532155036926}, "shared": {"value": false, "score": 7.152557373046875e-07}}, "references": [{"label": "(Caselli et al., 2021)", "normalizedForm": "Caselli et al., 2021", "refKey": 7}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "HatEval", "normalizedForm": "HatEval", "offsetStart": 114, "offsetEnd": 121}, "context": "We randomly split Davidson and Waseem into train (80%), development (10%), and test (10%), whereas in the case of HatEval, we use the standard partition of the shared task. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9999470710754395}, "created": {"value": false, "score": 4.887580871582031e-06}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999470710754395}, "created": {"value": true, "score": 0.971354067325592}, "shared": {"value": false, "score": 1.430511474609375e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "HateBERT", "normalizedForm": "HateBERT", "offsetStart": 153, "offsetEnd": 161}, "context": "The analysis of the Masked Language Model fine-tuning of HateBERT on the target corpus displayed improvements in general as compared to only fine-tuning HateBERT over the source corpus, suggesting that it helps in adapting the model towards target-specific language variations.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9994422793388367}, "created": {"value": false, "score": 0.0001361370086669922}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999910593032837}, "created": {"value": true, "score": 0.9245532155036926}, "shared": {"value": false, "score": 7.152557373046875e-07}}, "references": [{"label": "(Caselli et al., 2021)", "normalizedForm": "Caselli et al., 2021", "refKey": 7}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "HateBERT", "normalizedForm": "HateBERT", "offsetStart": 175, "offsetEnd": 183}, "context": "Cross-corpora evaluation in Table 3 shows that all the UDA methods experience drop in average performance when compared to the no-adaptation case of supervised fine-tuning of HateBERT.", "mentionContextAttributes": {"used": {"value": true, "score": 0.8697358965873718}, "created": {"value": false, "score": 6.198883056640625e-06}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999910593032837}, "created": {"value": true, "score": 0.9245532155036926}, "shared": {"value": false, "score": 7.152557373046875e-07}}, "references": [{"label": "(Caselli et al., 2021)", "normalizedForm": "Caselli et al., 2021", "refKey": 7}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "HatEval", "normalizedForm": "HatEval", "offsetStart": 237, "offsetEnd": 244}, "context": "4 Experimental Setup We present experiments over three different publicly available abusive language corpora from Twitter as they cover different forms of abuse, namely Davidson (Davidson et al., 2017),Waseem (Waseem and Hovy, 2016) and HatEval (Basile et al., 2019).", "mentionContextAttributes": {"used": {"value": false, "score": 0.001425027847290039}, "created": {"value": true, "score": 0.971354067325592}, "shared": {"value": false, "score": 1.430511474609375e-06}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999470710754395}, "created": {"value": true, "score": 0.971354067325592}, "shared": {"value": false, "score": 1.430511474609375e-06}}}], "references": [{"refKey": 7, "tei": "<biblStruct xml:id=\"b7\">\n\t<monogr>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Tommaso</forename><surname>Caselli</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Valerio</forename><surname>Basile</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Jelena</forename><surname>Mitrovi\u0107</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Michael</forename><surname>Granitzer</surname></persName>\n\t\t</author>\n\t\t<idno>arXiv:2010.12472</idno>\n\t\t<title level=\"m\">Hatebert: Retraining bert for abusive language detection in english</title>\n\t\t<imprint>\n\t\t\t<date>2021</date>\n\t\t</imprint>\n\t</monogr>\n</biblStruct>\n"}], "runtime": 21165, "id": "3a6f79f79ad59521f656e3d273459d0747ec143c", "metadata": {"id": "3a6f79f79ad59521f656e3d273459d0747ec143c"}, "original_file_path": "../../datalake/Samuel/SOFTware-Sync/downloads/xml/hal-03204605.grobid.tei.xml", "file_name": "hal-03204605.grobid.tei.xml"}