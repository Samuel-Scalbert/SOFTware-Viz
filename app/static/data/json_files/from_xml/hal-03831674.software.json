{"application": "software-mentions", "version": "0.8.0", "date": "2024-10-07T12:07+0000", "md5": "E214A4B85E34B4BEAD70BBEE29560A68", "mentions": [{"type": "software", "software-type": "software", "software-name": {"rawForm": "pyannote", "normalizedForm": "pyannote", "offsetStart": 21, "offsetEnd": 29}, "context": "IER is obtained with pyannote.metrics", "mentionContextAttributes": {"used": {"value": true, "score": 0.9647032618522644}, "created": {"value": false, "score": 2.574920654296875e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9988609552383423}, "created": {"value": true, "score": 0.575087308883667}, "shared": {"value": false, "score": 2.384185791015625e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "audio", "normalizedForm": "audio", "offsetStart": 24, "offsetEnd": 29}, "context": "It can be modeled as an audio sequence labeling task.", "mentionContextAttributes": {"used": {"value": false, "score": 3.337860107421875e-05}, "created": {"value": false, "score": 0.01429903507232666}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999889135360718}, "created": {"value": true, "score": 0.575087308883667}, "shared": {"value": false, "score": 2.384185791015625e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "audio", "normalizedForm": "audio", "offsetStart": 29, "offsetEnd": 34}, "context": "It can aslo be modeled as an audio sequence labeling task.", "mentionContextAttributes": {"used": {"value": false, "score": 3.5762786865234375e-05}, "created": {"value": false, "score": 0.005886852741241455}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999889135360718}, "created": {"value": true, "score": 0.575087308883667}, "shared": {"value": false, "score": 2.384185791015625e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Seshat", "normalizedForm": "Seshat", "offsetStart": 36, "offsetEnd": 65}, "context": "The speech data were annotated with Seshat (Titeux* et al., 2020) and Praat (Boersma et al., 2002) softwares. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9999698400497437}, "created": {"value": false, "score": 2.6226043701171875e-06}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999698400497437}, "created": {"value": false, "score": 2.6226043701171875e-06}, "shared": {"value": false, "score": 1.1920928955078125e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "pyannote", "normalizedForm": "pyannote", "offsetStart": 56, "offsetEnd": 64}, "context": "When trained from scratch, the training is done for 200 pyannote epochs and the model is selected on the Meta-dev M dev .", "mentionContextAttributes": {"used": {"value": false, "score": 0.4756777882575989}, "created": {"value": false, "score": 0.0006452202796936035}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9988609552383423}, "created": {"value": true, "score": 0.575087308883667}, "shared": {"value": false, "score": 2.384185791015625e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "pyannote", "normalizedForm": "pyannote", "offsetStart": 69, "offsetEnd": 77}, "context": "We extended the speech processing toolkit from (Bredin et al., 2020) pyannote.audio to run our experiments. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9988609552383423}, "created": {"value": true, "score": 0.575087308883667}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9988609552383423}, "created": {"value": true, "score": 0.575087308883667}, "shared": {"value": false, "score": 2.384185791015625e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Praat", "normalizedForm": "Praat", "offsetStart": 70, "offsetEnd": 98}, "context": "The speech data were annotated with Seshat (Titeux* et al., 2020) and Praat (Boersma et al., 2002) softwares. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9999698400497437}, "created": {"value": false, "score": 2.6226043701171875e-06}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999698400497437}, "created": {"value": false, "score": 2.6226043701171875e-06}, "shared": {"value": false, "score": 1.1920928955078125e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "audio", "normalizedForm": "audio", "offsetStart": 78, "offsetEnd": 83}, "context": "We extended the speech processing toolkit from (Bredin et al., 2020) pyannote.audio to run our experiments. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9988609552383423}, "created": {"value": true, "score": 0.575087308883667}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999889135360718}, "created": {"value": true, "score": 0.575087308883667}, "shared": {"value": false, "score": 2.384185791015625e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "audio", "normalizedForm": "audio", "offsetStart": 78, "offsetEnd": 83}, "context": "First, a number of studies are trying to solve this problem directly from the audio signal and linguistic outputs, also referred to as Speaker Role Recognition.", "mentionContextAttributes": {"used": {"value": false, "score": 0.0005424618721008301}, "created": {"value": false, "score": 0.1558060646057129}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999889135360718}, "created": {"value": true, "score": 0.575087308883667}, "shared": {"value": false, "score": 2.384185791015625e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "audio", "normalizedForm": "audio", "offsetStart": 93, "offsetEnd": 98}, "context": "The first step is the Voice Activity Detection (VAD), i.e. obtain the speech segments in the audio signal.", "mentionContextAttributes": {"used": {"value": false, "score": 0.013857901096343994}, "created": {"value": false, "score": 0.00016307830810546875}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999889135360718}, "created": {"value": true, "score": 0.575087308883667}, "shared": {"value": false, "score": 2.384185791015625e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "audio", "normalizedForm": "audio", "offsetStart": 134, "offsetEnd": 139}, "context": "To solve and model this task, we used SincNet filters (Ravanelli and Bengio, 2018) to obtain adapted speech features vectors from the audio signal.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9999889135360718}, "created": {"value": false, "score": 0.005937039852142334}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999889135360718}, "created": {"value": true, "score": 0.575087308883667}, "shared": {"value": false, "score": 2.384185791015625e-07}}}], "references": [], "runtime": 11095, "id": "2cb0e6fdb0c527832fc39c8dee6f200ea468e666", "metadata": {"id": "2cb0e6fdb0c527832fc39c8dee6f200ea468e666"}, "original_file_path": "../../datalake/Samuel/SOFTware-Sync/downloads/xml/hal-03831674.grobid.tei.xml", "file_name": "hal-03831674.grobid.tei.xml"}