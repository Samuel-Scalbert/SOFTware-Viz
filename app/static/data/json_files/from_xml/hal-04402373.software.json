{"application": "software-mentions", "version": "0.8.0", "date": "2024-10-07T11:52+0000", "md5": "E756666DEC89A9F9FCE7405097E3A17B", "mentions": [{"type": "software", "software-type": "software", "software-name": {"rawForm": "sW", "normalizedForm": "sW", "offsetStart": 0, "offsetEnd": 2}, "context": "sW U GGY and sBLIM P are zero-shot tasks to evaluate spoken language models introduced in the Zerospeech Challenge 2021 (Nguyen et al., 2020):.", "mentionContextAttributes": {"used": {"value": false, "score": 0.0005170106887817383}, "created": {"value": false, "score": 0.0003979802131652832}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": false, "score": 0.23337161540985107}, "created": {"value": false, "score": 0.0003979802131652832}, "shared": {"value": false, "score": 1.5497207641601562e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "sW", "normalizedForm": "sW", "offsetStart": 0, "offsetEnd": 2}, "context": "sW U GGY is a list of pairs of word/non-word synthesized with the Google TTS API and filtered for the words that are in the Lib-riSpeech training set.", "mentionContextAttributes": {"used": {"value": false, "score": 0.23337161540985107}, "created": {"value": false, "score": 0.00012040138244628906}, "shared": {"value": false, "score": 1.5497207641601562e-06}}, "documentContextAttributes": {"used": {"value": false, "score": 0.23337161540985107}, "created": {"value": false, "score": 0.0003979802131652832}, "shared": {"value": false, "score": 1.5497207641601562e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "sBLIM", "normalizedForm": "sBLIM", "offsetStart": 0, "offsetEnd": 5}, "context": "sBLIM P is a list of pairs of syntactically correct/incorrect synthesized sentences. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.1760677695274353}, "created": {"value": false, "score": 3.1828880310058594e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": false, "score": 0.1760677695274353}, "created": {"value": false, "score": 0.0003979802131652832}, "shared": {"value": false, "score": 2.384185791015625e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "tGSLM", "normalizedForm": "tGSLM", "offsetStart": 0, "offsetEnd": 5}, "context": "tGSLM is a LM that learns to generate speech sentences by predicting its training data.", "mentionContextAttributes": {"used": {"value": false, "score": 4.76837158203125e-05}, "created": {"value": false, "score": 0.0011755824089050293}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999090433120728}, "created": {"value": true, "score": 0.9995833039283752}, "shared": {"value": false, "score": 1.0728836059570312e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "CrowdMOS", "normalizedForm": "CrowdMOS", "offsetStart": 4, "offsetEnd": 12}, "context": "The CrowdMOS package (Ribeiro et al., 2011) was used with the recommended recipes for detecting and discarding inaccurate scores. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9956445693969727}, "created": {"value": false, "score": 2.9921531677246094e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9956445693969727}, "created": {"value": false, "score": 2.9921531677246094e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "references": [{"label": "(Ribeiro et al., 2011)", "normalizedForm": "Ribeiro et al., 2011", "refKey": 59, "offsetStart": 21523, "offsetEnd": 21545}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "WaveGlow", "normalizedForm": "WaveGlow", "offsetStart": 4, "offsetEnd": 12}, "context": "and WaveGlow. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.008113086223602295}, "created": {"value": false, "score": 0.0009794235229492188}, "shared": {"value": false, "score": 1.430511474609375e-06}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9998781681060791}, "created": {"value": false, "score": 0.0009794235229492188}, "shared": {"value": false, "score": 1.430511474609375e-06}}, "references": [{"label": "(Prenger et al., 2018)", "normalizedForm": "Prenger et al., 2018", "refKey": 55}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Tacotron", "normalizedForm": "Tacotron", "offsetStart": 4, "offsetEnd": 12}, "version": {"rawForm": "2.0", "normalizedForm": "2.0", "offsetStart": 12, "offsetEnd": 15}, "context": "The Tacotron2.0 from Lakhotia et al. (2021); Kharitonov et al. (2022) was trained on LJ.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9993294477462769}, "created": {"value": false, "score": 2.5033950805664062e-05}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9994226694107056}, "created": {"value": false, "score": 0.4285791516304016}, "shared": {"value": false, "score": 8.344650268554688e-07}}, "references": [{"label": "Lakhotia et al. (2021)", "normalizedForm": "Lakhotia et al. (2021)", "refKey": 40, "offsetStart": 40426, "offsetEnd": 40448}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "sW", "normalizedForm": "sW", "offsetStart": 5, "offsetEnd": 7}, "context": "Both sW U GGY and sBLIM P require the spoken LM to attribute a higher probability to the correct element in each pair. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.13290584087371826}, "created": {"value": false, "score": 2.384185791015625e-06}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": false, "score": 0.23337161540985107}, "created": {"value": false, "score": 0.0003979802131652832}, "shared": {"value": false, "score": 1.5497207641601562e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "tGSLM", "normalizedForm": "tGSLM", "offsetStart": 5, "offsetEnd": 10}, "context": "Once tGSLM training is done, we use it to generate spoken sentences.", "mentionContextAttributes": {"used": {"value": false, "score": 0.20492017269134521}, "created": {"value": false, "score": 0.00047338008880615234}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999090433120728}, "created": {"value": true, "score": 0.9995833039283752}, "shared": {"value": false, "score": 1.0728836059570312e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "tGSLM", "normalizedForm": "tGSLM", "offsetStart": 6, "offsetEnd": 11}, "context": "200ms-tGSLM should expect a memory reduction by a factor of \u03d5(16,1500,1024)  \u03d5(16,300,1024) \u2248 5.83 compared to GSLM.", "mentionContextAttributes": {"used": {"value": false, "score": 0.031538546085357666}, "created": {"value": false, "score": 2.7418136596679688e-06}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999090433120728}, "created": {"value": true, "score": 0.9995833039283752}, "shared": {"value": false, "score": 1.0728836059570312e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Adam", "normalizedForm": "Adam", "offsetStart": 7, "offsetEnd": 11}, "context": "We use Adam optimizer with a weight decay of 0.1.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9999768733978271}, "created": {"value": false, "score": 1.9073486328125e-06}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999768733978271}, "created": {"value": false, "score": 2.384185791015625e-06}, "shared": {"value": false, "score": 2.384185791015625e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "tGSLM", "normalizedForm": "tGSLM", "offsetStart": 7, "offsetEnd": 12}, "context": "First, tGSLM is trained on 32 Nvidia V100-32Go GPUs for 30 hours. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.5494699478149414}, "created": {"value": false, "score": 0.00018662214279174805}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999090433120728}, "created": {"value": true, "score": 0.9995833039283752}, "shared": {"value": false, "score": 1.0728836059570312e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "tGSLM", "normalizedForm": "tGSLM", "offsetStart": 9, "offsetEnd": 14}, "context": "Finally, tGSLM continuations will not preserve any regional accentuation from the prompt, as our model only generates speech in the voice of the single speaker of the LJ dataset.", "mentionContextAttributes": {"used": {"value": false, "score": 0.00021582841873168945}, "created": {"value": false, "score": 0.0004394650459289551}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999090433120728}, "created": {"value": true, "score": 0.9995833039283752}, "shared": {"value": false, "score": 1.0728836059570312e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "FAISS", "normalizedForm": "FAISS", "offsetStart": 10, "offsetEnd": 15}, "context": "Using the FAISS library (Johnson et al., 2017), we index (l i ) i\u2264N into a k-NN graph called the lexical space. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9979422688484192}, "created": {"value": false, "score": 2.6226043701171875e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9979422688484192}, "created": {"value": false, "score": 2.6226043701171875e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "references": [{"label": "(Johnson et al., 2017)", "normalizedForm": "Johnson et al., 2017", "refKey": 26, "offsetStart": 17957, "offsetEnd": 17979}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "tGSLM", "normalizedForm": "tGSLM", "offsetStart": 11, "offsetEnd": 16}, "context": "Therefore, tGSLM inherits from ethical concerns associated with text-based LM, speech encoders and speech synthesizers.", "mentionContextAttributes": {"used": {"value": false, "score": 0.00046575069427490234}, "created": {"value": false, "score": 0.000258028507232666}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999090433120728}, "created": {"value": true, "score": 0.9995833039283752}, "shared": {"value": false, "score": 1.0728836059570312e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "tGSLM", "normalizedForm": "tGSLM", "offsetStart": 11, "offsetEnd": 16}, "context": "Second, if tGSLM is used to continue a speech prompt, the continuation might be inconsistent for accents of underrepresented groups in the training data.", "mentionContextAttributes": {"used": {"value": false, "score": 0.04569631814956665}, "created": {"value": false, "score": 1.3470649719238281e-05}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999090433120728}, "created": {"value": true, "score": 0.9995833039283752}, "shared": {"value": false, "score": 1.0728836059570312e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "tGSLM", "normalizedForm": "tGSLM", "offsetStart": 11, "offsetEnd": 16}, "context": "Therefore, tGSLM cost (12nd + 2nd)L + 5nd + d * 100000.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9998658895492554}, "created": {"value": false, "score": 4.410743713378906e-06}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999090433120728}, "created": {"value": true, "score": 0.9995833039283752}, "shared": {"value": false, "score": 1.0728836059570312e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "sBLIM", "normalizedForm": "sBLIM", "offsetStart": 13, "offsetEnd": 18}, "context": "sW U GGY and sBLIM P are zero-shot tasks to evaluate spoken language models introduced in the Zerospeech Challenge 2021 (Nguyen et al., 2020):. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.0005170106887817383}, "created": {"value": false, "score": 0.0003979802131652832}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": false, "score": 0.1760677695274353}, "created": {"value": false, "score": 0.0003979802131652832}, "shared": {"value": false, "score": 2.384185791015625e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "LJ", "normalizedForm": "LJ", "offsetStart": 14, "offsetEnd": 16}, "context": "Generation at LJ-VERT But you have been wanting to teach me all her life in the world of her own healthy health and she has her fathers abilities an your pride.", "mentionContextAttributes": {"used": {"value": false, "score": 0.0010041594505310059}, "created": {"value": false, "score": 0.004245400428771973}, "shared": {"value": false, "score": 4.0531158447265625e-06}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999603033065796}, "created": {"value": false, "score": 0.004245400428771973}, "shared": {"value": false, "score": 4.0531158447265625e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "tGSLM", "normalizedForm": "tGSLM", "offsetStart": 15, "offsetEnd": 20}, "context": "Training 200ms-tGSLM on Libri-light 60k(Kahn et al., 2019), a larger but noisier corpus, slightly undermined the performance.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9999090433120728}, "created": {"value": false, "score": 5.125999450683594e-06}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999090433120728}, "created": {"value": true, "score": 0.9995833039283752}, "shared": {"value": false, "score": 1.0728836059570312e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Tacotron", "normalizedForm": "Tacotron", "offsetStart": 16, "offsetEnd": 24}, "version": {"rawForm": "2.0", "normalizedForm": "2.0"}, "context": "2022) trained a Tacotron2.0", "mentionContextAttributes": {"used": {"value": true, "score": 0.9718167185783386}, "created": {"value": false, "score": 8.809566497802734e-05}, "shared": {"value": false, "score": 8.344650268554688e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9994226694107056}, "created": {"value": false, "score": 0.4285791516304016}, "shared": {"value": false, "score": 8.344650268554688e-07}}, "references": [{"label": "Shen et al. (2017)", "normalizedForm": "Shen et al. (2017)", "refKey": 64}, {"label": "Shen et al. (2017)", "normalizedForm": "Shen et al. (2017)", "refKey": 64}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "tGSLM", "normalizedForm": "tGSLM", "offsetStart": 17, "offsetEnd": 22}, "context": "The topline gold-tGSLM produces much lower perplexities than GSLM and 200ms-tGSLM.", "mentionContextAttributes": {"used": {"value": false, "score": 0.0008502006530761719}, "created": {"value": false, "score": 5.7220458984375e-06}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999090433120728}, "created": {"value": true, "score": 0.9995833039283752}, "shared": {"value": false, "score": 1.0728836059570312e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "tGSLM", "normalizedForm": "tGSLM", "offsetStart": 17, "offsetEnd": 22}, "context": "The topline gold-tGSLM, once again gets much stronger results.", "mentionContextAttributes": {"used": {"value": false, "score": 0.0013731718063354492}, "created": {"value": false, "score": 2.3484230041503906e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999090433120728}, "created": {"value": true, "score": 0.9995833039283752}, "shared": {"value": false, "score": 1.0728836059570312e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "sBLIM", "normalizedForm": "sBLIM", "offsetStart": 18, "offsetEnd": 23}, "context": "Both sW U GGY and sBLIM P require the spoken LM to attribute a higher probability to the correct element in each pair. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.13290584087371826}, "created": {"value": false, "score": 2.384185791015625e-06}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": false, "score": 0.1760677695274353}, "created": {"value": false, "score": 0.0003979802131652832}, "shared": {"value": false, "score": 2.384185791015625e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "LibriSpeech", "normalizedForm": "LibriSpeech", "offsetStart": 19, "offsetEnd": 30}, "context": "Let us segment the LibriSpeech corpus every 200ms and embed the speech segments with both SSE models so that we get two collections of acoustic tokens: (a mf cc i ) i\u2264N and (a w2v2 i ) i\u2264N .", "mentionContextAttributes": {"used": {"value": true, "score": 0.79320228099823}, "created": {"value": false, "score": 2.2172927856445312e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999727010726929}, "created": {"value": false, "score": 0.0003751516342163086}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "references": [{"label": "(Panayotov et al., 2015)", "normalizedForm": "Panayotov et al., 2015", "refKey": 49}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "tGSLM", "normalizedForm": "tGSLM", "offsetStart": 20, "offsetEnd": 25}, "context": "for future works on tGSLM.", "mentionContextAttributes": {"used": {"value": false, "score": 0.0001735091209411621}, "created": {"value": true, "score": 0.7286121249198914}, "shared": {"value": false, "score": 5.960464477539062e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999090433120728}, "created": {"value": true, "score": 0.9995833039283752}, "shared": {"value": false, "score": 1.0728836059570312e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Hifi-Gan", "normalizedForm": "Hifi-Gan", "offsetStart": 20, "offsetEnd": 28}, "context": "We leave the use of Hifi-Gan instead of Tacotron2.0 ", "mentionContextAttributes": {"used": {"value": true, "score": 0.957377552986145}, "created": {"value": false, "score": 6.079673767089844e-06}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.957377552986145}, "created": {"value": false, "score": 6.079673767089844e-06}, "shared": {"value": false, "score": 2.384185791015625e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "tGSLM", "normalizedForm": "tGSLM", "offsetStart": 21, "offsetEnd": 26}, "context": "In addition to 200ms-tGSLM and GSLM we evaluate a topline called character-gold that are speech utterances obtained with Text-To-Speech (Tacotron2.0 from Shen et al. (2017)) taking in input the transcriptions of LJ and LibriSpeech utterances.", "mentionContextAttributes": {"used": {"value": true, "score": 0.8367838859558105}, "created": {"value": false, "score": 1.0967254638671875e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999090433120728}, "created": {"value": true, "score": 0.9995833039283752}, "shared": {"value": false, "score": 1.0728836059570312e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "tGSLM", "normalizedForm": "tGSLM", "offsetStart": 25, "offsetEnd": 30}, "context": "The general structure of tGSLM is presented in Figure 1. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.0006393790245056152}, "created": {"value": false, "score": 0.0014085173606872559}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999090433120728}, "created": {"value": true, "score": 0.9995833039283752}, "shared": {"value": false, "score": 1.0728836059570312e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "LJ", "normalizedForm": "LJ", "offsetStart": 26, "offsetEnd": 28}, "context": "4.1 Datasets and settings LJ Speech (LJ), LibriSpeech (LS), Libri-light 6k clean (LL6k-clean) are three corpora of studio recordings of read English of respectively 24, 1k and 6k hours (Ito and Johnson, 2017;Panayotov et al., 2015;Rivi\u00e8re and Dupoux, 2021).", "mentionContextAttributes": {"used": {"value": true, "score": 0.513558030128479}, "created": {"value": false, "score": 2.1457672119140625e-06}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999603033065796}, "created": {"value": false, "score": 0.004245400428771973}, "shared": {"value": false, "score": 4.0531158447265625e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "FAISS", "normalizedForm": "FAISS", "offsetStart": 27, "offsetEnd": 32}, "context": "Sampling is performed in a FAISS k-NN (Johnson et al., 2017) that contains all the lexical tokens segmented in the dev-clean and test-clean from the LibriSpeech (roughly 10 hours of speech).", "mentionContextAttributes": {"used": {"value": true, "score": 0.7101846933364868}, "created": {"value": false, "score": 1.704692840576172e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9979422688484192}, "created": {"value": false, "score": 2.6226043701171875e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "references": [{"label": "(Johnson et al., 2017)", "normalizedForm": "Johnson et al., 2017", "refKey": 26}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "tGSLM", "normalizedForm": "tGSLM", "offsetStart": 28, "offsetEnd": 33}, "context": "We call the resulting model tGSLM (token-based GSLM).", "mentionContextAttributes": {"used": {"value": false, "score": 0.00045424699783325195}, "created": {"value": true, "score": 0.9995833039283752}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999090433120728}, "created": {"value": true, "score": 0.9995833039283752}, "shared": {"value": false, "score": 1.0728836059570312e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "LibriSpeech", "normalizedForm": "LibriSpeech", "offsetStart": 28, "offsetEnd": 39}, "context": "Given a segmentation of the LibriSpeech dev-clean subset, all speech segments are embedded into fixed-size vectors.", "mentionContextAttributes": {"used": {"value": false, "score": 0.38097089529037476}, "created": {"value": false, "score": 1.633167266845703e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999727010726929}, "created": {"value": false, "score": 0.0003751516342163086}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "references": [{"label": "(Panayotov et al., 2015)", "normalizedForm": "Panayotov et al., 2015", "refKey": 49}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "WaveGlow", "normalizedForm": "WaveGlow", "offsetStart": 29, "offsetEnd": 59}, "context": "and decoded into speech with WaveGlow (Prenger et al., 2018), a neural vocoder. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9998781681060791}, "created": {"value": false, "score": 0.0004979372024536133}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9998781681060791}, "created": {"value": false, "score": 0.0009794235229492188}, "shared": {"value": false, "score": 1.430511474609375e-06}}, "references": [{"label": "(Prenger et al., 2018)", "normalizedForm": "Prenger et al., 2018", "refKey": 55}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "tGSLM", "normalizedForm": "tGSLM", "offsetStart": 30, "offsetEnd": 35}, "context": "Yet, from what we have heard, tGSLM still struggles to generate sentences that fully make sense, so we do not think that post-processing is required at the moment.", "mentionContextAttributes": {"used": {"value": false, "score": 0.00020951032638549805}, "created": {"value": false, "score": 0.0003948807716369629}, "shared": {"value": false, "score": 1.0728836059570312e-06}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999090433120728}, "created": {"value": true, "score": 0.9995833039283752}, "shared": {"value": false, "score": 1.0728836059570312e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "sW", "normalizedForm": "sW", "offsetStart": 31, "offsetEnd": 33}, "context": "GSLM has a little advantage on sW U GGY and sBLIM P and an 200ms-tGSLM a slight advantage on ABX sem and ABX P OS .", "mentionContextAttributes": {"used": {"value": false, "score": 0.02243095636367798}, "created": {"value": false, "score": 8.106231689453125e-06}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": false, "score": 0.23337161540985107}, "created": {"value": false, "score": 0.0003979802131652832}, "shared": {"value": false, "score": 1.5497207641601562e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "LJ", "normalizedForm": "LJ", "offsetStart": 37, "offsetEnd": 39}, "context": "4.1 Datasets and settings LJ Speech (LJ), LibriSpeech (LS), Libri-light 6k clean (LL6k-clean) are three corpora of studio recordings of read English of respectively 24, 1k and 6k hours (Ito and Johnson, 2017;Panayotov et al., 2015;Rivi\u00e8re and Dupoux, 2021).", "mentionContextAttributes": {"used": {"value": true, "score": 0.513558030128479}, "created": {"value": false, "score": 2.1457672119140625e-06}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999603033065796}, "created": {"value": false, "score": 0.004245400428771973}, "shared": {"value": false, "score": 4.0531158447265625e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "tGSLM", "normalizedForm": "tGSLM", "offsetStart": 38, "offsetEnd": 43}, "context": "Due to the several modules at work in tGSLM (SSE model, LexEmb function, transformer decoder and seq2seq decoder), a large grid-search on hyper-parameters has been necessary which makes this work quite resourceconsuming. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.0001996159553527832}, "created": {"value": false, "score": 0.013350069522857666}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999090433120728}, "created": {"value": true, "score": 0.9995833039283752}, "shared": {"value": false, "score": 1.0728836059570312e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "tGSLM", "normalizedForm": "tGSLM", "offsetStart": 39, "offsetEnd": 44}, "context": "on LL6k-clean4 while the topline, gold-tGSLM, is trained only on LibriSpeech corpus5 .", "mentionContextAttributes": {"used": {"value": false, "score": 0.03413045406341553}, "created": {"value": false, "score": 3.933906555175781e-06}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999090433120728}, "created": {"value": true, "score": 0.9995833039283752}, "shared": {"value": false, "score": 1.0728836059570312e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "tGSLM", "normalizedForm": "tGSLM", "offsetStart": 39, "offsetEnd": 44}, "context": "MMOS scores are not available for gold-tGSLM has the speech decoder did not work properly.", "mentionContextAttributes": {"used": {"value": false, "score": 0.00813370943069458}, "created": {"value": false, "score": 3.933906555175781e-06}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999090433120728}, "created": {"value": true, "score": 0.9995833039283752}, "shared": {"value": false, "score": 1.0728836059570312e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "tGSLM", "normalizedForm": "tGSLM", "offsetStart": 39, "offsetEnd": 44}, "context": "Therefore, even at inference time, our tGSLM should be much faster than GSLM to run.", "mentionContextAttributes": {"used": {"value": false, "score": 0.0002948641777038574}, "created": {"value": false, "score": 0.0005654692649841309}, "shared": {"value": false, "score": 4.76837158203125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999090433120728}, "created": {"value": true, "score": 0.9995833039283752}, "shared": {"value": false, "score": 1.0728836059570312e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Adam", "normalizedForm": "Adam", "offsetStart": 40, "offsetEnd": 44}, "context": "We use a dropout probability of 0.1 and Adam optimizer with a weight decay of 0.1. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9997223019599915}, "created": {"value": false, "score": 2.384185791015625e-06}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999768733978271}, "created": {"value": false, "score": 2.384185791015625e-06}, "shared": {"value": false, "score": 2.384185791015625e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Tacotron", "normalizedForm": "Tacotron", "offsetStart": 40, "offsetEnd": 48}, "version": {"rawForm": "2.0", "normalizedForm": "2.0", "offsetStart": 48, "offsetEnd": 51}, "context": "We leave the use of Hifi-Gan instead of Tacotron2.0 ", "mentionContextAttributes": {"used": {"value": true, "score": 0.957377552986145}, "created": {"value": false, "score": 6.079673767089844e-06}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9994226694107056}, "created": {"value": false, "score": 0.4285791516304016}, "shared": {"value": false, "score": 8.344650268554688e-07}}, "references": [{"label": "Shen et al. (2017)", "normalizedForm": "Shen et al. (2017)", "refKey": 64}, {"label": "Shen et al. (2017)", "normalizedForm": "Shen et al. (2017)", "refKey": 64}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Tacotron", "normalizedForm": "Tacotron", "offsetStart": 40, "offsetEnd": 48}, "version": {"rawForm": "2.0", "normalizedForm": "2.0"}, "context": "In order to make use of this pretrained Tacotron2.0", "mentionContextAttributes": {"used": {"value": true, "score": 0.9994226694107056}, "created": {"value": false, "score": 1.0371208190917969e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9994226694107056}, "created": {"value": false, "score": 0.4285791516304016}, "shared": {"value": false, "score": 8.344650268554688e-07}}, "references": [{"label": "Shen et al. (2017)", "normalizedForm": "Shen et al. (2017)", "refKey": 64}, {"label": "Shen et al. (2017)", "normalizedForm": "Shen et al. (2017)", "refKey": 64}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "LibriSpeech", "normalizedForm": "LibriSpeech", "offsetStart": 42, "offsetEnd": 53}, "context": "4.1 Datasets and settings LJ Speech (LJ), LibriSpeech (LS), Libri-light 6k clean (LL6k-clean) are three corpora of studio recordings of read English of respectively 24, 1k and 6k hours (Ito and Johnson, 2017;Panayotov et al., 2015;Rivi\u00e8re and Dupoux, 2021).", "mentionContextAttributes": {"used": {"value": true, "score": 0.513558030128479}, "created": {"value": false, "score": 2.1457672119140625e-06}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999727010726929}, "created": {"value": false, "score": 0.0003751516342163086}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "references": [{"label": "(Panayotov et al., 2015)", "normalizedForm": "Panayotov et al., 2015", "refKey": 49}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "sBLIM", "normalizedForm": "sBLIM", "offsetStart": 44, "offsetEnd": 49}, "context": "GSLM has a little advantage on sW U GGY and sBLIM P and an 200ms-tGSLM a slight advantage on ABX sem and ABX P OS .", "mentionContextAttributes": {"used": {"value": false, "score": 0.02243095636367798}, "created": {"value": false, "score": 8.106231689453125e-06}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": false, "score": 0.1760677695274353}, "created": {"value": false, "score": 0.0003979802131652832}, "shared": {"value": false, "score": 2.384185791015625e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "tGSLM", "normalizedForm": "tGSLM", "offsetStart": 44, "offsetEnd": 49}, "context": "Overall, regarding speech generation, 200ms-tGSLM outperforms sylseg-tGSLM, dpparse-tGSLM and also GSLM.", "mentionContextAttributes": {"used": {"value": false, "score": 0.009016990661621094}, "created": {"value": false, "score": 4.172325134277344e-06}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999090433120728}, "created": {"value": true, "score": 0.9995833039283752}, "shared": {"value": false, "score": 1.0728836059570312e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "LibriSpeech", "normalizedForm": "LibriSpeech", "offsetStart": 44, "offsetEnd": 55}, "context": "tences with a VERT equal to the VERT of the LibriSpeech.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9860070943832397}, "created": {"value": false, "score": 1.0609626770019531e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999727010726929}, "created": {"value": false, "score": 0.0003751516342163086}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "references": [{"label": "(Panayotov et al., 2015)", "normalizedForm": "Panayotov et al., 2015", "refKey": 49}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "tGSLM", "normalizedForm": "tGSLM", "offsetStart": 46, "offsetEnd": 51}, "context": "To study the impact of speech segmentation on tGSLM, we trained this model on LibriSpeech with two extra segmentation methods: SylSeg (R\u00e4s\u00e4nen et al., 2018), and DP-Parse (Algayres et al., 2022b) 8 .", "mentionContextAttributes": {"used": {"value": true, "score": 0.9998134970664978}, "created": {"value": false, "score": 0.00018459558486938477}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999090433120728}, "created": {"value": true, "score": 0.9995833039283752}, "shared": {"value": false, "score": 1.0728836059570312e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "tGSLM", "normalizedForm": "tGSLM", "offsetStart": 48, "offsetEnd": 53}, "context": "For a sequence of 1 second (therefore n = 5 for tGSLM and n = 25 for GSLM), by replacing those values in the former calculation, we find that a forward in tGSLM cost 1.1e6 which is 5 times less costly than a forward in GSLM that would cost 5.2e6.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9979392886161804}, "created": {"value": false, "score": 2.6226043701171875e-06}, "shared": {"value": false, "score": 8.344650268554688e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999090433120728}, "created": {"value": true, "score": 0.9995833039283752}, "shared": {"value": false, "score": 1.0728836059570312e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "tGSLM", "normalizedForm": "tGSLM", "offsetStart": 48, "offsetEnd": 53}, "context": "The acoustic tokens that are the input of 200ms-tGSLM are extracted in a preprocessing step.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9972612857818604}, "created": {"value": false, "score": 1.2159347534179688e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999090433120728}, "created": {"value": true, "score": 0.9995833039283752}, "shared": {"value": false, "score": 1.0728836059570312e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "tGSLM", "normalizedForm": "tGSLM", "offsetStart": 50, "offsetEnd": 55}, "context": "Even though true word boundaries strongly benefit tGSLM, using unsupervised speech segmentation methods did not prove beneficial.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9838488101959229}, "created": {"value": false, "score": 1.1801719665527344e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999090433120728}, "created": {"value": true, "score": 0.9995833039283752}, "shared": {"value": false, "score": 1.0728836059570312e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "LJ", "normalizedForm": "LJ", "offsetStart": 52, "offsetEnd": 54}, "context": "In green dashed lines appear two VERT anchor points LJ-VERT(=0.113) and LS-VERT(=0.189).", "mentionContextAttributes": {"used": {"value": true, "score": 0.5956971049308777}, "created": {"value": false, "score": 3.5881996154785156e-05}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999603033065796}, "created": {"value": false, "score": 0.004245400428771973}, "shared": {"value": false, "score": 4.0531158447265625e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "LibriSpeech", "normalizedForm": "LibriSpeech", "offsetStart": 52, "offsetEnd": 63}, "context": "It is trained on 32 GPUs, for 30k iterations on the LibriSpeech. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.0428352952003479}, "created": {"value": false, "score": 0.0003751516342163086}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999727010726929}, "created": {"value": false, "score": 0.0003751516342163086}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "references": [{"label": "(Panayotov et al., 2015)", "normalizedForm": "Panayotov et al., 2015", "refKey": 49}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "LibriSpeech", "normalizedForm": "LibriSpeech", "offsetStart": 55, "offsetEnd": 66}, "context": "All speech segments corresponding to real words in the LibriSpeech dev-clean set are indexed in k-NN graphs on their acoustic or lexical form.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9851104021072388}, "created": {"value": false, "score": 2.086162567138672e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999727010726929}, "created": {"value": false, "score": 0.0003751516342163086}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "references": [{"label": "(Panayotov et al., 2015)", "normalizedForm": "Panayotov et al., 2015", "refKey": 49}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "WaveGlow", "normalizedForm": "WaveGlow", "offsetStart": 56, "offsetEnd": 64}, "context": "Then, speech is generated from the mel filterbanks by a WaveGlow vocoder (Prenger et al., 2018). ", "mentionContextAttributes": {"used": {"value": true, "score": 0.7773439288139343}, "created": {"value": false, "score": 7.37905502319336e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9998781681060791}, "created": {"value": false, "score": 0.0009794235229492188}, "shared": {"value": false, "score": 1.430511474609375e-06}}, "references": [{"label": "(Prenger et al., 2018)", "normalizedForm": "Prenger et al., 2018", "refKey": 55, "offsetStart": 18909, "offsetEnd": 18931}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "tGSLM", "normalizedForm": "tGSLM", "offsetStart": 62, "offsetEnd": 67}, "context": "We report in Table 11 sWUGGY and sBLIMP scores of GSLM, 200ms-tGSLM and the large units (called BC-30k and BC-2k).", "mentionContextAttributes": {"used": {"value": true, "score": 0.9995400905609131}, "created": {"value": false, "score": 4.5299530029296875e-06}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999090433120728}, "created": {"value": true, "score": 0.9995833039283752}, "shared": {"value": false, "score": 1.0728836059570312e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "tGSLM", "normalizedForm": "tGSLM", "offsetStart": 63, "offsetEnd": 68}, "context": "First, these experiments show the necessity of q for the 200ms-tGSLM to generate spoken sentences.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9987552165985107}, "created": {"value": false, "score": 0.00017493963241577148}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999090433120728}, "created": {"value": true, "score": 0.9995833039283752}, "shared": {"value": false, "score": 1.0728836059570312e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "LibriSpeech", "normalizedForm": "LibriSpeech", "offsetStart": 63, "offsetEnd": 74}, "context": "First, we retrieved the gold-standard word segmentation of the LibriSpeech corpus and embedded all word tokens with the SSE model from Algayres et al. (2022a).", "mentionContextAttributes": {"used": {"value": true, "score": 0.9999727010726929}, "created": {"value": false, "score": 0.00017398595809936523}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999727010726929}, "created": {"value": false, "score": 0.0003751516342163086}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "references": [{"label": "(Panayotov et al., 2015)", "normalizedForm": "Panayotov et al., 2015", "refKey": 49}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "tGSLM", "normalizedForm": "tGSLM", "offsetStart": 65, "offsetEnd": 70}, "context": "GSLM has a little advantage on sW U GGY and sBLIM P and an 200ms-tGSLM a slight advantage on ABX sem and ABX P OS .", "mentionContextAttributes": {"used": {"value": false, "score": 0.02243095636367798}, "created": {"value": false, "score": 8.106231689453125e-06}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999090433120728}, "created": {"value": true, "score": 0.9995833039283752}, "shared": {"value": false, "score": 1.0728836059570312e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Tacotron", "normalizedForm": "Tacotron", "offsetStart": 65, "offsetEnd": 73}, "version": {"rawForm": "2.0", "normalizedForm": "2.0", "offsetStart": 73, "offsetEnd": 76}, "context": "Finally, the Hu-BERT units are given as input to the pre-trained Tacotron2.0 to be decoded into spoken utterances.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9799981713294983}, "created": {"value": false, "score": 2.574920654296875e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9994226694107056}, "created": {"value": false, "score": 0.4285791516304016}, "shared": {"value": false, "score": 8.344650268554688e-07}}, "references": [{"label": "Shen et al. (2017)", "normalizedForm": "Shen et al. (2017)", "refKey": 64}, {"label": "Shen et al. (2017)", "normalizedForm": "Shen et al. (2017)", "refKey": 64}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "LibriSpeech", "normalizedForm": "LibriSpeech", "offsetStart": 65, "offsetEnd": 76}, "context": "on LL6k-clean4 while the topline, gold-tGSLM, is trained only on LibriSpeech corpus5 .", "mentionContextAttributes": {"used": {"value": false, "score": 0.03413045406341553}, "created": {"value": false, "score": 3.933906555175781e-06}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999727010726929}, "created": {"value": false, "score": 0.0003751516342163086}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "references": [{"label": "(Panayotov et al., 2015)", "normalizedForm": "Panayotov et al., 2015", "refKey": 49}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Google TTS API", "normalizedForm": "Google TTS API", "offsetStart": 66, "offsetEnd": 80}, "context": "sW U GGY is a list of pairs of word/non-word synthesized with the Google TTS API and filtered for the words that are in the Lib-riSpeech training set. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.23337161540985107}, "created": {"value": false, "score": 0.00012040138244628906}, "shared": {"value": false, "score": 1.5497207641601562e-06}}, "documentContextAttributes": {"used": {"value": false, "score": 0.23337161540985107}, "created": {"value": false, "score": 0.00012040138244628906}, "shared": {"value": false, "score": 1.5497207641601562e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "tGSLM", "normalizedForm": "tGSLM", "offsetStart": 68, "offsetEnd": 73}, "context": "By doing so, we might have overfitted the English language and made tGSLM specifically good at generating English speech. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9829434156417847}, "created": {"value": true, "score": 0.7554594874382019}, "shared": {"value": false, "score": 5.960464477539062e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999090433120728}, "created": {"value": true, "score": 0.9995833039283752}, "shared": {"value": false, "score": 1.0728836059570312e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "tGSLM", "normalizedForm": "tGSLM", "offsetStart": 68, "offsetEnd": 73}, "context": "Nonetheless, our table of results does not show the performances of tGSLM in a much lower temperature regime.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9995015859603882}, "created": {"value": false, "score": 6.127357482910156e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999090433120728}, "created": {"value": true, "score": 0.9995833039283752}, "shared": {"value": false, "score": 1.0728836059570312e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "tGSLM", "normalizedForm": "tGSLM", "offsetStart": 69, "offsetEnd": 74}, "context": "Overall, regarding speech generation, 200ms-tGSLM outperforms sylseg-tGSLM, dpparse-tGSLM and also GSLM.", "mentionContextAttributes": {"used": {"value": false, "score": 0.009016990661621094}, "created": {"value": false, "score": 4.172325134277344e-06}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999090433120728}, "created": {"value": true, "score": 0.9995833039283752}, "shared": {"value": false, "score": 1.0728836059570312e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "tGSLM", "normalizedForm": "tGSLM", "offsetStart": 70, "offsetEnd": 75}, "context": "Regarding the perplexity scores from Table 1, compared to GSLM, 200ms-tGSLM is slightly better at LJ-VERT and slightly worse at LS-VERT.", "mentionContextAttributes": {"used": {"value": true, "score": 0.6106167435646057}, "created": {"value": false, "score": 1.1563301086425781e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999090433120728}, "created": {"value": true, "score": 0.9995833039283752}, "shared": {"value": false, "score": 1.0728836059570312e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "tGSLM", "normalizedForm": "tGSLM", "offsetStart": 72, "offsetEnd": 77}, "context": "The scores show that these units perform equally well to GSLM and 200ms-tGSLM.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9931244254112244}, "created": {"value": false, "score": 7.271766662597656e-06}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999090433120728}, "created": {"value": true, "score": 0.9995833039283752}, "shared": {"value": false, "score": 1.0728836059570312e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Tacotron", "normalizedForm": "Tacotron", "offsetStart": 74, "offsetEnd": 82}, "version": {"rawForm": "2.0", "normalizedForm": "2.0", "offsetStart": 82, "offsetEnd": 85}, "context": "Specifically, the sampled HuBERT units are mapped to mel-filterbanks with Tacotron2.0 ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9989733695983887}, "created": {"value": false, "score": 2.5033950805664062e-06}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9994226694107056}, "created": {"value": false, "score": 0.4285791516304016}, "shared": {"value": false, "score": 8.344650268554688e-07}}, "references": [{"label": "Shen et al. (2017)", "normalizedForm": "Shen et al. (2017)", "refKey": 64}, {"label": "Shen et al. (2017)", "normalizedForm": "Shen et al. (2017)", "refKey": 64}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "tGSLM", "normalizedForm": "tGSLM", "offsetStart": 76, "offsetEnd": 81}, "context": "The topline gold-tGSLM produces much lower perplexities than GSLM and 200ms-tGSLM.", "mentionContextAttributes": {"used": {"value": false, "score": 0.0008502006530761719}, "created": {"value": false, "score": 5.7220458984375e-06}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999090433120728}, "created": {"value": true, "score": 0.9995833039283752}, "shared": {"value": false, "score": 1.0728836059570312e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "LibriSpeech", "normalizedForm": "LibriSpeech", "offsetStart": 76, "offsetEnd": 87}, "context": "Then, we clustered with k-means and hierarchicalk-means all the SSEs in the LibriSpeech into K classes. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9999648332595825}, "created": {"value": false, "score": 8.225440979003906e-06}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999727010726929}, "created": {"value": false, "score": 0.0003751516342163086}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "references": [{"label": "(Panayotov et al., 2015)", "normalizedForm": "Panayotov et al., 2015", "refKey": 49}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "tGSLM", "normalizedForm": "tGSLM", "offsetStart": 78, "offsetEnd": 83}, "context": "ABX scores are obtained, for GSLM at the 9th layer of the transformer and for tGSLM with the lexical tokens.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9965085387229919}, "created": {"value": false, "score": 2.7418136596679688e-06}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999090433120728}, "created": {"value": true, "score": 0.9995833039283752}, "shared": {"value": false, "score": 1.0728836059570312e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "LibriSpeech", "normalizedForm": "LibriSpeech", "offsetStart": 78, "offsetEnd": 89}, "context": "To study the impact of speech segmentation on tGSLM, we trained this model on LibriSpeech with two extra segmentation methods: SylSeg (R\u00e4s\u00e4nen et al., 2018), and DP-Parse (Algayres et al., 2022b) 8 . ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9998134970664978}, "created": {"value": false, "score": 0.00018459558486938477}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999727010726929}, "created": {"value": false, "score": 0.0003751516342163086}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "references": [{"label": "(Panayotov et al., 2015)", "normalizedForm": "Panayotov et al., 2015", "refKey": 49}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "tGSLM", "normalizedForm": "tGSLM", "offsetStart": 81, "offsetEnd": 86}, "context": "Given a prompt of t acoustic tokens (a 0 , ..., a t ), we do a forward pass into tGSLM.", "mentionContextAttributes": {"used": {"value": false, "score": 0.18778133392333984}, "created": {"value": false, "score": 1.5139579772949219e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999090433120728}, "created": {"value": true, "score": 0.9995833039283752}, "shared": {"value": false, "score": 1.0728836059570312e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "LibriSpeech", "normalizedForm": "LibriSpeech", "offsetStart": 83, "offsetEnd": 94}, "context": "ASR transcripts are obtained with a pretrained large Wav2Vec 2.0 model, trained on LibriSpeech-960h combined with a standard KenLM", "mentionContextAttributes": {"used": {"value": true, "score": 0.9999481439590454}, "created": {"value": false, "score": 3.5762786865234375e-06}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999727010726929}, "created": {"value": false, "score": 0.0003751516342163086}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "references": [{"label": "(Panayotov et al., 2015)", "normalizedForm": "Panayotov et al., 2015", "refKey": 49}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "tGSLM", "normalizedForm": "tGSLM", "offsetStart": 84, "offsetEnd": 89}, "context": "Overall, regarding speech generation, 200ms-tGSLM outperforms sylseg-tGSLM, dpparse-tGSLM and also GSLM.", "mentionContextAttributes": {"used": {"value": false, "score": 0.009016990661621094}, "created": {"value": false, "score": 4.172325134277344e-06}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999090433120728}, "created": {"value": true, "score": 0.9995833039283752}, "shared": {"value": false, "score": 1.0728836059570312e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "LJ", "normalizedForm": "LJ", "offsetStart": 85, "offsetEnd": 87}, "context": "The Tacotron2.0 from Lakhotia et al. (2021); Kharitonov et al. (2022) was trained on LJ.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9993294477462769}, "created": {"value": false, "score": 2.5033950805664062e-05}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999603033065796}, "created": {"value": false, "score": 0.004245400428771973}, "shared": {"value": false, "score": 4.0531158447265625e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "tGSLM", "normalizedForm": "tGSLM", "offsetStart": 87, "offsetEnd": 92}, "context": "Figure 4 is a visualization of the acoustic and lexical representation learned by gold-tGSLM which echo a work on speech word embeddings from Chung and Glass (2018).", "mentionContextAttributes": {"used": {"value": false, "score": 0.03548097610473633}, "created": {"value": false, "score": 3.063678741455078e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999090433120728}, "created": {"value": true, "score": 0.9995833039283752}, "shared": {"value": false, "score": 1.0728836059570312e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "LibriSpeech", "normalizedForm": "LibriSpeech", "offsetStart": 89, "offsetEnd": 100}, "context": "L,h1,h2,h3 and the transformer are trained on 32 GPUs, for 200k iterations on either the LibriSpeech or LL6k-clean. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9880207180976868}, "created": {"value": false, "score": 1.6689300537109375e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999727010726929}, "created": {"value": false, "score": 0.0003751516342163086}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "references": [{"label": "(Panayotov et al., 2015)", "normalizedForm": "Panayotov et al., 2015", "refKey": 49}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "LJ", "normalizedForm": "LJ", "offsetStart": 90, "offsetEnd": 92}, "context": "These points are the mean VERT scores obtained on batches of sentences from, respectively LJ and LibriSpeech datasets.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9999603033065796}, "created": {"value": false, "score": 3.0994415283203125e-06}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999603033065796}, "created": {"value": false, "score": 0.004245400428771973}, "shared": {"value": false, "score": 4.0531158447265625e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "LJ", "normalizedForm": "LJ", "offsetStart": 93, "offsetEnd": 95}, "context": "From Table 1, for the high-temperature regime that leads to diversity scores in the range of LJ and Librispeech, 200ms-tGSLM is slightly better than GSLM and gets close scores with the topline.", "mentionContextAttributes": {"used": {"value": false, "score": 0.02431398630142212}, "created": {"value": false, "score": 2.658367156982422e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999603033065796}, "created": {"value": false, "score": 0.004245400428771973}, "shared": {"value": false, "score": 4.0531158447265625e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "tGSLM", "normalizedForm": "tGSLM", "offsetStart": 93, "offsetEnd": 98}, "context": "Given a speech prompt, segmented and encoded into (a 0 , ..., a t ), we do a forward pass in tGSLM and search for the nearest neighbors of h 1 output in the lexical space.", "mentionContextAttributes": {"used": {"value": false, "score": 0.36750638484954834}, "created": {"value": false, "score": 2.5272369384765625e-05}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999090433120728}, "created": {"value": true, "score": 0.9995833039283752}, "shared": {"value": false, "score": 1.0728836059570312e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "tGSLM", "normalizedForm": "tGSLM", "offsetStart": 96, "offsetEnd": 101}, "context": "Yet, we have experienced a problem with the speech decoder (described in Section 3.2.2) of gold-tGSLM.", "mentionContextAttributes": {"used": {"value": true, "score": 0.853594183921814}, "created": {"value": false, "score": 0.0006427168846130371}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999090433120728}, "created": {"value": true, "score": 0.9995833039283752}, "shared": {"value": false, "score": 1.0728836059570312e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "LibriSpeech", "normalizedForm": "LibriSpeech", "offsetStart": 96, "offsetEnd": 107}, "context": "word types in the 10hour-long Buckeye corpus (Pitt et al., 2005), and \u224890k in the 960-hour-long LibriSpeech (Panayotov et al., 2015).", "mentionContextAttributes": {"used": {"value": true, "score": 0.9655813574790955}, "created": {"value": false, "score": 3.814697265625e-06}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999727010726929}, "created": {"value": false, "score": 0.0003751516342163086}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "references": [{"label": "(Panayotov et al., 2015)", "normalizedForm": "Panayotov et al., 2015", "refKey": 49, "offsetStart": 52869, "offsetEnd": 52893}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "LibriSpeech", "normalizedForm": "LibriSpeech", "offsetStart": 97, "offsetEnd": 108}, "context": "These points are the mean VERT scores obtained on batches of sentences from, respectively LJ and LibriSpeech datasets.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9999603033065796}, "created": {"value": false, "score": 3.0994415283203125e-06}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999727010726929}, "created": {"value": false, "score": 0.0003751516342163086}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "references": [{"label": "(Panayotov et al., 2015)", "normalizedForm": "Panayotov et al., 2015", "refKey": 49}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "LJ", "normalizedForm": "LJ", "offsetStart": 98, "offsetEnd": 100}, "context": "Regarding the perplexity scores from Table 1, compared to GSLM, 200ms-tGSLM is slightly better at LJ-VERT and slightly worse at LS-VERT.", "mentionContextAttributes": {"used": {"value": true, "score": 0.6106167435646057}, "created": {"value": false, "score": 1.1563301086425781e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999603033065796}, "created": {"value": false, "score": 0.004245400428771973}, "shared": {"value": false, "score": 4.0531158447265625e-06}}}, {"type": "software", "software-type": "implicit", "software-name": {"rawForm": "library", "normalizedForm": "library", "offsetStart": 98, "offsetEnd": 130}, "context": "We chose that method as this later speech synthesiser comes pre-trained in the textlesslib Python library (Kharitonov et al., 2022). ", "mentionContextAttributes": {"used": {"value": true, "score": 0.8267543315887451}, "created": {"value": false, "score": 0.0019115805625915527}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.8267543315887451}, "created": {"value": false, "score": 0.0019115805625915527}, "shared": {"value": false, "score": 1.1920928955078125e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "tGSLM", "normalizedForm": "tGSLM", "offsetStart": 109, "offsetEnd": 114}, "context": "In the LL6kclean corpus, sentences are 60s-long on average with make n = 1500 for GSLM and n = 300 for 200ms-tGSLM.", "mentionContextAttributes": {"used": {"value": false, "score": 0.3828955292701721}, "created": {"value": false, "score": 1.0371208190917969e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999090433120728}, "created": {"value": true, "score": 0.9995833039283752}, "shared": {"value": false, "score": 1.0728836059570312e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "tGSLM", "normalizedForm": "tGSLM", "offsetStart": 111, "offsetEnd": 116}, "context": "ABX scores are again obtained for GSLM with embeddings extracted from the 9th layer of the transformer and for tGSLM from the lexical tokens.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9997090697288513}, "created": {"value": false, "score": 2.6226043701171875e-06}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999090433120728}, "created": {"value": true, "score": 0.9995833039283752}, "shared": {"value": false, "score": 1.0728836059570312e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Tacotron", "normalizedForm": "Tacotron", "offsetStart": 111, "offsetEnd": 119}, "version": {"rawForm": "2", "normalizedForm": "2", "offsetStart": 119, "offsetEnd": 120}, "context": "When a final a T token is sampled, (a 0 , ..., a T ) is decoded into HuBERT units and speech is generated with Tacotron2.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9816141128540039}, "created": {"value": false, "score": 4.291534423828125e-06}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9994226694107056}, "created": {"value": false, "score": 0.4285791516304016}, "shared": {"value": false, "score": 8.344650268554688e-07}}, "references": [{"label": "Shen et al. (2017)", "normalizedForm": "Shen et al. (2017)", "refKey": 64}, {"label": "Shen et al. (2017)", "normalizedForm": "Shen et al. (2017)", "refKey": 64}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "tGSLM", "normalizedForm": "tGSLM", "offsetStart": 114, "offsetEnd": 119}, "context": "When conditioned on very low temperature, GSLM can generate very simple and intelligible sentences, whereas 200ms-tGSLM start to produce gibberish.", "mentionContextAttributes": {"used": {"value": false, "score": 7.319450378417969e-05}, "created": {"value": false, "score": 1.2755393981933594e-05}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999090433120728}, "created": {"value": true, "score": 0.9995833039283752}, "shared": {"value": false, "score": 1.0728836059570312e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "tGSLM", "normalizedForm": "tGSLM", "offsetStart": 119, "offsetEnd": 124}, "context": "From Table 1, for the high-temperature regime that leads to diversity scores in the range of LJ and Librispeech, 200ms-tGSLM is slightly better than GSLM and gets close scores with the topline.", "mentionContextAttributes": {"used": {"value": false, "score": 0.02431398630142212}, "created": {"value": false, "score": 2.658367156982422e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999090433120728}, "created": {"value": true, "score": 0.9995833039283752}, "shared": {"value": false, "score": 1.0728836059570312e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Lib-", "normalizedForm": "Lib", "offsetStart": 124, "offsetEnd": 128}, "context": "sW U GGY is a list of pairs of word/non-word synthesized with the Google TTS API and filtered for the words that are in the Lib-riSpeech training set. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.23337161540985107}, "created": {"value": false, "score": 0.00012040138244628906}, "shared": {"value": false, "score": 1.5497207641601562e-06}}, "documentContextAttributes": {"used": {"value": false, "score": 0.23337161540985107}, "created": {"value": false, "score": 0.00012040138244628906}, "shared": {"value": false, "score": 1.5497207641601562e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "SylSeg", "normalizedForm": "SylSeg", "offsetStart": 127, "offsetEnd": 155}, "context": "To study the impact of speech segmentation on tGSLM, we trained this model on LibriSpeech with two extra segmentation methods: SylSeg (R\u00e4s\u00e4nen et al., 2018), and DP-Parse (Algayres et al., 2022b) 8 . ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9998134970664978}, "created": {"value": false, "score": 0.00018459558486938477}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9998134970664978}, "created": {"value": false, "score": 0.00018459558486938477}, "shared": {"value": false, "score": 2.384185791015625e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Tacotron", "normalizedForm": "Tacotron", "offsetStart": 128, "offsetEnd": 136}, "version": {"rawForm": "2", "normalizedForm": "2", "offsetStart": 136, "offsetEnd": 137}, "context": "Yet, recent work on textless speech synthesis Kreuk et al. (2021); Kharitonov et al. (2021a) skip the spectrogram prediction of Tacotron2.0 ", "mentionContextAttributes": {"used": {"value": false, "score": 0.0006768703460693359}, "created": {"value": false, "score": 0.1827297806739807}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9994226694107056}, "created": {"value": false, "score": 0.4285791516304016}, "shared": {"value": false, "score": 8.344650268554688e-07}}, "references": [{"label": "Shen et al. (2017)", "normalizedForm": "Shen et al. (2017)", "refKey": 64}, {"label": "Shen et al. (2017)", "normalizedForm": "Shen et al. (2017)", "refKey": 64}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "tGSLM", "normalizedForm": "tGSLM", "offsetStart": 129, "offsetEnd": 134}, "context": "Our sampling method is summarized in Figure 2. We start by collecting a few dozen hours of speech that have not been seen during tGSLM training.", "mentionContextAttributes": {"used": {"value": false, "score": 0.41836345195770264}, "created": {"value": false, "score": 0.06411373615264893}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999090433120728}, "created": {"value": true, "score": 0.9995833039283752}, "shared": {"value": false, "score": 1.0728836059570312e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "tGSLM", "normalizedForm": "tGSLM", "offsetStart": 131, "offsetEnd": 136}, "context": "To complete our analysis, we provide in Table 1, performances on the zero-shot tasks scores that are comparable for GSLM and 200ms-tGSLM.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9956212639808655}, "created": {"value": false, "score": 0.0005799531936645508}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999090433120728}, "created": {"value": true, "score": 0.9995833039283752}, "shared": {"value": false, "score": 1.0728836059570312e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "tGSLM", "normalizedForm": "tGSLM", "offsetStart": 136, "offsetEnd": 141}, "context": "In practice, we observe a lower memory reduction (\u2248 4.76) which can be explained by the additional parameters that are present in 200ms-tGSLM and not in GSLM, namely the LexEmb function and three prediction heads.", "mentionContextAttributes": {"used": {"value": false, "score": 0.3749963045120239}, "created": {"value": false, "score": 1.1444091796875e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999090433120728}, "created": {"value": true, "score": 0.9995833039283752}, "shared": {"value": false, "score": 1.0728836059570312e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Tacotron", "normalizedForm": "Tacotron", "offsetStart": 137, "offsetEnd": 145}, "version": {"rawForm": "2.0", "normalizedForm": "2.0", "offsetStart": 145, "offsetEnd": 148}, "context": "In addition to 200ms-tGSLM and GSLM we evaluate a topline called character-gold that are speech utterances obtained with Text-To-Speech (Tacotron2.0 from Shen et al. (2017)) taking in input the transcriptions of LJ and LibriSpeech utterances. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.8367844223976135}, "created": {"value": false, "score": 1.0967254638671875e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9994226694107056}, "created": {"value": false, "score": 0.4285791516304016}, "shared": {"value": false, "score": 8.344650268554688e-07}}, "references": [{"label": "Shen et al. (2017)", "normalizedForm": "Shen et al. (2017)", "refKey": 64, "offsetStart": 26906, "offsetEnd": 26924}, {"label": "Shen et al. (2017)", "normalizedForm": "Shen et al. (2017)", "refKey": 64, "offsetStart": 26906, "offsetEnd": 26924}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "tGSLM", "normalizedForm": "tGSLM", "offsetStart": 139, "offsetEnd": 144}, "context": "It seems to generate fully intelligible speech only when it is trained to decode SSEs of same-size speech chunks, as is the case for 200ms-tGSLM.", "mentionContextAttributes": {"used": {"value": false, "score": 0.0005023479461669922}, "created": {"value": false, "score": 6.54458999633789e-05}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999090433120728}, "created": {"value": true, "score": 0.9995833039283752}, "shared": {"value": false, "score": 1.0728836059570312e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "LibriSpeech", "normalizedForm": "LibriSpeech", "offsetStart": 140, "offsetEnd": 151}, "context": "As for perplexity, we report in Table 1, the MMOS for batches of spoken generations that have a diversity score equal to the VERT of either LibriSpeech (MMOS@LS-VERT) or LJ (MMOS@LJ-VERT). ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9422494173049927}, "created": {"value": false, "score": 1.8358230590820312e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999727010726929}, "created": {"value": false, "score": 0.0003751516342163086}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "references": [{"label": "(Panayotov et al., 2015)", "normalizedForm": "Panayotov et al., 2015", "refKey": 49}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Tacotron", "normalizedForm": "Tacotron", "offsetStart": 143, "offsetEnd": 151}, "version": {"rawForm": "2.0", "normalizedForm": "2.0", "offsetStart": 151, "offsetEnd": 154}, "context": "Finally, our decoding method is based on a seq2seq transformer that produces HuBERT frames which are decoded into speech with a combination of Tacotron2.0 ", "mentionContextAttributes": {"used": {"value": true, "score": 0.5636308193206787}, "created": {"value": false, "score": 0.4285791516304016}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9994226694107056}, "created": {"value": false, "score": 0.4285791516304016}, "shared": {"value": false, "score": 8.344650268554688e-07}}, "references": [{"label": "Shen et al. (2017)", "normalizedForm": "Shen et al. (2017)", "refKey": 64}, {"label": "Shen et al. (2017)", "normalizedForm": "Shen et al. (2017)", "refKey": 64}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "LibriSpeech", "normalizedForm": "LibriSpeech", "offsetStart": 149, "offsetEnd": 160}, "context": "Sampling is performed in a FAISS k-NN (Johnson et al., 2017) that contains all the lexical tokens segmented in the dev-clean and test-clean from the LibriSpeech (roughly 10 hours of speech).", "mentionContextAttributes": {"used": {"value": true, "score": 0.7101868987083435}, "created": {"value": false, "score": 1.704692840576172e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999727010726929}, "created": {"value": false, "score": 0.0003751516342163086}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "references": [{"label": "(Panayotov et al., 2015)", "normalizedForm": "Panayotov et al., 2015", "refKey": 49}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "tGSLM", "normalizedForm": "tGSLM", "offsetStart": 155, "offsetEnd": 160}, "context": "For a sequence of 1 second (therefore n = 5 for tGSLM and n = 25 for GSLM), by replacing those values in the former calculation, we find that a forward in tGSLM cost 1.1e6 which is 5 times less costly than a forward in GSLM that would cost 5.2e6.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9979392886161804}, "created": {"value": false, "score": 2.6226043701171875e-06}, "shared": {"value": false, "score": 8.344650268554688e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999090433120728}, "created": {"value": true, "score": 0.9995833039283752}, "shared": {"value": false, "score": 1.0728836059570312e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "LJ", "normalizedForm": "LJ", "offsetStart": 167, "offsetEnd": 169}, "context": "Finally, tGSLM continuations will not preserve any regional accentuation from the prompt, as our model only generates speech in the voice of the single speaker of the LJ dataset.", "mentionContextAttributes": {"used": {"value": false, "score": 0.00021582841873168945}, "created": {"value": false, "score": 0.0004394650459289551}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999603033065796}, "created": {"value": false, "score": 0.004245400428771973}, "shared": {"value": false, "score": 4.0531158447265625e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "LJ", "normalizedForm": "LJ", "offsetStart": 170, "offsetEnd": 172}, "context": "As for perplexity, we report in Table 1, the MMOS for batches of spoken generations that have a diversity score equal to the VERT of either LibriSpeech (MMOS@LS-VERT) or LJ (MMOS@LJ-VERT).", "mentionContextAttributes": {"used": {"value": true, "score": 0.9422494173049927}, "created": {"value": false, "score": 1.8358230590820312e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999603033065796}, "created": {"value": false, "score": 0.004245400428771973}, "shared": {"value": false, "score": 4.0531158447265625e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "WordPiece", "normalizedForm": "WordPiece", "offsetStart": 187, "offsetEnd": 213}, "context": "This is one of the reasons why recent state-of-the-art text-based LM (Radford et al., 2019) typically use a tokenizer representing word or subword units (Byte Pair Encoding (Gage, 1994), WordPiece (Wu et al., 2016), Unigram (Kudo, 2018)). ", "mentionContextAttributes": {"used": {"value": false, "score": 0.0006437897682189941}, "created": {"value": false, "score": 1.728534698486328e-05}, "shared": {"value": false, "score": 7.152557373046875e-07}}, "documentContextAttributes": {"used": {"value": false, "score": 0.0006437897682189941}, "created": {"value": false, "score": 1.728534698486328e-05}, "shared": {"value": false, "score": 7.152557373046875e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "LJ", "normalizedForm": "LJ", "offsetStart": 212, "offsetEnd": 214}, "context": "In addition to 200ms-tGSLM and GSLM we evaluate a topline called character-gold that are speech utterances obtained with Text-To-Speech (Tacotron2.0 from Shen et al. (2017)) taking in input the transcriptions of LJ and LibriSpeech utterances.", "mentionContextAttributes": {"used": {"value": true, "score": 0.8367844223976135}, "created": {"value": false, "score": 1.0967254638671875e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999603033065796}, "created": {"value": false, "score": 0.004245400428771973}, "shared": {"value": false, "score": 4.0531158447265625e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "LibriSpeech", "normalizedForm": "LibriSpeech", "offsetStart": 219, "offsetEnd": 230}, "context": "In addition to 200ms-tGSLM and GSLM we evaluate a topline called character-gold that are speech utterances obtained with Text-To-Speech (Tacotron2.0 from Shen et al. (2017)) taking in input the transcriptions of LJ and LibriSpeech utterances.", "mentionContextAttributes": {"used": {"value": true, "score": 0.8367844223976135}, "created": {"value": false, "score": 1.0967254638671875e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999727010726929}, "created": {"value": false, "score": 0.0003751516342163086}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "references": [{"label": "(Panayotov et al., 2015)", "normalizedForm": "Panayotov et al., 2015", "refKey": 49}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "tGSLM", "normalizedForm": "tGSLM", "offsetStart": 264, "offsetEnd": 269}, "context": "Taking the calculation of a forward cost from Pan et al. (2021), for a sequence length of size n, and a transformer of L = 16 layers and dimension d = 1024, a forward pass costs (12nd + 2nd) * L. This would be the cost of a forward pass in the GSLM model, but our tGSLM costs a little bit more with its LexEmb function and its sampling procedure.", "mentionContextAttributes": {"used": {"value": true, "score": 0.8019033074378967}, "created": {"value": false, "score": 5.245208740234375e-06}, "shared": {"value": false, "score": 7.152557373046875e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999090433120728}, "created": {"value": true, "score": 0.9995833039283752}, "shared": {"value": false, "score": 1.0728836059570312e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "LJ", "normalizedForm": "LJ", "offsetStart": 948, "offsetEnd": 950}, "context": "Than time of missus esplanade visit her own house and china herself alone of theirlocal service and the frere settlements where built for thousand of the happiest teachers Father and mother were all seated at boston waiting for the empty school at ostrog at nine o'clock a the knight of july evening a ninth jeanne annie eighteen Minutes later he heard a bill calling against the young man who had denounce him his face became a melancholy shake in his astonishment what is it said george A poor boy in has a good power for somebody's harm to be at heaven what could you to givewhat this seemed to him a hard proposition Paper it was needless to be summoned to it by the princess and the girl became very much surprise and said about recovering the bicycle with her finger to To touch it he s a gentle young ma'am and does not see any other foreign of mortality unconnected with her father who is afraid of his flesh to GSLM examples Generation at LJ-VERT They did time in the desert two or three hundred years afterward among them the castle was not my father and they were found in palestine by fire and they Another excellent is descended a breath let the corp of a prisoner and a blow was begun the bell rang the gunsprang from the captain's paul and dropped into the And then the passing future would have been too much but to waittill the end of the week and after a little time she had gone down to the palace But he passed along entirely untouched and was still together so frightened in the morning he went to look out for some place with a barian laine and then he The brast of the bravest of the entire youth and of many of the slaves of the counillllors or of every fine breed and of the princess of france has He had not in the least delicate way of helping her but had helld her into a pretty soft and a passionate graceful manner he told her every day because But that man did not fight in the second place no ne did pay the attention to poverty it is not tpossible to suspect that the man of the previous But all the people had come to see me and had not seen me again and they felt as if i were again coming to see me and so little And people stared at him for a moment as if they were dead but he had not told them of his destiny that he would do so and they had And a cow calling up his pipe said that no sign of the procession was ever heard and that no punishment was made or judgment was made nor any other Generation at LS-VERT His proposals that being so poing doubful i should very much regard and alia in boa's addition to cloak the great morning had given me a plague off waivering she Someone to found a brown line and dance spent a moment over the vessel all saw the fair young chinese yard and dry he would waved dances in bubbles from All cathics that are not due to f co notion or naturalist that is intentionble but if there is a personality of faith in them who was intentupon for seeing The reef made the partets at the corner of a platform with them rose and ground on the floor of the lobby and of chapter fourteen two thousand se of As if sudden impulse were convinced of their usual impulses and a strong exercise upon them or rather in their progress to bring their education to the reduction of manly And rushing off from the cold winds in the west in the silence of the rock the cherry wavering soft quietness of people makes breath so cheap in a course He had been burden with visitor and had petched his old preserance for death and mary the intamminable enterprising scenes caused by constantiis this trumpetts und drrawn courage and he Great worked done an artificial lines of bounding is below the had an arm as it were it lookedfted itself and everything was so exquisite that the site was hard To jew knew that they had been driven a doctor adreadful mattering to you the young girl whom eyed by a relatives ever since daily matters while a week before Evenings in a ball volume whose close ways were rotted in whther cuts that fiddler devilalonsome his wife soldiers were harassing a women with deafferenren american last grading under fair In the introduction, we argued that the clustering of a large collection of word-size speech fragments is a daunting challenge.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9975225329399109}, "created": {"value": false, "score": 1.0371208190917969e-05}, "shared": {"value": false, "score": 3.2186508178710938e-06}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999603033065796}, "created": {"value": false, "score": 0.004245400428771973}, "shared": {"value": false, "score": 4.0531158447265625e-06}}}], "references": [{"refKey": 59, "tei": "<biblStruct xml:id=\"b59\">\n\t<analytic>\n\t\t<title level=\"a\" type=\"main\">CROWDMOS: An approach for crowdsourcing mean opinion score studies</title>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Flavio</forename><surname>Ribeiro</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Dinei</forename><surname>Florencio</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Cha</forename><surname>Zhang</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Michael</forename><surname>Seltzer</surname></persName>\n\t\t</author>\n\t\t<idno type=\"DOI\">10.1109/icassp.2011.5946971</idno>\n\t</analytic>\n\t<monogr>\n\t\t<title level=\"m\">2011 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>\n\t\t<imprint>\n\t\t\t<publisher>IEEE</publisher>\n\t\t\t<date type=\"published\" when=\"2011-05\">2011</date>\n\t\t\t<biblScope unit=\"page\" from=\"2416\" to=\"2419\" />\n\t\t</imprint>\n\t</monogr>\n</biblStruct>\n"}, {"refKey": 55, "tei": "<biblStruct xml:id=\"b55\">\n\t<monogr>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Ryan</forename><surname>Prenger</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Rafael</forename><surname>Valle</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Bryan</forename><surname>Catanzaro</surname></persName>\n\t\t</author>\n\t\t<idno>CoRR, abs/1811.00002</idno>\n\t\t<title level=\"m\">Waveglow: A flow-based generative network for speech synthesis</title>\n\t\t<imprint>\n\t\t\t<date>2018</date>\n\t\t</imprint>\n\t</monogr>\n</biblStruct>\n"}, {"refKey": 40, "tei": "<biblStruct xml:id=\"b40\">\n\t<monogr>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Kushal</forename><surname>Lakhotia</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Evgeny</forename><surname>Kharitonov</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Wei-Ning</forename><surname>Hsu</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Yossi</forename><surname>Adi</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Adam</forename><surname>Polyak</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Benjamin</forename><surname>Bolte</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Anh</forename><surname>Tu</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Jade</forename><surname>Nguyen</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Alexei</forename><surname>Copet</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Adelrahman</forename><surname>Baevski</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Emmanuel</forename><surname>Mohamed</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><surname>Dupoux</surname></persName>\n\t\t</author>\n\t\t<idno>CoRR, abs/2102.01192</idno>\n\t\t<title level=\"m\">Generative spoken language modeling from raw audio</title>\n\t\t<imprint>\n\t\t\t<date>2021</date>\n\t\t</imprint>\n\t</monogr>\n</biblStruct>\n"}, {"refKey": 26, "tei": "<biblStruct xml:id=\"b26\">\n\t<monogr>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Jeff</forename><surname>Johnson</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Matthijs</forename><surname>Douze</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Herv\u00e9</forename><surname>J\u00e9gou</surname></persName>\n\t\t</author>\n\t\t<idno>CoRR, abs/1702.08734</idno>\n\t\t<title level=\"m\">Billion-scale similarity search with gpus</title>\n\t\t<imprint>\n\t\t\t<date>2017</date>\n\t\t</imprint>\n\t</monogr>\n</biblStruct>\n"}, {"refKey": 64, "tei": "<biblStruct xml:id=\"b64\">\n\t<monogr>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Jonathan</forename><surname>Shen</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Ruoming</forename><surname>Pang</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Ron</forename><forename type=\"middle\">J</forename><surname>Weiss</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Mike</forename><surname>Schuster</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Navdeep</forename><surname>Jaitly</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Zongheng</forename><surname>Yang</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Zhifeng</forename><surname>Chen</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Yu</forename><surname>Zhang</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Yuxuan</forename><surname>Wang</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">R</forename><forename type=\"middle\">J</forename><surname>Skerry-Ryan</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Rif</forename><forename type=\"middle\">A</forename><surname>Saurous</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Yannis</forename><surname>Agiomyrgiannakis</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Yonghui</forename><surname>Wu</surname></persName>\n\t\t</author>\n\t\t<idno type=\"DOI\">10.48550/ARXIV.1712.05884</idno>\n\t\t<title level=\"m\">Natural tts synthesis by conditioning wavenet on mel spectrogram predictions</title>\n\t\t<imprint>\n\t\t\t<date>2017</date>\n\t\t</imprint>\n\t</monogr>\n</biblStruct>\n"}, {"refKey": 49, "tei": "<biblStruct xml:id=\"b49\">\n\t<analytic>\n\t\t<title level=\"a\" type=\"main\">Librispeech: An ASR corpus based on public domain audio books</title>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Vassil</forename><surname>Panayotov</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Guoguo</forename><surname>Chen</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Daniel</forename><surname>Povey</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Sanjeev</forename><surname>Khudanpur</surname></persName>\n\t\t</author>\n\t\t<idno type=\"DOI\">10.1109/icassp.2015.7178964</idno>\n\t</analytic>\n\t<monogr>\n\t\t<title level=\"m\">2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>\n\t\t<imprint>\n\t\t\t<publisher>IEEE</publisher>\n\t\t\t<date type=\"published\" when=\"2015-04\">2015</date>\n\t\t\t<biblScope unit=\"page\">5210</biblScope>\n\t\t</imprint>\n\t</monogr>\n</biblStruct>\n"}], "runtime": 20881, "id": "23a0e16ef50b8aeb575945e30acb2a34e88cfb8b", "metadata": {"id": "23a0e16ef50b8aeb575945e30acb2a34e88cfb8b"}, "original_file_path": "../../datalake/Samuel/SOFTware-Sync/downloads/xml/hal-04402373.grobid.tei.xml", "file_name": "hal-04402373.grobid.tei.xml"}