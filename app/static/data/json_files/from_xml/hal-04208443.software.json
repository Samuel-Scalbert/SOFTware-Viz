{"application": "software-mentions", "version": "0.8.0", "date": "2024-10-07T11:49+0000", "md5": "9BC0CA84BD1E0BC2A46A77CB98F73626", "mentions": [{"type": "software", "software-type": "software", "software-name": {"rawForm": "sWUGGY", "normalizedForm": "sWUGGY", "offsetStart": 8, "offsetEnd": 14}, "context": "For the sWUGGY and sBLIMP evaluations, we use the 'big' transformer language model from Lakhotia et al. (2021).", "mentionContextAttributes": {"used": {"value": true, "score": 0.9996863603591919}, "created": {"value": false, "score": 1.3113021850585938e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9996863603591919}, "created": {"value": false, "score": 0.10085535049438477}, "shared": {"value": false, "score": 8.344650268554688e-07}}, "references": [{"label": "(Nguyen et al., 2020;", "normalizedForm": "(Nguyen et al., 2020", "refKey": 22}, {"label": "Lakhotia et al., 2021)", "normalizedForm": "Lakhotia et al., 2021)", "refKey": 16}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "sBLIMP", "normalizedForm": "sBLIMP", "offsetStart": 19, "offsetEnd": 25}, "context": "For the sWUGGY and sBLIMP evaluations, we use the 'big' transformer language model from Lakhotia et al. (2021).", "mentionContextAttributes": {"used": {"value": true, "score": 0.9996863603591919}, "created": {"value": false, "score": 1.3113021850585938e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9996863603591919}, "created": {"value": false, "score": 0.10085630416870117}, "shared": {"value": false, "score": 8.344650268554688e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "HuBERT", "normalizedForm": "HuBERT", "offsetStart": 20, "offsetEnd": 26}, "context": "Differently, we use HuBERT (instead of mHuBERT). ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9838235378265381}, "created": {"value": false, "score": 3.6954879760742188e-06}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9998133778572083}, "created": {"value": true, "score": 0.9655426144599915}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "references": [{"label": "(Hsu et al., 2021)", "normalizedForm": "Hsu et al., 2021", "refKey": 8}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "sWUGGY", "normalizedForm": "sWUGGY", "offsetStart": 24, "offsetEnd": 30}, "context": "The spot-the-word task (sWUGGY) requires detecting the real word from a pair of short utterances such as 'brick' vs. 'blick.'", "mentionContextAttributes": {"used": {"value": false, "score": 0.0012592673301696777}, "created": {"value": false, "score": 0.00017505884170532227}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9996863603591919}, "created": {"value": false, "score": 0.10085535049438477}, "shared": {"value": false, "score": 8.344650268554688e-07}}, "references": [{"label": "(Nguyen et al., 2020;", "normalizedForm": "(Nguyen et al., 2020", "refKey": 22}, {"label": "Lakhotia et al., 2021)", "normalizedForm": "Lakhotia et al., 2021)", "refKey": 16}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "WaveGlow", "normalizedForm": "WaveGlow", "offsetStart": 24, "offsetEnd": 92}, "context": "based model followed by WaveGlow (Prenger et al., 2019) vocoder. Later, Polyak et al. (2021) proposed a unit-based vocoder based on the HiFi-GAN architecture to convert units to speech directly. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.00013637542724609375}, "created": {"value": false, "score": 0.04647397994995117}, "shared": {"value": false, "score": 5.960464477539062e-07}}, "documentContextAttributes": {"used": {"value": false, "score": 0.00013637542724609375}, "created": {"value": false, "score": 0.04647397994995117}, "shared": {"value": false, "score": 5.960464477539062e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "HuBERT", "normalizedForm": "HuBERT", "offsetStart": 27, "offsetEnd": 33}, "context": "Similar to prior work, for HuBERT and WavLM, we use the ninth and sixth layers for wav2vec2.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9095834493637085}, "created": {"value": false, "score": 5.6624412536621094e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9998133778572083}, "created": {"value": true, "score": 0.9655426144599915}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "references": [{"label": "(Hsu et al., 2021)", "normalizedForm": "Hsu et al., 2021", "refKey": 8}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "HuBERT", "normalizedForm": "HuBERT", "offsetStart": 34, "offsetEnd": 40}, "context": "The WavLM model is trained with a HuBERT architecture, with more data and noisy samples.", "mentionContextAttributes": {"used": {"value": false, "score": 0.0003108978271484375}, "created": {"value": false, "score": 0.058455586433410645}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9998133778572083}, "created": {"value": true, "score": 0.9655426144599915}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "references": [{"label": "(Hsu et al., 2021)", "normalizedForm": "Hsu et al., 2021", "refKey": 8}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "HuBERT", "normalizedForm": "HuBERT", "offsetStart": 39, "offsetEnd": 45}, "context": "For readability, we report results for HuBERT in the main paper.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9996944665908813}, "created": {"value": false, "score": 0.04079627990722656}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9998133778572083}, "created": {"value": true, "score": 0.9655426144599915}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "references": [{"label": "(Hsu et al., 2021)", "normalizedForm": "Hsu et al., 2021", "refKey": 8}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "mHuBERT", "normalizedForm": "mHuBERT", "offsetStart": 39, "offsetEnd": 46}, "context": "Differently, we use HuBERT (instead of mHuBERT). ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9838235378265381}, "created": {"value": false, "score": 3.6954879760742188e-06}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9838235378265381}, "created": {"value": false, "score": 3.6954879760742188e-06}, "shared": {"value": false, "score": 2.384185791015625e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "HuBERT", "normalizedForm": "HuBERT", "offsetStart": 43, "offsetEnd": 49}, "context": "In contrast to previous methods that train HuBERT from scratch, which takes up to seven days on 32 GPUs, our method converges in a few hours on a single GPU.", "mentionContextAttributes": {"used": {"value": false, "score": 0.0001226663589477539}, "created": {"value": false, "score": 0.014045298099517822}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9998133778572083}, "created": {"value": true, "score": 0.9655426144599915}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "references": [{"label": "(Hsu et al., 2021)", "normalizedForm": "Hsu et al., 2021", "refKey": 8}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "HuBERT", "normalizedForm": "HuBERT", "offsetStart": 44, "offsetEnd": 50}, "context": "We further investigate our metric on top of HuBERT (Hsu et al., 2021), wav2vec2 (Baevski et al., 2020), and WavLM (Chen et al., 2022).", "mentionContextAttributes": {"used": {"value": true, "score": 0.9771709442138672}, "created": {"value": false, "score": 0.00410991907119751}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9998133778572083}, "created": {"value": true, "score": 0.9655426144599915}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "references": [{"label": "(Hsu et al., 2021)", "normalizedForm": "Hsu et al., 2021", "refKey": 8, "offsetStart": 12595, "offsetEnd": 12613}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "sBLIMP", "normalizedForm": "sBLIMP", "offsetStart": 46, "offsetEnd": 52}, "context": "Differently, the acceptability judgment test (sBLIMP) requires detecting the syntactically correct sentence from a pair of sentences, one of which is syntactically correct and the other is wrong.", "mentionContextAttributes": {"used": {"value": false, "score": 0.015478432178497314}, "created": {"value": false, "score": 3.600120544433594e-05}, "shared": {"value": false, "score": 4.76837158203125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9996863603591919}, "created": {"value": false, "score": 0.10085630416870117}, "shared": {"value": false, "score": 8.344650268554688e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "HuBERT", "normalizedForm": "HuBERT", "offsetStart": 47, "offsetEnd": 53}, "context": "We study our method using the base versions of HuBERT, wav2vec2, and WavLM.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9326733946800232}, "created": {"value": false, "score": 3.5643577575683594e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9998133778572083}, "created": {"value": true, "score": 0.9655426144599915}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "references": [{"label": "(Hsu et al., 2021)", "normalizedForm": "Hsu et al., 2021", "refKey": 8}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "HuBERT", "normalizedForm": "HuBERT", "offsetStart": 47, "offsetEnd": 53}, "context": "We study our method using the base versions of HuBERT, wav2vec2, and WavLM.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9326733946800232}, "created": {"value": false, "score": 3.5643577575683594e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9998133778572083}, "created": {"value": true, "score": 0.9655426144599915}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "references": [{"label": "(Hsu et al., 2021)", "normalizedForm": "Hsu et al., 2021", "refKey": 8}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "HuBERT", "normalizedForm": "HuBERT", "offsetStart": 59, "offsetEnd": 65}, "context": "WavLM (Chen et al., 2022) proposes adopting the well-known HuBERT model (Hsu et al., 2021) and training it with an additional denoising process.", "mentionContextAttributes": {"used": {"value": false, "score": 0.00022488832473754883}, "created": {"value": false, "score": 0.0028591156005859375}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9998133778572083}, "created": {"value": true, "score": 0.9655426144599915}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "references": [{"label": "(Hsu et al., 2021)", "normalizedForm": "Hsu et al., 2021", "refKey": 8, "offsetStart": 28178, "offsetEnd": 28196}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "HuBERT", "normalizedForm": "HuBERT", "offsetStart": 65, "offsetEnd": 71}, "context": "The authors use a k-means model trained on top of a multilingual HuBERT (mHu-BERT) for speech representation. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.8750660419464111}, "created": {"value": false, "score": 3.230571746826172e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9998133778572083}, "created": {"value": true, "score": 0.9655426144599915}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "references": [{"label": "(Hsu et al., 2021)", "normalizedForm": "Hsu et al., 2021", "refKey": 8}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "sWUGGY", "normalizedForm": "sWUGGY", "offsetStart": 74, "offsetEnd": 80}, "context": "We evaluate the proposed method using the standard GSLM setup, i.e., ABX, sWUGGY, sBLIMP.", "mentionContextAttributes": {"used": {"value": false, "score": 0.0006775856018066406}, "created": {"value": false, "score": 0.01965034008026123}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9996863603591919}, "created": {"value": false, "score": 0.10085535049438477}, "shared": {"value": false, "score": 8.344650268554688e-07}}, "references": [{"label": "(Nguyen et al., 2020;", "normalizedForm": "(Nguyen et al., 2020", "refKey": 22}, {"label": "Lakhotia et al., 2021)", "normalizedForm": "Lakhotia et al., 2021)", "refKey": 16}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "-BERT", "normalizedForm": "-BERT", "offsetStart": 76, "offsetEnd": 81}, "context": "The authors use a k-means model trained on top of a multilingual HuBERT (mHu-BERT) for speech representation. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.8750660419464111}, "created": {"value": false, "score": 3.230571746826172e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.8750660419464111}, "created": {"value": false, "score": 3.230571746826172e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "sWUGGY", "normalizedForm": "sWUGGY", "offsetStart": 81, "offsetEnd": 87}, "context": "For instance, when considering 200 or 500 units, the absolute improvement of the sWUGGY score is 4.17 and 3.21, respectively.", "mentionContextAttributes": {"used": {"value": false, "score": 0.02215665578842163}, "created": {"value": false, "score": 6.794929504394531e-06}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9996863603591919}, "created": {"value": false, "score": 0.10085535049438477}, "shared": {"value": false, "score": 8.344650268554688e-07}}, "references": [{"label": "(Nguyen et al., 2020;", "normalizedForm": "(Nguyen et al., 2020", "refKey": 22}, {"label": "Lakhotia et al., 2021)", "normalizedForm": "Lakhotia et al., 2021)", "refKey": 16}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "sWUGGY", "normalizedForm": "sWUGGY", "offsetStart": 82, "offsetEnd": 88}, "context": "Then, we evaluate our methods using generative zero-shot evaluation tasks such as sWUGGY and sBLIMP (Nguyen et al., 2020;Lakhotia et al., 2021). ", "mentionContextAttributes": {"used": {"value": true, "score": 0.7675244808197021}, "created": {"value": false, "score": 0.011937379837036133}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9996863603591919}, "created": {"value": false, "score": 0.10085535049438477}, "shared": {"value": false, "score": 8.344650268554688e-07}}, "references": [{"label": "(Nguyen et al., 2020;", "normalizedForm": "(Nguyen et al., 2020", "refKey": 22}, {"label": "Lakhotia et al., 2021)", "normalizedForm": "Lakhotia et al., 2021)", "refKey": 16}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "sBLIMP", "normalizedForm": "sBLIMP", "offsetStart": 82, "offsetEnd": 88}, "context": "We evaluate the proposed method using the standard GSLM setup, i.e., ABX, sWUGGY, sBLIMP.", "mentionContextAttributes": {"used": {"value": false, "score": 0.0006775856018066406}, "created": {"value": false, "score": 0.01965034008026123}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9996863603591919}, "created": {"value": false, "score": 0.10085630416870117}, "shared": {"value": false, "score": 8.344650268554688e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "HuBERT", "normalizedForm": "HuBERT", "offsetStart": 84, "offsetEnd": 90}, "context": "Interestingly, while presenting better performance on various downstream tasks than HuBERT, their ABX, sWUGGY, and sBLIMP scores are lower. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.061508893966674805}, "created": {"value": false, "score": 5.125999450683594e-06}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9998133778572083}, "created": {"value": true, "score": 0.9655426144599915}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "references": [{"label": "(Hsu et al., 2021)", "normalizedForm": "Hsu et al., 2021", "refKey": 8}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "HuBERT", "normalizedForm": "HuBERT", "offsetStart": 84, "offsetEnd": 90}, "context": "In Figure 2, we use our metric to study the robustness of k-means trained on top of HuBERT with various augmentations and values of K.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9998133778572083}, "created": {"value": false, "score": 0.0035547614097595215}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9998133778572083}, "created": {"value": true, "score": 0.9655426144599915}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "references": [{"label": "(Hsu et al., 2021)", "normalizedForm": "Hsu et al., 2021", "refKey": 8}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "sBLIMP", "normalizedForm": "sBLIMP", "offsetStart": 93, "offsetEnd": 142}, "context": "Then, we evaluate our methods using generative zero-shot evaluation tasks such as sWUGGY and sBLIMP (Nguyen et al., 2020;Lakhotia et al., 2021). ", "mentionContextAttributes": {"used": {"value": true, "score": 0.7675244808197021}, "created": {"value": false, "score": 0.011937379837036133}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9996863603591919}, "created": {"value": false, "score": 0.10085630416870117}, "shared": {"value": false, "score": 8.344650268554688e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "sWUGGY", "normalizedForm": "sWUGGY", "offsetStart": 103, "offsetEnd": 109}, "context": "Interestingly, while presenting better performance on various downstream tasks than HuBERT, their ABX, sWUGGY, and sBLIMP scores are lower.", "mentionContextAttributes": {"used": {"value": false, "score": 0.061508893966674805}, "created": {"value": false, "score": 5.125999450683594e-06}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9996863603591919}, "created": {"value": false, "score": 0.10085535049438477}, "shared": {"value": false, "score": 8.344650268554688e-07}}, "references": [{"label": "(Nguyen et al., 2020;", "normalizedForm": "(Nguyen et al., 2020", "refKey": 22}, {"label": "Lakhotia et al., 2021)", "normalizedForm": "Lakhotia et al., 2021)", "refKey": 16}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "sBLIMP", "normalizedForm": "sBLIMP", "offsetStart": 115, "offsetEnd": 121}, "context": "Interestingly, while presenting better performance on various downstream tasks than HuBERT, their ABX, sWUGGY, and sBLIMP scores are lower.", "mentionContextAttributes": {"used": {"value": false, "score": 0.061508893966674805}, "created": {"value": false, "score": 5.125999450683594e-06}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9996863603591919}, "created": {"value": false, "score": 0.10085630416870117}, "shared": {"value": false, "score": 8.344650268554688e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "HuBERT", "normalizedForm": "HuBERT", "offsetStart": 116, "offsetEnd": 122}, "context": "The quantizer is learned on top of a trained encoder, e.g., k-means trained on each embedding vector extracted from HuBERT.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9848892688751221}, "created": {"value": false, "score": 1.8715858459472656e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9998133778572083}, "created": {"value": true, "score": 0.9655426144599915}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "references": [{"label": "(Hsu et al., 2021)", "normalizedForm": "Hsu et al., 2021", "refKey": 8}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "HuBERT", "normalizedForm": "HuBERT", "offsetStart": 131, "offsetEnd": 137}, "context": "In the previous section, we presented a pseudolabeling approach that relies on a converged quantizer E 0 , e.g., k-means on top of HuBERT.", "mentionContextAttributes": {"used": {"value": false, "score": 0.00010001659393310547}, "created": {"value": true, "score": 0.9655426144599915}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9998133778572083}, "created": {"value": true, "score": 0.9655426144599915}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "references": [{"label": "(Hsu et al., 2021)", "normalizedForm": "Hsu et al., 2021", "refKey": 8}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "sWUGGY", "normalizedForm": "sWUGGY", "offsetStart": 135, "offsetEnd": 141}, "context": "Overall the proposed tasks can be divided into four main groups: (i) acoustic encoding using ABX, bitrat, (ii) language encoding using sWUGGY, sBLIMP (Nguyen et al., 2020;Lakhotia et al., 2021), (iii) resynthesis using Phoneme/Word Error Rate; (iv) speech generation using VERT (Lakhotia et al., 2021), Meaningfulness Mean Opinion Score.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9748736023902893}, "created": {"value": false, "score": 8.702278137207031e-06}, "shared": {"value": false, "score": 8.344650268554688e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9996863603591919}, "created": {"value": false, "score": 0.10085535049438477}, "shared": {"value": false, "score": 8.344650268554688e-07}}, "references": [{"label": "(Nguyen et al., 2020;", "normalizedForm": "(Nguyen et al., 2020", "refKey": 22, "offsetStart": 8859, "offsetEnd": 8880}, {"label": "Lakhotia et al., 2021)", "normalizedForm": "Lakhotia et al., 2021)", "refKey": 16, "offsetStart": 8880, "offsetEnd": 8902}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "pyroomacoustics", "normalizedForm": "pyroomacoustics", "offsetStart": 137, "offsetEnd": 177}, "context": "We follow a similar setting of Chazan et al. ( 2021), in which we consider an Acoustic Transfer Function (ATF) to be simulated using the pyroomacoustics (Scheibler et al., 2018) audio room simulations package. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.03913223743438721}, "created": {"value": false, "score": 0.425076961517334}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": false, "score": 0.03913223743438721}, "created": {"value": false, "score": 0.425076961517334}, "shared": {"value": false, "score": 2.384185791015625e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "sBLIMP", "normalizedForm": "sBLIMP", "offsetStart": 143, "offsetEnd": 192}, "context": "Overall the proposed tasks can be divided into four main groups: (i) acoustic encoding using ABX, bitrat, (ii) language encoding using sWUGGY, sBLIMP (Nguyen et al., 2020;Lakhotia et al., 2021), (iii) resynthesis using Phoneme/Word Error Rate; (iv) speech generation using VERT (Lakhotia et al., 2021), Meaningfulness Mean Opinion Score.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9748736023902893}, "created": {"value": false, "score": 8.702278137207031e-06}, "shared": {"value": false, "score": 8.344650268554688e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9996863603591919}, "created": {"value": false, "score": 0.10085630416870117}, "shared": {"value": false, "score": 8.344650268554688e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "sWUGGY", "normalizedForm": "sWUGGY", "offsetStart": 151, "offsetEnd": 157}, "context": "Specifically, we evaluate the newly proposed speech encoders when considering zero-shot evaluation tasks considering encoding and modeling, i.e., ABX, sWUGGY, and sBLIMP (Nguyen et al., 2020), together with a high-level downstream task in the form of speechto-speech translation.", "mentionContextAttributes": {"used": {"value": false, "score": 0.000998079776763916}, "created": {"value": false, "score": 0.10085535049438477}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9996863603591919}, "created": {"value": false, "score": 0.10085535049438477}, "shared": {"value": false, "score": 8.344650268554688e-07}}, "references": [{"label": "(Nguyen et al., 2020;", "normalizedForm": "(Nguyen et al., 2020", "refKey": 22}, {"label": "Lakhotia et al., 2021)", "normalizedForm": "Lakhotia et al., 2021)", "refKey": 16}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "HuBERT", "normalizedForm": "HuBERT", "offsetStart": 152, "offsetEnd": 158}, "context": "The high numbers and the monotonicity of the UED as a function of K are consistent for all values of K, augmentations, and models we experimented with (HuBERT, wav2vec2, and WavLM).", "mentionContextAttributes": {"used": {"value": true, "score": 0.9997537732124329}, "created": {"value": false, "score": 1.1444091796875e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9998133778572083}, "created": {"value": true, "score": 0.9655426144599915}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "references": [{"label": "(Hsu et al., 2021)", "normalizedForm": "Hsu et al., 2021", "refKey": 8}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "sBLIMP", "normalizedForm": "sBLIMP", "offsetStart": 163, "offsetEnd": 169}, "context": "Specifically, we evaluate the newly proposed speech encoders when considering zero-shot evaluation tasks considering encoding and modeling, i.e., ABX, sWUGGY, and sBLIMP (Nguyen et al., 2020), together with a high-level downstream task in the form of speechto-speech translation.", "mentionContextAttributes": {"used": {"value": false, "score": 0.000998079776763916}, "created": {"value": false, "score": 0.10085630416870117}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9996863603591919}, "created": {"value": false, "score": 0.10085630416870117}, "shared": {"value": false, "score": 8.344650268554688e-07}}}], "references": [{"refKey": 22, "tei": "<biblStruct xml:id=\"b22\">\n\t<analytic>\n\t\t<title level=\"a\" type=\"main\">The zero resource speech benchmark 2021: Metrics and baselines for unsupervised spoken language modeling</title>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Anh</forename><surname>Tu</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Maureen</forename><surname>Nguyen</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Patricia</forename><surname>De Seyssel</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Morgane</forename><surname>Roz\u00e9</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Evgeny</forename><surname>Rivi\u00e8re</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Alexei</forename><surname>Kharitonov</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Ewan</forename><surname>Baevski</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Emmanuel</forename><surname>Dunbar</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><surname>Dupoux</surname></persName>\n\t\t</author>\n\t</analytic>\n\t<monogr>\n\t\t<title level=\"m\">NeurIPS -Self-Supervised Learning for Speech and Audio Processing Workshop</title>\n\t\t<imprint>\n\t\t\t<date>2020</date>\n\t\t</imprint>\n\t</monogr>\n</biblStruct>\n"}, {"refKey": 16, "tei": "<biblStruct xml:id=\"b16\">\n\t<analytic>\n\t\t<title level=\"a\" type=\"main\">On Generative Spoken Language Modeling from Raw Audio</title>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Kushal</forename><surname>Lakhotia</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Eugene</forename><surname>Kharitonov</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Wei-Ning</forename><surname>Hsu</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Yossi</forename><surname>Adi</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Adam</forename><surname>Polyak</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Benjamin</forename><surname>Bolte</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Tu-Anh</forename><surname>Nguyen</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Jade</forename><surname>Copet</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Alexei</forename><surname>Baevski</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Abdelrahman</forename><surname>Mohamed</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Emmanuel</forename><surname>Dupoux</surname></persName>\n\t\t</author>\n\t</analytic>\n\t<monogr>\n\t\t<title level=\"j\">TACL</title>\n\t\t<imprint>\n\t\t\t<date>2021</date>\n\t\t</imprint>\n\t</monogr>\n</biblStruct>\n"}, {"refKey": 8, "tei": "<biblStruct xml:id=\"b8\">\n\t<analytic>\n\t\t<title level=\"a\" type=\"main\">HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units</title>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Wei-Ning</forename><surname>Hsu</surname></persName>\n\t\t\t<idno type=\"ORCID\">0000-0001-5546-5217</idno>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Benjamin</forename><surname>Bolte</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Yao-Hung Hubert</forename><surname>Tsai</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Kushal</forename><surname>Lakhotia</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Ruslan</forename><surname>Salakhutdinov</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Abdelrahman</forename><surname>Mohamed</surname></persName>\n\t\t</author>\n\t\t<idno type=\"DOI\">10.1109/taslp.2021.3122291</idno>\n\t</analytic>\n\t<monogr>\n\t\t<title level=\"j\">IEEE/ACM Transactions on Audio, Speech, and Language Processing</title>\n\t\t<title level=\"j\" type=\"abbrev\">IEEE/ACM Trans. Audio Speech Lang. Process.</title>\n\t\t<idno type=\"ISSN\">2329-9290</idno>\n\t\t<idno type=\"ISSNe\">2329-9304</idno>\n\t\t<imprint>\n\t\t\t<biblScope unit=\"volume\">29</biblScope>\n\t\t\t<biblScope unit=\"page\" from=\"3451\" to=\"3460\" />\n\t\t\t<date type=\"published\" when=\"2021\">2021</date>\n\t\t\t<publisher>Institute of Electrical and Electronics Engineers (IEEE)</publisher>\n\t\t</imprint>\n\t</monogr>\n</biblStruct>\n"}], "runtime": 5322, "id": "7be904fab0e6e38df13b08f8cf8cf8e896c065f0", "metadata": {"id": "7be904fab0e6e38df13b08f8cf8cf8e896c065f0"}, "original_file_path": "../../datalake/Samuel/SOFTware-Sync/downloads/xml/hal-04208443.grobid.tei.xml", "file_name": "hal-04208443.grobid.tei.xml"}