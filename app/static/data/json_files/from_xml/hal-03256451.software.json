{"application": "software-mentions", "version": "0.8.0", "date": "2024-10-07T12:48+0000", "md5": "380AA218B94FB3C415A9162E74B16C56", "mentions": [{"type": "software", "software-type": "software", "software-name": {"rawForm": "sineSPE", "normalizedForm": "sineSPE", "offsetStart": 2, "offsetEnd": 9}, "context": "\u2022 sineSPE: We set the number of sines K = 5.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9999902248382568}, "created": {"value": false, "score": 3.4570693969726562e-06}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999902248382568}, "created": {"value": false, "score": 0.00031507015228271484}, "shared": {"value": false, "score": 2.384185791015625e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "convSPE", "normalizedForm": "convSPE", "offsetStart": 2, "offsetEnd": 9}, "context": "\u2022 convSPE: We use filters of size 128.", "mentionContextAttributes": {"used": {"value": true, "score": 0.8707099556922913}, "created": {"value": false, "score": 4.887580871582031e-06}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.999993085861206}, "created": {"value": false, "score": 0.09803891181945801}, "shared": {"value": false, "score": 5.960464477539062e-07}}, "references": [{"label": "(Figures 11 and 12)", "normalizedForm": "Figures 11 and 12", "refKey": 0}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "SPE", "normalizedForm": "SPE", "offsetStart": 4, "offsetEnd": 7}, "context": "For SPE, we considered both the gated and ungated variants with as many realizations as fit in memory (between 16 and 64).", "mentionContextAttributes": {"used": {"value": true, "score": 0.9999521970748901}, "created": {"value": false, "score": 2.944469451904297e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999746084213257}, "created": {"value": true, "score": 0.9999332427978516}, "shared": {"value": false, "score": 0.007222533226013184}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "convSPE", "normalizedForm": "convSPE", "offsetStart": 4, "offsetEnd": 11}, "context": "Our convSPE variant involves convolving random noise.", "mentionContextAttributes": {"used": {"value": false, "score": 4.291534423828125e-05}, "created": {"value": false, "score": 0.09803891181945801}, "shared": {"value": false, "score": 5.960464477539062e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.999993085861206}, "created": {"value": false, "score": 0.09803891181945801}, "shared": {"value": false, "score": 5.960464477539062e-07}}, "references": [{"label": "(Figures 11 and 12)", "normalizedForm": "Figures 11 and 12", "refKey": 0}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "sineSPE", "normalizedForm": "sineSPE", "offsetStart": 4, "offsetEnd": 11}, "context": "For sineSPE, we choose the number of sines K = 5; for convSPE, the convolutional filter size is set to be 128, 512 for the gated and ungated variants respectively.", "mentionContextAttributes": {"used": {"value": false, "score": 0.3714142441749573}, "created": {"value": false, "score": 7.033348083496094e-06}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999902248382568}, "created": {"value": false, "score": 0.00031507015228271484}, "shared": {"value": false, "score": 2.384185791015625e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "sineSPE", "normalizedForm": "sineSPE", "offsetStart": 5, "offsetEnd": 12}, "context": "Both sineSPE and convSPE are much more stable in this regard, confirming the result from section 3.2 that SPE extrapolates better beyond the training sequence length.", "mentionContextAttributes": {"used": {"value": false, "score": 0.00771331787109375}, "created": {"value": false, "score": 1.9073486328125e-06}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999902248382568}, "created": {"value": false, "score": 0.00031507015228271484}, "shared": {"value": false, "score": 2.384185791015625e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "SPE", "normalizedForm": "SPE", "offsetStart": 8, "offsetEnd": 11}, "context": "Indeed, SPE as it is presented here holds theoretically for dot-product attention kernels only, but our results given in Table 1 suggest that this generalizes, asking an interesting research question.", "mentionContextAttributes": {"used": {"value": false, "score": 0.07817280292510986}, "created": {"value": false, "score": 0.0005331635475158691}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999746084213257}, "created": {"value": true, "score": 0.9999332427978516}, "shared": {"value": false, "score": 0.007222533226013184}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "convSPE", "normalizedForm": "convSPE", "offsetStart": 10, "offsetEnd": 17}, "context": "Regarding convSPE, its performance in the LRA is not as remarkable as it is for the music generation experiment reported later in section 3.2.", "mentionContextAttributes": {"used": {"value": false, "score": 0.1986016035079956}, "created": {"value": false, "score": 1.33514404296875e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.999993085861206}, "created": {"value": false, "score": 0.09803891181945801}, "shared": {"value": false, "score": 5.960464477539062e-07}}, "references": [{"label": "(Figures 11 and 12)", "normalizedForm": "Figures 11 and 12", "refKey": 0}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "convSPE", "normalizedForm": "convSPE", "offsetStart": 10, "offsetEnd": 17}, "context": "The gated convSPE is able to look much further back in the middle layers than its ungated counterpart.", "mentionContextAttributes": {"used": {"value": false, "score": 0.0009659528732299805}, "created": {"value": false, "score": 0.0001024007797241211}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.999993085861206}, "created": {"value": false, "score": 0.09803891181945801}, "shared": {"value": false, "score": 5.960464477539062e-07}}, "references": [{"label": "(Figures 11 and 12)", "normalizedForm": "Figures 11 and 12", "refKey": 0}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "SPE", "normalizedForm": "SPE", "offsetStart": 12, "offsetEnd": 15}, "context": "We evaluate SPE (the gated variant) on two efficient Transformer models: the (softmax) Performer (Choromanski et al., 2020), and a Linear Transformer (Katharopoulos et al., 2020) with a ReLU feature map, i.e. choosing \u03c6(\u2022) = max(0, \u2022) element-wise in (3). 4 It should be noted that the ReLU feature map does not approximate the softmax kernel, which SPE is designed for (see assumption 8).", "mentionContextAttributes": {"used": {"value": true, "score": 0.9990382194519043}, "created": {"value": false, "score": 3.6954879760742188e-06}, "shared": {"value": false, "score": 8.344650268554688e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999746084213257}, "created": {"value": true, "score": 0.9999332427978516}, "shared": {"value": false, "score": 0.007222533226013184}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "SPE", "normalizedForm": "SPE", "offsetStart": 13, "offsetEnd": 16}, "context": "As expected, SPE variants greatly outperform APE in terms of translation invariance.", "mentionContextAttributes": {"used": {"value": false, "score": 0.010897159576416016}, "created": {"value": false, "score": 6.67572021484375e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999746084213257}, "created": {"value": true, "score": 0.9999332427978516}, "shared": {"value": false, "score": 0.007222533226013184}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "SPE", "normalizedForm": "SPE", "offsetStart": 13, "offsetEnd": 16}, "context": "In any case, SPE scores are remarkably stable across positions, contrarily to APE, which rapidly degrades beyond the training length.", "mentionContextAttributes": {"used": {"value": false, "score": 0.0016965866088867188}, "created": {"value": false, "score": 4.1604042053222656e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999746084213257}, "created": {"value": true, "score": 0.9999332427978516}, "shared": {"value": false, "score": 0.007222533226013184}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "SPE", "normalizedForm": "SPE", "offsetStart": 13, "offsetEnd": 16}, "context": "We share the SPE modules across all layers of the Performer, but not across the attention heads, resulting in 512 learned positional kernels P d ) (number of heads \u00d7 key dimensions per head.", "mentionContextAttributes": {"used": {"value": true, "score": 0.5242013931274414}, "created": {"value": false, "score": 4.5180320739746094e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999746084213257}, "created": {"value": true, "score": 0.9999332427978516}, "shared": {"value": false, "score": 0.007222533226013184}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "SPE", "normalizedForm": "SPE", "offsetStart": 14, "offsetEnd": 17}, "context": "Convolutional SPE yields:", "mentionContextAttributes": {"used": {"value": false, "score": 0.0010648369789123535}, "created": {"value": false, "score": 4.291534423828125e-06}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999746084213257}, "created": {"value": true, "score": 0.9999332427978516}, "shared": {"value": false, "score": 0.007222533226013184}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "SPE", "normalizedForm": "SPE", "offsetStart": 16, "offsetEnd": 19}, "context": "We can see that SPE-based models have at most 3.1 % more parameters than the baselines.", "mentionContextAttributes": {"used": {"value": false, "score": 0.0011647343635559082}, "created": {"value": false, "score": 3.838539123535156e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999746084213257}, "created": {"value": true, "score": 0.9999332427978516}, "shared": {"value": false, "score": 0.007222533226013184}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "SPE", "normalizedForm": "SPE", "offsetStart": 17, "offsetEnd": 20}, "context": "The key idea for SPE is to see the attention kernel P d (m, n) as a covariance:", "mentionContextAttributes": {"used": {"value": false, "score": 0.00027817487716674805}, "created": {"value": false, "score": 0.25505930185317993}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999746084213257}, "created": {"value": true, "score": 0.9999332427978516}, "shared": {"value": false, "score": 0.007222533226013184}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "convSPE", "normalizedForm": "convSPE", "offsetStart": 17, "offsetEnd": 24}, "context": "Both sineSPE and convSPE are much more stable in this regard, confirming the result from section 3.2 that SPE extrapolates better beyond the training sequence length.", "mentionContextAttributes": {"used": {"value": false, "score": 0.00771331787109375}, "created": {"value": false, "score": 1.9073486328125e-06}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.999993085861206}, "created": {"value": false, "score": 0.09803891181945801}, "shared": {"value": false, "score": 5.960464477539062e-07}}, "references": [{"label": "(Figures 11 and 12)", "normalizedForm": "Figures 11 and 12", "refKey": 0}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "convSPE", "normalizedForm": "convSPE", "offsetStart": 17, "offsetEnd": 24}, "context": "On the contrary, convSPE achieved by far the highest accuracy with R = 4.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9998821020126343}, "created": {"value": false, "score": 1.1920928955078125e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.999993085861206}, "created": {"value": false, "score": 0.09803891181945801}, "shared": {"value": false, "score": 5.960464477539062e-07}}, "references": [{"label": "(Figures 11 and 12)", "normalizedForm": "Figures 11 and 12", "refKey": 0}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "SPE", "normalizedForm": "SPE", "offsetStart": 18, "offsetEnd": 21}, "context": "For convolutional SPE, we share Q and K across all layers (but not across attention heads); for sinusoidal SPE, Q and K are unique to each layer and head; in both cases, layerspecific gating is employed.", "mentionContextAttributes": {"used": {"value": false, "score": 0.004274249076843262}, "created": {"value": false, "score": 6.16312026977539e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999746084213257}, "created": {"value": true, "score": 0.9999332427978516}, "shared": {"value": false, "score": 0.007222533226013184}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Pathfinder", "normalizedForm": "Pathfinder", "offsetStart": 18, "offsetEnd": 28}, "context": "We do not include Pathfinder (a synthetic image classification task) as we were unable to reproduce the results of Tay et al. on this task, even through correspondence with the authors.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9994944334030151}, "created": {"value": false, "score": 6.783008575439453e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9994944334030151}, "created": {"value": false, "score": 6.783008575439453e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "SPE", "normalizedForm": "SPE", "offsetStart": 19, "offsetEnd": 22}, "context": "We notice that all SPE variants, especially convSPE, behave much better than APE for token positions beyond 2 048.", "mentionContextAttributes": {"used": {"value": false, "score": 0.0010606646537780762}, "created": {"value": false, "score": 4.887580871582031e-06}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999746084213257}, "created": {"value": true, "score": 0.9999332427978516}, "shared": {"value": false, "score": 0.007222533226013184}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "SPE", "normalizedForm": "SPE", "offsetStart": 19, "offsetEnd": 22}, "context": "This suggests that SPE inherits this celebrated advantage of RPE (Huang et al., 2018) while being applicable to much longer sequences.", "mentionContextAttributes": {"used": {"value": false, "score": 0.001416921615600586}, "created": {"value": false, "score": 2.753734588623047e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999746084213257}, "created": {"value": true, "score": 0.9999332427978516}, "shared": {"value": false, "score": 0.007222533226013184}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "SPE", "normalizedForm": "SPE", "offsetStart": 19, "offsetEnd": 22}, "context": "In models that use SPE, Q and K are shared across all layers (but not across attention heads); layer-specific gating is employed for models trained with gated SPE.", "mentionContextAttributes": {"used": {"value": false, "score": 0.003587067127227783}, "created": {"value": false, "score": 7.021427154541016e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999746084213257}, "created": {"value": true, "score": 0.9999332427978516}, "shared": {"value": false, "score": 0.007222533226013184}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "SPE", "normalizedForm": "SPE", "offsetStart": 20, "offsetEnd": 23}, "context": "We clearly see that SPE substantially outperforms APE in both metrics.", "mentionContextAttributes": {"used": {"value": false, "score": 0.009880900382995605}, "created": {"value": false, "score": 0.00013399124145507812}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999746084213257}, "created": {"value": true, "score": 0.9999332427978516}, "shared": {"value": false, "score": 0.007222533226013184}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "SPE", "normalizedForm": "SPE", "offsetStart": 20, "offsetEnd": 23}, "context": "In models employing SPE, APE is removed.", "mentionContextAttributes": {"used": {"value": false, "score": 0.00016176700592041016}, "created": {"value": false, "score": 0.00028055906295776367}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999746084213257}, "created": {"value": true, "score": 0.9999332427978516}, "shared": {"value": false, "score": 0.007222533226013184}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "convSPE", "normalizedForm": "convSPE", "offsetStart": 20, "offsetEnd": 27}, "context": "On this metric, our convSPE variant performs the", "mentionContextAttributes": {"used": {"value": true, "score": 0.6692786812782288}, "created": {"value": false, "score": 0.058579981327056885}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.999993085861206}, "created": {"value": false, "score": 0.09803891181945801}, "shared": {"value": false, "score": 5.960464477539062e-07}}, "references": [{"label": "(Figures 11 and 12)", "normalizedForm": "Figures 11 and 12", "refKey": 0}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "SPE", "normalizedForm": "SPE", "offsetStart": 21, "offsetEnd": 24}, "context": "(ug: trained without SPE gating).", "mentionContextAttributes": {"used": {"value": true, "score": 0.935672402381897}, "created": {"value": false, "score": 1.33514404296875e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999746084213257}, "created": {"value": true, "score": 0.9999332427978516}, "shared": {"value": false, "score": 0.007222533226013184}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "SPE", "normalizedForm": "SPE", "offsetStart": 25, "offsetEnd": 28}, "context": "\u2022 We study the impact of SPE on performance on the Long-Range Arena benchmark (Tay et al., 2021) and two music generation tasks.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9834604859352112}, "created": {"value": false, "score": 0.0013990998268127441}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999746084213257}, "created": {"value": true, "score": 0.9999332427978516}, "shared": {"value": false, "score": 0.007222533226013184}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "convSPE", "normalizedForm": "convSPE", "offsetStart": 25, "offsetEnd": 32}, "context": "The same can be said for convSPE (Figures 11 and 12).", "mentionContextAttributes": {"used": {"value": false, "score": 0.17727786302566528}, "created": {"value": false, "score": 4.0531158447265625e-06}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.999993085861206}, "created": {"value": false, "score": 0.09803891181945801}, "shared": {"value": false, "score": 5.960464477539062e-07}}, "references": [{"label": "(Figures 11 and 12)", "normalizedForm": "Figures 11 and 12", "refKey": 0, "offsetStart": 47739, "offsetEnd": 47758}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "SPE", "normalizedForm": "SPE", "offsetStart": 26, "offsetEnd": 29}, "context": "5Regarding performance of SPE, we first notice that the sineSPE variant yields the best results on three tasks, which is a strong achievement and validates our approach, especially considering the difficulty of this evaluation benchmark.", "mentionContextAttributes": {"used": {"value": false, "score": 0.06614184379577637}, "created": {"value": false, "score": 2.2649765014648438e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999746084213257}, "created": {"value": true, "score": 0.9999332427978516}, "shared": {"value": false, "score": 0.007222533226013184}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "SPE", "normalizedForm": "SPE", "offsetStart": 27, "offsetEnd": 30}, "context": "We propose two variants of SPE to handle this important special case, illustrated in Figure 2. The first variant yields periodic covariance functions.", "mentionContextAttributes": {"used": {"value": false, "score": 4.3392181396484375e-05}, "created": {"value": true, "score": 0.9667505025863647}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999746084213257}, "created": {"value": true, "score": 0.9999332427978516}, "shared": {"value": false, "score": 0.007222533226013184}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "SPE", "normalizedForm": "SPE", "offsetStart": 31, "offsetEnd": 34}, "context": "In practice, we can share some SPE parameters across the network, notably across layers, to strongly reduce computing time and memory usage.", "mentionContextAttributes": {"used": {"value": false, "score": 0.0003198385238647461}, "created": {"value": false, "score": 0.005257606506347656}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999746084213257}, "created": {"value": true, "score": 0.9999332427978516}, "shared": {"value": false, "score": 0.007222533226013184}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "SPE", "normalizedForm": "SPE", "offsetStart": 31, "offsetEnd": 34}, "context": "This behaviour is not found in SPE variants, which consistently attend to all positions.", "mentionContextAttributes": {"used": {"value": false, "score": 0.0004234910011291504}, "created": {"value": false, "score": 3.707408905029297e-05}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999746084213257}, "created": {"value": true, "score": 0.9999332427978516}, "shared": {"value": false, "score": 0.007222533226013184}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "SPE", "normalizedForm": "SPE", "offsetStart": 33, "offsetEnd": 36}, "context": "In our experiments, we show that SPE brings an interesting gain in performance for large-scale transformer models (Choromanski et al., 2020;Katharopoulos et al., 2020), as compared to classical (sinusoidal) PE.", "mentionContextAttributes": {"used": {"value": false, "score": 0.004310786724090576}, "created": {"value": false, "score": 0.027281463146209717}, "shared": {"value": false, "score": 4.76837158203125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999746084213257}, "created": {"value": true, "score": 0.9999332427978516}, "shared": {"value": false, "score": 0.007222533226013184}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "sineSPE", "normalizedForm": "sineSPE", "offsetStart": 35, "offsetEnd": 42}, "context": "From Figure 7, we can observe that sineSPE learns to exploit a wide range of frequencies, and that convSPE is effective within small query-key offsets corresponding to the filter size, as expected.", "mentionContextAttributes": {"used": {"value": false, "score": 0.004227578639984131}, "created": {"value": false, "score": 0.00022166967391967773}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999902248382568}, "created": {"value": false, "score": 0.00031507015228271484}, "shared": {"value": false, "score": 2.384185791015625e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "SPE", "normalizedForm": "SPE", "offsetStart": 36, "offsetEnd": 39}, "context": "Nevertheless, it is possible to use SPE with any feature map in practice, allowing us to include Linear Transformer-ReLU as an interesting test of generalization to alternative kernels.", "mentionContextAttributes": {"used": {"value": false, "score": 0.00017493963241577148}, "created": {"value": false, "score": 0.00022464990615844727}, "shared": {"value": false, "score": 4.76837158203125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999746084213257}, "created": {"value": true, "score": 0.9999332427978516}, "shared": {"value": false, "score": 0.007222533226013184}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "SPE", "normalizedForm": "SPE", "offsetStart": 37, "offsetEnd": 40}, "context": "We consider baseline APE, as well as SPE: sinusoidal or convolutional, with or without gating, resulting in 5 different models.", "mentionContextAttributes": {"used": {"value": false, "score": 0.0013173222541809082}, "created": {"value": false, "score": 1.6808509826660156e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999746084213257}, "created": {"value": true, "score": 0.9999332427978516}, "shared": {"value": false, "score": 0.007222533226013184}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "SPE", "normalizedForm": "SPE", "offsetStart": 39, "offsetEnd": 42}, "context": "On the contrary, the scores of ungated SPE models, i.e., models in which we enforce the incorporation of positional information in every layer, remain remarkably consistent throughout the positions.", "mentionContextAttributes": {"used": {"value": false, "score": 0.00032329559326171875}, "created": {"value": false, "score": 7.700920104980469e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999746084213257}, "created": {"value": true, "score": 0.9999332427978516}, "shared": {"value": false, "score": 0.007222533226013184}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "SPE", "normalizedForm": "SPE", "offsetStart": 39, "offsetEnd": 42}, "context": "In the main document, we discussed how SPE asymptotically leads to the desired cross-covariance structure as R grows to infinity.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9987472295761108}, "created": {"value": false, "score": 0.3835017681121826}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999746084213257}, "created": {"value": true, "score": 0.9999332427978516}, "shared": {"value": false, "score": 0.007222533226013184}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "SPE", "normalizedForm": "SPE", "offsetStart": 40, "offsetEnd": 43}, "context": "The most straightforward application of SPE arises when we pick P d (m, n) = P d (m -n), i.e. a stationary position kernel, which was coined in as choosing relative attention in Shaw et al. (2018) and boils down to enforcing a Toeplitz structure for the cross-covariance matrix", "mentionContextAttributes": {"used": {"value": false, "score": 0.000938713550567627}, "created": {"value": false, "score": 0.0004699230194091797}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999746084213257}, "created": {"value": true, "score": 0.9999332427978516}, "shared": {"value": false, "score": 0.007222533226013184}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "SPE", "normalizedForm": "SPE", "offsetStart": 40, "offsetEnd": 43}, "context": "Such behavior is not seen in any of our SPE models.", "mentionContextAttributes": {"used": {"value": false, "score": 0.0001571178436279297}, "created": {"value": false, "score": 0.0024948716163635254}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999746084213257}, "created": {"value": true, "score": 0.9999332427978516}, "shared": {"value": false, "score": 0.007222533226013184}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "sineSPE", "normalizedForm": "sineSPE", "offsetStart": 44, "offsetEnd": 51}, "context": "Variant I. Relative and periodic attention (sineSPE).", "mentionContextAttributes": {"used": {"value": true, "score": 0.9998121857643127}, "created": {"value": false, "score": 4.0531158447265625e-06}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999902248382568}, "created": {"value": false, "score": 0.00031507015228271484}, "shared": {"value": false, "score": 2.384185791015625e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "convSPE", "normalizedForm": "convSPE", "offsetStart": 44, "offsetEnd": 51}, "context": "We notice that all SPE variants, especially convSPE, behave much better than APE for token positions beyond 2 048.", "mentionContextAttributes": {"used": {"value": false, "score": 0.0010606646537780762}, "created": {"value": false, "score": 4.887580871582031e-06}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.999993085861206}, "created": {"value": false, "score": 0.09803891181945801}, "shared": {"value": false, "score": 5.960464477539062e-07}}, "references": [{"label": "(Figures 11 and 12)", "normalizedForm": "Figures 11 and 12", "refKey": 0}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "SPE", "normalizedForm": "SPE", "offsetStart": 45, "offsetEnd": 48}, "context": "\u2022 We propose Stochastic Positional Encoding (SPE) as a general PE scheme in the keys domain, that enforces a particular attention pattern devised in the attention domain.", "mentionContextAttributes": {"used": {"value": false, "score": 0.0001430511474609375}, "created": {"value": true, "score": 0.9998923540115356}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999746084213257}, "created": {"value": true, "score": 0.9999332427978516}, "shared": {"value": false, "score": 0.007222533226013184}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "SPE", "normalizedForm": "SPE", "offsetStart": 47, "offsetEnd": 50}, "context": "This can bring significant memory savings when SPE is used as a drop-in addition to networks trained with large batch sizes.", "mentionContextAttributes": {"used": {"value": false, "score": 9.40561294555664e-05}, "created": {"value": false, "score": 4.553794860839844e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999746084213257}, "created": {"value": true, "score": 0.9999332427978516}, "shared": {"value": false, "score": 0.007222533226013184}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "pytorch", "normalizedForm": "pytorch", "offsetStart": 47, "offsetEnd": 54}, "context": "Our music Performers are implemented using the pytorch-fast-transformers package,8 modified as necessary to incorporate SPE. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.49382609128952026}, "created": {"value": true, "score": 0.6066081523895264}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": false, "score": 0.49382609128952026}, "created": {"value": true, "score": 0.6066081523895264}, "shared": {"value": false, "score": 2.384185791015625e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "sineSPE", "normalizedForm": "sineSPE", "offsetStart": 47, "offsetEnd": 54}, "context": "The models included in the main document -APE, sineSPE and convSPE -all use a batch size of 10 and finished training in about 3 h, 5 h and 6 h, respectively, using 9.7 GB, 14.4 GB and 14.8 GB of GPU memory.", "mentionContextAttributes": {"used": {"value": true, "score": 0.8213565945625305}, "created": {"value": false, "score": 6.830692291259766e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999902248382568}, "created": {"value": false, "score": 0.00031507015228271484}, "shared": {"value": false, "score": 2.384185791015625e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "SPE", "normalizedForm": "SPE", "offsetStart": 49, "offsetEnd": 52}, "context": "We propose a new Stochastic Positional Encoding (SPE), based on filtering random noise.", "mentionContextAttributes": {"used": {"value": false, "score": 7.76052474975586e-05}, "created": {"value": true, "score": 0.9999332427978516}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999746084213257}, "created": {"value": true, "score": 0.9999332427978516}, "shared": {"value": false, "score": 0.007222533226013184}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "SPE", "normalizedForm": "SPE", "offsetStart": 52, "offsetEnd": 55}, "context": "On the contrary, the relatively good performance of SPE on this task is in fact remarkable, especially considering that the baseline systems for this task use learnable APE.", "mentionContextAttributes": {"used": {"value": false, "score": 0.02722877264022827}, "created": {"value": false, "score": 4.8041343688964844e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999746084213257}, "created": {"value": true, "score": 0.9999332427978516}, "shared": {"value": false, "score": 0.007222533226013184}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "SPE", "normalizedForm": "SPE", "offsetStart": 53, "offsetEnd": 56}, "context": "We consistently use K = 10 for sinusoidal (periodic) SPE and filters of length 128 for convolutional SPE.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9984862804412842}, "created": {"value": false, "score": 1.430511474609375e-06}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999746084213257}, "created": {"value": true, "score": 0.9999332427978516}, "shared": {"value": false, "score": 0.007222533226013184}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "convSPE", "normalizedForm": "convSPE", "offsetStart": 54, "offsetEnd": 61}, "context": "Relative (vanishing) attention with regular sampling (convSPE).", "mentionContextAttributes": {"used": {"value": true, "score": 0.999993085861206}, "created": {"value": false, "score": 1.3709068298339844e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.999993085861206}, "created": {"value": false, "score": 0.09803891181945801}, "shared": {"value": false, "score": 5.960464477539062e-07}}, "references": [{"label": "(Figures 11 and 12)", "normalizedForm": "Figures 11 and 12", "refKey": 0}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "convSPE", "normalizedForm": "convSPE", "offsetStart": 54, "offsetEnd": 61}, "context": "For sineSPE, we choose the number of sines K = 5; for convSPE, the convolutional filter size is set to be 128, 512 for the gated and ungated variants respectively.", "mentionContextAttributes": {"used": {"value": false, "score": 0.3714142441749573}, "created": {"value": false, "score": 7.033348083496094e-06}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.999993085861206}, "created": {"value": false, "score": 0.09803891181945801}, "shared": {"value": false, "score": 5.960464477539062e-07}}, "references": [{"label": "(Figures 11 and 12)", "normalizedForm": "Figures 11 and 12", "refKey": 0}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "sineSPE", "normalizedForm": "sineSPE", "offsetStart": 55, "offsetEnd": 62}, "context": "Regarding performance of SPE, we first notice that the sineSPE variant yields the best results on three tasks, which is a strong achievement and validates our approach, especially considering the difficulty of this evaluation benchmark. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.06309407949447632}, "created": {"value": false, "score": 5.817413330078125e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999902248382568}, "created": {"value": false, "score": 0.00031507015228271484}, "shared": {"value": false, "score": 2.384185791015625e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "SPE", "normalizedForm": "SPE", "offsetStart": 59, "offsetEnd": 62}, "context": "Next, comparing Figures 9 and 10, it is obvious that gated SPE gives the model the freedom to switch off PE in some heads to achieve global attention (see Figure 9), whereas the attention of ungated sineSPE (Figure 10) largely stays periodic, which might not be always desirable.", "mentionContextAttributes": {"used": {"value": false, "score": 0.0034317970275878906}, "created": {"value": false, "score": 2.6226043701171875e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999746084213257}, "created": {"value": true, "score": 0.9999332427978516}, "shared": {"value": false, "score": 0.007222533226013184}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "convSPE", "normalizedForm": "convSPE", "offsetStart": 59, "offsetEnd": 66}, "context": "The models included in the main document -APE, sineSPE and convSPE -all use a batch size of 10 and finished training in about 3 h, 5 h and 6 h, respectively, using 9.7 GB, 14.4 GB and 14.8 GB of GPU memory.", "mentionContextAttributes": {"used": {"value": true, "score": 0.8213565945625305}, "created": {"value": false, "score": 6.830692291259766e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.999993085861206}, "created": {"value": false, "score": 0.09803891181945801}, "shared": {"value": false, "score": 5.960464477539062e-07}}, "references": [{"label": "(Figures 11 and 12)", "normalizedForm": "Figures 11 and 12", "refKey": 0}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "sineSPE", "normalizedForm": "sineSPE", "offsetStart": 76, "offsetEnd": 83}, "context": "In Figure 7, we display 16 randomly picked resulting templates P d for both sineSPE and convSPE, trained with gating.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9997443556785583}, "created": {"value": false, "score": 0.00031507015228271484}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999902248382568}, "created": {"value": false, "score": 0.00031507015228271484}, "shared": {"value": false, "score": 2.384185791015625e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Band-in-a-Box", "normalizedForm": "Band-in-a-Box", "offsetStart": 78, "offsetEnd": 91}, "context": "The Groove2Groove MIDI dataset 11 consists of accompaniments generated by the Band-in-a-Box software (BIAB). ", "mentionContextAttributes": {"used": {"value": true, "score": 0.721933126449585}, "created": {"value": false, "score": 1.430511474609375e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.721933126449585}, "created": {"value": false, "score": 1.430511474609375e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}}, {"type": "software", "software-type": "implicit", "software-name": {"rawForm": "implementations", "normalizedForm": "implementations", "offsetStart": 78, "offsetEnd": 93}, "context": "\u2022 We provide additional resources on our companion website,2 including Python implementations of SPE for Py-Torch and JAX/Flax.", "mentionContextAttributes": {"used": {"value": false, "score": 5.412101745605469e-05}, "created": {"value": true, "score": 0.9667177200317383}, "shared": {"value": false, "score": 0.007222414016723633}}, "documentContextAttributes": {"used": {"value": false, "score": 5.412101745605469e-05}, "created": {"value": true, "score": 0.9667177200317383}, "shared": {"value": false, "score": 0.007222414016723633}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "sineSPE", "normalizedForm": "sineSPE", "offsetStart": 82, "offsetEnd": 89}, "context": "On the contrary, we empirically observe that our non-vanishing sinusoidal version sineSPE does behave better in these particular tasks.", "mentionContextAttributes": {"used": {"value": false, "score": 0.001139521598815918}, "created": {"value": false, "score": 0.0002269148826599121}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999902248382568}, "created": {"value": false, "score": 0.00031507015228271484}, "shared": {"value": false, "score": 2.384185791015625e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Py-Torch", "normalizedForm": "Py-Torch", "offsetStart": 84, "offsetEnd": 92}, "context": "Finally, although this may change in the near future, deep learning frameworks like Py-Torch do not easily integrate convolutions in the frequency domain.", "mentionContextAttributes": {"used": {"value": false, "score": 3.910064697265625e-05}, "created": {"value": false, "score": 0.0005710721015930176}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "documentContextAttributes": {"used": {"value": false, "score": 3.910064697265625e-05}, "created": {"value": false, "score": 0.0005710721015930176}, "shared": {"value": false, "score": 3.5762786865234375e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "SPE", "normalizedForm": "SPE", "offsetStart": 86, "offsetEnd": 89}, "context": "We use batch size = 4, and set the learning rate to 0.0001 for APE and 0.0002 for all SPE models.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9999746084213257}, "created": {"value": false, "score": 1.1444091796875e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999746084213257}, "created": {"value": true, "score": 0.9999332427978516}, "shared": {"value": false, "score": 0.007222533226013184}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "convSPE", "normalizedForm": "convSPE", "offsetStart": 88, "offsetEnd": 95}, "context": "In Figure 7, we display 16 randomly picked resulting templates P d for both sineSPE and convSPE, trained with gating.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9997443556785583}, "created": {"value": false, "score": 0.00031507015228271484}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.999993085861206}, "created": {"value": false, "score": 0.09803891181945801}, "shared": {"value": false, "score": 5.960464477539062e-07}}, "references": [{"label": "(Figures 11 and 12)", "normalizedForm": "Figures 11 and 12", "refKey": 0}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "SPE", "normalizedForm": "SPE", "offsetStart": 93, "offsetEnd": 96}, "context": "As we will see later in our music generation experiments, there are tasks where our proposed SPE clearly yields remarkable improvements.", "mentionContextAttributes": {"used": {"value": false, "score": 0.000277101993560791}, "created": {"value": true, "score": 0.9972136616706848}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999746084213257}, "created": {"value": true, "score": 0.9999332427978516}, "shared": {"value": false, "score": 0.007222533226013184}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "sineSPE", "normalizedForm": "sineSPE", "offsetStart": 94, "offsetEnd": 101}, "context": "While it is only marginally better than APE for ListOps and Text, it is worth mentioning that sineSPE combined with the Linear Transformer-ReLU yields an accuracy improvement of \u223c3 % on Retrieval compared to the best result obtained by Tay et al. (2021).", "mentionContextAttributes": {"used": {"value": false, "score": 0.003657400608062744}, "created": {"value": false, "score": 2.1457672119140625e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999902248382568}, "created": {"value": false, "score": 0.00031507015228271484}, "shared": {"value": false, "score": 2.384185791015625e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "SPE", "normalizedForm": "SPE", "offsetStart": 97, "offsetEnd": 100}, "context": "\u2022 We provide additional resources on our companion website,2 including Python implementations of SPE for Py-Torch and JAX/Flax.", "mentionContextAttributes": {"used": {"value": false, "score": 5.412101745605469e-05}, "created": {"value": true, "score": 0.9667174816131592}, "shared": {"value": false, "score": 0.007222533226013184}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999746084213257}, "created": {"value": true, "score": 0.9999332427978516}, "shared": {"value": false, "score": 0.007222533226013184}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "convSPE", "normalizedForm": "convSPE", "offsetStart": 99, "offsetEnd": 106}, "context": "From Figure 7, we can observe that sineSPE learns to exploit a wide range of frequencies, and that convSPE is effective within small query-key offsets corresponding to the filter size, as expected.", "mentionContextAttributes": {"used": {"value": false, "score": 0.004227697849273682}, "created": {"value": false, "score": 0.00022166967391967773}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.999993085861206}, "created": {"value": false, "score": 0.09803891181945801}, "shared": {"value": false, "score": 5.960464477539062e-07}}, "references": [{"label": "(Figures 11 and 12)", "normalizedForm": "Figures 11 and 12", "refKey": 0}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Pathfinder", "normalizedForm": "Pathfinder", "offsetStart": 100, "offsetEnd": 110}, "context": "An overview of the Long-Range Arena (Tay et al., 2021) tasks is given in table 2. We do not include Pathfinder (a synthetic image classification task) or its harder variant Pathfinder-X in this paper as we were unable to reproduce the results of Tay et al. on  In all LRA experiments, we employ gated SPE with R \u2208 {32, 64}. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9471338391304016}, "created": {"value": false, "score": 1.2159347534179688e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9994944334030151}, "created": {"value": false, "score": 6.783008575439453e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "SPE", "normalizedForm": "SPE", "offsetStart": 101, "offsetEnd": 104}, "context": "We consistently use K = 10 for sinusoidal (periodic) SPE and filters of length 128 for convolutional SPE.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9984862804412842}, "created": {"value": false, "score": 1.430511474609375e-06}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999746084213257}, "created": {"value": true, "score": 0.9999332427978516}, "shared": {"value": false, "score": 0.007222533226013184}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Py-", "normalizedForm": "Py", "offsetStart": 105, "offsetEnd": 108}, "context": "\u2022 We provide additional resources on our companion website,2 including Python implementations of SPE for Py-Torch and JAX/Flax.", "mentionContextAttributes": {"used": {"value": false, "score": 5.412101745605469e-05}, "created": {"value": true, "score": 0.9667174816131592}, "shared": {"value": false, "score": 0.007222533226013184}}, "documentContextAttributes": {"used": {"value": false, "score": 5.412101745605469e-05}, "created": {"value": true, "score": 0.9667174816131592}, "shared": {"value": false, "score": 0.007222533226013184}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "SPE", "normalizedForm": "SPE", "offsetStart": 106, "offsetEnd": 109}, "context": "Both sineSPE and convSPE are much more stable in this regard, confirming the result from section 3.2 that SPE extrapolates better beyond the training sequence length.", "mentionContextAttributes": {"used": {"value": false, "score": 0.00771331787109375}, "created": {"value": false, "score": 1.9073486328125e-06}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999746084213257}, "created": {"value": true, "score": 0.9999332427978516}, "shared": {"value": false, "score": 0.007222533226013184}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "SPE", "normalizedForm": "SPE", "offsetStart": 107, "offsetEnd": 110}, "context": "For convolutional SPE, we share Q and K across all layers (but not across attention heads); for sinusoidal SPE, Q and K are unique to each layer and head; in both cases, layerspecific gating is employed.", "mentionContextAttributes": {"used": {"value": false, "score": 0.004274249076843262}, "created": {"value": false, "score": 6.16312026977539e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999746084213257}, "created": {"value": true, "score": 0.9999332427978516}, "shared": {"value": false, "score": 0.007222533226013184}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "sineSPE", "normalizedForm": "sineSPE", "offsetStart": 114, "offsetEnd": 121}, "context": "For qualitative assessment, we first display in Figure 1 one attention pattern for each PE model: APE and (gated) sineSPE/convSPE, obtained as an average over 20 from-scratch generations for a chosen (layer, head).", "mentionContextAttributes": {"used": {"value": true, "score": 0.9969915151596069}, "created": {"value": false, "score": 5.0067901611328125e-06}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999902248382568}, "created": {"value": false, "score": 0.00031507015228271484}, "shared": {"value": false, "score": 2.384185791015625e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "SPE", "normalizedForm": "SPE", "offsetStart": 120, "offsetEnd": 123}, "context": "Our music Performers are implemented using the pytorch-fast-transformers package,8 modified as necessary to incorporate SPE.", "mentionContextAttributes": {"used": {"value": false, "score": 0.49382418394088745}, "created": {"value": true, "score": 0.6066072583198547}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999746084213257}, "created": {"value": true, "score": 0.9999332427978516}, "shared": {"value": false, "score": 0.007222533226013184}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "convSPE", "normalizedForm": "convSPE", "offsetStart": 122, "offsetEnd": 129}, "context": "For qualitative assessment, we first display in Figure 1 one attention pattern for each PE model: APE and (gated) sineSPE/convSPE, obtained as an average over 20 from-scratch generations for a chosen (layer, head).", "mentionContextAttributes": {"used": {"value": true, "score": 0.9969915151596069}, "created": {"value": false, "score": 5.0067901611328125e-06}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.999993085861206}, "created": {"value": false, "score": 0.09803891181945801}, "shared": {"value": false, "score": 5.960464477539062e-07}}, "references": [{"label": "(Figures 11 and 12)", "normalizedForm": "Figures 11 and 12", "refKey": 0}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "SPE", "normalizedForm": "SPE", "offsetStart": 129, "offsetEnd": 132}, "context": "This removes the distinction between relative and absolute positions, which might explain why trainable APE performs better than SPE on this task.)", "mentionContextAttributes": {"used": {"value": false, "score": 0.0001919865608215332}, "created": {"value": false, "score": 0.00018984079360961914}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999746084213257}, "created": {"value": true, "score": 0.9999332427978516}, "shared": {"value": false, "score": 0.007222533226013184}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "SPE", "normalizedForm": "SPE", "offsetStart": 134, "offsetEnd": 137}, "context": "We use code from the official LRA repository, including the authors' Transformer implementation, modified as necessary to incorporate SPE.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9924556016921997}, "created": {"value": false, "score": 0.001077115535736084}, "shared": {"value": false, "score": 1.3113021850585938e-06}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999746084213257}, "created": {"value": true, "score": 0.9999332427978516}, "shared": {"value": false, "score": 0.007222533226013184}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "SPE", "normalizedForm": "SPE", "offsetStart": 136, "offsetEnd": 139}, "context": "The natural future directions for our study are (i) Signal-dependent PE that incorporates the input sequence as an additional input for SPE, (ii) Nonstationary PE that utilizes both relative and absolute positions, (iii) Extending our approach to arbitrary attention kernels, e.g.", "mentionContextAttributes": {"used": {"value": false, "score": 0.0002091526985168457}, "created": {"value": false, "score": 0.041422903537750244}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999746084213257}, "created": {"value": true, "score": 0.9999332427978516}, "shared": {"value": false, "score": 0.007222533226013184}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "SPE", "normalizedForm": "SPE", "offsetStart": 138, "offsetEnd": 141}, "context": "Since we observe some variation between different runs, we train and evaluate each model 3 times (except for Performer with convolutional SPE, which is computationally more costly) and report the mean and standard deviation of the results.", "mentionContextAttributes": {"used": {"value": false, "score": 0.2599923014640808}, "created": {"value": false, "score": 1.0967254638671875e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999746084213257}, "created": {"value": true, "score": 0.9999332427978516}, "shared": {"value": false, "score": 0.007222533226013184}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "SPE", "normalizedForm": "SPE", "offsetStart": 155, "offsetEnd": 158}, "context": "Cascading several convolutions as in the VGGNet (Simonyan & Zisserman, 2014) may be a convenient way to augment the expressive power of this convolutional SPE variant.", "mentionContextAttributes": {"used": {"value": false, "score": 0.00014281272888183594}, "created": {"value": false, "score": 3.0994415283203125e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999746084213257}, "created": {"value": true, "score": 0.9999332427978516}, "shared": {"value": false, "score": 0.007222533226013184}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "SPE", "normalizedForm": "SPE", "offsetStart": 159, "offsetEnd": 162}, "context": "In models that use SPE, Q and K are shared across all layers (but not across attention heads); layer-specific gating is employed for models trained with gated SPE.", "mentionContextAttributes": {"used": {"value": false, "score": 0.003587067127227783}, "created": {"value": false, "score": 7.021427154541016e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999746084213257}, "created": {"value": true, "score": 0.9999332427978516}, "shared": {"value": false, "score": 0.007222533226013184}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Pathfinder-X", "normalizedForm": "Pathfinder-X", "offsetStart": 173, "offsetEnd": 185}, "context": "An overview of the Long-Range Arena (Tay et al., 2021) tasks is given in table 2. We do not include Pathfinder (a synthetic image classification task) or its harder variant Pathfinder-X in this paper as we were unable to reproduce the results of Tay et al. on  In all LRA experiments, we employ gated SPE with R \u2208 {32, 64}. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9471338391304016}, "created": {"value": false, "score": 1.2159347534179688e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9471338391304016}, "created": {"value": false, "score": 1.2159347534179688e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "sineSPE", "normalizedForm": "sineSPE", "offsetStart": 199, "offsetEnd": 206}, "context": "Next, comparing Figures 9 and 10, it is obvious that gated SPE gives the model the freedom to switch off PE in some heads to achieve global attention (see Figure 9), whereas the attention of ungated sineSPE (Figure 10) largely stays periodic, which might not be always desirable.", "mentionContextAttributes": {"used": {"value": false, "score": 0.0034317970275878906}, "created": {"value": false, "score": 2.6226043701171875e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999902248382568}, "created": {"value": false, "score": 0.00031507015228271484}, "shared": {"value": false, "score": 2.384185791015625e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "SPE", "normalizedForm": "SPE", "offsetStart": 301, "offsetEnd": 304}, "context": "An overview of the Long-Range Arena (Tay et al., 2021) tasks is given in table 2. We do not include Pathfinder (a synthetic image classification task) or its harder variant Pathfinder-X in this paper as we were unable to reproduce the results of Tay et al. on  In all LRA experiments, we employ gated SPE with R \u2208 {32, 64}.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9471338391304016}, "created": {"value": false, "score": 1.2159347534179688e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999746084213257}, "created": {"value": true, "score": 0.9999332427978516}, "shared": {"value": false, "score": 0.007222533226013184}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "SPE", "normalizedForm": "SPE", "offsetStart": 350, "offsetEnd": 353}, "context": "We evaluate SPE (the gated variant) on two efficient Transformer models: the (softmax) Performer (Choromanski et al., 2020), and a Linear Transformer (Katharopoulos et al., 2020) with a ReLU feature map, i.e. choosing \u03c6(\u2022) = max(0, \u2022) element-wise in (3). 4 It should be noted that the ReLU feature map does not approximate the softmax kernel, which SPE is designed for (see assumption 8).", "mentionContextAttributes": {"used": {"value": true, "score": 0.9990382194519043}, "created": {"value": false, "score": 3.6954879760742188e-06}, "shared": {"value": false, "score": 8.344650268554688e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999746084213257}, "created": {"value": true, "score": 0.9999332427978516}, "shared": {"value": false, "score": 0.007222533226013184}}}], "references": [{"refKey": 0, "tei": "<biblStruct xml:id=\"b0\">\n\t<monogr>\n\t\t<title level=\"m\" type=\"main\">Relative Positional Encoding for Transformers with Linear Complexity</title>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Antoine</forename><surname>Liutkus</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Ond\u0159ej</forename><surname>C\u00edfka</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Shih-Lun</forename><surname>Wu</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Umut</forename><surname>\u015eim\u015fekli</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Yi-Hsuan</forename><surname>Yang</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Gael</forename><surname>Richard</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Umut</forename><forename type=\"middle\">S</forename><surname>\u00b8ims \u00b8ekli</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Ga\u00ebl</forename><surname>Richard</surname></persName>\n\t\t</author>\n\t\t<idno>75C14769E140181132B7CA9D35723E44</idno>\n\t\t<imprint>\n\t\t\t<date></date>\n\t\t</imprint>\n\t</monogr>\n</biblStruct>\n"}], "runtime": 22411, "id": "d227d1a09a459051b7b2790aaac0d4425571958f", "metadata": {"id": "d227d1a09a459051b7b2790aaac0d4425571958f"}, "original_file_path": "../../datalake/Samuel/SOFTware-Sync/downloads/xml/hal-03256451.grobid.tei.xml", "file_name": "hal-03256451.grobid.tei.xml"}