{"application": "software-mentions", "version": "0.8.0", "date": "2024-10-07T11:57+0000", "md5": "8F09F7A8705977D06B9A3B5B50ED4B29", "mentions": [{"type": "software", "software-type": "software", "software-name": {"rawForm": "HuBERT", "normalizedForm": "HuBERT", "offsetStart": 0, "offsetEnd": 6}, "context": "HuBERT and CPC) as well as SSE models from Algayres et al. (2022) are not trained to encode short sequence of speech, especially extracted as chunks from a sentence.", "mentionContextAttributes": {"used": {"value": false, "score": 0.001183629035949707}, "created": {"value": false, "score": 1.5020370483398438e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": false, "score": 0.03446495532989502}, "created": {"value": false, "score": 0.00022608041763305664}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "references": [{"label": "(Baevski et al., 2020;", "normalizedForm": "(Baevski et al., 2020", "refKey": 3}, {"label": "Hsu et al., 2021)", "normalizedForm": "Hsu et al., 2021)", "refKey": 21}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "HuBERT", "normalizedForm": "HuBERT", "offsetStart": 0, "offsetEnd": 6}, "context": "HuBERT or CPC (Baevski et al., 2020;Hsu et al., 2021;van den Oord et al., 2019) are trained with masked language modelling in the spirit of text-based LM but on the raw speech signal.", "mentionContextAttributes": {"used": {"value": false, "score": 0.019437670707702637}, "created": {"value": false, "score": 3.349781036376953e-05}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "documentContextAttributes": {"used": {"value": false, "score": 0.03446495532989502}, "created": {"value": false, "score": 0.00022608041763305664}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "references": [{"label": "(Baevski et al., 2020;", "normalizedForm": "(Baevski et al., 2020", "refKey": 3, "offsetStart": 29916, "offsetEnd": 29938}, {"label": "Hsu et al., 2021;", "normalizedForm": "Hsu et al., 2021", "refKey": 21, "offsetStart": 29938, "offsetEnd": 29955}, {"label": "van den Oord et al., 2019)", "normalizedForm": "van den Oord et al., 2019)", "refKey": 44, "offsetStart": 29955, "offsetEnd": 29981}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "HuBERT", "normalizedForm": "HuBERT", "offsetStart": 3, "offsetEnd": 9}, "context": "or HuBERT (Baevski et al., 2020;Hsu et al., 2021), and mean-pool the frames along the time axis.", "mentionContextAttributes": {"used": {"value": false, "score": 0.03446495532989502}, "created": {"value": false, "score": 3.933906555175781e-06}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": false, "score": 0.03446495532989502}, "created": {"value": false, "score": 0.00022608041763305664}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "references": [{"label": "(Baevski et al., 2020;", "normalizedForm": "(Baevski et al., 2020", "refKey": 3, "offsetStart": 7532, "offsetEnd": 7554}, {"label": "Hsu et al., 2021)", "normalizedForm": "Hsu et al., 2021)", "refKey": 21, "offsetStart": 7554, "offsetEnd": 7571}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Zerospeech", "normalizedForm": "Zerospeech", "offsetStart": 4, "offsetEnd": 14}, "context": "The Zerospeech Challenge 2017 (Dunbar et al., 2017) provides five corpora to evaluate speech segmentation systems.", "mentionContextAttributes": {"used": {"value": false, "score": 0.0011334419250488281}, "created": {"value": false, "score": 3.254413604736328e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.999984622001648}, "created": {"value": true, "score": 0.9998239874839783}, "shared": {"value": false, "score": 3.5762786865234375e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Zerospeech", "normalizedForm": "Zerospeech", "offsetStart": 7, "offsetEnd": 17}, "context": "In the Zerospeech Challenge 2017 (Dunbar et al., 2017), two metrics measure how well discovered boundaries matches with the gold word boundaries. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.8224144577980042}, "created": {"value": false, "score": 1.800060272216797e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.999984622001648}, "created": {"value": true, "score": 0.9998239874839783}, "shared": {"value": false, "score": 3.5762786865234375e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "HuBERT", "normalizedForm": "HuBERT", "offsetStart": 10, "offsetEnd": 16}, "context": "Large and HuBERT Large are strong baseline systems for our ABX metrics.", "mentionContextAttributes": {"used": {"value": false, "score": 0.00010979175567626953}, "created": {"value": false, "score": 0.00022608041763305664}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": false, "score": 0.03446495532989502}, "created": {"value": false, "score": 0.00022608041763305664}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "references": [{"label": "(Baevski et al., 2020;", "normalizedForm": "(Baevski et al., 2020", "refKey": 3}, {"label": "Hsu et al., 2021)", "normalizedForm": "Hsu et al., 2021)", "refKey": 21}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Wav2vec", "normalizedForm": "Wav2vec", "offsetStart": 22, "offsetEnd": 29}, "version": {"rawForm": "0", "normalizedForm": "0"}, "context": "The results show that Wav2vec2.0", "mentionContextAttributes": {"used": {"value": true, "score": 0.9999089241027832}, "created": {"value": false, "score": 5.602836608886719e-06}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999908208847046}, "created": {"value": false, "score": 5.412101745605469e-05}, "shared": {"value": false, "score": 1.6689300537109375e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Wav2vec", "normalizedForm": "Wav2vec", "offsetStart": 29, "offsetEnd": 36}, "version": {"rawForm": "0", "normalizedForm": "0"}, "context": "Speech-to-frames models like Wav2vec2.0,", "mentionContextAttributes": {"used": {"value": false, "score": 9.739398956298828e-05}, "created": {"value": false, "score": 1.5735626220703125e-05}, "shared": {"value": false, "score": 1.6689300537109375e-06}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999908208847046}, "created": {"value": false, "score": 5.412101745605469e-05}, "shared": {"value": false, "score": 1.6689300537109375e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Librispeech", "normalizedForm": "Librispeech", "offsetStart": 37, "offsetEnd": 48}, "context": "The sentences are extracted from the Librispeech dataset, a 960 hours corpus of read English literature.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9999346733093262}, "created": {"value": false, "score": 3.933906555175781e-06}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999675750732422}, "created": {"value": true, "score": 0.9203346967697144}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "references": [{"label": "(Panayotov et al., 2015)", "normalizedForm": "Panayotov et al., 2015", "refKey": 45}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Librispeech", "normalizedForm": "Librispeech", "offsetStart": 40, "offsetEnd": 51}, "context": "The triplets are all extracted from the Librispeech corpus training set.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9987753033638}, "created": {"value": false, "score": 1.5497207641601562e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999675750732422}, "created": {"value": true, "score": 0.9203346967697144}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "references": [{"label": "(Panayotov et al., 2015)", "normalizedForm": "Panayotov et al., 2015", "refKey": 45}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Zerospeech", "normalizedForm": "Zerospeech", "offsetStart": 42, "offsetEnd": 52}, "context": "Most speech segmentation systems from the Zerospeech Challenge 2017 rely on off-the-shelf self-supervised representations of speech.", "mentionContextAttributes": {"used": {"value": false, "score": 0.0002402663230895996}, "created": {"value": false, "score": 4.482269287109375e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.999984622001648}, "created": {"value": true, "score": 0.9998239874839783}, "shared": {"value": false, "score": 3.5762786865234375e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Wav2vec", "normalizedForm": "Wav2vec", "offsetStart": 47, "offsetEnd": 54}, "version": {"rawForm": "0", "normalizedForm": "0"}, "context": "The SSE model takes as input the pre-extracted Wav2vec2.0", "mentionContextAttributes": {"used": {"value": false, "score": 0.0585591197013855}, "created": {"value": false, "score": 2.6226043701171875e-06}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999908208847046}, "created": {"value": false, "score": 5.412101745605469e-05}, "shared": {"value": false, "score": 1.6689300537109375e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Zerospeech", "normalizedForm": "Zerospeech", "offsetStart": 60, "offsetEnd": 70}, "context": "We used the transcriptions of the development sets from the Zerospeech Challenge 2017 to give to the k-means the true number of clusters is it suppose to find when clustering SSEs from L 0 and L. The value of L 0w and L w are given by the size of the cluster in which w is found.", "mentionContextAttributes": {"used": {"value": true, "score": 0.999984622001648}, "created": {"value": false, "score": 8.7738037109375e-05}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.999984622001648}, "created": {"value": true, "score": 0.9998239874839783}, "shared": {"value": false, "score": 3.5762786865234375e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Wav2vec", "normalizedForm": "Wav2vec", "offsetStart": 66, "offsetEnd": 73}, "version": {"rawForm": "0", "normalizedForm": "0", "offsetStart": 75, "offsetEnd": 76}, "context": "One reason for that could be that self-supervised speech systems (Wav2vec2.0, ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9946243166923523}, "created": {"value": false, "score": 1.4185905456542969e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999908208847046}, "created": {"value": false, "score": 5.412101745605469e-05}, "shared": {"value": false, "score": 1.6689300537109375e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Librispeech", "normalizedForm": "Librispeech", "offsetStart": 76, "offsetEnd": 87}, "context": "We proceed by first training a SSE model from Algayres et al. (2022) on the Librispeech.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9516748189926147}, "created": {"value": false, "score": 0.3540545105934143}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999675750732422}, "created": {"value": true, "score": 0.9203346967697144}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "references": [{"label": "(Panayotov et al., 2015)", "normalizedForm": "Panayotov et al., 2015", "refKey": 45}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Zerospeech", "normalizedForm": "Zerospeech", "offsetStart": 86, "offsetEnd": 96}, "context": "We introduce a new speech segmentation pipeline that sets the state-of-the-art on the Zerospeech's datasets at 16.8 token F1.", "mentionContextAttributes": {"used": {"value": false, "score": 0.00013148784637451172}, "created": {"value": true, "score": 0.9998239874839783}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.999984622001648}, "created": {"value": true, "score": 0.9998239874839783}, "shared": {"value": false, "score": 3.5762786865234375e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Wav2vec", "normalizedForm": "Wav2vec", "offsetStart": 87, "offsetEnd": 94}, "version": {"rawForm": "0", "normalizedForm": "0"}, "context": "We represent speech as 20ms frames obtained by selecting the 8th layer of a pretrained Wav2vec2.0", "mentionContextAttributes": {"used": {"value": true, "score": 0.9999908208847046}, "created": {"value": false, "score": 1.1920928955078125e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999908208847046}, "created": {"value": false, "score": 5.412101745605469e-05}, "shared": {"value": false, "score": 1.6689300537109375e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Librispeech", "normalizedForm": "Librispeech", "offsetStart": 95, "offsetEnd": 106}, "context": "For each unique word found in the triplets, we sample 10 occurrences from the sentences of the Librispeech corpus.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9999675750732422}, "created": {"value": false, "score": 1.704692840576172e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999675750732422}, "created": {"value": true, "score": 0.9203346967697144}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "references": [{"label": "(Panayotov et al., 2015)", "normalizedForm": "Panayotov et al., 2015", "refKey": 45}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Zerospeech", "normalizedForm": "Zerospeech", "offsetStart": 97, "offsetEnd": 107}, "context": "Table 4 reports the token F1 and boundary F1 obtained by these models over the 5 datasets of the Zerospeech challenge 2017.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9998971223831177}, "created": {"value": false, "score": 6.4373016357421875e-06}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.999984622001648}, "created": {"value": true, "score": 0.9998239874839783}, "shared": {"value": false, "score": 3.5762786865234375e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Wav2vec", "normalizedForm": "Wav2vec", "offsetStart": 100, "offsetEnd": 107}, "version": {"rawForm": "0", "normalizedForm": "0"}, "context": "A naive SSE model would be to extract frame-level features of a speech sequence, using for instance Wav2vec2.0", "mentionContextAttributes": {"used": {"value": false, "score": 0.048721909523010254}, "created": {"value": false, "score": 5.412101745605469e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999908208847046}, "created": {"value": false, "score": 5.412101745605469e-05}, "shared": {"value": false, "score": 1.6689300537109375e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Librispeech", "normalizedForm": "Librispeech", "offsetStart": 105, "offsetEnd": 116}, "context": "For the ABX and sSIMI tasks, we train a BERT model as a Speech Language Model on the training set of the Librispeech dataset (Panayotov et al., 2015), composed of 960 hours of English recordings.", "mentionContextAttributes": {"used": {"value": false, "score": 0.21782219409942627}, "created": {"value": false, "score": 0.008385539054870605}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999675750732422}, "created": {"value": true, "score": 0.9203346967697144}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "references": [{"label": "(Panayotov et al., 2015)", "normalizedForm": "Panayotov et al., 2015", "refKey": 45, "offsetStart": 25620, "offsetEnd": 25644}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Zerospeech", "normalizedForm": "Zerospeech", "offsetStart": 115, "offsetEnd": 125}, "context": "Regarding speech segmentation, we compare DP-Parse with the three best speech segmentation models submitted at the Zerospeech Challenge 2017 (Bhati et al., 2020;Kamper et al., 2017b;R\u00e4s\u00e4nen et al., 2015).", "mentionContextAttributes": {"used": {"value": true, "score": 0.9968215227127075}, "created": {"value": false, "score": 9.417533874511719e-06}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.999984622001648}, "created": {"value": true, "score": 0.9998239874839783}, "shared": {"value": false, "score": 3.5762786865234375e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Librispeech", "normalizedForm": "Librispeech", "offsetStart": 125, "offsetEnd": 136}, "context": "Despite our computational speedup, our current system would face challenges to scale up the approach to larger datasets than Librispeech. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.00047278404235839844}, "created": {"value": true, "score": 0.9203346967697144}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999675750732422}, "created": {"value": true, "score": 0.9203346967697144}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "references": [{"label": "(Panayotov et al., 2015)", "normalizedForm": "Panayotov et al., 2015", "refKey": 45}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Zerospeech", "normalizedForm": "Zerospeech", "offsetStart": 126, "offsetEnd": 136}, "context": "To set \u03b2, we follow the observation from Algayres et al. (2022): around 50% of the segments in the development dataset of the Zerospeech Challenge 2017 appear to have a frequency of one.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9707130193710327}, "created": {"value": false, "score": 7.939338684082031e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.999984622001648}, "created": {"value": true, "score": 0.9998239874839783}, "shared": {"value": false, "score": 3.5762786865234375e-07}}}], "references": [{"refKey": 3, "tei": "<biblStruct xml:id=\"b3\">\n\t<monogr>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Alexei</forename><surname>Baevski</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Henry</forename><surname>Zhou</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Abdelrahman</forename><surname>Mohamed</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Michael</forename><surname>Auli</surname></persName>\n\t\t</author>\n\t\t<idno>CoRR, abs/2006.11477</idno>\n\t\t<title level=\"m\">wav2vec 2.0: A framework for self-supervised learning of speech representations</title>\n\t\t<imprint>\n\t\t\t<date>2020</date>\n\t\t</imprint>\n\t</monogr>\n</biblStruct>\n"}, {"refKey": 21, "tei": "<biblStruct xml:id=\"b21\">\n\t<analytic>\n\t\t<title level=\"a\" type=\"main\">HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units</title>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Wei-Ning</forename><surname>Hsu</surname></persName>\n\t\t\t<idno type=\"ORCID\">0000-0001-5546-5217</idno>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Benjamin</forename><surname>Bolte</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Yao-Hung Hubert</forename><surname>Tsai</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Kushal</forename><surname>Lakhotia</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Ruslan</forename><surname>Salakhutdinov</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Abdelrahman</forename><surname>Mohamed</surname></persName>\n\t\t</author>\n\t\t<idno type=\"DOI\">10.1109/taslp.2021.3122291</idno>\n\t</analytic>\n\t<monogr>\n\t\t<title level=\"j\">IEEE/ACM Transactions on Audio, Speech, and Language Processing</title>\n\t\t<title level=\"j\" type=\"abbrev\">IEEE/ACM Trans. Audio Speech Lang. Process.</title>\n\t\t<idno type=\"ISSN\">2329-9290</idno>\n\t\t<idno type=\"ISSNe\">2329-9304</idno>\n\t\t<imprint>\n\t\t\t<biblScope unit=\"volume\">29</biblScope>\n\t\t\t<biblScope unit=\"page\" from=\"3451\" to=\"3460\" />\n\t\t\t<date type=\"published\" when=\"2021\">2021</date>\n\t\t\t<publisher>Institute of Electrical and Electronics Engineers (IEEE)</publisher>\n\t\t</imprint>\n\t</monogr>\n</biblStruct>\n"}, {"refKey": 44, "tei": "<biblStruct xml:id=\"b44\">\n\t<monogr>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Aaron</forename><surname>Van Den Oord</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Yazhe</forename><surname>Li</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Oriol</forename><surname>Vinyals</surname></persName>\n\t\t</author>\n\t\t<title level=\"m\">Representation learning with contrastive predictive coding</title>\n\t\t<imprint>\n\t\t\t<date>2019</date>\n\t\t</imprint>\n\t</monogr>\n</biblStruct>\n"}, {"refKey": 45, "tei": "<biblStruct xml:id=\"b45\">\n\t<analytic>\n\t\t<title level=\"a\" type=\"main\">Librispeech: An ASR corpus based on public domain audio books</title>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Vassil</forename><surname>Panayotov</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Guoguo</forename><surname>Chen</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Daniel</forename><surname>Povey</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Sanjeev</forename><surname>Khudanpur</surname></persName>\n\t\t</author>\n\t\t<idno type=\"DOI\">10.1109/icassp.2015.7178964</idno>\n\t</analytic>\n\t<monogr>\n\t\t<title level=\"m\">2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>\n\t\t<imprint>\n\t\t\t<publisher>IEEE</publisher>\n\t\t\t<date type=\"published\" when=\"2015-04\">2015</date>\n\t\t</imprint>\n\t</monogr>\n</biblStruct>\n"}], "runtime": 12282, "id": "7dd60ba9e4a6883ec6fb12f0a011404feb13a6b5", "metadata": {"id": "7dd60ba9e4a6883ec6fb12f0a011404feb13a6b5"}, "original_file_path": "../../datalake/Samuel/SOFTware-Sync/downloads/xml/hal-03831873.grobid.tei.xml", "file_name": "hal-03831873.grobid.tei.xml"}