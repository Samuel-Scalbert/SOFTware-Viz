{"application": "software-mentions", "version": "0.8.0", "date": "2024-10-07T11:47+0000", "md5": "92645BEE2B43FAA6AF781F2298A02A24", "mentions": [{"type": "software", "software-type": "software", "software-name": {"rawForm": "HuBERT", "normalizedForm": "HuBERT", "offsetStart": 0, "offsetEnd": 6}, "context": "HuBERT. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.012256324291229248}, "created": {"value": false, "score": 0.0002225041389465332}, "shared": {"value": false, "score": 1.3113021850585938e-06}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9988412261009216}, "created": {"value": false, "score": 0.0002225041389465332}, "shared": {"value": false, "score": 1.3113021850585938e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "HuBERT", "normalizedForm": "HuBERT", "offsetStart": 0, "offsetEnd": 6}, "context": "HuBERT and CPC seem to be giving the best results, for both humans and models better capturing phonetic information than other models at equivalent bitrates.", "mentionContextAttributes": {"used": {"value": false, "score": 0.006061196327209473}, "created": {"value": false, "score": 1.2516975402832031e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9988412261009216}, "created": {"value": false, "score": 0.0002225041389465332}, "shared": {"value": false, "score": 1.3113021850585938e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "CrowdMOS", "normalizedForm": "CrowdMOS", "offsetStart": 4, "offsetEnd": 12}, "context": "The CrowdMOS package (Ribeiro et al., 2011) was used for all subjective experiments using the recommended recipes for detecting and discarding inaccurate scores. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9999793767929077}, "created": {"value": false, "score": 1.2040138244628906e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999793767929077}, "created": {"value": false, "score": 1.2040138244628906e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "references": [{"label": "(Ribeiro et al., 2011)", "normalizedForm": "Ribeiro et al., 2011", "refKey": 67, "offsetStart": 21195, "offsetEnd": 21217}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "LibriSpeech", "normalizedForm": "LibriSpeech", "offsetStart": 5, "offsetEnd": 16}, "context": "When LibriSpeech is offered as input, the u2S component cannot adapt to this nominally better input and ends up yielding lower quality outputs. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.00029724836349487305}, "created": {"value": false, "score": 6.258487701416016e-05}, "shared": {"value": false, "score": 4.76837158203125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.999972939491272}, "created": {"value": false, "score": 0.0004978179931640625}, "shared": {"value": false, "score": 4.76837158203125e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "fairseq", "normalizedForm": "fairseq", "offsetStart": 19, "offsetEnd": 26}, "context": "github.com/pytorch/fairseq/.../language_model", "mentionContextAttributes": {"used": {"value": false, "score": 0.0002244114875793457}, "created": {"value": false, "score": 2.8014183044433594e-05}, "shared": {"value": true, "score": 0.9668394923210144}}, "documentContextAttributes": {"used": {"value": false, "score": 0.47602754831314087}, "created": {"value": false, "score": 0.02231884002685547}, "shared": {"value": true, "score": 0.9668394923210144}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "LJ Speech", "normalizedForm": "LJ Speech", "offsetStart": 31, "offsetEnd": 40}, "context": "All u2S models were trained on LJ Speech (LJ) (Ito and Johnson, 2017).", "mentionContextAttributes": {"used": {"value": true, "score": 0.9999240636825562}, "created": {"value": false, "score": 3.3855438232421875e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.999972939491272}, "created": {"value": false, "score": 4.2438507080078125e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "references": [{"label": "(Ito and Johnson, 2017)", "normalizedForm": "Ito and Johnson, 2017", "refKey": 29, "offsetStart": 27250, "offsetEnd": 27273}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "LJ Speech", "normalizedForm": "LJ Speech", "offsetStart": 35, "offsetEnd": 44}, "context": "From the viewpoint of the encoder, LJ Speech is out-of-domain; therefore, one would expect that the units are making more errors than for the trained LibriSpeech.", "mentionContextAttributes": {"used": {"value": false, "score": 0.09051495790481567}, "created": {"value": false, "score": 2.6226043701171875e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.999972939491272}, "created": {"value": false, "score": 4.2438507080078125e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "references": [{"label": "(Ito and Johnson, 2017)", "normalizedForm": "Ito and Johnson, 2017", "refKey": 29}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "HuBERT", "normalizedForm": "HuBERT", "offsetStart": 42, "offsetEnd": 48}, "context": "In terms of systems, the best one here is HuBERT.", "mentionContextAttributes": {"used": {"value": false, "score": 0.0005502104759216309}, "created": {"value": false, "score": 5.710124969482422e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9988412261009216}, "created": {"value": false, "score": 0.0002225041389465332}, "shared": {"value": false, "score": 1.3113021850585938e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "LibriSpeech", "normalizedForm": "LibriSpeech", "offsetStart": 42, "offsetEnd": 53}, "context": "Note that we always use dev_clean set for LibriSpeech, and a random hold-out set of 1000 samples for LJ Speech that were not seen during training or validation.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9900070428848267}, "created": {"value": false, "score": 1.0251998901367188e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.999972939491272}, "created": {"value": false, "score": 0.0004978179931640625}, "shared": {"value": false, "score": 4.76837158203125e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "HuBERT", "normalizedForm": "HuBERT", "offsetStart": 44, "offsetEnd": 50}, "context": "Across 3 speech encoders (CPC, wav2vec 2.0, HuBERT), we find that the number of discrete units (50, 100, or 200) matters in a task-dependent and encoder-dependent way, and that some combinations approach text-based systems.", "mentionContextAttributes": {"used": {"value": false, "score": 0.38476991653442383}, "created": {"value": false, "score": 2.6106834411621094e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9988412261009216}, "created": {"value": false, "score": 0.0002225041389465332}, "shared": {"value": false, "score": 1.3113021850585938e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "LibriSpeech", "normalizedForm": "LibriSpeech", "offsetStart": 45, "offsetEnd": 56}, "context": "We use a LARGE wav2vec 2.0 model, trained on LibriSpeech-960h with CTC loss from scratch.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9816128015518188}, "created": {"value": false, "score": 3.6954879760742188e-06}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.999972939491272}, "created": {"value": false, "score": 0.0004978179931640625}, "shared": {"value": false, "score": 4.76837158203125e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "LJSpeech", "normalizedForm": "LJSpeech", "offsetStart": 47, "offsetEnd": 55}, "context": "All the speech synthesis models are trained on LJSpeech and the synthesized speech is evaluated on LibriSpeech and LJ Speech. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.999972939491272}, "created": {"value": false, "score": 4.2438507080078125e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.999972939491272}, "created": {"value": false, "score": 4.2438507080078125e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "fairseq", "normalizedForm": "fairseq", "offsetStart": 47, "offsetEnd": 72}, "context": "We use the Transformer model as implemented in fairseq (Ott et al., 2019). ", "mentionContextAttributes": {"used": {"value": false, "score": 0.47602754831314087}, "created": {"value": false, "score": 0.02231884002685547}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": false, "score": 0.47602754831314087}, "created": {"value": false, "score": 0.02231884002685547}, "shared": {"value": true, "score": 0.9668394923210144}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "LJ Speech", "normalizedForm": "LJ Speech", "offsetStart": 54, "offsetEnd": 63}, "context": "On the other hand, the u2S component has learned from LJ Speech encoded with these units, and might have learned to compensate for these lower quality units.", "mentionContextAttributes": {"used": {"value": false, "score": 0.3966940641403198}, "created": {"value": false, "score": 3.0517578125e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.999972939491272}, "created": {"value": false, "score": 4.2438507080078125e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "references": [{"label": "(Ito and Johnson, 2017)", "normalizedForm": "Ito and Johnson, 2017", "refKey": 29}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "HuBERT", "normalizedForm": "HuBERT", "offsetStart": 56, "offsetEnd": 62}, "context": "Unlike CPC and wav2vec 2.0 that use a contrastive loss, HuBERT is trained with a masked prediction task similar to BERT (Devlin et al., 2019) but with masked continuous audio signals as inputs.", "mentionContextAttributes": {"used": {"value": false, "score": 0.0004146695137023926}, "created": {"value": false, "score": 2.0265579223632812e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9988412261009216}, "created": {"value": false, "score": 0.0002225041389465332}, "shared": {"value": false, "score": 1.3113021850585938e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "LJ Speech", "normalizedForm": "LJ Speech", "offsetStart": 60, "offsetEnd": 69}, "context": "On PER, we found a domain effect: resynthesizing input from LJ Speech yields lower PER than from LibriSpeech on all unsupervised models.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9998979568481445}, "created": {"value": false, "score": 1.6808509826660156e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.999972939491272}, "created": {"value": false, "score": 4.2438507080078125e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "references": [{"label": "(Ito and Johnson, 2017)", "normalizedForm": "Ito and Johnson, 2017", "refKey": 29}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Lib-rilight", "normalizedForm": "Lib-rilight", "offsetStart": 66, "offsetEnd": 77}, "context": "We introduced a suite of metrics, baselines, and first results on Lib-rilight that sets the playing field for future work.", "mentionContextAttributes": {"used": {"value": false, "score": 3.3974647521972656e-05}, "created": {"value": true, "score": 0.9998782873153687}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": false, "score": 3.3974647521972656e-05}, "created": {"value": true, "score": 0.9998782873153687}, "shared": {"value": false, "score": 1.1920928955078125e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "WaveGlow", "normalizedForm": "WaveGlow", "offsetStart": 74, "offsetEnd": 104}, "context": "For waveform generation, we use the pre-trained flow-based neural vocoder WaveGlow (Prenger et al., 2019). ", "mentionContextAttributes": {"used": {"value": true, "score": 0.69549959897995}, "created": {"value": false, "score": 0.004677236080169678}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.69549959897995}, "created": {"value": false, "score": 0.004677236080169678}, "shared": {"value": false, "score": 2.384185791015625e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "HuBERT", "normalizedForm": "HuBERT", "offsetStart": 75, "offsetEnd": 81}, "context": "The results are presented in terms of bitrate for 4 encoders (LogMel, CPC, HuBERT and wav2vec 2.0) varying in number of units (50,100,200).", "mentionContextAttributes": {"used": {"value": true, "score": 0.9988412261009216}, "created": {"value": false, "score": 1.8715858459472656e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9988412261009216}, "created": {"value": false, "score": 0.0002225041389465332}, "shared": {"value": false, "score": 1.3113021850585938e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Google TTS API", "normalizedForm": "Google TTS API", "offsetStart": 79, "offsetEnd": 93}, "context": "The test set (sWUGGY) consists of 5,000 word-pseudoword pairs generated by the Google TTS API, filtered for the word being present in the LibriSpeech 960h training set (Panayotov et al., 2015). ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9194200038909912}, "created": {"value": false, "score": 3.0875205993652344e-05}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9194200038909912}, "created": {"value": false, "score": 3.0875205993652344e-05}, "shared": {"value": false, "score": 3.5762786865234375e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "HuBERT", "normalizedForm": "HuBERT", "offsetStart": 92, "offsetEnd": 98}, "context": "For wav2vec, the performances on all metrics increase with more units, whereas, for CPC and HuBERT a U-shaped pattern emerges on most metrics, with best scores for units of intermediate sizes.", "mentionContextAttributes": {"used": {"value": false, "score": 0.002472102642059326}, "created": {"value": false, "score": 1.2874603271484375e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9988412261009216}, "created": {"value": false, "score": 0.0002225041389465332}, "shared": {"value": false, "score": 1.3113021850585938e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "LibriSpeech", "normalizedForm": "LibriSpeech", "offsetStart": 97, "offsetEnd": 108}, "context": "On PER, we found a domain effect: resynthesizing input from LJ Speech yields lower PER than from LibriSpeech on all unsupervised models. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9998979568481445}, "created": {"value": false, "score": 1.6808509826660156e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.999972939491272}, "created": {"value": false, "score": 0.0004978179931640625}, "shared": {"value": false, "score": 4.76837158203125e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "LibriSpeech", "normalizedForm": "LibriSpeech", "offsetStart": 99, "offsetEnd": 110}, "context": "All the speech synthesis models are trained on LJSpeech and the synthesized speech is evaluated on LibriSpeech and LJ Speech. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.999972939491272}, "created": {"value": false, "score": 4.2438507080078125e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.999972939491272}, "created": {"value": false, "score": 0.0004978179931640625}, "shared": {"value": false, "score": 4.76837158203125e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "LJ Speech", "normalizedForm": "LJ Speech", "offsetStart": 101, "offsetEnd": 110}, "context": "Note that we always use dev_clean set for LibriSpeech, and a random hold-out set of 1000 samples for LJ Speech that were not seen during training or validation.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9900070428848267}, "created": {"value": false, "score": 1.0251998901367188e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.999972939491272}, "created": {"value": false, "score": 4.2438507080078125e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "references": [{"label": "(Ito and Johnson, 2017)", "normalizedForm": "Ito and Johnson, 2017", "refKey": 29}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "LibriSpeech", "normalizedForm": "LibriSpeech", "offsetStart": 103, "offsetEnd": 114}, "context": "We use k-means to convert continuous frame representations into discrete representation by training on LibriSpeech clean-100h (Panayotov et al., 2015).", "mentionContextAttributes": {"used": {"value": true, "score": 0.7315223813056946}, "created": {"value": false, "score": 0.0004978179931640625}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.999972939491272}, "created": {"value": false, "score": 0.0004978179931640625}, "shared": {"value": false, "score": 4.76837158203125e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Lib-riSpeech", "normalizedForm": "Lib-riSpeech", "offsetStart": 105, "offsetEnd": 141}, "context": "We use the BASE 12 transformer-layer model trained for two iterations (Hsu et al., 2021) on 960 hours of Lib-riSpeech (Panayotov et al., 2015). ", "mentionContextAttributes": {"used": {"value": true, "score": 0.99433434009552}, "created": {"value": false, "score": 9.775161743164062e-06}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.99433434009552}, "created": {"value": false, "score": 9.775161743164062e-06}, "shared": {"value": false, "score": 1.1920928955078125e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "LJ Speech", "normalizedForm": "LJ Speech", "offsetStart": 115, "offsetEnd": 124}, "context": "All the speech synthesis models are trained on LJSpeech and the synthesized speech is evaluated on LibriSpeech and LJ Speech. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.999972939491272}, "created": {"value": false, "score": 4.2438507080078125e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.999972939491272}, "created": {"value": false, "score": 4.2438507080078125e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "references": [{"label": "(Ito and Johnson, 2017)", "normalizedForm": "Ito and Johnson, 2017", "refKey": 29}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "LibriSpeech", "normalizedForm": "LibriSpeech", "offsetStart": 138, "offsetEnd": 149}, "context": "The test set (sWUGGY) consists of 5,000 word-pseudoword pairs generated by the Google TTS API, filtered for the word being present in the LibriSpeech 960h training set (Panayotov et al., 2015).", "mentionContextAttributes": {"used": {"value": true, "score": 0.9194200038909912}, "created": {"value": false, "score": 3.0875205993652344e-05}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.999972939491272}, "created": {"value": false, "score": 0.0004978179931640625}, "shared": {"value": false, "score": 4.76837158203125e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "DeepCluster", "normalizedForm": "DeepCluster", "offsetStart": 138, "offsetEnd": 169}, "context": "The targets are obtained through unsupervised clustering of raw speech features or learned features from earlier iterations, motivated by DeepCluster (Caron et al., 2018). ", "mentionContextAttributes": {"used": {"value": true, "score": 0.8663893938064575}, "created": {"value": false, "score": 6.4849853515625e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.8663893938064575}, "created": {"value": false, "score": 6.4849853515625e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "LibriSpeech", "normalizedForm": "LibriSpeech", "offsetStart": 150, "offsetEnd": 161}, "context": "From the viewpoint of the encoder, LJ Speech is out-of-domain; therefore, one would expect that the units are making more errors than for the trained LibriSpeech. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.09051495790481567}, "created": {"value": false, "score": 2.6226043701171875e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.999972939491272}, "created": {"value": false, "score": 0.0004978179931640625}, "shared": {"value": false, "score": 4.76837158203125e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "LibriSpeech", "normalizedForm": "LibriSpeech", "offsetStart": 152, "offsetEnd": 163}, "context": "Here, we settled on selecting the temperature on a model-by-model basis by constructing a continuation task: we take the 1,000 shortest utterances from LibriSpeech testclean that are at least 6 seconds long, and use the first 3 seconds as prompts for the uLM (after transcribing them into pseudo-texts).", "mentionContextAttributes": {"used": {"value": true, "score": 0.9998069405555725}, "created": {"value": false, "score": 4.76837158203125e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.999972939491272}, "created": {"value": false, "score": 0.0004978179931640625}, "shared": {"value": false, "score": 4.76837158203125e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "HuBERT", "normalizedForm": "HuBERT", "offsetStart": 153, "offsetEnd": 159}, "context": "(4) we systematically study the effect of the type of encoding units by factorially crossing three recent speech-to-unit encoders, CPC, Wave2vec 2.0 and HuBERT, with three codebook sizes for the discrete units, 50, 100, 200.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9620263576507568}, "created": {"value": false, "score": 1.4662742614746094e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9988412261009216}, "created": {"value": false, "score": 0.0002225041389465332}, "shared": {"value": false, "score": 1.3113021850585938e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "LibriSpeech", "normalizedForm": "LibriSpeech", "offsetStart": 164, "offsetEnd": 175}, "context": "The logic of this last test is that it provides a more direct measure of the information lost in the 2 We use a BASE wav2vec 2.0 phoneme detection model trained on LibriSpeech-960h with CTC loss from scratch.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9999092817306519}, "created": {"value": false, "score": 1.9073486328125e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.999972939491272}, "created": {"value": false, "score": 0.0004978179931640625}, "shared": {"value": false, "score": 4.76837158203125e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "LibriSpeech", "normalizedForm": "LibriSpeech", "offsetStart": 176, "offsetEnd": 187}, "context": "The frozen phone recognition model is trained using BASE wav2vec model architecture with Connectionist Temporal Classification (CTC) loss (Graves et al., 2006) from scratch on LibriSpeech 960hours dataset. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9954838752746582}, "created": {"value": false, "score": 2.5033950805664062e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.999972939491272}, "created": {"value": false, "score": 0.0004978179931640625}, "shared": {"value": false, "score": 4.76837158203125e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "LibriSpeech", "normalizedForm": "LibriSpeech", "offsetStart": 196, "offsetEnd": 207}, "context": "The frozen ASR model is trained using LARGE wav2vec model architecture with Connectionist Temporal Classification (CTC) loss (Graves et al., 2006) from scratch (not using the pretrained model) on LibriSpeech 960hours dataset.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9981227517127991}, "created": {"value": false, "score": 1.4185905456542969e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.999972939491272}, "created": {"value": false, "score": 0.0004978179931640625}, "shared": {"value": false, "score": 4.76837158203125e-07}}}], "references": [{"refKey": 67, "tei": "<biblStruct xml:id=\"b67\">\n\t<analytic>\n\t\t<title level=\"a\" type=\"main\">CROWDMOS: An approach for crowdsourcing mean opinion score studies</title>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Flavio</forename><surname>Ribeiro</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Dinei</forename><surname>Florencio</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Cha</forename><surname>Zhang</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Michael</forename><surname>Seltzer</surname></persName>\n\t\t</author>\n\t\t<idno type=\"DOI\">10.1109/icassp.2011.5946971</idno>\n\t</analytic>\n\t<monogr>\n\t\t<title level=\"m\">2011 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>\n\t\t<imprint>\n\t\t\t<publisher>IEEE</publisher>\n\t\t\t<date type=\"published\" when=\"2011-05\">2011</date>\n\t\t\t<biblScope unit=\"page\" from=\"2416\" to=\"2419\" />\n\t\t</imprint>\n\t</monogr>\n</biblStruct>\n"}, {"refKey": 29, "tei": "<biblStruct xml:id=\"b29\">\n\t<monogr>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Keith</forename><surname>Ito</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Linda</forename><surname>Johnson</surname></persName>\n\t\t</author>\n\t\t<title level=\"m\">The lj speech dataset</title>\n\t\t<imprint>\n\t\t\t<date>2017</date>\n\t\t</imprint>\n\t</monogr>\n</biblStruct>\n"}], "runtime": 9990, "id": "e975342e40673c7a67409dbd1549de90029ff030", "metadata": {"id": "e975342e40673c7a67409dbd1549de90029ff030"}, "original_file_path": "../../datalake/Samuel/SOFTware-Sync/downloads/xml/hal-03329219.grobid.tei.xml", "file_name": "hal-03329219.grobid.tei.xml"}