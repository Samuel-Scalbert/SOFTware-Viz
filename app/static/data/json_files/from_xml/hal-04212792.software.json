{"application": "software-mentions", "version": "0.8.0", "date": "2024-10-07T12:00+0000", "md5": "715E96344B1CC28C87AD51847399B24B", "mentions": [{"type": "software", "software-type": "software", "software-name": {"rawForm": "MIME", "normalizedForm": "MIME", "offsetStart": 0, "offsetEnd": 28}, "context": "MIME (Majumder et al., 2020) used the emotion mimicry strategy to match the user's emotion based on the text context. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9931938648223877}, "created": {"value": false, "score": 1.728534698486328e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9931938648223877}, "created": {"value": false, "score": 0.0005400776863098145}, "shared": {"value": false, "score": 4.76837158203125e-07}}}, {"type": "software", "software-type": "implicit", "software-name": {"rawForm": "code", "normalizedForm": "code", "offsetStart": 4, "offsetEnd": 8}, "url": {"rawForm": "https://github.com/ neuromaancer/hedge_prediction", "normalizedForm": "https://github.com/ neuromaancer/hedge_prediction", "offsetStart": 25, "offsetEnd": 74}, "context": "The code is available in https://github.com/ neuromaancer/hedge_prediction 4 Results", "mentionContextAttributes": {"used": {"value": false, "score": 6.842613220214844e-05}, "created": {"value": false, "score": 0.000629127025604248}, "shared": {"value": true, "score": 0.9922667145729065}}, "documentContextAttributes": {"used": {"value": false, "score": 6.842613220214844e-05}, "created": {"value": false, "score": 0.000629127025604248}, "shared": {"value": true, "score": 0.9922667145729065}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "XGBoost", "normalizedForm": "XGBoost", "offsetStart": 4, "offsetEnd": 11}, "context": "For XGBoost and LightGBM, the worst performance is observed when NB and CS were removed, respectively, which implies that these features may provide important information for these models.", "mentionContextAttributes": {"used": {"value": false, "score": 0.41339588165283203}, "created": {"value": false, "score": 1.1205673217773438e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9983519315719604}, "created": {"value": false, "score": 3.123283386230469e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "AttnLSTM", "normalizedForm": "AttnLSTM", "offsetStart": 4, "offsetEnd": 12}, "context": "The AttnLSTM models, which incor-  porate attention mechanisms, do not show significant improvements over the regular LSTM models.", "mentionContextAttributes": {"used": {"value": false, "score": 0.0013977885246276855}, "created": {"value": false, "score": 1.2636184692382812e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9998300075531006}, "created": {"value": false, "score": 3.325939178466797e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "MoEL", "normalizedForm": "MoEL", "offsetStart": 11, "offsetEnd": 34}, "context": "Similarly, MoEL (Lin et al., 2019) model achieved varying degrees of accuracy in the same dataset -38% for the top 1, 63% for the top 3, and 74% for the top 5 for emotion detection, further emphasizing the difficulty of the task.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9991845488548279}, "created": {"value": false, "score": 1.3470649719238281e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9991845488548279}, "created": {"value": false, "score": 0.0005400776863098145}, "shared": {"value": false, "score": 5.960464477539062e-07}}, "references": [{"label": "(Lin et al., 2019)", "normalizedForm": "Lin et al., 2019", "refKey": 19}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "XGBoost", "normalizedForm": "XGBoost", "offsetStart": 17, "offsetEnd": 24}, "context": "The LightGBM and XGBoost models, which only use features without turn embeddings, also display competitive performance compared to the MLP, LSTM, and AttnLSTM models using all features. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.0018879175186157227}, "created": {"value": false, "score": 1.3709068298339844e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9983519315719604}, "created": {"value": false, "score": 3.123283386230469e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "MIME", "normalizedForm": "MIME", "offsetStart": 31, "offsetEnd": 35}, "context": "Despite the notable success of MIME and MoEL in predicting emotions or conversational strategies, they do not incorporate the social context (e.g., the relationship between speakers), or the emotional tenor of the conversation up until that point, nor do they include important nonverbal behaviors into reasoning and decision-making processes. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.0005093812942504883}, "created": {"value": false, "score": 0.0005400776863098145}, "shared": {"value": false, "score": 4.76837158203125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9931938648223877}, "created": {"value": false, "score": 0.0005400776863098145}, "shared": {"value": false, "score": 4.76837158203125e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "XGBoost", "normalizedForm": "XGBoost", "offsetStart": 31, "offsetEnd": 38}, "context": "From Table 1, the LightGBM and XGBoost models without embeddings achieved relatively low scores for F1 scores, precision and recall, indicating limited performance in terms of balanced precision and recall.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9983519315719604}, "created": {"value": false, "score": 3.123283386230469e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9983519315719604}, "created": {"value": false, "score": 3.123283386230469e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "AttnLSTM", "normalizedForm": "AttnLSTM", "offsetStart": 35, "offsetEnd": 43}, "context": "However, the attention-based LSTM (AttnLSTM) model did not significantly outperform the standard LSTM model in any metric.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9934160709381104}, "created": {"value": false, "score": 1.33514404296875e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9998300075531006}, "created": {"value": false, "score": 3.325939178466797e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "MoEL", "normalizedForm": "MoEL", "offsetStart": 37, "offsetEnd": 41}, "context": "The Mixture of Empathetic Listeners (MoEL) model (Lin et al., 2019) generates empathetic responses by recognizing the user's emotional state, using emotion-specific multi-agent listeners to respond, and then combining these responses based on the emotion distribution.", "mentionContextAttributes": {"used": {"value": false, "score": 0.0001709461212158203}, "created": {"value": false, "score": 0.0002498030662536621}, "shared": {"value": false, "score": 5.960464477539062e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9991845488548279}, "created": {"value": false, "score": 0.0005400776863098145}, "shared": {"value": false, "score": 5.960464477539062e-07}}, "references": [{"label": "(Lin et al., 2019)", "normalizedForm": "Lin et al., 2019", "refKey": 19, "offsetStart": 8066, "offsetEnd": 8084}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "AttnLSTM", "normalizedForm": "AttnLSTM", "offsetStart": 39, "offsetEnd": 47}, "context": "Interestingly, the best performance of AttnLSTM was achieved when the rapport feature was removed, suggesting that the attention mechanism could compensate for loss of rapport.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9998300075531006}, "created": {"value": false, "score": 3.325939178466797e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9998300075531006}, "created": {"value": false, "score": 3.325939178466797e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "MoEL", "normalizedForm": "MoEL", "offsetStart": 40, "offsetEnd": 44}, "context": "Despite the notable success of MIME and MoEL in predicting emotions or conversational strategies, they do not incorporate the social context (e.g., the relationship between speakers), or the emotional tenor of the conversation up until that point, nor do they include important nonverbal behaviors into reasoning and decision-making processes. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.0005093812942504883}, "created": {"value": false, "score": 0.0005400776863098145}, "shared": {"value": false, "score": 4.76837158203125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9991845488548279}, "created": {"value": false, "score": 0.0005400776863098145}, "shared": {"value": false, "score": 5.960464477539062e-07}}, "references": [{"label": "(Lin et al., 2019)", "normalizedForm": "Lin et al., 2019", "refKey": 19}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Mechanical Turk", "normalizedForm": "Mechanical Turk", "offsetStart": 49, "offsetEnd": 64}, "publisher": {"rawForm": "Amazon", "normalizedForm": "Amazon", "offsetStart": 42, "offsetEnd": 48}, "context": "The rapport annotation was carried out by Amazon Mechanical Turk (AMT) annotators as described in Madaio et al. (2018). ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9999535083770752}, "created": {"value": false, "score": 2.8252601623535156e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999535083770752}, "created": {"value": false, "score": 2.8252601623535156e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "in Several Layers", "normalizedForm": "in Several Layers", "offsetStart": 64, "offsetEnd": 81}, "context": "In our study, we use the widely-used DAMSL (Dialogue Act Markup in Several Layers) (Jurafsky, 1997) coding schema to annotate dialogue turns by using a state-of-the-art dialogue act classifier with context-aware self-attention (Raheja and Tetreault, 2019). ", "mentionContextAttributes": {"used": {"value": true, "score": 0.639464795589447}, "created": {"value": false, "score": 0.10441923141479492}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.639464795589447}, "created": {"value": false, "score": 0.10441923141479492}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "references": [{"label": "(Jurafsky, 1997)", "normalizedForm": "Jurafsky, 1997", "refKey": 14, "offsetStart": 15563, "offsetEnd": 15579}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "DialAct", "normalizedForm": "DialAct", "offsetStart": 74, "offsetEnd": 81}, "context": "The LSTM and MLP models showed a significant drop in performance when the DialAct feature was removed, suggesting a substantial dependency of models on the DialAct feature for their prediction capabilities. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9812411665916443}, "created": {"value": false, "score": 4.601478576660156e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9986121654510498}, "created": {"value": false, "score": 0.0035448074340820312}, "shared": {"value": false, "score": 3.5762786865234375e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "XGBoost", "normalizedForm": "XGBoost", "offsetStart": 108, "offsetEnd": 115}, "context": "This observation is substantiated by subsequent ablation studies, where classic classification models, like XGBoost and LightGBM, experienced a significant decline in F1 score when removing nonverbal behavior features.", "mentionContextAttributes": {"used": {"value": true, "score": 0.7528731822967529}, "created": {"value": false, "score": 1.6450881958007812e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9983519315719604}, "created": {"value": false, "score": 3.123283386230469e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "SHAP", "normalizedForm": "SHAP", "offsetStart": 129, "offsetEnd": 133}, "context": "In this study, we use Shapley values to interpret the contributions of extracted features in our classification models using the SHAP python package (Lundberg and Lee, 2017).", "mentionContextAttributes": {"used": {"value": true, "score": 0.9991903901100159}, "created": {"value": false, "score": 0.0007224678993225098}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9991903901100159}, "created": {"value": false, "score": 0.0007224678993225098}, "shared": {"value": false, "score": 2.384185791015625e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "AttnLSTM", "normalizedForm": "AttnLSTM", "offsetStart": 150, "offsetEnd": 158}, "context": "The LightGBM and XGBoost models, which only use features without turn embeddings, also display competitive performance compared to the MLP, LSTM, and AttnLSTM models using all features.", "mentionContextAttributes": {"used": {"value": false, "score": 0.0018879175186157227}, "created": {"value": false, "score": 1.3709068298339844e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9998300075531006}, "created": {"value": false, "score": 3.325939178466797e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "DialAct", "normalizedForm": "DialAct", "offsetStart": 156, "offsetEnd": 163}, "context": "The LSTM and MLP models showed a significant drop in performance when the DialAct feature was removed, suggesting a substantial dependency of models on the DialAct feature for their prediction capabilities. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9812411665916443}, "created": {"value": false, "score": 4.601478576660156e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9986121654510498}, "created": {"value": false, "score": 0.0035448074340820312}, "shared": {"value": false, "score": 3.5762786865234375e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "DialAct", "normalizedForm": "DialAct", "offsetStart": 175, "offsetEnd": 182}, "context": "Our study considered 6 groups of features: Conversational Strategies (CS), Tutoring Strategies (TS), Nonverbal Behaviors (NB), Contextual Information (ConInfo), Dialogue Act (DialAct), and Rapport.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9986121654510498}, "created": {"value": false, "score": 0.0035448074340820312}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9986121654510498}, "created": {"value": false, "score": 0.0035448074340820312}, "shared": {"value": false, "score": 3.5762786865234375e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "DialAct", "normalizedForm": "DialAct", "offsetStart": 214, "offsetEnd": 221}, "context": "Figure 1 shows that we divide a vector of turns into 6 parts: turn embedding, conversational strategies (CS), tutoring strategies (TS), nonverbal behaviors (NB), contextual information (ConInfo) and dialogue acts (DialAct). ", "mentionContextAttributes": {"used": {"value": true, "score": 0.6632230281829834}, "created": {"value": false, "score": 6.23464584350586e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9986121654510498}, "created": {"value": false, "score": 0.0035448074340820312}, "shared": {"value": false, "score": 3.5762786865234375e-07}}}], "references": [{"refKey": 19, "tei": "<biblStruct xml:id=\"b19\">\n\t<analytic>\n\t\t<title level=\"a\" type=\"main\">MoEL: Mixture of Empathetic Listeners</title>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Zhaojiang</forename><surname>Lin</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Andrea</forename><surname>Madotto</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Jamin</forename><surname>Shin</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Peng</forename><surname>Xu</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Pascale</forename><surname>Fung</surname></persName>\n\t\t</author>\n\t\t<idno type=\"DOI\">10.18653/v1/d19-1012</idno>\n\t</analytic>\n\t<monogr>\n\t\t<title level=\"m\">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>\n\t\t<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</meeting>\n\t\t<imprint>\n\t\t\t<publisher>Association for Computational Linguistics</publisher>\n\t\t\t<date type=\"published\" when=\"2019\">2019</date>\n\t\t\t<biblScope unit=\"page\">132</biblScope>\n\t\t</imprint>\n\t</monogr>\n</biblStruct>\n"}, {"refKey": 14, "tei": "<biblStruct xml:id=\"b14\">\n\t<monogr>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Dan</forename><surname>Jurafsky</surname></persName>\n\t\t</author>\n\t\t<title level=\"m\">Switchboard swbd-damsl shallowdiscourse-function annotation coders manual</title>\n\t\t<imprint>\n\t\t\t<date>1997</date>\n\t\t</imprint>\n\t</monogr>\n</biblStruct>\n"}], "runtime": 19516, "id": "1be438013e3593ffc6186b0f5d65606d345c72eb", "metadata": {"id": "1be438013e3593ffc6186b0f5d65606d345c72eb"}, "original_file_path": "../../datalake/Samuel/SOFTware-Sync/downloads/xml/hal-04212792.grobid.tei.xml", "file_name": "hal-04212792.grobid.tei.xml"}