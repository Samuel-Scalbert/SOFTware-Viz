{"application": "software-mentions", "version": "0.8.0", "date": "2024-10-07T11:52+0000", "md5": "01E3AE1E359A681C3F04D997A8C947ED", "mentions": [{"type": "software", "software-type": "software", "software-name": {"rawForm": "VGGish", "normalizedForm": "VGGish", "offsetStart": 0, "offsetEnd": 6}, "context": "VGGish is able to provide 128-dimensional embedding vectors ai for 1 s audio frames. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.0001609325408935547}, "created": {"value": false, "score": 7.486343383789062e-05}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9449588656425476}, "created": {"value": false, "score": 0.003691375255584717}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "references": [{"label": "[8]", "normalizedForm": "[8]", "refKey": 8}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "VGGish", "normalizedForm": "VGGish", "offsetStart": 0, "offsetEnd": 6}, "context": "VGGish embeddings are replicated over all tokens of the corresponding YAMNet tag. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9449588656425476}, "created": {"value": false, "score": 1.4424324035644531e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9449588656425476}, "created": {"value": false, "score": 0.003691375255584717}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "references": [{"label": "[8]", "normalizedForm": "[8]", "refKey": 8}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "VGGish", "normalizedForm": "VGGish", "offsetStart": 0, "offsetEnd": 6}, "context": "VGGish embeddings perform significantly better, hinting that providing the model with information on the sequence of sound events is critical in audio-only conditioning designs. ", "mentionContextAttributes": {"used": {"value": false, "score": 6.711483001708984e-05}, "created": {"value": false, "score": 9.131431579589844e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9449588656425476}, "created": {"value": false, "score": 0.003691375255584717}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "references": [{"label": "[8]", "normalizedForm": "[8]", "refKey": 8}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "AudioCaps", "normalizedForm": "AudioCaps", "offsetStart": 0, "offsetEnd": 9}, "context": "AudioCaps comprises training, validation, and evaluation splits of about 49000, 485 and 955 audio extracts.", "mentionContextAttributes": {"used": {"value": false, "score": 0.0006380677223205566}, "created": {"value": false, "score": 9.703636169433594e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9876536726951599}, "created": {"value": false, "score": 0.0009616613388061523}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "references": [{"label": "[17]", "normalizedForm": "[17]", "refKey": 17}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "VGGish", "normalizedForm": "VGGish", "offsetStart": 12, "offsetEnd": 18}, "context": "Compared to VGGish, the lower temporal detail in PANNs embeddings is compensated by greater semantic content, which translates to higher performance on AudioSet tagging. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.0006340146064758301}, "created": {"value": false, "score": 6.008148193359375e-05}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9449588656425476}, "created": {"value": false, "score": 0.003691375255584717}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "references": [{"label": "[8]", "normalizedForm": "[8]", "refKey": 8}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "SPIDEr", "normalizedForm": "SPIDEr", "offsetStart": 29, "offsetEnd": 35}, "context": "This variant achieves higher SPIDEr than both baselines, which suggests that BART language modeling parameters can already produce high quality captions.", "mentionContextAttributes": {"used": {"value": false, "score": 0.001153111457824707}, "created": {"value": false, "score": 3.0875205993652344e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9688838720321655}, "created": {"value": false, "score": 0.0009616613388061523}, "shared": {"value": false, "score": 2.384185791015625e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "SPIDEr", "normalizedForm": "SPIDEr", "offsetStart": 36, "offsetEnd": 46}, "context": "The overall performance is given by SPIDEr [20], the average of CIDEr and SPICE, which is the main metric of the DCASE challenge task 6 on captioning.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9688838720321655}, "created": {"value": false, "score": 1.1563301086425781e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9688838720321655}, "created": {"value": false, "score": 0.0009616613388061523}, "shared": {"value": false, "score": 2.384185791015625e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "AdamW", "normalizedForm": "AdamW", "offsetStart": 36, "offsetEnd": 46}, "context": "Optimization is performed using the AdamW [21] optimizer with parameters \u03b21 = 0.9 and \u03b22 = 0.999, and with a learning rate of 10 -5 . ", "mentionContextAttributes": {"used": {"value": true, "score": 0.986031174659729}, "created": {"value": false, "score": 5.125999450683594e-06}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.986031174659729}, "created": {"value": false, "score": 5.125999450683594e-06}, "shared": {"value": false, "score": 1.1920928955078125e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "AudioCaps", "normalizedForm": "AudioCaps", "offsetStart": 41, "offsetEnd": 50}, "context": "All the experiments are conducted on the AudioCaps dataset [17].", "mentionContextAttributes": {"used": {"value": true, "score": 0.9876536726951599}, "created": {"value": false, "score": 1.2874603271484375e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9876536726951599}, "created": {"value": false, "score": 0.0009616613388061523}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "references": [{"label": "[17]", "normalizedForm": "[17]", "refKey": 17, "offsetStart": 9269, "offsetEnd": 9273}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "AudioCaps", "normalizedForm": "AudioCaps", "offsetStart": 55, "offsetEnd": 64}, "context": "The best model achieves stateof-the-art performance on AudioCaps with 46.5 SPIDEr.", "mentionContextAttributes": {"used": {"value": false, "score": 0.08742552995681763}, "created": {"value": false, "score": 0.0009616613388061523}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9876536726951599}, "created": {"value": false, "score": 0.0009616613388061523}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "references": [{"label": "[17]", "normalizedForm": "[17]", "refKey": 17}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "VGGish", "normalizedForm": "VGGish", "offsetStart": 63, "offsetEnd": 69}, "context": "Contrary to audio-only experiments, PANNs performs better than VGGish when paired with YAMNet tags. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.00040525197982788086}, "created": {"value": false, "score": 3.2782554626464844e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9449588656425476}, "created": {"value": false, "score": 0.003691375255584717}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "references": [{"label": "[8]", "normalizedForm": "[8]", "refKey": 8}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "SPIDEr", "normalizedForm": "SPIDEr", "offsetStart": 75, "offsetEnd": 81}, "context": "The best model achieves stateof-the-art performance on AudioCaps with 46.5 SPIDEr.", "mentionContextAttributes": {"used": {"value": false, "score": 0.08742552995681763}, "created": {"value": false, "score": 0.0009616613388061523}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9688838720321655}, "created": {"value": false, "score": 0.0009616613388061523}, "shared": {"value": false, "score": 2.384185791015625e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "AudioCaps", "normalizedForm": "AudioCaps", "offsetStart": 84, "offsetEnd": 93}, "context": "First, the model conditioned on PANNs embeddings, which only contain one vector for AudioCaps examples, fails to generate well-structured captions.", "mentionContextAttributes": {"used": {"value": false, "score": 0.0022906064987182617}, "created": {"value": false, "score": 4.863739013671875e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9876536726951599}, "created": {"value": false, "score": 0.0009616613388061523}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "references": [{"label": "[17]", "normalizedForm": "[17]", "refKey": 17}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "VGGish", "normalizedForm": "VGGish", "offsetStart": 89, "offsetEnd": 95}, "context": "Thus, we explore deep embeddings from the penultimate layer of two other tagging models: VGGish [8] and PANNs [15], specifically the Wavegram-Logmel-CNN variant.", "mentionContextAttributes": {"used": {"value": false, "score": 0.00016129016876220703}, "created": {"value": false, "score": 0.003691375255584717}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9449588656425476}, "created": {"value": false, "score": 0.003691375255584717}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "references": [{"label": "[8]", "normalizedForm": "[8]", "refKey": 8, "offsetStart": 7991, "offsetEnd": 7994}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "SPIDEr", "normalizedForm": "SPIDEr", "offsetStart": 129, "offsetEnd": 135}, "context": "First, the TopDown-AlignedAtt model in the Audio-Caps dataset paper [17] achieves the best reported performance according to the SPIDEr metric.", "mentionContextAttributes": {"used": {"value": false, "score": 0.36034220457077026}, "created": {"value": false, "score": 1.704692840576172e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9688838720321655}, "created": {"value": false, "score": 0.0009616613388061523}, "shared": {"value": false, "score": 2.384185791015625e-07}}}], "references": [{"refKey": 8, "tei": "<biblStruct xml:id=\"b8\">\n\t<analytic>\n\t\t<title level=\"a\" type=\"main\">CNN architectures for large-scale audio classification</title>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Shawn</forename><surname>Hershey</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Sourish</forename><surname>Chaudhuri</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Daniel</forename><forename type=\"middle\">P W</forename><surname>Ellis</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Jort</forename><forename type=\"middle\">F</forename><surname>Gemmeke</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Aren</forename><surname>Jansen</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">R</forename><forename type=\"middle\">Channing</forename><surname>Moore</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Manoj</forename><surname>Plakal</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Devin</forename><surname>Platt</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Rif</forename><forename type=\"middle\">A</forename><surname>Saurous</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Bryan</forename><surname>Seybold</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Malcolm</forename><surname>Slaney</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Ron</forename><forename type=\"middle\">J</forename><surname>Weiss</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Kevin</forename><surname>Wilson</surname></persName>\n\t\t</author>\n\t\t<idno type=\"DOI\">10.1109/icassp.2017.7952132</idno>\n\t</analytic>\n\t<monogr>\n\t\t<title level=\"m\">2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>\n\t\t<imprint>\n\t\t\t<publisher>IEEE</publisher>\n\t\t\t<date type=\"published\" when=\"2017-03\">2017</date>\n\t\t\t<biblScope unit=\"page\">135</biblScope>\n\t\t</imprint>\n\t</monogr>\n</biblStruct>\n"}, {"refKey": 17, "tei": "<biblStruct xml:id=\"b17\">\n\t<analytic>\n\t\t<title level=\"a\" type=\"main\">AudioCaps: Generating captions for audios in the wild</title>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">C</forename><forename type=\"middle\">D</forename><surname>Kim</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">B</forename><surname>Kim</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">H</forename><surname>Lee</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">G</forename><surname>Kim</surname></persName>\n\t\t</author>\n\t</analytic>\n\t<monogr>\n\t\t<title level=\"m\">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>\n\t\t<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>\n\t\t<imprint>\n\t\t\t<date>June 2019</date>\n\t\t\t<biblScope unit=\"volume\">1</biblScope>\n\t\t\t<biblScope unit=\"page\">132</biblScope>\n\t\t</imprint>\n\t</monogr>\n</biblStruct>\n"}], "runtime": 13513, "id": "c6a5af342831eff8288d67b977b150317acbaa32", "metadata": {"id": "c6a5af342831eff8288d67b977b150317acbaa32"}, "original_file_path": "../../datalake/Samuel/SOFTware-Sync/downloads/xml/hal-03522488.grobid.tei.xml", "file_name": "hal-03522488.grobid.tei.xml"}