{"application": "software-mentions", "version": "0.8.0", "date": "2024-10-07T11:59+0000", "md5": "09E8FF0A351BC3446819F20D6AC4DF86", "mentions": [{"type": "software", "software-type": "software", "software-name": {"rawForm": "mBERT", "normalizedForm": "mBERT", "offsetStart": 9, "offsetEnd": 14}, "context": "We study mBERT with a novel behavioral test that disentangles the task fine-tuning influence from the pretraining step ( \u00a72.1), and a structural analysis on the intermediate representations ( \u00a72.2). ", "mentionContextAttributes": {"used": {"value": false, "score": 0.17789965867996216}, "created": {"value": false, "score": 0.00040417909622192383}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999762773513794}, "created": {"value": true, "score": 0.9999120235443115}, "shared": {"value": false, "score": 2.9802322387695312e-06}}, "references": [{"label": "(Devlin et al., 2019)", "normalizedForm": "Devlin et al., 2019", "refKey": 0}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "mBERT", "normalizedForm": "mBERT", "offsetStart": 12, "offsetEnd": 17}, "context": "We focus on mBERT (Devlin et al., 2019), a 12-layer model trained on the concatenation of 104 monolingual Wikipedia corpora, including our languages of study.", "mentionContextAttributes": {"used": {"value": false, "score": 0.00047028064727783203}, "created": {"value": true, "score": 0.9967531561851501}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999762773513794}, "created": {"value": true, "score": 0.9999120235443115}, "shared": {"value": false, "score": 2.9802322387695312e-06}}, "references": [{"label": "(Devlin et al., 2019)", "normalizedForm": "Devlin et al., 2019", "refKey": 0, "offsetStart": 8499, "offsetEnd": 8520}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "mBERT", "normalizedForm": "mBERT", "offsetStart": 16, "offsetEnd": 21}, "context": "This shows that mBERT reproduces the standard cross-lingual pipeline described by Ruder et al. (2019) without any explicit supervision signal for it. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.11485815048217773}, "created": {"value": false, "score": 1.5020370483398438e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999762773513794}, "created": {"value": true, "score": 0.9999120235443115}, "shared": {"value": false, "score": 2.9802322387695312e-06}}, "references": [{"label": "(Devlin et al., 2019)", "normalizedForm": "Devlin et al., 2019", "refKey": 0}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "mBERT", "normalizedForm": "mBERT", "offsetStart": 17, "offsetEnd": 22}, "context": "We conclude that mBERT's upper layers do not contribute to cross-language transfer.", "mentionContextAttributes": {"used": {"value": false, "score": 0.012314200401306152}, "created": {"value": false, "score": 0.00013899803161621094}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999762773513794}, "created": {"value": true, "score": 0.9999120235443115}, "shared": {"value": false, "score": 2.9802322387695312e-06}}, "references": [{"label": "(Devlin et al., 2019)", "normalizedForm": "Devlin et al., 2019", "refKey": 0}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "mBERT", "normalizedForm": "mBERT", "offsetStart": 34, "offsetEnd": 39}, "context": "Coloring is computed based on how mBERT with RANDOM-INIT performs compared to the REF model", "mentionContextAttributes": {"used": {"value": true, "score": 0.9997760653495789}, "created": {"value": false, "score": 3.5762786865234375e-06}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999762773513794}, "created": {"value": true, "score": 0.9999120235443115}, "shared": {"value": false, "score": 2.9802322387695312e-06}}, "references": [{"label": "(Devlin et al., 2019)", "normalizedForm": "Devlin et al., 2019", "refKey": 0}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "mBERT", "normalizedForm": "mBERT", "offsetStart": 43, "offsetEnd": 48}, "context": "Recently, Gonen et al. (2020) suggest that mBERT learns a language encoding component and an abstract cross-lingual component.", "mentionContextAttributes": {"used": {"value": false, "score": 6.151199340820312e-05}, "created": {"value": false, "score": 0.016821444034576416}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999762773513794}, "created": {"value": true, "score": 0.9999120235443115}, "shared": {"value": false, "score": 2.9802322387695312e-06}}, "references": [{"label": "(Devlin et al., 2019)", "normalizedForm": "Devlin et al., 2019", "refKey": 0}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "mBERT", "normalizedForm": "mBERT", "offsetStart": 68, "offsetEnd": 73}, "context": "In SRC-TRG, SRC indicates the source language on which we fine-tune mBERT, and TRG the target language on which we evaluate it.", "mentionContextAttributes": {"used": {"value": false, "score": 0.4993564486503601}, "created": {"value": false, "score": 1.6927719116210938e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999762773513794}, "created": {"value": true, "score": 0.9999120235443115}, "shared": {"value": false, "score": 2.9802322387695312e-06}}, "references": [{"label": "(Devlin et al., 2019)", "normalizedForm": "Devlin et al., 2019", "refKey": 0}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "mBERT", "normalizedForm": "mBERT", "offsetStart": 73, "offsetEnd": 78}, "context": "By combining those experiments on 17 languages and 3 tasks, we show that mBERT is constructed from: (1) a multilingual encoder in the lower layers, which aligns hidden representations across languages and is critical for cross-language transfer, and (2) a task-specific, language-agnostic predictor that has little effect to cross-language transfer, in the upper layers. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.3869514465332031}, "created": {"value": false, "score": 0.007789134979248047}, "shared": {"value": false, "score": 8.344650268554688e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999762773513794}, "created": {"value": true, "score": 0.9999120235443115}, "shared": {"value": false, "score": 2.9802322387695312e-06}}, "references": [{"label": "(Devlin et al., 2019)", "normalizedForm": "Devlin et al., 2019", "refKey": 0}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "mBERT", "normalizedForm": "mBERT", "offsetStart": 74, "offsetEnd": 79}, "context": "In Figure 1, we present the similarities between Russian and English with mBERT pretrained and fine-tuned on the three tasks. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.1978398561477661}, "created": {"value": true, "score": 0.9521704316139221}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999762773513794}, "created": {"value": true, "score": 0.9999120235443115}, "shared": {"value": false, "score": 2.9802322387695312e-06}}, "references": [{"label": "(Devlin et al., 2019)", "normalizedForm": "Devlin et al., 2019", "refKey": 0}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "mBERT", "normalizedForm": "mBERT", "offsetStart": 74, "offsetEnd": 79}, "context": "In this work, we are interested in understanding the mechanism that leads mBERT to perform zero-shot cross-lingual transfer.", "mentionContextAttributes": {"used": {"value": false, "score": 0.0001270771026611328}, "created": {"value": true, "score": 0.9999120235443115}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999762773513794}, "created": {"value": true, "score": 0.9999120235443115}, "shared": {"value": false, "score": 2.9802322387695312e-06}}, "references": [{"label": "(Devlin et al., 2019)", "normalizedForm": "Devlin et al., 2019", "refKey": 0}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "mBERT", "normalizedForm": "mBERT", "offsetStart": 86, "offsetEnd": 91}, "context": "By combining behavioral and structural analyses (Belinkov et al., 2020), we show that mBERT operates as the stacking of two modules: (1) A multilingual encoder, located in the lower part of the model, critical for cross-lingual transfer, is in charge of aligning multilingual representations; and (2) a task-specific, language-agnostic predictor which has little importance for cross-lingual transfer and is dedicated to performing the downstream task. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.018675625324249268}, "created": {"value": false, "score": 0.0005134344100952148}, "shared": {"value": false, "score": 2.9802322387695312e-06}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999762773513794}, "created": {"value": true, "score": 0.9999120235443115}, "shared": {"value": false, "score": 2.9802322387695312e-06}}, "references": [{"label": "(Devlin et al., 2019)", "normalizedForm": "Devlin et al., 2019", "refKey": 0}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "mBERT", "normalizedForm": "mBERT", "offsetStart": 90, "offsetEnd": 95}, "context": "We do so for each source-target language pairs using the representation of the pretrained mBERT model as well as for mBERT fine-tuned on each downstream task.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9906090497970581}, "created": {"value": false, "score": 3.325939178466797e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999762773513794}, "created": {"value": true, "score": 0.9999120235443115}, "shared": {"value": false, "score": 2.9802322387695312e-06}}, "references": [{"label": "(Devlin et al., 2019)", "normalizedForm": "Devlin et al., 2019", "refKey": 0}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "mBERT", "normalizedForm": "mBERT", "offsetStart": 92, "offsetEnd": 97}, "context": "This mechanism that emerges out-of-the-box, without any explicit supervision, suggests that mBERT behaves like the standard cross-lingual pipeline. ", "mentionContextAttributes": {"used": {"value": false, "score": 7.843971252441406e-05}, "created": {"value": false, "score": 0.0009445548057556152}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999762773513794}, "created": {"value": true, "score": 0.9999120235443115}, "shared": {"value": false, "score": 2.9802322387695312e-06}}, "references": [{"label": "(Devlin et al., 2019)", "normalizedForm": "Devlin et al., 2019", "refKey": 0}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "mBERT", "normalizedForm": "mBERT", "offsetStart": 93, "offsetEnd": 98}, "context": "Focusing on syntax, Chi et al. (2020) recently showed that the multilingual version of BERT (mBERT) (Devlin et al., 2019), encodes linguistic properties in shared multilingual sub-spaces.", "mentionContextAttributes": {"used": {"value": false, "score": 0.0005925893783569336}, "created": {"value": false, "score": 0.005463838577270508}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999762773513794}, "created": {"value": true, "score": 0.9999120235443115}, "shared": {"value": false, "score": 2.9802322387695312e-06}}, "references": [{"label": "(Devlin et al., 2019)", "normalizedForm": "Devlin et al., 2019", "refKey": 0, "offsetStart": 3036, "offsetEnd": 3057}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "mBERT", "normalizedForm": "mBERT", "offsetStart": 117, "offsetEnd": 122}, "context": "We do so for each source-target language pairs using the representation of the pretrained mBERT model as well as for mBERT fine-tuned on each downstream task.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9906090497970581}, "created": {"value": false, "score": 3.325939178466797e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999762773513794}, "created": {"value": true, "score": 0.9999120235443115}, "shared": {"value": false, "score": 2.9802322387695312e-06}}, "references": [{"label": "(Devlin et al., 2019)", "normalizedForm": "Devlin et al., 2019", "refKey": 0}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "mBERT", "normalizedForm": "mBERT", "offsetStart": 133, "offsetEnd": 138}, "context": "Fine-tuning Data For all the cross-lingual experiments, we use English, Russian and Arabic as source languages on which we fine-tune mBERT.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9998511075973511}, "created": {"value": false, "score": 1.2755393981933594e-05}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999762773513794}, "created": {"value": true, "score": 0.9999120235443115}, "shared": {"value": false, "score": 2.9802322387695312e-06}}, "references": [{"label": "(Devlin et al., 2019)", "normalizedForm": "Devlin et al., 2019", "refKey": 0}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "mBERT", "normalizedForm": "mBERT", "offsetStart": 134, "offsetEnd": 139}, "context": "In addition to the results presented in \u00a74.2, we report in Figure 4, a comparison of the cross-lingual similarity per hidden layer of mBERT fine-tuned on NER, across target languages.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9999762773513794}, "created": {"value": false, "score": 3.6954879760742188e-06}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999762773513794}, "created": {"value": true, "score": 0.9999120235443115}, "shared": {"value": false, "score": 2.9802322387695312e-06}}, "references": [{"label": "(Devlin et al., 2019)", "normalizedForm": "Devlin et al., 2019", "refKey": 0}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "mBERT", "normalizedForm": "mBERT", "offsetStart": 189, "offsetEnd": 194}, "context": "Figure 5: Cross-Lingual similarity (CKA) similarity ( \u00a74.2) of hidden representations of a source language (English) sentences with a target language sentences on fine-tuned and pretrained mBERT. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9999537467956543}, "created": {"value": false, "score": 4.649162292480469e-06}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999762773513794}, "created": {"value": true, "score": 0.9999120235443115}, "shared": {"value": false, "score": 2.9802322387695312e-06}}, "references": [{"label": "(Devlin et al., 2019)", "normalizedForm": "Devlin et al., 2019", "refKey": 0}]}], "references": [{"refKey": 0, "tei": "<biblStruct xml:id=\"b0\">\n\t<analytic>\n\t\t<title level=\"a\" type=\"main\">First Align, then Predict: Understanding the Cross-Lingual Ability of Multilingual BERT</title>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Benjamin</forename><surname>Muller</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Yanai</forename><surname>Elazar</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Beno\u00eet</forename><surname>Sagot</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Djam\u00e9</forename><surname>Seddah</surname></persName>\n\t\t</author>\n\t\t<idno type=\"DOI\">10.18653/v1/2021.eacl-main.189</idno>\n\t\t<idno>arXiv:1309.4168.</idno>\n\t</analytic>\n\t<monogr>\n\t\t<title level=\"m\">Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume</title>\n\t\t<meeting>the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume</meeting>\n\t\t<imprint>\n\t\t\t<publisher>Association for Computational Linguistics</publisher>\n\t\t\t<date type=\"published\" when=\"2021\" />\n\t\t\t<biblScope unit=\"page\" from=\"2214\" to=\"2231\" />\n\t\t</imprint>\n\t</monogr>\n</biblStruct>\n"}], "runtime": 18699, "id": "f7d488a1a06d306a18f85503e12a335845d97690", "metadata": {"id": "f7d488a1a06d306a18f85503e12a335845d97690"}, "original_file_path": "../../datalake/Samuel/SOFTware-Sync/downloads/xml/hal-03239087.grobid.tei.xml", "file_name": "hal-03239087.grobid.tei.xml"}