{"application": "software-mentions", "version": "0.8.0", "date": "2024-10-07T11:38+0000", "md5": "EEE67CDFE2443D15DC96902F8BA42592", "mentions": [{"type": "software", "software-type": "software", "software-name": {"rawForm": "GloVe", "normalizedForm": "GloVe", "offsetStart": 10, "offsetEnd": 15}, "context": "While for GloVe vectors we do not need the original, trained model in order to use the embeddings, for the BERT embeddings we require the pre-trained language models that we can then fine tune using the datasets of the downstream task.", "mentionContextAttributes": {"used": {"value": true, "score": 0.5814458727836609}, "created": {"value": false, "score": 4.7326087951660156e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9976211190223694}, "created": {"value": false, "score": 0.10092931985855103}, "shared": {"value": false, "score": 2.384185791015625e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "GloVe", "normalizedForm": "GloVe", "offsetStart": 21, "offsetEnd": 26}, "context": "We experimented with GloVe (300-dimensional) embeddings [30], using pre-trained word representations in all our models. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9935277104377747}, "created": {"value": false, "score": 0.002092719078063965}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9976211190223694}, "created": {"value": false, "score": 0.10092931985855103}, "shared": {"value": false, "score": 2.384185791015625e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "GloVe", "normalizedForm": "GloVe", "offsetStart": 58, "offsetEnd": 63}, "context": "Table 4 shows the results with BERT embeddings instead of GloVe, using feature ablation (syntactic vs all features) and two datasets for training to test whether this can improve performance.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9976211190223694}, "created": {"value": false, "score": 9.775161743164062e-06}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9976211190223694}, "created": {"value": false, "score": 0.10092931985855103}, "shared": {"value": false, "score": 2.384185791015625e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "GloVe", "normalizedForm": "GloVe", "offsetStart": 99, "offsetEnd": 104}, "context": "However, this best BERT baseline does not outperform the best results with the attention model and GloVe embeddings.", "mentionContextAttributes": {"used": {"value": false, "score": 0.003432154655456543}, "created": {"value": false, "score": 1.6808509826660156e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9976211190223694}, "created": {"value": false, "score": 0.10092931985855103}, "shared": {"value": false, "score": 2.384185791015625e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "AllenNLP", "normalizedForm": "AllenNLP", "offsetStart": 103, "offsetEnd": 112}, "context": "Textual entailment represents the class (amongst entailment, contradiction, or neutral) obtained using AllenNLP2 , a textual entailment model based on decomposable attention [27]. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.5726994872093201}, "created": {"value": false, "score": 2.9802322387695312e-06}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.5726994872093201}, "created": {"value": false, "score": 2.9802322387695312e-06}, "shared": {"value": false, "score": 1.1920928955078125e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "GloVe", "normalizedForm": "GloVe", "offsetStart": 107, "offsetEnd": 112}, "context": "We propose as baseline the model that performed the best, with the baseline using attention mechanism with GloVe embeddings and syntactic features trained on the web and essay datasets (0.544 macro average F 1 ). ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9756972789764404}, "created": {"value": false, "score": 0.10092931985855103}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9976211190223694}, "created": {"value": false, "score": 0.10092931985855103}, "shared": {"value": false, "score": 2.384185791015625e-07}}}], "references": [], "runtime": 6191, "id": "f064f2b3efcc5da4f53b01263f92f4aa3e37f8ad", "metadata": {"id": "f064f2b3efcc5da4f53b01263f92f4aa3e37f8ad"}, "original_file_path": "../../datalake/Samuel/SOFTware-Sync/downloads/xml/hal-02972180.grobid.tei.xml", "file_name": "hal-02972180.grobid.tei.xml"}