{"application": "software-mentions", "version": "0.8.0", "date": "2024-10-07T11:45+0000", "md5": "4D67F9903B0F0A52A250ED9FFFCA86B9", "mentions": [{"type": "software", "software-type": "software", "software-name": {"rawForm": "MEDeA", "normalizedForm": "MEDeA", "offsetStart": 0, "offsetEnd": 5}, "context": "MEDeA learns a single model for all possible language pairs, on 50 epochs. ", "mentionContextAttributes": {"used": {"value": false, "score": 2.872943878173828e-05}, "created": {"value": false, "score": 0.00022846460342407227}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999593496322632}, "created": {"value": true, "score": 0.9595263004302979}, "shared": {"value": false, "score": 4.76837158203125e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "MEDeA", "normalizedForm": "MEDeA", "offsetStart": 0, "offsetEnd": 5}, "context": "MEDeA is trained with the real dataset, on all language combinations possible (IT, ES, LA) at once, with early stopping at 50 epochs. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.004215538501739502}, "created": {"value": false, "score": 0.00016558170318603516}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999593496322632}, "created": {"value": true, "score": 0.9595263004302979}, "shared": {"value": false, "score": 4.76837158203125e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "SacreBLEU", "normalizedForm": "SacreBLEU", "offsetStart": 7, "offsetEnd": 16}, "context": "We use SacreBLEU, Post (2018)'s implementation", "mentionContextAttributes": {"used": {"value": true, "score": 0.9997406601905823}, "created": {"value": false, "score": 3.0279159545898438e-05}, "shared": {"value": false, "score": 9.5367431640625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9997406601905823}, "created": {"value": false, "score": 3.0279159545898438e-05}, "shared": {"value": false, "score": 9.5367431640625e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "SacreBLEU", "normalizedForm": "SacreBLEU", "offsetStart": 7, "offsetEnd": 16}, "context": "We use SacreBLEU, Post (2018)'s implementation", "mentionContextAttributes": {"used": {"value": true, "score": 0.9997406601905823}, "created": {"value": false, "score": 3.0279159545898438e-05}, "shared": {"value": false, "score": 9.5367431640625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9997406601905823}, "created": {"value": false, "score": 3.0279159545898438e-05}, "shared": {"value": false, "score": 9.5367431640625e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "MEDeA", "normalizedForm": "MEDeA", "offsetStart": 13, "offsetEnd": 18}, "context": "Neural Model MEDeA (Multiway Encoder Decoder Architecture) is our implementation of one of the classical approaches in neural MT: the sequence-to-sequence encoderdecoder model with attention (Bahdanau et al., 2015; Luong et al., 2015). ", "mentionContextAttributes": {"used": {"value": false, "score": 0.00015914440155029297}, "created": {"value": true, "score": 0.9595263004302979}, "shared": {"value": false, "score": 4.76837158203125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999593496322632}, "created": {"value": true, "score": 0.9595263004302979}, "shared": {"value": false, "score": 4.76837158203125e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Post", "normalizedForm": "Post", "offsetStart": 18, "offsetEnd": 22}, "context": "We use SacreBLEU, Post (2018)'s implementation", "mentionContextAttributes": {"used": {"value": true, "score": 0.9997406601905823}, "created": {"value": false, "score": 3.0279159545898438e-05}, "shared": {"value": false, "score": 9.5367431640625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9997406601905823}, "created": {"value": false, "score": 3.0279159545898438e-05}, "shared": {"value": false, "score": 9.5367431640625e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Post", "normalizedForm": "Post", "offsetStart": 18, "offsetEnd": 22}, "context": "We use SacreBLEU, Post (2018)'s implementation", "mentionContextAttributes": {"used": {"value": true, "score": 0.9997406601905823}, "created": {"value": false, "score": 3.0279159545898438e-05}, "shared": {"value": false, "score": 9.5367431640625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9997406601905823}, "created": {"value": false, "score": 3.0279159545898438e-05}, "shared": {"value": false, "score": 9.5367431640625e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "EtymDB", "normalizedForm": "EtymDB", "offsetStart": 20, "offsetEnd": 26}, "version": {"rawForm": "2.0", "normalizedForm": "2.0", "offsetStart": 27, "offsetEnd": 30}, "context": "Raw Data Extraction EtymDB 2.0 (Fourrier and Sagot,  2020) is a database of lexemes (i.e. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.0034543871879577637}, "created": {"value": false, "score": 1.7523765563964844e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999634027481079}, "created": {"value": false, "score": 1.7523765563964844e-05}, "shared": {"value": false, "score": 5.245208740234375e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "EtymDB", "normalizedForm": "EtymDB", "offsetStart": 37, "offsetEnd": 43}, "version": {"rawForm": "2.0", "normalizedForm": "2.0"}, "context": "To generate the cognate dataset from EtymDB, we followed the inheritance etymological paths between words; two words form a cognate pair if they share a common ancestor9 in one of their common parent languages (Old Latin, Proto-Italic, or Proto-Indo-European for LA-IT and LA-ES, Vulgar Latin, Latin, and the previous languages for IT-ES).", "mentionContextAttributes": {"used": {"value": true, "score": 0.9999634027481079}, "created": {"value": false, "score": 1.2755393981933594e-05}, "shared": {"value": false, "score": 5.245208740234375e-06}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999634027481079}, "created": {"value": false, "score": 1.7523765563964844e-05}, "shared": {"value": false, "score": 5.245208740234375e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "MEDeA", "normalizedForm": "MEDeA", "offsetStart": 44, "offsetEnd": 49}, "context": "MOSES is trained on the same data splits as MEDeA, shuffled in the same order, to predict 1 to 3 best results.", "mentionContextAttributes": {"used": {"value": false, "score": 0.005039811134338379}, "created": {"value": false, "score": 0.0003437995910644531}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999593496322632}, "created": {"value": true, "score": 0.9595263004302979}, "shared": {"value": false, "score": 4.76837158203125e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "MEDeA", "normalizedForm": "MEDeA", "offsetStart": 49, "offsetEnd": 54}, "context": "[rUstIkUm] (neut.), and [rUstIkss] (nonsense) by MEDeA, vs [rUkUstI],[rUIkOst] and [UsrtIkwUs], three meaningless forms by MOSES.16", "mentionContextAttributes": {"used": {"value": true, "score": 0.9994133710861206}, "created": {"value": false, "score": 5.7220458984375e-06}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999593496322632}, "created": {"value": true, "score": 0.9595263004302979}, "shared": {"value": false, "score": 4.76837158203125e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "MEDeA", "normalizedForm": "MEDeA", "offsetStart": 49, "offsetEnd": 54}, "context": "[rUstIkUm] (neut.), and [rUstIkss] (nonsense) by MEDeA, vs [rUkUstI],[rUIkOst] and [UsrtIkwUs], three meaningless forms by MOSES.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9981156587600708}, "created": {"value": false, "score": 5.602836608886719e-06}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999593496322632}, "created": {"value": true, "score": 0.9595263004302979}, "shared": {"value": false, "score": 4.76837158203125e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "MEDeA", "normalizedForm": "MEDeA", "offsetStart": 49, "offsetEnd": 54}, "context": "[rUstIkUm] (neut.), and [rUstIkss] (nonsense) by MEDeA, vs [rUkUstI],", "mentionContextAttributes": {"used": {"value": true, "score": 0.9999593496322632}, "created": {"value": false, "score": 1.1205673217773438e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999593496322632}, "created": {"value": true, "score": 0.9595263004302979}, "shared": {"value": false, "score": 4.76837158203125e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "GIZA++", "normalizedForm": "GIZA++", "offsetStart": 53, "offsetEnd": 59}, "context": "We first tokenise and align the bilingual data using GIZA++ (Och and  Ney, 2003), then train a 3-gram language model of the output (Heafield, 2011), a phrase table that stores weighted correspondences between source and target phonemes (we use 80% of our training data) and a reordering model. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.10736501216888428}, "created": {"value": false, "score": 0.0005428194999694824}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": false, "score": 0.10736501216888428}, "created": {"value": false, "score": 0.0005428194999694824}, "shared": {"value": false, "score": 2.384185791015625e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "MEDeA", "normalizedForm": "MEDeA", "offsetStart": 66, "offsetEnd": 71}, "context": "We study the impact of the hidden dimension on the performance of MEDeA.", "mentionContextAttributes": {"used": {"value": true, "score": 0.7453370094299316}, "created": {"value": false, "score": 0.0033999085426330566}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999593496322632}, "created": {"value": true, "score": 0.9595263004302979}, "shared": {"value": false, "score": 4.76837158203125e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "MEDeA", "normalizedForm": "MEDeA", "offsetStart": 76, "offsetEnd": 81}, "context": "For all experiments, each phone is embedded as a vector of length 5, 13 and MEDeA is trained with batches of size 30, a batch dropout of 0.2, no layer or attention dropout, and Adam optimisation with a 0.01 learning rate.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9718835949897766}, "created": {"value": false, "score": 1.1205673217773438e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999593496322632}, "created": {"value": true, "score": 0.9595263004302979}, "shared": {"value": false, "score": 4.76837158203125e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "MEDeA", "normalizedForm": "MEDeA", "offsetStart": 170, "offsetEnd": 175}, "context": "This analysis focuses on data sizes of 1000 and above, as the impact of very small datasets (500 word pairs per language) on the prediction BLEU scores of both MOSES and MEDeA will be specifically discussed in the next section.", "mentionContextAttributes": {"used": {"value": false, "score": 0.009376227855682373}, "created": {"value": false, "score": 0.00023752450942993164}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999593496322632}, "created": {"value": true, "score": 0.9595263004302979}, "shared": {"value": false, "score": 4.76837158203125e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "MEDeA", "normalizedForm": "MEDeA", "offsetStart": 303, "offsetEnd": 308}, "context": "Along the same lines, we also observe that using 2 or 3 best experiments barely improves the result for the first two situations (adds 2 to 5 points from 1-best to 3-best on average), when it considerably increases the BLEU score for the prediction from daughter to mother language (20 to 25 points for MEDeA, 10 to 15 points for MOSES).", "mentionContextAttributes": {"used": {"value": false, "score": 0.3069475293159485}, "created": {"value": false, "score": 1.9073486328125e-06}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999593496322632}, "created": {"value": true, "score": 0.9595263004302979}, "shared": {"value": false, "score": 4.76837158203125e-07}}}], "references": [], "runtime": 7937, "id": "5551140d150370c65a0ab5e370940bb198a7c469", "metadata": {"id": "5551140d150370c65a0ab5e370940bb198a7c469"}, "original_file_path": "../../datalake/Samuel/SOFTware-Sync/downloads/xml/hal-02529929.grobid.tei.xml", "file_name": "hal-02529929.grobid.tei.xml"}