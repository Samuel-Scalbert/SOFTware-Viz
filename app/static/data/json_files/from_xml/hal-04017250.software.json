{"application": "software-mentions", "version": "0.8.0", "date": "2024-10-07T11:54+0000", "md5": "F728C6C416EE7B531B4ED20A5B002797", "mentions": [{"type": "software", "software-type": "software", "software-name": {"rawForm": "BERTweet", "normalizedForm": "BERTweet", "offsetStart": 4, "offsetEnd": 12}, "context": "The BERTweet model is trained on tweets.", "mentionContextAttributes": {"used": {"value": false, "score": 0.0007345080375671387}, "created": {"value": false, "score": 0.2237643599510193}, "shared": {"value": false, "score": 4.76837158203125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999574422836304}, "created": {"value": true, "score": 0.9414715766906738}, "shared": {"value": false, "score": 8.344650268554688e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "BERTweet", "normalizedForm": "BERTweet", "offsetStart": 8, "offsetEnd": 16}, "context": "For the BERTweet embeddings and the single task approach, we find that using 2 attention heads does not significantly improve the average score compared to Zampieri et al. ( 2022) system.", "mentionContextAttributes": {"used": {"value": true, "score": 0.5078301429748535}, "created": {"value": false, "score": 8.463859558105469e-06}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999574422836304}, "created": {"value": true, "score": 0.9414715766906738}, "shared": {"value": false, "score": 8.344650268554688e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "BERTweet", "normalizedForm": "BERTweet", "offsetStart": 34, "offsetEnd": 42}, "context": "Table 1 shows that in the case of BERTweet embeddings, the multi-task system significantly outperforms the baseline system: the baseline reaches 72.7% of the average macro-F1 score, compared to 74.5% and 74.0% using 2 and 4 attention heads with multi-task learning, respectively.", "mentionContextAttributes": {"used": {"value": false, "score": 0.010220706462860107}, "created": {"value": false, "score": 2.0265579223632812e-06}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999574422836304}, "created": {"value": true, "score": 0.9414715766906738}, "shared": {"value": false, "score": 8.344650268554688e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "BERTweet", "normalizedForm": "BERTweet", "offsetStart": 51, "offsetEnd": 59}, "context": "So, in this article, we are experimenting with the BERTweet and the HateBERT embeddings.", "mentionContextAttributes": {"used": {"value": false, "score": 0.004978775978088379}, "created": {"value": true, "score": 0.9414715766906738}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999574422836304}, "created": {"value": true, "score": 0.9414715766906738}, "shared": {"value": false, "score": 8.344650268554688e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "BERTweet", "normalizedForm": "BERTweet", "offsetStart": 84, "offsetEnd": 92}, "context": "We carried out our experiments on four corpora and using two contextual embeddings: BERTweet and HateBERT.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9999574422836304}, "created": {"value": true, "score": 0.5860810875892639}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999574422836304}, "created": {"value": true, "score": 0.9414715766906738}, "shared": {"value": false, "score": 8.344650268554688e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "BERTweet", "normalizedForm": "BERTweet", "offsetStart": 94, "offsetEnd": 102}, "context": "Our best configuration of HSD system with multi-task learning, two heads of selfattention and BERTweet embeddings improves the average score by 1% relative compared to the Zampieri et al. (2022) HSD system (74.5% versus 73.5%).", "mentionContextAttributes": {"used": {"value": false, "score": 0.0006049871444702148}, "created": {"value": false, "score": 0.000405728816986084}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999574422836304}, "created": {"value": true, "score": 0.9414715766906738}, "shared": {"value": false, "score": 8.344650268554688e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "BERTweet", "normalizedForm": "BERTweet", "offsetStart": 97, "offsetEnd": 105}, "context": "As no corpus annotated in terms of both MWE and hate speech is available, we cannot finetune the BERTweet or HateBERT models for multi-task learning.", "mentionContextAttributes": {"used": {"value": false, "score": 0.0012091994285583496}, "created": {"value": false, "score": 0.002625703811645508}, "shared": {"value": false, "score": 4.76837158203125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999574422836304}, "created": {"value": true, "score": 0.9414715766906738}, "shared": {"value": false, "score": 8.344650268554688e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "BERTweet", "normalizedForm": "BERTweet", "offsetStart": 129, "offsetEnd": 137}, "context": "To generate contextual token embeddings, we use state-ofthe-art transformers-based models trained on tweets or hateful data: the BERTweet-base model (Nguyen et al., 2020), the HateBERT model (Caselli et al., 2021), and the fBERT model (Sarkar et al., 2021).", "mentionContextAttributes": {"used": {"value": true, "score": 0.8654881715774536}, "created": {"value": false, "score": 0.0020726919174194336}, "shared": {"value": false, "score": 8.344650268554688e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999574422836304}, "created": {"value": true, "score": 0.9414715766906738}, "shared": {"value": false, "score": 8.344650268554688e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "BERTweet", "normalizedForm": "BERTweet", "offsetStart": 136, "offsetEnd": 144}, "context": "For the two studied embeddings, the best performance is achieved by the multi-task system with 2 attention heads: 74.5% and 74.2% using BERTweet and HateBERT, respectively.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9950156807899475}, "created": {"value": false, "score": 1.9311904907226562e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999574422836304}, "created": {"value": true, "score": 0.9414715766906738}, "shared": {"value": false, "score": 8.344650268554688e-07}}}], "references": [], "runtime": 8990, "id": "0f5a29492fb5e44db5accba6ab0cfd9338de7a97", "metadata": {"id": "0f5a29492fb5e44db5accba6ab0cfd9338de7a97"}, "original_file_path": "../../datalake/Samuel/SOFTware-Sync/downloads/xml/hal-04017250.grobid.tei.xml", "file_name": "hal-04017250.grobid.tei.xml"}