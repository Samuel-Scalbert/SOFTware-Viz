{"application": "software-mentions", "version": "0.8.0", "date": "2024-10-07T11:57+0000", "md5": "A671D9A1C9155A902456B8FC5FEEA260", "mentions": [{"type": "software", "software-type": "software", "software-name": {"rawForm": "Glove", "normalizedForm": "Glove", "offsetStart": 0, "offsetEnd": 5}, "context": "Glove focuses on the global context instead of local one and uses a word-word co-occurrence matrix computed from the entire corpus. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.0006697177886962891}, "created": {"value": false, "score": 8.285045623779297e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9969049096107483}, "created": {"value": false, "score": 0.0004506707191467285}, "shared": {"value": false, "score": 2.384185791015625e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "FastText", "normalizedForm": "FastText", "offsetStart": 0, "offsetEnd": 8}, "context": "FastText is based on the CBOW architecture but using \ud835\udc5b-grams as input instead of full words. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.0004705190658569336}, "created": {"value": false, "score": 0.00032782554626464844}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9986816048622131}, "created": {"value": false, "score": 0.006463348865509033}, "shared": {"value": false, "score": 1.7881393432617188e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "FlauBERT", "normalizedForm": "FlauBERT", "offsetStart": 0, "offsetEnd": 8}, "context": "FlauBERT fared even worse. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9094663858413696}, "created": {"value": false, "score": 2.1576881408691406e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9998874664306641}, "created": {"value": false, "score": 0.0007305741310119629}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "references": [{"label": "[15]", "normalizedForm": "[15]", "refKey": 15}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "FastText", "normalizedForm": "FastText", "offsetStart": 0, "offsetEnd": 8}, "context": "FastText and FlauBERT are more based on a character or sub-word level. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.006217062473297119}, "created": {"value": false, "score": 5.91278076171875e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9986816048622131}, "created": {"value": false, "score": 0.006463348865509033}, "shared": {"value": false, "score": 1.7881393432617188e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "CamemBERT", "normalizedForm": "CamemBERT", "offsetStart": 0, "offsetEnd": 9}, "context": "CamemBERT and FlauBERT have been already trained for classification, so we used their classifier based on neural networks.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9998377561569214}, "created": {"value": false, "score": 0.0001289844512939453}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999487400054932}, "created": {"value": false, "score": 0.0018892288208007812}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "references": [{"label": "[14]", "normalizedForm": "[14]", "refKey": 14}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "CamemBERT", "normalizedForm": "CamemBERT", "offsetStart": 0, "offsetEnd": 9}, "context": "CamemBERT was trained on OSCAR [16], whose size is 138 GB after cleaning, while FlauBERT used 24 corpora from different sources (Wikipedia, books, Common Crawl, etc.), whose overall size is only half as the CamemBERT training data (71 GB).", "mentionContextAttributes": {"used": {"value": true, "score": 0.9998874664306641}, "created": {"value": false, "score": 4.887580871582031e-06}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999487400054932}, "created": {"value": false, "score": 0.0018892288208007812}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "references": [{"label": "[14]", "normalizedForm": "[14]", "refKey": 14}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "FlauBERT", "normalizedForm": "FlauBERT", "offsetStart": 13, "offsetEnd": 21}, "context": "FastText and FlauBERT are more based on a character or sub-word level. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.006217062473297119}, "created": {"value": false, "score": 5.91278076171875e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9998874664306641}, "created": {"value": false, "score": 0.0007305741310119629}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "references": [{"label": "[15]", "normalizedForm": "[15]", "refKey": 15}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "FlauBERT", "normalizedForm": "FlauBERT", "offsetStart": 14, "offsetEnd": 22}, "context": "CamemBERT and FlauBERT have been already trained for classification, so we used their classifier based on neural networks.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9998377561569214}, "created": {"value": false, "score": 0.0001289844512939453}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9998874664306641}, "created": {"value": false, "score": 0.0007305741310119629}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "references": [{"label": "[15]", "normalizedForm": "[15]", "refKey": 15}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "FlauBERT", "normalizedForm": "FlauBERT", "offsetStart": 38, "offsetEnd": 46}, "context": "They also differ for their tokenizer: FlauBERT uses a basic Byte Pair Encoding [17], whereas Camem-BERT prefers its extension, called SentencePiece [18].", "mentionContextAttributes": {"used": {"value": false, "score": 0.0007908940315246582}, "created": {"value": false, "score": 4.5299530029296875e-06}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9998874664306641}, "created": {"value": false, "score": 0.0007305741310119629}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "references": [{"label": "[15]", "normalizedForm": "[15]", "refKey": 15}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "FastText", "normalizedForm": "FastText", "offsetStart": 42, "offsetEnd": 50}, "context": "We chose a smaller number of features for FastText because of its need of memory storage. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.6972516775131226}, "created": {"value": false, "score": 0.006463348865509033}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9986816048622131}, "created": {"value": false, "score": 0.006463348865509033}, "shared": {"value": false, "score": 1.7881393432617188e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Skipgram", "normalizedForm": "Skipgram", "offsetStart": 45, "offsetEnd": 53}, "context": "Dynomant et al. [9] compare mainly Word2Vec (Skipgram, CBOW), FastText and Glove on a specific dataset (health-related documents) written in French.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9969049096107483}, "created": {"value": false, "score": 2.6226043701171875e-06}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9969049096107483}, "created": {"value": false, "score": 0.12236428260803223}, "shared": {"value": false, "score": 3.5762786865234375e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "ELMo", "normalizedForm": "ELMo", "offsetStart": 46, "offsetEnd": 50}, "context": "For example, Embeddings from Language Models (ELMo) [5] is a twolayer bidirectional language model using Bi-LSTM.", "mentionContextAttributes": {"used": {"value": false, "score": 5.936622619628906e-05}, "created": {"value": false, "score": 1.1205673217773438e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": false, "score": 0.0008714795112609863}, "created": {"value": false, "score": 0.0008404254913330078}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "references": [{"label": "[5]", "normalizedForm": "[5]", "refKey": 5, "offsetStart": 6126, "offsetEnd": 6129}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Glove", "normalizedForm": "Glove", "offsetStart": 49, "offsetEnd": 54}, "context": "The authors find Word2Vec to perform better than Glove, and FastText to be the worst model in that setting. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.5158169269561768}, "created": {"value": false, "score": 2.396106719970703e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9969049096107483}, "created": {"value": false, "score": 0.0004506707191467285}, "shared": {"value": false, "score": 2.384185791015625e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "FastText", "normalizedForm": "FastText", "offsetStart": 52, "offsetEnd": 60}, "context": "Non-contextual methods such as Word2Vec, Doc2Vec or FastText lagged behind. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.2402309775352478}, "created": {"value": false, "score": 3.898143768310547e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9986816048622131}, "created": {"value": false, "score": 0.006463348865509033}, "shared": {"value": false, "score": 1.7881393432617188e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "ELMo", "normalizedForm": "ELMo", "offsetStart": 55, "offsetEnd": 59}, "context": "However, although pre-trained (Bi-)LSTM models such as ELMo have recently become available for French, 2 we were not aware of them at the time we planned our experiments.", "mentionContextAttributes": {"used": {"value": false, "score": 0.0006011724472045898}, "created": {"value": false, "score": 0.0003025531768798828}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "documentContextAttributes": {"used": {"value": false, "score": 0.0008714795112609863}, "created": {"value": false, "score": 0.0008404254913330078}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "references": [{"label": "[5]", "normalizedForm": "[5]", "refKey": 5}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "FastText", "normalizedForm": "FastText", "offsetStart": 60, "offsetEnd": 68}, "context": "The authors find Word2Vec to perform better than Glove, and FastText to be the worst model in that setting.", "mentionContextAttributes": {"used": {"value": true, "score": 0.5158169269561768}, "created": {"value": false, "score": 2.396106719970703e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9986816048622131}, "created": {"value": false, "score": 0.006463348865509033}, "shared": {"value": false, "score": 1.7881393432617188e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "FastText", "normalizedForm": "FastText", "offsetStart": 62, "offsetEnd": 70}, "context": "Dynomant et al. [9] compare mainly Word2Vec (Skipgram, CBOW), FastText and Glove on a specific dataset (health-related documents) written in French.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9969049096107483}, "created": {"value": false, "score": 2.6226043701171875e-06}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9986816048622131}, "created": {"value": false, "score": 0.006463348865509033}, "shared": {"value": false, "score": 1.7881393432617188e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "CamemBERT", "normalizedForm": "CamemBERT", "offsetStart": 62, "offsetEnd": 71}, "context": "In general, classical methods gave better results, except for CamemBERT, which has similar results.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9976790547370911}, "created": {"value": false, "score": 2.47955322265625e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999487400054932}, "created": {"value": false, "score": 0.0018892288208007812}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "references": [{"label": "[14]", "normalizedForm": "[14]", "refKey": 14}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "FlauBERT", "normalizedForm": "FlauBERT", "offsetStart": 67, "offsetEnd": 75}, "context": "Finally, the models use different strategies for the masking task: FlauBERT masks sub-word, whereas CamemBERT masks the whole word.", "mentionContextAttributes": {"used": {"value": false, "score": 0.0027690529823303223}, "created": {"value": false, "score": 1.2636184692382812e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9998874664306641}, "created": {"value": false, "score": 0.0007305741310119629}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "references": [{"label": "[15]", "normalizedForm": "[15]", "refKey": 15}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "CamemBERT", "normalizedForm": "CamemBERT", "offsetStart": 70, "offsetEnd": 79}, "context": "Comparing text representations, classical methods (mainly TF-IDF) and CamemBERT achieved the best F1-Score. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9999487400054932}, "created": {"value": false, "score": 5.364418029785156e-06}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999487400054932}, "created": {"value": false, "score": 0.0018892288208007812}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "references": [{"label": "[14]", "normalizedForm": "[14]", "refKey": 14}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Skipgram", "normalizedForm": "Skipgram", "offsetStart": 73, "offsetEnd": 81}, "context": "Two architectures have been released: continuous bag-of-words (CBOW) and Skipgram. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.0001518726348876953}, "created": {"value": false, "score": 0.12236428260803223}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9969049096107483}, "created": {"value": false, "score": 0.12236428260803223}, "shared": {"value": false, "score": 3.5762786865234375e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Glove", "normalizedForm": "Glove", "offsetStart": 75, "offsetEnd": 80}, "context": "Dynomant et al. [9] compare mainly Word2Vec (Skipgram, CBOW), FastText and Glove on a specific dataset (health-related documents) written in French.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9969049096107483}, "created": {"value": false, "score": 2.6226043701171875e-06}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9969049096107483}, "created": {"value": false, "score": 0.0004506707191467285}, "shared": {"value": false, "score": 2.384185791015625e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Skipgram", "normalizedForm": "Skipgram", "offsetStart": 77, "offsetEnd": 85}, "context": "The main difference is CBOW uses neighbors to predict the target word, while Skipgram uses the target to predict its neighbors.", "mentionContextAttributes": {"used": {"value": false, "score": 0.000742495059967041}, "created": {"value": false, "score": 1.4424324035644531e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9969049096107483}, "created": {"value": false, "score": 0.12236428260803223}, "shared": {"value": false, "score": 3.5762786865234375e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "CamemBERT", "normalizedForm": "CamemBERT", "offsetStart": 77, "offsetEnd": 86}, "context": "Therefore, we decided to try pre-trained Transformer models for French, like CamemBERT [14] and FlauBERT [15].", "mentionContextAttributes": {"used": {"value": true, "score": 0.7215011715888977}, "created": {"value": false, "score": 0.0007305741310119629}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999487400054932}, "created": {"value": false, "score": 0.0018892288208007812}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "references": [{"label": "[14]", "normalizedForm": "[14]", "refKey": 14, "offsetStart": 14912, "offsetEnd": 14916}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Glove", "normalizedForm": "Glove", "offsetStart": 78, "offsetEnd": 83}, "context": "\ud835\udc41 -grams help to prevent the Out-of-Vocabulary problem that suffer Word2Vec, Glove or classical representations. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.0001621246337890625}, "created": {"value": false, "score": 0.0004506707191467285}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9969049096107483}, "created": {"value": false, "score": 0.0004506707191467285}, "shared": {"value": false, "score": 2.384185791015625e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "FlauBERT", "normalizedForm": "FlauBERT", "offsetStart": 80, "offsetEnd": 88}, "context": "CamemBERT was trained on OSCAR [16], whose size is 138 GB after cleaning, while FlauBERT used 24 corpora from different sources (Wikipedia, books, Common Crawl, etc.), whose overall size is only half as the CamemBERT training data (71 GB).", "mentionContextAttributes": {"used": {"value": true, "score": 0.9998874664306641}, "created": {"value": false, "score": 4.887580871582031e-06}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9998874664306641}, "created": {"value": false, "score": 0.0007305741310119629}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "references": [{"label": "[15]", "normalizedForm": "[15]", "refKey": 15}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "ELMo", "normalizedForm": "ELMo", "offsetStart": 83, "offsetEnd": 87}, "context": "It has been proven that Transformers are faster and more efficient than (Bi)-LSTM (ELMo) or CNN (Word2Vec).", "mentionContextAttributes": {"used": {"value": false, "score": 0.0008714795112609863}, "created": {"value": false, "score": 0.0008404254913330078}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": false, "score": 0.0008714795112609863}, "created": {"value": false, "score": 0.0008404254913330078}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "references": [{"label": "[5]", "normalizedForm": "[5]", "refKey": 5}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "SciBERT", "normalizedForm": "SciBERT", "offsetStart": 85, "offsetEnd": 92}, "context": "Thus, some specific domain models have been trained such as BERTTweet for Twitter or SciBERT for the biological domain.", "mentionContextAttributes": {"used": {"value": false, "score": 0.0009096860885620117}, "created": {"value": false, "score": 0.03327345848083496}, "shared": {"value": false, "score": 7.152557373046875e-07}}, "documentContextAttributes": {"used": {"value": false, "score": 0.0009096860885620117}, "created": {"value": false, "score": 0.03327345848083496}, "shared": {"value": false, "score": 7.152557373046875e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "ELMo", "normalizedForm": "ELMo", "offsetStart": 87, "offsetEnd": 91}, "context": "BERT uses parallel attention layers instead of sequential recurrent neural networks as ELMo does. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.00013911724090576172}, "created": {"value": false, "score": 7.510185241699219e-06}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": false, "score": 0.0008714795112609863}, "created": {"value": false, "score": 0.0008404254913330078}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "references": [{"label": "[5]", "normalizedForm": "[5]", "refKey": 5}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "FastText", "normalizedForm": "FastText", "offsetStart": 88, "offsetEnd": 96}, "context": "Then, we tried non-contextual embedding with Word2Vec (Skip-Gram), Doc2Vec (PV-DM), and FastText. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9986816048622131}, "created": {"value": false, "score": 0.0023130178451538086}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9986816048622131}, "created": {"value": false, "score": 0.006463348865509033}, "shared": {"value": false, "score": 1.7881393432617188e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "FlauBERT", "normalizedForm": "FlauBERT", "offsetStart": 96, "offsetEnd": 104}, "context": "Therefore, we decided to try pre-trained Transformer models for French, like CamemBERT [14] and FlauBERT [15].", "mentionContextAttributes": {"used": {"value": true, "score": 0.7215011715888977}, "created": {"value": false, "score": 0.0007305741310119629}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9998874664306641}, "created": {"value": false, "score": 0.0007305741310119629}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "references": [{"label": "[15]", "normalizedForm": "[15]", "refKey": 15, "offsetStart": 14930, "offsetEnd": 14934}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "CamemBERT", "normalizedForm": "CamemBERT", "offsetStart": 100, "offsetEnd": 109}, "context": "Finally, the models use different strategies for the masking task: FlauBERT masks sub-word, whereas CamemBERT masks the whole word.", "mentionContextAttributes": {"used": {"value": false, "score": 0.0027690529823303223}, "created": {"value": false, "score": 1.2636184692382812e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999487400054932}, "created": {"value": false, "score": 0.0018892288208007812}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "references": [{"label": "[14]", "normalizedForm": "[14]", "refKey": 14}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "CamemBERT", "normalizedForm": "CamemBERT", "offsetStart": 129, "offsetEnd": 138}, "context": "In summary, we found out that, among the models we tested, the classic representation TF-IDF and the most state-of-the-art model CamemBERT are the ones that stand out for the classification task.", "mentionContextAttributes": {"used": {"value": true, "score": 0.99979168176651}, "created": {"value": false, "score": 0.0018892288208007812}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999487400054932}, "created": {"value": false, "score": 0.0018892288208007812}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "references": [{"label": "[14]", "normalizedForm": "[14]", "refKey": 14}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "FastText", "normalizedForm": "FastText", "offsetStart": 140, "offsetEnd": 151}, "context": "As Word2Vec, two architectures have been built Finally, there exist other models similar to Word2Vec, such as Global Vectors (GloVe) [3] or FastText [4]. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.00032520294189453125}, "created": {"value": false, "score": 0.003997504711151123}, "shared": {"value": false, "score": 1.7881393432617188e-06}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9986816048622131}, "created": {"value": false, "score": 0.006463348865509033}, "shared": {"value": false, "score": 1.7881393432617188e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "FastText", "normalizedForm": "FastText", "offsetStart": 197, "offsetEnd": 205}, "context": "The comparison of the text representations for the classifications tasks highlights that classical representations such as TD-IDF have better results than non-contextual word-embbedings (Word2vec, FastText, etc.) or quite similar with state-of-the-art models (CamemBERT).", "mentionContextAttributes": {"used": {"value": false, "score": 0.045422255992889404}, "created": {"value": false, "score": 1.8835067749023438e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9986816048622131}, "created": {"value": false, "score": 0.006463348865509033}, "shared": {"value": false, "score": 1.7881393432617188e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "CamemBERT", "normalizedForm": "CamemBERT", "offsetStart": 207, "offsetEnd": 216}, "context": "CamemBERT was trained on OSCAR [16], whose size is 138 GB after cleaning, while FlauBERT used 24 corpora from different sources (Wikipedia, books, Common Crawl, etc.), whose overall size is only half as the CamemBERT training data (71 GB).", "mentionContextAttributes": {"used": {"value": true, "score": 0.9998874664306641}, "created": {"value": false, "score": 4.887580871582031e-06}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999487400054932}, "created": {"value": false, "score": 0.0018892288208007812}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "references": [{"label": "[14]", "normalizedForm": "[14]", "refKey": 14}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "CamemBERT", "normalizedForm": "CamemBERT", "offsetStart": 260, "offsetEnd": 269}, "context": "The comparison of the text representations for the classifications tasks highlights that classical representations such as TD-IDF have better results than non-contextual word-embbedings (Word2vec, FastText, etc.) or quite similar with state-of-the-art models (CamemBERT).", "mentionContextAttributes": {"used": {"value": false, "score": 0.045422255992889404}, "created": {"value": false, "score": 1.8835067749023438e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999487400054932}, "created": {"value": false, "score": 0.0018892288208007812}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "references": [{"label": "[14]", "normalizedForm": "[14]", "refKey": 14}]}], "references": [{"refKey": 15, "tei": "<biblStruct xml:id=\"b15\">\n\t<analytic>\n\t\t<title level=\"a\" type=\"main\">FlauBERT: Unsupervised language model pre-training for French</title>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">H</forename><surname>Le</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">L</forename><surname>Vial</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">J</forename><surname>Frej</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">V</forename><surname>Segonne</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">M</forename><surname>Coavoux</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">B</forename><surname>Lecouteux</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">A</forename><surname>Allauzen</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">B</forename><surname>Crabb\u00e9</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">L</forename><surname>Besacier</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">D</forename><surname>Schwab</surname></persName>\n\t\t</author>\n\t</analytic>\n\t<monogr>\n\t\t<title level=\"m\">Proceedings of the 12th Language Resources and Evaluation Conference, European Language Resources Association</title>\n\t\t<meeting>the 12th Language Resources and Evaluation Conference, European Language Resources Association</meeting>\n\t\t<imprint>\n\t\t\t<date>2020</date>\n\t\t\t<biblScope unit=\"page\">2490</biblScope>\n\t\t</imprint>\n\t</monogr>\n</biblStruct>\n"}, {"refKey": 14, "tei": "<biblStruct xml:id=\"b14\">\n\t<analytic>\n\t\t<title level=\"a\" type=\"main\">CamemBERT: a Tasty French Language Model</title>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Louis</forename><surname>Martin</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Benjamin</forename><surname>Muller</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Pedro</forename><forename type=\"middle\">Javier</forename><surname>Ortiz Su\u00e1rez</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Yoann</forename><surname>Dupont</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Laurent</forename><surname>Romary</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">\u00c9ric</forename><surname>De La Clergerie</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Djam\u00e9</forename><surname>Seddah</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Beno\u00eet</forename><surname>Sagot</surname></persName>\n\t\t</author>\n\t\t<idno type=\"DOI\">10.18653/v1/2020.acl-main.645</idno>\n\t</analytic>\n\t<monogr>\n\t\t<title level=\"m\">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>\n\t\t<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>\n\t\t<imprint>\n\t\t\t<publisher>Association for Computational Linguistics</publisher>\n\t\t\t<date type=\"published\" when=\"2020\">2020</date>\n\t\t\t<biblScope unit=\"page\">7219</biblScope>\n\t\t</imprint>\n\t</monogr>\n</biblStruct>\n"}, {"refKey": 5, "tei": "<biblStruct xml:id=\"b5\">\n\t<analytic>\n\t\t<title level=\"a\" type=\"main\">Deep Contextualized Word Representations</title>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Matthew</forename><surname>Peters</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Mark</forename><surname>Neumann</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Mohit</forename><surname>Iyyer</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Matt</forename><surname>Gardner</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Christopher</forename><surname>Clark</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Kenton</forename><surname>Lee</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Luke</forename><surname>Zettlemoyer</surname></persName>\n\t\t</author>\n\t\t<idno type=\"DOI\">10.18653/v1/n18-1202</idno>\n\t</analytic>\n\t<monogr>\n\t\t<title level=\"m\">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)</title>\n\t\t<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)</meeting>\n\t\t<imprint>\n\t\t\t<publisher>Association for Computational Linguistics</publisher>\n\t\t\t<date type=\"published\" when=\"2018\">2018</date>\n\t\t</imprint>\n\t</monogr>\n</biblStruct>\n"}], "runtime": 16165, "id": "c4f4ebb45c537045a63a0e27b8a16c4fecde4336", "metadata": {"id": "c4f4ebb45c537045a63a0e27b8a16c4fecde4336"}, "original_file_path": "../../datalake/Samuel/SOFTware-Sync/downloads/xml/hal-04346759.grobid.tei.xml", "file_name": "hal-04346759.grobid.tei.xml"}