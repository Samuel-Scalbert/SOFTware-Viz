{"application": "software-mentions", "version": "0.8.0", "date": "2024-10-07T12:42+0000", "md5": "B55DEA777E21E10C3A1BA49C9A77FA70", "mentions": [{"type": "software", "software-type": "software", "software-name": {"rawForm": "fastText", "normalizedForm": "fastText", "offsetStart": 0, "offsetEnd": 8}, "context": "fastText embedding: It is an extension of Mikolov's embedding [8].", "mentionContextAttributes": {"used": {"value": false, "score": 0.0004744529724121094}, "created": {"value": false, "score": 4.6253204345703125e-05}, "shared": {"value": false, "score": 5.960464477539062e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9998464584350586}, "created": {"value": false, "score": 0.006826341152191162}, "shared": {"value": false, "score": 1.7881393432617188e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "fastText", "normalizedForm": "fastText", "offsetStart": 0, "offsetEnd": 8}, "context": "fastText model: We use pre-trained fastText embedding model and apply this model to generate one embedding for each word of a given comment.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9616010785102844}, "created": {"value": false, "score": 0.006826341152191162}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9998464584350586}, "created": {"value": false, "score": 0.006826341152191162}, "shared": {"value": false, "score": 1.7881393432617188e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "fastText", "normalizedForm": "fastText", "offsetStart": 4, "offsetEnd": 12}, "context": "The fastText approach is based on the skipgram model, where each word is represented as a bag of character n-grams [16], [17].", "mentionContextAttributes": {"used": {"value": false, "score": 0.00029921531677246094}, "created": {"value": false, "score": 2.6106834411621094e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9998464584350586}, "created": {"value": false, "score": 0.006826341152191162}, "shared": {"value": false, "score": 1.7881393432617188e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "fastText", "normalizedForm": "fastText", "offsetStart": 10, "offsetEnd": 18}, "context": "For both, fastText and BERT embeddings, we decided to remove the numbers and all the special characters except '!' , '?', ',', '.' and apostrophe.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9625515341758728}, "created": {"value": false, "score": 0.0008650422096252441}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9998464584350586}, "created": {"value": false, "score": 0.006826341152191162}, "shared": {"value": false, "score": 1.7881393432617188e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "fastText", "normalizedForm": "fastText", "offsetStart": 29, "offsetEnd": 37}, "context": "Unlike Mikolov's embeddings, fastText is able to provide an embedding for misspelled word, rare words or words that were not present in the training corpus, because fastText uses character n-gram word tokenization.", "mentionContextAttributes": {"used": {"value": false, "score": 0.0006704330444335938}, "created": {"value": false, "score": 0.00021535158157348633}, "shared": {"value": false, "score": 4.76837158203125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9998464584350586}, "created": {"value": false, "score": 0.006826341152191162}, "shared": {"value": false, "score": 1.7881393432617188e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "fastText", "normalizedForm": "fastText", "offsetStart": 29, "offsetEnd": 37}, "context": "In feature-based approaches, fastText and BERT embeddings are used as input features to CNN and Bi-LSTM classifiers.", "mentionContextAttributes": {"used": {"value": false, "score": 0.00025087594985961914}, "created": {"value": false, "score": 4.8160552978515625e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9998464584350586}, "created": {"value": false, "score": 0.006826341152191162}, "shared": {"value": false, "score": 1.7881393432617188e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "fastText", "normalizedForm": "fastText", "offsetStart": 31, "offsetEnd": 39}, "context": "For example, Facebook provided fastText model, Google provided several BERT models for different languages.", "mentionContextAttributes": {"used": {"value": false, "score": 0.13569539785385132}, "created": {"value": false, "score": 0.00048553943634033203}, "shared": {"value": false, "score": 5.960464477539062e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9998464584350586}, "created": {"value": false, "score": 0.006826341152191162}, "shared": {"value": false, "score": 1.7881393432617188e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "fastText", "normalizedForm": "fastText", "offsetStart": 35, "offsetEnd": 43}, "context": "fastText model: We use pre-trained fastText embedding model and apply this model to generate one embedding for each word of a given comment.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9616010785102844}, "created": {"value": false, "score": 0.006826341152191162}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9998464584350586}, "created": {"value": false, "score": 0.006826341152191162}, "shared": {"value": false, "score": 1.7881393432617188e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "fastText", "normalizedForm": "fastText", "offsetStart": 36, "offsetEnd": 44}, "context": "The obtained embeddings from either fastText or BERT models are then used as input to a DNN classifier.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9998464584350586}, "created": {"value": false, "score": 1.430511474609375e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9998464584350586}, "created": {"value": false, "score": 0.006826341152191162}, "shared": {"value": false, "score": 1.7881393432617188e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "fastText", "normalizedForm": "fastText", "offsetStart": 48, "offsetEnd": 56}, "context": "Thanks to the bag of character n-grams model of fastText, every word in a given comment will have an embedding, even out-of-vocabulary and rare words.", "mentionContextAttributes": {"used": {"value": false, "score": 7.617473602294922e-05}, "created": {"value": false, "score": 0.0002994537353515625}, "shared": {"value": false, "score": 1.7881393432617188e-06}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9998464584350586}, "created": {"value": false, "score": 0.006826341152191162}, "shared": {"value": false, "score": 1.7881393432617188e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "fastText", "normalizedForm": "fastText", "offsetStart": 50, "offsetEnd": 58}, "context": "For feature-based approaches, we used pre-trained fastText and BERT models to obtain the sequence of embeddings for a given comment.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9992179870605469}, "created": {"value": false, "score": 0.005617380142211914}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9998464584350586}, "created": {"value": false, "score": 0.006826341152191162}, "shared": {"value": false, "score": 1.7881393432617188e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "fastText", "normalizedForm": "fastText", "offsetStart": 56, "offsetEnd": 64}, "context": "From table IV and table V, it can be observed that both fastText and BERT embeddings provide nearly the same results.", "mentionContextAttributes": {"used": {"value": false, "score": 0.001777946949005127}, "created": {"value": false, "score": 1.1801719665527344e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9998464584350586}, "created": {"value": false, "score": 0.006826341152191162}, "shared": {"value": false, "score": 1.7881393432617188e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "fastText", "normalizedForm": "fastText", "offsetStart": 64, "offsetEnd": 72}, "context": "Thus, it is possible to have embeddings for rare words, like in fastText.", "mentionContextAttributes": {"used": {"value": false, "score": 6.42538070678711e-05}, "created": {"value": false, "score": 0.0002785325050354004}, "shared": {"value": false, "score": 5.960464477539062e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9998464584350586}, "created": {"value": false, "score": 0.006826341152191162}, "shared": {"value": false, "score": 1.7881393432617188e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "CrowdFlower", "normalizedForm": "CrowdFlower", "offsetStart": 64, "offsetEnd": 75}, "context": "The data set contains 24883 tweets and annotations performed by CrowdFlower. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.7826975584030151}, "created": {"value": false, "score": 8.58306884765625e-06}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.7826975584030151}, "created": {"value": false, "score": 8.58306884765625e-06}, "shared": {"value": false, "score": 2.384185791015625e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "fastText", "normalizedForm": "fastText", "offsetStart": 80, "offsetEnd": 88}, "context": "We perform toxic speech classification using two powerful word representations: fastText and BERT embeddings.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9925186634063721}, "created": {"value": false, "score": 0.000331878662109375}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9998464584350586}, "created": {"value": false, "score": 0.006826341152191162}, "shared": {"value": false, "score": 1.7881393432617188e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "fastText", "normalizedForm": "fastText", "offsetStart": 114, "offsetEnd": 122}, "context": "Table IV gives the macro average F1 results for binary classification task using Bi-LSTM and CNN classifiers with fastText and BERT embeddings as input features.", "mentionContextAttributes": {"used": {"value": true, "score": 0.7523061633110046}, "created": {"value": false, "score": 4.649162292480469e-06}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9998464584350586}, "created": {"value": false, "score": 0.006826341152191162}, "shared": {"value": false, "score": 1.7881393432617188e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "fastText", "normalizedForm": "fastText", "offsetStart": 136, "offsetEnd": 144}, "context": "First, each comment is represented as a sequence of words or word-pieces and for each word or wordpiece, an embedding is computed using fastText or BERT.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9915515184402466}, "created": {"value": false, "score": 1.4066696166992188e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9998464584350586}, "created": {"value": false, "score": 0.006826341152191162}, "shared": {"value": false, "score": 1.7881393432617188e-06}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "fastText", "normalizedForm": "fastText", "offsetStart": 165, "offsetEnd": 173}, "context": "Unlike Mikolov's embeddings, fastText is able to provide an embedding for misspelled word, rare words or words that were not present in the training corpus, because fastText uses character n-gram word tokenization.", "mentionContextAttributes": {"used": {"value": false, "score": 0.0006704330444335938}, "created": {"value": false, "score": 0.00021535158157348633}, "shared": {"value": false, "score": 4.76837158203125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9998464584350586}, "created": {"value": false, "score": 0.006826341152191162}, "shared": {"value": false, "score": 1.7881393432617188e-06}}}], "references": [], "runtime": 47992, "id": "f50e7c9cb7797494a918c097c5a85e272467ac5c", "metadata": {"id": "f50e7c9cb7797494a918c097c5a85e272467ac5c"}, "original_file_path": "../../datalake/Samuel/SOFTware-Sync/downloads/xml/hal-02448197.grobid.tei.xml", "file_name": "hal-02448197.grobid.tei.xml"}