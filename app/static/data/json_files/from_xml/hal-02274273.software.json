{"application": "software-mentions", "version": "0.8.0", "date": "2024-10-07T11:36+0000", "md5": "026250DAECEAD51038B3A8D6235119B6", "mentions": [{"type": "software", "software-type": "software", "software-name": {"rawForm": "Adam", "normalizedForm": "Adam", "offsetStart": 33, "offsetEnd": 37}, "context": "All our models are trained using Adam [30].", "mentionContextAttributes": {"used": {"value": true, "score": 0.9244284629821777}, "created": {"value": false, "score": 0.0005325675010681152}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.999584972858429}, "created": {"value": false, "score": 0.0005325675010681152}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "references": [{"label": "[30]", "normalizedForm": "[30]", "refKey": 30, "offsetStart": 23735, "offsetEnd": 23739}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Adam", "normalizedForm": "Adam", "offsetStart": 34, "offsetEnd": 38}, "context": "For the CNN encoderdecoder we use Adam's default parameters and stop the training after one epoch. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.999584972858429}, "created": {"value": false, "score": 1.1801719665527344e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.999584972858429}, "created": {"value": false, "score": 0.0005325675010681152}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "references": [{"label": "[30]", "normalizedForm": "[30]", "refKey": 30}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Imagenet", "normalizedForm": "Imagenet", "offsetStart": 37, "offsetEnd": 45}, "context": "We use a resnet-18 [3] pretrained on Imagenet [27] to extract features from input frames (f i1 , f i2 ).", "mentionContextAttributes": {"used": {"value": true, "score": 0.9994333386421204}, "created": {"value": false, "score": 2.9802322387695312e-06}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9994333386421204}, "created": {"value": false, "score": 7.212162017822266e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "references": [{"label": "[27]", "normalizedForm": "[27]", "refKey": 27, "offsetStart": 22271, "offsetEnd": 22275}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Imagenet", "normalizedForm": "Imagenet", "offsetStart": 40, "offsetEnd": 48}, "context": "This DNN uses a resnet-18 pretrained on Imagenet to extract features from the image, from which a deconvolution network is trained to predict the semantic mask (distinguishing three types of entities: background, occluders and objects). ", "mentionContextAttributes": {"used": {"value": false, "score": 0.24233591556549072}, "created": {"value": false, "score": 7.212162017822266e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9994333386421204}, "created": {"value": false, "score": 7.212162017822266e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "references": [{"label": "[27]", "normalizedForm": "[27]", "refKey": 27}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "DeepMind", "normalizedForm": "DeepMind", "offsetStart": 43, "offsetEnd": 51}, "context": "There is also recent dataset proposed by a DeepMind team [44]. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.00036275386810302734}, "created": {"value": false, "score": 0.08318358659744263}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": false, "score": 0.00036275386810302734}, "created": {"value": false, "score": 0.08318358659744263}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "references": [{"label": "[44]", "normalizedForm": "[44]", "refKey": 44, "offsetStart": 36435, "offsetEnd": 36439}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "CodaLab", "normalizedForm": "CodaLab", "offsetStart": 58, "offsetEnd": 65}, "url": {"rawForm": "www.intphys.com", "normalizedForm": "www.intphys.com"}, "context": "The evaluation is done upon submission of these scores in CodaLab, and the results are automatically presented in a leaderboard. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9784969091415405}, "created": {"value": false, "score": 8.678436279296875e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9784969091415405}, "created": {"value": false, "score": 0.00022214651107788086}, "shared": {"value": false, "score": 1.8358230590820312e-05}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "CodaLab", "normalizedForm": "CodaLab", "offsetStart": 95, "offsetEnd": 102}, "url": {"rawForm": "www.intphys.com", "normalizedForm": "www.intphys.com", "offsetStart": 108, "offsetEnd": 123}, "context": "For evaluating on the test set, participants are invited to submit their system and results on CodaLab (see www.intphys.com) ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9663670063018799}, "created": {"value": false, "score": 0.00022214651107788086}, "shared": {"value": false, "score": 1.8358230590820312e-05}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9784969091415405}, "created": {"value": false, "score": 0.00022214651107788086}, "shared": {"value": false, "score": 1.8358230590820312e-05}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Mechanical Turk", "normalizedForm": "Mechanical Turk", "offsetStart": 224, "offsetEnd": 239}, "publisher": {"rawForm": "Amazon", "normalizedForm": "Amazon", "offsetStart": 217, "offsetEnd": 223}, "context": "To give a second reference to evaluate physical understanding in models, and provide a good description of human performance on this benchmark, we presented the 3600 videos from each block to human participants using Amazon Mechanical Turk. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9999716281890869}, "created": {"value": false, "score": 0.04522407054901123}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999716281890869}, "created": {"value": false, "score": 0.04522407054901123}, "shared": {"value": false, "score": 1.1920928955078125e-07}}}], "references": [{"refKey": 30, "tei": "<biblStruct xml:id=\"b30\">\n\t<analytic>\n\t\t<title level=\"a\" type=\"main\">Adam: A method for stochastic optimization</title>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">D</forename><forename type=\"middle\">P</forename><surname>Kingma</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">J</forename><surname>Ba</surname></persName>\n\t\t</author>\n\t\t<idno>abs/1412.6980</idno>\n\t</analytic>\n\t<monogr>\n\t\t<title level=\"j\">CoRR</title>\n\t\t<imprint>\n\t\t\t<date>2014</date>\n\t\t</imprint>\n\t</monogr>\n</biblStruct>\n"}, {"refKey": 27, "tei": "<biblStruct xml:id=\"b27\">\n\t<analytic>\n\t\t<title level=\"a\" type=\"main\">Imagenet large scale visual recognition challenge</title>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">O</forename><surname>Russakovsky</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">J</forename><surname>Deng</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">H</forename><surname>Su</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">J</forename><surname>Krause</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">S</forename><surname>Satheesh</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">S</forename><surname>Ma</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Z</forename><surname>Huang</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">A</forename><surname>Karpathy</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">A</forename><surname>Khosla</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">M</forename><surname>Bernstein</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">A</forename><forename type=\"middle\">C</forename><surname>Berg</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">L</forename><surname>Fei-Fei</surname></persName>\n\t\t</author>\n\t\t<idno type=\"DOI\">10.1007/s11263-015-0816-y</idno>\n\t</analytic>\n\t<monogr>\n\t\t<title level=\"j\">International Journal of Computer Vision</title>\n\t\t<imprint>\n\t\t\t<biblScope unit=\"volume\">115</biblScope>\n\t\t\t<biblScope unit=\"issue\">3</biblScope>\n\t\t\t<biblScope unit=\"page\">252</biblScope>\n\t\t\t<date>2015</date>\n\t\t</imprint>\n\t</monogr>\n</biblStruct>\n"}, {"refKey": 44, "tei": "<biblStruct xml:id=\"b44\">\n\t<analytic>\n\t\t<title level=\"a\" type=\"main\">Probing physics knowledge using tools from developmental psychology</title>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">L</forename><surname>Piloto</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">A</forename><surname>Weinstein</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">D</forename><surname>Tb</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">A</forename><surname>Ahuja</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">M</forename><surname>Mirza</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">G</forename><surname>Wayne</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">D</forename><surname>Amos</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">C</forename><surname>Hung</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">M</forename><forename type=\"middle\">M</forename><surname>Botvinick</surname></persName>\n\t\t</author>\n\t\t<idno>abs/1804.01128</idno>\n\t</analytic>\n\t<monogr>\n\t\t<title level=\"j\">CoRR</title>\n\t\t<imprint>\n\t\t\t<date>2018</date>\n\t\t</imprint>\n\t</monogr>\n</biblStruct>\n"}], "runtime": 31116, "id": "29d9f9b0090d1ae1da06b8ac97ce14101997e410", "metadata": {"id": "29d9f9b0090d1ae1da06b8ac97ce14101997e410"}, "original_file_path": "../../datalake/Samuel/SOFTware-Sync/downloads/xml/hal-02274273.grobid.tei.xml", "file_name": "hal-02274273.grobid.tei.xml"}