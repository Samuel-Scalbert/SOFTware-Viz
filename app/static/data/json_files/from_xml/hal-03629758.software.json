{"application": "software-mentions", "version": "0.8.0", "date": "2024-10-07T11:57+0000", "md5": "C070F1CA9E9DE65DE96882D920917756", "mentions": [{"type": "software", "software-type": "software", "software-name": {"rawForm": "Wav2", "normalizedForm": "Wav2", "offsetStart": 0, "offsetEnd": 4}, "version": {"rawForm": "2.0", "normalizedForm": "2.0"}, "context": "Wav2Vec2.0:", "mentionContextAttributes": {"used": {"value": true, "score": 0.5086859464645386}, "created": {"value": false, "score": 1.0609626770019531e-05}, "shared": {"value": false, "score": 2.2649765014648438e-06}}, "documentContextAttributes": {"used": {"value": true, "score": 0.999994158744812}, "created": {"value": true, "score": 0.9561930298805237}, "shared": {"value": false, "score": 2.2649765014648438e-06}}, "references": [{"label": "(Paszke et al. 2019)", "normalizedForm": "Paszke et al. 2019", "refKey": 0}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Wav2", "normalizedForm": "Wav2", "offsetStart": 0, "offsetEnd": 4}, "version": {"rawForm": "2.0", "normalizedForm": "2.0"}, "context": "Wav2Vec2.0 contextual embeddings: Table 2. shows the results with the last but two-layer of contextual transformer block C of Wav2Vec2.0", "mentionContextAttributes": {"used": {"value": true, "score": 0.9999674558639526}, "created": {"value": false, "score": 6.794929504394531e-06}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.999994158744812}, "created": {"value": true, "score": 0.9561930298805237}, "shared": {"value": false, "score": 2.2649765014648438e-06}}, "references": [{"label": "(Paszke et al. 2019)", "normalizedForm": "Paszke et al. 2019", "refKey": 0}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Wav2", "normalizedForm": "Wav2", "offsetStart": 4, "offsetEnd": 8}, "version": {"rawForm": "2.0", "normalizedForm": "2.0"}, "context": "The Wav2Vec2.0 showed remarkable improvement in ASR (Baevski et al. 2020), emotion detection (Pepino et al. 2021).", "mentionContextAttributes": {"used": {"value": true, "score": 0.9996102452278137}, "created": {"value": false, "score": 1.2874603271484375e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.999994158744812}, "created": {"value": true, "score": 0.9561930298805237}, "shared": {"value": false, "score": 2.2649765014648438e-06}}, "references": [{"label": "(Paszke et al. 2019)", "normalizedForm": "Paszke et al. 2019", "refKey": 0}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "WaveVec", "normalizedForm": "WaveVec", "offsetStart": 4, "offsetEnd": 11}, "context": "The WaveVec2.0 model (Baevski et al. 2020) is a self-supervised representation learning framework of raw audio, and is comprised of three modules including feature encoder f : X \u2192 Z, contextual block transformer g : Z \u2192 C and quantization block Z \u2192 Q as depicted in Fig. 2. ", "mentionContextAttributes": {"used": {"value": false, "score": 0.0015883445739746094}, "created": {"value": false, "score": 0.00020056962966918945}, "shared": {"value": false, "score": 4.76837158203125e-07}}, "documentContextAttributes": {"used": {"value": false, "score": 0.0015883445739746094}, "created": {"value": false, "score": 0.00020056962966918945}, "shared": {"value": false, "score": 4.76837158203125e-07}}, "references": [{"label": "(Baevski et al. 2020", "normalizedForm": "(Baevski et al. 2020", "refKey": 3, "offsetStart": 7984, "offsetEnd": 8004}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Wav2", "normalizedForm": "Wav2", "offsetStart": 6, "offsetEnd": 10}, "version": {"rawForm": "2.0", "normalizedForm": "2.0"}, "context": "Using Wav2Vec2.0", "mentionContextAttributes": {"used": {"value": true, "score": 0.999994158744812}, "created": {"value": false, "score": 2.6226043701171875e-06}, "shared": {"value": false, "score": 9.5367431640625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.999994158744812}, "created": {"value": true, "score": 0.9561930298805237}, "shared": {"value": false, "score": 2.2649765014648438e-06}}, "references": [{"label": "(Paszke et al. 2019)", "normalizedForm": "Paszke et al. 2019", "refKey": 0}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "SpeechBrain", "normalizedForm": "SpeechBrain", "offsetStart": 7, "offsetEnd": 18}, "context": "We use SpeechBrain toolkit (Ravanelli et al. 2021) for the extraction of 192-dimensional speaker embeddings. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9988728165626526}, "created": {"value": false, "score": 0.0029771924018859863}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9988728165626526}, "created": {"value": false, "score": 0.0029771924018859863}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "references": [{"label": "(Ravanelli et al. 2021)", "normalizedForm": "Ravanelli et al. 2021", "refKey": 27, "offsetStart": 12113, "offsetEnd": 12136}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Wav2", "normalizedForm": "Wav2", "offsetStart": 14, "offsetEnd": 18}, "version": {"rawForm": "2.0", "normalizedForm": "2.0"}, "context": "We found that Wav2Vec2.0-based", "mentionContextAttributes": {"used": {"value": true, "score": 0.9999841451644897}, "created": {"value": false, "score": 2.1457672119140625e-06}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.999994158744812}, "created": {"value": true, "score": 0.9561930298805237}, "shared": {"value": false, "score": 2.2649765014648438e-06}}, "references": [{"label": "(Paszke et al. 2019)", "normalizedForm": "Paszke et al. 2019", "refKey": 0}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Wav2", "normalizedForm": "Wav2", "offsetStart": 18, "offsetEnd": 22}, "version": {"rawForm": "2.0", "normalizedForm": "2.0"}, "context": "Each layer of the Wav2Vec2.0", "mentionContextAttributes": {"used": {"value": true, "score": 0.999925971031189}, "created": {"value": false, "score": 1.8596649169921875e-05}, "shared": {"value": false, "score": 4.76837158203125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.999994158744812}, "created": {"value": true, "score": 0.9561930298805237}, "shared": {"value": false, "score": 2.2649765014648438e-06}}, "references": [{"label": "(Paszke et al. 2019)", "normalizedForm": "Paszke et al. 2019", "refKey": 0}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "PyTorch", "normalizedForm": "PyTorch", "offsetStart": 31, "offsetEnd": 38}, "context": "We extract embeddings from the PyTorch version of Wav2Vec2.0 (Paszke et al. 2019).", "mentionContextAttributes": {"used": {"value": true, "score": 0.9962214231491089}, "created": {"value": false, "score": 0.0024651288986206055}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9977144002914429}, "created": {"value": false, "score": 0.0024651288986206055}, "shared": {"value": false, "score": 7.152557373046875e-07}}, "references": [{"label": "(Paszke et al. 2019)", "normalizedForm": "Paszke et al. 2019", "refKey": 0}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "LibriSpeech", "normalizedForm": "LibriSpeech", "offsetStart": 31, "offsetEnd": 42}, "context": "models trained on VoxCeleb and LibriSpeech datasets respectively.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9999258518218994}, "created": {"value": false, "score": 2.110004425048828e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999258518218994}, "created": {"value": false, "score": 0.008199036121368408}, "shared": {"value": false, "score": 2.384185791015625e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Wav2", "normalizedForm": "Wav2", "offsetStart": 42, "offsetEnd": 46}, "version": {"rawForm": "2.0", "normalizedForm": "2.0"}, "context": "Moreover, the prior application of LDA on Wav2Vec2.0", "mentionContextAttributes": {"used": {"value": true, "score": 0.9998830556869507}, "created": {"value": false, "score": 4.315376281738281e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.999994158744812}, "created": {"value": true, "score": 0.9561930298805237}, "shared": {"value": false, "score": 2.2649765014648438e-06}}, "references": [{"label": "(Paszke et al. 2019)", "normalizedForm": "Paszke et al. 2019", "refKey": 0}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Wav2", "normalizedForm": "Wav2", "offsetStart": 43, "offsetEnd": 47}, "version": {"rawForm": "2.0", "normalizedForm": "2.0"}, "context": "Moreover, concatenating the ECAPA-TDNN and Wav2Vec2.0", "mentionContextAttributes": {"used": {"value": true, "score": 0.9947885274887085}, "created": {"value": false, "score": 3.6954879760742188e-06}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.999994158744812}, "created": {"value": true, "score": 0.9561930298805237}, "shared": {"value": false, "score": 2.2649765014648438e-06}}, "references": [{"label": "(Paszke et al. 2019)", "normalizedForm": "Paszke et al. 2019", "refKey": 0}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Wav2", "normalizedForm": "Wav2", "offsetStart": 48, "offsetEnd": 52}, "version": {"rawForm": "2.0", "normalizedForm": "2.0"}, "context": "Fusion: In addition, we fuse the ECAPA-TDNN and Wav2Vec2.0", "mentionContextAttributes": {"used": {"value": true, "score": 0.9968855977058411}, "created": {"value": false, "score": 0.0008834004402160645}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.999994158744812}, "created": {"value": true, "score": 0.9561930298805237}, "shared": {"value": false, "score": 2.2649765014648438e-06}}, "references": [{"label": "(Paszke et al. 2019)", "normalizedForm": "Paszke et al. 2019", "refKey": 0}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Wav2", "normalizedForm": "Wav2", "offsetStart": 50, "offsetEnd": 54}, "version": {"rawForm": "2.0", "normalizedForm": "2.0", "offsetStart": 57, "offsetEnd": 80}, "context": "We extract embeddings from the PyTorch version of Wav2Vec2.0 (Paszke et al. 2019).", "mentionContextAttributes": {"used": {"value": true, "score": 0.9962214231491089}, "created": {"value": false, "score": 0.0024651288986206055}, "shared": {"value": false, "score": 3.5762786865234375e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.999994158744812}, "created": {"value": true, "score": 0.9561930298805237}, "shared": {"value": false, "score": 2.2649765014648438e-06}}, "references": [{"label": "(Paszke et al. 2019)", "normalizedForm": "Paszke et al. 2019", "refKey": 0, "offsetStart": 13461, "offsetEnd": 13481}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Wav2", "normalizedForm": "Wav2", "offsetStart": 52, "offsetEnd": 56}, "version": {"rawForm": "2.0", "normalizedForm": "2.0"}, "context": "By fine-tuning towards ASR, it is possible that the Wav2Vec2.0", "mentionContextAttributes": {"used": {"value": false, "score": 0.108181893825531}, "created": {"value": false, "score": 6.139278411865234e-05}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.999994158744812}, "created": {"value": true, "score": 0.9561930298805237}, "shared": {"value": false, "score": 2.2649765014648438e-06}}, "references": [{"label": "(Paszke et al. 2019)", "normalizedForm": "Paszke et al. 2019", "refKey": 0}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Wav2", "normalizedForm": "Wav2", "offsetStart": 54, "offsetEnd": 58}, "version": {"rawForm": "2.0", "normalizedForm": "2.0"}, "context": "While computing the final score p from ECAPA-TDNN and Wav2Vec2.0", "mentionContextAttributes": {"used": {"value": true, "score": 0.9999604225158691}, "created": {"value": false, "score": 2.6226043701171875e-06}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.999994158744812}, "created": {"value": true, "score": 0.9561930298805237}, "shared": {"value": false, "score": 2.2649765014648438e-06}}, "references": [{"label": "(Paszke et al. 2019)", "normalizedForm": "Paszke et al. 2019", "refKey": 0}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "PyTorch", "normalizedForm": "PyTorch", "offsetStart": 54, "offsetEnd": 61}, "context": "For implementing the proposed pipeline for NN, we use PyTorch library (Paszke et al. 2019), and for LDA, KNN, and NBC, we have used the Scikit-learn (Pedregosa et al. 2011) toolkit. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9977144002914429}, "created": {"value": false, "score": 0.0004991292953491211}, "shared": {"value": false, "score": 7.152557373046875e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9977144002914429}, "created": {"value": false, "score": 0.0024651288986206055}, "shared": {"value": false, "score": 7.152557373046875e-07}}, "references": [{"label": "(Paszke et al. 2019)", "normalizedForm": "Paszke et al. 2019", "refKey": 0, "offsetStart": 14906, "offsetEnd": 14926}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Wav2", "normalizedForm": "Wav2", "offsetStart": 57, "offsetEnd": 61}, "version": {"rawForm": "2.0", "normalizedForm": "2.0"}, "context": "We investigated ECAPA-TDNN-based speaker recognition and Wav2Vec2.0-based", "mentionContextAttributes": {"used": {"value": true, "score": 0.9999666213989258}, "created": {"value": false, "score": 5.841255187988281e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.999994158744812}, "created": {"value": true, "score": 0.9561930298805237}, "shared": {"value": false, "score": 2.2649765014648438e-06}}, "references": [{"label": "(Paszke et al. 2019)", "normalizedForm": "Paszke et al. 2019", "refKey": 0}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Wav2", "normalizedForm": "Wav2", "offsetStart": 74, "offsetEnd": 78}, "version": {"rawForm": "2.0", "normalizedForm": "2.0"}, "context": "-We also provide an analysis of the impact of using different layers from Wav2Vec2.0 and their concatenation in SD and also investigate the impact of combining information from ECAPA-TDNN and Wav2Vec2.0", "mentionContextAttributes": {"used": {"value": true, "score": 0.8269919753074646}, "created": {"value": false, "score": 1.7404556274414062e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.999994158744812}, "created": {"value": true, "score": 0.9561930298805237}, "shared": {"value": false, "score": 2.2649765014648438e-06}}, "references": [{"label": "(Paszke et al. 2019)", "normalizedForm": "Paszke et al. 2019", "refKey": 0}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "LibriSpeech", "normalizedForm": "LibriSpeech", "offsetStart": 74, "offsetEnd": 85}, "context": "speech recognition as two separate pre-text tasks trained on VoxCeleb and LibriSpeech, respectively. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.999407172203064}, "created": {"value": false, "score": 0.0001538991928100586}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999258518218994}, "created": {"value": false, "score": 0.008199036121368408}, "shared": {"value": false, "score": 2.384185791015625e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Wav2", "normalizedForm": "Wav2", "offsetStart": 77, "offsetEnd": 81}, "version": {"rawForm": "2.0", "normalizedForm": "2.0"}, "context": "From the results, we can observe that for SD, the contextual embeddings from Wav2Vec2.0", "mentionContextAttributes": {"used": {"value": true, "score": 0.9999020099639893}, "created": {"value": false, "score": 1.5139579772949219e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.999994158744812}, "created": {"value": true, "score": 0.9561930298805237}, "shared": {"value": false, "score": 2.2649765014648438e-06}}, "references": [{"label": "(Paszke et al. 2019)", "normalizedForm": "Paszke et al. 2019", "refKey": 0}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Wav2", "normalizedForm": "Wav2", "offsetStart": 79, "offsetEnd": 83}, "version": {"rawForm": "2.0", "normalizedForm": "2.0"}, "context": "In addition, the results show that the contextual layers from L6 to L12 of the Wav2Vec2.0", "mentionContextAttributes": {"used": {"value": true, "score": 0.9999815225601196}, "created": {"value": false, "score": 1.0848045349121094e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.999994158744812}, "created": {"value": true, "score": 0.9561930298805237}, "shared": {"value": false, "score": 2.2649765014648438e-06}}, "references": [{"label": "(Paszke et al. 2019)", "normalizedForm": "Paszke et al. 2019", "refKey": 0}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Wav2", "normalizedForm": "Wav2", "offsetStart": 87, "offsetEnd": 91}, "version": {"rawForm": "2.0", "normalizedForm": "2.0"}, "context": "We further improved SD performance by combining embedding from different layers of the Wav2Vec2.0", "mentionContextAttributes": {"used": {"value": true, "score": 0.9997552037239075}, "created": {"value": false, "score": 0.0031015872955322266}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.999994158744812}, "created": {"value": true, "score": 0.9561930298805237}, "shared": {"value": false, "score": 2.2649765014648438e-06}}, "references": [{"label": "(Paszke et al. 2019)", "normalizedForm": "Paszke et al. 2019", "refKey": 0}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Wav2", "normalizedForm": "Wav2", "offsetStart": 90, "offsetEnd": 94}, "version": {"rawForm": "2.0", "normalizedForm": "2.0"}, "context": "Finally, we have shown that combining two embeddings and concatenating multiple layers of Wav2Vec2.0", "mentionContextAttributes": {"used": {"value": false, "score": 0.11267209053039551}, "created": {"value": false, "score": 0.03883552551269531}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.999994158744812}, "created": {"value": true, "score": 0.9561930298805237}, "shared": {"value": false, "score": 2.2649765014648438e-06}}, "references": [{"label": "(Paszke et al. 2019)", "normalizedForm": "Paszke et al. 2019", "refKey": 0}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Wav2", "normalizedForm": "Wav2", "offsetStart": 126, "offsetEnd": 130}, "version": {"rawForm": "2.0", "normalizedForm": "2.0"}, "context": "Wav2Vec2.0 contextual embeddings: Table 2. shows the results with the last but two-layer of contextual transformer block C of Wav2Vec2.0", "mentionContextAttributes": {"used": {"value": true, "score": 0.9999674558639526}, "created": {"value": false, "score": 6.794929504394531e-06}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.999994158744812}, "created": {"value": true, "score": 0.9561930298805237}, "shared": {"value": false, "score": 2.2649765014648438e-06}}, "references": [{"label": "(Paszke et al. 2019)", "normalizedForm": "Paszke et al. 2019", "refKey": 0}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Scikit-learn", "normalizedForm": "Scikit-learn", "offsetStart": 136, "offsetEnd": 148}, "context": "For implementing the proposed pipeline for NN, we use PyTorch library (Paszke et al. 2019), and for LDA, KNN, and NBC, we have used the Scikit-learn (Pedregosa et al. 2011) toolkit. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9977144002914429}, "created": {"value": false, "score": 0.0004991292953491211}, "shared": {"value": false, "score": 7.152557373046875e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9977144002914429}, "created": {"value": false, "score": 0.0004991292953491211}, "shared": {"value": false, "score": 7.152557373046875e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Wav2", "normalizedForm": "Wav2", "offsetStart": 159, "offsetEnd": 163}, "version": {"rawForm": "2.0", "normalizedForm": "2.0"}, "context": "-We explore the use of speaker embeddings extracted from emphasized channel attention, propagation and aggregation (ECAPA)-TDNN (Desplanques et al. 2020), and Wav2Vec2.0", "mentionContextAttributes": {"used": {"value": false, "score": 0.005730748176574707}, "created": {"value": false, "score": 0.09679895639419556}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.999994158744812}, "created": {"value": true, "score": 0.9561930298805237}, "shared": {"value": false, "score": 2.2649765014648438e-06}}, "references": [{"label": "(Paszke et al. 2019)", "normalizedForm": "Paszke et al. 2019", "refKey": 0}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Wav2", "normalizedForm": "Wav2", "offsetStart": 165, "offsetEnd": 169}, "version": {"rawForm": "2.0", "normalizedForm": "2.0"}, "context": "In particular, we explore audio representations obtained using emphasized channel attention, propagation, and aggregation time delay neural network (ECAPA-TDNN) and Wav2Vec2.0", "mentionContextAttributes": {"used": {"value": false, "score": 0.008128762245178223}, "created": {"value": true, "score": 0.7742180824279785}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.999994158744812}, "created": {"value": true, "score": 0.9561930298805237}, "shared": {"value": false, "score": 2.2649765014648438e-06}}, "references": [{"label": "(Paszke et al. 2019)", "normalizedForm": "Paszke et al. 2019", "refKey": 0}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Adam", "normalizedForm": "Adam", "offsetStart": 168, "offsetEnd": 172}, "context": "The model is fed with 80 dimensional mean normalized log Mel-filterbank energy features and is trained on a large Voxceleb dataset with AAM-softmax loss function using Adam optimizer having a cycling learning rate between 1e-8 and 1e-3. ", "mentionContextAttributes": {"used": {"value": true, "score": 0.9944966435432434}, "created": {"value": false, "score": 3.6597251892089844e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999732971191406}, "created": {"value": false, "score": 3.6597251892089844e-05}, "shared": {"value": false, "score": 5.960464477539062e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Wav2", "normalizedForm": "Wav2", "offsetStart": 182, "offsetEnd": 186}, "version": {"rawForm": "2.0", "normalizedForm": "2.0"}, "context": "In this work, use the Minkowski metric distance from eq. ( 3) with p = 2 (Euclidean) to fit the K-NN on the SEP-28k dataset using embeddings computed from pre-trained ECAPA-TDNN and Wav2Vec2.0.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9976208806037903}, "created": {"value": false, "score": 4.0531158447265625e-06}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.999994158744812}, "created": {"value": true, "score": 0.9561930298805237}, "shared": {"value": false, "score": 2.2649765014648438e-06}}, "references": [{"label": "(Paszke et al. 2019)", "normalizedForm": "Paszke et al. 2019", "refKey": 0}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "LibriSpeech", "normalizedForm": "LibriSpeech", "offsetStart": 188, "offsetEnd": 199}, "context": "The authors have released several pre-trained feature embeddings with dimensions of 768 (base) and 1024 (large) and we are using the base one (768-dimensional) pre-trained on 960 hours of LibriSpeech dataset and then fine-tuned for ASR using CTC loss function by adding a linear layer on top of the contextual block.", "mentionContextAttributes": {"used": {"value": false, "score": 0.042430877685546875}, "created": {"value": false, "score": 0.008199036121368408}, "shared": {"value": false, "score": 2.384185791015625e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999258518218994}, "created": {"value": false, "score": 0.008199036121368408}, "shared": {"value": false, "score": 2.384185791015625e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Wav2", "normalizedForm": "Wav2", "offsetStart": 192, "offsetEnd": 196}, "version": {"rawForm": "2.0", "normalizedForm": "2.0"}, "context": "-We also provide an analysis of the impact of using different layers from Wav2Vec2.0 and their concatenation in SD and also investigate the impact of combining information from ECAPA-TDNN and Wav2Vec2.0", "mentionContextAttributes": {"used": {"value": true, "score": 0.8269919753074646}, "created": {"value": false, "score": 1.7404556274414062e-05}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.999994158744812}, "created": {"value": true, "score": 0.9561930298805237}, "shared": {"value": false, "score": 2.2649765014648438e-06}}, "references": [{"label": "(Paszke et al. 2019)", "normalizedForm": "Paszke et al. 2019", "refKey": 0}]}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Adam", "normalizedForm": "Adam", "offsetStart": 193, "offsetEnd": 197}, "context": "The downstream NN is trained with a batch size of 128 using a normal sum of cross entropy loss (L tot = L f + L d , L f :FluentBranch loss, L d :DisfluentBranch loss) function optimized by the Adam optimizer with a learning rate of 1e-2, and the training is stopped using an early stopping scheme on validation loss with the patience of seven.", "mentionContextAttributes": {"used": {"value": true, "score": 0.9999732971191406}, "created": {"value": false, "score": 1.7881393432617188e-06}, "shared": {"value": false, "score": 5.960464477539062e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.9999732971191406}, "created": {"value": false, "score": 3.6597251892089844e-05}, "shared": {"value": false, "score": 5.960464477539062e-07}}}, {"type": "software", "software-type": "software", "software-name": {"rawForm": "Wav2", "normalizedForm": "Wav2", "offsetStart": 205, "offsetEnd": 209}, "version": {"rawForm": "2.0", "normalizedForm": "2.0"}, "context": "-We provide a novel way for SD, which exploits the information from the fully connected (FC) layer of ECAPA-TDNN (Desplanques et al. 2020) and draws on speech information from several hidden layers of the Wav2vec2.0", "mentionContextAttributes": {"used": {"value": false, "score": 0.00011789798736572266}, "created": {"value": true, "score": 0.9561930298805237}, "shared": {"value": false, "score": 1.1920928955078125e-07}}, "documentContextAttributes": {"used": {"value": true, "score": 0.999994158744812}, "created": {"value": true, "score": 0.9561930298805237}, "shared": {"value": false, "score": 2.2649765014648438e-06}}, "references": [{"label": "(Paszke et al. 2019)", "normalizedForm": "Paszke et al. 2019", "refKey": 0}]}], "references": [{"refKey": 0, "tei": "<biblStruct xml:id=\"b0\">\n\t<analytic>\n\t\t<title level=\"a\" type=\"main\">Stuttering detection using speaker representations and self-supervised contextual embeddings</title>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Shakeel</forename><forename type=\"middle\">A</forename><surname>Sheikh</surname></persName>\n\t\t\t<idno type=\"ORCID\">0000-0001-9822-8422</idno>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Md</forename><surname>Sahidullah</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Fabrice</forename><surname>Hirsch</surname></persName>\n\t\t</author>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">Slim</forename><surname>Ouni</surname></persName>\n\t\t</author>\n\t\t<idno type=\"DOI\">10.1007/s10772-023-10032-1</idno>\n\t\t<idno>33E8107066A8684F48CC4960E39C8431</idno>\n\t</analytic>\n\t<monogr>\n\t\t<title level=\"j\">International Journal of Speech Technology</title>\n\t\t<title level=\"j\" type=\"abbrev\">Int J Speech Technol</title>\n\t\t<idno type=\"ISSN\">1381-2416</idno>\n\t\t<idno type=\"ISSNe\">1572-8110</idno>\n\t\t<imprint>\n\t\t\t<biblScope unit=\"volume\">26</biblScope>\n\t\t\t<biblScope unit=\"issue\">2</biblScope>\n\t\t\t<biblScope unit=\"page\" from=\"521\" to=\"530\" />\n\t\t\t<date type=\"published\" when=\"2023-06-26\" />\n\t\t\t<publisher>Springer Science and Business Media LLC</publisher>\n\t\t</imprint>\n\t</monogr>\n</biblStruct>\n"}, {"refKey": 3, "tei": "<biblStruct xml:id=\"b3\">\n\t<analytic>\n\t\t<title level=\"a\" type=\"main\">Wav2vec 2.0: A framework for self-supervised learning of speech representations</title>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">A</forename><surname>Baevski</surname></persName>\n\t\t</author>\n\t</analytic>\n\t<monogr>\n\t\t<title level=\"m\">Advances in NIPS</title>\n\t\t<imprint>\n\t\t\t<date>2020</date>\n\t\t\t<biblScope unit=\"volume\">33</biblScope>\n\t\t\t<biblScope unit=\"page\">12460</biblScope>\n\t\t</imprint>\n\t</monogr>\n</biblStruct>\n"}, {"refKey": 27, "tei": "<biblStruct xml:id=\"b27\">\n\t<monogr>\n\t\t<author>\n\t\t\t<persName><forename type=\"first\">M</forename><surname>Ravanelli</surname></persName>\n\t\t</author>\n\t\t<idno>arXiv:2106.04624</idno>\n\t\t<title level=\"m\">SpeechBrain: A general-purpose speech toolkit</title>\n\t\t<imprint>\n\t\t\t<date>2021</date>\n\t\t</imprint>\n\t</monogr>\n</biblStruct>\n"}], "runtime": 13992, "id": "0800339aced20a2ac31e80db4df9823fd3eca36a", "metadata": {"id": "0800339aced20a2ac31e80db4df9823fd3eca36a"}, "original_file_path": "../../datalake/Samuel/SOFTware-Sync/downloads/xml/hal-03629758.grobid.tei.xml", "file_name": "hal-03629758.grobid.tei.xml"}